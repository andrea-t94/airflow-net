1. Decided to generate 3 instructions per request. This proved to be really cost effective using batch messages, I spent <2$ (with one rewuest for one instruction at time >5)
2. created a lightweight DAG parser that checks basic things (e.g. missing import checks, Xcom, task groups, connection and task are properly working, Operators and Sesnor are properly inputed)
3. looking for quantised versions that can work decently on my mac m1 pro. I noticed that batch has to be extremely low, limited also the output tolkens and now looking for a cross platform inference solution that works with cpu -> llama.cpp with gguf models
4. using M1 GPU as backed from llama.cppu I increased latency of 2.5X, and offlading multiple models (4 workers) I've increased throughut, halving total gen time
5. I am now using python llama.cpp server wrapper with n_batch = n_ctx 2048, that should be sufficient to cover prompt + output without cutting it. Client fires in paralled workers (4), same perf of 4., since on server side it's still sequential. 4. should be better because we were putting 4 models thouhg. My take is that continuous batching and optimisations of the server increased latency, while mem bandwidht problems of having 4 models in GPU decreased it for the 4. -> I want to create a script that reports all of those per by different methods so that I can write a blogpost
6. Huge improvements with c++ llama server, with true parallelism. Thanks to continous batching I can parallelly fills multiple requests to one single model, thus increasing GPU utilisation rate while minimising mem bandwidht (real bottleneck for my simple implementation)
7. Again huge improvements, testing with llama bench I've seen that we are almost at mem bounded at 182 t/s across 8 workers. We can do it because ctx size has been tuned on expected DAG dataset. Only one worker we were compute bound, we were not usinng GPU well.
8. run a full data processing at 0.18DAG/s. Considering avg DAG token len to be 1500-2000 tokesn, this is perfectly in line with 180 t/s
9. Fine tuning will be on Colab+Unslot since I did some calculation and, even using MLX on M1, it will take approx 20h for 3 epochs with LoRa. Training is compute bound, with the benchmarks we see that M1 can arrive to 1200 t/s in inference, considering training 3x compute effort we can aim for 300-400 t/s. Assuming 1K tokens per instruction per 7K instructions, we need to gent 7M tokens -> 20h
10. I am adding distilled instructions from Qwen 2.5 coder 32B, suing magpie technique
11. I will finetune on COlab+unsloth with NVIDIA because