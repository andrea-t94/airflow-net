1. Decided to generate 3 instructions per request. This proved to be really cost effective using batch messages, I spent <2$ (with one rewuest for one instruction at time >5)
2. created a lightweight DAG parser that checks basic things (e.g. missing import checks, Xcom, task groups, connection and task are properly working, Operators and Sesnor are properly inputed)
3. looking for quantised versions that can work decently on my mac m1 pro. I noticed that batch has to be extremely low, limited also the output tolkens and now looking for a cross platform inference solution that works with cpu -> llama.cpp with gguf models
4. using M1 GPU as backed from llama.cppu I increased latency of 2.5X, and offlading multiple models (4 workers) I've increased throughut, halving total gen time
5. I am now using python llama.cpp server wrapper with n_batch = n_ctx 2048, that should be sufficient to cover prompt + output without cutting it. Client fires in paralled workers (4), same perf of 4., since on server side it's still sequential. 4. should be better because we were putting 4 models thouhg. My take is that continuous batching and optimisations of the server increased latency, while mem bandwidht problems of having 4 models in GPU decreased it for the 4. -> I want to create a script that reports all of those per by different methods so that I can write a blogpost
6. Huge improvements with c++ llama server, with true parallelism. Thanks to continous batching I can parallelly fills multiple requests to one single model, thus increasing GPU utilisation rate while minimising mem bandwidht (real bottleneck for my simple implementation)
7. Again huge improvements, testing with llama bench I've seen that we are almost at mem bounded at 182 t/s across 8 workers. We can do it because ctx size has been tuned on expected DAG dataset. Only one worker we were compute bound, we were not usinng GPU well.