{"instruction": "Create a dynamic task routing DAG that randomly selects one of four predefined branches during each daily run. Design a workflow that demonstrates flexible task execution with random path selection.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_task_routing',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef select_branch():\n    return choice(['branch1', 'branch2', 'branch3', 'branch4'])\n\ndef execute_task(task_id):\n    return PythonOperator(\n        task_id=task_id,\n        python_callable=select_branch,\n        dag=dag,\n    )\n\nbranch1_task = execute_task('branch1')\nbranch2_task = execute_task('branch2')\nbranch3_task = execute_task('branch3')\nbranch4_task = execute_task('branch4')\n\nbranch1_task >> branch2_task\nbranch2_task >> branch3_task\nbranch3_task >> branch4_task", "metadata": {"file_name": "dag_0_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using the BranchPythonOperator, illustrating how tasks can dynamically choose their execution path based on runtime logic. Implement a workflow that randomly selects and executes one of multiple parallel task branches.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import BranchPythonOperator, PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_branch_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef branch_task():\n    return choice(['task1', 'task2'])\n\ndef task1():\n    print('Executing task1')\n\ndef task2():\n    print('Executing task2')\n\ntask1_branch = PythonOperator(\n    task_id='task1_branch',\n    python_callable=branch_task,\n    dag=dag,\n)\n\ntask1_branch >> BashOperator(\n    task_id='task1',\n    bash_command='echo \"Executing task1\"',\n    dag=dag,\n)\n\ntask2_branch = PythonOperator(\n    task_id='task2_branch',\n    python_callable=branch_task,\n    dag=dag,\n)\n\ntask2_branch >> BashOperator(\n    task_id='task2',\n    bash_command='echo \"Executing task2\"',\n    dag=dag,\n)", "metadata": {"file_name": "dag_0_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Design a demonstration DAG that showcases multiple Bash command executions with different task dependencies. Create a workflow that runs several echo commands and demonstrates task chaining and conditional execution.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'bash_command_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef echo_command():\n    bash = BashOperator(\n        task_id='echo_command',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\ndef echo_command_with_condition():\n    bash = BashOperator(\n        task_id='echo_command_with_condition',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: '{{ task_instance.xcom_pull(task_ids=\"echo_command\") }}',\n        dag=dag,\n    )\n\ndef echo_command_with_task_chain():\n    bash = BashOperator(\n        task_id='echo_command_with_task_chain',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: '{{ task_instance.xcom_pull(task_ids=\"echo_command\") }}',\n        dag=dag,\n    )\n\ndef echo_command_with_task_chain_and_condition():\n    bash = BashOperator(\n        task_id='echo_command_with_task_chain_and_condition',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: '{{ task_instance.xcom_pull(task_ids=\"echo_command\") }}',\n        dag=dag,\n    )\n\nbash_command_execution = BashOperator(\n    task_id='bash_command_execution',\n    bash_command='echo \"Hello, World!\"',\n    dag=dag,\n)\n\nbash_command_execution >> echo_command()\nbash_command_execution >> echo_command_with_condition()\nbash_command_execution >> echo_command_with_task_chain()\nbash_command_execution >> echo_command_with_task_chain_and_condition()", "metadata": {"file_name": "dag_1_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Build an Airflow pipeline that illustrates task orchestration techniques using BashOperator, including parallel task generation, sequential dependencies, and handling potential task skipping scenarios. Develop a DAG that explores different ways of defining and connecting computational tasks.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'parallel_task_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef parallel_task():\n    bash_task = BashOperator(\n        task_id='parallel_task',\n        bash_command='echo \"Parallel task executed successfully\"',\n        dag=dag,\n    )\n\n    bash_task2 = BashOperator(\n        task_id='parallel_task2',\n        bash_command='echo \"Parallel task2 executed successfully\"',\n        dag=dag,\n    )\n\n    bash_task2.set_upstream(bash_task)\n\n# Task 1: Parallel task\nparallel_task_task = PythonOperator(\n    task_id='parallel_task_task',\n    python_callable=parallel_task,\n    dag=dag,\n)\n\n# Task 2: Parallel task2\nparallel_task2_task = PythonOperator(\n    task_id='parallel_task2_task',\n    python_callable=parallel_task,\n    dag=dag,\n)\n\nparallel_task_task >> parallel_task2_task", "metadata": {"file_name": "dag_1_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Construct an example workflow that simulates a basic data processing pipeline using shell commands, showing how Airflow can coordinate multiple computational steps with different execution patterns. Create a DAG that demonstrates dynamic task generation and conditional execution strategies.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'basic_data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ngenerate_data = BashOperator(\n    task_id='generate_data',\n    bash_command='echo \"Generating data...\"',\n    dag=dag,\n)\n\nprocess_data = BashOperator(\n    task_id='process_data',\n    bash_command='echo \"Processing data...\"',\n    dag=dag,\n)\n\nclean_data = BashOperator(\n    task_id='clean_data',\n    bash_command='echo \"Cleaning data...\"',\n    dag=dag,\n)\n\nvalidate_data = BashOperator(\n    task_id='validate_data',\n    bash_command='echo \"Validating data...\"',\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\ngenerate_data >> process_data\nprocess_data >> clean_data\nclean_data >> validate_data\nvalidate_data >> end", "metadata": {"file_name": "dag_1_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Design a dynamic workflow that randomly selects and executes one of four predefined branches during each daily run. Create a DAG that demonstrates flexible task routing using Python's random selection and Airflow's branching capabilities.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_branch():\n    branches = ['branch1', 'branch2', 'branch3', 'branch4']\n    return choice(branches)\n\ndef execute_branch(branch):\n    print(f\"Executing branch: {branch}\")\n\nwith DAG(\n    dag_id='dynamic_workflow',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['example'],\n) as dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=random_branch,\n        provide_context=True,\n    )\n\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=execute_branch,\n        provide_context=True,\n    )\n\n    task1 >> task2", "metadata": {"file_name": "dag_2_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Design a workflow that demonstrates conditional task execution based on the day of the week. Create a DAG that branches tasks differently for weekdays and weekends, showing how Airflow can dynamically route workflow paths.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'conditional_task_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef weekday_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\ndef weekend_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\nweekday_task = PythonOperator(\n    task_id='weekday_task',\n    python_callable=weekday_task,\n    dag=dag,\n)\n\nweekend_task = PythonOperator(\n    task_id='weekend_task',\n    python_callable=weekend_task,\n    dag=dag,\n)\n\nweekday_task >> weekend_task", "metadata": {"file_name": "dag_3_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single initial task triggers a random selection of processing paths. Create a workflow that demonstrates how data processing can be dynamically routed through different branches based on runtime conditions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.email import EmailOperator\n\ndef random_selection():\n    import random\n    return random.choice(['path1', 'path2', 'path3'])\n\ndef process_data():\n    import pandas as pd\n    df = pd.read_csv('data.csv')\n    return df\n\ndef send_email(data):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = f\"Subject: Data Processing Report\\n\\n{data}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_with_data(data):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = f\"Subject: Data Processing Report\\n\\n{data}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_with_data_and_path(data, path):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = f\"Subject: Data Processing Report\\n\\n{data}\\n\\nProcessing Path: {path}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_with_data_and_path_and_data(data, path, data_path):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = f\"Subject: Data Processing Report\\n\\n{data}\\n\\nProcessing Path: {path}\\n\\nData Path: {data_path}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_with_data_and_path_and_data_and_path(data, path, data_path, data_path2):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = f\"Subject: Data Processing Report\\n\\n{data}\\n\\nProcessing Path: {path}\\n\\nData Path: {data_path}\\n\\nData Path 2: {data_path2}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_with_data_and_path_and_data_and_path_and_data(data, path", "metadata": {"file_name": "dag_0_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using a branching mechanism that randomly chooses between multiple potential workflow paths. Implement a DAG that illustrates how to dynamically select and execute different task sequences based on runtime decisions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'conditional_task_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef conditional_task_execution():\n    print(\"Task 1 executed\")\n\ndef task_2():\n    print(\"Task 2 executed\")\n\ndef task_3():\n    print(\"Task 3 executed\")\n\ndef task_4():\n    print(\"Task 4 executed\")\n\ndef task_5():\n    print(\"Task 5 executed\")\n\ndef task_6():\n    print(\"Task 6 executed\")\n\ndef task_7():\n    print(\"Task 7 executed\")\n\ndef task_8():\n    print(\"Task 8 executed\")\n\ndef task_9():\n    print(\"Task 9 executed\")\n\ndef task_10():\n    print(\"Task 10 executed\")\n\ndef task_11():\n    print(\"Task 11 executed\")\n\ndef task_12():\n    print(\"Task 12 executed\")\n\ndef task_13():\n    print(\"Task 13 executed\")\n\ndef task_14():\n    print(\"Task 14 executed\")\n\ndef task_15():\n    print(\"Task 15 executed\")\n\ndef task_16():\n    print(\"Task 16 executed\")\n\ndef task_17():\n    print(\"Task 17 executed\")\n\ndef task_18():\n    print(\"Task 18 executed\")\n\ndef task_19():\n    print(\"Task 19 executed\")\n\ndef task_20():\n    print(\"Task 20 executed\")\n\ndef task_21():\n    print(\"Task 21 executed\")\n\ndef task_22():\n    print(\"Task 22 executed\")\n\ndef task_23():\n    print(\"Task 23 executed\")\n\ndef task_24():\n    print(\"Task 24 executed\")\n\ndef task_25():\n    print(\"Task 25 executed\")\n\ndef task_26():\n    print(\"Task 26 executed\")\n\ndef task_27():\n    print(\"Task 27 executed\")\n\ndef task_28():\n    print(\"Task 28 executed\")\n\ndef task_29():\n    print(\"Task 29 executed\")\n\ndef task_30():\n    print(\"Task 30 executed\")\n\ndef task_31():\n    print(\"Task 31 executed\")\n\ndef task_32():\n    print(\"Task 32 executed\")\n\ndef task_33():\n    print(\"Task 33 executed\")\n\ndef task_34():\n    print(\"Task 34 executed\")\n\ndef task_35():\n    print(\"Task 35 executed\")\n\ndef task_36():\n    print(\"Task 36 executed\")\n\ndef task_37():\n    print(\"Task 37 executed\")\n\ndef task_38():\n    print(\"Task 38 executed\")\n\ndef task_39():\n    print(\"Task 39 executed\")\n\ndef task_40():\n    print(\"Task 40 executed\")\n\ndef task_41():\n    print(\"Task 41 executed\")\n\ndef task_42():\n    print(\"Task 42 executed\")\n\ndef task_43():\n    print(\"Task 43 executed\")\n\ndef task_44():\n    print(\"Task 44 executed\")\n\ndef task_45():\n    print(\"Task 45 executed\")\n\ndef task_46():\n    print(\"Task 46 executed\")\n\ndef task_47():\n    print(\"Task 47 executed\")\n\ndef task_48():\n    print(\"Task 48 executed\")\n\ndef task_49():\n    print(\"Task 49 executed\")\n\ndef task_50():\n    print(\"Task 50 executed\")\n\ndef task_51():\n    print(\"Task 51 executed\")\n\ndef task_52():\n    print(\"Task 52 executed\")\n\ndef task_53():\n    print(\"Task 53 executed\")\n\ndef task_54():\n    print(\"Task 54 executed\")\n\ndef task_55():\n    print(\"Task 55 executed\")\n\ndef task_56():\n    print(\"Task 56 executed\")\n\ndef task_57():\n    print(\"Task 57 executed\")\n\ndef task_58():\n    print(\"Task 58 executed\")\n\ndef task_59():\n    print(\"Task 59 executed\")\n\ndef task_60():\n    print(\"Task 60 executed\")\n\ndef task_61():\n    print(\"Task 61 executed\")\n\ndef task_62():\n    print(\"Task 62 executed\")\n\ndef task_63():\n    print", "metadata": {"file_name": "dag_2_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single task randomly determines which subsequent tasks will be executed. Create a workflow that demonstrates how to implement probabilistic routing in data processing pipelines using Airflow's task branching decorator.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_routing',\n    default_args=default_args,\n    schedule_interval='@once',\n)\n\ndef random_task():\n    return 'task1'\n\ndef task1():\n    print('Task 1 executed')\n\ndef task2():\n    print('Task 2 executed')\n\ndef task3():\n    print('Task 3 executed')\n\ndef task4():\n    print('Task 4 executed')\n\ndef task5():\n    print('Task 5 executed')\n\ndef task6():\n    print('Task 6 executed')\n\ndef task7():\n    print('Task 7 executed')\n\ndef task8():\n    print('Task 8 executed')\n\ndef task9():\n    print('Task 9 executed')\n\ndef task10():\n    print('Task 10 executed')\n\ndef task11():\n    print('Task 11 executed')\n\ndef task12():\n    print('Task 12 executed')\n\ndef task13():\n    print('Task 13 executed')\n\ndef task14():\n    print('Task 14 executed')\n\ndef task15():\n    print('Task 15 executed')\n\ndef task16():\n    print('Task 16 executed')\n\ndef task17():\n    print('Task 17 executed')\n\ndef task18():\n    print('Task 18 executed')\n\ndef task19():\n    print('Task 19 executed')\n\ndef task20():\n    print('Task 20 executed')\n\ndef task21():\n    print('Task 21 executed')\n\ndef task22():\n    print('Task 22 executed')\n\ndef task23():\n    print('Task 23 executed')\n\ndef task24():\n    print('Task 24 executed')\n\ndef task25():\n    print('Task 25 executed')\n\ndef task26():\n    print('Task 26 executed')\n\ndef task27():\n    print('Task 27 executed')\n\ndef task28():\n    print('Task 28 executed')\n\ndef task29():\n    print('Task 29 executed')\n\ndef task30():\n    print('Task 30 executed')\n\ndef task31():\n    print('Task 31 executed')\n\ndef task32():\n    print('Task 32 executed')\n\ndef task33():\n    print('Task 33 executed')\n\ndef task34():\n    print('Task 34 executed')\n\ndef task35():\n    print('Task 35 executed')\n\ndef task36():\n    print('Task 36 executed')\n\ndef task37():\n    print('Task 37 executed')\n\ndef task38():\n    print('Task 38 executed')\n\ndef task39():\n    print('Task 39 executed')\n\ndef task40():\n    print('Task 40 executed')\n\ndef task41():\n    print('Task 41 executed')\n\ndef task42():\n    print('Task 42 executed')\n\ndef task43():\n    print('Task 43 executed')\n\ndef task44():\n    print('Task 44 executed')\n\ndef task45():\n    print('Task 45 executed')\n\ndef task46():\n    print('Task 46 executed')\n\ndef task47():\n    print('Task 47 executed')\n\ndef task48():\n    print('Task 48 executed')\n\ndef task49():\n    print('Task 49 executed')\n\ndef task50():\n    print('Task 50 executed')\n\ndef task51():\n    print('Task 51 executed')\n\ndef task52():\n    print('Task 52 executed')\n\ndef task53():\n    print('Task 53 executed')\n\ndef task54():\n    print('Task 54 executed')\n\ndef task55():\n    print('Task 55 executed')\n\ndef task56():\n    print('Task 56 executed')\n\ndef task57():\n    print('Task 57 executed')\n\ndef task58():\n    print('Task 58 executed')\n\ndef task59():\n    print('Task 59 executed')\n\ndef task60():\n    print('Task 60 executed')\n\ndef task61():\n    print('Task 61 executed')\n\ndef task62():\n    print('Task 62 executed')\n\ndef task63():\n    print('Task 63 executed')\n\ndef task64():\n    print('Task 64 executed')\n\ndef task65():\n    print('Task 65 executed')\n\ndef task66():\n    print('Task 66 executed')\n\ndef task67", "metadata": {"file_name": "dag_2_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Design a data quality monitoring workflow that tracks data integrity during an ingestion process. Create a DAG that handles both successful data processing and error scenarios with clear branching and reporting.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_quality_monitoring',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ningest_data = BashOperator(\n    task_id='ingest_data',\n    bash_command='python /path/to/ingest_data.py',\n    dag=dag,\n)\n\ncheck_data_quality = PythonOperator(\n    task_id='check_data_quality',\n    python_callable=check_data_quality,\n    op_kwargs={'ingest_data': ingest_data},\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> ingest_data >> check_data_quality >> end", "metadata": {"file_name": "dag_4_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Build an Airflow pipeline that demonstrates conditional task execution based on data integrity checks. Implement a workflow that can automatically route tasks differently depending on whether data passes or fails validation.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_integrity_check',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef check_data_integrity():\n    print(\"Data integrity check completed successfully.\")\n\ndef route_task(task):\n    if task.xcom_pull(task_ids='check_data_integrity'):\n        task.do_xcom_push(key='route', value='success')\n    else:\n        task.do_xcom_push(key='route', value='failure')\n\ncheck_data_integrity = PythonOperator(\n    task_id='check_data_integrity',\n    python_callable=check_data_integrity,\n    provide_context=True,\n)\n\nroute_task = PythonOperator(\n    task_id='route_task',\n    python_callable=route_task,\n    op_kwargs={'task': check_data_integrity},\n)\n\ncheck_data_integrity >> route_task", "metadata": {"file_name": "dag_4_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Build an Airflow pipeline that showcases branching logic using BranchDayOfWeekOperator to demonstrate conditional task selection. Implement a workflow that executes different tasks based on whether the current day matches specific day criteria.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndef branch_day_of_week():\n    today = datetime.now()\n    day_of_week = today.weekday()\n    if day_of_week == 0:\n        return \"Monday\"\n    elif day_of_week == 1:\n        return \"Tuesday\"\n    elif day_of_week == 2:\n        return \"Wednesday\"\n    elif day_of_week == 3:\n        return \"Thursday\"\n    elif day_of_week == 4:\n        return \"Friday\"\n    elif day_of_week == 5:\n        return \"Saturday\"\n    elif day_of_week == 6:\n        return \"Sunday\"\n\ndef execute_task(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task2(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task3(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task4(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task5(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task6(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task7(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task8(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"Sunday\":\n        return \"Sunday task\"\n\ndef execute_task9(task):\n    if task == \"Monday\":\n        return \"Monday task\"\n    elif task == \"Tuesday\":\n        return \"Tuesday task\"\n    elif task == \"Wednesday\":\n        return \"Wednesday task\"\n    elif task == \"Thursday\":\n        return \"Thursday task\"\n    elif task == \"Friday\":\n        return \"Friday task\"\n    elif task == \"Saturday\":\n        return \"Saturday task\"\n    elif task == \"", "metadata": {"file_name": "dag_3_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Develop an Airflow DAG that illustrates dynamic task routing using day-of-week conditions for data processing workflows. Create a scenario where tasks are selectively executed based on whether the current day meets predefined scheduling requirements.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_day_of_week_task_routing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef process_data():\n    print(\"Processing data\")\n\ndef process_data_with_day_of_week():\n    print(\"Processing data with day of week\")\n\ndef process_data_with_day_of_week_and_time():\n    print(\"Processing data with day of week and time\")\n\ndef process_data_with_day_of_week_and_time_and_time():\n    print(\"Processing data with day of week, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and", "metadata": {"file_name": "dag_3_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
{"instruction": "Develop a data processing pipeline that includes stages for data ingestion, analysis, integrity verification, and error handling. Create a workflow that can gracefully manage data quality issues and generate appropriate reports or alerts.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators import (StageToRedshiftOperator, LoadFactOperator,\n                                LoadDimensionTableOperator, DataQualityOperator)\n\n# Define Airflow DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2022, 1, 1),\n    'depends_on_past': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG('udacity_dag',\n          default_args=default_args,\n          description='Airflow DAG for Data Integration',\n          schedule_interval='@hourly',\n          max_active_runs=1)\n\n# Stage data to Redshift\nstage_to_redshift = PythonOperator(\n    task_id='stage_to_redshift',\n    python_callable=StageToRedshiftOperator,\n    dag=dag,\n    redshift_conn_id='redshift',\n    aws_credentials_id='aws_credentials',\n    table='staging_tables',\n    s3_bucket='udacity-dend',\n    s3_key='data/airflow/airflow-dag-data/airflow-dag-data-01.csv',\n    region='us-west-2',\n    copy_options='json',\n    aws_conn_id='aws_credentials',\n    redshift_conn_id='redshift',\n    dag=dag,\n)\n\n# Load fact table\nload_fact = PythonOperator(\n    task_id='load_fact',\n    python_callable=LoadFactOperator,\n    dag=dag,\n    redshift_conn_id='redshift',\n    aws_credentials_id='aws_credentials',\n    table='fact_table',\n    s3_bucket='udacity-dend',\n    s3_key='data/airflow/airflow-dag-data/airflow-dag-data-02.csv',\n    region='us-west-2',\n    copy_options='json',\n    aws_conn_id='aws_credentials',\n    redshift_conn_id='redshift',\n    dag=dag,\n)\n\n# Load dimension table\nload_dimension = PythonOperator(\n    task_id='load_dimension',\n    python_callable=LoadDimensionTableOperator,\n    dag=dag,\n    redshift_conn_id='redshift',\n    aws_credentials_id='aws_credentials',\n    table='dimension_table',\n    s3_bucket='udacity-dend',\n    s3_key='data/airflow/airflow-dag-data/airflow-dag-data-03.csv',\n    region='us-west-2',\n    copy_options='json',\n    aws_conn_id='aws_credentials',\n    redshift_conn_id='redshift',\n    dag=dag,\n)\n\n# Data quality check\ndata_quality = DataQualityOperator(\n    task_id='data_quality',\n    redshift_conn_id='redshift',\n    aws_credentials_id='aws_credentials',\n    table='fact_table',\n    dag=dag,\n)\n\n# Stage to Redshift\nstage_to_redshift >> load_fact >> load_dimension >> data_quality", "metadata": {"file_name": "dag_4_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "backend": "llama.cpp-parallel"}}
