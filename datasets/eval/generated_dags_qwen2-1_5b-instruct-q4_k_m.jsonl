{"instruction": "Create a dynamic task routing DAG that randomly selects one of four predefined branches during each daily run. Design a workflow that demonstrates flexible task execution with random path selection.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_task_routing',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef select_branch():\n    return choice(['branch1', 'branch2', 'branch3', 'branch4'])\n\ndef execute_task(task_id):\n    return PythonOperator(\n        task_id=task_id,\n        python_callable=select_branch,\n        dag=dag,\n    )\n\nbranch1_task = execute_task('branch1')\nbranch2_task = execute_task('branch2')\nbranch3_task = execute_task('branch3')\nbranch4_task = execute_task('branch4')\n\nbranch1_task >> branch2_task\nbranch2_task >> branch3_task\nbranch3_task >> branch4_task", "metadata": {"file_name": "dag_0_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 1, "backend": "llama.cpp"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using the BranchPythonOperator, illustrating how tasks can dynamically choose their execution path based on runtime logic. Implement a workflow that randomly selects and executes one of multiple parallel task branches.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import BranchPythonOperator, PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_branch_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef branch_task():\n    return 'task1'\n\ndef task1():\n    print('Task 1 executed')\n\ndef task2():\n    print('Task 2 executed')\n\ndef task3():\n    print('Task 3 executed')\n\ndef task4():\n    print('Task 4 executed')\n\ndef task5():\n    print('Task 5 executed')\n\ndef task6():\n    print('Task 6 executed')\n\ndef task7():\n    print('Task 7 executed')\n\ndef task8():\n    print('Task 8 executed')\n\ndef task9():\n    print('Task 9 executed')\n\ndef task10():\n    print('Task 10 executed')\n\ndef task11():\n    print('Task 11 executed')\n\ndef task12():\n    print('Task 12 executed')\n\ndef task13():\n    print('Task 13 executed')\n\ndef task14():\n    print('Task 14 executed')\n\ndef task15():\n    print('Task 15 executed')\n\ndef task16():\n    print('Task 16 executed')\n\ndef task17():\n    print('Task 17 executed')\n\ndef task18():\n    print('Task 18 executed')\n\ndef task19():\n    print('Task 19 executed')\n\ndef task20():\n    print('Task 20 executed')\n\ndef task21():\n    print('Task 21 executed')\n\ndef task22():\n    print('Task 22 executed')\n\ndef task23():\n    print('Task 23 executed')\n\ndef task24():\n    print('Task 24 executed')\n\ndef task25():\n    print('Task 25 executed')\n\ndef task26():\n    print('Task 26 executed')\n\ndef task27():\n    print('Task 27 executed')\n\ndef task28():\n    print('Task 28 executed')\n\ndef task29():\n    print('Task 29 executed')\n\ndef task30():\n    print('Task 30 executed')\n\ndef task31():\n    print('Task 31 executed')\n\ndef task32():\n    print('Task 32 executed')\n\ndef task33():\n    print('Task 33 executed')\n\ndef task34():\n    print('Task 34 executed')\n\ndef task35():\n    print('Task 35 executed')\n\ndef task36():\n    print('Task 36 executed')\n\ndef task37():\n    print('Task 37 executed')\n\ndef task38():\n    print('Task 38 executed')\n\ndef task39():\n    print('Task 39 executed')\n\ndef task40():\n    print('Task 40 executed')\n\ndef task41():\n    print('Task 41 executed')\n\ndef task42():\n    print('Task 42 executed')\n\ndef task43():\n    print('Task 43 executed')\n\ndef task44():\n    print('Task 44 executed')\n\ndef task45():\n    print('Task 45 executed')\n\ndef task46():\n    print('Task 46 executed')\n\ndef task47():\n    print('Task 47 executed')\n\ndef task48():\n    print('Task 48 executed')\n\ndef task49():\n    print('Task 49 executed')\n\ndef task50():\n    print('Task 50 executed')\n\ndef task51():\n    print('Task 51 executed')\n\ndef task52():\n    print('Task 52 executed')\n\ndef task53():\n    print('Task 53 executed')\n\ndef task54():\n    print('Task 54 executed')\n\ndef task55():\n    print('Task 55 executed')\n\ndef task56():\n    print('Task 56 executed')\n\ndef task57():\n    print('Task 57 executed')\n\ndef task58():\n    print('Task 58 executed')\n\ndef task59():\n    print('Task 59 executed')\n\ndef task60():\n    print('Task 60 executed')\n\ndef task61():\n    print('Task 61 executed')\n\ndef task62():\n    print('Task 62 executed')\n\ndef task63():\n    print('Task 63 executed')\n\ndef task64():\n    print('Task 64 executed')\n\ndef task65():\n    print('Task 65 executed')\n\ndef task66():\n    print('Task", "metadata": {"file_name": "dag_0_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 2, "backend": "llama.cpp"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single initial task triggers a random selection of processing paths. Create a workflow that demonstrates how data processing can be dynamically routed through different branches based on runtime conditions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.email import EmailOperator\n\ndef random_selection():\n    import random\n    return random.choice(['path1', 'path2', 'path3'])\n\ndef process_data():\n    import pandas as pd\n    df = pd.read_csv('data.csv')\n    return df\n\ndef send_email(data):\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.base import MIMEBase\n    from email import encoders\n\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = 'recipient_email@example.com'\n    msg['Subject'] = 'Data Processing Report'\n\n    body = 'Here is the data processing report:\\n\\n' + data.to_string(index=False)\n    msg.attach(MIMEText(body, 'plain'))\n\n    filename = 'processed_data.csv'\n    attachment = open('processed_data.csv', 'rb')\n\n    part = MIMEBase('application', 'octet-stream')\n    part.set_payload(attachment.read())\n    encoders.encode_base64(part)\n    part.add_header('Content-Disposition', 'attachment', filename=filename)\n    msg.attach(part)\n\n    server = smtplib.SMTP('smtp.gmail.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_email_password')\n    text = msg.as_string()\n    server.sendmail('your_email@example.com', 'recipient_email@example.com', text)\n    server.quit()\n\ndef send_email_task(data):\n    EmailOperator(\n        task_id='send_email',\n        to='recipient_email@example.com',\n        subject='Data Processing Report',\n        html_content=data,\n        dag=dag\n    )\n\ndef process_data_task(data):\n    data = process_data()\n    EmailOperator(\n        task_id='send_email',\n        to='recipient_email@example.com',\n        subject='Data Processing Report',\n        html_content=data.to_string(index=False),\n        dag=dag\n    )\n\ndef run_dag():\n    dag = DAG(\n        'data_processing_dag',\n        start_date=datetime(2023, 1, 1),\n        schedule_interval='@daily',\n        catchup=False\n    )\n\n    initial_task = DummyOperator(\n        task_id='initial_task',\n        dag=dag\n    )\n\n    random_selection_task = PythonOperator(\n        task_id='random_selection',\n        python_callable=random_selection,\n        dag=dag\n    )\n\n    process_data_task = PythonOperator(\n        task_id='process_data',\n        python_callable=process_data,\n        dag=dag\n    )\n\n    send_email_task = PythonOperator(\n        task_id='send_email',\n        python_callable=send_email,\n        op_args=[data],\n        dag=dag\n    )\n\n    initial_task >> random_selection_task >> process_data_task >> send_email_task\n    return dag\n\nif __name__ == '__main__':\n    run_dag()", "metadata": {"file_name": "dag_0_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 3, "backend": "llama.cpp"}}
{"instruction": "Design a demonstration DAG that showcases multiple Bash command executions with different task dependencies. Create a workflow that runs several echo commands and demonstrates task chaining and conditional execution.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'bash_command_execution',\n    default_args=default_args,\n    schedule_interval=None,\n)\n\ndef echo_command():\n    bash = BashOperator(\n        task_id='echo_command',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\ndef echo_command_with_condition():\n    bash = BashOperator(\n        task_id='echo_command_with_condition',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: 'condition1',\n        dag=dag,\n    )\n\ndef echo_command_with_task_chain():\n    bash = BashOperator(\n        task_id='echo_command_with_task_chain',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: 'condition2',\n        dag=dag,\n    )\n\ndef echo_command_with_task_chain_and_condition():\n    bash = BashOperator(\n        task_id='echo_command_with_task_chain_and_condition',\n        bash_command='echo \"Hello, World!\"',\n        if_cond=lambda: 'condition3',\n        dag=dag,\n    )\n\necho_command()\necho_command_with_condition()\necho_command_with_task_chain()\necho_command_with_task_chain_and_condition()", "metadata": {"file_name": "dag_1_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 4, "backend": "llama.cpp"}}
{"instruction": "Build an Airflow pipeline that illustrates task orchestration techniques using BashOperator, including parallel task generation, sequential dependencies, and handling potential task skipping scenarios. Develop a DAG that explores different ways of defining and connecting computational tasks.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'parallel_tasks',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef parallel_task():\n    bash_task = BashOperator(\n        task_id='parallel_task',\n        bash_command='echo \"Parallel task executed\"',\n        dag=dag,\n    )\n\n    bash_task2 = BashOperator(\n        task_id='parallel_task2',\n        bash_command='echo \"Parallel task executed\"',\n        dag=dag,\n    )\n\n    bash_task2.set_upstream(bash_task)\n\n# Define the Python function to be executed for each task\ndef execute_task(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task2(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task3(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task4(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task5(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task6(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task7(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task8(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task9(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task10(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task11(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task12(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash_command='echo \"Executing task: {}\"'.format(task_id),\n        dag=dag,\n    )\n\n    bash_task.set_upstream(bash_task2)\n\n# Define the Python function to be executed for each task\ndef execute_task13(task_id):\n    bash_task = BashOperator(\n        task_id=task_id,\n        bash", "metadata": {"file_name": "dag_1_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 5, "backend": "llama.cpp"}}
{"instruction": "Construct an example workflow that simulates a basic data processing pipeline using shell commands, showing how Airflow can coordinate multiple computational steps with different execution patterns. Create a DAG that demonstrates dynamic task generation and conditional execution strategies.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'basic_data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef generate_data():\n    print(\"Generating data...\")\n\ndef process_data():\n    print(\"Processing data...\")\n\ndef clean_data():\n    print(\"Cleaning data...\")\n\ndef transform_data():\n    print(\"Transforming data...\")\n\ndef load_data():\n    print(\"Loading data...\")\n\ngenerate_data_task = PythonOperator(\n    task_id='generate_data',\n    python_callable=generate_data,\n    dag=dag,\n)\n\nprocess_data_task = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    dag=dag,\n)\n\nclean_data_task = PythonOperator(\n    task_id='clean_data',\n    python_callable=clean_data,\n    dag=dag,\n)\n\ntransform_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload_data_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag,\n)\n\ngenerate_data_task >> process_data_task\nprocess_data_task >> clean_data_task\nclean_data_task >> transform_data_task\ntransform_data_task >> load_data_task", "metadata": {"file_name": "dag_1_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 6, "backend": "llama.cpp"}}
{"instruction": "Design a dynamic workflow that randomly selects and executes one of four predefined branches during each daily run. Create a DAG that demonstrates flexible task routing using Python's random selection and Airflow's branching capabilities.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_branch():\n    branches = ['branch1', 'branch2', 'branch3', 'branch4']\n    return choice(branches)\n\ndef execute_branch(branch):\n    print(f\"Executing branch: {branch}\")\n\nwith DAG(\n    dag_id='dynamic_workflow_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['example'],\n) as dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=random_branch,\n        provide_context=True,\n    )\n\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=execute_branch,\n        provide_context=True,\n    )\n\n    task1 >> task2", "metadata": {"file_name": "dag_2_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 7, "backend": "llama.cpp"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using a branching mechanism that randomly chooses between multiple potential workflow paths. Implement a DAG that illustrates how to dynamically select and execute different task sequences based on runtime decisions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\ndef random_task():\n    return \"Random Task\"\n\ndef conditional_task():\n    return \"Conditional Task\"\n\ndef execute_task():\n    return \"Execute Task\"\n\ndef execute_task_2():\n    return \"Execute Task 2\"\n\ndef execute_task_3():\n    return \"Execute Task 3\"\n\ndef execute_task_4():\n    return \"Execute Task 4\"\n\ndef execute_task_5():\n    return \"Execute Task 5\"\n\ndef execute_task_6():\n    return \"Execute Task 6\"\n\ndef execute_task_7():\n    return \"Execute Task 7\"\n\ndef execute_task_8():\n    return \"Execute Task 8\"\n\ndef execute_task_9():\n    return \"Execute Task 9\"\n\ndef execute_task_10():\n    return \"Execute Task 10\"\n\ndef execute_task_11():\n    return \"Execute Task 11\"\n\ndef execute_task_12():\n    return \"Execute Task 12\"\n\ndef execute_task_13():\n    return \"Execute Task 13\"\n\ndef execute_task_14():\n    return \"Execute Task 14\"\n\ndef execute_task_15():\n    return \"Execute Task 15\"\n\ndef execute_task_16():\n    return \"Execute Task 16\"\n\ndef execute_task_17():\n    return \"Execute Task 17\"\n\ndef execute_task_18():\n    return \"Execute Task 18\"\n\ndef execute_task_19():\n    return \"Execute Task 19\"\n\ndef execute_task_20():\n    return \"Execute Task 20\"\n\ndef execute_task_21():\n    return \"Execute Task 21\"\n\ndef execute_task_22():\n    return \"Execute Task 22\"\n\ndef execute_task_23():\n    return \"Execute Task 23\"\n\ndef execute_task_24():\n    return \"Execute Task 24\"\n\ndef execute_task_25():\n    return \"Execute Task 25\"\n\ndef execute_task_26():\n    return \"Execute Task 26\"\n\ndef execute_task_27():\n    return \"Execute Task 27\"\n\ndef execute_task_28():\n    return \"Execute Task 28\"\n\ndef execute_task_29():\n    return \"Execute Task 29\"\n\ndef execute_task_30():\n    return \"Execute Task 30\"\n\ndef execute_task_31():\n    return \"Execute Task 31\"\n\ndef execute_task_32():\n    return \"Execute Task 32\"\n\ndef execute_task_33():\n    return \"Execute Task 33\"\n\ndef execute_task_34():\n    return \"Execute Task 34\"\n\ndef execute_task_35():\n    return \"Execute Task 35\"\n\ndef execute_task_36():\n    return \"Execute Task 36\"\n\ndef execute_task_37():\n    return \"Execute Task 37\"\n\ndef execute_task_38():\n    return \"Execute Task 38\"\n\ndef execute_task_39():\n    return \"Execute Task 39\"\n\ndef execute_task_40():\n    return \"Execute Task 40\"\n\ndef execute_task_41():\n    return \"Execute Task 41\"\n\ndef execute_task_42():\n    return \"Execute Task 42\"\n\ndef execute_task_43():\n    return \"Execute Task 43\"\n\ndef execute_task_44():\n    return \"Execute Task 44\"\n\ndef execute_task_45():\n    return \"Execute Task 45\"\n\ndef execute_task_46():\n    return \"Execute Task 46\"\n\ndef execute_task_47():\n    return \"Execute Task 47\"\n\ndef execute_task_48():\n    return \"Execute Task 48\"\n\ndef execute_task_49():\n    return \"Execute Task 49\"\n\ndef execute_task_50():\n    return \"Execute Task 50\"\n\ndef execute_task_51():\n    return \"Execute Task 51\"\n\ndef execute_task_52():\n    return \"Execute Task 52\"\n\ndef execute_task_53():\n    return \"Execute Task 53\"\n\ndef execute_task_54():\n    return \"Execute Task 54\"\n\ndef execute_task_55():\n    return \"Execute Task 55\"\n\ndef execute_task_56():\n    return \"Execute Task 56\"\n\ndef execute_task_57():\n    return \"Execute Task 57\"\n\ndef execute_task_58():\n    return \"Execute Task 58\"\n\ndef execute_task_59():\n    return \"Execute Task 59\"\n\ndef execute_task_60():\n    return \"Execute Task 60\"\n\ndef execute_task_61():\n    return \"Execute Task 61\"\n\ndef execute_task_62", "metadata": {"file_name": "dag_2_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 8, "backend": "llama.cpp"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single task randomly determines which subsequent tasks will be executed. Create a workflow that demonstrates how to implement probabilistic routing in data processing pipelines using Airflow's task branching decorator.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_routing',\n    default_args=default_args,\n    schedule_interval='@once',\n)\n\ndef random_task():\n    return 'Task 1'\n\ndef task_1():\n    return 'Task 1'\n\ndef task_2():\n    return 'Task 2'\n\ndef task_3():\n    return 'Task 3'\n\ndef task_4():\n    return 'Task 4'\n\ndef task_5():\n    return 'Task 5'\n\ndef task_6():\n    return 'Task 6'\n\ndef task_7():\n    return 'Task 7'\n\ndef task_8():\n    return 'Task 8'\n\ndef task_9():\n    return 'Task 9'\n\ndef task_10():\n    return 'Task 10'\n\ndef task_11():\n    return 'Task 11'\n\ndef task_12():\n    return 'Task 12'\n\ndef task_13():\n    return 'Task 13'\n\ndef task_14():\n    return 'Task 14'\n\ndef task_15():\n    return 'Task 15'\n\ndef task_16():\n    return 'Task 16'\n\ndef task_17():\n    return 'Task 17'\n\ndef task_18():\n    return 'Task 18'\n\ndef task_19():\n    return 'Task 19'\n\ndef task_20():\n    return 'Task 20'\n\ndef task_21():\n    return 'Task 21'\n\ndef task_22():\n    return 'Task 22'\n\ndef task_23():\n    return 'Task 23'\n\ndef task_24():\n    return 'Task 24'\n\ndef task_25():\n    return 'Task 25'\n\ndef task_26():\n    return 'Task 26'\n\ndef task_27():\n    return 'Task 27'\n\ndef task_28():\n    return 'Task 28'\n\ndef task_29():\n    return 'Task 29'\n\ndef task_30():\n    return 'Task 30'\n\ndef task_31():\n    return 'Task 31'\n\ndef task_32():\n    return 'Task 32'\n\ndef task_33():\n    return 'Task 33'\n\ndef task_34():\n    return 'Task 34'\n\ndef task_35():\n    return 'Task 35'\n\ndef task_36():\n    return 'Task 36'\n\ndef task_37():\n    return 'Task 37'\n\ndef task_38():\n    return 'Task 38'\n\ndef task_39():\n    return 'Task 39'\n\ndef task_40():\n    return 'Task 40'\n\ndef task_41():\n    return 'Task 41'\n\ndef task_42():\n    return 'Task 42'\n\ndef task_43():\n    return 'Task 43'\n\ndef task_44():\n    return 'Task 44'\n\ndef task_45():\n    return 'Task 45'\n\ndef task_46():\n    return 'Task 46'\n\ndef task_47():\n    return 'Task 47'\n\ndef task_48():\n    return 'Task 48'\n\ndef task_49():\n    return 'Task 49'\n\ndef task_50():\n    return 'Task 50'\n\ndef task_51():\n    return 'Task 51'\n\ndef task_52():\n    return 'Task 52'\n\ndef task_53():\n    return 'Task 53'\n\ndef task_54():\n    return 'Task 54'\n\ndef task_55():\n    return 'Task 55'\n\ndef task_56():\n    return 'Task 56'\n\ndef task_57():\n    return 'Task 57'\n\ndef task_58():\n    return 'Task 58'\n\ndef task_59():\n    return 'Task 59'\n\ndef task_60():\n    return 'Task 60'\n\ndef task_61():\n    return 'Task 61'\n\ndef task_62():\n    return 'Task 62'\n\ndef task_63():\n    return 'Task 63'\n\ndef task_64():\n    return 'Task 64'\n\ndef task_65():\n    return 'Task 65'\n\ndef task_66():\n    return 'Task 66'\n\ndef task_", "metadata": {"file_name": "dag_2_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 9, "backend": "llama.cpp"}}
{"instruction": "Design a workflow that demonstrates conditional task execution based on the day of the week. Create a DAG that branches tasks differently for weekdays and weekends, showing how Airflow can dynamically route workflow paths.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'conditional_task_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef weekday_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\ndef weekend_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\nweekday_task = PythonOperator(\n    task_id='weekday_task',\n    python_callable=weekday_task,\n    dag=dag,\n)\n\nweekend_task = PythonOperator(\n    task_id='weekend_task',\n    python_callable=weekend_task,\n    dag=dag,\n)\n\nweekday_task >> weekend_task", "metadata": {"file_name": "dag_3_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 10, "backend": "llama.cpp"}}
{"instruction": "Build an Airflow pipeline that showcases branching logic using BranchDayOfWeekOperator to demonstrate conditional task selection. Implement a workflow that executes different tasks based on whether the current day matches specific day criteria.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'branch_day_of_week',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef branch_day_of_week():\n    today = datetime.now()\n    day_of_week = today.weekday()\n    if day_of_week == 0:\n        return 'Monday'\n    elif day_of_week == 1:\n        return 'Tuesday'\n    elif day_of_week == 2:\n        return 'Wednesday'\n    elif day_of_week == 3:\n        return 'Thursday'\n    elif day_of_week == 4:\n        return 'Friday'\n    elif day_of_week == 5:\n        return 'Saturday'\n    elif day_of_week == 6:\n        return 'Sunday'\n\ndef execute_task(task):\n    task.execute(context={})\n\nwith dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=branch_day_of_week,\n        dag=dag\n    )\n\n    task2 = BashOperator(\n        task_id='task2',\n        bash_command='echo \"Task 2 executed\"',\n        dag=dag\n    )\n\n    task3 = PythonOperator(\n        task_id='task3',\n        python_callable=execute_task,\n        op_args=[task2],\n        dag=dag\n    )", "metadata": {"file_name": "dag_3_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 11, "backend": "llama.cpp"}}
{"instruction": "Develop an Airflow DAG that illustrates dynamic task routing using day-of-week conditions for data processing workflows. Create a scenario where tasks are selectively executed based on whether the current day meets predefined scheduling requirements.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_day_of_week_task_routing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef process_data():\n    print(\"Processing data\")\n\ndef process_data_with_weekday():\n    print(\"Processing data on weekdays\")\n\ndef process_data_with_weekend():\n    print(\"Processing data on weekends\")\n\ndef process_data_with_weekday_and_time():\n    print(\"Processing data on weekdays at 9am\")\n\ndef process_data_with_weekend_and_time():\n    print(\"Processing data on weekends at 10am\")\n\ndef process_data_with_weekday_and_time_and_date():\n    print(\"Processing data on weekdays at 9am on 2023-01-01\")\n\ndef process_data_with_weekend_and_time_and_date():\n    print(\"Processing data on weekends at 10am on 2023-01-01\")\n\ndef process_data_with_weekday_and_time_and_date_and_time():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am\")\n\ndef process_data_with_weekend_and_time_and_date_and_time():\n    print(\"Processing data on weekends at 10am on 2023-01-01 at 10am\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am on 2023-01-01\")\n\ndef process_data_with_weekend_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekends at 10am on 2023-01-01 at 10am on 2023-01-01\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date_and_time():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am on 2023-01-01 at 11am\")\n\ndef process_data_with_weekend_and_time_and_date_and_time_and_date_and_time():\n    print(\"Processing data on weekends at 10am on 2023-01-01 at 10am on 2023-01-01 at 11am\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am on 2023-01-01 at 11am on 2023-01-01\")\n\ndef process_data_with_weekend_and_time_and_date_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekends at 10am on 2023-01-01 at 10am on 2023-01-01 at 11am on 2023-01-01\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date_and_time_and_date_and_time():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am on 2023-01-01 at 11am on 2023-01-01 at 12pm\")\n\ndef process_data_with_weekend_and_time_and_date_and_time_and_date_and_time_and_date_and_time():\n    print(\"Processing data on weekends at 10am on 2023-01-01 at 10am on 2023-01-01 at 11am on 2023-01-01 at 12pm\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekdays at 9am on 2023-01-01 at 10am on 2023-01-01 at 11am on 2023-01-01 at 12pm on 2023-01-01\")\n\ndef process_data_with_weekend", "metadata": {"file_name": "dag_3_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 12, "backend": "llama.cpp"}}
{"instruction": "Design a data quality monitoring workflow that tracks data integrity during an ingestion process. Create a DAG that handles both successful data processing and error scenarios with clear branching and reporting.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_quality_monitoring',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ningest_data = BashOperator(\n    task_id='ingest_data',\n    bash_command='python /path/to/ingest_data.py',\n    dag=dag,\n)\n\ncheck_data_quality = PythonOperator(\n    task_id='check_data_quality',\n    python_callable=check_data_quality,\n    op_args=[ingest_data],\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> ingest_data >> check_data_quality >> end", "metadata": {"file_name": "dag_4_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 13, "backend": "llama.cpp"}}
{"instruction": "Build an Airflow pipeline that demonstrates conditional task execution based on data integrity checks. Implement a workflow that can automatically route tasks differently depending on whether data passes or fails validation.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_integrity_check_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef check_data_integrity():\n    print(\"Data integrity check is successful.\")\n\ndef fail_data_integrity():\n    print(\"Data integrity check failed.\")\n\ndef route_data_integrity():\n    print(\"Data integrity check is successful, routing to next task.\")\n\ndef fail_data_integrity_and_route_to_error():\n    print(\"Data integrity check failed, routing to error task.\")\n\ndef success_data_integrity():\n    print(\"Data integrity check is successful, no action required.\")\n\ndef error_data_integrity():\n    print(\"Data integrity check failed, no action required.\")\n\ndef dummy_task():\n    print(\"Dummy task executed.\")\n\ndef task1():\n    print(\"Task 1 executed.\")\n\ndef task2():\n    print(\"Task 2 executed.\")\n\ndef task3():\n    print(\"Task 3 executed.\")\n\ndef task4():\n    print(\"Task 4 executed.\")\n\ndef task5():\n    print(\"Task 5 executed.\")\n\ndef task6():\n    print(\"Task 6 executed.\")\n\ndef task7():\n    print(\"Task 7 executed.\")\n\ndef task8():\n    print(\"Task 8 executed.\")\n\ndef task9():\n    print(\"Task 9 executed.\")\n\ndef task10():\n    print(\"Task 10 executed.\")\n\ndef task11():\n    print(\"Task 11 executed.\")\n\ndef task12():\n    print(\"Task 12 executed.\")\n\ndef task13():\n    print(\"Task 13 executed.\")\n\ndef task14():\n    print(\"Task 14 executed.\")\n\ndef task15():\n    print(\"Task 15 executed.\")\n\ndef task16():\n    print(\"Task 16 executed.\")\n\ndef task17():\n    print(\"Task 17 executed.\")\n\ndef task18():\n    print(\"Task 18 executed.\")\n\ndef task19():\n    print(\"Task 19 executed.\")\n\ndef task20():\n    print(\"Task 20 executed.\")\n\ndef task21():\n    print(\"Task 21 executed.\")\n\ndef task22():\n    print(\"Task 22 executed.\")\n\ndef task23():\n    print(\"Task 23 executed.\")\n\ndef task24():\n    print(\"Task 24 executed.\")\n\ndef task25():\n    print(\"Task 25 executed.\")\n\ndef task26():\n    print(\"Task 26 executed.\")\n\ndef task27():\n    print(\"Task 27 executed.\")\n\ndef task28():\n    print(\"Task 28 executed.\")\n\ndef task29():\n    print(\"Task 29 executed.\")\n\ndef task30():\n    print(\"Task 30 executed.\")\n\ndef task31():\n    print(\"Task 31 executed.\")\n\ndef task32():\n    print(\"Task 32 executed.\")\n\ndef task33():\n    print(\"Task 33 executed.\")\n\ndef task34():\n    print(\"Task 34 executed.\")\n\ndef task35():\n    print(\"Task 35 executed.\")\n\ndef task36():\n    print(\"Task 36 executed.\")\n\ndef task37():\n    print(\"Task 37 executed.\")\n\ndef task38():\n    print(\"Task 38 executed.\")\n\ndef task39():\n    print(\"Task 39 executed.\")\n\ndef task40():\n    print(\"Task 40 executed.\")\n\ndef task41():\n    print(\"Task 41 executed.\")\n\ndef task42():\n    print(\"Task 42 executed.\")\n\ndef task43():\n    print(\"Task 43 executed.\")\n\ndef task44():\n    print(\"Task 44 executed.\")\n\ndef task45():\n    print(\"Task 45 executed.\")\n\ndef task46():\n    print(\"Task 46 executed.\")\n\ndef task47():\n    print(\"Task 47 executed.\")\n\ndef task48():\n    print(\"Task 48 executed.\")\n\ndef task49():\n    print(\"Task 49 executed.\")\n\ndef task50():\n    print(\"Task 50 executed.\")\n\ndef task51():\n    print(\"Task 51 executed.\")\n\ndef task52():\n    print(\"Task 52 executed.\")\n\ndef task53():\n    print(\"Task 53 executed.\")\n\ndef task54():\n    print(\"Task 54 executed.\")\n\ndef task55():\n    print(\"Task 55 executed.\")\n\ndef task56():\n    print(\"Task 56 executed.\")\n\ndef task57():\n    print(\"Task 57 executed.\")\n\ndef task58():\n    print", "metadata": {"file_name": "dag_4_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 14, "backend": "llama.cpp"}}
{"instruction": "Develop a data processing pipeline that includes stages for data ingestion, analysis, integrity verification, and error handling. Create a workflow that can gracefully manage data quality issues and generate appropriate reports or alerts.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ningest_data = BashOperator(\n    task_id='ingest_data',\n    bash_command='python /path/to/ingest_data.py',\n    dag=dag,\n)\n\nprocess_data = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    op_args=[ingest_data],\n    dag=dag,\n)\n\nverify_data = BashOperator(\n    task_id='verify_data',\n    bash_command='python /path/to/verify_data.py',\n    dag=dag,\n)\n\nhandle_errors = BashOperator(\n    task_id='handle_errors',\n    bash_command='python /path/to/handle_errors.py',\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> ingest_data >> process_data >> verify_data >> handle_errors >> end", "metadata": {"file_name": "dag_4_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen2-1.5b-instruct-q4_k_m-gguf", "generation_id": 15, "backend": "llama.cpp"}}
