{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single initial task triggers a random selection of processing paths. Create a workflow that demonstrates how data processing can be dynamically routed through different branches based on runtime conditions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.email import EmailOperator\n\ndef random_selection():\n    import random\n    return random.choice(['path1', 'path2', 'path3'])\n\ndef process_data():\n    import pandas as pd\n    # Load data from a CSV file\n    df = pd.read_csv('data.csv')\n    # Perform data processing\n    # ...\n\ndef send_email():\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.base import MIMEBase\n    from email import encoders\n\n    # ...\n\ndef send_report():\n    import pandas as pd\n    # Load data from a CSV file\n    df = pd.read_csv('data.csv')\n    # Perform data processing\n    # ...\n\ndef send_report_email():\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.base import MIMEBase\n    from email import encoders\n\n    # ...\n\ndef send_report_email():\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.base import MIMEBase\n    from email import encoders\n\n    # ...\n\ndef send_report_email():\n    import smtplib\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.base import MIMEBase\n    from email import encoders\n\n    # ...\n\ndef send_report_email():\n    import smtplib\n    from email.mime.multipart import M", "metadata": {"file_name": "dag_0_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using the BranchPythonOperator, illustrating how tasks can dynamically choose their execution path based on runtime logic. Implement a workflow that randomly selects and executes one of multiple parallel task branches.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import BranchPythonOperator, PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_branching',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef branch_task():\n    return choice(['task1', 'task2'])\n\ndef task1():\n    print(\"Executing task1\")\n\ndef task2():\n    print(\"Executing task2\")\n\ndef task3():\n    print(\"Executing task3\")\n\ndef task4():\n    print(\"Executing task4\")\n\ndef task5():\n    print(\"Executing task5\")\n\ndef task6():\n    print(\"Executing task6\")\n\ndef task7():\n    print(\"Executing task7\")\n\ndef task8():\n    print(\"Executing task8\")\n\ndef task9():\n    print(\"Executing task9\")\n\ndef task10():\n    print(\"Executing task10\")\n\ndef task11():\n    print(\"Executing task11\")\n\ndef task12():\n    print(\"Executing task12\")\n\ndef task13():\n    print(\"Executing task13\")\n\ndef task14():\n    print(\"Executing task14\")\n\ndef task15():\n    print(\"Executing task15\")\n\ndef task16():\n    print(\"Executing task16\")\n\ndef task17():\n    print(\"Executing task17\")\n\ndef task18():\n    print(\"Executing task18\")\n\ndef task19():\n    print(\"Executing task19\")\n\ndef", "metadata": {"file_name": "dag_0_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Create a dynamic task routing DAG that randomly selects one of four predefined branches during each daily run. Design a workflow that demonstrates flexible task execution with random path selection.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_task():\n    print(\"Random task executed.\")\n\ndef task_branch_1():\n    print(\"Task branch 1 executed.\")\n\ndef task_branch_2():\n    print(\"Task branch 2 executed.\")\n\ndef task_branch_3():\n    print(\"Task branch 3 executed.\")\n\ndef task_branch_4():\n    print(\"Task branch 4 executed.\")\n\ndef task_random():\n    print(\"Task executed randomly.\")\n\ndef task_random_branch_1():\n    print(\"Task branch 1 executed randomly.\")\n\ndef task_random_branch_2():\n    print(\"Task branch 2 executed randomly.\")\n\ndef task_random_branch_3():\n    print(\"Task branch 3 executed randomly.\")\n\ndef task_random_branch_4():\n    print(\"Task branch 4 executed randomly.\")\n\nwith DAG(\n    dag_id=\"dynamic_task_routing\",\n    default_args=default_args,\n    schedule_interval=\"@daily\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    task1 = PythonOperator(\n        task_id=\"task1\",\n        python_callable=random_task,\n        dag=dag,\n    )\n\n    task2 = PythonOperator(\n        task_id=\"task2\",\n        python_callable=task_branch_1,\n        dag=dag,\n    )\n\n    task3 = PythonOperator(\n        task_id=\"task3\",\n        python_callable=task_branch_2,\n        dag=dag,\n    )\n\n    task4 = PythonOperator(\n        task_id=\"task4\",\n        python_callable=task_branch_3,\n        dag=d", "metadata": {"file_name": "dag_0_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a demonstration DAG that showcases multiple Bash command executions with different task dependencies. Create a workflow that runs several echo commands and demonstrates task chaining and conditional execution.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'bash_command_execution_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef echo_command():\n    bash_command = BashOperator(\n        task_id='echo_command',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_2 = BashOperator(\n        task_id='echo_command_2',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_3 = BashOperator(\n        task_id='echo_command_3',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_4 = BashOperator(\n        task_id='echo_command_4',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_5 = BashOperator(\n        task_id='echo_command_5',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_6 = BashOperator(\n        task_id='echo_command_6',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_7 = BashOperator(\n        task_id='echo_command_7',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\n    echo_command_8 = BashOperator(\n        task_id='", "metadata": {"file_name": "dag_1_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using a branching mechanism that randomly chooses between multiple potential workflow paths. Implement a DAG that illustrates how to dynamically select and execute different task sequences based on runtime decisions.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_task():\n    return choice(['task1', 'task2', 'task3'])\n\ndef execute_task(task):\n    print(f\"Executing task: {task}\")\n\ndef select_task():\n    task = random_task()\n    execute_task(task)\n\nwith DAG(\n    dag_id='conditional_task_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['conditional_task'],\n) as dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=select_task,\n    )", "metadata": {"file_name": "dag_2_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a dynamic workflow that randomly selects and executes one of four predefined branches during each daily run. Create a DAG that demonstrates flexible task routing using Python's random selection and Airflow's branching capabilities.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_workflow',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef select_branch():\n    branches = ['branch1', 'branch2', 'branch3', 'branch4']\n    return choice(branches)\n\ndef execute_branch(branch):\n    print(f\"Executing branch: {branch}\")\n\nwith dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=select_branch,\n        dag=dag,\n    )\n\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=execute_branch,\n        op_args=[task1.output],\n        dag=dag,\n    )", "metadata": {"file_name": "dag_2_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Construct an example workflow that simulates a basic data processing pipeline using shell commands, showing how Airflow can coordinate multiple computational steps with different execution patterns. Create a DAG that demonstrates dynamic task generation and conditional execution strategies.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'basic_data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef generate_data():\n    print(\"Generating data...\")\n\ndef process_data():\n    print(\"Processing data...\")\n\ndef clean_data():\n    print(\"Cleaning data...\")\n\ndef transform_data():\n    print(\"Transforming data...\")\n\ndef load_data():\n    print(\"Loading data...\")\n\ngenerate_data_task = PythonOperator(\n    task_id='generate_data',\n    python_callable=generate_data,\n    dag=dag,\n)\n\nprocess_data_task = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    dag=dag,\n)\n\nclean_data_task = PythonOperator(\n    task_id='clean_data',\n    python_callable=clean_data,\n    dag=dag,\n)\n\ntransform_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload_data_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag,\n)\n\ngenerate_data_task >> process_data_task\nprocess_data_task >> clean_data_task\nclean_data_task >> transform_data_task\ntransform_data_task >> load_data_task", "metadata": {"file_name": "dag_1_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that illustrates task orchestration techniques using BashOperator, including parallel task generation, sequential dependencies, and handling potential task skipping scenarios. Develop a DAG that explores different ways of defining and connecting computational tasks.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'parallel_task_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef parallel_task():\n    bash_task = BashOperator(\n        task_id='parallel_task',\n        bash_command='echo \"Parallel task executed successfully\"',\n        dag=dag,\n    )\n\ndef sequential_task():\n    bash_task = BashOperator(\n        task_id='sequential_task',\n        bash_command='echo \"Sequential task executed successfully\"',\n        dag=dag,\n    )\n\ndef skip_task():\n    bash_task = BashOperator(\n        task_id='skip_task',\n        bash_command='echo \"Skip task executed successfully\"',\n        dag=dag,\n    )\n\ndef parallel_task_with_retry():\n    bash_task = BashOperator(\n        task_id='parallel_task_with_retry',\n        bash_command='echo \"Parallel task with retry executed successfully\"',\n        retries=3,\n        retry_delay=timedelta(minutes=5),\n        dag=dag,\n    )\n\ndef sequential_task_with_retry():\n    bash_task = BashOperator(\n        task_id='sequential_task_with_retry',\n        bash_command='echo \"Sequential task with retry executed successfully\"',\n        retries=3,\n        retry_delay=timedelta(minutes=5),\n        dag=dag,\n    )\n\ndef skip_task_with_retry():\n    bash_task = BashOperator(\n        task_id='skip_task_with_retry',\n        bash_command='echo \"Skip task with retry executed successfully\"',", "metadata": {"file_name": "dag_1_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a workflow that demonstrates conditional task execution based on the day of the week. Create a DAG that branches tasks differently for weekdays and weekends, showing how Airflow can dynamically route workflow paths.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'conditional_task_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef weekday_task():\n    print(\"Executing task on a weekday\")\n\ndef weekend_task():\n    print(\"Executing task on a weekend\")\n\ndef execute_task(task_func, task_name):\n    task = task_func()\n    task.run(task_name)\n\ntask1 = PythonOperator(\n    task_id='weekday_task',\n    python_callable=weekday_task,\n    dag=dag,\n)\n\ntask2 = PythonOperator(\n    task_id='weekend_task',\n    python_callable=weekend_task,\n    dag=dag,\n)\n\ntask1 >> execute_task\ntask2 >> execute_task", "metadata": {"file_name": "dag_3_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single task randomly determines which subsequent tasks will be executed. Create a workflow that demonstrates how to implement probabilistic routing in data processing pipelines using Airflow's task branching decorator.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_routing',\n    default_args=default_args,\n    schedule_interval='@once',\n)\n\ndef random_task():\n    return 'task_1'\n\ndef task_1():\n    return 'task_1'\n\ndef task_2():\n    return 'task_2'\n\ndef task_3():\n    return 'task_3'\n\ndef task_4():\n    return 'task_4'\n\ndef task_5():\n    return 'task_5'\n\ndef task_6():\n    return 'task_6'\n\ndef task_7():\n    return 'task_7'\n\ndef task_8():\n    return 'task_8'\n\ndef task_9():\n    return 'task_9'\n\ndef task_10():\n    return 'task_10'\n\ndef task_11():\n    return 'task_11'\n\ndef task_12():\n    return 'task_12'\n\ndef task_13():\n    return 'task_13'\n\ndef task_14():\n    return 'task_14'\n\ndef task_15():\n    return 'task_15'\n\ndef task_16():\n    return 'task_16'\n\ndef task_17():\n    return 'task_17'\n\ndef task_18():\n    return 'task_18'", "metadata": {"file_name": "dag_2_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a data quality monitoring workflow that tracks data integrity during an ingestion process. Create a DAG that handles both successful data processing and error scenarios with clear branching and reporting.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_quality_monitoring',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\nprocess_data = BashOperator(\n    task_id='process_data',\n    bash_command='python /path/to/your/data_quality_monitoring.py',\n    dag=dag,\n)\n\ncheck_data = PythonOperator(\n    task_id='check_data',\n    python_callable=check_data_quality,\n    op_kwargs={'data_quality_file': 'data_quality_monitoring.py'},\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> process_data >> check_data >> end", "metadata": {"file_name": "dag_4_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases branching logic using BranchDayOfWeekOperator to demonstrate conditional task selection. Implement a workflow that executes different tasks based on whether the current day matches specific day criteria.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'branch_day_of_week',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef branch_day_of_week():\n    today = datetime.now()\n    day_of_week = today.strftime('%A')\n    if day_of_week == 'Monday':\n        return 'Monday'\n    elif day_of_week == 'Tuesday':\n        return 'Tuesday'\n    elif day_of_week == 'Wednesday':\n        return 'Wednesday'\n    elif day_of_week == 'Thursday':\n        return 'Thursday'\n    elif day_of_week == 'Friday':\n        return 'Friday'\n    elif day_of_week == 'Saturday':\n        return 'Saturday'\n    elif day_of_week == 'Sunday':\n        return 'Sunday'\n\ndef execute_task(task):\n    if task == 'Monday':\n        return 'Monday'\n    elif task == 'Tuesday':\n        return 'Tuesday'\n    elif task == 'Wednesday':\n        return 'Wednesday'\n    elif task == 'Thursday':\n        return 'Thursday'\n    elif task == 'Friday':\n        return 'Friday'\n    elif task == 'Saturday':\n        return 'Saturday'\n    elif task == 'Sunday':\n        return 'Sunday'\n\ndef execute_task2(task):\n    if task == 'Monday':\n        return 'Monday'\n    elif task == 'Tuesday':\n        return 'Tuesday'\n    elif task == 'Wednesday':\n        return 'Wednesday'\n    elif task", "metadata": {"file_name": "dag_3_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop an Airflow DAG that illustrates dynamic task routing using day-of-week conditions for data processing workflows. Create a scenario where tasks are selectively executed based on whether the current day meets predefined scheduling requirements.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_task_routing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef process_data():\n    print(\"Processing data\")\n\ndef process_data_with_weekday():\n    print(\"Processing data on weekday\")\n\ndef process_data_with_weekend():\n    print(\"Processing data on weekend\")\n\ndef process_data_with_weekday_and_time():\n    print(\"Processing data on weekday and specific time\")\n\ndef process_data_with_weekend_and_time():\n    print(\"Processing data on weekend and specific time\")\n\ndef process_data_with_weekday_and_time_and_date():\n    print(\"Processing data on weekday and specific time and date\")\n\ndef process_data_with_weekend_and_time_and_date():\n    print(\"Processing data on weekend and specific time and date\")\n\ndef process_data_with_weekday_and_time_and_date_and_time():\n    print(\"Processing data on weekday and specific time and date and time\")\n\ndef process_data_with_weekend_and_time_and_date_and_time():\n    print(\"Processing data on weekend and specific time and date and time\")\n\ndef process_data_with_weekday_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekday and specific time and date and time and date\")\n\ndef process_data_with_weekend_and_time_and_date_and_time_and_date():\n    print(\"Processing data on weekend and specific time and date and time and date\")\n\ndef process_data_with_weekday", "metadata": {"file_name": "dag_3_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that demonstrates conditional task execution based on data integrity checks. Implement a workflow that can automatically route tasks differently depending on whether data passes or fails validation.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_integrity_check_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef data_integrity_check():\n    print(\"Data integrity check completed successfully.\")\n\ndef data_integrity_failure():\n    print(\"Data integrity check failed.\")\n\ndef data_integrity_success():\n    print(\"Data integrity check passed.\")\n\ndef data_integrity_error():\n    print(\"Data integrity check encountered an error.\")\n\ndef data_integrity_check_task():\n    with dag:\n        task1 = DummyOperator(task_id='task1', dag=dag)\n        task2 = DummyOperator(task_id='task2', dag=dag)\n        task3 = DummyOperator(task_id='task3', dag=dag)\n        task4 = DummyOperator(task_id='task4', dag=dag)\n        task5 = DummyOperator(task_id='task5', dag=dag)\n        task6 = DummyOperator(task_id='task6', dag=dag)\n        task7 = DummyOperator(task_id='task7', dag=dag)\n        task8 = DummyOperator(task_id='task8', dag=dag)\n        task9 = DummyOperator(task_id='task9', dag=dag)\n        task10 = DummyOperator(task_id='task10', dag=dag)\n        task11 = DummyOperator(task_id='task11', dag=dag)\n        task12 = Dummy", "metadata": {"file_name": "dag_4_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop a data processing pipeline that includes stages for data ingestion, analysis, integrity verification, and error handling. Create a workflow that can gracefully manage data quality issues and generate appropriate reports or alerts.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ningest_data = BashOperator(\n    task_id='ingest_data',\n    bash_command='python /path/to/ingest_data.py',\n    dag=dag,\n)\n\nprocess_data = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    op_args=['/path/to/processed_data'],\n    dag=dag,\n)\n\nverify_data = BashOperator(\n    task_id='verify_data',\n    bash_command='python /path/to/verify_data.py',\n    dag=dag,\n)\n\nhandle_errors = BashOperator(\n    task_id='handle_errors',\n    bash_command='python /path/to/error_handling.py',\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> ingest_data >> process_data >> verify_data >> handle_errors >> end", "metadata": {"file_name": "dag_4_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
