{"instruction": "Design a dynamic workflow that randomly selects and executes one of four predefined branches during each daily run. Create a DAG that demonstrates flexible task routing using Python's random selection and Airflow's branching capabilities.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef select_branch():\n    branches = ['branch1', 'branch2', 'branch3', 'branch4']\n    return choice(branches)\n\ndef execute_branch(branch):\n    print(f\"Executing branch: {branch}\")\n\nwith DAG(\n    dag_id='dynamic_workflow',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['example'],\n) as dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=select_branch,\n        dag=dag,\n    )\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=execute_branch,\n        op_args=[task1.output],\n        dag=dag,\n    )", "metadata": {"file_name": "dag_2_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that illustrates task orchestration techniques using BashOperator, including parallel task generation, sequential dependencies, and handling potential task skipping scenarios. Develop a DAG that explores different ways of defining and connecting computational tasks.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'parallel_task_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef parallel_task():\n    bash_task = BashOperator(\n        task_id='parallel_task',\n        bash_command='echo \"Parallel task executed successfully\"',\n        dag=dag,\n    )\n\n    bash_task2 = BashOperator(\n        task_id='parallel_task2',\n        bash_command='echo \"Parallel task2 executed successfully\"',\n        dag=dag,\n    )\n\n    bash_task2.set_upstream(bash_task)\n\nparallel_task2 = PythonOperator(\n    task_id='parallel_task2',\n    python_callable=parallel_task,\n    dag=dag,\n)\n\nparallel_task2.set_upstream(bash_task)\nparallel_task2.set_upstream(bash_task2)", "metadata": {"file_name": "dag_1_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Create a dynamic task routing DAG that randomly selects one of four predefined branches during each daily run. Design a workflow that demonstrates flexible task execution with random path selection.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_task_routing',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef random_task():\n    print(\"Random task executed.\")\n\ndef task1():\n    print(\"Task 1 executed.\")\n\ndef task2():\n    print(\"Task 2 executed.\")\n\ndef task3():\n    print(\"Task 3 executed.\")\n\ndef task4():\n    print(\"Task 4 executed.\")\n\ndef task_routing():\n    task = choice([task1, task2, task3, task4])\n    task()\n\nwith dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=task1,\n        dag=dag,\n    )\n\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=task2,\n        dag=dag,\n    )\n\n    task3 = PythonOperator(\n        task_id='task3',\n        python_callable=task3,\n        dag=dag,\n    )\n\n    task4 = PythonOperator(\n        task_id='task4',\n        python_callable=task4,\n        dag=dag,\n    )\n\n    task_routing()", "metadata": {"file_name": "dag_0_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Construct an example workflow that simulates a basic data processing pipeline using shell commands, showing how Airflow can coordinate multiple computational steps with different execution patterns. Create a DAG that demonstrates dynamic task generation and conditional execution strategies.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'basic_data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef generate_data():\n    print(\"Generating data...\")\n\ndef process_data():\n    print(\"Processing data...\")\n\ndef clean_data():\n    print(\"Cleaning data...\")\n\ndef transform_data():\n    print(\"Transforming data...\")\n\ndef load_data():\n    print(\"Loading data...\")\n\ngenerate_data_task = PythonOperator(\n    task_id='generate_data',\n    python_callable=generate_data,\n    dag=dag,\n)\n\nprocess_data_task = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    dag=dag,\n)\n\nclean_data_task = PythonOperator(\n    task_id='clean_data',\n    python_callable=clean_data,\n    dag=dag,\n)\n\ntransform_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload_data_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag,\n)\n\ngenerate_data_task >> process_data_task\nprocess_data_task >> clean_data_task\nclean_data_task >> transform_data_task\ntransform_data_task >> load_data_task", "metadata": {"file_name": "dag_1_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using a branching mechanism that randomly chooses between multiple potential workflow paths. Implement a DAG that illustrates how to dynamically select and execute different task sequences based on runtime decisions.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_task():\n    return choice(['task1', 'task2', 'task3'])\n\ndef execute_task(task):\n    print(f\"Executing task: {task}\")\n\ndef select_task(task):\n    if task == 'task1':\n        return 'task1'\n    elif task == 'task2':\n        return 'task2'\n    else:\n        return 'task3'\n\nwith DAG(\n    dag_id='random_task_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['random', 'task', 'conditional'],\n) as dag:\n    task1 = PythonOperator(\n        task_id='task1',\n        python_callable=random_task,\n        provide_context=True,\n    )\n\n    task2 = PythonOperator(\n        task_id='task2',\n        python_callable=random_task,\n        provide_context=True,\n    )\n\n    task3 = PythonOperator(\n        task_id='task3',\n        python_callable=random_task,\n        provide_context=True,\n    )\n\n    task4 = PythonOperator(\n        task_id='task4',\n        python_callable=execute_task,\n        provide_context=True,\n    )\n\n    task5 = PythonOperator(\n        task_id='task5',\n        python_callable=select_task,\n        provide_context=True,\n    )\n\n    task1 >> task2\n    task2 >> task3\n    task3 >> task4\n    task4 >> task5", "metadata": {"file_name": "dag_2_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a demonstration DAG that showcases multiple Bash command executions with different task dependencies. Create a workflow that runs several echo commands and demonstrates task chaining and conditional execution.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'bash_command_execution_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef echo_command():\n    bash = BashOperator(\n        task_id='echo_command',\n        bash_command='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\ndef echo_command_with_condition():\n    bash = BashOperator(\n        task_id='echo_command_with_condition',\n        bash_command='echo \"Hello, World!\"',\n        if_true='echo \"Hello, World!\"',\n        if_false='echo \"Hello, World!\"',\n        dag=dag,\n    )\n\ndef echo_command_with_task_chain():\n    bash = BashOperator(\n        task_id='echo_command_with_task_chain',\n        bash_command='echo \"Hello, World!\"',\n        if_true='echo \"Hello, World!\"',\n        if_false='echo \"Hello, World!\"',\n        task_chain=True,\n        dag=dag,\n    )\n\necho_command = PythonOperator(\n    task_id='echo_command',\n    python_callable=echo_command,\n    dag=dag,\n)\n\necho_command_with_condition = PythonOperator(\n    task_id='echo_command_with_condition',\n    python_callable=echo_command_with_condition,\n    dag=dag,\n)\n\necho_command_with_task_chain = PythonOperator(\n    task_id='echo_command_with_task_chain',\n    python_callable=echo_command_with_task_chain,\n    dag=dag,\n)\n\necho_command >> echo_command_with_condition\necho_command_with_condition >> echo_command_with_task_chain", "metadata": {"file_name": "dag_1_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a workflow that demonstrates conditional task execution based on the day of the week. Create a DAG that branches tasks differently for weekdays and weekends, showing how Airflow can dynamically route workflow paths.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'conditional_task_execution',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef weekday_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\ndef weekend_task():\n    print(f\"Executing task on {datetime.now().strftime('%A')}\")\n\nweekday_task = PythonOperator(\n    task_id='weekday_task',\n    python_callable=weekday_task,\n    dag=dag,\n)\n\nweekend_task = PythonOperator(\n    task_id='weekend_task',\n    python_callable=weekend_task,\n    dag=dag,\n)\n\nweekday_task >> weekend_task", "metadata": {"file_name": "dag_3_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Design a data quality monitoring workflow that tracks data integrity during an ingestion process. Create a DAG that handles both successful data processing and error scenarios with clear branching and reporting.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_quality_monitoring',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\nprocess_data = BashOperator(\n    task_id='process_data',\n    bash_command='python /path/to/your/data_quality_monitoring.py',\n    dag=dag,\n)\n\ncheck_data = PythonOperator(\n    task_id='check_data',\n    python_callable=check_data_quality,\n    op_args=[process_data],\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> process_data >> check_data >> end", "metadata": {"file_name": "dag_4_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that demonstrates conditional task execution based on data integrity checks. Implement a workflow that can automatically route tasks differently depending on whether data passes or fails validation.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_integrity_check_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef check_data_integrity():\n    print(\"Data integrity check is successful\")\n\ndef fail_data_integrity():\n    print(\"Data integrity check failed\")\n\ndef route_task(task):\n    if task.xcom_pull(task_ids='check_data_integrity'):\n        task.do_xcom_push(key='route', value='success')\n    else:\n        task.do_xcom_push(key='route', value='failure')\n\ncheck_data_integrity = PythonOperator(\n    task_id='check_data_integrity',\n    python_callable=check_data_integrity,\n    provide_context=True,\n    dag=dag,\n)\n\nfail_data_integrity = PythonOperator(\n    task_id='fail_data_integrity',\n    python_callable=fail_data_integrity,\n    provide_context=True,\n    dag=dag,\n)\n\nroute_task = DummyOperator(\n    task_id='route_task',\n    dag=dag,\n)\n\ncheck_data_integrity >> route_task\nroute_task >> fail_data_integrity", "metadata": {"file_name": "dag_4_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop a data processing pipeline that includes stages for data ingestion, analysis, integrity verification, and error handling. Create a workflow that can gracefully manage data quality issues and generate appropriate reports or alerts.", "input": {"airflow_version": "2.7.2"}, "output": "from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_processing_pipeline',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\nstart = DummyOperator(task_id='start', dag=dag)\n\ningest_data = BashOperator(\n    task_id='ingest_data',\n    bash_command='python /path/to/ingest_data.py',\n    dag=dag,\n)\n\nprocess_data = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data_function,\n    op_args=[ingest_data],\n    dag=dag,\n)\n\nverify_data = BashOperator(\n    task_id='verify_data',\n    bash_command='python /path/to/verify_data.py',\n    dag=dag,\n)\n\nhandle_errors = BashOperator(\n    task_id='handle_errors',\n    bash_command='python /path/to/handle_errors.py',\n    dag=dag,\n)\n\nend = DummyOperator(task_id='end', dag=dag)\n\nstart >> ingest_data >> process_data >> verify_data >> handle_errors >> end", "metadata": {"file_name": "dag_4_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single initial task triggers a random selection of processing paths. Create a workflow that demonstrates how data processing can be dynamically routed through different branches based on runtime conditions.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_data_processing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef random_process():\n    print(\"Random process triggered\")\n\ndef process_1():\n    print(\"Processing 1\")\n\ndef process_2():\n    print(\"Processing 2\")\n\ndef process_3():\n    print(\"Processing 3\")\n\ndef process_4():\n    print(\"Processing 4\")\n\ndef process_5():\n    print(\"Processing 5\")\n\ndef process_6():\n    print(\"Processing 6\")\n\ndef process_7():\n    print(\"Processing 7\")\n\ndef process_8():\n    print(\"Processing 8\")\n\ndef process_9():\n    print(\"Processing 9\")\n\ndef process_10():\n    print(\"Processing 10\")\n\ndef process_11():\n    print(\"Processing 11\")\n\ndef process_12():\n    print(\"Processing 12\")\n\ndef process_13():\n    print(\"Processing 13\")\n\ndef process_14():\n    print(\"Processing 14\")\n\ndef process_15():\n    print(\"Processing 15\")\n\ndef process_16():\n    print(\"Processing 16\")\n\ndef process_17():\n    print(\"Processing 17\")\n\ndef process_18():\n    print(\"Processing 18\")\n\ndef process_19():\n    print(\"Processing 19\")\n\ndef process_20():\n    print(\"Processing 20\")\n\ndef process_21():\n    print(\"Processing 21\")\n\ndef process_22():\n    print(\"Processing 22\")\n\ndef process_23():\n    print(\"Processing 23\")\n\ndef process_24():\n    print(\"Processing 24\")\n\ndef process_25():\n    print(\"Processing 25\")\n\ndef process_26():\n    print(\"Processing 26\")\n\ndef process_27():\n    print(\"Processing 27\")\n\ndef process_28():\n    print(\"Processing 28\")\n\ndef process_29():\n    print(\"Processing 29\")\n\ndef process_30():\n    print(\"Processing 30\")\n\ndef process_31():\n    print(\"Processing 31\")\n\ndef process_32():\n    print(\"Processing 32\")\n\ndef process_33():\n    print(\"Processing 33\")\n\ndef process_34():\n    print(\"Processing 34\")\n\ndef process_35():\n    print(\"Processing 35\")\n\ndef process_36():\n    print(\"Processing 36\")\n\ndef process_37():\n    print(\"Processing 37\")\n\ndef process_38():\n    print(\"Processing 38\")\n\ndef process_39():\n    print(\"Processing 39\")\n\ndef process_40():\n    print(\"Processing 40\")\n\ndef process_41():\n    print(\"Processing 41\")\n\ndef process_42():\n    print(\"Processing 42\")\n\ndef process_43():\n    print(\"Processing 43\")\n\ndef process_44():\n    print(\"Processing 44\")\n\ndef process_45():\n    print(\"Processing 45\")\n\ndef process_46():\n    print(\"Processing 46\")\n\ndef process_47():\n    print(\"Processing 47\")\n\ndef process_48():\n    print(\"Processing 48\")\n\ndef process_49():\n    print(\"Processing 49\")\n\ndef process_50():\n    print(\"Processing 50\")\n\ndef process_51():\n    print(\"Processing 51\")\n\ndef process_52():\n    print(\"Processing 52\")\n\ndef process_53():\n    print(\"Processing 53\")\n\ndef process_54():\n    print(\"Processing 54\")\n\ndef process_55():\n    print(\"Processing", "metadata": {"file_name": "dag_0_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases conditional task execution using the BranchPythonOperator, illustrating how tasks can dynamically choose their execution path based on runtime logic. Implement a workflow that randomly selects and executes one of multiple parallel task branches.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import BranchPythonOperator, PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'random_task_branch_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\ndef branch_task():\n    return 'task1'\n\ndef task1():\n    print('Executing task1')\n\ndef task2():\n    print('Executing task2')\n\ndef task3():\n    print('Executing task3')\n\ndef task4():\n    print('Executing task4')\n\ndef task5():\n    print('Executing task5')\n\ndef task6():\n    print('Executing task6')\n\ndef task7():\n    print('Executing task7')\n\ndef task8():\n    print('Executing task8')\n\ndef task9():\n    print('Executing task9')\n\ndef task10():\n    print('Executing task10')\n\ndef task11():\n    print('Executing task11')\n\ndef task12():\n    print('Executing task12')\n\ndef task13():\n    print('Executing task13')\n\ndef task14():\n    print('Executing task14')\n\ndef task15():\n    print('Executing task15')\n\ndef task16():\n    print('Executing task16')\n\ndef task17():\n    print('Executing task17')\n\ndef task18():\n    print('Executing task18')\n\ndef task19():\n    print('Executing task19')\n\ndef task20():\n    print('Executing task20')\n\ndef task21():\n    print('Executing task21')\n\ndef task22():\n    print('Executing task22')\n\ndef task23():\n    print('Executing task23')\n\ndef task24():\n    print('Executing task24')\n\ndef task25():\n    print('Executing task25')\n\ndef task26():\n    print('Executing task26')\n\ndef task27():\n    print('Executing task27')\n\ndef task28():\n    print('Executing task28')\n\ndef task29():\n    print('Executing task29')\n\ndef task30():\n    print('Executing task30')\n\ndef task31():\n    print('Executing task31')\n\ndef task32():\n    print('Executing task32')\n\ndef task33():\n    print('Executing task33')\n\ndef task34():\n    print('Executing task34')\n\ndef task35():\n    print('Executing task35')\n\ndef task36():\n    print('Executing task36')\n\ndef task37():\n    print('Executing task37')\n\ndef task38():\n    print('Executing task38')\n\ndef task39():\n    print('Executing task39')\n\ndef task40():\n    print('Executing task40')\n\ndef task41():\n    print('Executing task41')\n\ndef task42():\n    print('Executing task42')\n\ndef task43():\n    print('Executing task43')\n\ndef task44():\n    print('Executing task44')\n\ndef task45():\n    print('Executing task45')\n\ndef task46():\n    print('Executing task46')\n\ndef task47():\n    print('Executing task47')\n\ndef task48():\n    print('Executing task48')\n\ndef task49():\n    print('Executing task49')\n\ndef task50():\n    print('Executing task50')\n\ndef task51():\n    print('Executing task51')\n\ndef task52():\n    print('Executing task52')\n\ndef task53():\n    print('Executing task53')\n\ndef task54():\n    print('Executing task54')\n\ndef task55():\n    print('Executing task55')\n\ndef task56():\n    print('Executing task56')\n\ndef task57():\n    print('Executing task57')\n\ndef task58():\n    print('Executing task58')\n\ndef task59():\n    print('Executing task59')\n\ndef task", "metadata": {"file_name": "dag_0_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Build an Airflow pipeline that showcases branching logic using BranchDayOfWeekOperator to demonstrate conditional task selection. Implement a workflow that executes different tasks based on whether the current day matches specific day criteria.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'branch_day_of_week',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef branch_day_of_week():\n    today = datetime.now()\n    day_of_week = today.strftime('%A')\n    if day_of_week == 'Monday':\n        task1 = BashOperator(\n            task_id='task1',\n            bash_command='echo \"Monday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Tuesday':\n        task2 = BashOperator(\n            task_id='task2',\n            bash_command='echo \"Tuesday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Wednesday':\n        task3 = BashOperator(\n            task_id='task3',\n            bash_command='echo \"Wednesday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Thursday':\n        task4 = BashOperator(\n            task_id='task4',\n            bash_command='echo \"Thursday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Friday':\n        task5 = BashOperator(\n            task_id='task5',\n            bash_command='echo \"Friday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Saturday':\n        task6 = BashOperator(\n            task_id='task6',\n            bash_command='echo \"Saturday task executed\"',\n            dag=dag\n        )\n    elif day_of_week == 'Sunday':\n        task7 = BashOperator(\n            task_id='task7',\n            bash_command='echo \"Sunday task executed\"',\n            dag=dag\n        )\n    else:\n        task8 = BashOperator(\n            task_id='task8',\n            bash_command='echo \"Invalid day selected\"',\n            dag=dag\n        )\n\ntask1 = PythonOperator(\n    task_id='task1',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask2 = PythonOperator(\n    task_id='task2',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask3 = PythonOperator(\n    task_id='task3',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask4 = PythonOperator(\n    task_id='task4',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask5 = PythonOperator(\n    task_id='task5',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask6 = PythonOperator(\n    task_id='task6',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask7 = PythonOperator(\n    task_id='task7',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask8 = PythonOperator(\n    task_id='task8',\n    python_callable=branch_day_of_week,\n    dag=dag\n)\n\ntask1 >> task2\ntask2 >> task3\ntask3 >> task4\ntask4 >> task5\ntask5 >> task6\ntask6 >> task7\ntask7 >> task8", "metadata": {"file_name": "dag_3_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop an Airflow DAG that simulates a decision-making process where a single task randomly determines which subsequent tasks will be executed. Create a workflow that demonstrates how to implement probabilistic routing in data processing pipelines using Airflow's task branching decorator.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom random import choice\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndef random_task():\n    return choice(['task1', 'task2', 'task3'])\n\ndef execute_next_task(task):\n    return task.execute(context={})\n\nwith DAG(\n    'random_task_dag',\n    default_args=default_args,\n    schedule_interval='@once',\n    catchup=False,\n    tags=['random_task'],\n) as dag:\n    task1 = BashOperator(\n        task_id='task1',\n        bash_command='echo \"Task 1 executed\"',\n        retries=3,\n    )\n\n    task2 = BashOperator(\n        task_id='task2',\n        bash_command='echo \"Task 2 executed\"',\n        retries=3,\n    )\n\n    task3 = BashOperator(\n        task_id='task3',\n        bash_command='echo \"Task 3 executed\"',\n        retries=3,\n    )\n\n    task4 = BashOperator(\n        task_id='task4',\n        bash_command='echo \"Task 4 executed\"',\n        retries=3,\n    )\n\n    task5 = BashOperator(\n        task_id='task5',\n        bash_command='echo \"Task 5 executed\"',\n        retries=3,\n    )\n\n    task6 = BashOperator(\n        task_id='task6',\n        bash_command='echo \"Task 6 executed\"',\n        retries=3,\n    )\n\n    task7 = BashOperator(\n        task_id='task7',\n        bash_command='echo \"Task 7 executed\"',\n        retries=3,\n    )\n\n    task8 = BashOperator(\n        task_id='task8',\n        bash_command='echo \"Task 8 executed\"',\n        retries=3,\n    )\n\n    task9 = BashOperator(\n        task_id='task9',\n        bash_command='echo \"Task 9 executed\"',\n        retries=3,\n    )\n\n    task10 = BashOperator(\n        task_id='task10',\n        bash_command='echo \"Task 10 executed\"',\n        retries=3,\n    )\n\n    task11 = BashOperator(\n        task_id='task11',\n        bash_command='echo \"Task 11 executed\"',\n        retries=3,\n    )\n\n    task12 = BashOperator(\n        task_id='task12',\n        bash_command='echo \"Task 12 executed\"',\n        retries=3,\n    )\n\n    task13 = BashOperator(\n        task_id='task13',\n        bash_command='echo \"Task 13 executed\"',\n        retries=3,\n    )\n\n    task14 = BashOperator(\n        task_id='task14',\n        bash_command='echo \"Task 14 executed\"',\n        retries=3,\n    )\n\n    task15 = BashOperator(\n        task_id='task15',\n        bash_command='echo \"Task 15 executed\"',\n        retries=3,\n    )\n\n    task16 = BashOperator(\n        task_id='task16',\n        bash_command='echo \"Task 16 executed\"',\n        retries=3,\n    )\n\n    task17 = BashOperator(\n        task_id='task17',\n        bash_command='echo \"Task 17 executed\"',\n        retries=3,\n    )\n\n    task18 = BashOperator(\n        task_id='task18',\n        bash_command='echo \"Task 18 executed\"',\n        retries=3,\n    )\n\n    task19 = BashOperator(\n        task_id='task19',\n        bash_command='echo \"Task 19 executed\"',\n        retries=3,\n    )\n\n    task20 = BashOperator(\n        task_id='task20',\n        bash_command='echo \"Task 20 executed\"',\n        retries=3,\n    )\n\n    task21 = BashOperator(\n        task_id='task21',\n        bash_command='echo \"Task 21 executed\"',\n        retries=3,\n    )\n\n    task22 = BashOperator(", "metadata": {"file_name": "dag_2_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
{"instruction": "Develop an Airflow DAG that illustrates dynamic task routing using day-of-week conditions for data processing workflows. Create a scenario where tasks are selectively executed based on whether the current day meets predefined scheduling requirements.", "input": {"airflow_version": "2.7.2"}, "output": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dynamic_day_of_week_task_routing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\ndef process_data():\n    print(\"Processing data\")\n\ndef process_data_with_day_of_week():\n    print(\"Processing data with day of week\")\n\ndef process_data_with_day_of_week_and_time():\n    print(\"Processing data with day of week and time\")\n\ndef process_data_with_day_of_week_and_time_and_time():\n    print(\"Processing data with day of week, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time, time, time, time, time, time, time, time, and time\")\n\ndef process_data_with_day_of_week_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time_and_time():\n    print(\"Processing data with day of week, time, time, time, time, time, time,", "metadata": {"file_name": "dag_3_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "model": "qwen-server", "backend": "llama-server-client"}}
