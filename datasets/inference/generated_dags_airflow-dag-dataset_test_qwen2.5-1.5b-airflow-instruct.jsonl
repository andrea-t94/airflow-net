{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I have a pandas DataFrame `df` with a column `text` that contains strings. I want to create a new column `text_len` that contains the length of each string in the `text` column. How can I achieve this?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.standard.operators.python import PythonOperator\n\n\n@task(task_id=\"test_task\")\ndef test_function():\n    print(\"Hello world!\")\n\n\nwith DAG(\n    dag_id=\"example_pendulum\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_function()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that showcases the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class. Build a simple workflow that triggers an empty operator after standard work hours to illustrate scheduling flexibility.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.weekday_timetable import AfterWorkdayTimetable, WeekdayTimetable\n\n# [START howto_operator_custom_timetable]\nwith DAG(\n    dag_id=\"example_timetable\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    dagrun_timeout=pendulum.duration(seconds=60),\n    tags=[\"example\"],\n    after_workday_timetable=AfterWorkdayTimetable(days=[WeekDay.SATURDAY], hours=[8], minutes=[0]),\n    weekday_timetable=WeekdayTimetable(\n        start_days=[WeekDay.TUESDAY], end_day=WeekDay.SUNDAY, start_hour=8, start_minute=0\n    ),\n) as dag:\n    run_this = EmptyOperator(task_id=\"run_this\")\n    run_this[(\"pre_task\" if dag.dagrun_context['pre_task' in dag.dagrun_context] else \"\")]"}], "metadata": {"file_name": "dag_1063_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Tell me how to use TensorFlow with Python.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "import os\nfrom datetime import datetime\n\nimport tensorflow as tf\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.python import PythonOperator\n\n\n@task.python\ndef hello_world():\n    print(\"Hello World!\")\n\n\n@task.python\ndef learn_keras():\n    print(tf.keras.applications.vgg19.VGG19().summary())\n\n\ndef hello_world_callable():\n    print(\"Hello World!\")\n\n\ndef learn_keras_callable():\n    print(tf.keras.applications.vgg19.VGG19().summary())\n\n\nenv_id = os.environ.get(\"ENV_ID\")\nlearn_keras_task = learn_keras_callable()\nhello_world_task = hello_world_callable()\nlearn_keras_task >> hello_world_task\n\n\n# [START dag_def]\ndag = DAG(\n    \"example_tensorflow\",\n    schedule=\"* * * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    default_args={\"retries\": 1},\n    tags=[\"example\"],\n)\n# [END dag_def]\n\nif __name__ == \"__main__\":\n    from airflow.utils.trigger_rule import TriggerRule\n\n    dag.triggers = [\n        AirflowTriggerOperator(task_id=\"hello_world_task\", task_trigger_id=\"hello_world_callable\", trigger_rule=TriggerRule.ALL_DONE)\n    ]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n\nSNOWFLAKE_SAMPLE_TABLE = os.environ.get(\"SNOWFLAKE_SAMPLE_TABLE\", \"SAMPLE_TABLE\")\nSNOWFLAKE_CONNECTION = os.environ.get(\"SNOWFLAKE_CONNECTION\", \"snowflake_default\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"snowflake_sample_table\"\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"snowflake_conn_id\": SNOWFLAKE_CONNECTION,\n        \"snowflake_sql_path\": f\"SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n        \"conn_id\": SNOWFLAKE_CONNECTION,\n        \"sql_path\": f\"SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n        \"timeout\": 60 * 20,\n        \"retries\": 10,\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_snowflake_sample_table]\n    load_data = SnowflakeOperator(task_id=\"load_data\", sql=\"SELECT * FROM sample_table\")\n    # [END howto_operator_snowflake_sample_table]\n\n    load_data"}], "metadata": {"file_name": "dag_1798_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data exploration DAG that uses Azure Data Lake Storage to perform a remote file inventory of Parquet-formatted files in a specific directory. Construct an automated task that lists file metadata to support data validation and tracking processes.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import AzureDataLakeListOperator\n\nLOCAL_CONNECTION_ID = os.environ.get(\"LOCAL_CONNECTION_ID\", \"postgres_default\")\n\nwith DAG(\n    \"adls-list-dag\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"adls\"],\n) as dag:\n    @task\n    def get_adls_connection():\n        \"\"\"\n        Get ADLS connection. Connection ID can be replaced with connection object.\n        \"\"\"\n        return {\n            \"conn_id\": LOCAL_CONNECTION_ID,\n            \"path\": \"azureadlsfile\",\n        }\n\n    list_adls_files = AzureDataLakeListOperator(task_id=\"list_adls_files\", conn_id=\"adls_default\")\n    get_adls_connection()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2154_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data management workflow for tracking employee information in a Vertica database. Create a process that demonstrates creating, populating, querying, and cleaning up an employee records table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.vertica.operators.vertica import VerticaOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_vertica_operator\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"vertica_conn_id\": \"vertica_conn\", \"owner\": \"pawel\"},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_vertica_operator]\n    create_employee_table = VerticaOperator(\n        task_id=\"create_employee_table\",\n        sql=r\"\"\"\n        CREATE TABLE IF NOT EXISTS employees (\n            id IDENTITY,\n            name VARCHAR NOT NULL,\n            salary NUMERIC NOT NULL,\n            hire_date DATE NOT NULL\n        );\n        \"\"\",\n        vertica_conn_id=\"vertica_conn\",\n    )\n\n    insert_employee_record = VerticaOperator(\n        task_id=\"insert_employee_record\",\n        sql=r\"\"\"INSERT INTO employees(name, salary, hire_date) VALUES ('Alice', 50000, '2020-01-01')\"\"\",\n        vertica_conn_id=\"vertica_conn\",\n    )\n\n    select_employee_record = VerticaOperator(\n        task_id=\"select_employee_record\",\n        sql=r\"\"\"SELECT * FROM employees WHERE name = 'Alice'\"\"\",\n        vertica_conn_id=\"vertica_conn\",\n    )\n\n    drop_employee_table = VerticaOperator(\n        task_id=\"drop_employee_table\",\n        sql=r\"\"\"DROP TABLE IF EXISTS employees\"\"\",\n        vertica_conn_id=\"vertica_conn\",\n    )\n    # [END howto_vertica_operator]\n\n    create_employee_table >> insert_employee_record >> select_employee_record >> drop_employee_table"}], "metadata": {"file_name": "dag_1418_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I want to format the date and time from a Unix timestamp to a human-readable format using Python. How can I do this?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"print_date\")\ndef print_date(time: pendulum.DateTime):\n    \"\"\"\n    Print the date and time.\n\n    :param time DateTime to be printed.\n    \"\"\"\n    print(time)\n\n\nwith DAG(\n    dag_id=\"example_pendulum_function\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n):\n    pass"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a data processing workflow with explicit initialization and cleanup stages, using bash commands to represent different phases of data preparation, processing, and finalization. Create a DAG that illustrates how setup tasks can prepare an environment and teardown tasks can perform necessary cleanup operations around the main data processing logic.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef run_this_func():\n    print(\"WOOT\")\n\n\nwith DAG(\n    \"example_setup_cleanup\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    run_this_func()"}], "metadata": {"file_name": "dag_22_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I need to convert a string to binary in Python. How can I do that?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def string_to_binary(input_string):\n    \"\"\"\n    Convert a given string to its binary representation.\n    \n    Args:\n    input_string (str): The string to be converted to binary.\n    \n    Returns:\n    str: The binary representation of the input string.\n    \"\"\"\n    return ''.join(format(ord(char), '08b') for char in input_string)\n\n# Example usage\nprint(string_to_binary(\"Hello World\"))  # Output: 01001000011001010110110001110011011011110111001101101110001101110011100110110111001101111"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I modify the color of a button in a form using the `setStyle` method in Python with the PyQt5 library?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.qlik.operators.qlik import QliksdOpenFormOperator\n\ntry:\n    from airflow.providers.qlik.hooks.qlik import QliksdHook  # type: ignore[missing-import]\nexcept ImportError:\n    print(\"Airflow provider qlik is not installed.\")\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = dag dag_id=\"qlik_form_trigger\"\nDAG_ID = ENV_ID\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"qlik\"],\n) as dag:\n    # [START howto_qlik_form_trigger]\n    t1 = task(task_id=\"set_value_form_form_url\")\n    t2 = task(task_id=\"set_value_form_form_name\")\n    t3 = task(task_id=\"set_value_form_data\")\n    t4 = QliksdOpenFormOperator(\n        task_id=\"form_id\",\n        form_url=\"{{ task_instance.xcom_pull('set_value_form_form_url') }}\",\n        form_name=\"{{ task_instance.xcom_pull('set_value_form_form_name') }}\",\n        data=\"{{ task_instance.xcom_pull('set_value_form_data') }}\",\n    )\n    # [END howto_qlik_form_trigger]\n    chain(t1, t2, t3, t4)\n    t1 >> t2 >> t3"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kafka-based FizzBuzz game that demonstrates message streaming and event processing. Create a system that generates numbers, checks for divisibility by 3 and 5, and triggers specific actions based on number characteristics.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.default.operators.bash import BashOperator\n\n\n@task\ndef generate_kafka_message(number: int):\n    return f\"Number {number} is divisible by 3 or 5\"\n\n\n@task\ndef consume_kafka_message(message: str):\n    print(f\"Consumed message: {message}\")\n\n\ndef run():\n    dag = DAG(\n        dag_id=\"example_kafka_to_operator\",\n        start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n        schedule=None,\n        tags=[\"example\"],\n    )\n\n    generate = generate_kafka_message.override(task_id=\"generate_kafka_message\")(job_id=\"123\")\n    consume = consume_kafka_message.override(task_id=\"consume_kafka_message\")(generate.output)\n\n    generate >> consume\n\n\nif __name__ == \"__main__\":\n    run()"}], "metadata": {"file_name": "dag_1533_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how preparatory and cleanup operations can be automatically managed around core task execution. Create a DAG that illustrates task dependency and group management with clear initialization and finalization steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_me\")\ndef run_me():\n    print(\"woot! woot! woot!\")\n\n\nwith DAG(\n    \"example_task_group\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    run_me()"}], "metadata": {"file_name": "dag_1446_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "\u5728Python\u4e2d\uff0c`len()`\u51fd\u6570\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u5b57\u7b26\u4e32\u3001\u5217\u8868\u3001\u5143\u7ec4\u4ee5\u53ca\u5b57\u5178\u7b49\u96c6\u5408\u7c7b\u578b\u7684\u5143\u7d20\u6570\u91cf\u3002\u7136\u800c\uff0c\u5b83\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u6574\u6570\u7c7b\u578b\u7684\u6570\u636e\u3002\u5982\u679c\u6211\u60f3\u8ba1\u7b97\u4e00\u4e2a\u6574\u6570\u7684\u4f4d\u6570\uff0c\u5e94\u8be5\u600e\u4e48\u505a\u5462\uff1f\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def calculate_integer_bits(number):\n    \"\"\"\n    Calculate the number of bits required to represent a non-negative integer.\n    \n    Args:\n    number (int): A non-negative integer whose bit length is to be calculated.\n    \n    Returns:\n    int: The number of bits required to represent the integer.\n    \"\"\"\n    if number == 0:\n        return 1\n    return int(math.log2(number)) + 1\n\n# Example usage\nprint(calculate_integer_bits(0))    # Output: 1\nprint(calculate_integer_bits(1))    # Output: 1\nprint(calculate_integer_bits(2))    # Output: 2\nprint(calculate_integer_bits(1023)) # Output: 10"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration workflow that transfers files from a cloud storage bucket to an SFTP destination with multiple transfer strategies. Create a pipeline that supports different file transfer modes including copying individual files, moving files, and transferring entire directory contents.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.sftp.transfers.local_to_sftp import LocalFilesystemToSFTPSensor\nfrom airflow.providers.sftp.transfers.sftp_to_sftp import SFTPSynchronize\nfrom airflow.providers.sftp.transfers.sftp_to_local import SFTPToLocalFilesystemSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp\"\n\nlocal_file_path = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nremote_file_path = os.environ.get(\"REMOTE_FILE_PATH\", \"/remotefile.txt\")\nsftp_conn = os.environ.get(\"SFTP_CONN_ID\", \"sftp_default\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def setup_sftp_connection():\n        conn = Connection(\n            conn_id=sftp_conn,\n            conn_type=\"sftp\",\n        )\n        conn_extra = {\"host\": \"example@sftp.example.com\", \"path\": \"/example/sftp/example.file\"}\n        conn_extra_json = json.dumps(conn_extra)\n        print(conn_extra_json)\n        conn.set_extra(conn_extra_json)\n\n    setup_sftp_connection_task = setup_sftp_connection()\n\n    # [START howto_operator_sftp_to_local]\n    task1 = LocalFilesystemToSFTPSensor(\n        task_id=\"local_to_sftp_sensor\",\n        local_filepath=local_file_path,\n        remote_filepath=remote_file_path,\n        sftp_conn_id=sftp_conn,\n    )\n    # [END howto_operator_sftp_to_local]\n\n    # [START howto_operator_sftp_sync]\n    task2 = SFTPSynchronize(\n        task_id=\"sftp_to_sftp_sync\",\n        remote_filepath=remote_file_path,\n        local_filepath=local_file_path,\n        sftp_conn_id=sftp_conn,\n    )\n    # [END howto_operator_sftp_sync]\n\n    # [START howto_operator_sftp_to_local_async]\n    task3 = LocalFilesystemToSFTPSensor(\n        task_id=\"local_to_sftp_sensor_async\",\n        local_filepath=local_file_path,\n        remote_filepath=remote_file_path,\n        sftp_conn_id=sftp_conn,\n        deferrable=True,\n    )\n    # [END howto_operator_sftp_to_local_async]\n\n    # [START howto_operator_sftp_to_local_sensor]\n    task4 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor\",\n        remote_filepath=remote_file_path,\n        local_filepath=local_file_path,\n        sftp_conn_id=sftp_conn,\n    )\n    # [END howto_operator_sftp_to_local_sensor]\n\n    task5 = BashOperator(\n        task_id=\"remove_file\",\n        bash_command=f\"rm {local_file_path}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    setup_sftp_connection_task >> task1 >> task2 >> task3 >> task4 >> task5"}], "metadata": {"file_name": "dag_1298_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data migration workflow that enables seamless transfer of structured CSV data from cloud object storage into a columnar analytics database. Create an Airflow task that supports efficient, automated data loading from cloud storage to a queryable data platform.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_to_trino\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection(conn_id: str):\n        conn = Connection(\n            conn_id=conn_id,\n            conn_type=\"trino\",\n            description=\"Example Trino connection.\",\n            extra={\"sql_server\": \"example_sql_server\", \"example_key\": \"example_value\"},\n        )\n        db = Role(name=\"example_sql_server\")\n        user = Role(name=\"example_user\")\n        TaskGroup(task_id=\"users_group\", role=db)([\n            BashOperator(\n                task_id=\"create_user\",\n                bash_command=\"if [ $(id -u) != 0 ]; then echo 'You must be the superuser to execute this script!'; exit 1;fi;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"create_database\",\n                bash_command=\"createdb example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"create_user_table\",\n                bash_command=\"echo \\\"create table users (id varchar(100), name varchar(100));\\\" | sqlite3 example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"insert_data\",\n                bash_command=\"echo \\\"insert into users values ('1', 'John Doe');\\\" | sqlite3 example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"select_data\",\n                bash_command=\"echo \\\"select * from users;\\\" | sqlite3 example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"drop_user_table\",\n                bash_command=\"echo \\\"drop table users;\\\" | sqlite3 example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"drop_database\",\n                bash_command=\"echo \\\"drop database example_sql_server;\\\" | sqlite3 example_sql_server;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n        ], role=user)\n\n    create_connection_task = create_connection(conn_id=\"example_sql_server\")\n\n    @task\n    def create_connection_sql_server(conn_id: str):\n        conn = Connection(\n            conn_id=conn_id,\n            conn_type=\"sql_server\",\n            description=\"Example sql_server connection.\",\n            extra={\"example_key\": \"example_value\"},\n        )\n        db = Role(name=\"example_db\")\n        user = Role(name=\"example_user\")\n        TaskGroup(task_id=\"users_group\", role=db)([\n            BashOperator(\n                task_id=\"create_user\",\n                bash_command=\"if [ $(id -u) != 0 ]; then echo 'You must be the superuser to execute this script!'; exit 1;fi;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ),\n            BashOperator(\n                task_id=\"create_sql_server_database\",\n                bash_command=\"create database example_db;\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            ],\n            role=user)\n\n    create_connection_sql_server_task = create_connection_sql_server(conn_id=\"example_sql_server\")\n\n    @task\n    def drop_connection_sql_server(conn_id: str):\n        sql = f\"drop connection {conn_id};\"\n        SQLExecuteQueryOperator(task_id=\"drop_connection_sql_server\", sql=sql)\n\n    drop_connection_sql_server_task = drop_connection_sql_server(conn_id=\"example_sql_server\")\n\n    execute_query = SQLExecuteQueryOperator(\n        task_id=\"execute_query\", sql=\"select * from users;\"\n    )\n\n    drop_connection_task = BashOperator(\n        task_id=\"drop_connection_example_sql_server\",\n        bash_command=\"drop connection example_sql_server;\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    @task\n    def clean_up():\n        conn_id = os.environ.get(\"TRINO_CONNECTION_ID\")\n        if conn_id:\n            BashOperator(\n                task_id=\"remove_connection_env_variable\",\n                bash_command=f\"unset {conn_id};\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            )\n\n    clean_up_task = clean_up()\n\n    [create_connection_task, create_connection_sql_server_task] >> execute_query\n    [create_connection_sql_server_task, drop_connection_sql_server_task] >> drop_connection_task\n    [create_connection_sql_server_task, drop_connection_task, clean_up_task] >> os.environ.get(\n        \"TRINO_CONNECTION_ID\"\n    )"}], "metadata": {"file_name": "dag_992_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Generate a Python code to create a DataFrame from a dictionary where each key-value pair in the dictionary represents a column and its corresponding values. The dictionary provided is: `{'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'City': ['New York', 'Los Angeles', 'Chicago']}`. How can I handle missing values if any of the lists have a different length?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pandas as pd\n\n# Generate a DataFrame from a dictionary with a list of strings.\ndef generate_dataframe_with_dict() -> pd.DataFrame:\n    \"\"\"\n    Generate a DataFrame from a dictionary with a list of strings.\n\n    Returns:\n        pd.DataFrame: DataFrame created from the dictionary.\n    \"\"\"\n    # Create a dictionary with a list of strings as the value for each key.\n    data = {\n        \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"Age\": [25, 30, 35],\n        \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n    }\n\n    # Convert the dictionary to a DataFrame.\n    dataframe = pd.DataFrame(data)\n\n    return dataframe\n\n# Generate a DataFrame from a list of lists.\ndef generate_dataframe_with_list_of_lists() -> pd.DataFrame:\n    \"\"\"\n    Generate a DataFrame from a list of lists.\n\n    Returns:\n        pd.DataFrame: DataFrame created from the list of lists.\n    \"\"\"\n    # Create a list of lists with different lengths.\n    data = [\n        [\"Alice\", \"New York\", 25],\n        [\"Bob\", \"Los Angeles\", 30],\n        [\"Charlie\", \"Chicago\", 35],\n    ]\n\n    # Convert the list of lists to a DataFrame.\n    dataframe = pd.DataFrame(data)\n\n    return dataframe\n\n# Generate a DataFrame from a list of tuples.\ndef generate_dataframe_with_list_of_tuples() -> pd.DataFrame:\n    \"\"\"\n    Generate a DataFrame from a list of tuples.\n\n    Returns:\n        pd.DataFrame: DataFrame created from the list of tuples.\n    \"\"\"\n    # Create a list of tuples with different lengths.\n    data = [\n        (\"Alice\", \"New York\", 25),\n        (\"Bob\", \"Los Angeles\", 30),\n        (\"Charlie\", \"Chicago\", 35),\n    ]\n\n    # Convert the list of tuples to a DataFrame.\n    dataframe = pd.DataFrame(data)\n\n    return dataframe\n\n# Generate a DataFrame from a list of dictionaries.\ndef generate_dataframe_with_list_of_dicts() -> pd.DataFrame:\n    \"\"\"\n    Generate a DataFrame from a list of dictionaries.\n\n    Returns:\n        pd.DataFrame: DataFrame created from the list of dictionaries.\n    \"\"\"\n    # Create a list of dictionaries with different lengths.\n    data = [\n        {\"Name\": \"Alice\", \"Age\": 25, \"City\": \"New York\"},\n        {\"Name\": \"Bob\", \"Age\": 30, \"City\": \"Los Angeles\"},\n        {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"},\n    ]\n\n    # Convert the list of dictionaries to a DataFrame.\n    dataframe = pd.DataFrame(data)\n\n    return dataframe\n\n# Generate a DataFrame with missing values.\ndef generate_dataframe_with_missing_values() -> pd.DataFrame:\n    \"\"\"\n    Generate a DataFrame with missing values.\n\n    Returns:\n        pd.DataFrame: DataFrame created with missing values.\n    \"\"\"\n    # Create a dictionary with a list of strings, including a missing value.\n    data = {\n        \"Name\": [\"Alice\", \"Bob\", None],\n        \"Age\": [25, 30, 35],\n        \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n    }\n\n    # Convert the dictionary to a DataFrame.\n    dataframe = pd.DataFrame(data)\n\n    return dataframe"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-stage workflow that demonstrates task grouping and sequential processing using Airflow. Create a DAG that shows how complex workflows can be organized into logical sections with nested task groups.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    print(pendulum.datetime.now())\n\n\n@task(task_id=\"print_date_2\")\ndef print_date_2():\n    print(pendulum.datetime.now())\n\n\nwith DAG(\n    \"example_deferrable_task\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n):\n    print_date()\n    print_date_2()"}], "metadata": {"file_name": "dag_354_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I create a Python function to retrieve the details of an Elastic Network Interface (ENI) using the Boto3 library?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom botocore.client import BaseClient\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dag import DAG\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.python import void_task\nfrom airflow.utils.types import StringList\nfrom airflow.utils.weekday import WeekDay\n\n# [START howto_elb_arg_formatter]\ndef get_elb_args(*, load_balancer_name: str | None = None, load_balancer_domain_name: str | None = None)\n    return {\n        \"LoadBalancerName\": load_balancer_name,\n        \"LoadBalancerDomainName\": load_balancer_domain_name,\n    }\n# [END howto_elb_arg_formatter]\n\n# [START howto_elb_filter]\ndef get_elb_filters(namespace: StringList, prefix: str)\n    return [\n        {\"Name\": prefix + \"InstanceId.\", \"Values\": namespace, \"Scope\": \"global\", \"MatchSet\": \"all\"},\n    ]\n# [END howto_elb_filter]\n\n# [START howto_eni_arg_formatter]\ndef get_eni_args(*, security_group_names: StringList)\n    return {\"SecurityGroupNames\": security_group_names}\n# [END howto_eni_arg_formatter]\n\n\n@task\ndef get_aws_account_id() -> int:\n    \"\"\"\n    Returns the AWS Account ID\n    \"\"\"\n    return 123456789012\n\n\n@task\ndef create_security_group(security_group_name: StringList, aws_account_id: int) -> BaseClient:\n    print(f\"Creating security group {security_group_name} with AWs Account ID {aws_account_id}\")\n    response = {\n        \"SecurityGroup\": {\n            \"GroupId\": \"sg-508e19b4\",\n            \"Description\": \"New Security Group\",\n            \"VpcId\": \"vpc-28dcaeb6\",\n            \"IpPermissions\": [\n                {\n                    \"FromPort\": 22,\n                    \"IpProtocol\": \"all\",\n                    \"IpRanges\": [{\"Value\": \"0.0.0.0/0\"}],\n                },\n                {\n                    \"FromPort\": 80,\n                    \"IpProtocol\": \"all\",\n                    \"IpRanges\": [{\"Value\": \"0.0.0.0/0\"}],\n                    \"PortRange\": {\"Value\": \"1-65535\"},\n                },\n            ],\n        },\n        \"AssociateSecurityGroupRequest\": {\n            \"GroupId\": \"sg-508e19b4\",\n            \"SecurityGroupNames\": security_group_name,\n        },\n        \"CreateSecurityGroupRequest\": {\"Description\": \"New Security Group\", \"VpcId\": \"vpc-28dcaeb6\"},\n    }\n    client: BaseClient = void_task()\n    client.create_security_group = void_task()\n    client.create_security_group.return_value = create_security_group\n    return client\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(security_group_id: str):\n    print(f\"Deleting security group {security_group_id}\")\n    client: BaseClient = void_task()\n    client.delete_security_group(GroupId=security_group_id)\n\n\nwith DAG(\n    dag_id=\"example_boto3\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    get_aws_account_id_task = get_aws_account_id()\n    security_groups_to_create: List[str] = [\"sg-1\", \"sg-2\", \"sg-3\"]\n    security_group_names = [f\"{g}-eni\" for g in security_groups_to_create]\n    eni_security_groups = [f\"{g}-eni\" for g in security_groups_to_create]\n\n    # [START howto_elb_rds]\n    get_eni_from_elb = BaseOperator(\n        task_id=\"get_eni_from_elb\",\n        python_callable=\u7684\u662fELB\u83b7\u53d6ENI\u4eceELB,\n        op_kwargs={\n            \"load_balancer_name\": \"test-lb\",\n            \"load_balancer_domain_name\": \"test-elb\",\n            \"eni_args\": get_eni_args(security_group_names=eni_security_groups),\n        },\n    )\n    # [END howto_elb_rds]\n\n    # [START howto_eni_create]\n    create_eni = BaseOperator(\n        task_id=\"eni_create\",\n        python_callable=\u7684\u662fENI\u521b\u5efa,\n        op_kwargs={\n            \"eni_args\": get_eni_args(security_group_names=eni_security_groups),\n            \"eni_result\": void_task(),\n        },\n    )\n    # [END howto_eni_create]\n\n    # [START howto_eni_get]\n    get_eni = BaseOperator(\n        task_id=\"eni_get\",\n        python_callable=\u7684\u662fENI\u83b7\u53d6,\n        op_kwargs={\"eni_args\": get_eni_args(instance_id=\"i-0c6b88dd7f447dddb\")},\n    )\n    # [END howto_eni_get]\n\n    # [START howto_eni_filter]\n    get_eni_from_elb_filter = BaseOperator(\n        task_id=\"eni_from_elb_filter\",\n        python_callable=\u7684\u662fELB\u83b7\u53d6ENI\u8fc7\u6ee4,\n        op_kwargs={\n            \"eni_args\": get_elb_args(),\n            \"filters\": get_elb_filters(namespace=security_group_names, prefix=\"SecurityGroupIngress.\"),\n        },\n    )\n    # [END howto_eni_filter]\n\n    (\n        get_aws_account_id_task\n        >> create_security_group(security_group_names, get_aws_account_id_task.output)\n        >> [create_eni, *eni_security_groups]  # type: ignore[arg-type]\n        >> get_eni\n        >> get_eni_from_elb\n        >> get_eni_from_elb_filter\n        >> delete_security_group\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that retrieves Campaign Manager metrics, generates a standard performance report across campaigns, and demonstrates batch conversion tracking and reporting capabilities. The DAG should illustrate how to interact with Campaign Manager's reporting and conversion APIs using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.report import (\n    GoogleCampaignManagerGetReportOperator,\n    GoogleCampaignManagerReportOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.report import GoogleCampaignManagerReportSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GMP_PROJECT_ID\", \"default\")\n\nDAG_ID = \"example_campaign_manager\"\n\n# [START howto_campaign_manager_report_sensor]\n# [START howto_campaign_manager_report_operator]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"report\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"google_marketing_platform\",\n        conn_type=\"google_marketplace\",\n        project_id=PROJECT_ID,\n        user=\"user\",\n        password=\"password\",\n    )\n\n    # [START howto_campaign_manager_report_sensor_decorator]\n    get_report_async = GoogleCampaignManagerReportSensor(\n        report_id=\"{{ task_instance.xcom_pull('create_report')['reportId'] }}\",\n        task_id=\"get_report_async\",\n    )\n    # [END howto_campaign_manager_report_sensor_decorator]\n\n    # [START howto_campaign_manager_report_operator_decorator]\n    create_report = GoogleCampaignManagerReportOperator(\n        task_id=\"create_report\",\n        body={\"parameters\": {\"dimensions\": [\"campaign_id\"], \"date_range\": {\"begin_date\": \"today-1y\", \"end_date\": \"today\"}}},\n        report_id=\"{{ task_instance.xcom_pull('create_report')['reportId'] }}\",\n    )\n    # [END howto_campaign_manager_report_operator_decorator]\n\n    # [START howto_campaign_manager_report_sensor_decorator]\n    get_report_sensor = GoogleCampaignManagerReportSensor(\n        report_id=\"{{ task_instance.xcom_pull('create_report')['reportId'] }}\",\n        task_id=\"get_report_sensor\",\n    )\n    # [END howto_campaign_manager_report_sensor_decorator]\n\n    # [START howto_campaign_manager_connection_decorator]\n    set_up_connection = GoogleCampaignManagerReportOperator(\n        task_id=\"set_up_connection\",\n        connection_id=conn.id,\n        body={\"parameters\": {\"dimensions\": [\"campaign_id\"], \"date_range\": {\"begin_date\": \"today-1y\", \"end_date\": \"today\"}}},\n    )\n    # [END howto_campaign_manager_connection_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_decorator]\n    get_report = GoogleCampaignManagerReportOperator(\n        task_id=\"get_report\", report_id=\"123\", destination_url=\"http://example.com/\"\n    )\n    # [END howto_campaign_manager_get_report_operator_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_async_decorator]\n    get_report_async\n    # [END howto_campaign_manager_get_report_operator_async_decorator]\n\n    # [START howto_campaign_manager_remove_report_operator_decorator]\n    remove_report = GoogleCampaignManagerReportOperator(\n        task_id=\"remove_report\", report_id=\"123\", delete_report=True\n    )\n    # [END howto_campaign_manager_remove_report_operator_decorator]\n\n    # [START howto_campaign_manager_create_connection_operator_decorator]\n    create_connection = GoogleCampaignManagerReportOperator(\n        task_id=\"create_connection\", connection_id=conn.id, body={\"labels\": [{\"value\": \"example.com\"}]}\n    )\n    # [END howto_campaign_manager_create_connection_operator_decorator]\n\n    # [START howto_campaign_manager_remove_connection_operator_decorator]\n    remove_connection = GoogleCampaignManagerReportOperator(task_id=\"remove_connection\", connection_id=conn.id, trigger_rule=\"all_done\")\n    # [END howto_campaign_manager_remove_connection_operator_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_batch_decorator]\n    get_report_batch = GoogleCampaignManagerReportOperator(\n        task_id=\"get_report_batch\", batch_size=3, parameters={\"dimensions\": [\"campaign_id\"]}\n    )\n    # [END howto_campaign_manager_get_report_operator_batch_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_batch_async_decorator]\n    get_report_batch_async = GoogleCampaignManagerReportOperator(\n        task_id=\"get_report_batch_async\", batch_size=3, parameters={\"dimensions\": [\"campaign_id\"]}\n    )\n    # [END howto_campaign_manager_get_report_operator_batch_async_decorator]\n\n    # [START howto_campaign_manager_remove_report_operator_batch_decorator]\n    remove_report_batch = GoogleCampaignManagerReportOperator(\n        task_id=\"remove_report_batch\", report_ids=[\"123\", \"456\"], delete_report=True\n    )\n    # [END howto_campaign_manager_remove_report_operator_batch_decorator]\n\n    # [START howto_campaign_manager_create_connection_operator_batch_decorator]\n    create_connection_batch = GoogleCampaignManagerReportOperator(\n        task_id=\"create_connection_batch\", connection_id=conn.id, body={\"labels\": [{\"value\": \"example.com\"}]}\n    )\n    # [END howto_campaign_manager_create_connection_operator_batch_decorator]\n\n    # [START howto_campaign_manager_remove_connection_operator_batch_decorator]\n    remove_connection_batch = GoogleCampaignManagerReportOperator(\n        task_id=\"remove_connection_batch\", report_ids=[\"123\", \"456\"], trigger_rule=\"all_done\"\n    )\n    # [END howto_campaign_manager_remove_connection_operator_batch_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_batch_async_decorator]\n    get_report_batch_async\n    # [END howto_campaign_manager_get_report_operator_batch_async_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_custom_batch_decorator]\n    get_report_custom_batch = GoogleCampaignManagerReportOperator(\n        task_id=\"get_report_custom_batch\",\n        report_ids=[],\n        custom_batch_size=3,\n        parameters={\"dimensions\": [\"campaign_id\"]},\n    )\n    # [END howto_campaign_manager_get_report_operator_custom_batch_decorator]\n\n    # [START howto_campaign_manager_get_report_operator_custom_batch_async_decorator]\n    get_report_custom_batch_async = GoogleCampaignManagerReportOperator(\n        task_id=\"get_report_custom_batch_async\",\n        report_ids=[],\n        custom_batch_size=3,\n        parameters={\"dimensions\": [\"campaign_id\"]},\n    )\n    # [END howto_campaign_manager_get_report_operator_custom_batch_async_decorator]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)\n\n\n# [START howto_campaign_manager_report_sensor_async]\ntest_report_sensor = GoogleCampaignManagerReportSensorAsync(\n    report_id=\"12345\", task_id=\"test_report_sensor\"\n)\n# [END howto_campaign_manager_report_sensor_async]"}], "metadata": {"file_name": "dag_1336_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data transfer workflow that automatically moves files between source and destination buckets. Create a DAG that demonstrates GCS transfer job creation, file upload, and bucket management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_to_sftp\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\nSFTP_PATH = \"/tmp/\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"sftp\"],\n) as dag:\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"sftp_default\",\n            user=\"airflow\",\n            password=\"airflow\",\n            handler=__import__(\"airflow.providers.sftp.hooks.sftp_hook\"),\n        )\n        conn.expose_sftp = True\n        conn.description = \"SFTP connection for demo purposes\"\n        return conn\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def upload_file():\n        file_path = f\"{SFTP_PATH}{FILE_NAME}\"\n        bucket = f\"{BUCKET_NAME}\"\n        with open(file_path) as file:\n            file_content = file.read()\n        gcs_to_gcs = GCSToGCSOperator(\n            task_id=\"upload_file_to_gcs\",\n            source_bucket=bucket,\n            source_object=file_path,\n            destination_bucket=BUCKET_NAME,\n            destination_object=file_name=\"file.txt\",\n            replace=True,\n        )\n\n    create_sftp_connection = create_connection()\n\n    # [START howto_operator_gcs_to_sftp]\n    gcs_to_sftp = GCSToSFTPOperator(\n        task_id=\"gcs_to_sftp\",\n        sftp_path=SFTP_PATH,\n        gcs_bucket=BUCKET_NAME,\n        filename=FILE_NAME,\n    )\n    # [END howto_operator_gcs_to_sftp]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    BashOperator(\n        task_id=\"delete_connection\",\n        bash_command=f\"airflow providers sftp delete_connection {create_connection().conn_id}\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file()\n        >> create_sftp_connection\n        >> gcs_to_sftp\n        # TEST TEARDOWN\n        >> delete_bucket\n        >> BashOperator(bash_command=\"airflow providers sftp delete_connection {{ task_instance.xcom_pull('create_connection')['conn_id'] }}\")\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1291_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Data Loss Prevention (DLP) workflow that demonstrates the complete lifecycle of a DLP job in Google Cloud, from creation to deletion. Design a DAG that showcases how to programmatically manage sensitive data inspection tasks using Airflow and Google Cloud DLP operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobOperator,\n    CloudDLPDeleteJobOperator,\n    CloudDLP infiltration_detection_for_file_operator,\n    CloudDLPinfiltrationDetectionOptionsForFileOperator,\n    CloudDLPlist_jobs_operator,\n    CloudDLPupdate_job_operator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"dlp_job\"\nFILE_NAME = \"example_inspection_file.txt\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    create_job = CloudDLPCreateJobOperator(\n        task_id=\"create_dlp_job\",\n        project_id=PROJECT_ID,\n        dlp_job={\n            \"inspect_files_options\": {\n                \"infraction_threshold\": {\n                    \"Infiltration\": {\"value\": 1},\n                },\n                \"infiltration_detection_for_file_operator\": CloudDLPinfiltrationDetectionOptionsForFileOperator(\n                    task_id=\"inffiltration_detection_for_file_operator\",\n                    file_input={\n                        \"input_string\": {\"value\": f\"temp {{file_{FILE_NAME}}}}\".replace(\"\\\\{{\", \"{{\").replace(\n                            \"\\\\}}\", \"}}\").replace(',', ',\\n    '')\n                    },\n                ),\n            },\n            \"display_name\": \"dlp_job_test\",\n        },\n    )\n    # [START howto_operator_gcp_dlp_list_job_output]\n    list_job_output = CloudDLPlist_jobs_operator(\n        task_id=\"list_dlp_job_output\",\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_dlp_list_job_output]\n    # [START howto_operator_gcp_dlp_update_job]\n    update_job_output = CloudDLPupdate_job_operator(\n        task_id=\"update_dlp_job_timeout\",\n        project_id=PROJECT_ID,\n        dlp_job_id=\"dlp_job_test\",\n        dlp_job={\"operation_timeout_seconds\": 600},\n    )\n    # [END howto_operator_gcp_dlp_update_job]\n    delete_job = CloudDLPDeleteJobOperator(\n        task_id=\"delete_dlp_job\",\n        project_id=PROJECT_ID,\n        dlp_job_id=\"dlp_job_test\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_job >> [list_job_output, update_job_output] >> delete_job\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done.\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2017_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates a simple file transfer workflow from a local temporary file to an AWS S3 bucket. Create a process that generates a temporary file, transfers it to cloud storage, and then cleans up the temporary resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_sql\"\n\ns3_conn = Connection(\n    conn_id=\"s3_default\",\n    schema=\"airflow\",\n    conn_type=\"s3\",\n    host=\"s3-us-gov.amazonaws.com\",\n    login=\"akia1yt3x3wxcwp4vyhf\",\n    password=\"33kx5b1qU8kUkx+9b38t+4Q==\",\n    region=\"us-gov\",\n)\n\ntable = \"s3_to_sql_test\"\ncolumn = \"test\"\nconn = Connection(\n    conn_id=\"s3_default\",\n    schema=\"airflow\",\n    conn_type=\"s3\",\n    host=\"s3-us-gov.amazonaws.com\",\n    login=\"akia1yt3x3wxcwp4vyhf\",\n    password=\"33kx5b1qU8kUkx+9b38t+4Q==\",\n    region=\"us-gov\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_table():\n        from airflow.providers.standard.sql.operators.sql import SQLExecuteQueryOperator\n\n        SQLExecuteQueryOperator(task_id=\"create_table\", sql=f\"CREATE TABLE IF NOT EXISTS {table} (col_1 VARCHAR(10), col_2 VARCHAR(10))\")\n\n    @task\n    def insert_data():\n        from airflow.providers.standard.sql.operators.sql import SQLExecuteQueryOperator\n\n        SQLExecuteQueryOperator(task_id=\"insert_data\", sql=f\"INSERT INTO {table} VALUES ('test1', 'test2')\")\n\n    @task\n    def drop_table():\n        from airflow.providers.standard.sql.operators.sql import SQLExecuteQueryOperator\n\n        SQLExecuteQueryOperator(task_id=\"drop_table\", sql=f\"DROP TABLE {table}\")\n\n    @task\n    def clear_connections():\n        from airflow.providers.standard.sql.operators.sql import SQLExecuteQueryOperator\n\n        SQLExecuteQueryOperator(task_id=\"clear_connections\", sql=f\"DELETE FROM connections WHERE id = {conn.id}\")\n\n    create_table_task = create_table()\n    insert_task = insert_data()\n    drop_table_task = drop_table()\n    clear_connections_task = clear_connections()\n\n    # [START howto_operator_s3_to_sql]\n    s3_to_sql = BashOperator(\n        task_id=\"s3_to_sql\",\n        bash_command=f\"echo '{{ task_instance.xcom_pull('s3_to_sql_test') }}'\",\n    )\n    # [END howto_operator_s3_to_sql]\n\n    create_table_task >> insert_task >> s3_to_sql >> drop_table_task >> clear_connections_task"}], "metadata": {"file_name": "dag_1156_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a multi-stage system monitoring and logging workflow, with tasks that print context information, log SQL queries, generate sleep-based parallel tasks, and execute environment-specific Python functions. Develop a DAG that demonstrates task chaining, dependency management, and runtime flexibility.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Print the date\"\n    print(pendulum.datetime now())\n\n\n@task(task_id=\"sleep_task\")\ndef sleep_func():\n    \"Sleep for 10 seconds\"\n    import time\n\n    time.sleep(10)\n\n\nwith DAG(\n    dag_id=\"example_python_decorator\",\n    schedule=None,\n    start_date=pendulum.datetime(now().year, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    print_date()\n    sleep_func()"}], "metadata": {"file_name": "dag_2217_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow for deploying and managing an EMR (Elastic MapReduce) job on an EKS (Elastic Kubernetes Service) cluster using AWS services. Create a data processing pipeline that demonstrates launching a Kubernetes cluster, configuring IAM roles, and running a sample Python computation job.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterAlreadyExistsError, EksHook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrStartJobFlowListenerOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrJobFlowSensor, EmrStepSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Monday\nfrom airflow.utils.trigger_rule import WeekDayTriggerRule\n\nDAG_ID = \"example_emr_eks\"\n\n# [START howto_operator_emr_create_job_flow]\ndef _create_job_flow(role_arn, cluster_name, cluster_role_arn, log_uri,efs_id):\n    boto3.client(\"emr\").create_job_flow(\n        JobFlowName=cluster_name,\n        Applications={\n            \"Python\": {},\n        },\n        LogUri=log_uri,\n        ClusterRole=cluster_role_arn,\n        Instances={\n            \"MasterInstanceType\": \"m5.large\",\n            \"SlaveInstanceType\": \"m5.large\",\n            \"InstanceCount\": 2,\n            \"TerminationProtected\": True,\n        },\n        VisibleToAllUsers=True,\n        ServiceRole=role_arn,\n        Eks={\n            \"ClusterName\": cluster_name,\n            \"FargateProfile\": {\n                \"EcsTaskDefinition\": f\"{cluster_name}-task-1\",\n            },\n            \"AdditionalAttributes\": {\n                \"createResourcesFromImages\": True,\n                \"create_access_sns_topic\": True,\n                \"create_access_ssm_parameter\": True,\n                \"create_endpoint_ssl_certificate\": True,\n                \"create_internet_gateway\": True,\n                \"create_nat_gateway\": False,\n                \"create_vpc_endpoint_service\": {\n                    \"ServiceName\": \"emr-eks\",\n                    \"Capacity\": {\n                        \"ClusterCount\": 1,\n                        \"InstanceType\": \"Small\",\n                        \"Min\": 1,\n                        \"Max\": 1,\n                    },\n                },\n            },\n        },\n    )\n\n\n# [END howto_operator_emr_create_job_flow]\n\n# [START howto_operator_emr_start_job_flow_listener]\ndef _start_job_flow_listener(current_state):\n    if current_state == \"RUNNING\":\n        boto3.client(\"emr\").start_job_flow_listener(JobFlowId=\"{{ task_instance.xcom_pull(\"\n            \"task_ids='create_job_flow', key='job_flow_id') }}\", MonitorInterval=10)\n\n\n# [END howto_operator_emr_start_job_flow_listener]\n\n# [START howto_operator_emr_add_steps]\ndef _add_steps(cluster_name, job_flow_id, python_package):\n    steps = [\n        {\n            \"Name\": \"Print The Time\",\n            \"ActionOnFailure\": \"TERMINATE\",\n            \"HadoopJarStep\": {\n                \"Args\": [\n                    \"org.apache.hadoop.util.Shell\",\n                    \"echo\", \"Hello World\",\n                    \"exit 0\",\n                ],\n                \"Jar\": \"emr-jars/hadoop-common-3.3.0.jar\",\n                \"Name\": \"Print The Time\",\n                \"HadoopConf\": {\n                    \"hadoop.log.file\": \"hdfs:///{{ task_instance.xcom_pull(\"\n                                      \"task_ids='create_job_flow', key='job_flow_id') }}/logs\",\n                    \"hadoop.log.level\": \"WARN\",\n                    \"hadoop.security.authentication\": \"AWSIAM\",\n                },\n            },\n        },\n        {\n            \"Name\": \"Calculate Pi\",\n            \"ActionOnFailure\": \"TERMINATE\",\n            \"PythonStep\": {\n                \"PythonScript\": f\"\"\"import sys, subprocess\n                                cmd = \"echo \\\\\\\"{}\\\\\\\" | head -c {}m | tr \\\\\\\"\\\\n\\\\\\\"\\\\\\\"\\\\\\\" | bc\"\n                                subprocess.Popen(cmd.format(*sys.argv[1:3]), shell=True, stdout=subprocess.PIPE),\n                                sys.exit(0)\n                                \"\"\",\n                \"PythonVersion\": 3,\n                \"Name\": \"Calculate Pi\",\n                \"LogUri\": \"{{ task_instance.xcom_pull(task_ids='create_job_flow', \"\n                          \"key='job_flow_id')}}\",\n                \"Jars\": [],\n                \"Args\": [],\n                \"PyFiles\": [f\"{cluster_name}-task-1\"],\n                \"NamePrefix\": \"\",\n                \"ArgsTimeout\": 60,\n                \"HadoopConf\": {\n                    \"mapred.job.queue.name\": \"emr\",\n                    \"hadoop.security.authentication\": \"AWSIAM\",\n                },\n            },\n        },\n        *python_package,\n    ]\n    boto3.client(\"emr\").add_steps(\n        JobFlowId=job_flow_id, ClusterId=\"{{ task_instance.xcom_pull(task_ids='create_job_flow', \"\n                                         \"key='job_flow_id') }}\", Steps=steps\n    )\n\n\n# [END howto_operator_emr_add_steps]\n\n# [START howto_operator_emr_create_job_flow]\ndef _create_job_flow_from_eks(\n    cluster_name, cluster_role_arn, log_uri, eks_cluster_name, eks_task_role_arn, fargate_task_def_name\n):\n    boto3.client(\"emr\").create_job_flow(\n        JobFlowName=cluster_name,\n        Applications={\n            \"Python\": {},\n        },\n        LogUri=log_uri,\n        ClusterRole=cluster_role_arn,\n        VisibleToAllUsers=True,\n        ServiceRole=\"arn:aws:iam::123456789012:role/EMR-SA-GIE\",\n        Eks={\n            \"ClusterName\": eks_cluster_name,\n            \"FargateProfile\": {\n                \"Configuration\": {\n                    \"Compute\": \"ENI\",\n                    \"FargateProfileVersion\": 1,\n                    \"RmiArn\": f\"arn:aws:rme:us-east-1:{ boto3.client(\"emr\").get_account_id () }:task/{ fargate_task_def_name }\",\n                    \"RoleArn\": eks_task_role_arn,\n                },\n            },\n        },\n    )\n\n\n# [END howto_operator_emr_create_job_flow]\n\n# [START howto_sensor_emr_job_flow]\ndef _job_flow_sensor(current_state):\n    if current_state == \"RUNNING\":\n        raise Exception(\"Job flow got stuck in running state\")\n\n\n# [END howto_sensor_emr_job_flow]\n\n# [START howto_sensor_emr_step]\ndef _step_sensor(current_state):\n    if current_state == \"RUNNING\":\n        raise Exception(\"Step got stuck in running state\")\n\n\n# [END howto_sensor_emr_step]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    on_finish=print,\n) as dag:\n    create_eks_cluster = EksHook(\n        task_id=\"create_eks_cluster\",\n        cluster_name=\"example-eks-cluster\",\n        cluster_role_arn=\"arn:aws:iam::123456789012:role/EMR-CodeCommit-Service-Role\",\n        resources_vpc_config={\n            \"AssignPublicIpRequirement\": \"ENABLED\",\n            \"Subnets\": [\"192.168.0.0/19\"],\n            \"EndpointPublicAccessEnabled\": True,\n            \"EndpointPrivateAccessEnabled\": True,\n        },\n    )\n\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\",\n        bucket_name=\"example-eks-bucket-{{ ds }}\",\n    )\n\n    upload_file = S3CreateObjectOperator(\n        task_id=\"upload_file\",\n        s3_bucket=\"example-eks-bucket-{{ ds }}\",\n        s3_key=\"example-python-package.tar.gz\",\n        data=\"example-python-package.tar.gz\",\n    )\n\n    create_job_flow_from_eks = _create_job_flow_from_eks(\n        \"example-eks-job-flow-from-eks\",\n        \"arn:aws:iam::123456789012:role/EMR-CodeCommit-Service-Role\",\n        \"s3://example-eks-bucket-{{ ds }}/example-python-package.tar.gz\",\n        \"example-eks-cluster\",  # Cluster Name\n        \"arn:aws:iam::123456789012:role/EMR-CodeCommit-Service-Role\",  # Cluster Role ARN\n        fargate_task_def_name=\"emr-eks-advanced-task-definition-20230516-1515\",  # Task Definition Name\n    )\n\n    # [START howto_operator_emr_create_job_flow]\n    create_job_flow = _create_job_flow_from_eks(\n        \"example-eks-job-flow\", \"arn:aws:iam::123456789012:role/EMR-CodeCommit-Service-Role\", \"s3://example-eks-bucket-{{ ds }}/example-python-package.tar.gz\", \"example-eks-cluster\", \"arn:aws:iam::123456789012:role/EMR-CodeCommit-Service-Role\", \"example-eks-cluster\"\n    )\n    # [END howto_operator_emr_create_job_flow]\n\n    # [START howto_sensor_emr_job_flow]\n    check_job_flow_state = EmrJobFlowSensor(\n        task_id=\"check_job_flow_state\",\n        job_flow_id=\"{{ task_instance.xcom_pull(task_ids='create_job_flow', key='job_flow_id') }}\","}], "metadata": {"file_name": "dag_390_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nENV_ID = os.environ.get(env_id_key=ENV_ID_KEY)\nDAG_ID = \"quick_sight_ingestion\"\n\n# Externally fetched variables:\nSAMPLE_DATA_BUCKET = os.environ.get(\"SAMPLE_DATA_BUCKET\")\nSAMPLE_DATA_PATH = os.environ.get(\"SAMPLE_DATA_PATH\")\n\nSAMPLE_DATA_FILE = \"sample-project\"\nDATASET_NAME = \"SampleProjectDataset\"\nINGESTION_NAME = \"SampleProjectIngestion\"\n\nsys_test_context_task = SystemTestContextBuilder().add_env_var(env_id_key=ENV_ID_KEY).build()\n\n# The URI of a sample json file containing sample data.\nSAMPLE_DATA_FILE_URI = f\"s3://{SAMPLE_DATA_BUCKET}/{SAMPLE_DATA_PATH}/json/{SAMPLE_DATA_FILE}.json\"\n\n\n@task\ndef create_connection(conn_id_name: str, conn_params: dict):\n    boto3.client(\"awsisv3connregistry\").create_connection(\n        connection_id=conn_id_name,\n        connection_params=json.dumps(conn_params),\n    )\n\n\n@task\ndef update_connection(conn_id_name, conn_params):\n    boto3.client(\"awsisv3connregistry\").update_connection(\n        connection_id=conn_id_name,\n        connection_params=json.dumps(conn_params),\n    )\n\n\n@task\ndef delete_connection(conn_id_name: str):\n    boto3.client(\"awsisv3connregistry\").delete_connection(\n        connection_id=conn_id_name,\n    )\n\n\n@task\ndef create_dataset(dataset_name: str, bucket: str, prefix: str):\n    hook = QuickSightHook()\n    response = hook.create_dataset(\n        DatasetId=None,\n        DatasetName=dataset_name,\n        Bucket=bucket,\n        Prefix=prefix,\n    )\n    return response[\"Dataset\"][\"Id\"]\n\n\n@task\ndef get_connection_id():\n    return \"aws_datawarehouse\"\n\n\n@task\ndef set_up_connection(conn_id_name, conn, *args, **kwargs):\n    conn_id = get_connection_id()\n    new_conn = Connection(\n        conn_id=conn_id,\n        conn_type=\"s3\",\n        description=\"S3 connection for QuickSight ingestion\",\n        host=conn[\"s3_bucket\"],\n        log_chunk_size=1048576,\n        replace=True,\n    )\n    dag_instance = kwargs[\"task_instance\"]\n    dag_instance.xcom_push(task_id=\"return_connection\", value=new_conn)\n    dag_instance.connect(getattr(args, \"task_name\"), new_conn)\n\n\n@task\ndef get_aki():\n    return boto3.client(\"quicksight\").get_aki()[\"akiId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(ingestion_name: str):\n    hook = QuickSightHook()\n    log.info(f\"Deleting ingestion {ingestion_name}...\")\n    hook.delete_ingestion(IngestionId=ingestion_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dataset(dataset_name: str):\n    hook = QuickSightHook()\n    log.info(f\"Deleting dataset {dataset_name}...\")\n    hook.delete_dataset(DatasetName=dataset_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    conn_id = f\"{env_id}-quicksight-connection\"\n    bucket = f\"{env_id}-bucket\"\n    prefix = f\"{env_id}/prefix\"\n    aki = get_aki()\n\n    create_connection(conn_id, {\"bucket\": SAMPLE_DATA_BUCKET, \"prefix\": SAMPLE_DATA_PATH})\n    set_up_connection(conn_id, {\"bucket\": SAMPLE_DATA_BUCKET, \"prefix\": SAMPLE_DATA_PATH})\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightCreateIngestionOperator(\n        task_id=\"create_ingestion\",\n        dataset_id=None,\n        bucket=bucket,\n        prefix=prefix,\n        aki=aki,\n        ingestion_name=INGESTION_NAME,\n    )\n    # [END howto_operator_quicksight_create_ingestion]\n\n    test_context | create_connection | set_up_connection\n\n    create_ingestion\n\n    # [START howto_sensor_quicksight_ingestion]\n    wait_for_ingestion = QuickSightSensor(\n        task_id=\"wait_for_ingestion\",\n        ingestion_id=\"{{ task_instance.xcom_pull('create_ingestion', key='aki') }}\",\n    )\n    # [END howto_sensor_quicksight_ingestion]\n\n    wait_for_ingestion\n\n    # [START howto_operator_quicksight_delete_ingestion]\n    delete_ingestion(ingestion_name=INGESTION_NAME)\n    # [END howto_operator_quicksight_delete_ingestion]\n\n    delete_ingestion(ingestion_name=INGESTION_NAME)\n\n    # [START howto_operator_quicksight_delete_dataset]\n    delete_dataset(dataset_name=DATASET_NAME)\n    # [END howto_operator_quicksight_delete_dataset]\n\n    delete_dataset(dataset_name=DATASET_NAME)\n\n    dag"}], "metadata": {"file_name": "dag_1112_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates basic Pig script execution in a local Hadoop environment. Create a workflow that lists directory contents as a simple example of system interaction.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.hadoop.operators.hadoop import HadoopExecutePyClassicalOperator\n\n\n@task\ndef create_temp_dir():\n    \"\"\"\n    Example task to create temp directory which will be deleted when task will be cancelled.\n    \"\"\"\n    os.system(\"sudo hadoop fs -mkdir -p /tmp/\")\n    os.system(\"sudo hadoop fs -chown -p hdfs:hdfs  /tmp\")\n\n\n@task\ndef create_input_file():\n    \"\"\"\n    Example task to create input file for Pig.\n    \"\"\"\n    os.system(\"sudo hadoop fs -put pig_input.txt /tmp/\")\n\n\ndef _get_python_classical_command(output_dir, jar, pig_class):\n    return f\"\"\"\n        import os\n        os.environ[\"HADOOP_LIBCLASSPATH\"] = '{jar}'\n        os.environ[\"HADOOP_USER_NAME\"] = 'hdfs'\n\n        input = '/tmp/pig_input.txt'\n        output = '{output_dir}'\n        birds = 'cats, dogs, birds'\n        {pig_class}(input, output, birds)\n        \"\"\"\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_hadoop_pig\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"owner\": \"datascience\", \"retries\": 1},\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    temp_dir_create = create_temp_dir()\n    input_file_create = create_input_file()\n\n    # [START howto_operator_hadoop_pig]\n    pig_classical_task = HadoopExecutePyClassicalOperator(\n        task_id=\"pig_classical_task\",\n        jar=\"hadoop-lib/hadoop-common-2.7.0.jar\",\n        input_files=[\"pig_input.txt\"],\n        python_class=_get_python_classical_command(\"/tmp/out\", \"hadoop-lib\", \"RunPigClassical\"),\n        hdfs_tmp_dir=\"/tmp\",\n        hdfs_output_dir=\"/tmp/out\",\n        log_dir=\"tmp\",\n    )\n    # [END howto_operator_hadoop_pig]\n\n    (temp_dir_create >> input_file_create >> pig_classical_task)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the temp file\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_110_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a diagnostic DAG that performs systematic database introspection by executing sequential SQL queries to discover and catalog data resources in an Alibaba AnalyticDB Spark cluster. Develop a workflow that provides visibility into the available data schemas and table structures.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.alibaba.cloud.operators.analyticdb_spark import AlipayDbSparkOperator\n\nBUCKET = os.environ.get(\"BUCKET\")\nDB_ID = os.environ.get(\"DB_ID\")\nDB_NAME = os.environ.get(\"DB_NAME\")\nTABLE_NAME = os.environ.get(\"TABLE_NAME\")\nSQL = os.environ.get(\"SQL\")\n\n\nwith DAG(\n    \"example_alipay_db_spark\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_alipay_db_spark]\n    alipay_db_spark = AlipayDbSparkOperator(\n        task_id=\"alipay_db_spark\",\n        sql=SQL,\n        db_id=DB_ID,\n        db_name=DB_NAME,\n        bucket=BUCKET,\n        table_name=TABLE_NAME,\n        trigger=\"all\",\n    )\n    # [END howto_operator_alipay_db_spark]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1068_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python function to plot a normal distribution using matplotlib? Certainly! Below is a Python code snippet that generates a plot of a normal distribution using the `matplotlib` and `numpy` libraries. This ensures compatibility and covers the necessary aspects of plotting such a distribution, including setting labels and a title for better understanding of the plot.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a function to plot a normal distribution\ndef plot_normal_distribution():\n    \"\"\"\n    This function plots a normal distribution using matplotlib.\n    \"\"\"\n    # Define the range for the x-axis\n    x = np.linspace(-5, 5, 1000)\n    \n    # Calculate the y-values for the normal distribution\n    y = 1 / (np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)\n    \n    # Plot the normal distribution\n    plt.plot(x, y)\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    plt.show()\n\n# Function to check the correctness of the plot_normal_distribution function\ndef check_plot_normal_distribution():\n    \"\"\"\n    This function checks the correctness of the plot_normal_distribution function by calling it and verifying the output.\n    \"\"\"\n    plot_normal_distribution()\n    print(\"The function executed and plotted a normal distribution successfully.\")\n\n# Execute the check function\ncheck_plot_normal_distribution()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complex cloud infrastructure setup, including creating a Compute Engine instance, configuring a PostgreSQL database, and transferring data to Google Sheets. Showcase a multi-step cloud data workflow with infrastructure provisioning and data migration.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.compute_v1 import ComputeEngineAPI\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.sheets import GoogleCloudSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_gcs import MssqlToGCSOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_mssql import S3ToMSSQLOperator\nfrom airflow.providers.google.sheets.transfers.mssql_to_gcs import MssqlToSheetsOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_compute_mssql_to_sheets\"\n\nREGION = \"europe-west1\"\nZONE = REGION + \"-a\"\nNETWORK = \"cloud-samples-network\"\nMSSQL_INSTANCE_NAME = f\"{REGION}-a-mssql-instance\"\nSHEET_ID = \"test-sheets-id\"\n\nDB_NAME = \"test_db\"\nDB_PORT = 1433\nMSSQL_BODY = {\"name\": MSSQL_INSTANCE_NAME, \"device r\u00f4le\": \"BASIC\"}\nSHEET_BODY = {\n    \"properties\": {\n        \"title\": \"Test Sheets\",\n        \"sheets\": [{\"properties\": {\"title\": \"1\"}}],\n    }\n}\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.csv\"\nFILE_SHEETS = \"file_sheets.json\"\nFILE_SHEETS_URL = f\"gs://{BUCKET_NAME}/{FILE_SHEETS}\"\nSOURCE_ACOUSTIC_DATASET = \"sqlserver\"\nSOURCE_TABLE_NAME = \"tpch.sf_sales_person\"\nDESTINATION_SHEET_ID = \"test-sheets-id\"\nSHEETS_CONNECTION = Connection(\n    conn_id=\"sheets\",\n    schema=\"google_cloud_sheets\",\n    conn_type=\"google_cloud_sheets\",\n    extra={\"scope\": \"https://www.googleapis.com/auth/spreadsheets,https://www.googleapis.com/auth/compute\"},\n    login=\"user\",\n    password=\"password\",\n)\nPROJECT_ID_FOR_GCS = \"test-project-id\"\nSOURCE_ENDPOINT = f\"mysql://{DB_NAME}-{DB_PORT}-{PROJECT_ID_FOR_GCS}:{DB_PORT}/{DB_NAME}\"\n\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"compute\", \"mssql\", \"sheets\"],\n) as dag:\n    @task\n    def create_destination_sheets_connection():\n        \"\"\"\n        Example task to create a Sheets connection.\n        \"\"\"\n        from airflow.providers.google.sheets.transfers.mssql_to_gcs import MssqlToSheetsOperator\n\n        MssqlToSheetsOperator(\n            task_id=\"create_destination_sheets_connection\",\n            sheet_id=DESTINATION_SHEET_ID,\n            spreadsheet_json={\"properties\": {\"title\": DESTINATION_SHEET_ID}},\n            gcp_conn_id=SHEETS_CONNECTION.id,\n        )\n\n    create_sheets_connection = create_destination_sheets_connection()\n\n    @task\n    def set_up_connection(compute_conn):\n        \"\"\"\n        Example function to create a instance address.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.compute import (\n            ComputeEngineDeleteInstanceAddressOperator,\n            ComputeEngineInsertInstanceAddressOperator,\n        )\n\n        from airflow.models import Connection\n        from airflow.utils import db\n\n        addresses = [\n            \"global/address/{address}\".format(address=\"address-1\"),\n        ]\n        address_spec = {\n            \"global\": {\n                \"network\": \"global\",\n                \"range\": \"8.0.0.0/25\",\n            }\n        }\n\n        compute_conn = compute_conn or ComputeEngineAPI()\n        insert_address = ComputeEngineInsertInstanceAddressOperator(\n            task_id=\"insert_address\", project_id=PROJECT_ID, body=address_spec, zone=ZONE\n        )\n        delete_address = ComputeEngineDeleteInstanceAddressOperator(\n            task_id=\"delete_address\", project_id=PROJECT_ID, address=\"address-1\", zone=ZONE\n        )\n        db.merge_connection(SHEETS_CONNECTION)\n\n        for address in addresses:\n            DeleteInstanceAddressOperator(\n                task_id=\"delete_address_\" + address, project_id=PROJECT_ID, address=address\n            )\n\n        DeleteInstanceAddressOperator(task_id=\"delete_address_all\", project_id=PROJECT_ID, address=\"address-1\")\n\n    create_address = set_up_connection(ComputeEngineHook(project_id=PROJECT_ID))\n\n    @task\n    def create_sheets_connection_external():\n        \"\"\"\n        Example task to create a Sheets connection external.\n        \"\"\"\n        from airflow.providers.google.sheets.transfers.mssql_to_gcs import MssqlToSheetsOperator\n\n        MssqlToSheetsOperator(\n            task_id=\"create_destination_sheets_connection\",\n            sheet_id=DESTINATION_SHEET_ID,\n            spreadsheet_json={\"properties\": {\"title\": DESTINATION_SHEET_ID}},\n            gcp_conn_id=SHEETS_CONNECTION.id,\n        )\n\n    create_external_sheets_connection = create_sheets_connection_external()\n\n    @task\n    def create_network_interface():\n        \"\"\"\n        Example function to create a network interface.\n        \"\"\"\n        from airflow.models import Connection\n        from airflow.utils import db\n\n        connections = [\n            Connection(\n                conn_id=\"network_interface\",\n                conn_type=\"google_cloud_compute\",\n                extra={\n                    \"project\": PROJECT_ID,\n                    \"network\": \"default\",\n                    \"machine_type\": \"n1-standard-1\",\n                    \"disks\": [\n                        {\n                            \"boot\": True,\n                            \"device_name\": \"disk-1\",\n                            \"initialize_params\": {\n                                \"disk_size_gb\": 10,\n                                \"disk_name\": \"disk-1\",\n                                \"source_image_uri\": \"projects/debian-cloud/global/addresses/global-addresses/address-1\",\n                            },\n                        }\n                    ],\n                    \"network_interfaces\": [\n                        {\n                            \"access_configs\": [\n                                {\n                                    \"name\": \"External NAT\",\n                                    \"network_tier\": \"PREMIUM\",\n                                }\n                            ],\n                            \"network\": \"global\",\n                            \"subnetwork\": \"global/networks/default\",\n                        }\n                    ],\n                },\n            )\n        ]\n        db.merge_connection(Connection(id=\"network_interface\", conn_type=\"google_cloud_compute\"))\n\n        CreateInstanceAddressOperator(\n            task_id=\"create_address\", project_id=PROJECT_ID, address=\"address-1\", zone=ZONE\n        )\n\n        CreateInstanceAddressOperator(\n            task_id=\"create_address_external\",\n            project_id=PROJECT_ID,\n            address=\"address-2\",\n            zone=ZONE,\n            is_global=True,\n        )\n\n        CreateInstanceAddressOperator(\n            task_id=\"create_address_all\",\n            project_id=PROJECT_ID,\n            address=\"address-3\",\n            zone=ZONE,\n        )\n\n    create_network_interface_request = create_network_interface()\n\n    @task\n    def create_instance(compute_conn, request_id):\n        \"\"\"\n        Example function to create a instance.\n        \"\"\"\n        from airflow.models import Connection\n        from airflow.utils import db\n\n        connections = [\n            Connection(\n                conn_id=\"instance\",\n                conn_type=\"google_cloud_compute\",\n                extra={\n                    \"project\": PROJECT_ID,\n                    \"zone\": \"europe-west1-d\",\n                    \"request_id\": request_id,\n                },\n            )\n        ]\n        db.merge_connection(Connection(id=\"instance\", conn_type=\"google_cloud_compute\"))\n\n        instance = {\n            \"name\": \"tuto-i-3\",\n            \"machine_type\": \"n1-standard-1\",\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"device_name\": \"disk-1\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": 10,\n                        \"disk_name\": \"disk-1\",\n                        \"source_image_uri\": \"projects/debian-cloud/global/addresses/address-1\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\n                            \"name\": \"External NAT\",\n                            \"network_tier\": \"PREMIUM\",\n                        }\n                    ],\n                    \"network\": \"global\",\n                    \"subnetwork\": \"global/networks/default\",\n                }\n            ],\n        }\n        compute_conn = ComputeEngineAPI()\n        compute_conn.insert(project_id=PROJECT_ID, body=instance, zone=ZONE, request_id=request_id)\n\n    create_instance_3 = create_instance(compute_conn=PROJECT_ID_FOR_GCS, request_id=\"tuto-i-3\")\n\n    @task\n    def create_instance(request_id):\n        \"\"\"\n        Example function to create a instance.\n        \"\"\"\n        from airflow.models import Connection\n        from airflow.utils import db\n\n        connections = [\n            Connection(\n                conn_id=\"instance\",\n                conn_type=\"google_cloud_compute\",\n                extra={\n                    \"project\": PROJECT_ID,\n                    \"zone\": \"europe-west1-d\",\n                    \"request_id\": request_id,\n                },\n            )\n        ]\n        db.merge_connection(Connection(id=\"instance\", conn_type=\"google_cloud_compute\"))\n\n        instance = {\n            \"name\": \"tuto-i-2\",\n            \"machine_type\": \"n1-standard-1\",\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"device_name\": \"disk-1\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": 10,\n                        \"disk_name\": \"disk-1\",\n                        \"source_image_uri\": \"projects/debian-cloud/global/addresses/address-2\",\n                    },\n                }\n            ],\n            \"network_interfaces\":"}], "metadata": {"file_name": "dag_910_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Compute Engine instance templates and instance group managers. Create a DAG that demonstrates how to create, copy, and update instance templates with different configurations in a cloud infrastructure environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineCopyInstanceTemplateOperator,\n    ComputeEngineDeleteInstanceGroupManagerOperator,\n    ComputeEngineDeleteInstanceGroupOperator,\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceGroupManagerOperator,\n    ComputeEngineInsertInstanceGroupOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineCopyInstanceTemplateOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_igm\"\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nNETWORK = \"default\"\n\nSHORT_MACHINE_TYPE = \"n1-standard-1\"\nLONG_MACHINE_TYPE = \"n1-standard-1\"\n\nGCE_INSTANCE_NAME = \"instance-igm-test\"\nGCE_INSTANCE_TEMPLATE_NAME = f\"igm-template-{DAG_ID}\"\nGCE_INSTANCE_GROUP_MANAGER_NAME = f\"instance-group-{DAG_ID}\"\nGCE_INSTANCE_GROUP_MANAGER_DESCRIPTION = \"Test description\"\nSHORT_MACHINE_TYPE_IIGM = \"n1-standard-1\"\nLONG_MACHINE_TYPE_IIGM = \"n1-standard-1\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_FILE = \"file.tar\"\n\nGCP_COMPUTE_CLIENT = \"compute\"\n\nDESCRIPTION = \"Test description\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute\", \"igm\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    move_file_to_bucket = GCSSynchronizeBucketsOperator(\n        task_id=\"move_file_to_bucket\",\n        source_bucket=BUCKET_NAME,\n        source_object=BUCKET_FILE,\n        destination_bucket=BUCKET_NAME,\n        destination_object=BUCKET_FILE,\n    )\n\n    # [START howto_operator_copy_igm]\n    copy_igm = ComputeEngineCopyInstanceTemplateOperator(\n        task_id=\"copy-igm\",\n        project_id=PROJECT_ID,\n        resource_id=GCE_INSTANCE_TEMPLATE_NAME,\n        body={\n            \"description\": DESCRIPTION,\n            \"name\": GCE_INSTANCE_TEMPLATE_NAME,\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\n                            \"name\": \"Project Network Default Access Config\",\n                            \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n                        }\n                    ],\n                    \"network_endpoint_group\": {},\n                    \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                }\n            ],\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": GCE_INSTANCE_NAME,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"disk_name\": GCE_INSTANCE_NAME,\n                        \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [{\"name\": \"Project Network Default Access Config\", \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\"}],\n                    \"network\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                    \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                    \"machine_spec\": {\n                        \"disks\": [\n                            {\n                                \"auto_delete\": True,\n                                \"boot\": True,\n                                \"device_name\": GCE_INSTANCE_NAME,\n                                \"initialize_params\": {\n                                    \"disk_size_gb\": \"10\",\n                                    \"disk_name\": f\"{GCE_INSTANCE_NAME}-disk\",\n                                    \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                                },\n                            }\n                        ],\n                        \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE}\",\n                        \"disks\": [\n                            {\n                                \"auto_delete\": True,\n                                \"boot\": True,\n                                \"device_name\": GCE_INSTANCE_NAME,\n                                \"initialize_params\": {\n                                    \"disk_size_gb\": \"10\",\n                                    \"disk_name\": f\"{GCE_INSTANCE_NAME}-disk\",\n                                    \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                                },\n                            }\n                        ],\n                        \"network_interfaces\": [\n                            {\n                                \"access_configs\": [\n                                    {\n                                        \"name\": \"Project Network Default Access Config\",\n                                        \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n                                    }\n                                ],\n                                \"network\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                                \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                            }\n                        ],\n                    },\n                }\n            ],\n        },\n    )\n    # [END howto_operator_copy_igm]\n\n    # [START howto_operator_create_igm_manager]\n    create_igm_manager = ComputeEngineInsertInstanceGroupManagerOperator(\n        task_id=\"create-igm-manager\",\n        project_id=PROJECT_ID,\n        resource_id=GCE_INSTANCE_GROUP_MANAGER_NAME,\n        body={\n            \"description\": GCE_INSTANCE_GROUP_MANAGER_DESCRIPTION,\n            \"base_instance_name\": GCE_INSTANCE_NAME,\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE}\",\n            \"size\": 1,\n        },\n    )\n    # [END howto_operator_create_igm_manager]\n\n    # [START howto_operator_delete_igm_manager]\n    delete_igm_manager = ComputeEngineDeleteInstanceGroupManagerOperator(\n        task_id=\"delete-igm-manager\",\n        project_id=PROJECT_ID,\n        resource_id=GCE_INSTANCE_GROUP_MANAGER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_delete_igm_manager]\n\n    # [START howto_operator_delete_igm]\n    delete_igm = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete-igm\", project_id=PROJECT_ID, resource_id=GCE_INSTANCE_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_delete_igm]\n\n    # [START howto_operator_copy_igm_to_igm]\n    copy_igm_to_igm = ComputeEngineCopyInstanceTemplateOperator(\n        task_id=\"copy-igm-to-igm\",\n        project_id=PROJECT_ID,\n        resource_id=GCE_INSTANCE_TEMPLATE_NAME,\n        body={\n            \"description\": DESCRIPTION,\n            \"name\": GCE_INSTANCE_TEMPLATE_NAME,\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\n                            \"name\": \"Project Network Default Access Config\",\n                            \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n                        }\n                    ],\n                    \"network_endpoint_group\": {},\n                    \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                }\n            ],\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": GCE_INSTANCE_NAME,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"disk_name\": GCE_INSTANCE_NAME,\n                        \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\n                            \"name\": \"Project Network Default Access Config\",\n                            \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n                        }\n                    ],\n                    \"network\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                    \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                    \"machine_spec\": {\n                        \"disks\": [\n                            {\n                                \"auto_delete\": True,\n                                \"boot\": True,\n                                \"device_name\": GCE_INSTANCE_NAME,\n                                \"initialize_params\": {\n                                    \"disk_size_gb\": \"10\",\n                                    \"disk_name\": f\"{GCE_INSTANCE_NAME}-disk\",\n                                    \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                                },\n                            }\n                        ],\n                        \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE}\",\n                        \"disks\": [\n                            {\n                                \"auto_delete\": True,\n                                \"boot\": True,\n                                \"device_name\": GCE_INSTANCE_NAME,\n                                \"initialize_params\": {\n                                    \"disk_size_gb\": \"10\",\n                                    \"disk_name\": f\"{GCE_INSTANCE_NAME}-disk\",\n                                    \"source_image_uri\": f\"projects/google-compute-engine/templates/{SHORT_MACHINE_TYPE}\",\n                                },\n                            }\n                        ],\n                        \"network_interfaces\": [\n                            {\n                                \"access_configs\": [\n                                    {\n                                        \"name\": \"Project Network Default Access Config\",\n                                        \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n                                    }\n                                ],\n                                \"network\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                                \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{NETWORK}\",\n                            }\n                        ],\n                    },\n                }\n            ],\n        },\n    )\n    # [END howto_operator_copy_igm_to_igm]\n\n    # [START howto_operator_delete_igm]\n    delete_igm = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete-igm\", project_id=PROJECT"}], "metadata": {"file_name": "dag_1204_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Campaign Manager reporting and conversion tracking workflow that retrieves digital marketing performance metrics and logs conversion events. Create a system that can dynamically pull campaign data, generate reports, and record conversion activities using Campaign Manager 360 APIs.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.google.marketing_platform.operators.report import GoogleMarketingPlatformRunReportOperator\nfrom airflow.providers.google.marketing_platform.sensors.report import GoogleMarketingPlatformReportSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GMP_PROJECT_ID\", \"default\")\n\nDAG_ID = \"example_marketing_platform\"\n\nGMP_CONNECTION = f\"{PROJECT_ID}-mp-context\"\nGMP_ADVERTISER_ID = os.environ.get(\"MARKETING_PLATFORM_ADVERTISER_ID\", 123)\nGMP_REPORT_NAME = \"report_123\"\nGMP_REPORT_SUGGESTION_NAME = f\"recommendations_{GMP_REPORT_NAME}\"\nGMP_CONNECTION_SUGGEST_NAME = f\"{GMP_CONNECTION}-suggest\"\nGMP_ADVERTISER_ID_SUGGEST = os.environ.get(\"MARKETING_PLATFORM_ADVERTISER_ID_SUGGEST\", 456\")\nGMP_REPORT_SUGGEST_PATH = f\"reports/{GMP_REPORT_SUGGEST_NAME}.xml\"\nGMP_REPORT_LIST_PATH = \"reports/list.xml\"\nGMP_REPORT_SUGGEST_PATH_SUG = f\"reports/{GMP_REPORT_SUGGEST_NAME}.xml\"\nGMP_REPORT_SENSOR = \"reports.get\"\nGMP_REPORT_SENSOR_WAIT = \"reports.get\"\nGMP_ROLE = f\"{PROJECT_ID}-mp-role\"\nGMP_ADVERTISER_ID_ROLE = os.environ.get(\"MARKETING_PLATFORM_ADVERTISER_ID_ROLE\", \"123\")\nGMP_ADVERTISER_ID_ROLE_SUG = os.environ.get(\"MARKETING_PLATFORM_ADVERTISER_ID_ROLE_SUG\", \"456\")\nGMP_PERMISSIONS = [\n    {\n        \"type\": \"PROJECT\",\n        \"role\": \"IAM_ID_SCOPE\",\n    },\n    {\n        \"type\": \"ADVERTISER\",\n        \"project\": PROJECT_ID,\n        \"role\": \"MARKETING_PLATFORM_ADVERTISER_ID\",\n        \"value\": GMP_ADVERTISER_ID,\n    },\n    {\n        \"type\": \"ADVERTISER\",\n        \"project\": PROJECT_ID,\n        \"role\": \"MARKETING_PLATFORM_ADVERTISER_ID_SUGGEST\",\n        \"value\": GMP_ADVERTISER_ID_SUGGEST,\n    },\n]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_advertiser_connection():\n        conn = Connection(\n            conn_id=GMP_CONNECTION,\n            conn_type=\"google_market_platform\",\n        )\n        conn_extra = {\n            \"advertiserId\": GMP_ADVERTISER_ID,\n            \"firebaseProject\": PROJECT_ID,\n        }\n        conn.add_extra_params(param_objects=Param(**conn_extra))\n\n        role = Role(\n            role_id=GMP_ROLE,\n            prj_id=PROJECT_ID,\n        )\n        role.add_permission(\"ADVERTISER\")\n        role.add_permission(\"PROJECT\")\n\n        role_for_suggest = Role(\n            role_id=GMP_ROLE,\n            prj_id=PROJECT_ID,\n        )\n        role_for_suggest.add_permission(\"ADVERTISER\")\n        role_for_suggest.add_permission(\"PROJECT\")\n\n        bash_command = BashOperator(\n            task_id=\"create_advertiser_connection\",\n            bash_command=f\"python -m airflow.providers.google.marketing_platform.operators.connection_create {GMP_CONNECTION} {GMP_ADVERTISER_ID} {PROJECT_ID}\",\n        )\n        bash_command_for_suggest = BashOperator(\n            task_id=\"create_advertiser_connection_suggest\",\n            bash_command=f\"python -m airflow.providers.google.marketing_platform.operators.connection_create {GMP_CONNECTION_SUGGEST} {GMP_ADVERTISER_ID_SUGGEST} {PROJECT_ID}\",\n        )\n\n        [conn, role, bash_command_for_suggest] >> role_for_suggest >> [role_for_suggest, trigger]\n\n    @task\n    def create_report_suggestion():\n        report_suggestion = GoogleMarketingPlatformRunReportOperator(\n            task_id=\"create_report_suggestion\",\n            gmp_conn_id=GMP_CONNECTION,\n            report_name=GMP_REPORT_SUGGEST_NAME,\n            include_deleted=False,\n            include_pii=False,\n            load_to_storage=False,\n        )\n\n        trigger = Trigger.trigger(\n            task_id=\"my_trigger\",\n            python_callable=lambda: print(\"Goodbye, world!\"),\n            task_params={\"report_suggestion_id\": report_suggestion.output},\n        )\n\n        [report_suggestion, trigger] >> task\n\n    @task\n    def create_reports():\n        report = GoogleMarketingPlatformRunReportOperator(\n            task_id=\"create_reports\",\n            gmp_conn_id=GMP_CONNECTION,\n            report_name=GMP_REPORT_NAME,\n            include_deleted=False,\n            include_pii=False,\n            load_to_storage=False,\n        )\n\n        trigger = Trigger.trigger(\n            task_id=\"my_trigger\",\n            python_callable=lambda: print(\"Goodbye, world!\"),\n            task_params={\"report_id\": report.output},\n        )\n\n        [report, trigger] >> task\n\n    @task\n    def create_reports_list():\n        reports_list = GoogleMarketingPlatformReportSensor(\n            task_id=\"create_reports_list\",\n            gmp_conn_id=GMP_CONNECTION,\n            name=GMP_REPORT_LIST_PATH,\n            deferrable=False,\n        )\n\n        [reports_list] >> task\n\n    @task\n    def delete_reports_list():\n        BashOperator(\n            task_id=\"delete_reports_list\",\n            bash_command=f\"python -m airflow.providers.google.marketing_platform.operators.report_delete {GMP_REPORT_LIST_PATH}\",\n        ) >> task\n\n    @task\n    def delete_report_suggestion():\n        BashOperator(\n            task_id=\"delete_report_suggestion\",\n            bash_command=f\"python -m airflow.providers.google.marketing_platform.operators.report_delete {GMP_REPORT_SUGGEST_NAME}\",\n        )\n\n    @task\n    def delete_advertiser_connection():\n        BashOperator(\n            task_id=\"delete_advertiser_connection\",\n            bash_command=f\"python -m airflow.providers.google.marketing_platform.operators.connection_delete {GMP_CONNECTION}\",\n        )\n\n    create_advertiser_connection() >> create_report_suggestion() >> create_reports() >> create_reports_list() >> delete_reports_list() >> delete_report_suggestion() >> delete_advertiser_connection() >> task"}], "metadata": {"file_name": "dag_1733_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that extracts CSV files from a specified Google Cloud Storage bucket and loads them into a predefined Trino table, supporting efficient and reproducible data ingestion processes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.providers.trino.operators.trino import TrinoRunOperator\nfrom airflow.providers.trino.sensors.trino import TrinoRunSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino\"\n\nconn_id = \"trino_default\"\n\nsh_command = \"\"\"\nsudo -E\necho 'create file '$(date | cut -c1-10).txt;' > /dev/null 2>&1 || true;\necho 'echo -e \\\"Name\\tAge\\\\tedgeId\\\\tSex\\\\tAddress\\\\tPhoneNum\\\\tDtOfBirth\\\\tDtOfJoining\\\\tDtOfPromotion\\\\tDtOfRetirement\\\\tSalary\\\\tDesignation\\\\tDepartment\\\\tCity\\\\tState\\\\tCountry\\\\tDtOfMarriage\\\\tDtOfDeath\\\\tDtOfMarriageEdgeId\\\\tDtOfDeathEdgeId\\\\tMaritalStatus\\\\tPhoneType\\\\tMobileNumber\\\\tLandlineNumber\\\\tEmailId\\\\tIsEmailConfirmed\\\\tIsPhoneConfirmed\\\\tDtOfNextSalaryIncrement\\\\tAmountOfNextSalaryIncrement\\\\tDtOfNextBonus\\\\tAmountOfNextBonus;'\nwhile IFS=$'\\t' read -r name age edge_id sex address phone_num dt_of_birth dt_of_joining dt_of_promotion dt_of_retirement salary designation department city state country dt_of_marriage dt_of_death dt_of_marriage_edge_id dt_of_death_edge_id marital_status phone_type mobile_number landline_number email_id is_email_confirmed dt_of_next_salary_increment amount_of_next_salary_increment dt_of_next_bonus amount_of_next_bonus;\ndo\n    echo -e \"${name}\\t${age}\\t${edge_id}\\t${sex}\\t${address}\\t${phone_num}\\t${dt_of_birth}\\t${dt_of_joining}\\t${dt_of_promotion}\\t${dt_of_retirement}\\t${salary}\\t${designation}\\t${department}\\t${city}\\t${state}\\t${country}\\t${dt_of_marriage}\\t${dt_of_death}\\t${dt_of_marriage_edge_id}\\t${dt_of_death_edge_id}\\t${marital_status}\\t${phone_type}\\t${mobile_number}\\t${landline_number}\\t${email_id}\\t${is_email_confirmed}\\t${is_phone_confirmed}\\t${dt_of_next_salary_increment}\\t${amount_of_next_salary_increment}\\t${dt_of_next_bonus}\\t${amount_of_next_bonus};\ndone < '$(date | cut -c1-10).txt'\n\"\"\"\n\n\n@task\ndef create_postgres_connection():\n    # This connection id should exist in the environment and be defined in the test context\n    connection = Connection(\n        conn_id=conn_id,\n        conn_type=\"trino\",\n        host=\"localhost\",\n        login=\"root\",\n        password=\"password\",\n        port=7474,\n    )\n    db_conn = TrinoHook()\n    db_conn.create_connection(connection)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    generate_shell_script = BashOperator(\n        task_id=\"generate_shell_script\",\n        bash_command=sh_command,\n    )\n\n    # [START howto_operator_trino_run]\n    run_query = TrinoRunOperator(task_id=\"run_query\", sql=\"empty.sql\", conn_id=conn_id)\n    # [END howto_operator_trino_run]\n\n    # [START howto_sensor_trino_run]\n    check_run = TrinoRunSensor(task_id=\"check_run\", sql=\"empty.sql\", conn_id=conn_id)\n    # [END howto_sensor_trino_run]\n\n    @task\n    def remove_postgres_connection():\n        # This connection id should exist in the environment and be defined in the test context\n        TrinoHook().delete_connection(conn_id)\n\n    remove_postgres_connection()\n    generate_shell_script >> run_query >> check_run >> remove_postgres_connection()"}], "metadata": {"file_name": "dag_1415_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates a complete machine learning model lifecycle, including provisioning AWS resources, configuring SageMaker training jobs, creating model artifacts, and setting up a scalable inference endpoint. The workflow should demonstrate end-to-end machine learning infrastructure management with robust error handling and resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateModelOperator,\n    SageMakerDeleteModelOperator,\n    SageMakerModelOperator,\n    SageMakerStartTrainingJobOperator,\n    SageMakerStopTrainingJobOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import SageMakerMonitoringSensor, SageMakerTrainingJobSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"sagemaker_training_job\"\nsys_test_context_task = SystemTestContextBuilder().add_env_var(\n    ENV_ID_KEY, \"TEST_ENV_ID\"\n).build()\n\n# Repository and model are reserved for the system test and users need to have a working S3 bucket\n# and need to have the repository and model available online.\ndef _get_bucket_and_key(training_job_name):\n    return (\n        f\"{S3Hook().conn.create_bucket(Bucket=training_job_name)/key}\",\n        f\"{training_job_name}/\",\n    )\n\n\n# Create a training job and a model for the system test to clean up\ndef _create_training_job_and_model(env_id, bucket, key):\n    model_data = json.dumps({\"S3DataUrl\": f\"{bucket}/{key}/})\n    script_data = json.dumps({\"S3ScriptUrl\": f\"{bucket}/{key}/train_script.py\"})\n\n    # Repository and model are reserved for the system test and users need to have a working S3 bucket\n    # and have the repository and model available online.\n    cmd = f\"\"\"\n        python3.7 -m boto3.s3.get_object --bucket {bucket} --key model.tar --response-contenttype application/octet-stream |\n        python3.7 -m sm --repository local --model {model_data} --script {script_data} --handler train --role sm-training-pi-role\n        \"\"\"\n    log.info(\"Starting training job, cmd: %s\", cmd)\n    training_job_name = f\"{env_id}-sm-training-pi-job\"\n    subprocess.Popen(cmd, shell=True, stdout=BashOperator(\n        task_id=\"start_training_job_python37\", bash_command=\"\"\n    ).bash_command)\n\n    # Wait until training job creation is complete\n    log.info(\"Waiting for training job %s to reach StatusCompleted\", training_job_name)\n    training_job_arn = BashOperator(\n        task_id=\"training_job_arn\", bash_command=f\"aws sm describe-training-job --training-job-name {training_job_name} --query 'TrainingJobDescriptions[0].TrainingJobArn' --output text\"\n    ).xcom_pull(task_ids=[\"training_job_arn\"], key=training_job_name)\n\n    return training_job_arn\n\n\ndef _get_endpoint_config(endpoint_name, instance_type, deploy_count):\n    return {\n        \"InstanceTypeConfig\": {\"InstanceTypeSet\": [instance_type]},\n        \"DeploymentConfig\": {\"DeploymentType\": \"SPARK', 'MinItems\": deploy_count, \"MaxItems\": deploy_count},\n    }\n\n\ndef _create_endpoint_config_output(training_job_arn, instance_type, deploy_count):\n    return {\n        \"TrainingJobName\": training_job_arn,\n        \"InstanceType\": instance_type,\n        \"InitialDeploymentCount\": deploy_count,\n        \"InitialProcessingCapacity\": 0.1,\n    }\n\n\ndef _create_endpoint_config(endpoint_name, training_job_arn, instance_type, deploy_count):\n    model_id = BashOperator(\n        task_id=\"model_id\",\n        bash_command=f\"aws sm describe-models --query 'ModelDescriptions[?TrainingJobName=='{training_job_arn}'].&[?ModelId]' --filter FilterName='TrainingJobName',FilterValue='{training_job_arn}'\"\n    ).xcom_pull(task_ids=[\"model_id\"], key=training_job_arn)\n\n    # Repository and model are reserved for the system test and users need to have a working S3 bucket\n    # and have the model available online.\n    cmd = f\"\"\"\n        python3.7 -m boto3.s3.get_object --bucket {bucket} --key model.tar --response-contenttype application/octet-stream |\n        python3.7 -m sm --repository local --model {model_id} --export-model {endpoint_name} --instance-type {instance_type}\n        \"\"\"\n    log.info(\"Starting to create endpoint config, cmd: %s\", cmd)\n    endpoint_config_name = f\"{env_id}-sm-endpoint-config\"\n    subprocess.Popen(cmd, shell=True, stdout=BashOperator(\n        task_id=\"create_endpoint_config_output\", bash_command=\"\"\n    ).bash_command)\n\n\ndef _get_endpoint_config(endpoint_name):\n    return BashOperator(\n        task_id=\"get_endpoint_config\",\n        bash_command=f\"aws sm describe-endpoint-config --endpointConfigName {endpoint_name} --query 'EndpointConfigDescription.ModelConfig[?ModelId]' --filter FilterName='EndpointName',FilterValue='{endpoint_name}'\"\n    ).xcom_pull(task_ids=[\"get_endpoint_config\"], key=endpoint_name)\n\n\ndef _stop_training_job(training_job_arn):\n    SageMakerStopTrainingJobOperator(task_id=\"stop_training_job\", training_job_name=training_job_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    bucket = f\"{env_id}-sm-bucket\"\n    training_job_name = f\"{env_id}-sm-training-pi-job\"\n    model_name = f\"{env_id}-sm-model\"\n    export_model_name = f\"{env_id}-sm-export-model\"\n    endpoint_name = f\"{env_id}-sm-endpoint\"\n    instance_type = \"ml.m5.large\"\n    deploy_count = 1\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket)\n\n    @task\n    def create_model_with_export(bucket, model_id):\n        hook = S3Hook()\n        model_temp_file = hook.load_file(\n            filename=\"Pi.tar\", key=model_id, bucket_name=bucket, replace=True\n        )\n\n        # Repository and model are reserved for the system test and users need to have a working S3 bucket\n        # and have the model available online.\n        cmd = f\"\"\"\n            python3.7 -m boto3.s3.get_object --bucket {bucket} --key {model_id} --response-contenttype application/octet-stream |\n            python3.7 -m sm --repository local --model {model_temp_file} --export-model {export_model_name} --instance-type g3.2xlarge\n            \"\"\"\n        log.info(\"Starting to create model with export, cmd: %s\", cmd)\n        model_arn = BashOperator(\n            task_id=\"create_model_with_export_arn\", bash_command=\"\"\n        ).xcom_pull(task_ids=[\"create_model_with_export\"], key=model_id)\n\n        return model_arn\n\n    create_model_with_export_output = BashOperator(\n        task_id=\"create_model_with_export_output\", bash_command=\"\"\n    )\n\n    # [START howto_sagemaker_start_training_job_operator]\n    start_training_job = SageMakerTrainingJobOperator(\n        task_id=\"start_training_job\",\n        training_job_name=training_job_name,\n        hyperparameters={\n            \"train_config\": \"single_model\",\n            \"training_data\": f\"s3://{bucket}/model.tar\",\n            \"train_transform\": f\"s3://{bucket}/train_transform.py\",\n            \"output_path\": f\"s3://{bucket}/{export_model_name}\",\n            \"job_name\": training_job_name,\n            \"training_type\": \"Classification\",\n            \"training_data_split\": \"80-10-10-10\",\n            \"sample_size\": 1000,\n            \"num_classes\": 10,\n            \"estimator\": f\"--estimator python3.7 -m sm --repository local --model {model_id} --instance-type {instance_type}\",\n        },\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_sagemaker_start_training_job_operator]\n\n    # [START howto_sagemaker_training_job_sensor]\n    wait_for_training_job = SageMakerTrainingJobSensor(\n        task_id=\"wait_for_training_job\",\n        training_job_name=training_job_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_sagemaker_training_job_sensor]\n\n    # [START howto_sagemaker_stop_training_job_operator]\n    stop_training_job = SageMakerStopTrainingJobOperator(\n        task_id=\"stop_training_job\", training_job_name=training_job_name\n    )\n    # [END howto_sagemarker_stop_training_job_operator]\n\n    # [START howto_sagemaker_sensors_monitor_training_job]\n    monitor_training_job_begin = SageMakerTrainingJobSensor(\n        task_id=\"monitor_training_job_begin\",\n        training_job_name=training_job_name,\n        poke_interval=10,\n    )\n\n    monitor_training_job_end = SageMakerMonitoringSensor(\n        task_id=\"monitor_training_job_end\",\n        training_job_name=training_job_name,\n    )\n    # [END howto_sagemaker_sensors_monitor_training_job]\n\n    # [START howto_sagemaker_create_model_operator"}], "metadata": {"file_name": "dag_1928_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data synchronization workflow that demonstrates file transfer and bucket management techniques. Create a DAG that showcases different synchronization strategies between source and destination buckets.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.transfers.copy_between_s3_buckets import CopyBetweenS3_bucketsOperator\nfrom airflow.providers.s3.transfers.s3_to_s3 import S3ToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_connection(conn_id_name: str):\n        conn = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"s3\",\n        )\n        conn_extra = {\n            \"aws_access_key_id\": \"YOUR_AWS_ACCESS_KEY_ID\",\n            \"aws_secret_access_key\": \"YOUR_AWS_SECRET_ACCESS_KEY\",\n        }\n        conn_extra_json = str(conn_extra)\n        conn.set_extra(conn_extra_json)\n\n    create_s3_to_s3_connection = create_connection(conn_id_name=\"s3_to_s3_conn\")\n\n    # [START howto_operator_s3_to_s3]\n    s3_to_s3_task = S3ToS3Operator(\n        task_id=\"s3_to_s3_task\",\n        source_s3=\"s3://airflow-system-tests-resources/s3_to_s3_source_1/\",\n        dest_s3=\"s3://airflow-system-tests-resources/s3_to_s3_dest_1/\",\n        force_delete_src=True,\n        replace=True,\n    )\n    # [END howto_operator_s3_to_s3]\n\n    # [START howto_operator_copy_s3_buckets]\n    copy_s3_buckets_task = CopyBetweenS3_buckets_buckets(\n        task_id=\"copy_s3_buckets_task\",\n        source_bucket=\"airflow-system-tests-resources\",\n        source_end_key=\"s3_to_s3_source_2_start_key\",\n        dest_bucket=\"airflow-system-tests-resources\",\n        dest_end_key=\"s3_to_s3_dest_2_start_key\",\n        force_move=True,\n    )\n    # [END howto_operator_copy_s3_buckets]\n\n    create_connection >> s3_to_s3_task >> copy_s3_buckets_task\n</pre>\n</code>"}], "metadata": {"file_name": "dag_211_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown tasks for Google Calendar event synchronization. Construct a workflow that showcases connection management, data transfer, and infrastructure cleanup using Google Cloud Platform operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.datajoin_v1 import CloudDataJoinApiHook\nfrom airflow.providers.google.cloud.operators.datajoin_v1 import (\n    DataJoinOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google.suite.sensors.datajoin_v1 import DataJoinOperatorSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_datajoin_v1\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = f\"spreadsheet_id_{DAG_ID}_{ENV_ID}\"\n\nSINGLE_SPREADSheet = {\n    \"properties\": {\n        \"title\": \"Test Sheet\",\n    },\n}\n\nGCS_PATH = f\"gs://{BUCKET_NAME}/\"\nQUERY = \"SELECT id, title, description, category_id, is_shared, display_name FROM sheet\"\nGCS_PATH_QUERY = f\"{GCS_PATH}/{QUERY}\"\nspreadsheet = GoogleSheetsCreateSpreadsheetOperator(task_id=\"spreadsheet_creation_task\", **SINGLE_SPREADSheet)\n\n\n@task\ndef create_connection(connection_id, datajoin_project_id, datajoin_dataset_id):\n    return CloudDataJoinApiHook().create_connection(\n        connection_id=connection_id,\n        project_id=datajoin_project_id,\n        dataset_id=datajoin_dataset_id,\n        google_cloud_connection=Connection(\n            conn_id=\"google_cloud_platform\",\n            schema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"key\": {\"type\": \"string\"},\n                    \"value\": {\"type\": \"string\"},\n                },\n                \"required\": [\"key\"],\n            },\n        ),\n    )\n\n\n@task\ndef add_input(connection_id, datajoin_project_id, datajoin_dataset_id, location=\"us\"):\n    return CloudDataJoinApiHook().create_input(\n        connection_id=connection_id,\n        project_id=datajoin_project_id,\n        dataset_id=datajoin_dataset_id,\n        display_name=f\"Input_{DAG_ID}_{ENV_ID}\",\n        storage_option={\"output_path\": GCS_PATH_QUERY, \"location\": location},\n    )\n\n\n@task\ndef remove_input(connection_id, datajoin_project_id, datajoin_dataset_id, trigger_rule=TriggerRule.ALL_DONE):\n    return CloudDataJoinApiHook().delete_input(\n        connection_id=connection_id, project_id=datajoin_project_id, dataset_id=datajoin_dataset_id, timeout=300.0, trigger_rule=trigger_rule\n    )\n\n\n@task\ndef remove_connection(connection_id, datajoin_project_id, datajoin_dataset_id, trigger_rule=TriggerRule.ALL_DONE):\n    return CloudDataJoinApiHook().delete_connection(\n        connection_id=connection_id, project_id=datajoin_project_id, dataset_id=datajoin_dataset_id, trigger_rule=trigger_rule\n    )\n\n\n@task\ndef setup_datajoin_project(datajoin_project_id):\n    return GCSCreateBucketOperator(task_id=\"bucket_creation_task\", bucket_name=BUCKET_NAME, project_id=datajoin_project_id)\n\n\n@task\ndef get_dataset_id(connection: object):\n    return connection.dataset_id\n\n\n@task\ndef get_vault_id(connection: object):\n    return connection.vault_id\n\n\n@task\ndef get_name(connection: object):\n    return connection.name\n\n\n@task\ndef get_display_name(connection: object):\n    return connection.display_name\n\n\n@task\ndef get_storage_option(connection: object):\n    return connection.storage_option\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"datajoin-v1\"],\n) as dag:\n    create_connection_task = create_connection(\"google_cloud_platform\", datajoin_project_id=PROJECT_ID)\n    run_move_task = BashOperator(task_id=\"move_sheets_to_gcs\", bash_command=f\"gsutil cp {SPREADSHEET_ID} {GCS_PATH}\")\n    remove_connection_task = remove_connection(\"google_cloud_platform\", datajoin_project_id, datajoin_dataset_id)\n    remove_input_task = remove_input(\"google_cloud_platform\", datajoin_project_id, datajoin_dataset_id)\n    setup_datajoin_task = setup_datajoin_project(datajoin_project_id)\n    get_dataset_id_task = get_dataset_id(create_connection_task.output)\n    get_vault_id_task = get_vault_id(create_connection_task.output)\n    get_name_task = get_name(create_connection_task.output)\n    get_display_name_task = get_display_name(create_connection_task.output)\n    get_storage_option_task = get_storage_option(create_connection_task.output)\n    datajoin_instance = DataJoinOperator(task_id=\"datajoin_task\", google_cloud_conn_id=\"google_cloud_platform\")\n    datajoin_operator = DataJoinOperator(\n        task_id=\"datajoin_operator\",\n        connections={\"connection_id\": \"google_cloud_platform\"},\n        inputs={\"input_id\": \"input_1\"},\n    )\n    datajoin_sensor = DataJoinOperatorSensor(\n        task_id=\"datajoin_sensor\",\n        join_id=\"test-join-id\",\n        google_cloud_conn_id=\"google_cloud_platform\",\n    )\n    delete_dag = BashOperator(\n        task_id=\"delete_dag\",\n        bash_command=f\"airflow dags delete -f {dag.__dag_id__}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        setup_datajoin_task\n        >> create_connection_task\n        >> run_move_task\n        >> add_input(\"google_cloud_platform\", datajoin_project_id, datajoin_dataset_id)\n        >> get_dataset_id_task\n        >> get_vault_id_task\n        >> get_name_task\n        >> get_display_name_task\n        >> get_storage_option_task\n        >> datajoin_instance\n        >> datajoin_operator\n        >> datajoin_sensor\n        >> remove_input_task\n        >> remove_connection_task\n        >> delete_dag\n    )"}], "metadata": {"file_name": "dag_1643_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases conditional task execution based on the most recent scheduling window. Implement a DAG that uses LatestOnlyOperator to control task execution and prevent redundant processing of older scheduled tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START dag_def]\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef finally_task():\n    print(\"Finally!\")\n\n\ndef main(dag):\n    # [START everything_in_xcom_is_pandas_df]\n    xcom_in = {\"num\": pendulum.datetime.now()}\n    # [END everything_in_xcom_is_pandas_df]\n\n    # [START xcom_out_pass_the_value]\n    xcom_out = {\"num\": pendulum.datetime.now()}\n    # [END xcom_out_pass_the_value]\n\n    BashOperator(task_id=\"print_num_in_xcom\", bash_command=\"echo \\\"{{ task_instance.xcom_pull('task_in_xcom') }}\\\"\", dag=dag)\n    BashOperator(\n        task_id=\"finally_task\",\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('task_in_xcom') }}\\\";echo \\\"{{ task_instance.xcom_pull('task_out_xcom') }}\\\";echo \\\"{{ task_instance.xcom_pull('everything_in_xcom_is_pandas_df') }}\\\";echo \\\"{{ task_instance.xcom_pull('num') }}\\\"\",\n        dag=dag,\n    )\n    BashOperator(task_id=\"task_in_xcom\", bash_command=\"echo \\\"{{ task_instance.xcom_pull('task_in_xcom') }}\\\"\", dag=dag)\n    BashOperator(\n        task_id=\"task_out_xcom\",\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('task_out_xcom') }}\\\";echo \\\"{{ task_instance.xcom_pull('everything_in_xcom_is_pandas_df') }}\\\";echo \\\"{{ task_instance.xcom_pull('num') }}\\\"\",\n        dag=dag,\n    )\n    BashOperator(task_id=\"finally_task\", bash_command=\"echo -e 'Finally!\\\\n{{ task_instance.xcom_pull('num') }}'\", dag=dag)\n\n\ndag = DAG(\n    \"example_xcom_basic\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n)\n# [END dag_def]\n\n\nif __name__ == \"__main__\":\n    main(dag)"}], "metadata": {"file_name": "dag_1040_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates data extract refresh processes for a business intelligence workbook, showing how to trigger and monitor data updates across different execution modes. Create a workflow that simulates real-world data refresh scenarios with explicit job status tracking and different refresh strategies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_to_sql\"\n\nconn_id = \"airflow-sql\"\n\nsql_conn = Connection(\n    conn_id=conn_id,\n    conn_type=\"sql\",\n    description=\"Connection to Adf and used for demos only.\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_adf_table():\n        print(\"Create table in ADF without any connection trick\")\n\n    @task\n    def refresh_adf_table():\n        print(\"Refresh table in ADF\")\n        refresh_sql_task = SQLExecuteQueryOperator(\n            task_id=\"refresh_sql_task\",\n            sql=f\"REFRESH QUERY {adf_query_id};\",\n            conn_id=conn_id,\n        )\n\n    @task\n    def create_sql_table():\n        print(\"Create table in SQL\")\n\n    @task\n    def get_data_from_sql():\n        print(\"Get data from table in SQL\")\n\n    @task\n    def get_data_from_adf():\n        print(\"Get data from table in ADF\")\n\n    start = BashOperator(task_id=\"start\", bash_command=\"touch /tmp/airflow/finished\")\n\n    adf_conn = Connection(\n        conn_id=conn_id,\n        conn_type=\"sql\",\n        description=\"Connection to ADF\",\n    )\n\n    adf_query_id = \"adf_query_id\"\n\n    sql_table_id = \"sql_table\"\n    adf_table_id = \"adf_table\"\n\n    # [START howto_operator_adf_to_sql]\n    adf_to_sql = SQLExecuteQueryOperator(\n        task_id=\"adf_to_sql_task\",\n        sql=f\"SELECT * FROM {sql_table_id}\",\n        conn_id=conn_id,\n    )\n    # [END howto_operator_adf_to_sql]\n\n    # [START howto_operator_adf_to_sql_trigger]\n    refresh_sql_task_trigger = BashOperator(\n        task_id=\"refresh_sql_task_trigger\",\n        bash_command=f\"touch /tmp/airflow/finished\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_adf_to_sql_trigger]\n\n    create_adf_table()\n    refresh_adf_table()\n    create_sql_table()\n    get_data_from_sql()\n    get_data_from_adf()\n    start\n    refresh_sql_task_trigger"}], "metadata": {"file_name": "dag_1005_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates text embedding using the Cohere AI service, showing how to convert text into vector representations. Create a workflow that can process multiple text inputs and generate their corresponding embeddings.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, dag\nfrom airflow.providers.cohere.operators.embedding import CohereEmbeddingOperator\nfrom airflow.settings import Session, json_config\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\nconnection_id = os.environ.get(\"COHERE_DEFAULT_CONNECTION_ID\", \"cohere_default_connection\")\noutput_id = os.environ.get(\"COHERE_OUTPUT_ID\", \"output_id\")\ndata_path = os.environ.get(\"COHERE_DATA_PATH\", \"tmp/cohere\")\nfile_name = os.environ.get(\"COHERE_FILE_NAME\", \"output.csv\")\nfile_path = os.environ.get(\"COHERE_FILE_PATH\", f\"{data_path}/{file_name}\")\njson_config_obj = json_config.copy()\njson_config_obj[\"ensure_ascii\"] = False\n\n\n@task\ndef create_output(output_id: str):\n    \"\"\"\n    Create an output for CohereEmbeddingOperator to get vector of input\n    \"\"\"\n    log.info(\"Creating output %s\", output_id)\n    output = Role(output_id)\n    session = Session()\n    if session.query(Connection).filter(Connection.id == connection_id).first():\n        log.info(\"Reusing existing connection %s\", connection_id)\n    else:\n        log.info(\"Creating new connection %s\", connection_id)\n        output.connection = connection_id\n    output.path = file_path\n    session.add(output)\n    session.commit()\n\n\n@task\ndef create_files(output_id: str):\n    \"\"\"\n    Create input and output files\n    \"\"\"\n    from airflow.providers.cohere.utils import create_file\n\n    log.info(\"Creating input file %s\", file_path)\n    create_file(file_path, str(len(session.query(TaskInstance).all())))\n\n\nwith dag(\n    \"example_cohere_embedding_dag\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    on_trigger=lambda ti: ti.state == TaskInstance.State.SUCCESS,\n) as dag:\n    create_output(output_id)\n    create_files(output_id)\n\n    # [START howto_operator_cohere_embedding]\n    embed = CohereEmbeddingOperator.partial(\n        task_id=\"cohere_embedding\",\n        output_id=output_id,\n        text=[u\"hello world test ci\", u\"hello ai are you fine?\"],\n        model=\"small\",\n        return_response=True,\n    ).expand(text=[u\"hello world test ci\", u\"hello ai are you fine?\"])\n    # [END howto_operator_cohere_embedding]\n\n    embed_output_id = embed.output_id\n    embed_output_id == output_id\n    embed_response = embed.output_response\n\n    @task\n    def check_response(response, text):\n        return len(response) == len(text) and all(v > 0 for v in response)\n\n    check_response(embed_response, [u\"hello world test ci\", u\"hello ai are you fine?\"])\n\n    @task\n    def delete_output(output_id: str):\n        \"\"\"\n        Delete the output role we created in the beginning\n        \"\"\"\n        log.info(\"Deleting output %s\", output_id)\n        session = Session()\n        output = session.query(Role).filter(Role.id == output_id).first()\n        if output:\n            session.delete(output)\n            session.commit()\n\n    delete_output(output_id)\n\n    @task\n    def delete_files(output_id: str):\n        \"\"\"\n        Delete the input and output files we created in the beginning\n        \"\"\"\n        log.info(\"Deleting files %s and %s\", file_path, file_name)\n        os.remove(file_path)\n\n    delete_files(output_id)"}], "metadata": {"file_name": "dag_1959_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates the technical process of starting and checking Looker PDT builds using specialized Airflow operators for cloud-based data transformation. The DAG should demonstrate task dependencies and asynchronous job monitoring in a data engineering pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.looker import LookerStartPdtOperator, LookerStopPdtOperator\n\ngcp_folder_id = models.Property(\n    model=models.Environment(),\n    default=\"\",\n    title=\"GCP Folder ID\",\n    description=\"The folder id of the LookML file.\",\n    role=\"READER\",\n)\n\nwith models.DAG(\n    dag_id=\"example_looker_pdt_operator\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_start_pdt]\n    start_pdt_job = LookerStartPdtOperator(task_id=\"start_pdt_job\", gcp_folder_id=gcp_folder_id)\n    # [END howto_operator_start_pdt]\n\n    # [START howto_operator_stop_pdt]\n    stop_pdt_job = LookerStopPdtOperator(task_id=\"stop_pdt_job\", dtl_file_path=\"bells_and_whistles.dtl\", user=\"looker\")\n    # [END howto_operator_stop_pdt]\n\n    start_pdt_job >> stop_pdt_job\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove\n    # the temp setup code.\n    list(watcher(dag)"}], "metadata": {"file_name": "dag_703_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a vector data ingestion workflow that populates a Pinecone vector database with predefined embedding vectors. Create a DAG that allows flexible configuration of vector data insertion with support for metadata and custom namespaces.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinecone.operators.pinecone import PineconeIngestOperator\n\nindex_name = \"test-pinecone-index\"\n\n\n@task\ndef create_index():\n    from airflow.providers.pinecone.hooks.pinecone import PineconeHook\n\n    PineconeHook().create_index(index_name=index_name)\n\n\n@task\ndef drop_index():\n    from airflow.providers.pinecone.hooks.pinecone import PineconeHook\n\n    PineconeHook().delete_index(index_name=index_name)\n\n\nwith DAG(\n    \"example_pinecone_ingest\",\n    schedule=pendulum.time_period(hour=1, minute=0),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    create_index()\n    # [START howto_pinecone_ingest]\n    ingest_task = PineconeIngestOperator(\n        task_id=\"pinecone_ingest_task\",\n        index_name=index_name,\n        input_list=[\n            (\"name-1\", \"name test 1\", \"test-1\", \"test-value-1\"),\n            (\"name-2\", \"name test 2\", \"test-2\", \"test-value-2\"),\n        ],\n        metadata=\"metadata\",\n        metadata_kwargs={\"metadata\": \"metadata\"},\n    )\n    # [END howto_pinecone_ingest]\n    drop_index()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1783_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "What are the steps to create a `reduce` function in Python that multiplies all elements in a list?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def reduce_function(x, y):\n    \"\"\"\n    A simple reduce function that multiplies all the numbers in the input list.\n    \"\"\"\n    return x * y\n\n\n# [START dag_decorator_usage]\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_decorator() -> BashOperator:\n    @task\n    def get_input() -> list:\n        return [1, 2, 3, 4, 5]\n\n    @task\n    def sum_it(nums: list) -> int:\n        return sum(nums)\n\n    reduce_task = BashOperator(task_id=\"reduce_python_function\", bash_command=f\"echo {reduce_function(*get_input())}\")\n\n\nexample_dag_decorator()\n# [END dag_decorator_usage]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Azure Blob Storage monitoring system using Airflow sensors that checks for specific file conditions before initiating downstream data processing tasks. Implement a robust file detection mechanism that allows workflow progression based on precise blob or prefix matching.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import AzureBlobADLSListOperator\nfrom airflow.providers.microsoft.azure.sensors.adls import AzureBlobADLSensors\n\nBUCKET_NAME = os.environ.get(\"BUCKET_NAME\", \"azure-blob-aks\")\nPREFIX = os.environ.get(\"PREFIX\", \"azure-blob-aks-file\")\nconn = Connection(\n    conn_id=\"azureaks\",\n    conn_type=\"azure_container_aks\",\n    extra={\n        \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP\", \"default\"),\n        \"name\": os.environ.get(\"CLUSTER_NAME\", \"default\"),\n    },\n)\n\nwith DAG(\n    \"example_adls_sensors\",\n    schedule_interval=datetime(2021, 1, 1, hour=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection():\n        \"\"\"A dummy function that creates a connection\"\"\"\n        conn\n\n    # [START howto_operator_adls_list]\n    list_adls_files = AzureBlobADLSListOperator(\n        task_id=\"list_adls_files\",\n        bucket_name=BUCKET_NAME,\n        prefix=PREFIX,\n    )\n    # [END howto_operator_adls_list]\n\n    # [START howto_sensor_adls]\n    watch_adls_files = AzureBlobADLSensors(\n        task_id=\"watch_adls_files\",\n        bucket_name=BUCKET_NAME,\n        prefix=PREFIX,\n        minduration=5,\n    )\n    # [END howto_sensor_adls]\n\n    create_connection() >> list_adls_files >> watch_adls_files\n\n    for task in [list_adls_files, watch_adls_files]:\n        task.depends_on_external = conn"}], "metadata": {"file_name": "dag_975_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete image processing workflow with Google Cloud services. Develop a systematic approach to handling image data through a series of interconnected tasks from bucket creation to image analysis.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImageSafeSearchOperator,\n    CloudVisionExtrapolateImageGeometricParametersOperator,\n    CloudVisionExtrapolateImageHistogramOperator,\n    CloudVisionExtractDocumentDocumentOperator,\n    CloudVisionImageAnnotateOperator,\n    CloudVisionImageDetectOperator,\n    CloudVisionImageDetectWebImageLabelsOperator,\n    CloudVisionImageDetectWebWebImageLabelsOperator,\n    CloudVisionImageProductSetCreateOperator,\n    CloudVisionImageProductSetDeleteOperator,\n    CloudVisionImageSetProductEntryCreateOperator,\n    CloudVisionImageSetProductEntryDeleteOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_vision_annotator\"\n\nBUCKET_NAME = f\"bucket_vision_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_vision_annotate_images_gcp_url]\nFILE_URL = \"gs://airflow-system-tests-resources/vision/label-image.jpg\"\n# [END howto_operator_vision_annotate_images_gcp_url]\n\n# [START howto_operator_vision_extrapolate_gerometric_parameters_gcp_url]\nFILE_URL_2 = \"gs://airflow-system-tests-resources/vision/extract-image.jpg\"\n# [END howto_operator_vision_extrapolate_gerometric_parameters_gcp_url]\n\n# [START howto_operator_vision_product_set_create_body]\nPRODUCT_SET_BODY = {\"image_types\": [\"IMAGE_URL\"], \"expose_image_labels\": [\"LABEL_ALL\"]}\n# [END howto_operator_vision_product_set_create_body]\n\n# [START howto_operator_vision_image_detect_body]\nIMAGE_DETECT_BODY = {\"source\": {\"image_url\": FILE_URL}}\n# [END howto_operator_vision_image_detect_body]\n\n# [START howto_operator_vision_image_detect_web_body]\nIMAGE_DETECT_WEB_BODY = {\"source\": {\"url\": FILE_URL_2}}\n# [END howto_operator_vision_image_detect_web_body]\n\n# [START howto_operator_vision_image_product_set_delete_body]\nPRODUCT_SET_DELETE_BODY = {\"image_types\": [\"IMAGE_URL\"], \"expose_image_labels\": [\"LABEL_ALL\"]}\n# [END howto_operator_vision_image_product_set_delete_body]\n\n# [START howto_operator_vision_image_set_product_entry_delete_body]\nIMAGE_SET_PRODUCT_ENTRY_DELETE_BODY = {\"image_type\": \"IMAGE_URL\", \"expose_labels\": [\"LABEL_ALL\"]}\n# [END howto_operator_vision_image_set_product_entry_delete_body]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision-annotator\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    @task\n    def upload_image_to_gcs():\n        # [START upload_image_to_gcs]\n        gcs_bucket_to_url = GCSToGCSOperator(\n            task_id=\"upload_file_to_gcs\",\n            source_bucket=BUCKET_NAME,\n            source_object=\"vision/label-image.jpg\",\n            destination_object=\"vision/label-image.jpg\",\n            project_id=PROJECT_ID,\n        )\n        # [END upload_image_to_gcs]\n\n        return gcs_bucket_to_url.output[\"relative_path\"]\n\n    @task\n    def upload_url_to_gcs():\n        # [START upload_url_to_gcs]\n        gcs_bucket_to_url = GCSToGCSOperator(\n            task_id=\"upload_url_to_gcs\",\n            source_bucket=BUCKET_NAME,\n            source_object=\"vision/extract-image.jpg\",\n            destination_object=\"vision/extract-image.jpg\",\n            project_id=PROJECT_ID,\n        )\n        # [END upload_url_to_gcs]\n\n        return gcs_bucket_to_url.output[\"relative_path\"]\n\n    upload_image_to_gcs_task = upload_image_to_gcs()\n    upload_url_to_gcs_task >> create_bucket\n\n    # [START howto_operator_vision_image_detect]\n    image_detect = CloudVisionImageDetectOperator(\n        task_id=\"image_detect\", http_method=\"POST\", url=IMAGE_DETECT_BODY[\"source\"][\"image_url\"]\n    )\n    # [END howto_operator_vision_image_detect]\n\n    # [START howto_operator_vision_image_detect_result]\n    image_detect_result = CloudVisionImageDetectOperator(\n        task_id=\"image_detect_result\",\n        http_method=\"POST\",\n        url=IMAGE_DETECT_BODY[\"source\"][\"image_url\"],\n        result=\"image\",\n    )\n    # [END howto_operator_vision_image_detect_result]\n\n    # [START howto_operator_vision_image_detect_web]\n    image_detect_web = CloudVisionImageDetectOperator(\n        task_id=\"image_detect_web\", http_method=\"POST\", url=IMAGE_DETECT_WEB_BODY[\"source\"][\"url\"]\n    )\n    # [END howto_operator_vision_image_detect_web]\n\n    # [START howto_operator_vision_image_detect_web_result]\n    image_detect_web_result = CloudVisionImageDetectOperator(\n        task_id=\"image_detect_web_result\",\n        http_method=\"POST\",\n        url=IMAGE_DETECT_WEB_BODY[\"source\"][\"url\"],\n        result=\"image\",\n    )\n    # [END howto_operator_vision_image_detect_web_result]\n\n    # [START howto_operator_vision_annotate_images_gcp_url]\n    image_annotate_gcp_url = CloudVisionImageAnnotateOperator(\n        task_id=\"image_annotate_gcp_url\",\n        http_method=\"POST\",  # Alternatively, you can set http_request.method to 'GET'.\n        url=IMAGE_DETECT_BODY[\"source\"][\"image_url\"],\n        features=[{\"type_\": \"LABEL_DETECTION\"}],\n    )\n    # [END howto_operator_vision_annotate_images_gcp_url]\n\n    # [START howto_operator_vision_annotate_images_output]\n    image_annotate = CloudVisionImageAnnotateOperator(\n        task_id=\"image_annotate\",\n        http_method=\"POST\",  # Alternatively, you can set http_request.method to 'GET'.\n        url=IMAGE_DETECT_BODY[\"source\"][\"image_url\"],\n        output_uri_prefix=\"/tmp/\",\n        features=[{\"type_\": \"LABEL_DETECTION\"}],\n    )\n    # [END howto_operator_vision_annotate_images_output]\n\n    # [START howto_operator_vision_extrapolate_image_geometric_parameters_gcp_url]\n    image_extrapolate_gerometric_parameters_gcp_url = CloudVisionExtrapolateImageGeometricParametersOperator(\n        task_id=\"image_extrapolate_gerometric_parameters_gcp_url\",\n        http_method=\"POST\",  # Alternatively, you can set http_request.method to 'GET'.\n        url=IMAGE_DETECT_WEB_BODY[\"source\"][\"url\"],\n        target_size={},\n        language_codes=[],\n        source_image_uRI=FILE_URL_2,\n    )\n    # [END howto_operator_vision_extrapolate_image_geometric_parameters_gcp_url]\n\n    # [START howto_operator_vision_extrapolate_image_geometric_parameters_result]\n    image_extrapolate_gerometric_parameters_result = CloudVisionExtrapolateImageGeometricParametersOperator(\n        task_id=\"image_extrapolate_gerometric_parameters_result\",\n        http_method=\"POST\",  # Alternatively, you can set http_request.method to 'GET'.\n        url=IMAGE_DETECT_WEB_BODY[\"source\"][\"url\"],\n        target_size={},\n        language_codes=[],\n        source_image_uRI=FILE_URL_2,\n    )\n    # [END howto_operator_vision_extrapolate_image_geometric_parameters_result]\n\n    # [START howto_operator_vision_extract_document_document]\n    image_extract_document_document = CloudVisionExtractDocumentDocumentOperator(\n        task_id=\"image_extract_document_document\",\n        http_method=\"POST\",  # Alternatively, you can set http_request.method to 'GET'.\n        url=IMAGE_DETECT_WEB_BODY[\"source\"][\"url\"],\n        language_codes=[\"en\"],\n        web_detection_params=IMAGE_DETECT_WEB_BODY[\"source\"][\"web\"],\n    )\n    # [END howto_operator_vision_extract_document_document]\n\n    # [START howto_operator_vision_image_product_set_delete_body]\n    image_product_set_delete = CloudVisionImageProductSetDeleteOperator(\n        task_id=\"image_product_set_delete\",\n        product_set_id=\"image_url\",\n        image_types=[\"IMAGE_URL\"],\n        expose_image_labels=[\"LABEL_ALL\"],\n        project_id=PROJECT_ID,\n        body=PRODUCT_SET_DELETE_BODY,\n    )\n    # [END howto_operator_vision_image_product_set_delete_body]\n\n    # [START howto_operator_vision_image_set_product_entry_create_body]\n    image_set_product_entry_create_body = {\n        \"image_type\": \"IMAGE_URL\",\n        \"expose_labels\": [\"LABEL_ALL\"],\n        \"product_set_id\": \"new_product_set\",\n    }\n    # [END howto_operator_vision_image_set_product_entry_create_body]\n\n    # [START howto_operator_vision_image_set_product_entry_delete_body]\n    image_set_product_entry_delete = CloudVisionImageSetProductEntryDeleteOperator(\n        task_id=\"image_set_product_entry_delete\",\n        image_type=\"IMAGE_URL\",\n        expose_labels=[\"LABEL_ALL\"],\n        product_set_id=\"new_product_set\",\n        project_id=PROJECT_ID,\n        body=IMAGE_SET_PRODUCT_ENTRY_DELETE_BODY,\n    )\n    # [END howto_operator_vision_image_set_product_entry_delete_body]\n\n    delete_product_set = CloudVisionImageProductSetDelete"}], "metadata": {"file_name": "dag_1328_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the complete lifecycle of Azure Service Bus messaging operations, from queue and topic creation through message transmission and subscription management. Develop a step-by-step demonstration of enterprise messaging infrastructure using cloud-native service bus capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.asb import (\n    ASBCreateQueueOperator,\n    ASBDeleteQueueOperator,\n    ASBReceiveMessageOperator,\n    ASBSendMessageOperator,\n    ASBStopMessageSubscriptionOperator,\n    ASBUpdateQueueAttributesOperator,\n)\nfrom airflow.providers.microsoft.azure.utils import get_resource_id\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_asb\"\n\n# ConnectionId of the connection used to connect to Azure. This connection should have\n# Azure Active Directory credentials in order to create a resource in the Azure cloud.\n# If accessing a managed service then be sure the connection is also configured with\n# the managed service service principal.\nCONNECTION_ID = \"connection_azure_service_bus\"\n\nQUEUE_NAME = f\"queue_{ENV_ID}\"\nMESSAGE = \"Test Message\"\nMESSAGE_LIST = [f\"{MESSAGE}--#{i}\" for i in range(10)]\nMESSAGE_SET = \"Test Message Set\"\nSUBSCRIPTION_NAME = f\"subscription_{ENV_ID}\"\nMESSAGE_SET_DESCRIPTION = \"Test Message Set Description\"\nQUEUE_ATTRIBUTES = {\"MessageCount\": 100}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azure-service-bus\"],\n) as dag:\n    @task\n    def create_connections():\n        connection_bus = Connection(\n            conn_id=CONNECTION_ID,\n            conn_type=\"azure_service_bus\",\n        )\n        bus_endpoint_id = get_resource_id(connection_bus)\n        conn = Connection(\n            conn_id=\"connection_azure_service_bus_default\",\n            conn_type=\"azure_service_bus\",\n        )\n        assert conn\n        assert conn.extra_parameters\n        extra = conn.extra_parameters\n        assert extra[\"azure_service_bus_endpoint_id\"] == bus_endpoint_id\n        assert extra[\"azure_service_bus_service_name\"] == \"servicebusContoso\"\n\n    @task\n    def create_queue():\n        asb_create_queue = ASBCreateQueueOperator(\n            task_id=\"asb_create_queue\",\n            queue_name=QUEUE_NAME,\n            bus_endpoint_id=get_resource_id(connection_bus),\n        )\n\n    @task\n    def create_message_set():\n        asb_create_message_set = ASBSendMessageOperator(\n            task_id=\"asb_create_message_set\",\n            message_set_id=extra[\"azure_service_bus_message_set_id\"],\n            queue_name=QUEUE_NAME,\n            bus_endpoint_id=get_resource_id(connection_bus),\n            messages=MESSAGE_SET,\n            message_set_properties={\"description\": MESSAGE_SET_DESCRIPTION},\n        )\n\n    @task\n    def stop_message_subscription(message_set_id):\n        asb_stop_message_subscription = ASBStopMessageSubscriptionOperator(\n            task_id=\"asb_stop_message_subscription\",\n            subscription_name=SUBSCRIPTION_NAME,\n            message_set_id=message_set_id,\n            bus_endpoint_id=get_resource_id(connection_bus),\n        )\n\n    @task\n    def update_queue_attributes():\n        asb_update_queue = ASBUpdateQueueAttributesOperator(\n            task_id=\"asb_update_queue\",\n            queue_name=QUEUE_NAME,\n            bus_endpoint_id=get_resource_id(connection_bus),\n            queue_attributes=QUEUE_ATTRIBUTES,\n        )\n\n    @task\n    def delete_queue():\n        asb_delete_queue = ASBDeleteQueueOperator(\n            task_id=\"asb_delete_queue\", queue_name=QUEUE_NAME, bus_endpoint_id=get_resource_id(connection_bus)\n        )\n\n    create_connection = create_connections()\n    create_queue = create_queue()\n    create_message_set = create_message_set()\n    stop_message_subscription = stop_message_subscription(MESSAGE_SET[\"message_set_id\"])\n    update_queue_attributes = update_queue_attributes()\n    delete_queue = delete_queue()\n    [stop_message_subscription, update_queue_attributes] >> create_queue >> create_connection >> create_message_set\n\n    (\n        create_connection()\n        >> create_queue()\n        >> create_message_set()\n        >> stop_message_subscription()\n        >> update_queue_attributes()\n        >> delete_queue()\n    )"}], "metadata": {"file_name": "dag_2157_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that exports a specific Firestore collection to Google Cloud Storage and then creates an external BigQuery table from the exported data. Build an automated pipeline that demonstrates cross-project data transfer and transformation between Firebase, GCS, and BigQuery.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.firebase_to_gcs import FirebaseToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"firebase_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-north1\"\nDATA_SAMPLE_DATASET = f\"bigquery_sample_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_FILE_NAME = \"event_list_firestore.csv\"\nDATA_SAMPLE_GCS_PATH = f\"gs://{BUCKET_NAME}/{DATA_SAMPLE_FILE_NAME}\"\nTABLE_NAME = \"firestore_to_gcs\"\nSCHEMA = [\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"email\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"role\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"firebase\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATA_SAMPLE_DATASET,\n        location=LOCATION,\n    )\n\n    create_schema = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_schema\",\n        dataset_id=TABLE_NAME,\n        location=LOCATION,\n    )\n\n    # [START howto_operator_firebase_to_gcs]\n    upload_firestore_to_gcs = FirebaseToGCSOperator(\n        task_id=\"upload_firestore_to_gcs\",\n        folder_name=\"V1 2020\",\n        file_list=[\"event_list_firestore.csv\"],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_firebase_to_gcs]\n\n    # [START howto_execute_operator_firebase_to_gcs]\n    execute_query = BigQueryInsertJobOperator(\n        task_id=\"execute_query\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(1) FROM `{PROJECT_ID}.{DATA_SAMPLE_DATASET}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_execute_operator_firebase_to_gcs]\n\n    # [START howto_operator_gcs_to_bigquery]\n    read_firestore_data = GCSToBigQueryOperator(\n        task_id=\"read_firestore_data\",\n        bucket=BUCKET_NAME,\n        source_objects=[DATA_SAMPLE_GCS_PATH],\n        destination_table={\"table_id\": TABLE_NAME, \"write_disposition\": \"WRITE_TRUNCATE\"},\n        schema_fields=SCHEMA,\n        autodetect=True,\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATA_SAMPLE_DATASET,\n        delete_contents=True,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        >> upload_firestore_to_gcs\n        >> create_schema\n        >> execute_query\n        # TEST BODY\n        >> read_firestore_data\n        # TEST TEARDOWN\n        >> delete_bucket\n        >> delete_dataset\n    )"}], "metadata": {"file_name": "dag_207_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nDAG_ID = \"example_quicksight\"\n\nSAMPLE_DATA = {\n    \"state\": \"CA\",\n    \"make\": \"Toyota\",\n    \"model\": \"Camry\",\n}\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nconn = boto3.client(\"quicksight\")\n\n\n@task\ndef create_data_source(connection_id: str, data_source_name: str):\n    conn.create_data_source(\n        DataSourceName=data_source_name,\n        DataSourceType=\"S3\",\n        RoleArn=os.environ[ROLE_ARN_KEY],\n        InputMode=\"S3_INPUT\",\n        InputParameters={\n            \"S3InputParameters\": {\"Bucket\": os.environ[\"DATA_BUCKET\"], \"Key\": \"sample-data.json\"}\n        },\n        ConnectionId=connection_id,\n    )\n\n\n@task\ndef create_dataset(data_source_name: str, dataset_name: str):\n    conn.create_dataset(\n        DatasetName=dataset_name,\n        DataSourceName=data_source_name,\n        RoleArn=os.environ[ROLE_ARN_KEY],\n    )\n\n\n@task\ndef create_ingestion(data_source_name: str, dataset_name: str, ingestion_name: str):\n    conn.create_ingestion(\n        DataSourceName=data_source_name,\n        DatasetName=dataset_name,\n        IngestionName=ingestion_name,\n        InputMode=\"S3_INPUT\",\n        InputParameters={\n            \"S3InputParameters\": {\n                \"Bucket\": os.environ[\"DATA_BUCKET\"],\n                \"Key\": \"ingestion-input.json\",\n            }\n        },\n        RoleArn=os.environ[ROLE_ARN_KEY],\n    )\n\n\n@task\ndef update_dataset_access(dataset_name: str, access_type: str, identity: str):\n    conn.update_dataset_access(\n        DatasetName=dataset_name, AccessType=access_type, Identity=identity\n    )\n\n\n@task\ndef delete_ingestion(ingestion_name: str):\n    conn.delete_ingestion(IngestionName=ingestion_name)\n\n\n@task\ndef delete_dataset(dataset_name: str):\n    conn.delete_dataset(DatasetName=dataset_name)\n\n\n@task\ndef delete_data_source(data_source_name: str):\n    conn.delete_data_source(DataSourceName=data_source_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    data_source_connection_id = f\"{env_id}-data-source\"\n    dataset_name = f\"{env_id}-dataset\"\n    ingestion_name = f\"{env_id}-ingestion\"\n    access_type = \"S3\"\n\n    # [START howto_operator_quicksight_create_data_source]\n    create_data_source = QuickSightHook(used_in_test=True).create_data_source\n    # [END howto_operator_quicksight_create_data_source]\n\n    # [START howto_operator_quicksight_create_dataset]\n    create_dataset = QuickSightHook(used_in_test=True).create_dataset\n    # [END howto_operator_quicksight_create_dataset]\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightHook(used_in_test=True).create_ingestion\n    # [END howto_operator_quicksight_create_ingestion]\n\n    # [START howto_sensor_quicksight_get_latest_visual]\n    get_latest_visual = QuickSightHook(used_in_test=True).get_latest_visual\n    # [END howto_sensor_quicksight_get_latest_visual]\n\n    # [START howto_operator_quicksight_update_dataset_access]\n    update_access = QuickSightHook(used_in_test=True).update_dataset_access\n    # [END howto_operator_quicksight_update_dataset_access]\n\n    # [START howto_sensor_quicksight_list_access]\n    list_access = QuickSightHook(used_in_test=True).list_access\n    # [END howto_sensor_quicksight_list_access]\n\n    (\n        # TEST SETUP\n        create_data_source(\n            connection_id=data_source_connection_id, data_source_name=\"data_source_1\"\n        )\n        >> create_dataset(data_source_name=\"data_source_1\", dataset_name=\"dataset_1\")\n        >> create_ingestion(\n            data_source_name=\"data_source_1\",\n            dataset_name=\"dataset_1\",\n            ingestion_name=ingestion_name,\n        )\n        >> update_dataset_access(\n            dataset_name=\"dataset_1\", access_type=access_type, identity=role_arn\n        )\n        >> get_latest_visual(\n            dataset_name=\"dataset_1\",\n            dataset_visual_name=\"latest_visual_1\",\n        )\n        >> list_access(dataset_name=\"dataset_1\")\n        # TEST TEARDOWN\n        >> delete_ingestion(ingestion_name)\n        >> delete_dataset(dataset_name)\n        >> delete_data_source(data_source_connection_id)\n    ).trigger = TriggerRule.ALL_DONE\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it done / stop the test\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_404_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated reporting workflow for Google Campaign Manager that creates, runs, and processes marketing performance reports. Design a data pipeline that extracts campaign metrics, transforms the data, and prepares it for business intelligence analysis.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.marketing_platform.operators.cm import (\n    GoogleMarketingPlatformCMAddColumnsToReportOperator,\n    GoogleMarketingPlatformCMAddFieldsToReportOperator,\n    GoogleMarketingPlatformCMCreateReportOperator,\n    GoogleMarketingPlatformCMCreateReportTemplateOperator,\n    GoogleMarketingPlatformCMDeleteReportOperator,\n    GoogleMarketingPlatformCMDeleteReportTemplateOperator,\n    GoogleMarketingPlatformCMGetReportOperator,\n    GoogleMarketingPlatformCMGetReportColumnsOperator,\n    GoogleMarketingPlatformCMGetReportFieldsOperator,\n    GoogleMarketingPlatformCMRunReportOperator,\n    GoogleMarketingPlatformCMRunReportTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"cm_reports\"\n\n# [START howto_cm_report_template]\nREPORT_TEMPLATE = {\n    \"name\": \"Report Template Name\",\n    \"label\": \"Report Template Label\",\n    \"format_spec\": \"DayOfMonth=1;Quarter=Q2;Year=2020\",\n    \"csv_options\": {\"write_disposition\": \"overwrite_default\"},\n}\n# [END howto_cm_report_template]\n\n# [START howto_cm_report]\nREPORT = {\"name\": \"Report Name\", \"label\": \"Report Label\", \"reportTemplate\": REPORT_TEMPLATE}\n# [END howto_cm_report]\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cm\"],\n) as dag:\n    # [START howto_cm_create_report_template_operator]\n    create_report_template = GoogleMarketingPlatformCMCreateReportTemplateOperator(task_id=\"create_report_template\")\n    # [END howto_cm_create_report_template_operator]\n\n    # [START howto_cm_run_report_template_operator]\n    run_report_template = GoogleMarketingPlatformCMRunReportTemplateOperator(task_id=\"run_report_template\", **REPORT)\n    # [END howto_cm_run_report_template_operator]\n\n    # [START howto_cm_create_report_operator]\n    create_report = GoogleMarketingPlatformCMCreateReportOperator(task_id=\"create_report\", **REPORT)\n    # [END howto_create_report_operator]\n\n    # [START howto_cm_get_report_operator]\n    get_report = GoogleMarketingPlatformCMGetReportOperator(task_id=\"get_report\", reportIdentifier={\"reportName\": \"Report Name\"})\n    # [END howto_cm_get_report_operator]\n\n    # [START howto_cm_get_report_columns_operator]\n    get_report_columns = GoogleMarketingPlatformCMGetReportColumnsOperator(\n        task_id=\"get_report_columns\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_get_report_columns_operator]\n\n    # [START howto_cm_get_report_fields_operator]\n    get_report_fields = GoogleMarketingPlatformCMGetReportFieldsOperator(\n        task_id=\"get_report_fields\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_get_report_fields_operator]\n\n    # [START howto_cm_add_columns_to_report_operator]\n    add_columns_to_report = GoogleMarketingPlatformCMAddColumnsToReportOperator(\n        task_id=\"add_columns_to_report\", reportIdentifier={\"reportName\": \"Report Name\"}, columns=[\"c1\", \"c2\"]\n    )\n    # [END howto_cm_add_columns_to_report_operator]\n\n    # [START howto_cm_add_fields_to_report_operator]\n    add_fields_to_report = GoogleMarketingPlatformCMAddFieldsToReportOperator(\n        task_id=\"add_fields_to_report\", reportIdentifier={\"reportName\": \"Report Name\"}, fields=[\"f1\", \"f2\"]\n    )\n    # [END howto_cm_add_fields_to_report_operator]\n\n    # [START howto_cm_delete_report_operator]\n    delete_report = GoogleMarketingPlatformCMDeleteReportOperator(\n        task_id=\"delete_report\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_delete_report_operator]\n\n    # [START howto_cm_delete_report_template_operator]\n    delete_report_template = GoogleMarketingPlatformCMDeleteReportTemplateOperator(\n        task_id=\"delete_report_template\", templateIdentifier={\"templateName\": \"Report Template Name\"}\n    )\n    # [END howto_cm_delete_report_template_operator]\n\n    # [START howto_cm_get_report_operator_def]\n    get_report_def = GoogleMarketingPlatformCMGetReportOperator(\n        task_id=\"get_report_def\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_get_report_operator_def]\n\n    # [START howto_cm_get_report_columns_operator_def]\n    get_report_columns_def = GoogleMarketingPlatformCMGetReportColumnsOperator(\n        task_id=\"get_report_columns_def\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_get_report_columns_operator_def]\n\n    # [START howto_cm_get_report_fields_operator_def]\n    get_report_fields_def = GoogleMarketingPlatformCMGetReportFieldsOperator(\n        task_id=\"get_report_fields_def\", reportIdentifier={\"reportName\": \"Report Name\"}\n    )\n    # [END howto_cm_get_report_fields_operator_def]\n\n    chain(\n        create_report_template,\n        run_report_template,\n        create_report,\n        get_report,\n        get_report_columns,\n        get_report_fields,\n        add_columns_to_report,\n        add_fields_to_report,\n        delete_report,\n        delete_report_template,\n        get_report_def,\n        get_report_columns_def,\n        get_report_fields_def,\n    )"}], "metadata": {"file_name": "dag_276_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function to find all the numbers in a given list that are divisible by 3 and are even.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def find_divisible_by_three_even(numbers):\n    \"\"\"\n    This function takes a list of integers as input and returns a list of numbers that are divisible by 3 and even.\n    \"\"\"\n    return [num for num in numbers if num % 3 == 0 and num % 2 == 0]\n\n# Example usage\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nresult = find_divisible_by_three_even(numbers)\nprint(result)  # Output: [6]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates cross-cloud file transfer capabilities, showcasing how to programmatically move data between Azure and Google Cloud storage systems. Implement a workflow that creates a destination bucket, transfers files, and then cleanly removes the temporary storage infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.common.azure.hooks.file_storage import AzureFileStorageHook\nfrom airflow.providers.common.azure.hooks.network_files import AzureNetworkFilesHook\nfrom airflow.providers.common.azure.operators.file_storage import AzureCreateDirectoryOperator\nfrom airflow.providers.common.azure.operators.network_files import AzureNetworkFilesCreateFolderOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nCONTAINER_NAME = os.environ.get(\"CONTAINER_NAME\", \"test-container\")\nFILE_STORAGE_ACCOUNT_NAME = os.environ.get(\"FILE_STORAGE_ACCOUNT_NAME\", \"test-file-storage-account\")\nAZURE_NETWORK_FILES_SHARE_NAME = f\"network-files-share-{ENV_ID}-{CONTAINER_NAME}\"\nAZURE_NETWORK_FILES_FOLDER_NAME = f\"network-files-folder-{ENV_ID}-{CONTAINER_NAME}\"\nFILE_STORAGE_DIRECTORY = os.environ.get(\"FILE_STORAGE_DIRECTORY\", \"tmp\")\nEXISTING_DIRECTORY = os.environ.get(\"EXISTING_DIRECTORY\", \"test-directory\")\nCONNECTION_ID = os.environ.get(\"CONNECTION_ID\", \"airflow_standard\")\n\n\n@task\ndef create_connection(connection_id: str):\n    \"\"\"Create Airflow Standard Connection\"\"\"\n    conn = Connection(\n        conn_id=connection_id,\n        conn_type=\"azurefile\",\n        description=\"Example AzureFile connection\",\n        extra={\n            \"mount_name\": \"test-mount\",\n            \"account_name\": FILE_STORAGE_ACCOUNT_NAME,\n            \"directory_path\": FILE_STORAGE_DIRECTORY,\n        },\n    )\n    hook = AzureFileStorageHook()\n    hook.create_mount(conn, \"test-mount\", FILE_STORAGE_ACCOUNT_NAME, \"\")\n\n    conn = Connection(\n        conn_id=connection_id,\n        conn_type=\"azurenetworkfiles\",\n        description=\"Example AzureNetworkFiles connection\",\n        extra={\n            \"share_name\": AZURE_NETWORK_FILES_SHARE_NAME,\n            \"folder_name\": AZURE_NETWORK_FILES_FOLDER_NAME,\n            \"network_file_share_conn_id\": CONNECTION_ID,\n        },\n    )\n    hook = AzureNetworkFilesHook()\n    hook.create_folder(conn, AZURE_NETWORK_FILES_SHARE_NAME, AZURE_NETWORK_FILES_FOLDER_NAME)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_all_files(folder_name: str, share_name: str, network_file_share_conn_id: str):\n    \"\"\"Delete all files in a folder\"\"\"\n    hook = AzureNetworkFilesHook(network_file_share_conn_id=network_file_share_conn_id)\n    hook.delete_files(share_name, folder_name)\n\n\nwith TaskGroup(\"file_storage_to_network_files\"):  # type: ignore[attr-defined]\n    \"\"\"TaskGroup to demo AzureFileStorage to AzureNetworkFiles transfer\"\"\"\n    create_folder = AzureNetworkFilesCreateFolderOperator(\n        task_id=\"create_folder\",\n        share_name=AZURE_NETWORK_FILES_SHARE_NAME,\n        folder_name=AZURE_NETWORK_FILES_FOLDER_NAME,\n        network_file_share_conn_id=CONNECTION_ID,\n    )\n\n    # [START howto_operator_azure_network_files_move_single_file]\n    move_file = BashOperator(\n        task_id=\"move_file\",\n        bash_command=f\"mv {EXISTING_DIRECTORY}/test-file.txt {AZURE_NETWORK_FILES_FOLDER_NAME}/\",\n    )\n    # [END howto_operator_azure_network_files_move_single_file]\n\n    delete_folder = BashOperator(\n        task_id=\"delete_folder\",\n        bash_command=f\"rmdir {AZURE_NETWORK_FILES_FOLDER_NAME} 2> /dev/null || true\",\n    )\n\n    # [START howto_operator azure_file_storage_create_directory]\n    create_directory = CreateDirectoryOperator(\n        task_id=\"create_directory\", directory_path=FILE_STORAGE_DIRECTORY\n    )\n    # [END howto_operator azure_file_storage_create_directory]\n\n    # [START howto_operator azure_file_storage_list_directory]\n    list_directory = ListDirectoryOperator(task_id=\"list_directory\", directory_path=FILE_STORAGE_DIRECTORY)\n    # [END howto_operator azure_file_storage_list_directory]\n\n    @task\n    def delete_connections():\n        \"\"\"Delete connections\"\"\"\n        hook = ConnectionHook()\n        hook.delete_all(conn_id=CONNECTION_ID)\n\n    delete_connections_task = delete_connections()\n\n\nwith TaskGroup(\"file_storage_to_network_files_no_hook\"):  # type: ignore[attr-defined]\n    \"\"\"TaskGroup to demo AzureFileStorage to AzureNetworkFiles transfer with disabled external hooks\"\"\"\n    create_folder_no_hook = BashOperator(\n        task_id=\"create_folder_no_hook\",\n        bash_command=f\"mkdir {AZURE_NETWORK_FILES_FOLDER_NAME}\",\n    )\n\n    # [START howto_operator_azure_network_files_move_single_file_no_hook]\n    move_file_no_hook = BashOperator(\n        task_id=\"move_file_no_hook\",\n        bash_command=f\"mv {EXISTING_DIRECTORY}/test-file.txt {AZURE_NETWORK_FILES_FOLDER_NAME}/\",\n    )\n    # [END howto_operator_azure_network_files_move_single_file_no_hook]\n\n    delete_folder_no_hook = BashOperator(\n        task_id=\"delete_folder_no_hook\",\n        bash_command=f\"rmdir {AZURE_NETWORK_FILES_FOLDER_NAME} 2> /dev/null || true\",\n    )\n\n    # [START howto_operator azure_file_storage_create_directory_no_hook]\n    create_directory_no_hook = BashOperator(\n        task_id=\"create_directory_no_hook\",\n        bash_command=f\"mkdir {FILE_STORAGE_DIRECTORY}\",\n    )\n    # [END howto_operator azure_file_storage_create_directory_no_hook]\n\n    # [START howto_operator azure_file_storage_list_directory_no_hook]\n    list_directory_no_hook = BashOperator(\n        task_id=\"list_directory_no_hook\", bash_command=f\"ls {FILE_STORAGE_DIRECTORY}\"\n    )\n    # [END howto_operator azure_file_storage_list_directory_no_hook]\n\n    @task\n    def delete_connections_no_hook():\n        \"\"\"Delete connections\"\"\"\n        hook = ConnectionHook()\n        hook.delete_all(conn_id=CONNECTION_ID)\n\n    delete_connections_task_no_hook = delete_connections_no_hook()\n\n\nwith TriggerRule.ALL_DONE task_group:\n\n    # [START howto_operator_azure_file_storage_create_directory_trigger]\n    create_directory_trigger = BashOperator(\n        task_id=\"create_directory_trigger\",\n        bash_command=f\"mkdir {AZURE_NETWORK_FILES_FOLDER_NAME}\",\n    )\n    # [END howto_operator_azure_file_storage_create_directory_trigger]\n\n    # [START howto_operator_azure_file_storage_list_directory_trigger]\n    list_directory_trigger = BashOperator(\n        task_id=\"list_directory_trigger\", bash_command=f\"ls {AZURE_NETWORK_FILES_FOLDER_NAME}\"\n    )\n    # [END howto_operator_azure_file_storage_list_directory_trigger]\n\n    # [START howto_operator_azure_network_files_move_single_file_trigger]\n    move_file_trigger = BashOperator(\n        task_id=\"move_file_trigger\",\n        bash_command=f\"mv {EXISTING_DIRECTORY}/test-file.txt {AZURE_NETWORK_FILES_FOLDER_NAME}/\",\n    )\n    # [END howto_operator_azure_network_files_move_single_file_trigger]\n\n    # [START howto_operator_azure_network_files_create_directory_trigger]\n    create_directory_trigger = BashOperator(\n        task_id=\"create_directory_trigger\",\n        bash_command=f\"mkdir {FILE_STORAGE_DIRECTORY}\",\n    )\n    # [END howto_operator_azure_network_files_create_directory_trigger]\n\n    # [START howto_operator_azure_network_files_list_directory_trigger]\n    list_directory_trigger = BashOperator(\n        task_id=\"list_directory_trigger\", bash_command=f\"ls {FILE_STORAGE_DIRECTORY}\"\n    )\n    # [END howto_operator_azure_network_files_list_directory_trigger]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example tests with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run()"}], "metadata": {"file_name": "dag_1568_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated pipeline for managing sensitive data detection configurations that demonstrates how to programmatically handle data privacy templates across different stages of processing. The workflow will show how to create, inspect, and clean up data protection configurations with clear, sequential steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator\n\nSNAKE_NAME = \"SNOWFLAKE_SAMPLE_SALES\"\nSNOWFLAKE_PERSISTENSE = \"PERSISTENCE_TEST_SET\"\n\n\n@task\ndef create_snowflake_table():\n    # [START howto_operator_snowflake]\n    os.environ[\"SNOWFLAKE_SAMPLE_SALES_TABLE\"] = \"sales\"\n    os.environ[\"SNOWFLAKE_PERSISTENSE_TEST_SET\"] = SNOWFLAKE_PERSISTENSE\n\n    snowflake_to_table = SnowflakeOperator(\n        task_id=\"create_table_table\",\n        sql=f\"\"\"CREATE TABLE {os.environ[\"SNOWFLAKE_SAMPLE_SALES_TABLE\"]} AS\n        SELECT * FROM {SNAKE_NAME} LIMIT 100;\"\"\",\n    )\n    __ = snowflake_to_table.execute()\n    # [END howto_operator_snowflake]\n\n    return True\n\n\n@task\ndef drop_snowflake_table():\n    # [START howto_operator_snowflake]\n    snowflake_to_table = SnowflakeOperator(\n        task_id=\"drop_table_table\",\n        sql=f\"\"\"DROP TABLE IF EXISTS {os.environ[\"SNOWFLAKE_SAMPLE_SALES_TABLE\"]};\"\"\",\n    )\n    __ = snowflake_to_table.execute()\n    # [END howto_operator_snowflake]\n\n    return True\n\n\n@task\ndef create_s3_bucket():\n    # [START howto_operator_s3_to_snowflake]\n    s3_to_snowflake = S3ToSnowflakeOperator(\n        task_id=\"create_bucket\",\n        snowflake_table=os.environ[\"SNOWFLAKE_PERSISTENSE_TEST_SET\"],\n        s3_bucket=\"bucket_name\",\n    )\n    __ = s3_to_snowflake.execute()\n    # [END howto_operator_s3_to_snowflake]\n\n    return True\n\n\n@task\ndef delete_s3_bucket():\n    # [START howto_operator_s3_to_snowflake]\n    s3_to_snowflake = S3ToSnowflakeOperator(\n        task_id=\"drop_bucket\",\n        snowflake_table=os.environ[\"SNOWFLAKE_PERSISTENSE_TEST_SET\"],\n        s3_bucket=\"bucket_name\",\n    )\n    __ = s3_to_snowflake.execute()\n    # [END howto_operator_s3_to_snowflake]\n\n    return True\n\n\ndef main():\n    # [START howto_operator_s3_to_snowflake_trigger]\n    s3_to_snowflake = S3ToSnowflakeOperator(\n        task_id=\"drop_bucket\",\n        snowflake_table=os.environ[\"SNOWFLAKE_PERSISTENSE_TEST_SET\"],\n        s3_bucket=\"bucket_name\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_operator_s3_to_snowflake_trigger]\n\n    create_s3_bucket()\n    create_snowflake_table()\n    drop_snowflake_table()\n    delete_s3_bucket()\n    s3_to_snowflake\n\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"file_name": "dag_1605_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that captures and exports DynamoDB table data to S3 with precise time-based incremental export capabilities. Create a system that enables point-in-time data recovery and demonstrates advanced cloud data transfer techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDYNAMODB_TABLE_NAME = \"TestTable\"\nSAMPLE_DATA = r\"\"\"\n{\"ID\":1,\"Name\":\"One\",\"Score\":100}\n{\"ID\":2,\"Name\":\"Two\",\"Score\":99}\n{\"ID\":3,\"Name\":\"Three\",\"Score\":98}\n\"\"\"\n\n\n@task\ndef get_dynamodb_table():\n    return boto3.resource(\"dynamodb\").Table(DYNAMODB_TABLE_NAME)\n\n\n@task\ndef get_s3_bucket(bucket_name):\n    return boto3.resource(\"s3\").Bucket(bucket_name)\n\n\nwith DAG(\n    dag_id=\"dynamodb_to_s3_with_function\",\n    schedule=\"* * * * *\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    bucket_name = f\"{env_id}-dynamodb-to-s3-bucket\"\n    bucket = get_s3_bucket(bucket_name)\n    bucket_create = S3CreateBucketOperator(task_id=\"bucket_create\", bucket_name=bucket_name)\n\n    object_key = f\"{env_id}-dynamodb-to-s3-object\"\n    put_body = lambda: bucket.CreateObject(key=object_key, body=sample_data)\n\n    get_bucket = get_s3_bucket(bucket_name)\n    table_name = f\"{env_id}-dynamodb-table\"\n    dynamodb_create = boto3.resource(\"dynamodb\").Table(table_name)\n    dynamodb_create_item = lambda: dynamodb_create.put_item(Item={\"TableName\": table_name, \"AttributeDefinitions\": [\n        {\"AttributeName\": \"ID\", \"AttributeType\": \"N\"},\n        {\"AttributeName\": \"Name\", \"AttributeType\": \"S\"},\n        {\"AttributeName\": \"Score\", \"AttributeType\": \"N\"},\n    ], \"KeySchema\": [{\"AttributeName\": \"ID\", \"KeyType\": \"HASH\"}, {\"AttributeName\": \"Name\", \"KeyType\": \"R\"}], \"ProvisionedThroughput\": {\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10}})\n\n    wait_until_create_table_complete = lambda table: (\n        table.get_attribute(Name=\"CreationDate\") and table.get_attribute(Name=\"TableName\")\n    )\n\n    create_table = DynamoDBToS3Operator(\n        task_id=\"dynamodb_to_s3_with_function\",\n        table=dynamodb_create.Table(table_name),\n        bucket=get_bucket(),\n        object_key=object_key,\n        input_format=\"json\",\n        input_mapping_function_name=\"get_input_mapping_function\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = S3CreateBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=bucket_name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        test_context,\n        bucket_create,\n        put_body,\n        get_bucket,\n        dynamodb_create,\n        dynamodb_create_item,\n        wait_until_create_table_complete(dynamodb_create.Table(table_name)),\n        create_table,\n        delete_bucket,\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1886_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases Azure Container Instances deployment patterns, including launching standard Docker images, using custom container registries, and configuring persistent volume attachments. Develop a systematic approach to container deployment that highlights different configuration scenarios.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateFromTemplateOperator,\n    ContainerInstancesDeleteOperator,\n    ContainerInstancesStartContainerOperator,\n    ContainerInstancesStopContainerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_container_instances\"\n\n# [START howto_operator_container_instances_create_from_template]\n@task\ndef create_cluster(template_name: str):\n    # This container image is available in the docker hub and is recommended\n    # to use for production clusters, else any official container image is supported.\n    image = \"centos:7\"\n    custom_container = {\n        \"image_source\": {\"container_image\": image},\n        \"image_scoped\": True,\n        \"name\": \"MyContainer\",\n    }\n    location = \"uksouth\"\n    resource_group_name = \"my-resource-group\"\n    template_name = f\"{DAG_ID}-{ENV_ID}\"\n    waiters_wait_interval = 10\n    min_nodes = 1\n    max_nodes = 3\n\n    ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"create_cluster\",\n        location=location,\n        resource_group_name=resource_group_name,\n        template_name=template_name,\n        custom_container=custom_container,\n        min_nodes=min_nodes,\n        max_nodes=max_nodes,\n        waiters_wait_interval=waiters_wait_interval,\n    )\n\n\n# [END howto_operator_container_instances_create_from_template]\n\n# [START howto_operator_container_instances_stop_container]\n@task\ndef stop_container(template_name: str, resource_group_name: str, container_name: str):\n    ContainerInstancesStopContainerOperator(\n        task_id=\"stop_container\",\n        location=\"uksouth\",\n        resource_group_name=resource_group_name,\n        container_name=container_name,\n        template_name=template_name,\n    )\n\n\n# [END howto_operator_container_instances_stop_container]\n\n# [START howto_operator_container_instances_start_container]\n@task\ndef start_container(template_name: str, resource_group_name: str, container_name: str):\n    ContainerInstancesStartContainerOperator(\n        task_id=\"start_container\",\n        location=\"uksouth\",\n        resource_group_name=resource_group_name,\n        container_name=container_name,\n        template_name=template_name,\n    )\n\n\n# [END howto_operator_container_instances_start_container]\n\n# [START howto_operator_container_instances_delete]\n@task\ndef delete_cluster(template_name: str, resource_group_name: str):\n    ContainerInstancesDeleteOperator(\n        task_id=\"delete_cluster\",\n        location=\"uksouth\",\n        resource_group_name=resource_group_name,\n        container_name=\"MyContainer\",\n        template_name=template_name,\n    )\n\n\n# [END howto_operator_container_instances_delete]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[DAG_ID],\n):\n    create_cluster(template_name=\"cluster-template\") >> stop_container(template_name=\"cluster-template\", resource_group_name=\"my-resource-group\", container_name=\"MyContainer\")\n    start_container(template_name=\"cluster-template\", resource_group_name=\"my-resource-group\", container_name=\"MyContainer\") >> delete_cluster(template_name=\"cluster-template\", resource_group_name=\"my-resource-group\", container_name=\"MyContainer\")"}], "metadata": {"file_name": "dag_618_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data extraction workflow that performs a targeted graph database query to pull specific node attributes for Tom Hanks on a given date. Configure the pipeline to flexibly retrieve graph data using parameterized Cypher queries within the Airflow orchestration framework.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datajoin import (\n    DataJoinOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_sheets import MsSqlToGoogleSheetOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_mssql_to_sheets\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET = {\n    \"properties\": {\"title\": \"Test1\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Sheet1\"}}],\n}\nSQL_TABLE = \"table_with_mssql_sheets_example\"\nSQL_CONN_ID = \"mssql_default\"\nSHEET_ID = \"1234567890\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mssql_to_sheets\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_mssql_to_sheets_example():\n        gs_client = (\n            GoogleSheetsCreateSpreadsheetOperator(\n                task_id=\"create_gs_example\",\n                body=SPREADSHEET,\n                project_id=PROJECT_ID),\n        )\n        sheet_id = gs_client.result().get(\"spreadsheetId\")\n        create_instance = (\n            GoogleSheetsCreateSpreadsheetOperator(\n                task_id=\"create_instance\",\n                body=SPREADSHEET,\n                project_id=PROJECT_ID),\n        )\n        table = (\n            MsSqlToGoogleSheetOperator(\n                task_id=\"insert_mssql_to_sheets_example\",\n                sql=f\"SELECT * from {SQL_TABLE}\",\n                sql_conn_id=SQL_CONN_ID,\n                spreadsheet_id=sheet_id,\n                gcp_project=PROJECT_ID,\n                refer_to_spreadsheet=True,\n                force=True),\n        )\n        delete_sheet = GCSDeleteBucketOperator(\n            task_id=\"delete_gs_bucket\",\n            bucket_name=BUCKET_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    create_mssql_to_sheets_example()\n\n    DataJoinOperator(\n        task_id=\"join_tables\",\n        inputs=[\n            {\"spreadsheet_id\": f\"{SPREADSHEET.get('sheets')[0].get('properties').get('title')).split('/')[-1]\"},\n            {\"table\": f\"{SQL_TABLE}\"},\n        ],\n        outputs=[\"sheet_id\"],\n        body=dict(\n            sourceFields=[\"Sheet1!A:A\"],\n            destinationFields=[\"Sheet1!B:B\"],\n            joinMode=\"AllFields\",\n            destinationTable=f\"{SQL_TABLE}__joined\",\n        ),\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_gcp_sheets_insert_matrix]\n    GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet\",\n        body=SPREADSHEET,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_sheets_insert_matrix]\n\n    # [START howto_operator_gcp_sheets_read_matrix]\n    GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet2\",\n        body=SPREADSHEET,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_sheets_read_matrix]\n\n    MsSqlToGoogleSheetOperator(\n        task_id=\"insert_mssql_to_sheets_2\",\n        sql=f\"SELECT * from {SQL_TABLE}\",\n        sql_conn_id=SQL_CONN_ID,\n        spreadsheet_id=f\"{SPREADSHEET.get('sheets')[1].get('properties').get('title')).split('/')[-1]\",\n        gcp_project=PROJECT_ID,\n        refer_to_spreadsheet=True,\n        force=True,\n    )\n\n    # [START howto_operator_gcp_sheets_write_matrix]\n    GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet3\",\n        body=SPREADSHEET,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_sheets_write_matrix]\n\n    DataJoinOperator(\n        task_id=\"join_matrices\",\n        inputs=[\n            {\"spreadsheet_id\": f\"{SPREADSHEET.get('sheets')[0].get('properties').get('title')).split('/')[-1]\"},\n            {\"table\": f\"{SQL_TABLE}__joined\"},\n        ],\n        outputs=[\"sheet_id\"],\n        body=dict(\n            sourceFields=[\"Sheet1!A:A\"],\n            destinationFields=[\"Sheet1!B:B\"],\n            joinMode=\"AllFields\",\n            destinationTable=f\"{SQL_TABLE}__joined__joined\",\n        ),\n        project_id=PROJECT_ID,\n    )\n\n    [create_bucket, delete_bucket] >> create_mssql_to_sheets_example()"}], "metadata": {"file_name": "dag_1763_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to trigger a computational task on Azure Batch, showcasing cloud-based distributed computing for executing a simple shell command.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.batch import BatchCreateJobQueueOperator, BatchTriggerJobOperator\nfrom airflow.providers.microsoft.azure.utils import get_default_batch_env\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"example_batch_operator\"\n\nbatch_conn = Connection(\n    conn_id=\"batch\",\n    conn_type=\"azure_batch\",\n    schema=get_default_batch_env(),\n    params={\n        \"service_name\": \"mybatchservice\",\n    },\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_service_batch():\n        batch_conn.xcom_push(key=\"service_uri\", value=\"{{ task_instance.xcom_pull(task_ids='get_service_uri', key='return_value') }}\")\n\n    @task\n    def create_pool():\n        batch_conn.xcom_pull(key=\"service_uri\", task_ids=\"get_service_batch_uri\", task_id=\"get_service_batch_uri\")\n        batch_task = BatchCreateJobQueueOperator(\n            task_id=\"create_pool\",\n            service_name=\"mybatchservice\",\n            job_queue_name=\"pool-queue\",\n            pool_size=1,\n            target_worker_count=1,\n            minimal_reservations={\"node\": 1},\n            plugin_customizations={\n                \"azure_batch_node_agent\": {\n                    \"enabled\": True,\n                }\n                # Note: `azure_batch_compute_l_integration_package` is required to enable Spark.\n                    # If you plan on running jobs that use Spark, then this is something you will need to enable.\n                    # Otherwise, this task will fail with an InvalidRequestException: MissingRequiredFeature\n                    # exception.\n                    .get(\"spark\"),\n                    \"enabled\": True,\n                },\n            },\n        )\n\n    @task\n    def get_service_batch_uri():\n        return batch_conn.xcom_pull(\n            task_ids=\"create_service_batch\",\n            key=\"service_uri\",\n        )\n\n    @task\n    def get_service_uri():\n        return batch_conn.xcom_pull(\n            task_ids=\"create_service_batch\",\n            key=\"uri\",\n        )\n\n    create_pool()\n    trigger_job = BatchTriggerJobOperator(task_id=\"trigger_job\", job_id=\"{{ task_instance.xcom_pull(task_ids='create_pool', key='job_id') }}\")\n\n    create_service_batch()\n    trigger_job >> create_pool()\n\n    batch_conn.open()"}], "metadata": {"file_name": "dag_2153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates automated Google Compute Engine instance lifecycle management, including instance creation, remote command execution via SSH, and instance deletion. The DAG should showcase how to programmatically manage cloud compute resources using OS Login authentication.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.providers.ssh.operators.ssh_xcom import SSHXComBoeing\nfrom airflow.providers.ssh.transfers.copy_file import CopyFileSSHOperator\nfrom airflow.settings import Session\nfrom airflow.utils.edgemodifier import edge, label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_ssh_transfer\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_key():\n        key_name = f\"ssh-key-{DAG_ID}-{ENV_ID}\"\n        key = Connection(\n            conn_id=key_name,\n            conn_type=SSH,\n            description=\"Connection to AWS instance\",\n            login=\"user\",\n            password=\"password\",\n            private_key=\"/home/airflow/gcs/ssh-keys/compendium.pem\",\n        )\n        session = Session()\n        session.add(key)\n        session.commit()\n\n    key_task = create_key()\n\n    # [START howto_transfer_copy_file_ssh]\n    copy_file_task = CopyFileSSHOperator(\n        task_id=\"ssh-copy-file-task\",\n        source_path=\"/home/airflow/gcs/ssh-files/ssh-copy.sh\",\n        destination_path=\"/home/airflow/gcs/ssh-files/destination-copy.sh\",\n        ssh_conn_id=key_name,\n    )\n    # [END howto_transfer_copy_file_ssh]\n\n    # [START howto_transfer_xcom_asiainfrastructure]\n    xcom_task = SSHXComBoeing(\n        task_id=\"ssh-xcom-task\",\n        ssh_conn_id=key_name,\n        xcom_extra={\"dest_extra\": \"/home/airflow/gcs/ssh-files/remote-extra-copy.sh\"},\n    )\n    # [END howto_transfer_xcom_asiainfrastructure]\n\n    edge(copy_file_task, xcom_task, disconnect=True)\n\n    t1 = edge(label(\"create_key\"), label(\"copy_file_task\"))\n    t2 = edge(label(\"create_key\"), label(\"xcom_task\"))\n    t3 = edge(label(\"t1\"), label(\"t2\"))\n    t4 = edge(label(\"t3\"), label(\"disconnect\"))\n    t5 = edge(label(\"t1\"), label(\"t4\"))\n    t6 = edge(label(\"t2\"), label(\"t4\"))\n    t7 = edge(label(\"t5\"), label(\"t6\"))\n\n    [t1, t2, t3, t4, t5, t6] >> [t7]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(watcher(dag=dag))\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_838_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of dynamically generating a local file and transferring it to AWS S3 using predefined operators and task chaining. The DAG should highlight task dependencies, error handling, and cloud storage interaction.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.base.operators.bash import BashOperator\nfrom airflow.providers.common.base.utils import make_temp_directory\nfrom airflow.providers.common.file_transfer.operators.s3_to_local import S3ToLocalOperator\nfrom airflow.providers.common.file_transfer.transfers.s3_to_local import S3ToLocalTransfer\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_local\"\n\ntemp_dir = make_temp_directory()\n\nconn = Connection(\n    conn_id=\"s3_to_local_transfer\",\n    conn_type=\"s3\",\n    host=\"s3.amazonaws.com\",\n    login=\"akia11yis7oz5x29697z\",\n    password=\"QdFq5o6jb3bCZw5nQ6t3t4c5QdFq5o6jb\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"max_active_runs\": 1},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_s3_to_local_transfer():\n        S3ToLocalTransfer(\n            conn_id=\"s3_to_local_transfer\",\n            s3_bucket=\"airflow-examples-s3\",\n            s3_key=\"s3_to_local_transfer\",\n            dest_path=temp_dir,\n            task_id=\"create_s3_to_local_transfer\",\n        )\n\n    create_s3_to_local_transfer_task = create_s3_to_local_transfer()\n\n    # [START howto_transfer_s3_to_local]\n    transfer_s3_to_local = S3ToLocalTransfer(\n        conn_id=\"s3_to_local_transfer\",\n        s3_bucket=\"airflow-examples-s3\",\n        s3_key=\"s3_to_local_transfer\",\n        dest_path=temp_dir,\n        task_id=\"transfer_s3_to_local\",\n    )\n    # [END howto_transfer_s3_to_local]\n\n    delete_temp_directory = BashOperator(\n        task_id=\"delete_temp_directory\",\n        bash_command=f\"rm -rf {temp_dir}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    BashOperator(\n        task_id=\"delete_connection\",\n        bash_command=f\"airflow connections delete {conn.id}\",\n    )\n\n    (\n        # TEST SETUP\n        create_s3_to_local_transfer_task\n        >> transfer_s3_to_local\n        # TEST BODY\n        >> delete_temp_directory\n        # TEST TEARDOWN\n        >> delete_connection\n    )"}], "metadata": {"file_name": "dag_402_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical workflow of provisioning and managing cloud infrastructure using Google Cloud Composer operators. Develop a step-by-step process that demonstrates environment configuration, version listing, and operational state management for cloud data processing platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.composer import GoogleCloudComposerHook\nfrom airflow.providers.google.cloud.operators.composer import (\n    ComposerCreateEnvironmentOperator,\n    ComposerDeleteEnvironmentOperator,\n    ComposerGetEnvironmentOperator,\n    ComposerListEnvironmentsOperator,\n    ComposerListVersionBackupsOperator,\n    ComposerListVersionsOperator,\n    ComposerRestoreDatabaseOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"composer_operator\"\n\nCONNECTION_ID = \"google_cloud_platform\"\n\nENVIRONMENT_NAME = f\"env-{ENV_ID}\"\n\nBALANCER_CONFIG_URI = \"gs://airflow-system-tests-resources/composer-balancer.yaml\"\nDB_TYPE = \"postgres\"\nDB_NAME = \"test_db\"\nDB_USER = \"test_db_user\"\nDB_PASSWORD = \"test_db_password\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"composer\", \"example\"],\n) as dag:\n    @task\n    def create_connection(connection_id: str):\n        connection = Connection(\n            conn_id=connection_id,\n            conn_type=\"google_cloud_platform\",\n        )\n        conn_extra = {\n            \"project_id\": PROJECT_ID,\n            \"environment_id\": ENVIRONMENT_NAME,\n            \"db_type\": DB_TYPE,\n            \"db_name\": DB_NAME,\n            \"db_user\": DB_USER,\n            \"db_password\": DB_PASSWORD,\n            \"balancer_config_uri\": BALANCER_CONFIG_URI,\n        }\n        connection_extra = {\n            \"extra_configurations\": [\n                {\n                    \"conf\": \"hdfs-site\",\n                    \"extra\": \"file,gzip\",\n                }\n            ]\n        }\n        connection.set_extra(conn_extra)\n        return connection\n\n    create_connection_task = create_connection(connection_id=CONNECTION_ID)\n\n    create_environment = ComposerCreateEnvironmentOperator(\n        task_id=\"create_environment\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n        region=\"us-central1\",\n        db_type=DB_TYPE,\n        db_name=DB_NAME,\n        db_user=DB_USER,\n        db_password=DB_PASSWORD,\n        balancer_config_uri=BALANCER_CONFIG_URI,\n        backup_storage_label_id=\"my-label-id\",\n    )\n\n    get_environment = ComposerGetEnvironmentOperator(\n        task_id=\"get_environment\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n    )\n\n    list_environments = ComposerListEnvironmentsOperator(\n        task_id=\"list_environments\",\n        project_id=PROJECT_ID,\n    )\n\n    list_versions = ComposerListVersionsOperator(\n        task_id=\"list_versions\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n    )\n\n    list_version_backups = ComposerListVersionBackupsOperator(\n        task_id=\"list_version_backups\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n    )\n\n    restore_db = RestoreDatabaseOperator(\n        task_id=\"restore_db\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n        db_type=DB_TYPE,\n        db_name=DB_NAME,\n        db_user=DB_USER,\n        db_password=DB_PASSWORD,\n    )\n\n    delete_environment = ComposerDeleteEnvironmentOperator(\n        task_id=\"delete_environment\",\n        project_id=PROJECT_ID,\n        environment_id=ENVIRONMENT_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_connection = AirflowConnectOperator(\n        task_id=\"delete_connection\",\n        connection_id=CONNECTION_ID,\n    )\n\n    (\n        create_connection_task\n        >> create_environment\n        >> get_environment\n        >> list_environments\n        >> list_versions\n        >> list_version_backups\n        >> restore_db\n        >> [delete_environment, delete_connection]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the temp setup\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_494_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow workflow that simulates a multi-stage data processing scenario using Python operators with different runtime environments. Create a pipeline that shows how tasks can be executed in standard, virtual, and external Python contexts while maintaining a clear task dependency structure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import (\n    ExternalPythonOperator,\n    PythonOperator,\n    PythonVirtualEnvironmentOperator,\n)\nfrom airflow.utils.context import Context\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Print the date\"\n    print(pendulum.datetime(now().year, now().month, now().day))\n\n\n@task(task_id=\"print_date_in_virtual_environment\")\ndef print_date_in_virtual_environment():\n    \"Print the date in a virtual environment\"\n    print(pendulum.datetime(now().year, now().month, now().day))\n\n\n@task(task_id=\"print_date_absolute_path\", task_kwargs={\"python_callable\": print_date})\ndef print_date_absolute_path():\n    \"Print the date using the absolute path of the BashOperator\"\n    print(pendulum.datetime(now().year, now().month, now().day))\n\n\n@task(task_id=\"print_date_with_virtual_environment_absolute_path\")\ndef print_date_with_virtual_environment_absolute_path():\n    \"Print the date using the absolute path of the BashOperator in a virtual environment\"\n    print(pendulum.datetime(now().year, now().month, now().day))\n\n\nwith DAG(\n    \"print_date_in_different_context\",\n    schedule=\"* * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1),\n    catchup=False,\n):\n    #\n    print_date()\n    BashOperator(task_id=\"print_date_bash_operator\", bash_command=\"echo $ENV_ID\")\n    #\n    print_date_in_virtual_environment()\n    BashOperator(\n        task_id=\"print_date_in_virtual_environment\",\n        bash_command=\"echo $ENV_ID\",\n        venv=\"/Users/airflow/gcs/venvs/python_3.8.5\",\n    )\n    #\n    print_date_absolute_path()\n    BashOperator(task_id=\"print_date_absolute_path\", bash_command=\"/bin/date\")\n    #\n    print_date_with_virtual_environment_absolute_path()\n    BashOperator(\n        task_id=\"print_date_with_virtual_environment_absolute_path\",\n        bash_command=\"/bin/date\",\n        venv=\"/Users/airflow/gcs/venvs/python_3.8.5\",\n    )"}], "metadata": {"file_name": "dag_1809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data privacy transformation pipeline that uses AWS Comprehend to automatically identify and anonymize sensitive personal information in unstructured text data. Create a workflow that takes raw data, applies intelligent PII masking, and generates a redacted output file.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.client import BaseClient\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.comprehend import AmazonComprehendHook\nfrom airflow.providers.amazon.aws.operators.comprehend import (\n    AmazonComprehendStartPiiEntitiesDetectionJobOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.comprehend import AmazonComprehendPiiEntitiesDetectionJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlogger = logging.getLogger(__name__)\n\n# Create a unique name for the DAG and a builder to create connections for the test\nDAG_ID = \"example_comprehend_pii_entities\"\ntest_context = SystemTestContextBuilder().build()\n\n# Define a function to get the ARN of a resource. This example looks for a resource\n# with the logical ID \"DynamoDbTable\" and returns its ARN.\ndef get_resource_arn(resource_arns, resource_ild):\n    \"\"\"\n    Returns the ARN of the first resource with the logical ID of `resource_ild`.\n    \"\"\"\n    return next(resource_arn for resource_arn in resource_arns if resource_arn.split(\"/\")[-1] == resource_ild)\n\n\n@task\ndef create_pii_entities_job_output(output_path: str):\n    \"\"\"\n    Creates an Amazon Comprehend PII Entities Detection Job Output. \"\"\"\n    logging.info(f\"Creating PII Entities Job Output at path {output_path}...\")\n    return {\"Path\": output_path}\n\n\n@task\ndef create_pii_entities_job(job_name: str, input_data_type: str, input_location: str, output_location: str):\n    \"\"\"\n    Creates an Amazon Comprehend PII Entities Detection Job. \"\"\"\n    logging.info(f\"Creating PII Entities Job {job_name}...\")\n    response = AmazonComprehendHook().startPiiEntitiesDetectionJob(\n        JobName=job_name,\n        InputData={\n            \"InputDataType\": input_data_type,\n            \"InputLocation\": {\"S3Location\": input_location},\n        },\n        OutputData={\"OutputDataType\": \"MULTI_CLASS\", \"S3Location\": output_location},\n    )\n    response[\"JobId\"]\n    logging.info(f\"PII Entities Job {job_name} started. Job ID is {response['JobId']}\")\n\n\n@task\ndef wait_for_pii_entities_job_sensor(job_id: str, hook: BaseClient):\n    \"\"\"\n    Wait for the PII Entities Job to complete. \"\"\"\n    logging.info(f\"Waiting for PII Entities Job {job_id} to complete...\")\n    sensor = AmazonComprehendPiiEntitiesDetectionJobSensor(\n        job_id=job_id, hook=hook\n    )\n    hook.get_pii_entities_detection_job_job_id(job_id)\n    sensor.wait_for_status(success_reason=\"JOB_SUCCESS_REASON\", timeout=60 * 60)\n\n\n@task\ndef delete_pii_entities_job(job_name: str, hook: BaseClient):\n    \"\"\"\n    Deletes an Amazon Comprehend PII Entities Detection Job with the specified job name. \"\"\"\n    logging.info(f\"Deleting PII Entities Job {job_name}...\")\n    hook.delete_pii_entities_detection_job(JobName=job_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = test_context.builder()\n    env_id = test_context.env_id\n\n    input_data = {\n        \"\u5e73\u539f\u8015\u5730\": \"PLAIN_FARM\",\n        \"\u9ad8\u901f\u516c\u8def\": \"HIGHWAY\",\n        \"\u57ce\u5e02\u5316\u7528\u5730\": \"U_R_BLDG\",\n        \"\u852c\u83dc\u5927\u68da\": \"V_GROUND_HARVEST\",\n        \"\u6cb3\u6d41\": \"RIVER\",\n        \"\u5de5\u4e1a\u7528\u5730\": \"INDUSTRY\",\n        \"\u516c\u56ed\": \"PARK\",\n        \"\u704c\u6e20\": \"DITCH\",\n        \"\u6797\u5730\": \"FOREST\",\n        \"\u94c1\u8def\": \"RAILWAY\",\n        \"\u704c\u6728\u6797\": \"SHRUBLAND\",\n        \"\u6c34\u57df\": \"WATER\",\n        \"\u6e56\u6cca\": \"LAKE\",\n        \"\u8015\u5730\": \"FARM\",\n        \"\u6e7f\u5730\": \"WETLAND\",\n        \"\u57ce\u5e02\u7528\u5730\": \"U_R_BLDG\",\n        \"\u7279\u6b8a\u7528\u5730\": \"OTHER\",\n        \"\u6c34\u57df\u8bbe\u65bd\": \"FACILITY_WATERS\",\n        \"\u4e54\u6728\u6797\": \"TREELAND\",\n        \"\u672a\u5229\u7528\u5730\": \"NONE_USE\",\n        \"\u4ea4\u901a\u7528\u5730\": \"T_R_BLDG\",\n        \"\u5efa\u8bbe\u7528\u5730\": \"BUILDING\",\n        \"\u4e61\u6751\u7528\u5730\": \"R_BLDG\",\n        \"\u519c\u7530\": \"FIELD\",\n        \"\u5176\u4ed6\": \"OTHER\",\n    }\n\n    raw_output_path = f\"s3://{env_id}/pii_entities_job_output/\"\n    redacted_output_path = f\"s3://{env_id}/redacted_pii_entities_job_output/\"\n    output_connection = Connection(\n        conn_type=\"s3\",\n        host=\"\",\n        schema=\"s3\",\n        conn_id=\"aws_default\",\n        region_name=\"us-east-1\",\n    )\n\n    # [START howto_operator_amazon_comprehend_start_pii_entities_detection_job]\n    start_pii_entities_job = AmazonComprehendStartPiiEntitiesDetectionJobOperator(\n        task_id=\"start_pii_entities_job\",\n        input_data_type=\"MULTI_CLASS\",\n        input_location=raw_output_path,\n        output_location=redacted_output_path,\n        job_name=f\"pii_entities_job_{env_id}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_amazon_comprehend_start_pii_entities_detection_job]\n\n    # [START howto_sensor_amazon_comprehend_pii_entities_detection_job]\n    wait_for_pii_entities_job = AmazonComprehendPiiEntitiesDetectionJobSensor(\n        job_id=start_pii_entities_job.output,\n        task_id=\"wait_for_pii_entities_job\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_sensor_amazon_comprehend_pii_entities_detection_job]\n\n    # Add relationships\n    start_pii_entities_job >> wait_for_pii_entities_job\n    wait_for_pii_entities_job_output = create_pii_entities_job_output(test_context.pii_entities_job_output_path)\n    create_pii_entities_job(\"pii_entities_job_\" + env_id, \"DynamoDbTable\", \"s3://airflow-system-tests-dynamodb/\"))\n    create_pii_entities_job(\n        \"pii_entities_job_\" + env_id + \"_2\", \"DynamoDbTable2\", \"s3://airflow-system-tests-dynamodb/2\"\n    )\n    wait_for_pii_entities_job >> delete_pii_entities_job(test_context.pii_entities_job_name)\n    wait_for_pii_entities_job >> get_resource_arn(\n        test_context.resource_ild_1_arn, \"DynamoDbTable\"\n    )\n    wait_for_pii_entities_job >> get_resource_arn(\n        test_context.resource_ild_2_arn, \"DynamoDbTable2\"\n    )\n    wait_for_pii_entities_job_output >> delete_pii_entities_job_output(\n        test_context.pii_entities_job_output_path\n    )\n    wait_for_pii_entities_job_output >> os.remove(test_context.pii_entities_job_output_path)\n    wait_for_pii_entities_job_output >> os.remove(test_context.pii_entities_job_output_path2)\n\n    # Add dependencies\n    [output_connection, system_test_context] >> test_context\n\n\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark it (correction of) success / failure\n# when execution is stopped with `avro_annotate_stop()`\nlist(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1080_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer and external table creation, focusing on the technical orchestration of moving metadata between Trino and Google BigQuery. Develop a step-by-step pipeline that demonstrates cloud data integration techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.trino_to_bigquery import TrinoToBigQueryOperator\nfrom airflow.providers.trino.operators.trino import TrinoQueryOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_to_bigquery\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"project\": \"airflow-system-tests\"},\n    schedule=None,\n    catchup=False,\n) as dag:\n    # [START howto_operator_trino_to_bigquery]\n    trino_to_bigquery_task = TrinoToBigQueryOperator(\n        task_id=\"trino_to_bigquery_task\",\n        conn_id=\"google_cloud_default\",\n        sql=\"SELECT 1 AS id FROM PLU WHERE store_id = 3000\",\n        destination_table=\"bigquery_table\",\n        schema_fields=[\n            (\"id\", \"INTEGER\"),\n        ],\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_operator_trino_to_bigquery]\n\n    # [START howto_operator_trino]\n    trino_task = TrinoQueryOperator(\n        task_id=\"trino_task\",\n        conn_id=\"google_cloud_default\",\n        sql=\"SELECT 1 AS id FROM PLU WHERE store_id = 3000\",\n        split_statement=False,\n    )\n    # [END howto_operator_trino]\n\n    trino_to_bigquery_task >> trino_task\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed or as failed)\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1275_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python program to simulate the rolling of a six-sided die using the `random` module. The program should prompt the user to input the number of times the die should be rolled and then generate random numbers between 1 and 6 for each roll, displaying the result of each roll. Additionally, the program should calculate and display the frequency of each outcome.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "import random\n\ndef roll_dice(num_rolls):\n    \"\"\"\n    Simulate rolling a six-sided die a specified number of times.\n\n    Args:\n    num_rolls (int): The number of times the die should be rolled.\n\n    Returns:\n    list: A list of integers representing the outcomes of each roll.\n    \"\"\"\n    outcomes = [random.randint(1, 6) for _ in range(num_rolls)]\n    return outcomes\n\ndef display_outcomes(outcomes):\n    \"\"\"\n    Display the outcomes of the die rolls.\n\n    Args:\n    outcomes (list): A list of integers representing the outcomes of each roll.\n    \"\"\"\n    print(\"Rolls:\", outcomes)\n    print(\"Frequencies:\", [outcomes.count(outcome) for outcome in set(outcomes)])\n\ndef main():\n    num_rolls = int(input(\"Enter the number of times you want to roll the die: \"))\n    outcomes = roll_dice(num_rolls)\n    display_outcomes(outcomes)\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I calculate the minimum pairwise Euclidean distance between points in a dataset using Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_python_decorator():\n    # [START system_test]\n    @task\n    def system_test():\n        print(\"My name is Euler and I am a system test!\")\n        return 6\n\n    # [END system_test]\n\n    @task\n    def get_distances(csv_file):\n        import pandas as pd\n        import math\n\n        data = pd.read_csv(csv_file)\n        result = []\n\n        for i, row1 in data.iterrows():\n            min_distance = float(\"inf\")\n            for j, row2 in data.iterrows():\n                if i == j:\n                    continue\n                dist = math.sqrt((row1[0] - row2[0]) ** 2 + (row1[1] - row2[1]) ** 2)\n                if dist < min_distance:\n                    min_distance = dist\n                    min_j = j\n            result.append([min_distance, data.loc[min_j, \"name\"]])\n\n        import csv\n\n        with open(\"output.csv\", \"w\") as output_file:\n            writer = csv.writer(output_file)\n            writer.writerows(result)\n\n    @task\n    def check_distance():\n        with open(\"output.csv\") as f:\n            lines = f.readlines()\n        print(lines)\n\n    # [START create_dag]\n    dag = example_python_decorator()\n    # [END create_dag]\n\n    # [START run_operator]\n    system_test() >> get_distances(\"test.csv\") >> check_distance()\n    # [END run_operator]\n\n\nexample_python_decorator()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases automated cloud storage interactions, including bucket creation, file transfer between cloud providers, and transfer job management. Develop a demonstration of cloud service integration using Google Cloud Transfer Service and AWS S3 operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_aws_s3 import S3ToAWS S3ToAWSOperator\nfrom airflow.providers.google.cloud.hooks.s3 import S3Hook as GoogleS3Hook\nfrom airflow.providers.google.cloud.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_s3 import S3ToS3Operator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_s3_to_aws_s3\"\n\nS3_BUCKET = f\"bucket_{DAG_ID}_{ENV_ID}\"\nS3_BUCKET_2 = f\"bucket_{DAG_ID}_{ENV_ID}_2\"\nS3_KEY = \"example_file.txt\"\nGCP_BUCKET = f\"bucket_gcp_{ENV_ID}\"\nGCP_OBJECT = \"example_file_2.txt\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_connections():\n        conn = Connection(\n            conn_id=\"s3\",\n            conn_type=\"s3\",\n            extra={\"bucket\": S3_BUCKET},\n        )\n        conn2 = Connection(\n            conn_id=\"aws_s3\",\n            conn_type=\"s3\",\n            extra={\"bucket\": GCP_BUCKET},\n        )\n        conn3 = Connection(\n            conn_id=\"google_s3\",\n            conn_type=\"s3\",\n            extra={\"bucket\": GCP_BUCKET},\n        )\n        os.environ[\"AWS_ACCESS_KEY_ID\"] = \"TEST_ACCESS_KEY\"\n        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"TEST_SECRET_KEY\"\n        os.environ[\"GCP_PROJECT_ID\"] = GCP_PROJECT_ID\n        return [conn, conn2, conn3]\n\n    create_connections_task = create_connections()\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=S3_BUCKET)\n\n    @task\n    def list_s3_buckets():\n        return [S3_BUCKET]\n\n    list_s3_buckets_task = list_s3_buckets()\n\n    @task\n    def create_s3_bucket_2():\n        conn = Connection(\n            conn_id=\"s3\",\n            conn_type=\"s3\",\n            extra={\"bucket\": S3_BUCKET_2},\n        )\n        os.environ[\"AWS_ACCESS_KEY_ID\"] = \"TEST_ACCESS_KEY\"\n        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"TEST_SECRET_KEY\"\n        return conn\n\n    create_s3_bucket_2_task = create_s3_bucket_2()\n\n    @task\n    def list_s3_buckets_2():\n        return [S3_BUCKET_2]\n\n    list_s3_buckets_2_task = list_s3_buckets_2()\n\n    copy_file_over_http = S3ToS3Operator(\n        task_id=\"copy_file_over_http\",\n        bucket_src=S3_BUCKET,\n        key_src=S3_KEY,\n        bucket_dest=GCP_BUCKET,\n        key_dest=GCP_OBJECT,\n        replace=True,\n    )\n\n    # [START howto_transfer_s3_to_aws_s3]\n    copy_file_to_aws = S3ToAWSOperator(\n        task_id=\"copy_file_to_aws\",\n        source=S3_BUCKET,\n        dest_=GCP_BUCKET,\n        # 's3_to_aws_s3' transfer operator requires destination bucket to exist\n        dest_key=GCP_OBJECT,\n        replace=True,\n    )\n    # [END howto_transfer_s3_to_aws_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=S3_BUCKET,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket_2 = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_2\",\n        bucket_name=S3_BUCKET_2,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clean_connections = BashOperator(\n        task_id=\"clean_connections\",\n        bash_command=f\"airflow cleanup connections -f -t s3 -t {GCP_BUCKET} --project={GCP_PROJECT_ID}\",\n    )\n\n    clean_connections_task2 = BashOperator(\n        task_id=\"clean_connections_task2\",\n        bash_command=f\"airflow cleanup connections -f -t {GCP_BUCKET} --project={GCP_PROJECT_ID}\",\n    )\n\n    list_gcp_buckets = GoogleS3Hook().list_buckets()\n\n    compare_list_contents = BashOperator(\n        task_id=\"compare_list_contents\",\n        bash_command=f\"diff <(echo {S3_BUCKET}) <(echo {GCP_BUCKET})\",\n    )\n\n    show_conn_s3 = BashOperator(\n        task_id=\"show_conn_s3\",\n        bash_command=f\"airflow show connection s3\",\n    )\n\n    show_conn_s3_2 = BashOperator(\n        task_id=\"show_conn_s3_2\",\n        bash_command=f\"airflow show connection {GCP_BUCKET}\",\n    )\n\n    show_s3_bucket = BashOperator(\n        task_id=\"show_s3_bucket\",\n        bash_command=f\"airflow logs s3\",\n    )\n\n    show_s3_bucket_2 = BashOperator(\n        task_id=\"show_s3_bucket_2\",\n        bash_command=f\"airflow logs {GCP_BUCKET}\",\n    )\n\n    show_s3_bucket_3 = BashOperator(\n        task_id=\"show_s3_bucket_3\",\n        bash_command=f\"airflow logs s3 --project-id {GCP_PROJECT_ID}\",\n    )\n\n    show_s3_bucket_4 = BashOperator(\n        task_id=\"show_s3_bucket_4\",\n        bash_command=f\"airflow logs {GCP_BUCKET} --project-id {GCP_PROJECT_ID}\",\n    )\n\n    show_s3_bucket_5 = BashOperator(\n        task_id=\"show_s3_bucket_5\",\n        bash_command=f\"airflow logs {GCP_BUCKET} --project {GCP_PROJECT_ID} --project-group my_group\",\n    )\n\n    group = DAG(\n        \"group\",\n        schedule=\"@once\",\n        start_date=datetime(2021, 1, 1),\n        catchup=False,\n        tags=[\"example\"],\n    )\n\n    task_group_1 = DAG(\"task_group_1\", schedule=\"@once\", start_date=datetime(2021, 1, 1))\n    task_group_2 = DAG(\"task_group_2\", schedule=\"@once\", start_date=datetime(2021, 1, 1))\n    task_group_3 = DAG(\"task_group_3\", schedule=\"@once\", start_date=datetime(2021, 1, 1))\n    task_group_4 = DAG(\"task_group_4\", schedule=\"@once\", start_date=datetime(2021, 1, 1))\n    task_group_5 = DAG(\"task_group_5\", schedule=\"@once\", start_date=datetime(2021, 1, 1))\n\n    create_connections_task >> create_s3_bucket\n    create_s3_bucket_2_task >> create_s3_bucket\n    list_s3_buckets_task >> list_s3_buckets_2_task\n\n    # [START howto_operator_transfer_s3_to_s3]\n    copy_file_to_s3 = S3ToS3Operator(\n        task_id=\"copy_file_to_s3\",\n        bucket_src=GCP_BUCKET,\n        key_src=GCP_OBJECT,\n        bucket_dest=S3_BUCKET,\n        key_dest=S3_KEY,\n        replace=True,\n    )\n    # [END howto_operator_transfer_s3_to_s3]\n\n    [clean_connections(), delete_s3_bucket, delete_s3_bucket_2, group, task_group_1, task_group_2, task_group_3, task_group_4, task_group_5]\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1683_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the sequential execution of different Spark operators, showing how complex data workflows can be orchestrated using modular task dependencies. Develop a demonstration pipeline that highlights task chaining and different Spark processing capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Print the date\"\n    print(pendulum.datetime now())\n\n\n@task(task_id=\"print_time\")\ndef print_time():\n    \"Print the current time\"\n    print(pendulum.time now())\n\n\n@task(task_id=\"print_day_of_week\")\ndef print_day_of_week():\n    \"Print the current day of the week\"\n    print(pendulum.day_of_week now())\n\n\n@task(task_id=\"print_day_of_month\")\ndef print_day_of_month():\n    \"Print the current day of the month\"\n    print(pendulum.day_of_month now())\n\n\n@task(task_id=\"print_month\")\ndef print_month():\n    \"Print the current month\"\n    print(pendulum.month now())\n\n\n@task(task_id=\"print_year\")\ndef print_year():\n    \"Print the current year\"\n    print(pendulum.year now())\n\n\n@task(task_id=\"print_quarter\")\ndef print_quarter():\n    \"Print the current quarter\"\n    print(pendulum.quarter now())\n\n\n@task(task_id=\"print_week_of_year\")\ndef print_week_of_year():\n    \"Print the current week of the year\"\n    print(pendulum.week_of_year now())\n\n\n@task(task_id=\"print_week_of_month\")\ndef print_week_of_month():\n    \"Print the current week of the month\"\n    print(pendulum.week_of_month now())\n\n\n@task(task_id=\"print_weekday_of_month\")\ndef print_weekday_of_month():\n    \"Print the current weekday of the month\"\n    print(pendulum.weekday_of_month now())\n\n\n@task(task_id=\"print_quarter_of_month\")\ndef print_quarter_of_month():\n    \"Print the current quarter of the month\"\n    print(pendulum.quarter_of_month now())\n\n\n@task(task_id=\"print_day_of_year\")\ndef print_day_of_year():\n    \"Print the current day of the year\"\n    print(pendulum.day_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Print the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month now())\n\n\n@task(task_id=\"print_day_of_quarter\")\ndef print_day_of_quarter():\n    \"Print the current day of the quarter\"\n    print(pendulum.day_of_quarter now())\n\n\n@task(task_id=\"print_day_of_year_of_month\")\ndef print_day_of_year_of_month():\n    \"Print the current day of the year of the month\"\n    print(pendulum.day_of_year_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Print the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year now())\n\n\n@task(task_id=\"print_day_of_quarter_of_year\")\ndef print_day_of_quarter_of_year():\n    \"Print the current day of the quarter of the year\"\n    print(pendulum.day_of_quarter_of_year now())\n\n\n@task(task_id=\"print_day_of_month_of_year\")\ndef print_day_of_month_of_year():\n    \"Print the current day of the month of the year\"\n    print(pendulum.day_of_month_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_month\")\ndef print_day_of_week_of_year_of_month():\n    \"Print the current day of the week of the year of the month\"\n    print(pendulum.day_of_week_of_year_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter\")\ndef print_day_of_week_of_year_of_quarter():\n    \"Print the current day of the week of the year of the quarter\"\n    print(pendulum.day_of_week_of_year_of_quarter now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month\")\ndef print_day_of_week_of_year_of_quarter_of_month():\n    \"Print the current day of the week of the year of the quarter of the month\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_year():\n    \"Print the current day of the week of the year of the quarter of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_year():\n    \"Print the current day of the week of the year of the quarter of the month of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_month\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_year_of_month():\n    \"Print the current day of the week of the year of the quarter of the month of the month\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_month():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the month\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_year():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the quarter\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter_of_month\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_month_of_month():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the quarter of the month\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_month_of_month now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_year():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the quarter of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter_of_year_of_year\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_year_of_year():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the quarter of the year of the year\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_year_of_year now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter\")\ndef print_day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_quarter():\n    \"Print the current day of the week of the year of the quarter of the month of the quarter of the year of the quarter of the year of the quarter\"\n    print(pendulum.day_of_week_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_quarter now())\n\n\n@task(task_id=\"print_day_of_week_of_year_of_quarter_of_month_of_year_of_quarter_of_month_of_quarter_of_year_of_quarter_of_month\")\ndef print_day_of_week_of_year"}], "metadata": {"file_name": "dag_1147_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that simulates a controlled delay scenario, demonstrating how asynchronous time-based sensors can manage workflow timing without blocking computational resources. Create a simple pipeline that waits for a predetermined time interval before proceeding to the next task.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"print_time\")\ndef print_time(delay_seconds: float):\n    print(pendulum.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n    print(\"Sleeping for {} seconds ...\".format(delay_seconds))\n\n\n@task(task_id=\"wait_time\")\ndef wait_time():\n    print(\"Waiting...\")\n    yield \"one\"\n\n\nwith DAG(\n    dag_id=\"example_time_sensor\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    tags=[\"example\"],\n    catchup=False,\n    default_args={\"max_active_runs\": 1},\n):\n    print_time(task_id=\"print_time\")\n    wait_time()"}], "metadata": {"file_name": "dag_30_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Redis for real-time message coordination and signaling between different stages of a workflow. Create a system where one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\ntry:\n    from airflow.providers.redis.operators.redis import RedisStandaloneOperator\nexcept ImportError:\n    from airflow.providers.standard.operators.redis import RedisStandaloneOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"redis_standalone\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo 1\")\n\n    # [START howto_operator_redis_standalone]\n\n    task_to_task = RedisStandaloneOperator(\n        task_id=\"wait_for_task\",\n        redis_conn_id=\"redis_default\",\n        keyword_params={\n            \"command\": \"sleep 10\",\n            \"timeout\": 60,\n        },\n        op_kwargs={\" dagrun_timeout\": 120},\n    )\n\n    # [END howto_operator_redis_standalone]\n\n    task_to_task2 = BashOperator(task_id=\"run_me2\", bash_command=\"echo 1\")\n\n    connection = Connection(\n        conn_id=\"redis_default\",\n        conn_type=\"redis\",\n        host=\"127.0.0.1\",\n        login=\"\",\n        password=\"\",\n        port=6379,\n    )\n\n    run_this >> task_to_task >> task_to_task2\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as success/failure\n    # when \"tearDown\" task with trigger rule of 'all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_642_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated image object detection training system that processes a predefined dataset and trains a machine learning model with configurable training parameters. Implement a scalable workflow that supports data ingestion, model training, and automated resource management in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.cncf.junit.hooks.jupyter_notebook import JupyterNotebookHook\nfrom airflow.providers.cncf.junit.sensors.jupyter_notebook import JupyterNotebookSensor\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.openlineage.operator import OpenLineageTestOperator\nfrom system.test_utils.watcher import watcher\n\nDAG_ID = \"example_jupyter_notebook\"\n\n# [START howto_configure_jupyter_notebook_hook]\nJUPYTER_NOTEBOOK_CONNECTION_KEY = \"jupyter_notebook_default\"\n\nhook = JupyterNotebookHook()\n# [END howto_configure_jupyter_notebook_hook]\n\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_notebook():\n        conn = Session().query(Connection).filter(Connection.url == \"http://jupyter_notebook:8888\").first()\n        if conn is None:\n            conn = Connection(\n                conn_id=JUPYTER_NOTEBOOK_CONNECTION_KEY,\n                conn_type=\"jupyter_notebook\",\n                login=\"foo\",\n                password=\"bar\",\n                description=\"Jupyter Notebook\",\n            )\n        hook.create_connection(conn)\n\n        role = Session().query(Role).filter(Role.name == \"test-jupyter\").first()\n        if role is None:\n            role = Role(name=\"test-jupyter\")\n        hook.create_role(role)\n\n        task = Session().query(TaskInstance).filter(\n            TaskInstance.dag_id == DAG_ID,\n            TaskInstance.task_id == \"create_notebook\",\n            TaskInstance.execution_date == datetime(2022, 1, 1),\n        ).first()\n        if task is None:\n            task = TaskInstance(\n                task_id=\"create_notebook\",\n                dttm=datetime(2022, 1, 1),\n                task=role,\n            )\n        Session().add(task)\n        Session().commit()\n\n    create_notebook_task = create_notebook()\n\n    @task\n    def get_notebook_url():\n        conn = hook.get_connection(JUPYTER_NOTEBOOK_CONNECTION_KEY)\n        return conn.login + \"@jupyter_notebook:8888\"\n\n    get_notebook_url_task = get_notebook_url()\n\n    @task\n    def save_notebook_file(path: str):\n        with open(path, \"w\") as file:\n            file.write(\"Hello, world!\\nThis is a test Jupyter notebook.\")\n\n    save_notebook_file_task = save_notebook_file(os.path.join(os.path.dirname(__file__), \"test_jupyter_notebook.ipynb\")\n\n    @task\n    def start_notebook(url: str):\n        os.system(f\"nbopen {url}\")\n\n    start_notebook_task = start_notebook(get_notebook_url_task.output)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def clean_up():\n        logging.info(\"Removing connection %s.\", JUPYTER_NOTEBOOK_CONNECTION_KEY)\n        hook.delete_connection(conn_id=JUPYTER_NOTEBOOK_CONNECTION_KEY)\n\n        logging.info(\"Removing task %s.\", create_notebook_task.id)\n        hook.delete_task(task_id=create_notebook_task.id)\n\n        logging.info(\"Removing task instance %s.\", task.id)\n        hook.delete_task(task_id=task.id)\n\n        Session().commit()\n\n    clean_up_task = clean_up()\n\n    watcher(create_notebook_task)\n    openlineage_test_operator(start_notebook_task, dag_id=DAG_ID)\n    watcher(start_notebook_task)\n    watcher(task)\n    watcher(clean_up_task)"}], "metadata": {"file_name": "dag_2109_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "\u8bf7\u95ee\u60a8\u80fd\u8be6\u7ec6\u8bf4\u660e\u4e00\u4e0b\u5982\u4f55\u5728Python\u4e2d\u5b9e\u73b0\u4e00\u4e2a\u6d45\u62f7\u8d1d\u5417\uff1f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u6211\u4eec\u9700\u8981\u4f7f\u7528\u6d45\u62f7\u8d1d\u5462\uff1f\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def shallow_copy_function():\n    \"\"\"\n    This function demonstrates how to use the copy() function in Python to create a shallow copy of a list.\n    A shallow copy creates a new list but links to the original list's elements, meaning changes to\n    those elements in the new list will affect the original list.\n    \"\"\"\n\n    # Original list\n    original_list = [[1, 2, 3], [4, 5, 6]]\n\n    # Shallow copy of the list\n    shallow_copied_list = copy.copy(original_list)\n\n    # Modify the original list\n    original_list[0][0] = 'one'\n\n    # After modification, both lists are affected\n    print(\"Original list:\", original_list)\n    print(\"Shallow copied list:\", shallow_copied_list)\n\n\n# Call the function to demonstrate shallow copying\nshallow_copy_function()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you help me with a Python script to merge two CSV files based on a common column and then save the merged file?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\n\nPATH_TO_SCRIPT = os.path.abspath(os.path.join(\"scripts\", \"merge_csv.py\"))\n\n\ndef task_start():\n    print(\"The script path is:\", PATH_TO_SCRIPT)\n\n\nwith models.DAG(\n    \"example_python_operator_for_script\",\n    start_date=datetime(2022, 11, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    t1 = BashOperator(task_id=\"print_date\", bash_command=\"date\", dag=dag)\n    t2 = BashOperator(task_id=\"print_time\", bash_command=\"date -d \\'+%H:%M\\'\", dag=dag)\n    t3 = BashOperator(task_id=\"print_day_week\", bash_command=\"date -d \\\"+%A\\\"\", dag=dag)\n    t4 = BashOperator(task_id=\"print_month_year\", bash_command=\"date -d \\\"+%B %Y\\\"\", dag=dag)\n\n    t1 >> [t2, t3, t4]\n\n    # [START howto_operator_python_for_script]\n    t5 = PythonOperator(\n        task_id=\"merge_csv\",\n        python_callable=lambda: os.system(PATH_TO_SCRIPT + \" file_1.csv file_2.csv output.csv\"),\n    )\n    # [END howto_operator_python_for_script]\n\n    t1 >> t5 >> [t2, t3, t4]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Blob Storage monitoring workflow that checks for specific file arrivals or file prefix patterns in a cloud storage container. Create an Airflow DAG that can trigger downstream processes based on the presence of expected files or file groups.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.blob import AzureBlobSensor\nfrom airflow.providers.microsoft.azure.transfers.local_to_blob import LocalFilesystemToBlobOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_blob\"\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"azureblob\",\n        description=\"AzureBlobConnection\",\n        extra={\"server\": \"your_storage_account_name\", \"container\": \"your_container_name\"},\n    )\n    print(conn)\n    print(conn.save())\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    blob_conn = create_connection(\"azure_blob_default\")\n\n    # [START howto_operator_local_to_blob]\n    upload_file = LocalFilesystemToBlobOperator(\n        task_id=\"upload_file\",\n        filename=\"file.txt\",\n        blob_name=\"your_blob_name\",\n        blob_conn_id=blob_conn,\n    )\n    # [END howto_operator_local_to_blob]\n\n    # [START howto_operator_blob_sensor]\n    wait_for_file = AzureBlobSensor(\n        task_id=\"wait_for_file\", blob_name=\"your_blob_name\", blob_conn_id=blob_conn\n    )\n    # [END howto_operator_blob_sensor]\n\n    upload_file >> wait_for_file\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_49_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "What is the most concise and linear Python code to find the third smallest element in an unsorted list?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport random\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.utils.trigger_rule import TriggerRule\n\ndag = DAG(\n    \"example_find_third_smallest\",\n    schedule=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    start_date=\"20210101\",\n    catchup=False,\n    tags=[\"example\"],\n)\n\n# [START howto_operator_find_third_smallest]\n@task\ndef find_third_smallest(numbers):\n    numbers = [n for n in numbers if n > 0]  # Remove negative numbers\n    numbers.sort()\n    return numbers[2]  # Return the third smallest number\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef stop_covid_pipeline():\n    print(\"Stopping COVID pipeline.\")\n    exit(0)\n\n\nwith dag:\n    # Second task depends on the result of the first task\n    find_third_smallest([6, 5, 8, -4, 1, 10]) >> stop_covid_pipeline()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a document classification training process using AWS services, focusing on creating, monitoring, and cleaning up a machine learning model for medical document categorization. Implement a task group that handles the entire classifier creation workflow with proper error handling and resource management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.client import BaseClient\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.translate import TranslateCreateClassifierOperator\nfrom airflow.providers.amazon.aws.sensors.translate import TranslateClassifierSensor\nfrom airflow.providers.amazon.aws.utils import get_aws_id\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_amazon_translate_enums]\nSOURCE_LANGUAGE_CODE = \"en\"\nTARGET_LANGUAGE_CODE = \"zh-Hans\"\nMODEL_VERSION = \"0.1\"\nINPUT_DATA_SPECTRA_TYPE = \"MULTI_CLASS\"\nDATA_SOURCE_TYPE = \"S3\"\nLANGUAGE_CODE = \"en\"\nMULTI_CLASS_CLASSIFICATIONS = [\n    {\"ClassId\": 1, \"ClassificationScore\": [0.6, 0.2, 0.1]},\n    {\"ClassId\": 2, \"ClassificationScore\": [0.2, 0.1, 0.7]},\n]\nROLE_ARN = os.environ.get(\"ROLE_ARN\")\nEXTRA_S3_CONFIG = {\n    \"BucketAccessRole\": {\n        \"Sid\": \"Allow S3 buckets to access source bucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\"Bucket\": {\"Sid\": \"Allow S3 buckets\", \"Effect\": \"Any\", \"Action\": [\"s3:GetObject\"]}},\n    }\n}\n# [END howto_amazon_translate_enums]\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_translate_classifier\"\nDAG_VERSION = \"v1.2.3\"\n\n\n@task\ndef create_connection(conn_id_name: str, s3_bucket: str, input_object_key: str):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n        description=\"Connection to S3 bucket containing training data.\",\n        extra=json.dumps({\"bucket\": s3_bucket, \"input_object_key\": input_object_key}),\n    )\n    session = Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef set_up_role_arn(conn_id_name: str):\n    s3_bucket = get_aws_id(lambda x: x.get(\"Bucket\"), conn_id_name)\n    input_object_key = \"translate_data.csv\"\n    create_bucket = S3Hook().conn.create_bucket(Bucket=s3_bucket)\n    input_object = S3Hook().conn.put_object(Bucket=s3_bucket, Key=input_object_key, Body=\"test\")\n    extra_role_arn = f\"arn:aws:iam::{get_aws_id(lambda x: x.get('Arn')), 'role', 'lambda-execution'}\"\n    TranslateHook = S3Hook()\n    TranslateHook.conn.put_bucket_access_role(\n        Bucket=s3_bucket, RoleName=extra_role_arn, RoleAssumeRolePolicyDocument=extra_s3_config\n    )\n    # Delete the temporary object\n    TranslateHook.conn.delete_object(Bucket=s3_bucket, Key=input_object_key)\n    # Delete the bucket\n    S3Hook().conn.delete_bucket(Bucket=s3_bucket)\n    log.info(\"Role ARN has been updated\")\n    # The role ARN is updated within the same Airflow run, so we re-set it here.\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_role_arn(conn_id_name: str):\n    s3_bucket = get_aws_id(lambda x: x.get(\"Bucket\"), conn_id_name)\n    role_arn = f\"arn:aws:iam::{get_aws_id(lambda x: x.get('Arn'))}:role/{conn_id_name}\"\n    S3Hook().conn.delete_bucket(AccessRoleArn=role_arn)\n    S3Hook().conn.delete_bucket(Bucket=s3_bucket)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\",\n        bucket_name=f\"{get_aws_id(lambda x: x.get('Bucket'))}\",\n    )\n\n    create_connection(\"conn_s3\", f\"{create_s3_bucket.output}\", \"input_data.csv\")\n\n    translate_create_classifier = TranslateCreateClassifierOperator(\n        task_id=\"translate_create_classifier\",\n        input_data_spectra=TARGET_LANGUAGE_CODE,\n        model_name=\"test_model\",\n        source_language_code=SOURCE_LANGUAGE_CODE,\n        input_data_config={\n            \"S3Config\": {\n                \"Bucket\": \"{{ params.bucket }}\",\n                \"Prefix\": \"{{ params.input_object_key }}\",\n            }\n        },\n        data_source_type=DATA_SOURCE_TYPE,\n        multi_class_classifications=MULTI_CLASS_CLASSIFICATIONS,\n        model_version=MODEL_VERSION,\n        language_code=LANGUAGE_CODE,\n        secondary_data_source_type=\"S3\",\n        secondary_input_data_spectra=\"en\",\n        secondary_input_data_config={\n            \"S3Config\": {\n                \"Bucket\": \"{{ params.bucket }}\",\n                \"Prefix\": \"{{ params.input_object_key }}\",\n            }\n        },\n    )\n\n    # [START howto_sensor_translate_classifier]\n    wait_for_classifier = TranslateClassifierSensor(\n        task_id=\"wait_for_classifier\",\n        model_name=\"test_model\",\n    )\n    # [END howto_sensor_translate_classifier]\n\n    @task\n    def remove_model(model_name: str):\n        log.info(\"Removing model %s.\", model_name)\n        is_removed = Session().conn.delete_model(ModelName=model_name)\n        log.info(\"Model removed status: %s\", is_removed[\"Model\"][\"ModelName\"] == model_name)\n\n    # [START howto_operator_translate_delete_model]\n    delete_model = Session().conn.delete_model\n    # [END howto_operator_translate_delete_model]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=create_s3_bucket.output,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        create_s3_bucket\n        >> create_connection(\"conn_s3\", \"{{ params.bucket}}\", \"{{ params.input_object_key}}\")\n        >> translate_create_classifier\n        >> wait_for_classifier\n        >> remove_model(\"test_model\")\n        >> S3CreateBucketOperator(task_id=\"create_s3_bucket\")\n        >> create_connection(\"conn_s3\", f\"{create_s3_bucket.output}\", \"input_data.csv\")\n        >> translate_create_classifier\n        >> wait_for_classifier\n        >> remove_model(\"test_model\")\n        >> S3DeleteBucketOperator(\n            task_id=\"delete_s3_bucket\", bucket_name=create_s3_bucket.output, trigger_rule=TriggerRule.ALL_DONE\n        )\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done.\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1475_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that showcases basic data persistence and retrieval techniques using an embedded database technology. Create a sequential task flow that demonstrates reading and writing operations in a lightweight, file-based key-value store.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_store\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"dbname\": \"test\"},\n    description=\"Demonstrate Airflow Local Store Operator\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    @task.local_store_operator(path=\"/tmp/local_store\", name=\"test_test\")\n    def gen_data():\n        import random\n        import string\n\n        return [dict(key=random.choice(string.ascii_uppercase + string.digits), val=random.choice(string.ascii_uppercase + string.digits))\n               for _ in range(10000000)]\n\n    gen_data() >> [task.LocalStoreOperator(task_id=x, path=f\"/tmp/local_store/{x}\") for x in [\"1\", \"2\"]]"}], "metadata": {"file_name": "dag_950_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS S3 data management workflow that demonstrates bucket creation, object manipulation, and metadata tagging. Create a DAG that showcases different S3 operations like creating buckets, uploading files, listing contents, and managing bucket tags.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.operators.s3_scout import S3ScoutOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = \"prod\"\nDAG_ID = \"example_s3scout\"\n\ns3_bucket = f\"{DAG_ID}-bucket\"\ns3_object = f\"{DAG_ID}-object\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    generate_id = BashOperator(task_id=\"generate_id\", bash_command=\":\", do_xcom_push=True)\n\n    @task\n    def create_connection(conn_id_name):\n        conn = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"s3\",\n        )\n        conn_extra = {\n            \"s3_science_access_role_arn\": \"arn:aws:iam::123456789012:role/ScienceAccessRole\",\n        }\n        conn_extra_json = json.dumps(conn_extra)\n        conn.set_extra(conn_extra_json)\n\n    create_connection_task = create_connection(\"s3_science_access_role_arn\")\n\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\",\n        bucket_name=s3_bucket,\n        region=\"us-east-1\",\n    )\n\n    @task\n    def upload_file_to_s3(bucket_name):\n        file_content = \"Test content\"\n        S3CreateObjectOperator(\n            task_id=\"upload_file_to_s3\",\n            bucket_name=bucket_name,\n            object_name=s3_object,\n            file_content=file_content,\n        )\n\n    upload_file_to_s3_task = upload_file_to_s3(s3_bucket)\n\n    @task\n    def list_s3(bucket_name):\n        return [{\"Prefix\": \"\"}]\n\n    list_s3_task = list_s3(s3_bucket)\n\n    @task\n    def tag_s3(bucket_name):\n        S3ScoutOperator(\n            task_id=\"tag_s3\",\n            bucket_name=bucket_name,\n            account_id=\"auto\",\n            comment=\"Auto tagged s3 bucket.\",\n            tags={\"Organization\": \"Auto\"},\n            region=\"us-east-1\",\n        )\n\n    tag_s3_task = tag_s3(s3_bucket)\n\n    @task\n    def check_scout_tag(bucket_name):\n        S3ScoutOperator(\n            task_id=\"check_s3_scout_tag\",\n            bucket_name=bucket_name,\n            region=\"us-east-1\",\n        )\n\n    check_scout_tag_task = check_scout_tag(s3_bucket)\n\n    generate_id >> create_connection_task >> create_s3_bucket >> upload_file_to_s3_task >> list_s3_task >> tag_s3_task >> check_scout_tag_task"}], "metadata": {"file_name": "dag_92_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Kubernetes cluster management workflow that demonstrates creating, using, and deleting a Google Kubernetes Engine (GKE) cluster for running sample workloads. The workflow should showcase asynchronous task execution and demonstrate basic pod deployment and cross-communication between tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.google.cloud.operators.kubernetes import (\n    KubernetesCreateClusterOperator,\n    KubernetesDeleteClusterOperator,\n    KubernetesDeployOperator,\n    KubernetesGetOperator,\n    KubernetesListPodOperator,\n    KubernetesPatchOperator,\n    KubernetesStartOperator,\n    KubernetesStopOperator,\n    KubernetesSyncOperator,\n    KubernetesTokenGeneratorOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"kubernetes\"\n\nGCP_LOCATION = \"europe-north1-a\"\nGCP_CLUSTER_NAME = f\"gke-debian-{ENV_ID}\"\nGCP_CLUSTER_BODY = {\n    \"name\": GCP_CLUSTER_NAME,\n    \"initial_node_count\": 1,\n    \"config\": {\n        \"clusterIPRange\": \"10.0.0.0/16\",\n        \"nodePools\": [\n            {\n                \"name\": f\"{GCP_CLUSTER_NAME}-pool\",\n                \"config\": {\n                    \"nodeMachineType\": \"n1-standard-1\",\n                    \"initialNodeCount\": 1,\n                },\n            },\n        ],\n    },\n}\n\nSAMPLE_FILE_NAME = \"sample-debian.sh\"\nSAMPLE_FILE_PATH = \"/tmp/samples/\"\nENVIRONMENT_YAML_PATH = \"environment.yaml\"\nENVIRONMENT_GCS_PATH = f\"gs://{PROJECT_ID}/airflow/tests/system/test_that/{SAMPLE_FILE_PATH}\"\nTOKEN_GENERATION_BODY = {\"image\": \"perl:5.34.0-2ubuntu2.1\"}\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"kubernetes\",\n        extra={\"cluster\": GCP_CLUSTER_NAME, \"namespace\": \"default\"},\n    )\n    print(conn)\n    return conn\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"kubernetes\"],\n) as dag:\n    generate_token = KubernetesTokenGeneratorOperator(\n        task_id=\"generate_token\",\n        project_id=PROJECT_ID,\n        body=GCP_CLUSTER_BODY,\n        location=GCP_LOCATION,\n    )\n\n    generate_token_async = KubernetesTokenGeneratorOperator(\n        task_id=\"generate_token_async\",\n        project_id=PROJECT_ID,\n        body=GCP_CLUSTER_BODY,\n        location=GCP_LOCATION,\n        deferrable=True,\n    )\n\n    create_cluster = KubernetesCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        body=GCP_CLUSTER_BODY,\n        location=GCP_LOCATION,\n    )\n\n    create_cluster_async = KubernetesCreateClusterOperator(\n        task_id=\"create_cluster_async\",\n        project_id=PROJECT_ID,\n        body=GCP_CLUSTER_BODY,\n        location=GCP_LOCATION,\n        deferrable=True,\n    )\n\n    get_cluster = KubernetesGetOperator(\n        task_id=\"get_cluster\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n    )\n\n    list_pod = KubernetesListPodOperator(\n        task_id=\"list_pod\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        namespace=\"default\",\n        query=\"metadata.node.nameinvalues=gke-debian-default-pool-0\",\n    )\n\n    sync_sample_file = LocalFilesystemToGCSOperator(\n        task_id=\"sync_sample_file\",\n        src=SAMPLE_FILE_PATH,\n        dst=SAMPLE_FILE_NAME,\n        public_bucket=ENVIRONMENT_GCS_PATH,\n    )\n\n    deploy_sample = KubernetesDeployOperator(\n        task_id=\"deploy_sample\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        namespace=\"default\",\n        image=\"perl:5.34.0-2ubuntu2.1\",\n        cmds=[\"sh\", \"-c\", f\"echo {os.path.basename(SAMPLE_FILE_PATH)} > {SAMPLE_FILE_PATH}\"],\n        name=\"sample-debian-pod\",\n        resources=requests={\"cpu\": \"25m\", \"memory\": \"100Mi\"},\n        namespace=\"default\",\n    )\n\n    patch_sample = KubernetesPatchOperator(\n        task_id=\"patch_sample\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        namespace=\"default\",\n        endpoint_name=\"sample-debian-pod\",\n        body={\"resources\": {\"requests\": {\"cpu\": \"80m\", \"memory\": \"200Mi\"}}},\n    )\n\n    stop_sample = KubernetesStopOperator(\n        task_id=\"stop_sample\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        namespace=\"default\",\n        name=\"sample-debian-pod\",\n    )\n\n    delete_cluster = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster_async = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster_async\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        deferrable=True,\n    )\n\n    delete_cluster_skip_iap = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster_skip_iap\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        skip_iap=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster_skip_iap_async = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster_skip_iap_async\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        skip_iap=True,\n        deferrable=True,\n    )\n\n    # [START howto_operator_kubernetes_connections]\n    conn_id = \"kubernetes\"\n    # [END howto_operator_kubernetes_connections]\n\n    create_connection_task = create_connection(conn_id_name=conn_id)\n\n    # [START howto_operator_create_connection_with_postgres]\n    conn_id_postgres = \"postgres\"\n    sql = f\"CREATE USER test;\");\n    create_connection_task(task_id=\"create_connection_with_postgres\", sql=sql, external_conn_id=\"postgres\")\n    # [END howto_operator_create_connection_with_postgres]\n\n    # [START howto_operator_sync_postgres]\n    sql = f\"CREATE USER test;\";\n    sync_postgres_task = KubernetesSyncOperator(\n        task_id=\"sync_postgres\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        external_conn_id=\"postgres\",\n        sql=sql,\n    )\n    # [END howto_operator_sync_postgres]\n\n    # [START howto_operator_get_postgres]\n    get_postgres_task = KubernetesGetOperator(\n        task_id=\"get_postgres\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        external_conn_id=\"postgres\",\n    )\n    # [END howto_operator_get_postgres]\n\n    # [START howto_operator_start_postgres]\n    start_postgres_task = KubernetesStartOperator(\n        task_id=\"start_postgres\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        external_conn_id=\"postgres\",\n    )\n    # [END howto_operator_start_postgres]\n\n    # [START howto_operator_stop_postgres]\n    stop_postgres_task = KubernetesStopOperator(\n        task_id=\"stop_postgres\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        external_conn_id=\"postgres\",\n    )\n    # [END howto_operator_stop_postgres]\n\n    # [START howto_operator_get_kubernetes_connections]\n    get_kubernetes_connections_task = KubernetesGetOperator(\n        task_id=\"get_kubernetes_connections\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n    )\n    # [END howto_operator_get_kubernetes_connections]\n\n    # [START howto_operator_create_cluster]\n    create_cluster_task = KubernetesCreateClusterOperator(\n        task_id=\"create_cluster_task\",\n        project_id=PROJECT_ID,\n        body=GCP_CLUSTER_BODY,\n        location=GCP_LOCATION,\n    )\n    # [END howto_operator_create_cluster]\n\n    # [START howto_operator_get_cluster]\n    get_cluster_task = KubernetesGetOperator(\n        task_id=\"get_cluster_task\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n    )\n    # [END howto_operator_get_cluster]\n\n    # [START howto_operator_list_cluster]\n    list_cluster_task = KubernetesListPodOperator(\n        task_id=\"list_cluster_task\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n        namespace=\"default\",\n        query=\"metadata.node.nameinvalues=gke-debian-default-pool-0\",\n    )\n    # [END howto_operator_list_cluster]\n\n    # [START howto_operator_delete_cluster]\n    delete_cluster_task = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster_task\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=GCP_CLUSTER_NAME,\n    )\n    # [END howto_operator_delete_cluster]\n\n    # [START howto_operator_delete_cluster_async]\n    delete_cluster_task_async = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster_task_async\","}], "metadata": {"file_name": "dag_2078_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that illustrates data processing and infrastructure management in a cloud-native Kubernetes environment using dynamic cluster provisioning and ephemeral pod execution. Create a workflow that demonstrates how to programmatically spin up computational resources, execute sample workloads, and cleanly tear down infrastructure after task completion.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.week import Week\nfrom airflow.settings import Session\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=Week.day(),\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo hello world\")\n\n    @task\n    def create_connection(conn_id_name: str):\n        session = Session()\n        session.add(Connection(conn_id_name=conn_id_name))\n        session.commit()\n\n    create_connection_task = create_connection(conn_id_name=\"singularity_conn\")\n\n    run_this >> create_connection_task\n\n    # [START howto_operator_singularity]\n    singularity_task = SingularityOperator(\n        task_id=\"singularity_task\",\n        image=\"docker://busybox\",\n        command=\"/bin/singularity --version\",\n    )\n    # [END howto_operator_singularity]\n\n    (\n        run_this\n        >> create_connection_task\n        >> singularity_task\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(watcher(dag))"}], "metadata": {"file_name": "dag_900_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced cloud storage transfer techniques using GCS and SFTP operators, with emphasis on handling various file transfer scenarios and implementing robust file transfer verification mechanisms. Develop a workflow that demonstrates different transfer methods while ensuring data integrity through sensor checks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.s3.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.sftp.transfers.sftp_to_s3 import SFTPToS3Operator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_s3\"\n\nsftp_conn = Connection(\n    conn_id=\"sftp_conn\",\n    schema=\"sftp\",\n    conn_type=\"sftp\",\n    host=\"sftp.hosting.kyma-project.io\",\n    login=\"user\",\n    password=\"password\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_s3_bucket():\n        BashOperator(\n            task_id=\"create_s3_bucket\",\n            bash_command=f\"aws s3 create bucket --bucket {DAG_ID} --region us-east-1\",\n        )\n\n    @task\n    def create_sftp_connection():\n        BashOperator(\n            task_id=\"create_sftp_connection\",\n            bash_command=f\"aws sftp create-connection --hostname {sftp_conn.host} --login {sftp_conn.login} --region us-east-1 --connection-id {sftp_conn.id} --password {sftp_conn.password}\",\n        )\n\n    @task\n    def create_sftp_file():\n        BashOperator(\n            task_id=\"create_sftp_file\",\n            bash_command=f\"touch {DAG_ID}/example_file.txt\",\n        )\n\n    @task\n    def get_file_from_sftp_to_s3(sftp_conn_id, s3_bucket, s3_key):\n        BashOperator(\n            task_id=\"get_file_from_sftp_to_s3\",\n            bash_command=f\"aws s3 cp {sftp_conn_id}/{s3_bucket}/{s3_key} s3://{s3_bucket}/{s3_key}\",\n        )\n\n    @task\n    def delete_sftp_file():\n        BashOperator(\n            task_id=\"delete_sftp_file\",\n            bash_command=f\"rm {DAG_ID}/example_file.txt\",\n        )\n\n    @task\n    def delete_s3_bucket(s3_bucket):\n        BashOperator(\n            task_id=\"delete_s3_bucket\",\n            bash_command=f\"aws s3 delete-bucket --bucket {s3_bucket} --force-delete\",\n        )\n\n    create_s3_bucket()\n    create_sftp_connection()\n    create_sftp_file()\n    get_file_from_sftp_to_s3(sftp_conn_id=\"sftp_conn\", s3_bucket=DAG_ID, s3_key=\"example_file.txt\")\n    delete_sftp_file()\n    delete_s3_bucket(s3_bucket=DAG_ID)"}], "metadata": {"file_name": "dag_2095_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data repository management workflow that demonstrates creating, updating, and deleting a Databricks repository while executing a data preparation notebook. The workflow should showcase automated repository lifecycle management and notebook execution in a cloud-based data processing environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_DEFAULT_ACCOUNT = os.environ.get(\"DATABRICKS_DEFAULT_ACCOUNT\")\n\n# Day of week the job should be triggered\n# For example \"0-11 0-23 /4 * * *\" will trigger the job every day between 00:00 and 23:45\nTRIGGER_SPECIFICATION = {\n    \"open_interval\": {\"minutes\": 45},\n    \"wakes_up_if_missing\": True,\n    \"when_to_trigger\": \"ON_DATABricks_JOB_SUCCESS\",\n    \"jobs\": [\n        {\n            \"jobId\": \"19966ae3-b2f4-48f9-b9f6-761767171d53\",\n            # If you want to trigger a specific notebook run, set 'notebookPath'\n            # For this notebook to trigger, the notebook path must exactly match the notebook path stored in Databricks Git\n            \"triggerSpec\": {\"jobId\": \"19966ae3-b2f4-48f9-b9f6-761767171d53\"},\n        }\n    ],\n}\n\nREPOSITORY_ID = int(os.environ.get(\"DATABRICKS_REPOSITORY_ID\", 12402))\nNOTEBOOK_PATH = os.environ.get(\"DATABRICKS_NOTBOOK_PATH\", \"pip-install\")\nDATABRAPID_TEST_DEFAULT_GROUP_ID = os.environ.get(\"DATABRAPID_TEST_DEFAULT_GROUP_ID\", \"databricks\")\n\nwith models.DAG(\n    dag_id=\"example_databricks_sensor\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_databricks_sensor]\n    check_job_state = DatabricksSensor(\n        databricks_conn_id=\"databricks_default\",\n        cluster_id=\"your_cluster_id\",\n        notebook_path=NOTEBOOK_PATH,\n        notebook_run_spec=TRIGGER_SPECIFICATION[\"jobs\"][0],\n        timeout=60 * 20,\n        poke_interval=60,\n        trigger_spec=TRIGGER_SPECIFICATION[\"triggerSpec\"],\n        group_id=DATABRAPID_TEST_DEFAULT_GROUP_ID,\n    )  # [END howto_databricks_sensor]\n\n    # Trigger the job (\"your_cluster_id\" can be obtained from the UI)\n    trigger_job = DatabricksSubmitRunOperator(\n        task_id=\"trigger_job\",\n        databricks_conn_id=\"databricks_default\",\n        cluster_id=\"your_cluster_id\",\n        notebook_path=NOTEBOOK_PATH,\n        notebook_run_spec=TRIGGER_SPECIFICATION[\"jobs\"][0],\n        group_id=DATABRAPID_TEST_DEFAULT_GROUP_ID,\n    )  # [END howto_databricks_sensor_trigger]\n\n    check_job_state >> trigger_job"}], "metadata": {"file_name": "dag_1161_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates metadata management tasks for data governance and discovery, focusing on systematically creating and configuring entry groups, entry types, and aspect types in a Google Cloud Dataplex environment. Develop a workflow that demonstrates the programmatic setup of data catalog components with clear, configurable parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateEntryGroupOperator,\n    DataplexCreateEntryOperator,\n    DataplexCreateAspectOperator,\n    DataplexCreateAspectTypeOperator,\n    DataplexCreateEntryTypeOperator,\n    DataplexCreateWorkflowOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteEntryOperator,\n    DataplexDeleteAspectOperator,\n    DataplexDeleteAspectTypeOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteEntryOperator,\n    DataplexDeleteEntryTypeOperator,\n    DataplexDeleteWorkflowOperator,\n    DataplexGetEntryGroupOperator,\n    DataplexGetEntryOperator,\n    DataplexGetAspectOperator,\n    DataplexGetAspectTypeOperator,\n    DataplexGetEntryTypeOperator,\n    DataplexListWorkflowsOperator,\n    DataplexRunWorkflowOperator,\n    DataplexStartWorkflowOperator,\n    DataplexStopWorkflowOperator,\n    DataplexUpdateEntryGroupOperator,\n    DataplexUpdateEntryOperator,\n    DataplexUpdateAspectOperator,\n    DataplexUpdateAspectTypeOperator,\n    DataplexUpdateEntryGroupOperator,\n    DataplexUpdateEntryOperator,\n    DataplexUpdateEntryTypeOperator,\n    DataplexUpdateWorkflowOperator,\n    DataplexWorkflowDeleteOperator,\n    DataplexWorkflowGetOperator,\n    DataplexWorkflowListOperator,\n    DataplexWorkflowRunOperator,\n    DataplexWorkflowStartOperator,\n    DataplexWorkflowStopOperator,\n    DataplexWorkflowUpdateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_dataplex\"\n\nENTRY_GROUP_NAME = f\"{PROJECT_ID}-entry-group-datasource-{ENV_ID}\".replace(\"_\", \"-\")\nENTRY_NAME = f\"{PROJECT_ID}-entry-datasource-{ENV_ID}\".replace(\"_\", \"-\")\nASPECT_TYPE_NAME = f\"{PROJECT_ID}-aspect-type-datasource-{ENV_ID}\".replace(\"_\", \"-\")\nASPECT_NAME = f\"{PROJECT_ID}-aspect-datasource-{ENV_ID}\".replace(\"_\", \"-\")\nENTRY_TYPE_NAME = f\"{PROJECT_ID}-entry-type-datasource-{ENV_ID}\".replace(\"_\", \"-\")\nWORKFLOW_NAME = f\"{PROJECT_ID}-workflow-{ENV_ID}\".replace(\"_\", \"-\")\nWORKFLOW_DELETE_TRIGGER = f\"{WORKFLOW_NAME}-delete\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"dataplex\"],\n) as dag:\n    # [START howto_operator_create_dataplex_entry_group]\n    create_entry_group = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        project_id=PROJECT_ID,  # This sceme is optional. You can set entry_group_id independently of project_id\n    )\n    # [END howto_operator_create_dataplex_entry_group]\n\n    # [START howto_operator_create_dataplex_entry]\n    create_entry = DataplexCreateEntryOperator(\n        task_id=\"create_entry\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_id=ENTRY_NAME,\n        project_id=PROJECT_ID,\n        body={\"fileSet\": {\"gcs\": {\"inputUri\": \"bq://airflow.example\"}}},\n    )\n    # [END howto_operator_create_dataplex_entry]\n\n    # [START howto_operator_create_dataplex_aspect_type]\n    create_aspect_type = DataplexCreateAspectTypeOperator(\n        task_id=\"create_aspect_type\",\n        aspect_type_id=ASPECT_TYPE_NAME,\n        project_id=PROJECT_ID,\n        body={\"aspectType\": {\"name\": \"DataSource\"}},\n    )\n    # [END howto_operator_create_dataplex_aspect_type]\n\n    # [START howto_operator_create_dataplex_aspect]\n    create_aspect = DataplexCreateAspectOperator(\n        task_id=\"create_aspect\",\n        entry_id=ENTRY_NAME,\n        aspect_type_id=ASPECT_TYPE_NAME,\n        aspect_name=ASPECT_NAME,\n        project_id=PROJECT_ID,\n        body={\"aspect\": {\"displayName\": \"DataSourceAspect\", \"metadata\": {\"schema\": {}}}}},\n    )\n    # [END howto_operator_create_dataplex_aspect]\n\n    # [START howto_operator_create_dataplex_entry_type]\n    create_entry_type = DataplexCreateEntryTypeOperator(\n        task_id=\"create_entry_type\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_type_id=ENTRY_TYPE_NAME,\n        project_id=PROJECT_ID,\n        body={\"entryType\": {\"name\": \"DataSourceType\"}},\n    )\n    # [END howto_operator_create_dataplex_entry_type]\n\n    # [START howto_operator_create_dataplex_workflow]\n    create_workflow = DataplexCreateWorkflowOperator(\n        task_id=\"create_workflow\",\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_NAME,\n        body={\"workflow\": {\"description\": \"Test workflow description\", \"enabled\": True}, \"entryTypes\": [{\"name\": ENTRY_TYPE_NAME}]},\n    )\n    # [END howto_operator_create_dataplex_workflow]\n\n    # [START howto_operator_update_dataplex_entry_group]\n    update_entry_group = DataplexUpdateEntryGroupOperator(\n        task_id=\"update_entry_group\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        project_id=PROJECT_ID,\n        body={\"labels\": {\"type\": \"DATASET\"}},\n    )\n    # [END howto_operator_update_dataplex_entry_group]\n\n    # [START howto_operator_update_dataplex_entry]\n    update_entry = DataplexUpdateEntryOperator(\n        task_id=\"update_entry\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_id=ENTRY_NAME,\n        project_id=PROJECT_ID,\n        body={\"fileSet\": {\"gcs\": {\"inputUri\": \"gcs://example/file\"}}},\n    )\n    # [END howto_operator_update_dataplex_entry]\n\n    # [START howto_operator_update_dataplex_aspect]\n    update_aspect = DataplexUpdateAspectOperator(\n        task_id=\"update_aspect\",\n        entry_id=ENTRY_NAME,\n        aspect_type_id=ASPECT_TYPE_NAME,\n        aspect_id=ASPECT_NAME,\n        project_id=PROJECT_ID,\n        body={\"aspect\": {\"displayName\": \"UpdatedDataSourceAspect\", \"metadata\": {\"schema\": {}}}}},\n    )\n    # [END howto_operator_update_dataplex_aspect]\n\n    # [START howto_operator_update_dataplex_aspect_type]\n    update_aspect_type = DataplexUpdateAspectTypeOperator(\n        task_id=\"update_aspect_type\",\n        aspect_type_id=ASPECT_TYPE_NAME,\n        project_id=PROJECT_ID,\n        body={\"aspectType\": {\"displayName\": \"UpdatedAspectType\", \"description\": \"UpdatedAspectTypeDescription\", \"enabled\": False}}},\n    )\n    # [END howto_operator_update_dataplex_aspect_type]\n\n    # [START howto_operator_update_dataplex_entry_type]\n    update_entry_type = DataplexUpdateEntryTypeOperator(\n        task_id=\"update_entry_type\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_type_id=ENTRY_TYPE_NAME,\n        project_id=PROJECT_ID,\n        body={\"entryType\": {\"displayName\": \"UpdatedDataSourceType\", \"description\": \"UpdatedAspectTypeDescription\"}},\n    )\n    # [END howto_operator_update_dataplex_entry_type]\n\n    # [START howto_operator_delete_dataplex_entry_type]\n    delete_entry_type = DataplexDeleteEntryTypeOperator(\n        task_id=\"delete_entry_type\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_type_id=ENTRY_TYPE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_dataplex_entry_type]\n\n    # [START howto_operator_delete_dataplex_aspect_type]\n    delete_aspect_type = DataplexDeleteAspectTypeOperator(\n        task_id=\"delete_aspect_type\",\n        aspect_type_id=ASPECT_TYPE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_aspect_type]\n\n    # [START howto_operator_delete_dataplex_aspect]\n    delete_aspect = DataplexDeleteAspectOperator(\n        task_id=\"delete_aspect\",\n        entry_id=ENTRY_NAME,\n        aspect_type_id=ASPECT_TYPE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_aspect]\n\n    # [START howto_operator_delete_dataplex_entry]\n    delete_entry = DataplexDeleteEntryOperator(\n        task_id=\"delete_entry\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        entry_id=ENTRY_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_entry]\n\n    # [START howto_operator_delete_dataplex_entry_group]\n    delete_entry_group = DataplexDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\",\n        entry_group_id=ENTRY_GROUP_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_entry_group]\n\n    # [START howto_operator_list_dataplex_workflows]\n    list_workflows = DataplexListWorkflowsOperator(\n        task_id=\"list_workflows\",\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_list_dataplex_workflows]\n\n    # [START howto_operator_run_dataplex_workflow]\n    run_workflow = DataplexRunWorkflowOperator(\n        task_id=\"run_workflow\",\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_NAME,\n        body={\"startRequest"}], "metadata": {"file_name": "dag_1222_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a sample workflow DAG that simulates a basic system monitoring and testing pipeline, using different system commands and containerized operations to showcase task dependency and execution strategies. Create a sequence of tasks that validate system time, perform wait operations, and execute a containerized process.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.providers.standard.sensors.bash import BashSensor\nfrom airflow.providers.standard.sensors.python import PythonSensor\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_bash_sensor\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    task_start = EmptyOperator(task_id=\"task_start\")\n    task_end = EmptyOperator(task_id=\"task_end\")\n\n    @task_start\n    def generate_output():\n        print(\"Hello World!\")\n\n    @task_start\n    def get_input():\n        import random\n\n        print(random.randint(0, 9))\n\n    @task_start\n    def need_sensor():\n        import random\n\n        print(random.randint(0, 9))\n        return True\n\n    @task_start\n    def sleep():\n        import random\n\n        print(random.randint(0, 9))\n        return\n\n    @task_start\n    def sleep_for_42():\n        import random\n\n        print(random.randint(0, 9))\n        return\n\n    @task_start\n    def need_sensor_then_stop():\n        import random\n\n        print(random.randint(0, 9))\n        return False\n\n    @task_start\n    def tearDown():\n        print(\"Sleeping a bit...\")\n        import random\n\n        print(random.randint(0, 9))\n\n    generate_output >> get_input\n    BashOperator(task_id=\"run_after_get_input\", bash_command=\"sleep 1\", trigger_rule=TriggerRule.NONE_FAILED)\n    BashSensor(task_id=\"wait_for_get_input\", bash_command=\"get_input\", timeout=2, mode=\"ExactlyOne\")\n    Label(\" Nothing expected here.\")\n    BashOperator(task_id=\"run_after_bash_sensor\", bash_command=\"sleep 2\", trigger_rule=TriggerRule.NONE_FAILED)\n    BashSensor(task_id=\"wait_for_bash_sensor\", bash_command=\"bash_sensor\", timeout=2, mode=\"ExactlyOne\")\n    Label(\"Nothing expected here.\")\n    BashOperator(task_id=\"tearDown\", task_fn=tearDown)\n\n    [task_start, task_end] >> Label(\"This task should be skipped\")"}], "metadata": {"file_name": "dag_127_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates setup and teardown tasks for a cloud infrastructure workflow. Create a DAG that shows how to manage resource allocation and cleanup across multiple task stages.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.operators.synapse import SynapseStoredProcedureOperator\n\nSTART_DATE = datetime(2023, 1, 1)\n\n# [START howto_synapse_azure_env]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"PROJECT_ID\")\n# [END howto_synapse_azure_env]\n\nwith models.DAG(\n    \"synapse-azure-sample\",\n    start_date=START_DATE,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_synapse_azure_create_table]\n    create_table_synapse_task = SynapseStoredProcedureOperator(\n        task_id=\"create_table_synapse_task\",\n        synapse_conn_id=\"synapse_default\",\n        stored_procedure=\"create_table_synapse\",\n        parameters={\"name\": \"test_table\"},\n        sql_output=True,\n        dag=dag,\n    )\n    # [END howto_synapse_azure_create_table]\n\n    # [START howto_synapse_azure_insert_data]\n    insert_data_synapse_task = SynapseStoredProcedureOperator(\n        task_id=\"insert_data_synapse_task\",\n        synapse_conn_id=\"synapse_default\",\n        stored_procedure=\"insert_data_synapse\",\n        parameters={\"table_name\": \"test_table\", \"values\": \"('value1', 'value2')\"},\n        dag=dag,\n    )\n    # [END howto_synapse_azure_insert_data]\n\n    # [START howto_synapse_azure_drop_table]\n    drop_table_synapse_task = SynapseStoredProcedureOperator(\n        task_id=\"drop_table_synapse_task\",\n        synapse_conn_id=\"synapse_default\",\n        stored_procedure=\"drop_table_synapse\",\n        dag=dag,\n    )\n    # [END howto_synapse_azure_drop_table]\n\n    delete_adls_task = ADLSDeleteOperator(\n        task_id=\"delete_adls_task\",\n        path=\"s3://synapse-bucket/airflow/tests/resources/\",\n        conn_id=\"azure_default\",\n        dag=dag,\n    )\n\n    (create_table_synapse_task >> insert_data_synapse_task >> drop_table_synapse_task >> delete_adls_task)"}], "metadata": {"file_name": "dag_23_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python code example that demonstrates using the `grpcio-reflection` for debugging a gRPC server in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.grpc.common import GrpcProvider\nfrom airflow.providers.grpc.sensors.grpc import GrpcSensor\n\n# [START howto_operator_grpc_sensors]\n# NOTE: Below we get ready to use a gRPC service that requires authentication.\n#       To do so, we need to fill in those 2 variables.  However, please be aware\n#       that your production code should not contain your client id and\n#       client secret.  The Airflow operator will take care of fetching those for you.\n#       Also please be aware that this service is exposed on a port other than the\n#       one we create in this example.  In this case it's 50051.\n\n# Client ID and Client Secret are generated when you create a new service in\n# Repeated these steps to generate a Client ID and Client Secret if you already\n# have a service set up: https://cloud.google.com/docs/authentication\n# Note: If you get a `google.auth.exceptions.DuplicateClientIdentifierError`\n# then it means a client with the same `client_id` already exists for another\n# service.  In that case you need to specify a value for `client_id_scopes`\n# when creating the client using the console or googleapis python package.\n\n\nwith DAG(\n    dag_id=\"example_grpc_sensors\",\n    schedule=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"grpc\", \"reflection\"],\n) as dag:\n    # [START howto_operator_grpc_sensors_def]\n    @task\n    def grpc_sensor_decorator():\n        # [END howto_operator_grpc_sensors_def]\n\n        # [START howto_operator_grpc_sensors_async]\n        @grpc_sensor_decorator()\n        async def grpc_sensor():\n            # Wait for the gRPC service to become available\n            while True:\n                print(\"Trying to connect to gRPC service...\")\n                # In a real scenario you would await a specific status on the gRPC service\n                await GrpcProvider._T_\\_weakref_timeout()\n                break\n\n        # [END howto_operator_grpc_sensors_async]\n        grpc_sensor()\n    # [END howto_operator_grpc_sensors]\n\n    # [START howto_operator_grpc_sensors_timeout]\n    @grpc_sensor_decorator()\n    def grpc_sensor_with_timeout():\n        # Wait for the gRPC service to become available\n        while True:\n            print(\"Trying to connect to gRPC service...\")\n            # In a real scenario you would await a specific status on the gRPC service\n            await GrpcProvider._T_\\_weakref_timeout()\n            break\n\n    grpc_sensor_with_timeout()\n    # [END howto_operator_grpc_sensors_timeout]\n\n    grpc_sensor_decorator()\n# [END howto_operator_grpc_sensors]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs data transfer operations between BigQuery tables, illustrating how to programmatically create table schemas, copy data between tables, and export data to cloud storage. Implement a complete data workflow that includes setup, data movement, and cleanup stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_bigquery_to_bigquery\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"test\"\nSCHEMA = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"1\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"1\"},\n]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        table_resource={\n            \"schema\": {\"fields\": SCHEMA},\n            \"inputAcceptanceTestConfig\": {\n                \"BigQueryToBigQueryCopy\": {\n                    \"sourceTable\": {\"projectId\": PROJECT_ID, \"datasetId\": DATASET_NAME, \"tableId\": TABLE_NAME},\n                }\n            },\n        },\n    )\n\n    # [START howto_operator_bigquery_to_bigquery]\n    copy_data = BigQueryCopyIntoTableOperator(\n        task_id=\"copy_data\",\n        source_project_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\",\n        destination_project_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.copy_table\",\n    )\n    # [END howto_operator_bigquery_to_bigquery]\n\n    read_data = BigQuerySelectOperator(\n        task_id=\"read_data\",\n        project=\"publicdata\",\n        dataset=\"bigquery',\n        sql=\"SELECT count(*) FROM `bigquery-public-data.stackoverflow.users`\",\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_bucket\n        >> create_table\n        # TEST BODY\n        >> copy_data\n        >> read_data\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_483_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates cloud storage file manipulation, showcasing task dependencies, file transfer between buckets, and executing a custom transformation script with proper error handling and cleanup.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_custom\"\nCONN_ID = f\"conn_{DAG_ID}_{ENV_ID}\"\nPREFIX = \"custom-transformation\"\n\nFILE_LOCAL_PATH = \"/custom\"\nFILE_NAMED_PATH = f\"{PREFIX}/named_file\"\nFILEenary_PATH = f\"{PREFIX}/empty_file\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_named_file():\n        \"\"\"\n        Custom task to create a file at the local path.\n        \"\"\"\n        os.system(f\"echo 'Custom transformation testing' > {FILE_LOCAL_PATH}\")\n\n    @task\n    def create_empty_file():\n        \"\"\"\n        Custom task to create an empty file at the local path.\n        \"\"\"\n        os.system(f\"echo >> {FILEenary_PATH}\")\n\n    @task\n    def create_connection(conn_id_name: str):\n        \"\"\"\n        Custom task to create connection.\n        \"\"\"\n        conn = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"s3\",\n            description=\"Connection for S3 resources.\",\n            host=\"\",\n            schema=\"\",\n            extra={\n                \"aws_access_key_id\": \"AKIAJL7QUXHETTSOHc6a\",\n                \"aws_secret_access_key\": \"1p8kGz8bM8j4t1b16f825b5b+9t6QD\",\n            },\n        )\n        conn.id = conn_id_name\n        conn.save()\n\n    @task\n    def create_role(role_name: str):\n        \"\"\"\n        Custom task to create role.\n        \"\"\"\n        Role(\n            role_id=role_name,\n            description=\"Role for custom transformation test.\",\n            minimum_access= {\n                \"S3FullAccess\": None,\n            },\n        )\n\n    @task\n    def create_task_group(task_group_name: str):\n        \"\"\"\n        Custom task to create TaskGroup.\n        \"\"\"\n        TaskGroup(\n            task_id=task_group_name,\n            description=\"TaskGroup for custom transformation test.\",\n        )\n\n    create_named_file() >> BashOperator(task_id=\"bash_example\", bash_command=f\"echo '{FILE_NAMED_PATH}'\")\n\n    create_empty_file() >> BashOperator(task_id=\"bash_example_2\", bash_command=f\"echo '{FILEenary_PATH}'\")\n\n    create_connection(conn_id_name=CONN_ID)\n    create_role(role_name=PREFIX)\n    create_task_group(task_group_name=PREFIX)\n\n    # [START howto_operator_custom]\n    t1 = BashOperator(task_id=\"print_date1\", bash_command=\"date\", dag=dag)\n    t1.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t2 = BashOperator(task_id=\"print_date2\", bash_command=\"date\", dag=dag)\n    t2.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t3 = BashOperator(task_id=\"print_date3\", bash_command=\"date\", dag=dag)\n    t3.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t4 = BashOperator(task_id=\"print_date4\", bash_command=\"date\", dag=dag)\n    t4.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t5 = BashOperator(task_id=\"print_date5\", bash_command=\"date\", dag=dag)\n    t5.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t6 = BashOperator(task_id=\"print_date6\", bash_command=\"date\", dag=dag)\n    t6.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t7 = BashOperator(task_id=\"print_date7\", bash_command=\"date\", dag=dag)\n    t7.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t8 = BashOperator(task_id=\"print_date8\", bash_command=\"date\", dag=dag)\n    t8.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t9 = BashOperator(task_id=\"print_date9\", bash_command=\"date\", dag=dag)\n    t9.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t10 = BashOperator(task_id=\"print_date10\", bash_command=\"date\", dag=dag)\n    t10.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t11 = BashOperator(task_id=\"print_date11\", bash_command=\"date\", dag=dag)\n    t11.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t12 = BashOperator(task_id=\"print_date12\", bash_command=\"date\", dag=dag)\n    t12.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t13 = BashOperator(task_id=\"print_date13\", bash_command=\"date\", dag=dag)\n    t13.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t14 = BashOperator(task_id=\"print_date14\", bash_command=\"date\", dag=dag)\n    t14.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t15 = BashOperator(task_id=\"print_date15\", bash_command=\"date\", dag=dag)\n    t15.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t16 = BashOperator(task_id=\"print_date16\", bash_command=\"date\", dag=dag)\n    t16.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t17 = BashOperator(task_id=\"print_date17\", bash_command=\"date\", dag=dag)\n    t17.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t18 = BashOperator(task_id=\"print_date18\", bash_command=\"date\", dag=dag)\n    t18.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t19 = BashOperator(task_id=\"print_date19\", bash_command=\"date\", dag=dag)\n    t19.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t20 = BashOperator(task_id=\"print_date20\", bash_command=\"date\", dag=dag)\n    t20.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t21 = BashOperator(task_id=\"print_date21\", bash_command=\"date\", dag=dag)\n    t21.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t22 = BashOperator(task_id=\"print_date22\", bash_command=\"date\", dag=dag)\n    t22.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t23 = BashOperator(task_id=\"print_date23\", bash_command=\"date\", dag=dag)\n    t23.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t24 = BashOperator(task_id=\"print_date24\", bash_command=\"date\", dag=dag)\n    t24.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t25 = BashOperator(task_id=\"print_date25\", bash_command=\"date\", dag=dag)\n    t25.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t26 = BashOperator(task_id=\"print_date26\", bash_command=\"date\", dag=dag)\n    t26.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t27 = BashOperator(task_id=\"print_date27\", bash_command=\"date\", dag=dag)\n    t27.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t28 = BashOperator(task_id=\"print_date28\", bash_command=\"date\", dag=dag)\n    t28.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t29 = BashOperator(task_id=\"print_date29\", bash_command=\"date\", dag=dag)\n    t29.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t30 = BashOperator(task_id=\"print_date30\", bash_command=\"date\", dag=dag)\n    t30.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t31 = BashOperator(task_id=\"print_date31\", bash_command=\"date\", dag=dag)\n    t31.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t32 = BashOperator(task_id=\"print_date32\", bash_command=\"date\", dag=dag)\n    t32.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t33 = BashOperator(task_id=\"print_date33\", bash_command=\"date\", dag=dag)\n    t33.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t34 = BashOperator(task_id=\"print_date34\", bash_command=\"date\", dag=dag)\n    t34.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t35 = BashOperator(task_id=\"print_date35\", bash_command=\"date\", dag=dag)\n    t35.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t36 = BashOperator(task_id=\"print_date36\", bash_command=\"date\", dag=dag)\n    t36.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t37 = BashOperator(task_id=\"print_date37\", bash_command=\"date\", dag=dag)\n    t37.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t38 = BashOperator(task_id=\"print_date38\", bash_command=\"date\", dag=dag)\n    t38.doc_md = \"Example DAG to create Custom Resources.\"\n\n    t39 = BashOperator"}], "metadata": {"file_name": "dag_1651_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I have a project where I need to create a function that determines the academic year based on a given date. The academic year typically starts on September 1st and ends on August 31st of the following year. The function should take a date as input and return the academic year in the format \"YYYY-YYYY\". For example, for a date of February 1, 2023, the function should return \"2022-2023\". How can I implement this function in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task\ndef get_year_start_date(date: datetime.date) -> str:\n    \"\"\"\n    Get the academic year start date based on the given date.\n\n    Args:\n        date (datetime.date): The input date.\n\n    Returns:\n        str: The academic year start date in the format \"YYYY-YYYY\".\n    \"\"\"\n    if date.month < 9:\n        return f\"{date.year}-{date.year + 1}\"\n    return f\"{date.year + 1}-{date.year + 2}\"\n\n\n@task\ndef get_year_end_date(date: datetime.date) -> str:\n    \"\"\"\n    Get the academic year end date based on the given date.\n\n    Args:\n        date (datetime.date): The input date.\n\n    Returns:\n        str: The academic year end date in the format \"YYYY-YYYY\".\n    \"\"\"\n    return f\"{date.year + 1}-{date.year + 2}\"\n\n\n@task\ndef generate_academic_year(start_date: str, end_date: str) -> str:\n    \"\"\"\n    Generate the academic year based on the start and end dates.\n\n    Args:\n        start_date (str): The start date of the academic year in the format \"YYYY-YYYY\".\n        end_date (str): The end date of the academic year in the format \"YYYY-YYYY\".\n\n    Returns:\n        str: The academic year in the format \"YYYY-YYYY\".\n    \"\"\"\n    return f\"{start_date} to {end_date}\"\n\n\n@task\ndef log_academic_year(academic_year: str) -> None:\n    \"\"\"\n    Log the academic year to a logger.\n\n    Args:\n        academic_year (str): The academic year to log.\n    \"\"\"\n    print(f\"Academic year: {academic_year}\")\n\n\nwith DAG(\n    \"example_academic_year_dag\",\n    default_args={\n        \"log_to_file\": True,\n    },\n) as dag:\n    start = datetime.date.today()\n    end = start + datetime.timedelta(days=365)\n    get_year_start = get_year_start_date(start)\n    get_year_end = get_year_end_date(end)\n    academic_year = generate_academic_year(get_year_start, get_year_end)\n    log_academic_year(academic_year)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that leverages Databricks for scalable data processing, showing how to dynamically create jobs, run notebooks, and manage computational resources for data transformation tasks. Implement a workflow that demonstrates flexible cluster configuration and notebook-based data preparation strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\n\n# [START databricks_env_variables]\nDRIZZLE_CONN_ID = os.environ.get(\"DRIZZLE_CONN_ID\")\nJOB_ID = os.environ.get(\"DRIZZLE_JOB_ID\")\nNOTEBOOK_PATH = os.environ.get(\"DRIZZLE_NOTEBOOK_PATH\", \"DatabricksNotebookNotebook\")\n# [END databricks_env_variables]\n\nwith DAG(\n    \"example_databricks_operator\",\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START databricks_submit_run_operator]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"spark_job\",\n        notebook_params={\"libraries\": [{\"pypi\": {\"package\": \"pandas\"}}]},\n        notebook_path=NOTEBOOK_PATH,\n        job_id=JOB_ID,\n        databricks_conn_id=DRIZZLE_CONN_ID,\n    )\n    # [END databricks_submit_run_operator]\n\nfrom tests_common.test_utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1555_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates complex Cloud SQL operations including instance creation, data export/import, database configuration changes, and infrastructure cleanup. Develop a systematic workflow that demonstrates enterprise-grade database infrastructure automation.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.transfers.sql_to_s3 import SQLToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_to_s3\"\n\n\n@task\ndef create_connection(conn_id_name):\n    connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"postgres\",\n        description=\"Postgres database connection for Airflow system tests.\",\n        login=\"test\",\n        password=\"test\",\n        host=\"127.0.0.1\",\n        port=5432,\n        extra={\n            \"pgconninfo\": \"sslmode=require\",\n        },\n    )\n    db_conn = os.environ.get(\"DB_CONNECTION\")\n    user = os.environ.get(\"POSTGRES_USER\")\n    host = os.environ.get(\"POSTGRES_HOST\")\n    port = os.environ.get(\"POSTGRES_PORT\")\n    extra = {\"pgconninfo\": f\"host={host} port={port} user={user} password={db_conn}\"}\n    if db_conn and user:\n        extra[\"pgconninfo\"] += f\"&dbname={db_conn}\"\n    connection.save()\n\n    # For tests with multiple databases\n    if \".\" in conn_id_name:\n        *database_names, schema = conn_id_name.split(\".\")\n        connection_extra = extra.copy()\n        connection_extra[\"pgconninfo\"] += f\"&schema={schema}\"\n        connection = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"postgres\",\n            description=\"Postgres database connection for Airflow system tests.\",\n            login=\"test\",\n            password=\"test\",\n            host=\"127.0.0.1\",\n            port=5432,\n            extra=connection_extra,\n        )\n        db_conn = os.environ.get(\"DB_CONNECTION\")\n        user = os.environ.get(\"POSTGRES_USER\")\n        host = os.environ.get(\"POSTGRES_HOST\")\n        port = os.environ.get(\"POSTGRES_PORT\")\n        extra = {\"pgconninfo\": f\"host={host} port={port} user={user} password={db_conn}\"}\n        if db_conn and user:\n            extra[\"pgconninfo\"] += f\"&dbname={db_conn}\"\n        connection.save()\n\n    return connection\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn_id = \"postgres_db\"\n    s3_bucket = \"bucket_test\"\n\n    # [START howto_transfer_sql_to_s3]\n    sql_to_s3_task = SQLToS3Operator(\n        task_id=\"sql_to_s3_task\",\n        sql=\"SELECT 1\",\n        conn_id=conn_id,\n        sql_conn_id=conn_id,\n        schema=\"postgres\",\n        external_table=True,\n        table_name=\"postgres_db\",\n        s3_bucket=s3_bucket,\n        s3_key=\"sql_output/\",\n        replace=False,\n    )\n    # [END howto_transfer_sql_to_s3]\n\n    create_connection_task = create_connection(conn_id)\n\n    for task in [sql_to_s3_task]:\n        task.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2005_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms JSON-based Jeopardy question data into vector-enabled database collections using Weaviate's vectorization capabilities. Implement a workflow that shows how to batch import data, handle different vectorization approaches, and manage database collections programmatically.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom weaviate.classes import Class\nfrom weaviate.collections import Collection\n\n# [START howto_weaviate_import]\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.weaviate.transfers.json_to_weaviate import JsonToWeaviateOperator\n\n# [END howto_weaviate_import]\n\n\n@task\ndef create_weaviate_collection_class():\n    \"\"\"\n    Example of creating a class in Weaviate. The Class name used here is 'Question'.\n    \"\"\"\n    #  The json file used for the example can be found at:\n    #  https://databricks-prod-resources.s3.amazonaws.com/8/28/656888699973627/1693679806276799/JSON/jeopardy-vector.json\n    collection_data_json = \"\"\"\n    [\n        {\n            \"Category\": \"History\",\n            \"Number\": \"19\",\n            \"Question\": \"What U.S. state is nicknamed the 'Beach State' because of its large number of beaches per square mile?\",\n            \"Answer\": \"Hawaii\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"20\",\n            \"Question\": \"The official tree of Puerto Rico is what plant?\",\n            \"Answer\": \"Dominican mahogany\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"21\",\n            \"Question\": \"Which sport is often played on a rink?\",\n            \"Answer\": \"Ice Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"22\",\n            \"Question\": \"The chemical formula for Methane is what?\",\n            \"Answer\": \"CH4\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"23\",\n            \"Question\": \"The Battle of Panipat took place in which year?\",\n            \"Answer\": \"1556\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"24\",\n            \"Question\": \"The scientific name of an Elephant is what?\",\n            \"Answer\": \"Elephas Maximus\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"25\",\n            \"Question\": \"Which sport is often played on a field?\",\n            \"Answer\": \"Soccer/Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"26\",\n            \"Question\": \"2,203.018 x 5,583.4 = 12,440,000.648644\",\n            \"Answer\": \"12440000.64844\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"27\",\n            \"Question\": \"Who wrote the book 'One Thousand and One Nights'? \",\n            \"Answer\": \"Anonymous\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"28\",\n            \"Question\": \"The scientific name for a Fish is what?\",\n            \"Answer\": \"Salmo\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"29\",\n            \"Question\": \"What is the official sport of Brazil?\",\n            \"Answer\": \"Futebol\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"30\",\n            \"Question\": \"2 x 3 x 5 x 7 = 210\",\n            \"Answer\": \"210\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"31\",\n            \"Question\": \"What was the name of the first computer in the world?\",\n            \"Answer\": \"ENIAC\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"32\",\n            \"Question\": \"What is the official sport of Australia?\",\n            \"Answer\": \"Football\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"33\",\n            \"Question\": \"The scientific name for a Cat is what?\",\n            \"Answer\": \"Felis Catus\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"34\",\n            \"Question\": \"The Battle of Somme took place in which year?\",\n            \"Answer\": \"1916\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"35\",\n            \"Question\": \"What is the official sport of South Africa?\",\n            \"Answer\": \"Cricket\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"36\",\n            \"Question\": \"2 x 5 x 7 x 11 = 770\",\n            \"Answer\": \"770\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"37\",\n            \"Question\": \"The Battle of Leipzig took place in which year?\",\n            \"Answer\": \"1813\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"38\",\n            \"Question\": \"What is the official sport of Canada?\",\n            \"Answer\": \"Ice Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"39\",\n            \"Question\": \"The scientific name for a Dog is what?\",\n            \"Answer\": \"Canis Lupus\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"40\",\n            \"Question\": \"The Battle of Hastings took place in which year?\",\n            \"Answer\": \"1066\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"41\",\n            \"Question\": \"What is the official sport of Ireland?\",\n            \"Answer\": \"Hurling\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"42\",\n            \"Question\": \"2 x 3 x 5 x 13 = 330\",\n            \"Answer\": \"330\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"43\",\n            \"Question\": \"The Battle of Salamanca took place in which year?\",\n            \"Answer\": \"1568\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"44\",\n            \"Question\": \"What is the official sport of New Zealand?\",\n            \"Answer\": \"Cricket\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"45\",\n            \"Question\": \"The scientific name for a Cow is what?\",\n            \"Answer\": \"Bos Taurus\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"46\",\n            \"Question\": \"The Battle of Waterloo took place in which year?\",\n            \"Answer\": \"1815\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"47\",\n            \"Question\": \"What is the official sport of Scotland?\",\n            \"Answer\": \"Soccer/Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"48\",\n            \"Question\": \"3 x 3 x 3 x 7 = 189\",\n            \"Answer\": \"189\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"49\",\n            \"Question\": \"The Battle of Leipzig took place in which year?\",\n            \"Answer\": \"1813\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"50\",\n            \"Question\": \"What is the official sport of Wales?\",\n            \"Answer\": \"Rugby\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"51\",\n            \"Question\": \"The scientific name for a Pig is what?\",\n            \"Answer\": \"Sus Scrofa\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"52\",\n            \"Question\": \"The Battle of Hastings took place in which year?\",\n            \"Answer\": \"1066\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"53\",\n            \"Question\": \"What is the official sport of Austria?\",\n            \"Answer\": \"Soccer/Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"54\",\n            \"Question\": \"The scientific name for a Horse is what?\",\n            \"Answer\": \"Equus Caballus\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"55\",\n            \"Question\": \"The Battle of Panipat took place in which year?\",\n            \"Answer\": \"1556\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"56\",\n            \"Question\": \"What is the official sport of Belgium?\",\n            \"Answer\": \"Soccer/Hockey\"\n        },\n        {\n            \"Category\": \"Science\",\n            \"Number\": \"57\",\n            \"Question\": \"The scientific name for a Goat is what?\",\n            \"Answer\": \"Capra Hircus\"\n        },\n        {\n            \"Category\": \"History\",\n            \"Number\": \"58\",\n            \"Question\": \"The Battle of Waterloo took place in which year?\",\n            \"Answer\": \"1815\"\n        },\n        {\n            \"Category\": \"Sports\",\n            \"Number\": \"59\",\n            \"Question\": \"What is the official sport of Finland?\",\n            \"Answer\": \"Soccer/Hockey\"\n        },\n        {\n            \"Category\": \"Science\","}], "metadata": {"file_name": "dag_1831_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow that generates vector embeddings for text data using OpenAI and stores them in a PostgreSQL database with vector capabilities. Create a data pipeline that demonstrates how to transform text into machine-readable vector representations and persist them for potential machine learning or semantic search applications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom pgvector.vector import PGVector\nfrom pgvector.vector_settings import VectorSettings\n\n# [START howto_operator_pgvector_qdrant]\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import PointStruct\n\n# [END howto_operator_pgvector_qdrant]\n\n# [START howto_operator_pgvector_embeddings_from_obj]\nfrom pgvector.vector import VectorFromObject\n\n# [END howto_operator_pgvector_embeddings_from_obj]\n\n\ndef get_id(point: PointStruct) -> str:\n    return point.id\n\n\ndef get_text(point: PointStruct) -> str:\n    return point.payload.get(\"text\")\n\n\nwith pendulum.datetime(2021, 1, 1) as dt:\n    # [START howto_operator_pgvector_insert_embedding]\n    client = QdrantClient(\"qdrant://my_database\")\n\n    embedding_vector = OpenAIEmbeddings()\n\n    # Insert text to Qdrant vector database\n    # Vectorizer expects string as input\n    vectorizer = VectorFromObject(get_id, get_text)\n\n    # Create collection if not exists\n    client.create_collection(\n        collection_name=\"my_collection\",\n        vector_settings=VectorSettings(precision=128),\n        points=[\n            PointStruct(id=\"1\", payload={\"text\": \"first\"}),\n            PointStruct(id=\"2\", payload={\"text\": \"two\"}),\n            vectorizer(vector=[1.0, 2.0]),  # or any other vector generator\n        ],\n    )\n    # [END howto_operator_pgvector_insert_embedding]\n\n    # [START howto_operator_pgvector_search_by_vector]\n    client.search(\n        collection_name=\"my_collection\",\n        query_vector=[1.0, 2.0],\n        limit=1,\n    )\n    # [END howto_operator_pgvector_search_by_vector]\n\n    # [START howto_operator_pgvector_search_by_id]\n    client.search(\n        collection_name=\"my_collection\",\n        query_id=[\"1\", \"2\"],\n        limit=2,\n    )\n    # [END howto_operator_pgvector_search_by_id]\n\n    # [START howto_operator_pgvector_search_multiple]\n    client.search(\n        collection_name=\"my_collection\",\n        query_vectors=[[1.0, 2.0], [1.0, 2.0]],\n        limit=2,\n    )\n    # [END howto_operator_pgvector_search_multiple]\n\n    # [START howto_operator_pgvector_cleanup]\n    client.delete_collection(\"my_collection\")\n    # [END howto_operator_pgvector_cleanup]"}], "metadata": {"file_name": "dag_989_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a cloud-based data processing job using Google Dataproc, focusing on the sequential task dependencies of cluster creation, job submission, and cluster deletion. Implement a robust workflow that handles infrastructure provisioning and job execution with clear, predictable task progression.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"google-cloud-dataproc-job-template\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"master_machine_type\": \"n1-standard-4\",\n        \"num_preemptible_workers\": 1,\n        \"preemptibility\": \"PREEMPTIBLE\",\n    },\n    \"worker_config\": {\n        \"num_preemptible_workers\": 2,\n        \"preemptibility\": \"PREEMPTIBLE\",\n        \"node_config\": {\n            \"image_version\": \"2.1\",\n            \"config_bucket\": \"airflow-system-tests-resources\",\n            \"jvm_options\": \"-Duser.timezone=-4.0\",\n            \"num_preemptible_workers\": 2,\n            \"preemptibility\": \"PREEMPTIBLE\",\n        },\n    },\n}\n\n# Jobs definitions\n# [START how_to_dataproc_submit_job_yql]\nJOB_1_YQL = {\n    \"reference\": {\"project_id\": PROJECT_ID, \"cluster_name\": CLUSTER_NAME},\n    \"config\": {\"yql\": {\"query\": {\"sql_string\": \"select * from hw\"}}},\n}\n# [END how_to_dataproc_submit_job_yql]\n\n# [START how_to_dataproc_submit_job_spark]\nJOB_2_SPARK = {\n    \"reference\": {\"project_id\": PROJECT_ID, \"cluster_name\": CLUSTER_NAME},\n    \"config\": {\"spark\": {\"appScript\": {\"script\": \"/scripts/hello_world_script.py\"}}},\n}\n# [END how_to_dataproc_submit_job_spark]\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"job_template\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        config=CLUSTER_CONFIG,\n    )\n\n    # [START how_to_dataproc_job_template_rendered]\n    render_job = DataprocSubmitJobOperator(\n        task_id=\"render_job\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        job=JOB_1_YQL,\n    )\n    # [END how_to_dataproc_job_template_rendered]\n\n    # [START how_to_dataproc_job_template_for_inlet]\n    inlet_job = DataprocSubmitJobOperator(\n        task_id=\"inlet_job\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        job=JOB_2_SPARK,\n        should_notify=True,\n    )\n    # [END how_to_dataproc_job_template_for_inlet]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> render_job\n        >> inlet_job\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils import get_test_run  # noqa: E402\n\n    # This test needs test_run() method which is created in test_system_tests\n    test_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_863_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you help me with a Python code to filter and sum a list of numbers based on a condition?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"filter_task\")\ndef filter_func(filters, context):\n    date = pendulum.context_to_datetime(context)\n    return any(str(date)[-2:] == s for s in filters)\n\n\n@task\ndef sum_func(body, context):\n    return sum(body)\n\n\n@task\ndef print_summary(body, context):\n    print(f\"Sum of numbers with last two digits in {context['ti'].task_id} is {body}\")\n\n\nwith DAG(\n    dag_id=\"example_filter_and_sum\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n):\n    filter_func.partial().expand(filters=[\"00\", \"11\", \"22\", \"33\", \"44\", \"55\"], task_id=\"filter_task\")\n    sum_func.partial().expand(body_ctx=\"filter_task\", task_id=\"sum_task\")\n    print_summary.partial().expand(body=\"sum_task_output\", task_id=\"print_summary\")"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across AWS services like RDS, EC2, and S3. Develop a modular pipeline that creates necessary cloud resources, configures security settings, and executes a data replication process.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsCreateDbSnapshotCopyOperator,\n    RdsDeleteDbSnapshotOperator,\n    RdsDeleteDbSnapshotCopyOperator,\n    RdsDropDbOperator,\n    RdsStartDbSnapshotCopyOperator,\n    RdsStopDbSnapshotCopyOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RdsSnapshotCopyJobCompletedSensor\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_DATA = [\n    (\"Airflow\", 1),\n    (\"Python\", 2),\n    (\"DAGs\", 3),\n    (\"System Test\", 4),\n]\n\nSQL_TABLE = \"sample_table\"\nSQL_CREATE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SQL_TABLE} (\n    flight_name VARCHAR(100),\n    num INTEGER\n    );\n\"\"\"\n\nSQL_INSERT = f\"INSERT INTO {SQL_TABLE} VALUES ({','}.split(), %(num)s)\"\nSQL_SELECT = f\"SELECT * FROM {SQL_TABLE}\"\nSQL_DROP = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSAMPLE_S3_PATH = \"s3://your_bucket_for_system_test/\"\nS3_DESTINATION = SAMPLE_S3_PATH + \"aws_datawarehouse_output/\"\nFILE_NAME = \"united-states-airline-passenger-data.csv\"\n\n\n@task\ndef create_connection(conn_id_name: str, rds_db_name: str, db_instance_endpoints: str):\n    RdsHook(aws_conn_id=conn_id_name).conn.create_db_connection(\n        db_type=\"RDS\", db_name=rds_db_name, endpoint=db_instance_endpoints\n    )\n\n\n@task\ndef create_sample_table(conn_id_name: str):\n    SQLHook(aws_conn_id=conn_id_name).conn.create_table(\n        SQL_TABLE,\n        SQL_CREATE,\n        SQL_INSERT.format(num=\"%(num)s%\"),\n        SQL_SELECT,\n        SQL_DROP,\n    )\n\n\n@task\ndef load_data_to_sample_table(conn_id_name: str):\n    SQLHook(aws_conn_id=conn_id_name).conn.execute(SQL_INSERT, dict(num=max(SAMPLE_DATA, key=lambda x: x[1])[1]))\n\n\n@task\ndef drop_sample_table(conn_id_name: str):\n    SQLHook(aws_conn_id=conn_id_name).conn.execute(SQL_DROP)\n\n\n@task\ndef create_security_group(sec_group_name: str):\n    RdsHook(aws_conn_id=\"rds_default\").conn.create_security_group(\n        GroupName=sec_group_name, Description=\"Security group for SageMaker console testing\"\n    )\n    RdsHook(aws_conn_id=\"rds_default\").conn.get_security_group(\n        GroupName=sec_group_name\n    ).id\n\n\n@task\ndef enable_vpc_security_grouping(sec_group_name: str):\n    RdsHook(aws_conn_id=\"rds_default\").conn.enable_vpc_security_grouping(sec_group_name)\n\n\n@task\ndef get_db_snapshot_copy_role_arn(conn_id_name: str):\n    return RdsHook(aws_conn_id=conn_id_name).conn.get_db_snapshot_copy_role_arn()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_name: str, conn_id_name: str):\n    RdsHook(aws_conn_id=conn_id_name).conn.delete_security_group(\n        GroupName=sec_group_name, ForceDelete=True\n    )\n\n\n@task\ndef cleanup_rds_connection(conn_id_name: str, rds_db_name: str):\n    RdsHook(aws_conn_id=conn_id_name).conn.delete_db_connection(\n        DbUser=username, DbName=rds_db_name, Endpoint=db_instance_endpoint\n    )\n\n\n@task\ndef cleanup_s3_bucket(bucket_name: str):\n    S3Hook(aws_conn_id=\"default\").delete_bucket(bucket_name=bucket_name)\n\n\nwith DAG(\n    \"aws_datawarehouse_system_test\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    rds_db_name = f\"{env_id}-db\"\n    rds_instance_endpoint = f\"{env_id}-db-endpoint\"\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_destination_key = f\"{env_id}/aws_datawarehouse_output/\"\n    sample_table = f\"{env_id}-sample-table\"\n\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"s3_create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    create_sample_db = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_db_name,\n        db_instance_class=\"db.t4g\",\n        engine=\"postgres\",\n        rds_db_name=rds_db_name,\n        db_instance_endpoint=rds_instance_endpoint,\n        vpc_security_group_ids=[enable_vpc_security_grouping(sec_group_name=\"sample-sg\")],\n        publicly_accessible=False,\n        master_username=\"adminuser\",\n        master_user_password=\"Cm9tIGV2ZW50aWNhdGlvbi4=\",\n        s3_data_path=f\"{SAMPLE_S3_PATH}{env_id}/\",\n        s3_data_key_prefix=f\"{S3_DESTINATION}{env_id}/\",\n        default_role=\"AWS_Identity_Access_Role\",\n        vpc_security_group_ids=[enable_vpc_security_grouping(sec_group_name=\"sample-sg\")],\n    )\n\n    load_sample_data = RdsCreateDbSnapshotCopyOperator(\n        task_id=\"snapshot_copy\",\n        db_snapshot_copy_args={\n            \"db_snapshot_identifier\": rds_db_name,\n            \"storage_medium\": \"IN_MEMORY\",\n            \"storage_size_gb\": 1,\n            \"source_location\": s3_destination_key,\n            \"path_prefix\": s3_destination_key,\n            \"rds_snapshot_copy_role_arn\": get_db_snapshot_copy_role_arn(conn_id_name=\"rds_default\"),\n        },\n    )\n\n    # [START howto_sensor_snapshot_copy]\n    wait_for_snapshot_copy = RdsSnapshotCopyJobCompletedSensor(\n        task_id=\"wait_for_snapshot_copy\",\n        job_identifier=load_sample_data.output,\n    )\n    # [END howto_sensor_snapshot_copy]\n\n    drop_sample_db = RdsDropDbOperator(\n        task_id=\"drop_sample_db\",\n        db_name=rds_db_name,\n    )\n\n    @task\n    def get_table_data(_ctx):\n        connection = Connection(\n            conn_id=\"rds_default\",\n            user=\"adminuser\",\n            password=\"Cm9tIGV2ZW50aWNhdGlvbi4=\",\n        )\n        sql_conn = SQLHook(sql_conn_id=connection.conn_id)\n        return sql_conn.execute(SQL_SELECT.format(table=SQL_TABLE))[0][0]\n\n    # [START howto_operator_sql]\n    show_table_data = SQLExecuteQueryOperator(\n        task_id=\"show_table_data\",\n        sql=SQL_SELECT.format(table=SQL_TABLE),\n        conn_id=\"rds_default\",\n        autocommit=True,\n    )\n    # [END howto_operator_sql]\n\n    # [START howto_operator_bash]\n    bash_command = BashOperator(\n        task_id=\"bash_command\",\n        bash_command=f\"echo {get_table_data(_ctx=_ctx)}\",\n    )\n    # [END howto_operator_bash]\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> create_sample_db\n        >> load_sample_data\n        >> wait_for_snapshot_copy\n        >> drop_sample_db\n        # TEST BODY\n        >> get_table_data\n        >> show_table_data\n        # TEST TEARDOWN\n        >> cleanup_rds_connection(conn_id_name=\"rds_default\", rds_db_name=rds_db_name)\n        >> cleanup_s3_bucket(bucket_name=s3_bucket_name)\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1477_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily Twitter data pipeline that collects tweets from multiple accounts, processes them for insights, and stores the results in a database. Create a workflow that captures incoming and outgoing tweets, performs sentiment analysis, and prepares data for further business intelligence.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, dag\nfrom airflow.models.api import ApiEnd\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskmap import TaskMap\nfrom airflow.models.trigger_dag import TriggerDag\nfrom airflow.models.trigger_dag_list import TriggerDagList\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.hooks.base import StandardHook\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.sdk import DAG, task\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.time import datetime\nfrom airflow.utils.weekly import Weekly\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekly import Weekly\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow.utils.trigger_reason import TriggerReason\nfrom airflow"}], "metadata": {"file_name": "dag_1939_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud file management using Azure operators. Develop a DAG that illustrates how to programmatically upload and remove files from a remote storage system.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import AzureADLSWriteOperator\nfrom airflow.providers.microsoft.azure.sensors.adls import AzureADLSSensor\n\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nREMOTE_FILE_PATH = os.environ.get(\"REMOTE_LOCAL_PATH\", \"remote.txt\")\nCONNECTION_ID = os.environ.get(\" CONNECTION_ID\")\n\n\nwith DAG(\n    \"adls-dags\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_adls_sensor():\n        connection = Connection(\n            conn_id=CONNECTION_ID, conn_type=AzureADLSSensor, extra={\"path\": REMOTE_FILE_PATH}\n        )\n        return connection\n\n    adls_sensor = create_adls_sensor()\n\n    @task\n    def create_file_in_adls(connection_id: str):\n        connection = Connection(\n            conn_id=connection_id, conn_type=AzureADLSWriteOperator, extra={\"path\": REMOTE_FILE_PATH}\n        )\n        adls_write_operator = AzureADLSWriteOperator(task_id=\"test\", connection=connection)\n        adls_write_operator.trigger_rule = \"all_done\"\n        adls_write_operator.wait_for_completion = False\n\n    create_file_in_adls()\n\n    # [START howto_operator_adls_sensor]\n    check_file = AzureADLSSensor(task_id=\"check_file\", connection_id=CONNECTION_ID, path=REMOTE_FILE_PATH)\n    # [END howto_operator_adls_sensor]\n\n    join_string = os.environ.get(\"JOIN_STRING\", \"and\")\n    message = f\"File created: {LOCAL_FILE_PATH:;10}  File written: {REMOTE_FILE_PATH:;10}\"\n    @task\n    def print_message(message: str):\n        print(message)\n\n    print_message(message)\n\n    create_file_in_adls() >> check_file"}], "metadata": {"file_name": "dag_1750_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task chaining and cross-task communication by translating text and then accessing the translation result through XCom. Demonstrate how different operators can interact and pass data between tasks in a sequential workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.asb_translate import ASBRenameTranslateOperator, ASBTranslateOperator\n\nASB_CONN_ID = os.environ.get(\"ASB_CONN_ID\", \"azure_translator_default\")\nREPLICA_ID = os.environ.get(\"REPLICA_ID\", \"translation-resource\")\nPRIVATEDLAN_CONNECTION = os.environ.get(\"PRIVATEDLAN_CONNECTION\", \"private_lan_default_connection\")\n\nwith DAG(\n    \"example_asb_translate_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # [START asb_translate_operator]\n    translate_text = ASBTranslateOperator(\n        task_id=\"translate_text\",\n        input_values=\"Me and my dog. He's got the cutest eyes you've ever seen.\",\n        source_language_code=\"en\",\n        target_language_code=\"fr\",\n        amount=1,\n        azure_translator_conn_id=ASB_CONN_ID,\n    )\n    # [END asb_translate_operator]\n\n    # [START asb_translate_operator_result]\n    translate_text_result = translate_text.output\n    # [END asb_translate_operator_result]\n\n    # [START asb_translate_operator_result_xcom]\n    xcom_task_result = XComTaskResultOperator(task_id=\"task_result\", xcom_key=\"result\", output=translate_text_result)\n    # [END asb_translate_operator_result_xcom]\n\n    # [START asb_translate_operator_with_xcom_input]\n    translate_text_with_xcom_input = ASBTranslateOperator(\n        task_id=\"translate_text_with_xcom_input\",\n        input_values=\"Me and my dog. He's got the cutest eyes you've ever seen.\",\n        source_language_code=\"en\",\n        target_language_code=\"fr\",\n        amount=1,\n        azure_translator_conn_id=ASB_CONN_ID,\n        xcom_key=\"value\",\n    )\n    # [END asb_translate_operator_with_xcom_input]\n\n    # [START asb_translate_operator_result_xcom_with_task]\n    translate_text_with_task_result = XComTaskResultOperator(\n        task_id=\"task_result_with_xcom\", xcom_key=\"result\", output=translate_text_with_xcom_input.output\n    )\n    # [END asb_translate_operator_result_xcom_with_task]\n\n    # [START asb_rename_translate_operator]\n    rename_text = ASBRenameTranslateOperator(\n        task_id=\"rename_text\",\n        input_values=\"Me and my dog. He's got the cutest eyes you've ever seen.\",\n        source_language_code=\"en\",\n        target_language_code=\"fr\",\n        amount=1,\n        azure_translator_conn_id=ASB_CONN_ID,\n        private_lan_connection=PRIVATEDLAN_CONNECTION,\n    )\n    # [END asb_rename_translate_operator]\n\n    # [START asb_rename_translate_operator_result]\n    rename_text_result = rename_text.output\n    # [END asb_rename_translate_operator_result]\n\n    # [START asb_rename_translate_operator_result_xcom]\n    xcom_rename_task_result = XComTaskResultOperator(\n        task_id=\"task_result_with_xcom\", xcom_key=\"result\", output=rename_text_result\n    )\n    # [END asb_rename_translate_operator_result_xcom]\n\n\nfrom tests_common.test_utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1297_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS cloud infrastructure orchestration with dynamic security configuration and EMR cluster management. Develop a workflow that creates an S3 bucket, configures security settings, launches an EMR cluster, executes a Spark step, and handles cluster lifecycle management.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.client import BaseClient\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.emr import AmazonEmrHook\nfrom airflow.providers.amazon.aws.hooks.s3 import AmazonS3Hook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    AmazonEmrCreateClusterOperator,\n    AmazonEmrDeleteClusterOperator,\n    AmazonEmrStartJobFlowOperator,\n    AmazonEmrStopJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import AmazonEmrJobSensor, AmazonEmrSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import weekday\nfrom airflow.utils.trigger_rule import WeekDayTriggerRule\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_emr\"\nSCHEMA = \"record:array(string)\"\nROLE_ARN_KEY = \"ROLE_ARN\"\nBUCKET_NAME_KEY = \"BUCKET_NAME\"\nINPUT_FILE = \"example-steps.csv\"\nOUTPUT = \"output.csv\"\nINPUT_S3 = \"s3://bucket/name/\"\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"EmrCluster\",\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceCount\": 1,\n                \"HadoopVersion\": \"3.3.0\",\n                \"InstanceType\": \"m5.xlarge\",\n            },\n        ],\n        \"TerminationPolicy\": {\"Name\": \"EARLY\", \"Seconds\": 1200},\n        \"HasSeparateMetaStoreInstance\": False,\n        \"Ec2Configuration\": {\n            \"InstanceType\": \"m5.xlarge\",\n            \"InstanceCount\": 1,\n        },\n    },\n    \"Applications\": [\n        {\"Name\": \"Spark\"},\n        {\"Name\": \"Hadoop\"},\n    ],\n    \"JobFlowRole\": \"EMR_EC2-Proxy-1276973379\",\n    \"ServiceRole\": \"EMR_Default_Security_Identity\",\n    \"VisibleToAllUsers\": True,\n    \"Config\": {\n        \"JobFlowRole\": \"EMR_EC2-Proxy-1276973379\",\n        \"Instances\": {\n            \"InstanceGroups\": [\n                {\n                    \"Name\": \"Primary node\",\n                    \"Market\": \"ON_DEMAND\",\n                    \"InstanceRole\": \"MASTER\",\n                    \"InstanceCount\": 1,\n                    \"InstanceType\": \"m5.xlarge\",\n                }\n            ],\n            \"TerminationPolicy\": {\"Name\": \"EARLY\", \"Seconds\": 1200},\n            \"HasSeparateMetaStoreInstance\": False,\n            \"Ec2Configuration\": {\n                \"InstanceType\": \"m5.xlarge\",\n                \"InstanceCount\": 1,\n            },\n        },\n        \"InstancesMetadataService\": {\n            \"Enabled\": False,\n            \"RoleArn\": \"\",\n        },\n        \"ServiceRole\": \"EMR_Default_Security_Identity\",\n        \"VisibleToAllUsers\": True,\n    },\n}\nS3_DESTINATION = \"s3://bucket/destination/\"\nSTAGE_1 = {\n    \"Name\": \"Starter\",\n    \"ActionOnFailure\": \"TERMINATE\",\n    \"HadoopJarStep\": {\n        \"Jar\": \"command-runner.jar\",\n        \"Args\": [f\"{INPUT_S3}{INPUT_FILE}\", f\"{S3_DESTINATION}{OUTPUT}\"],\n    },\n}\nSTAGE_2 = {\n    \"Name\": \"ShellScript\",\n    \"ActionOnFailure\": \"TERMINATE\",\n    \"ShellScriptStep\": {\n        \"ActionOnFailure\": \"TERMINATE\",\n        \"HadoopJarStep\": {\n            \"Jar\": \"Command-runner.sh\",\n            \"Args\": [\n                \"spark-submit\",\n                \"--master\",\n                \"spark://spark-node:7077\",\n                \"--conf\",\n                f\"spark.driver.cores={1}\",\n                \"--conf\",\n                f\"spark.driver.memory={1024}\",\n                f\"{INPUT_S3}{INPUT_FILE}\",\n                f\"{S3_DESTINATION}{OUTPUT}\",\n            ],\n        },\n    },\n}\nROLE_ARN = os.environ.get(ROLE_ARN_KEY, \"\")\nBUCKET_NAME = os.environ.get(BUCKET_NAME_KEY, \"\")\nJOB_FLOW_OVERRIDES[SCHEMA] = SCHEMA\nJOB_FLOW_OVERRIDES[BUCKET_NAME_KEY] = BUCKET_NAME\nSAMPLE_DATA_S3 = JOB_FLOW_OVERRIDES[SCHEMA].replace(\",\", \";\")\nSAMPLE_Bucket = JOB_FLOW_OVERRIDES[BUCKET_NAME_KEY].replace(\",\", \";\")\ndata = f\"\"\"amazonaws;csv;record;array(string);0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;"}], "metadata": {"file_name": "dag_389_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data pipeline that showcases vector embedding ingestion with flexible metadata handling and batch processing capabilities. Create a demonstration of how to programmatically insert vector data into a vector database using Airflow's integration with Pinecone.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinecone.operators.pinecone import PineconeIngestOperator\n\nindex_name = \"example-pinecone-index\"\n\n\n@task\ndef create_index():\n    from airflow.providers.pinecone.hooks.pinecone import PineconeHook\n\n    PineconeHook().create_index(index_name=index_name)\n\n\n@task\ndef drop_index():\n    from airflow.providers.pinecone.hooks.pinecone import PineconeHook\n\n    PineconeHook().delete_index(index_name=index_name)\n\n\nwith DAG(\n    \"example_pinecone_ingest\",\n    schedule=pendulum.period(pendulum.day_of_week.monday, pendulum.day_of_month.tuesday),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"pinecone\", \"ingest\"],\n) as dag:\n    create_index()\n    # [START howto_pinecone_ingest]\n    ingest_task = PineconeIngestOperator(\n        task_id=\"pinecone_ingest_task\",\n        index_name=index_name,\n        input_data=[\n            (\"sample text 1\", \"c1q0\", \"3423423424-3423-4323-3423-43233423423423\", \"9999-11-11\"),\n        ],\n    )\n    # [END howto_pinecone_ingest]\n    drop_index()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_639_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that demonstrates multiple generative AI model interactions using Vertex AI, showcasing text generation, embedding, and multimodal processing capabilities.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "import os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.hooks.openai import OpenAIHook\nfrom airflow.providers.openai.operators.openai import OpenAITextGenerationOperator\nfrom airflow.providers.openai.operators.openai import OpenAITextEmbeddingOperator\nfrom airflow.providers.openai.operators.openai import OpenAITextMultimodalEmbeddingOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_openai\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    connection = Connection(\n        conn_id=\"openai_default\",\n        conn_type=\"openai\",\n        extra={\"api_version\": \"2023-05-15\", \"api_key\": \"\"},\n    )\n\n    @task\n    def create_openai_default_connection():\n        dag.dag_run.xcom_pull(task_ids=[\"get_openai_default_xcom\"], key=\"return_value\", target_dag=dag)\n        openai_hook = OpenAIHook()\n        openai_hook.conn = openai_hook.get_connection(\"openai_default\")\n        return\n\n    get_openai_default_xcom = BashOperator(\n        task_id=\"get_openai_default_xcom\",\n        bash_command=f\"airflow xcom_pull {DAG_ID} get {connection.id}\",\n    )\n\n    @task\n    def check_connection():\n        assert openai_hook.conn is not None\n        return\n\n    generate_code = OpenAITextGenerationOperator(\n        task_id=\"generate_code\",\n        model=\"text-davinci-002\",\n        input_text=\"Hello World\",\n        max_tokens=1024,\n        output_format=\"json\",\n    )\n\n    embed_code = OpenAITextEmbeddingOperator(\n        task_id=\"embed_code\",\n        model=\"text-embedding-ada-002\",\n        input_text=\"Hello World\",\n        output_mode=\"vector\",\n    )\n\n    multimodal_embed_code = OpenAITextMultimodalEmbeddingOperator(\n        task_id=\"multimodal_embed_code\",\n        model=\"text-multimodal-embedding-ada-002\",\n        input_text=\"Hello World\",\n        input_json\u800c\u8a00=\"'\",\n        output_mode=\"vector\",\n    )\n\n    generate_hello_world = OpenAITextGenerationOperator(\n        task_id=\"generate_hello_world\",\n        model=\"text-davinci-002\",\n        input_text=\"Hello World\",\n        max_tokens=1024,\n        output_format=\"json\",\n    )\n\n    embed_hello_world = OpenAITextEmbeddingOperator(\n        task_id=\"embed_hello_world\",\n        model=\"text-embedding-ada-002\",\n        input_text=\"Hello World\",\n        output_mode=\"vector\",\n    )\n\n    multimodal_embed_hello_world = OpenAITextMultimodalEmbeddingOperator(\n        task_id=\"multimodal_embed_hello_world\",\n        model=\"text-multimodal-embedding-ada-002\",\n        input_text=\"Hello World\",\n        input_json\u800c\u8a00=\"'\",\n        output_mode=\"vector\",\n    )\n\n    @task\n    def check_embedding(**context):\n        get_xcom_value = context[\"ti\"].xcom_pull(task_ids=[\"embed_code\"], key=\"return_value\")\n        assert get_xcom_value.embedding_vector is not None\n        return\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_openai_default_connection():\n        dag.dag_run.xcom_pull(task_ids=[\"get_openai_default_xcom\"], key=\"return_value\", target_dag=dag)\n        openai_hook = OpenAIHook()\n        openai_hook.conn = openai_hook.get_connection(\"openai_default\")\n        openai_hook.conn.delete()\n        return\n\n    (\n        [create_openai_default_connection(), get_openai_default_xcom()]\n        >> check_connection()\n        >> [generate_code, embed_code, multimodal_embed_code]\n        >> generate_hello_world\n        >> embed_hello_world\n        >> multimodal_embed_hello_world\n        >> check_embedding()\n        >> delete_openai_default_connection()\n    )"}], "metadata": {"file_name": "dag_934_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates calling stored procedures with different parameter passing techniques. Create a DAG that showcases input and output parameter handling for database operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle_stored_procedure\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_oracle_stored_procedure_operator]\n    # Example of the use of OracleStoredProcedureOperator.\n    stored_proc = OracleStoredProcedureOperator(\n        task_id=\"stored_proc_example',\n        oracle_conn_id=\"oracle\",\n        procedure=\"Demo Stored Procedure',\n        parameters=(\n            'in_value',\n            [1, 2, 3]),\n        staging_table=\"STAGE_TMP',\n        query=True,\n    )\n    # [END howto_oracle_stored_procedure_operator]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1770_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an enterprise-level data extraction and cloud storage transfer process that demonstrates best practices for moving relational database content to a cloud storage platform. Construct an Airflow workflow that handles data export from Oracle, transfers the data to Google Cloud Storage in CSV format, and ensures proper resource management through automated bucket creation and deletion.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.oracle_to_gcs import OracleToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_oracle_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILENAME = \"test_file\"\n\n# Oracle connection parameters\norcl_conn_id = \"orcl\"\noracle_kwargs = {\n    \"sql\": \"SELECT 1 FROM dual\",\n    \"use_server_default_format\": True,\n}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"oracle\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_oracle_to_gcs]\n    upload_oracle_to_gcs = OracleToGCSOperator(\n        task_id=\"oracle_to_gcs\",\n        oracle_conn_id=orcl_conn_id,\n        oracle_kwargs=oracle_kwargs,\n        bucket=BUCKET_NAME,\n        filename=FILENAME,\n        replace=True,\n    )\n    # [END howto_operator_oracle_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n        assume_role=True,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_oracle_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_221_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I need a Python code snippet to compare the similarity between two text documents using the cosine similarity method.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.openai.sensors.openai import OpenAISensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# create a DAG for testing OpenAI Embedding Operator and OpenAI Sensor\nwith DAG(\n    dag_id=\"example_openai\",\n    start_date=None,  # Override to match your needs\n    catchup=False,\n    tags=[\"example\"],\n    schedule=None,\n    dependencies=[\n        \"openai_operator_sensor_dependency_1\",\n        \"openai_operator_sensor_dependency_2\",\n    ],\n) as dag:\n    # Empty task to override task dependency\n    task.openai_operator_sensor_dependency_1 = OpenAIEmbeddingOperator.partial(\n        task_id=\"openai_operator_sensor_dependency_1\"\n    ).expand(input_text=\"comparison\")  # type: ignore[arg-type]\n\n    # [START howto_operator_openai_embedding]\n    task.openai_operator = OpenAIEmbeddingOperator(\n        task_id=\"openai_operator\",\n        input_text=\"biology is the study of life forms both on and off Earth.\",\n    )\n    # [END howto_operator_openai_embedding]\n\n    # [START howto_sensor_openai]\n    sensor_openai = OpenAISensor(\n        task_id=\"sensor_openai\", input_text=\"space\", openai_conn_id=\"openai_default\"\n    )\n    # [END howto_sensor_openai]\n\n    sensor_openai >> task.openai_operator >> task.openai_operator_sensor_dependency_1"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data extraction and transfer system that demonstrates extracting data from a PostgreSQL database and moving it to a Google Cloud Storage bucket with full infrastructure provisioning. The solution should highlight how to programmatically create cloud resources, configure database connections, and perform secure data migrations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.postgres_to_gcs import PostgresToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_sheets_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET = {\n    \"properties\": {\"title\": \"Test Sheet\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Test Sheet\"}}],\n}\nPOSTGRES_CONN_ID = \"postgres_default\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sheets\", \"gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_spreadsheet = GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet\",\n        body=SPREADSHEET,\n        postgres_conn_id=POSTGRES_CONN_ID,\n        postgres_kwargs={\"charset\": \"utf8\"},\n    )\n\n    # [START howto_operator_pg_to_gcs]\n    pg_to_gcs = PostgresToGCSOperator(\n        task_id=\"pg_to_gcs\",\n        sql=\"select * from test_db.test_table\",\n        bucket=BUCKET_NAME,\n        postgres_conn_id=POSTGRES_CONN_ID,\n        postgres_kwargs={\"charset\": \"utf8\"},\n    )\n    # [END howto_operator_pg_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_spreadsheet\n        # TEST BODY\n        >> pg_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_576_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the complete lifecycle of a Cloud Data Fusion instance, including creation, pipeline deployment, execution, and cleanup. Implement a comprehensive demonstration of cloud data infrastructure management using Google Cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.datafusion import CloudDataFusionHook\nfrom airflow.providers.google.cloud.operators.datafusion import (\n    CloudDataFusionCreateInstanceOperator,\n    CloudDataFusionCreatePipelineOperator,\n    CloudDataFusionDeleteInstanceOperator,\n    CloudDataFusionDeletePipelineOperator,\n    CloudDataFusionGetInstanceOperator,\n    CloudDataFusionGetPipelineOperator,\n    CloudDataFusionListPipelinesOperator,\n    CloudDataFusionUpdateInstanceOperator,\n    CloudDataFusionUpdatePipelineOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.datafusion import CloudDataFusionPipelineStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_datafusion\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET = f\"gs://{BUCKET_NAME}\"\nFILE = \"test-file\"\nSTORE_NAME = f\"store_{DAG_ID}_{ENV_ID}\"\nSTORE = f\"gs://{BUCKET_NAME}/{FILE}\"\nQUERY = \"SELECT * FROM testdf\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datafusion\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_connection():\n        connection = Connection(\n            conn_id=\"test_connection\",\n            user=\"test_user\",\n            password=\"test_password\",\n            schema=\"test_schema\",\n            conn_type=\"gcs\",\n            extra={\"path\": STORE},\n        )\n        hook = CloudDataFusionHook()\n        hook.create_connection(connection=connection, name=\"test_connection\", project_id=PROJECT_ID)\n\n    create_connection_task = create_connection()\n\n    create_instance = CloudDataFusionCreateInstanceOperator(\n        task_id=\"create_instance\",\n        display_name=f\"test_display_name_{DAG_ID}_{ENV_ID}\",\n        query=QUERY,\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        name=\"test_display_name_{DAG_ID}_{ENV_ID}\",\n        schema_name=\"test_schema\",\n        connection_name=\"test_connection\",\n    )\n\n    instance_id = create_instance.output[\"instance_id\"]\n\n    create_pipeline = CloudDataFusionCreatePipelineOperator(\n        task_id=\"create_pipeline\",\n        storage_bucket=BUCKET,\n        application_id=\"test_application_id\",\n        name=f\"test_application_{DAG_ID}_{ENV_ID}\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        instance_id=instance_id,\n    )\n\n    pipeline_id = create_pipeline.output[\"pipeline_id\"]\n\n    pipeline_state = CloudDataFusionPipelineStateSensor(\n        task_id=\"pipeline_state\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        application_id=\"test_application_id\",\n        pipeline_id=pipeline_id,\n        target_state=\"RUNNING\",\n    )\n\n    list_pipelines = CloudDataFusionListPipelinesOperator(\n        task_id=\"list_pipelines\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n    )\n\n    get_pipeline = CloudDataFusionGetPipelineOperator(\n        task_id=\"get_pipeline\", region=\"europe-west1\", project_id=PROJECT_ID, application_id=\"test_application_id\", pipeline_id=pipeline_id\n    )\n\n    get_instance = CloudDataFusionGetInstanceOperator(\n        task_id=\"get_instance\", region=\"europe-west1\", project_id=PROJECT_ID, instance_id=instance_id\n    )\n\n    update_pipeline = CloudDataFusionUpdatePipelineOperator(\n        task_id=\"update_pipeline\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        application_id=\"test_application_id\",\n        pipeline_id=pipeline_id,\n        query=\"SELECT * FROM testdf?format=bigquery\",\n    )\n\n    update_instance = CloudDataFusionUpdateInstanceOperator(\n        task_id=\"update_instance\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        instance_id=instance_id,\n        new_display_name=f\"test_display_name_{DAG_ID}_{ENV_ID}\",\n    )\n\n    delete_pipeline = CloudDataFusionDeletePipelineOperator(\n        task_id=\"delete_pipeline\", region=\"europe-west1\", project_id=PROJECT_ID, application_id=\"test_application_id\", pipeline_id=pipeline_id\n    )\n\n    delete_instance = CloudDataFusionDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        region=\"europe-west1\",\n        project_id=PROJECT_ID,\n        instance_id=instance_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_connection_task\n        >> create_instance\n        >> create_pipeline\n        >> pipeline_state\n        >> list_pipelines\n        >> get_pipeline\n        >> get_instance\n        # TEST BODY\n        >> update_pipeline\n        >> update_instance\n        # TEST TEARDOWN\n        >> delete_pipeline\n        >> delete_instance\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1617_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates safe data transaction practices by implementing a complete transaction management scenario in Google Cloud Datastore. Construct a workflow that shows how to start a transaction and then safely roll it back without persisting potentially unwanted data modifications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreBeginTransactionOperator,\n    CloudDatastoreCommitOperator,\n    CloudDatastoreDeleteOperationOperator,\n    CloudDatastoreBeginRollbackOperator,\n    CloudDatastoreGetOperationOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_transaction\"\nLOCATION = os.environ.get(\"DATASTORE_TRANSACTION_LOCATION\", \"europe-west1\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datastore\"],\n) as dag:\n    # [START how_to_cloud_datastore_begin_transaction]\n    start_time = datetime.now()\n    transaction_id = \"{{ task_instance.xcom_pull(task_ids='get_id') }}\"\n\n    begin_transaction_inputs = {\n        \"partitionId\": {\"projectId\": PROJECT_ID, \"location\": LOCATION},\n        \"readTransaction\": {\"readWrite\": {\"mode\": \"TRANSACTION\"}},\n        \"transactions\": [{\"readWrite\": {\"id\": transaction_id}}],\n    }\n\n    start_transaction = CloudDatastoreBeginTransactionOperator(\n        task_id=\"start_transaction\",\n        **begin_transaction_inputs,\n    )\n    # [END how_to_cloud_datastore_begin_transaction]\n\n    # [START how_to_cloud_datastore_get_operation]\n    wait_for_operation = CloudDatastoreGetOperationOperator(\n        task_id=\"wait_for_operation\", operation_name=start_transaction.output[\"name\"]\n    )\n    # [END how_to_cloud_datastore_get_operation]\n\n    # [START how_to_cloud_datastore_begin_commit]\n    entity = {\"kind\": \"TestEntity\", \"id\": \"test_id\", \"value\": \"test_value\"}\n    await_operation = start_transaction.output[\"name\"]\n\n    commit_inputs = {\n        \"partitionId\": {\"projectId\": PROJECT_ID, \"location\": LOCATION},\n        \"transaction\": {\"readWrite\": {\"id\": \"{{ task_instance.xcom_pull(task_ids='get_id') }}\"}},\n        \"operations\": [entity],\n        \"waitForCompletion\": True,\n    }\n\n    begin_commit = CloudDatastoreBeginCommitOperator(task_id=\"begin_commit\", **commit_inputs)\n    # [END how_to_cloud_datastore_begin_commit]\n\n    # [START how_to_cloud_datastore_get_operation]\n    wait_for_commit = CloudDatastoreGetOperationOperator(\n        task_id=\"wait_for_commit\", operation_name=await_operation\n    )\n    # [END how_to_cloud_datastore_get_operation]\n\n    # [START how_to_cloud_datastore_delete_operation]\n    delete_commit_operation = CloudDatastoreDeleteOperationOperator(\n        task_id=\"delete_commit_operation\",\n        operation_name=wait_for_commit.output[\"name\"],\n    )\n    # [END how_to_cloud_datastore_delete_operation]\n\n    # [START how_to_cloud_datastore_begin_rollback]\n    rollback_id = \"{{ task_instance.xcom_pull(task_ids='get_id') }}\"\n    await_operation = commit_inputs[\"operations\"][0]\n\n    begin_rollback = CloudDatastoreBeginRollbackOperator(task_id=\"begin_rollback\", rollback_id=rollback_id)\n    # [END how_to_cloud_datastore_begin_rollback]\n\n    # [START how_to_cloud_datastore_get_operation]\n    wait_for_rollback = CloudDatastoreGetOperationOperator(\n        task_id=\"wait_for_rollback\", operation_name=await_operation\n    )\n    # [END how_to_cloud_datastore_get_operation]\n\n    # [END how_to_cloud_datastore_delete_operation]\n\n    (\n        # TEST SETUP\n        start_time\n        >> begin_transaction\n        # TEST BODY\n        >> begin_commit\n        >> wait_for_commit\n        >> delete_commit_operation\n        # TEST TEARDOWN\n        >> begin_rollback\n        >> wait_for_rollback\n        >> end_date\n    )\n\n    from tests.system.utils import get_test_run\n\n    # This test needs trigger rule of \"all\" to run\n    test_run = get_test_run(dag)\n    test_run.trigger_rule = TriggerRule.ALL_DONE"}], "metadata": {"file_name": "dag_876_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates Google Cloud Spanner's operational capabilities, including instance deployment, database schema evolution, and idempotent database modifications. Develop a comprehensive example that highlights different administrative tasks for cloud database management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.spanner import (\n    SpannerAddInstanceDatabaseOperator,\n    SpannerAddInstancePartitionOperator,\n    SpannerCreateInstanceOperator,\n    SpannerCreateInstancePartitionOperator,\n    SpannerDeleteInstanceOperator,\n    SpannerDeleteInstancePartitionOperator,\n    SpannerGetInstanceOperator,\n    SpannerImportDatabasesOperator,\n    SpannerInstanceIdOperator,\n    SpannerPatchInstanceOperator,\n    SpannerQueryOperator,\n    SpannerRemoveInstanceDatabaseOperator,\n    SpannerRemoveInstancePartitionOperator,\n    SpannerStartTransactionOperator,\n    SpannerStopTransactionOperator,\n    SpannerTrustCertificateOperator,\n    SpannerUploadDataOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"spanner\"\n\nGCP_LOCATION = \"europe-north1\"\n\n# Spanner instance id\nSPANNER_INSTANCE_ID = f\"spanner-{ENV_ID}\"\n# Spanner instance id for partition\nSPANNER_INSTANCE_ID_PART = f\"spanner-partition-{ENV_ID}\"\n# Database id\nSPANNER_DATABASE_ID = f\"spanner-db-{ENV_ID}\"\n# Partition id\nSPANNER_PARTITION_ID = f\"spanner-partition-database-{ENV_ID}\"\n\nINSERT_ROWS_SQL = \"INSERT table_name VALUES (1, 'value1'), (2, 'value2')\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"spanner\"],\n) as dag:\n    # [START howto_operator_spanner_create_instance]\n    create_instance = SpannerCreateInstanceOperator(\n        task_id=\"create_instance\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        labels={\"env\": \"production\"},\n    )\n    # [END howto_operator_spanner_create_instance]\n\n    # [START howto_operator_spanner_create_instance_result]\n    create_instance_result = models.ExternalTaskSensor(\n        task_id=\"create_instance_sensor\",\n        external_task_id=create_instance.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_create_instance_result]\n\n    # [START howto_operator_spanner_add_database]\n    add_database = SpannerAddInstanceDatabaseOperator(\n        task_id=\"add_database\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n    )\n    # [END howto_operator_spanner_add_database]\n\n    # [START howto_operator_spanner_add_database_result]\n    add_database_result = models.ExternalTaskSensor(\n        task_id=\"add_database_sensor\",\n        external_task_id=add_database.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_add_database_result]\n\n    # [START howto_operator_spanner_get_instance]\n    get_instance = SpannerGetInstanceOperator(\n        task_id=\"get_instance\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n        return_body=True,\n    )\n    # [END howto_operator_spanner_get_instance]\n\n    # [START howto_operator_spanner_get_instance_result]\n    get_instance_result = models.ExternalTaskSensor(\n        task_id=\"get_instance_sensor\",\n        external_task_id=get_instance.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_get_instance_result]\n\n    # [START howto_operator_spanner_instance_id]\n    get_instance_id = SpannerInstanceIdOperator(\n        task_id=\"get_instance_id\",\n        project_id=PROJECT_ID,\n        instance_id=SPANNER_INSTANCE_ID,\n    )\n    # [END howto_operator_spanner_instance_id]\n\n    # [START howto_operator_spanner_instance_id_result]\n    get_instance_id_result = models.ExternalTaskSensor(\n        task_id=\"get_instance_id_sensor\",\n        external_task_id=get_instance_id.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_instance_id_result]\n\n    # [START howto_operator_spanner_get_database]\n    get_database = SpannerQueryOperator(\n        task_id=\"get_database\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n        sql=INSERT_ROWS_SQL,\n        dag=dag,\n    )\n    # [END howto_operator_spanner_get_database]\n\n    # [START howto_operator_spanner_get_database_result]\n    get_database_result = models.ExternalTaskSensor(\n        task_id=\"get_database_sensor\",\n        external_task_id=get_database.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_get_database_result]\n\n    # [START howto_operator_spanner_drop_database]\n    drop_database = SpannerRemoveInstanceDatabaseOperator(\n        task_id=\"drop_database\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n    )\n    # [END start_transaction_operator_spanner_drop_database]\n\n    # [START howto_operator_spanner_drop_database_result]\n    drop_database_result = models.ExternalTaskSensor(\n        task_id=\"drop_database_sensor\",\n        external_task_id=drop_database.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_drop_database_result]\n\n    # [START howto_operator_spanner_start_transaction]\n    start_transaction = SpannerStartTransactionOperator(\n        task_id=\"start_transaction\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n    )\n    # [END start_transaction_operator_spanner_start_transaction]\n\n    # [START howto_operator_spanner_start_transaction_result]\n    start_transaction_result = models.ExternalTaskSensor(\n        task_id=\"start_transaction_sensor\",\n        external_task_id=start_transaction.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_start_transaction_result]\n\n    # [START howto_operator_spanner_commit_transaction]\n    commit_transaction = SpannerQueryOperator(\n        task_id=\"commit_transaction\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n        sql=\"COMMIT;\",\n        dag=dag,\n    )\n    # [END howto_operator_spanner_commit_transaction]\n\n    # [START howto_operator_spanner_commit_transaction_result]\n    commit_transaction_result = models.ExternalTaskSensor(\n        task_id=\"commit_transaction_sensor\",\n        external_task_id=commit_transaction.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_commit_transaction_result]\n\n    # [START howto_operator_spanner_stop_transaction]\n    stop_transaction = SpannerStopTransactionOperator(\n        task_id=\"stop_transaction\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n    )\n    # [END howto_operator_spanner_stop_transaction]\n\n    # [START howto_operator_spanner_stop_transaction_result]\n    stop_transaction_result = models.ExternalTaskSensor(\n        task_id=\"stop_transaction_sensor\",\n        external_task_id=stop_transaction.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END howto_operator_spanner_stop_transaction_result]\n\n    # [START howto_operator_spanner_upload_data]\n    upload_data = SpannerUploadDataOperator(\n        task_id=\"upload_data\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        partition_id=SPANNER_PARTITION_ID,\n        body={\"sql\": INSERT_ROWS_SQL},\n    )\n    # [END start_transaction_operator_spanner_upload_data]\n\n    # [START howto_operator_spanner_upload_data_result]\n    upload_data_result = models.ExternalTaskSensor(\n        task_id=\"upload_data_sensor\",\n        external_task_id=upload_data.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END start_transaction_operator_spanner_upload_data]\n\n    # [START howto_operator_spanner_get_partition_id]\n    get_partition_id = SpannerQueryOperator(\n        task_id=\"get_partition_id\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        partition_id=SPANNER_PARTITION_ID,\n        sql=\"SELECT table_name, column_name FROM information_schema.columns WHERE table_name='table_name'\",\n        dag=dag,\n    )\n    # [END start_transaction_operator_spanner_get_partition_id]\n\n    # [START howto_operator_spanner_get_partition_id_result]\n    get_partition_id_result = models.ExternalTaskSensor(\n        task_id=\"get_partition_id_sensor\",\n        external_task_id=get_partition_id.output,\n        task_trigger=TriggerRule.NONE,\n    )\n    # [END start_transaction_operator_spanner_get_partition_id_result]\n\n    # [START howto_operator_spanner_drop_partition]\n    drop_partition = SpannerRemoveInstancePartitionOperator(\n        task_id=\"drop_partition\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        instance_id=SPANNER_INSTANCE_ID,\n        partition_id=SPANNER_PARTITION_ID,\n    )\n    # [END start_transaction_operator_spanner_drop_partition]\n\n    # [START howto_operator_spanner_drop_partition_result]\n    drop_partition_result = models.ExternalTaskSensor(\n        task_id=\"drop_partition_sensor\",\n        external_task_id"}], "metadata": {"file_name": "dag_909_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can wait for specific conditions before triggering downstream tasks. Create a DAG that illustrates time-based, file-based, bash command, and Python function sensors to help learners understand conditional task execution.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.sensors.file_sensor import FileSensor\nfrom airflow.providers.standard.sensors.python import PythonSensor\nfrom airflow.providers.standard.sensors.time_sensor import TimeSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START howto_sensor_file]\n@task\ndef check_file():\n    print(\"Checking a file...\")\n    with open(\"aa\") as f:\n        print(f.read())\n    return True\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef stop():\n    print(\"Stopping...\")\n\n\nwith DAG(\n    dag_id=\"example_sensor_file\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n) as dag:\n    # Example of JSON parsing with pendulum.parse_date.\n    BashOperator(\n        task_id=\"parse_date\",\n        bash_command='jsonc -f - <<END\nprint jsonc.load(''\"aa\"')\nEND'\n    )\n\n    # Substitute \"aa\" with the actual path to your file.\n    check_file() >> FileSensor(path=\"aa\", task_id=\"file_sensor\")\n    check_file() >> BashOperator(task_id=\"stop\", bash_command=stop())\n    # The time sensor will tell us when the previous task finished.\n    TimeSensor(task_id=\"wait_for_start\", time_delta=\"1s\") >> BashOperator(task_id=\"bash_operator\")\n    # If the time sensor timeout, the BashOperator will be skipped\n# [END howto_sensor_file]"}], "metadata": {"file_name": "dag_1049_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms raw geographical and user data into structured database tables with multiple query scenarios. Design a workflow that handles data insertion, supports parameterized queries, and demonstrates different methods of database interaction using MsSql operators and hooks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.sql.operators.sql import SqlExecuteQueryOperator\nfrom airflow.utils.edgemodifier import edge, pop_edge\nfrom tests.system.providers.microsoft.sql.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_ms_sql_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    default_args={\"sql_conn_id\": \"sql\"},\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_ms_sql]\n    t1 = SqlExecuteQueryOperator(task_id=\"t1\", sql=\"SELECT 1\")\n    # [END howto_operator_ms_sql]\n\n    # [START howto_operator_ms_sql_with_params]\n    t2 = SqlExecuteQueryOperator(\n        task_id=\"t2\",\n        sql=\"SELECT 2\",\n        parameters={\"statement\": \"Hello World\"},\n    )\n    # [END howto_operator_ms_sql_with_params]\n\n    # [START howto_operator_ms_sql_template]\n    t3 = SqlExecuteQueryOperator(task_id=\"t3\", sql=\"SELECT %s\", parameters=[\"3\"])\n    # [END howto_operator_ms_sql_template]\n\n    # [START howto_operator_ms_sql_with_rows]\n    t4 = SqlExecuteQueryOperator(task_id=\"t4\", sql=\"SELECT CAST(1 AS INT) AS int_value\")\n    # [END howto_operator_ms_sql_with_rows]\n\n    # [START howto_operator_ms_sql_with_rows_result_filter]\n    t5 = SqlExecuteQueryOperator(\n        task_id=\"t5\", sql=\"SELECT CAST(1 AS INT) AS int_value\", result_filter=lambda task_instance, query_output: (\n            query_output.rows[0][0]\n        )\n    )\n    # [END howto_operator_ms_sql_with_rows_result_filter]\n\n    # [START howto_operator_ms_sql_with_rows_result_filter_function_def]\n    def get_first_value(**kwargs) -> int:\n        task_instance = kwargs[\"task_instance\"]\n        query_output = task_instance.xcom_pull(task_ids=[\"t4\"], key=\"query_output\")\n        return query_output.rows[0][0]\n\n    t6 = SqlExecuteQueryOperator(\n        task_id=\"t6\",\n        sql=\"SELECT CAST(1 AS INT) AS int_value\",\n        result_filter=get_first_value,\n    )\n    # [END howto_operator_ms_sql_with_rows_result_filter_function_def]\n\n    # [START howto_operator_ms_sql_template_result_filter]\n    t7 = SqlExecuteQueryOperator(\n        task_id=\"t7\", sql=\"SELECT @value = %s FROM my_table\", parameters=[\"4\"], result_filter=\"SELECT @value\"\n    )\n    # [END howto_operator_ms_sql_template_result_filter]\n    pop_edge(dag, t7, t6)\n\n    # [START howto_operator_ms_sql_where]\n    t8 = SqlExecuteQueryOperator(task_id=\"t8\", sql=\"SELECT 5 FROM my_table WHERE 1 = 1\")\n    # [END howto_operator_ms_sql_where]\n    t8 >> edge(t1, t2)\n\n    # [START howto_operator_ms_sql_with_schema]\n    t9 = SqlExecuteQueryOperator(task_id=\"t9\", sql=\"USE airflow;\", schema=\"airflow\")\n    # [END howto_operator_ms_sql_with_schema]\n    t9 >> edge(t1, t10)\n\n    # [START howto_operator_ms_sql_with_schema_result_filter]\n    t10 = SqlExecuteQueryOperator(\n        task_id=\"t10\", sql=\"USE airflow;\", schema=\"airflow\", result_filter=\"SELECT 1\"\n    )\n    # [END howto_operator_ms_sql_with_schema_result_filter]\n    t10 >> edge(t1, t11)\n\n    # [START howto_operator_ms_sql_with_schema_template]\n    t11 = SqlExecuteQueryOperator(\n        task_id=\"t11\", sql=\"USE airflow; SELECT @value = %s\", parameters=[\"2\"], result_filter=\"SELECT @value\"\n    )\n    # [END howto_operator_ms_sql_with_schema_template]\n    t11 >> edge(t1, t12)\n\n    # [START howto_operator_ms_sql_with_schema_template_result_filter]\n    t12 = SqlExecuteQueryOperator(\n        task_id=\"t12\",\n        sql=\"USE airflow; SELECT @value = %s\",\n        parameters=[\"2\"],\n        result_filter=\"SELECT @value\",\n    )\n    # [END howto_operator_ms_sql_with_schema_template_result_filter]\n    t12 >> edge(t1, t13)\n\n    # [START howto_operator_ms_sql_like]\n    t13 = SqlExecuteQueryOperator(task_id=\"t13\", sql=\"SELECT 3 FROM my_table WHERE name LIKE '%b%'\")\n    # [END howto_operator_ms_sql_like]\n    t13 >> edge(t1, t14)\n\n    # [START howto_operator_ms_sql_with_external_hooks]\n    t14 = SqlExecuteQueryOperator(\n        task_id=\"t14\",\n        sql=\"SELECT 3 FROM my_table WHERE name LIKE '%b%'\",\n        external_hooks={\n            t7: [t1],\n        },\n    )\n    # [END howto_operator_ms_sql_with_external_hooks]\n    t14 >> edge(t1, t15)\n\n    # [START howto_operator_ms_sql_with_external_hooks_result_filter]\n    t15 = SqlExecuteQueryOperator(\n        task_id=\"t15\",\n        sql=\"SELECT 3 FROM my_table WHERE name LIKE '%b%'\",\n        external_hooks={\n            t7: [t1],\n        },\n    )\n    # [END howto_operator_ms_sql_with_external_hooks_result_filter]\n    t15 >> edge(t1, t16)\n\n    # [START howto_operator_ms_sql_with_external_hooks_template]\n    t16 = SqlExecuteQueryOperator(\n        task_id=\"t16\",\n        sql=\"SELECT 3 FROM my_table WHERE name LIKE '%b%'\",\n        external_hooks={\n            t7: [t1],\n        },\n        result_filter=\"SELECT 3\",\n    )\n    # [END howto_operator_ms_sql_with_external_hooks_template]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\n# test_run.xcom_pull(key='return_value', task_ids=['t1'])\nprint(test_run.xcom_pull(key='return_value', task_ids=['t1']))"}], "metadata": {"file_name": "dag_630_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python code snippet to calculate the factorial of a number using recursion?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def factorial(n):\n    \"\"\"\n    Calculate the factorial of a non-negative integer n using recursion.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n# Example usage:\n# print(factorial(5))  # Output: 120"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a machine learning pipeline that trains a K-Nearest Neighbors classification model using Amazon SageMaker and deploys it as a real-time prediction endpoint. The pipeline should handle data upload, model training, endpoint deployment, and automatic cleanup of resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nimport tempfile\nfrom datetime import datetime\nfrom pathlib import Path\nfrom textwrap import dedent\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateExperimentOperator,\n    SageMakerCreateNotebookOperator,\n    SageMakerCreateProcessingJobOperator,\n    SageMakerDataTransformOperator,\n    SageMakerDeleteExperimentOperator,\n    SageMakerDeleteNotebookOperator,\n    SageMakerDeleteProcessingJobOperator,\n    SageMakerModelOperator,\n    SageMakerProcessingOperator,\n    SageMakerRegisterModelOperator,\n    SageMakerStartProcessingJobOperator,\n    SageMakerStopProcessingJobOperator,\n    SageMakerToS3DataOperator,\n    SageMakerToS3TransformConfigOperator,\n    SageMakerUpsertInferenceConfigOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingJobSensor,\n    SageMakerTrainingJobSensor,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sagemaker\"\n\nsg_aws_id = os.environ.get(\"SG_AWS_ID\")\nrole_aws_id = os.environ.get(\"ROLE_AWS_ID\")\n\nSAMPLE_SIZE = 1000\nTEST_DATA_S3_KEY = f\"{ENV_ID}/sagemaker/knn\"\nS3_DATA_BUCKET = f\"{ENV_ID}-sagemaker-knn\"\nS3_DATA_PREFIX = f\"{TEST_DATA_S3_KEY}/training-data\"\nS3_MODEL_OUTPUT = f\"{ENV_ID}/sagemaker/knn-model\"\nS3_EXPORT_OUTPUT = f\"{ENV_ID}/sagemaker/knn-export\"\nSTAGING_BUCKET = f\"{ENV_ID}-sagemaker\"\nTITANIC_S3_PATH = \"s3://{STAGING_BUCKET}/adult/custody/\"\nTITANIC_DATA_FILE = \"s3://{STAGING_BUCKET}/adult/custody/train.csv\"\nTITANIC_COLUMN_MAP = {\n    \"capital_gain\": \"Capital_Gain\",\n    \"capital_loss\": \"Capital_Loss\",\n    \"birth_date\": \"Birth_Day\",\n    \"gender\": \"Gender\",\n    \"race\": \"Race\",\n    \"age\": \"Age\",\n    \"salary\": \"Salary\",\n    \"has_worked\": \"Work\",\n}\nDROPOFF_COLUMNS = [\"Work\", \"Gender\", \"Race\", \"Capital_Gain\", \"Capital_Loss\", \"Age\", \"Salary\"]\nTITANIC_COLUMN_MAP_INV = {v: k for k, v in TITANIC_COLUMN_MAP.items()}\nTITANIC_DATASET = dedent(\n    \"\"\"\n        Age,Sex,Cabin,Embarked\n        7.92,P,NA,S\n        2.40,M,Q,\n        9.00,M,BQ\n        15.48,F,Q,\n        3.00,M,BQ\n        0.40,M,BQ\n        1.00,M,BQ\n        18.00,M,BQ\n        18.00,M,BQ\n        16.00,M,BQ\n        8.00,M,BQ\n        16.00,M,BQ\n        2.00,M,BQ\n        8.00,M,BQ\n        1.00,M,BQ\n        16.00,M,BQ\n        1.00,M,BQ\n        2.00,M,BQ\n        16.00,M,BQ\n        18.00,M,BQ\n        18.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,BQ\n        16.00,M,B"}], "metadata": {"file_name": "dag_97_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that monitors a specific Azure CosmosDB collection for document availability and triggers a subsequent document insertion task. Implement a dependency-based workflow that ensures document existence before performing write operations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.cosmos import AzureCosmosCreateCollectionOperator, AzureCosmosTriggerOperator\nfrom airflow.providers.microsoft.azure.utils import get_resource_properties\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"azure_cosmos_trigger\"\n\ncosmos_conn = Connection(\n    conn_id=\"azure_default\",\n    extra={\"azure_cosmos_key\": \"my-azure-cosmos-key\"},\n)\n\ncollection_name = f\"collection_{ENV_ID}\"\n\n\n@task\ndef create_collection(cosmos_conn):\n    # [START howto_operator_azurercosmos_createcollection]\n    cosmos_instance = get_resource_properties(cosmos_conn, \"azure_cosmos\")(azure_cosmos_instance)\n    create_collection = AzureCosmosCreateCollectionOperator(\n        task_id=\"create_collection\",\n        conn_id=cosmos_conn.conn_id,\n        location=\"SouthEast\",\n        database_name=f\"database_{ENV_ID}\",\n        collection_name=collection_name,\n        id=str(uuid4()),\n        through_point=1,\n        consistency_policy=dict(read_capacity_read=1, write_capacity_write=1),\n        database_settings={},\n        collection_settings={\n            \"partitionKey\": {\"paths\": [\"/partitionId\"]},\n        },\n        azure_cosmos_instance=cosmos_instance,\n    )\n    # [END howto_operator_azurercosmos_createcollection]\n    create_collection.wait_for_completion()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_collection(cosmos_conn, collection_name):\n    # [START howto_operator_azurercosmos_deletedatabase]\n    delete_collection = AzureCosmosTriggerOperator(\n        task_id=\"delete_collection\",\n        conn_id=cosmos_conn.conn_id,\n        request_body={\"action\": \"delete\", \"paths\": [{\"path\": f\"/collections/{collection_name}\"}]},\n        azure_cosmos_instance=get_resource_properties(cosmos_conn, \"azure_cosmos\")(azure_cosmos_instance),\n    )\n    # [END howto_operator_azurercosmos_deletedatabase]\n    delete_collection.wait_for_completion()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_collection(cosmos_conn)\n    delete_collection(cosmos_conn, collection_name)"}], "metadata": {"file_name": "dag_968_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data monitoring workflow that extracts specific column data from a SQL table and automatically sends the results to a designated Slack channel for real-time reporting and team awareness.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIOperator\nfrom airflow.providers.sqliter.hooks.sqlite import SqliteHook\nfrom airflow.providers.sqliter.operators.sqlite import SqliteToSlackOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"sqlite_to_slack\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    connection = Connection(\n        conn_id=\"sqlite_hook_test\",\n        conn_type=\"sqlite\",\n        host=\"db\",\n        login=\"db_user\",\n        password=\"db_password\",\n        port=3306,\n    )\n\n    @task\n    def create_table(sqlite_hook):\n        sqlite_hook.execute(\"CREATE TABLE employees (id INT, name VARCHAR, salary INT)\")\n\n    @task\n    def insert_data(sqlite_hook):\n        sqlite_hook.execute(\"INSERT INTO employees VALUES (1, 'John Doe', 60000)\")\n        sqlite_hook.execute(\"INSERT INTO employees VALUES (2, 'Jane Doe', 54000)\")\n\n    @task\n    def get_data(sqlite_hook):\n        rows = sqlite_hook.execute(\"SELECT * FROM employees\").records\n        return rows\n\n    @task\n    def check_data(rows):\n        print(rows)\n        assert rows == [\n            (1, \"John Doe\", 60000),\n            (2, \"Jane Doe\", 54000),\n        ]\n\n    # [START howto_operator_sqlite_to_slack]\n    sqlite_to_slack_task = SqliteToSlackOperator(\n        task_id=\"sqlite_to_slack_task\",\n        sql=\"SELECT * FROM employees\",\n        conn_id=\"sqlite_hook_test\",\n        slack_channel=\"my_channel\",\n        slack_api_token=\"my_api_token\",\n        slack_conn_id=\"slack_api\",\n        slack_channel_format=\"slack_channel\",\n        slack_api_conn_id=\"slack_api\",\n        rows_header=\"yes\",\n        slack_emoji=\"robot\",\n        timeout=30,\n        dag=dag,\n    )\n    # [END howto_operator_sqlite_to_slack]\n\n    @task\n    def delete_data(sqlite_hook):\n        sqlite_hook.execute(\"DELETE FROM employees\")\n\n    @task\n    def get_data_after_delete(sqlite_hook):\n        rows = sqlite_hook.execute(\"SELECT * FROM employees\").records\n        return rows\n\n    check_data(get_data_after_delete())\n\n    create_table(connection)\n    insert_data(connection)\n    delete_data(connection)\n    [sqlite_to_slack_task, get_data_after_delete()]\n\n\n    for task in [sqlite_to_slack_task, get_data_after_delete()]:\n        task.doc_md = f\"{task.task_id} documentation\""}], "metadata": {"file_name": "dag_651_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of AWS SageMaker notebook resources, illustrating how to programmatically control cloud computing infrastructure through defined task dependencies. Develop a step-by-step orchestration that shows creating, manipulating, and cleaning up a computational notebook instance.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, Task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_sagemaker]\nsagemaker_notebook_id = \"20201222-123456-test-notebook\"\nsagemaker_role_arn = \"arn:aws:iam::123456789012:role/role-name\"\n\np = BashOperator(\n    task_id=\"create_or_update_sagemaker_notebook\",\n    bash_command=f\"echo \\\"Creating or updating sage maker notebook with {sagemaker_notebook_id} and {sagemaker_role_arn} role.\\\"\",\n    trigger_rule=TriggerRule.ALL_DONE,\n)\np >> _update_sagemaker_notebook_role(sagemaker_notebook_id)\np >> _create_sagemaker_notebook(sagemaker_notebook_id)\n\n@task\ndef _create_sagemaker_notebook(sagemaker_notebook_id):\n    logging.info(\"Creating sagemaker notebook with id %s\", sagemaker_notebook_id)\n    # [START howto_operator_sagemaker_create]\n    create_sagemaker_notebook = BashOperator(\n        task_id=\"create_sagemaker_notebook\",\n        bash_command=f\"echo \\\"Creating sage maker notebook with {sagemaker_notebook_id} and {sagemaker_role_arn} role.\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_create]\n    create_sagemaker_notebook >> _update_sagemaker_notebook_instance_profile(sagemaker_notebook_id)\n    create_sagemaker_notebook\n\n@task\ndef _update_sagemaker_notebook_instance_profile(sagemaker_notebook_id):\n    logging.info(\"Updating sagemaker notebook instance profile for id %s\", sagemaker_notebook_id)\n    # [START howto_operator_sagemaker_update]\n    update_sagemaker_notebook_instance_profile = BashOperator(\n        task_id=\"update_sagemaker_notebook_instance_profile\",\n        bash_command=f\"echo \\\"Updating sage maker notebook instance profile for {sagemaker_notebook_id} with builder role.\\\\n{create_sagemaker_notebook}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_update]\n    update_sagemaker_notebook_instance_profile >> _delete_sagemaker_notebook_instance_profile(sagemaker_notebook_id)\n    _delete_sagemaker_notebook_instance_profile(sagemaker_notebook_id)\n\n@task\ndef _delete_sagemaker_notebook_instance_profile(sagemaker_notebook_id):\n    logging.info(\"Deleting sagemaker notebook instance profile for id %s\", sagemaker_notebook_id)\n    # [START howto_operator_sagemaker_delete]\n    delete_sagemaker_notebook_instance_profile = BashOperator(\n        task_id=\"delete_sagemaker_notebook_instance_profile\",\n        bash_command=f\"echo \\\"Deleting sage maker notebook instance profile for {sagemaker_notebook_id}.\\\\n{update_sagemaker_notebook_instance_profile}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_delete]\n    delete_sagemaker_notebook_instance_profile\n    # [START howto_operator_sagemaker_cleanup]\n    cleanup_sagemaker_notebook = BashOperator(\n        task_id=\"cleanup\",\n        bash_command=f\"echo \\\"Cleaning up resources.\\\\n{delete_sagemaker_notebook_instance_profile}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_cleanup]\n    cleanup_sagemaker_notebook\n    # [START howto_operator_connection]\n    connect_sagemaker_to_aws = BashOperator(\n        task_id=\"connect_sagemaker_to_aws\",\n        bash_command=f\"echo \\\"Connecting sagemaker to aws {sagemaker_role_arn}.\\\\n{update_sagemaker_notebook_instance_profile}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_connection]\n    connect_sagemaker_to_aws\n    # [START howto_operator_session]\n    create_session = BashOperator(\n        task_id=\"create_session\",\n        bash_command=f\"echo \\\"Creating session with default region.\\\\ndefault_region=$(aws --region default configure get Region)\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_session]\n    create_session >> session\n    # [START howto_operator_sagemaker]\n    create_sagemaker_notebook\n    # [END howto_operator_sagemaker]\n    # [START howto_operator_role]\n    create_sagemaker_notebook_with_role = BashOperator(\n        task_id=\"create_sagemaker_notebook_with_role\",\n        bash_command=f\"echo \\\"Creating sage maker notebook with {sagemaker_notebook_id} and {sagemaker_role_arn} role.\\\\n{create_sagemaker_notebook}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_role]\n    create_sagemaker_notebook_with_role >> _update_sagemaker_notebook_permission(\n        sagemaker_notebook_id, f\"{session[0].default_region}/permission/sageMakerApiAccess.json\"\n    )\n    # [START howto_operator_sagemaker_cleanup2]\n    cleanup_sagemaker_notebook\n    # [END howto_operator_sagemaker_cleanup2]\n    # [START howto_operator_role2]\n    update_sagemaker_notebook_permission = BashOperator(\n        task_id=\"update_sagemaker_notebook_permission\",\n        bash_command=f\"echo \\\"Updating sage maker notebook permission for {sagemaker_notebook_id} with builder role.\\\\n{create_sagemaker_notebook_with_role}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_role2]\n    update_sagemaker_notebook_permission\n    # [START howto_operator_connection2]\n    connect_sagemaker_to_aws\n    # [END howto_operator_connection2]\n    connect_sagemaker_to_aws\n    # [START howto_operator_sagemaker_2]\n    create_sagemaker_notebook_with_role\n    # [END howto_operator_sagemaker_2]\n    # [START howto_operator_sagemaker_invoke_endpoint]\n    invoke_endpoint = BashOperator(\n        task_id=\"invoke_endpoint\",\n        bash_command=f\"echo \\\"Invoking endpoint for {sagemaker_notebook_id} with input '{{{{ 'data=[[1,2],[3,4]]' }}{{'}}'}}.\\\\n{create_sagemaker_notebook_with_role}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_invoke_endpoint]\n    invoke_endpoint\n    # [START howto_operator_sagemaker_list_endpoints]\n    list_endpoints = BashOperator(\n        task_id=\"list_endpoints\",\n        bash_command=f\"echo \\\"Listing endpoints for {sagemaker_notebook_id}.\\\\n{create_sagemaker_notebook_with_role}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_list_endpoints]\n    list_endpoints\n    # [START howto_operator_sagemaker_invoke_endpoint2]\n    invoke_endpoint2 = BashOperator(\n        task_id=\"invoke_endpoint2\",\n        bash_command=f\"echo \\\"Invoking endpoint for {sagemaker_notebook_id} with input '{{{{ 'data=[[5,6],[7,8]]' }}{{'}}'}}.\\\\n{create_sagemaker_notebook_with_role}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_invoke_endpoint2]\n    invoke_endpoint2\n    # [START howto_operator_sagemaker_list_endpoints2]\n    list_endpoints2 = BashOperator(\n        task_id=\"list_endpoints2\",\n        bash_command=f\"echo \\\"Listing endpoints for {sagemaker_notebook_id}.\\\\n{create_sagemaker_notebook_with_role}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_list_endpoints2]\n    list_endpoints2\n    # [START howto_operator_sagemaker_delete_endpoint]\n    delete_endpoint = BashOperator(\n        task_id=\"delete_endpoint\",\n        bash_command=f\"echo \\\"Deleting endpoint for {sagemaker_notebook_id} with id {endpoint_id}.\\\\n{invoke_endpoint2}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_delete_endpoint]\n    delete_endpoint\n    # [START howto_operator_sagemaker_cleanup3]\n    cleanup_sagemaker_notebook\n    # [END howto_operator_sagemaker_cleanup3]\n    # [START howto_operator_role3]\n    update_sagemaker_endpoint_permission = BashOperator(\n        task_id=\"update_sagemaker_endpoint_permission\",\n        bash_command=f\"echo \\\"Updating sage maker endpoint permission for {sagemaker_notebook_id} with builder role.\\\\n{delete_endpoint}\\\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_role3]\n    update_sagemaker_endpoint_permission\n    # [START howto_operator_connection3]\n    connect_sagemaker_to_aws\n    # [END howto_operator_connection3]\n    connect_sagemaker_to_aws\n    # [START howto_operator_sagemaker_3]"}], "metadata": {"file_name": "dag_1126_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow that demonstrates transferring a local text file to an Amazon S3 bucket using Airflow, showcasing file management and cloud storage integration. Create a DAG that generates a sample text file, uploads it to S3, and then performs cleanup operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.ssm import SSMPutParameterOperator, S3GetObjectOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom airflow.sdk import chain\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_ssmin\"\nS3_BUCKET = f\"{ENV_ID}-example-ssmin-bucket\"\nS3_KEY = f\"{ENV_ID}-example-ssmin-key\"\nSSM_KEY = f\"{ENV_ID}-example-ssmin-key\"\nLOCAL_FILE_PATH = os.environ.get(\"SYSTEM_TESTS_LOCAL_PATH\", \"/tmp\")\nLOCAL_FILE_NAME = \"tmp.txt\"\n\n\n@task\ndef create_connection(connection_id: str):\n    connection = Connection(\n        conn_id=connection_id,\n        conn_type=\"s3\",\n        description=\"Connection to sample s3 airflow test resource\",\n    )\n    session = boto3.Session()\n    hook = S3Hook(session)\n    exists = hook.check_for_bucket_existence(bucket_name=S3_BUCKET)\n    if not exists:\n        hook.create_bucket(bucket_name=S3_BUCKET)\n\n\n@task\ndef create_parameter_at_ssm(connection_id: str):\n    SSMHook = boto3.client(\"ssm\")\n    SSMHook.put_parameter(\n        Name=SSM_KEY, Value=\"Sample SSMM Parameter\", Type=\"String\", Overwrite=True\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n):\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\", bucket_name=S3_BUCKET, region_name=\"us-east-1\"\n    )\n\n    @task\n    def generate_file(bucket_name: str, key: str):\n        S3Hook = S3Hook()\n        s3_conn = S3Hook.conn\n        file = open(os.path.join(LOCAL_FILE_PATH, LOCAL_FILE_NAME), \"w\")\n        file.write(f\"Content of {bucket_name}/{key}\")\n        file.close()\n        s3_conn.put_object(Bucket=bucket_name, Key=key, Body=f\"Content of {bucket_name}/{key}\")\n\n    generate_file_task = generate_file(bucket_name=S3_BUCKET, key=S3_KEY)\n\n    get_file_at_s3_path = S3GetObjectOperator(\n        task_id=\"get_file_at_s3_path\",\n        bucket_name=S3_BUCKET,\n        object_key=S3_KEY,\n        website_url=f\"s3://{S3_BUCKET}/{S3_KEY}\",\n    )\n\n    create_parameter_at_aws_lambda = SSMPutParameterOperator(\n        task_id=\"create_parameter_at_aws_lambda\",\n        connection_id=\"aws_lambda\",\n        name=SSM_KEY,\n        value=\"Sample SSMLambda Parameter\",\n        type=\"String\",\n    )\n\n    # [START howto_operator:ssm_get_param]\n    get_param_atawslambda = SSMGetParameterOperator(\n        task_id=\"get_param_atawslambda\",\n        connection_id=\"aws_lambda\",\n        name=SSM_KEY,\n    )\n    # [END howto_operator:ssm_get_param]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=S3_BUCKET,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        create_s3_bucket,\n        generate_file_task,\n        get_file_at_s3_path,\n        create_parameter_at_aws_lambda,\n        get_param_atawslambda,\n        delete_s3_bucket,\n    )"}], "metadata": {"file_name": "dag_1503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a cloud-based data discovery pipeline that leverages Google Dataproc to execute a simple Hive query across a dynamically created multi-node cluster. Construct a demonstration of scalable, on-demand data infrastructure that can quickly spin up and down for lightweight analytical tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_hive\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nREGION = \"europe-west1\"\nCLUSTER_NAME = f\"cluster_{DAG_ID}_{ENV_ID}\"\nHIVE_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"hive_job\": {\"query_list\": {\"sql_file\": [f\"{BUCKET_NAME}/hive.sql\"]}},\n}\nSCHEMA = {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"1ROW_PER_COLUMN\"}\nOPTIONS = {\"temp_location\": \"gs://test-hive-acc/\"}\nLOCATION = \"europe-west1-b\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    user_defined_macros={\"PROJECT_ID\": PROJECT_ID},\n    default_args={\"project_id\": PROJECT_ID},\n    location=LOCATION,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        num_workers=2,\n        master_machine_type=\"n1-standard-1\",\n        worker_machine_type=\"n1-standard-1\",\n        storage_bucket=BUCKET_NAME,\n        initial_user_config={\n            \"service_account_scopes\": [[\"https://www.googleapis.com/auth/cloud-platform\"]]\n        },\n    )\n\n    # [START how_to_run_hive_job_on_dataproc]\n    run_hive_job = DataprocSubmitJobOperator(\n        task_id=\"run_hive_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        job=HIVE_JOB,\n        options=OPTIONS,\n    )\n    # [END how_to_run_hive_job_on_dataproc]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_cluster\n        # TEST BODY\n        >> run_hive_job\n        # TEST TEARDOWN\n        >> delete_cluster\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_1234_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Hello Qwen, I want to know more about the use of conditional statements in Python. Can you provide me with some examples?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"hello_qwen\")\ndef hello_qwen():\n    print(\"Hello Qwen!\")\n\n\nwith DAG(\n    \"hello_qwen_dag\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    hello_qwen()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that validates and explores HTTP service connectivity through sensor checks and sequential HTTP request tasks. Implement a workflow that first checks service availability, then executes a series of HTTP interactions with different configuration parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_operator\"\n\nconn = Connection(\n    conn_id=\"service_name\",\n    schema=\"airflow\",\n    conn_type=\"postgres\",\n    host=\"postgres\",\n    login=\"postgres_user\",\n    password=\"postgres_password\",\n    port=5432,\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_tables():\n        \"\"\"\n        Example task to create tables in SQLExecuteQueryOperator.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"create_tables\")\n        sql_hook.run(\"CREATE TABLE IF NOT EXISTS users (user_id IDENTITY, user_name VARCHAR(255));\")\n        table_create_query = \"CREATE TABLE IF NOT EXISTS http_response (url_id IDENTITY, http_response_text TEXT);\"\n        sql_hook.run(table_create_query)\n\n    @task\n    def insert_data():\n        \"\"\"\n        Example task to insert data into SQLExecuteQueryOperator.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"insert_data\")\n        sql_hook.run(\"INSERT INTO users (user_name) VALUES ('John');\")\n        sql_hook.run(\"INSERT INTO http_response (http_response_text) VALUES ('Hello World');\")\n\n    @task\n    def insert_response_data(response_id: str):\n        \"\"\"\n        Example task to insert data into SQLExecuteQueryOperator.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"insert_response_data\")\n        sql_hook.run(\n            \"INSERT INTO http_response (url_id, http_response_text) VALUES (%s, %s);\",\n            parameters=(response_id, \"Response_1\"),\n        )\n\n    @task\n    def get_users():\n        \"\"\"\n        Example task to get data from SQLExecuteQueryOperator.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"get_users\")\n        users = sql_hook.get_records(\"SELECT * FROM users\")[0]\n        return users\n\n    @task\n    def get_http_response(http_response_id: str):\n        \"\"\"\n        Example task to get data from SQLExecuteQueryOperator.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"get_http_response\")\n        http_response = sql_hook.get_records(\"SELECT * FROM http_response\")[0]\n        return http_response\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_users_table():\n        \"\"\"\n        Example task to delete table.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"delete_users_table\")\n        sql_hook.run(\"DROP TABLE users;\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_http_response_table():\n        \"\"\"\n        Example task to delete table.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=\"service_name\", task_id=\"delete_http_response_table\")\n        sql_hook.run(\"DROP TABLE http_response;\")\n\n    create_tables_task = create_tables()\n    insert_data_task = insert_data()\n    insert_response_data_task = insert_response_data(\"69f763c5-36d9-4dfb-99e3-6c6b136e6b0a\")\n    get_users_task = get_users()\n    get_http_response_task = get_http_response(\"74763ae3-eba0-468d-960d-61a2a7e3bc3d\")\n    delete_users_table_task = delete_users_table()\n    delete_http_response_table_task = delete_http_response_table()\n\n    (\n        # TEST BODY\n        create_tables_task\n        >> insert_data_task\n        >> insert_response_data_task\n        >> get_users_task\n        >> get_http_response_task\n        # TEST TEARDOWN\n        >> delete_users_table_task\n        >> delete_http_response_table_task\n    )"}], "metadata": {"file_name": "dag_1346_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that demonstrates Google Cloud Storage bucket management, including creating, copying, and deleting files and buckets. Create a DAG that showcases file transfer operations between two dynamically generated cloud storage buckets.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSDeleteObjectsOperator,\n    GCSFileTransformOperator,\n    GCSListObjectsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"gcs_transform\"\n\nGCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nGCS_FILE_INPUT = \"fileinput\"\nGCS_FILE_OUTPUT = \"fileoutput\"\nGCS_FILE_OUTPUT_2 = \"fileoutput2\"\nSOURCE_GCS_OBJECT = \"tests/system tests/providers/google/cloud/that_will_be_deleted.txt\"\nDESTINATION_GCS_OBJECT = \"tests/system tests/dest_that_will_be_deleted.txt\"\n\nCONNECTION_ID = \"google_cloud_platform\"\n\nFILE_SYSTEM_ID = f\"fs_{PROJECT_ID}_{DAG_ID}\"\nBUCKET_FILE_PATH = f\"gs://{GCS_BUCKET_NAME}/{SOURCE_GCS_OBJECT}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    @task\n    def create_filesystem():\n        hook = GCSHook()\n        hook.create_system_test_file(FILE_SYSTEM_ID, bucket=GCS_BUCKET_NAME)\n\n    @task\n    def create_file():\n        hook = GCSHook()\n        hook.create_bucket(connection_id=CONNECTION_ID, bucket_name=GCS_BUCKET_NAME)\n        hook.upload_file(\n            connection_id=CONNECTION_ID, source_bucket=\"tests/system\", source_file=SOURCE_GCS_OBJECT, bucket_name=GCS_BUCKET_NAME, object_name=GCS_FILE_INPUT\n        )\n\n    @task\n    def delete_file():\n        hook = GCSHook()\n        hook.delete_files(connection_id=CONNECTION_ID, bucket_name=GCS_BUCKET_NAME, objects=[GCS_FILE_OUTPUT])\n\n    @task\n    def delete_files_2():\n        hook = GCSHook()\n        hook.delete_files(connection_id=CONNECTION_ID, bucket_name=GCS_BUCKET_NAME, objects=[GCS_FILE_OUTPUT_2])\n\n    @task\n    def create_file2():\n        hook = GCSHook()\n        hook.create_bucket(connection_id=CONNECTION_ID, bucket_name=GCS_BUCKET_NAME)\n        hook.upload_file(\n            connection_id=CONNECTION_ID, source_bucket=\"tests/system\", source_file=SOURCE_GCS_OBJECT, bucket_name=GCS_BUCKET_NAME, object_name=GCS_FILE_OUTPUT_2\n        )\n\n    create_file_system = create_filesystem()\n    create_file_2 = create_file2()\n\n    move_file = GCSToGCSOperator(\n        task_id=\"move_file\",\n        source_bucket=GCS_BUCKET_NAME,\n        source_object=BUCKET_FILE_PATH,\n        destination_bucket=GCS_BUCKET_NAME,\n        destination_object=GCS_FILE_OUTPUT,\n    )\n\n    move_file_2 = GCSToGCSOperator(\n        task_id=\"move_file_2\",\n        source_bucket=GCS_BUCKET_NAME,\n        source_object=BUCKET_FILE_PATH,\n        destination_bucket=GCS_BUCKET_NAME,\n        destination_object=GCS_FILE_OUTPUT_2,\n    )\n\n    list_gcs_objects = GCSListObjectsOperator(task_id=\"list_gcs_objects\", bucket_name=GCS_BUCKET_NAME)\n\n    remove_file = GCSDeleteObjectsOperator(\n        task_id=\"remove_file\",\n        bucket_name=GCS_BUCKET_NAME,\n        objects=[GCS_FILE_OUTPUT],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    remove_files_2 = GCSDeleteObjectsOperator(\n        task_id=\"remove_files_2\",\n        bucket_name=GCS_BUCKET_NAME,\n        objects=[GCS_FILE_OUTPUT_2],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_connection = Connection(\n        conn_id=CONNECTION_ID,\n        user=\"\",\n        password=\"\",\n        description=\"Google Cloud provider for testing purposes.\",\n        auto_insert=True,\n    )\n\n    add_connection = ConnectionBuilder(\n        connections=connections,\n        task_ids=[\"add_connection\"],\n        task_instance=context,\n    )\n\n    delete_connection = ConnectionBuilder(\n        connections=connections,\n        task_ids=[\"delete_connection\"],\n        task_instance=context,\n    )\n\n    build_connections = TaskBuilder()\n    build_connections.task_id(\"build_connections\")\n    build_connections.connection(create_connection)\n    build_connections.apt_package(name=\"google-cloud-sdk\")\n    build_connections.url(\"https://cloud.google.com/\")\n    build_connections.url(\"https://www.google.com/\")\n    build_connections.url(\"https://www.python.org/\")\n    build_connections.url(\"http://docs.python-requests.org/en/latest/\")\n    build_connections.build()\n\n    connections = {\n        \"google_cloud_platform\": Connection(\n            conn_id=\"google_cloud_platform\",\n            user=\"\",\n            password=\"\",\n            description=\"Google Cloud Platform provider for testing purposes.\",\n            auto_insert=True,\n        )\n    }\n\n    run_blocks = [\n        \"add_task\",\n        \"add_subtask\",\n        \"add_dependencies\",\n        \"remove_task\",\n        \"remove_subtask\",\n        \"remove_dependencies\",\n        \"get_connections\",\n        \"create_file_system\",\n        \"create_file_2\",\n        \"move_file\",\n        \"move_file_2\",\n        \"list_gcs_objects\",\n        \"remove_file\",\n        \"remove_files_2\",\n        \"delete_bucket\",\n        \"get_connections\",\n        \"get_task\",\n        \"get_task_subtasks\",\n        \"get_task_dependencies\",\n        \"get_task_outcomes\",\n        \"get_task_mapping\",\n        \"get_task_logs\",\n        \"get_task_xcom\",\n        \"get_task_xcom_result\",\n        \"get_task_xcom_results\",\n        \"get_task_xcom_key\",\n        \"get_task_xcom_keys\",\n        \"get_task_xcom_keys_with_context\",\n        \"get_task_xcom_values\",\n        \"get_task_xcom_values_with_context\",\n        \"get_task_xcom_values_multiple_paths\",\n        \"get_task_xcom_values_multiple_paths_with_context\",\n        \"get_task_xcom_values_recursive\",\n        \"get_task_xcom_values_recursive_with_context\",\n        \"get_task_xcom_values_with_outcomes\",\n        \"get_task_xcom_values_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_recursive_with_outcomes\",\n        \"get_task_xcom_values_recursive_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_recursive_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context\",\n        \"get_task_xcom_values_recursive_with_outcomes_multiple_paths_and_context_with_prefix\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and_context_with_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations\",\n        \"get_task_xcom_values_multiple_paths_with_outcomes_and_path_transformations_multiple_paths\",\n        \"get_task_xcom_values_with_outcomes_multiple_paths_and"}], "metadata": {"file_name": "dag_880_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates basic SQL query execution and stored procedure interactions. Create a DAG that showcases different methods of executing database operations using Oracle connection parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.oracle.operators.oracle import OracleExecuteQueryOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_query\", bash_command=\"true\")\n\n    # [START howto_oracle_operator]\n    oracle_task = OracleExecuteQueryOperator(\n        task_id=\"oracle_query\",\n        oracle_conn_id=\"oracle_default\",\n        sql=\"SELECT 1 AS result FROM DUAL\",\n        dag=dag,\n    )\n    # [END howto_oracle_operator]\n\n    run_this >> oracle_task"}], "metadata": {"file_name": "dag_708_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated file monitoring and processing workflow that detects new files in a source directory and moves them to a destination directory using Docker containers. Create a system that checks for file availability, moves the first detected file, and then prints its contents.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom airflow.providers.docker.transfers.local_to_remote import LocalFilesystemToRemoteFolder\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_example\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"conn_id\": \"example_docker\", \"owner\": \"onekazuki\"},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    container_name = \"test-docker-operate\"\n\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"example_docker\",\n            conn_type=\"docker\",\n        )\n        conn_extra = {\n            \"image\": \"centos:latest\",\n            \"command\": \"/bin/sleep 10000\",\n            \"network_mode\": \"none\",\n            \"volume\": \"/var/lib/docker/volumes:/var/lib/docker\",\n        }\n        conn_extra_update = {\n            \"ports\": [\n                \"22:22\",\n                \"50000:50000\",\n            ]\n        }\n        conn_extra_update[\"log_config\"] = {\n            \"max-size\": \"10m\",\n            \"max-file\": \"10\",\n        }\n        conn.update(extra_args=conn_extra, extra_args_update=conn_extra_update)\n        return conn\n\n    create_connection_task = create_connection()\n\n    # [START howto_operator_docker]\n    operate_task = DockerOperator(\n        docker_url=\"unix\",\n        command=\"/bin/sleep 10000\",\n        image=\"centos:latest\",\n        network_mode=\"none\",\n        volume=\"/var/lib/docker/volumes:/var/lib/docker\",\n        task_id=\"operate_task\",\n    )\n    # [END howto_operator_docker]\n\n    move_files_task = LocalFilesystemToRemoteFolder(\n        task_id=\"move_files_task\",\n        local_path=\"/tmp\",\n        remote_path=\"/tmp/remote_path/\",\n    )\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_container():\n        docker_client = docker.from_env()\n        docker_client.containers.remove(container_name, force=True)\n\n    remove_container_task = remove_container()\n\n    create_connection_task >> operate_task >> move_files_task >> remove_container_task"}], "metadata": {"file_name": "dag_452_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential execution of Trino database operations, illustrating how to create schemas, define tables, insert data, and perform template-based and parameterized queries. The workflow should highlight the dependency chaining and error handling capabilities of Airflow with a Trino data source.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.trino.operators.trino import TrinoTemplateOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"trino\"},\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino_template]\n\n    drop_table_trino_template = TrinoTemplateOperator(\n        task_id=\"drop_table_trino_template\",\n        sql=\"drop table if exists tpch.usertable;\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_template]\n\n    drop_table_trino_template >> create_table_trino_template\n\n    # [START howto_operator_trino_create_table]\n\n    create_table_trino_template = TrinoTemplateOperator(\n        task_id=\"create_table_trino_template\",\n        sql=\"create table usertable (user_id bigint, username varchar(50))\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_create_table]\n\n    create_table_trino_template >> insert_data_trino_template\n\n    # [START howto_operator_trino_insert]\n\n    insert_data_trino_template = TrinoTemplateOperator(\n        task_id=\"insert_data_trino_template\",\n        sql=\"insert into usertable values (1, 'John'), (2, 'Doe')\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_insert]\n\n    insert_data_trino_template >> select_data_trino_template\n\n    # [START howto_operator_trino_select]\n\n    select_data_trino_template = TrinoTemplateOperator(\n        task_id=\"select_data_trino_template\",\n        sql=\"select * from usertable\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_select]\n\n    select_data_trino_template >> get_connection\n\n    # [START howto_operator_trino_get_connection]\n\n    get_connection = TrinoTemplateOperator(\n        task_id=\"get_connection\",\n        sql=\"show connections;\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_get_connection]\n\n    get_connection >> show_version\n\n    # [START howto_operator_trino_show_version]\n\n    show_version = TrinoTemplateOperator(\n        task_id=\"show_version\",\n        sql=\"show version;\",\n        trigger_rule=\"all_done\",\n    )\n\n    # [END howto_operator_trino_show_version]\n\n    (drop_table_trino_template >> create_table_trino_template >> insert_data_trino_template\n     >> select_data_trino_template >> get_connection >> show_version)"}], "metadata": {"file_name": "dag_657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates various Google Cloud Storage sensor techniques, showcasing synchronous and asynchronous methods for detecting file upload completions and object changes. Build a workflow that illustrates different monitoring strategies for cloud storage events and file transfer validations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator, GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_sheets import GCSToSheetsOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"gcs_to_sheets\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"GCP_SHEETS_SPREADSHEET_ID\", \"1Zc1tJfqQZz4QZ1Zc2QzQwvZc2QzQxVfQZc2Q\")\nSHEET_NAME = os.environ.get(\"GCP_SHEETS_SHEET_NAME\", \"Sheet1\")\nGCS_FILE_PATH = os.environ.get(\"GCS_FILE_PATH\", \"gcs/example.xlsx\")\nCSV_FILE_NAME = os.environ.get(\"GCS_FILE_CSV_NAME\", \"output.csv\")\nWRITE_SETTINGS = {\n    \"preads_schema\": {\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"phone\": {\"type\": \"string\"},\n        }\n    },\n    \"storage_class\": \"DISK\",\n}\n\n\n@task\ndef create_connection():\n    connection = Connection(\n        conn_id=\"google_cloud_sheets\",\n        user=os.environ.get(\"SYSTEM_TESTS_GCP_USER\"),\n        password=os.environ.get(\"SYSTEM_TESTS_GCP_PASSWORD\"),\n    )\n    os.environ[\"SYSTEM_TESTS_GCP_PROJECT\"]  # Make connection id unique\n    connection.save()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"sheets\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def have_csv_file():\n        file_path = f\"gs://{BUCKET_NAME}/{CSV_FILE_NAME}\"\n        bash_command = f\"cat {file_path} || echo '{''}' > {file_path}\"\n        BashOperator(task_id=\"have-csv-file\", bash_command=bash_command)\n\n    upload_file_to_bucket = GCSToSheetsOperator(\n        task_id=\"upload_file_to_bucket\",\n        source_bucket=gcs_bucket=BUCKET_NAME,\n        source_object=gcs_object=GCS_FILE_PATH,\n        destination_object=CSV_FILE_NAME,\n        gcp_conn_id=\"google_cloud_sheets\",\n    )\n\n    create_spreadsheet = GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        body=WRITE_SETTINGS,\n        location=\"US\",\n        gcp_conn_id=\"google_cloud_sheets\",\n    )\n\n    insert_row = GCSToSheetsOperator(\n        task_id=\"insert_row\",\n        source_bucket=gcs_bucket=BUCKET_NAME,\n        source_object=gcs_object=f\"gs://{BUCKET_NAME}/{CSV_FILE_NAME}\",\n        destination_object=f\"gs://{BUCKET_NAME}/{CSV_FILE_NAME}\",\n        spreadsheet_id=SPREADSHEET_ID,\n        body={\"data\": [{\"v\": \"Test\"}], \"range\": f\"{SHEET_NAME}!A1\"},\n        gcp_conn_id=\"google_cloud_sheets\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_spreadsheet = GoogleSheetsDeleteSpreadsheetOperator(\n        task_id=\"delete_spreadsheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        gcp_conn_id=\"google_cloud_sheets\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    have_csv_file >> upload_file_to_bucket >> create_spreadsheet >> insert_row >> delete_spreadsheet >> delete_bucket"}], "metadata": {"file_name": "dag_1647_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data protection workflow that enables dynamic configuration of sensitive data detection rules using Google Cloud's Data Loss Prevention service. Create a pipeline that uploads custom dictionaries, defines information type patterns, and demonstrates how to programmatically manage data classification rules.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateStoredInfoTypeOperator,\n    CloudDLPDeleteStoredInfoTypeOperator,\n    CloudDLPGetInstanceInfoTypeOperator,\n    CloudDLPListStoredInfoTypesOperator,\n    CloudDLPUpdateStoredInfoTypeOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDLP_STORAGE_BUCKET_NAME = \"airflow-dlp-demo-storage\"\nFILE_NAME = \"dlp_file_to_classify.csv\"\nSTORAGE_SOURCE = f\"gs://{DLP_STORAGE_BUCKET_NAME}/{FILE_NAME}\"\nSTORAGE_DESTINATION = f\"gcs/{FILE_NAME}\"\nDLP_STORAGEDestination = {\"storage_source\": STORAGE_DESTINATION}\nFORMAT = {\"view_mode\": \"STANDARD\", \"column_headers\": True}\n\n# [START howto_object_storage_sample_get_id]\ndef get_id():\n    # Although any Google Cloud Storage URL may be used, the sample will\n    # validate only the format of the URL. Any valid URL to a file will be\n    # parsed and returned without further question.\n    from google.cloud import storage  # Import gcp adapter\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(DLP_STORAGE_BUCKET_NAME)\n    blob = bucket.blob(FILE_NAME)\n\n    url = blob.url\n    # Build a URL that will validate the format of the URL.\n    parsed_url = urllib.parse.urlparse(url)\n    print(parsed_url.path)\n    return parsed_url.path.lstrip(\"/\")\n\n\n# [END howto_object_storage_sample_get_id]\n\n# [START howto_create_stored_info_type_sample]\ndef create_stored_info_type():\n    # Although any name is valid, the sample will demonstrate creating a stored\n    # info type with a name derived from project and file name.\n    storage_bucket = PROJECT_ID + DLP_STORAGE_BUCKET_NAME\n    file_name = FILE_NAME\n    stored_info_type_name = f\"{storage_bucket}{file_name}\"\n\n    client = google.cloud.dlp_v2.DLPosite\n    dlp_stored_info_type = {\n        \"display_name\": stored_info_type_name,\n        \"storage_config\": DLP_STORAGEDestination,\n        \"info_type\": {\n            \"inspect_config\": {\"field_delimiters\": [\",\"]}\n        },\n    }\n\n    client.stored_info_types.create(body=dlp_stored_info_type)\n    print(f\"Stored info type created: {dlp_stored_info_type.name}\")\n    return dlp_stored_info_type.name\n\n\n# [END howto_create_stored_info_type_sample]\n\n# [START howto_update_stored_info_type_sample]\ndef update_stored_info_type():\n    # Although any name is valid, the sample will demonstrate updating a\n    # stored info type name to derive a unique name from project and file\n    # name.\n    storage_bucket = PROJECT_ID + DLP_STORAGE_BUCKET_NAME\n    file_name = FILE_NAME\n    stored_info_type_name = f\"{storage_bucket}{file_name}\"\n\n    client = google.cloud.dlp_v2.DLPosite\n    dlp_stored_info_type = {\n        \"display_name\": stored_info_type_name,\n        \"storage_config\": DLP_STORAGEDestination,\n        \"info_type\": {\n            \"inspect_config\": {\"field_delimiters\": [\",\"]}\n        },\n    }\n\n    dlp_stored_info_type.name = stored_info_type_name.replace(\"-info-type\", \"-info-type2\")\n    client.stored_info_types.update(dlp_stored_info_type)\n\n\n# [END howto_update_stored_info_type_sample]\n\n# [START howto_list_stored_info_types_sample]\ndef list_stored_info_types():\n    # For each stored info type listed, the API requests the information type\n    # specification, including its display name, storage configuration, and\n    # any custom inspect configurations associated with it.\n    client = google.cloud.dlp_v2.DLPosite\n    stored_info_types = client.stored_info_types.list()\n    for stored_info_type in stored_info_types:\n        print(f\"Stored info type: {stored_info_type.name}\")\n        print(f\"  Display name: {stored_info_type.display_name}\")\n        print(f\"  Storage config: {stored_info_type.storage_config}\")\n        print(f\"  Inspect configs: {stored_info_type.info_type.inspect_config.custom_inspect_config}\")\n        print(\"\")\n\n\n# [END howto_list_stored_info_types_sample]\n\n# [START howto_delete_stored_info_type_sample]\ndef delete_stored_info_type():\n    # It is never necessary to set the value of the id field when calling\n    # delete_resource_item. However, if you are curious you will notice that\n    # the id field is ignored during the delete operation.\n    client = google.cloud.dlp_v2.DLPosite\n    client.stored_info_types.delete(id=\"my_stored_info_type\")\n\n\n# [END howto_delete_stored_info_type_sample]\n\n\nwith models.DAG(\n    \"example_dlp_stored_info_type\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp\", \"stored-info-type\"],\n) as dag:\n    get_id_action = models.TaskAction(callback=get_id)\n    create_stored_info_type_action = models.TaskAction(callback=create_stored_info_type)\n    update_stored_info_type_action = models.TaskAction(callback=update_stored_info_type)\n    list_stored_info_types_action = models.TaskAction(callback=list_stored_info_types)\n    delete_stored_info_type_action = models.TaskAction(callback=delete_stored_info_type)\n\n    # [START howto_create_stored_info_type]\n    create_stored_info_type_task = models.Task(\n        task_id=\"create_stored_info_type\",\n        action=create_stored_info_type(),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_create_stored_info_type]\n\n    # [START howto_update_stored_info_type]\n    update_stored_info_type_task = models.Task(\n        task_id=\"update_stored_info_type\",\n        action=update_stored_info_type(),\n        trigger_rule=TriggerRule.ALL_DONE,\n        custom_fields=get_id_action,\n    )\n    # [END howto_update_stored_info_type]\n\n    # [START howto_list_stored_info_types]\n    list_stored_info_types_task = models.Task(\n        task_id=\"list_stored_info_types\", action=list_stored_info_types(), trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_list_stored_info_types]\n\n    # [START howto_delete_stored_info_type]\n    delete_stored_info_type_task = models.Task(\n        task_id=\"delete_stored_info_type\",\n        action=delete_stored_info_type(),\n        custom_fields=get_id_action,\n    )\n    # [END howto_delete_stored_info_type]\n\n    create_stored_info_type_task >> update_stored_info_type_task >> list_stored_info_types_task >> delete_stored_info_type_task"}], "metadata": {"file_name": "dag_1211_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of creating cloud resources, performing a data transfer, and then cleaning up those resources. Develop a systematic workflow that demonstrates provisioning, data movement, and resource teardown in a cloud environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.transfers.copy_into_s3 import S3ToSqlCopyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_sql_copy\"\n\nconn = Connection(\n    conn_id=\"conn_sql\", conn_type=\"sql\", sql=\"TESTSQL\", extra=\"TESTEXTRA\"\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    default_args={\"conn_id\": conn},\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def populate_table():\n        # [START howto_operator_s3_to_sql_copy]\n        sql_table = \"test_table\"\n        s3_bucket = \"test_bucket\"\n        s3_path = \"s3://test_bucket/example.sql\"\n\n        S3ToSqlCopyOperator(\n            task_id=sql_table,\n            sql=\"COPY %s FROM %s PARTITION (%s)\" % (sql_table, s3_path, s3_path),\n            force=True,\n        )\n        # [END howto_operator_s3_to_sql_copy]\n\n    populate_table()\n\n    ng = SQLExecuteQueryOperator(\n        task_id=\"create_table\",\n        sql=f\"CREATE TABLE {sql_table} (col_1 VARCHAR(10), col_2 VARCHAR(10))\",\n        conn_id=\"conn_sql\",\n    )\n\n    hd = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        sql=f\"INSERT INTO {sql_table} VALUES ('val_1', 'val_2'), ('val_1', 'val_2')\",\n        conn_id=\"conn_sql\",\n    )\n\n    nn = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        sql=f\"DROP TABLE {sql_table}\",\n        conn_id=\"conn_sql\",\n    )\n\n    ng >> hd >> nn\n    # [START howto_operator_create_connection]\n    C = SQLExecuteQueryOperator(task_id=\"create_connection\", sql=\"SELECT * FROM system.testconnections\", conn_id=\"airflow\")\n    # [END howto_operator_create_connection]\n    # [START howto_operator_get_connections]\n    C2 = SQLExecuteQueryOperator(\n        task_id=\"get_connections\",\n        sql=\"SELECT * FROM system.testconnections\",\n        conn_id=\"airflow\",\n    )\n    # [END howto_operator_get_connections]\n    # [START howto_operator_drop_connection]\n    C3 = SQLExecuteQueryOperator(task_id=\"drop_connection\", sql=\"DROP CONNECTION airflow\", conn_id=\"airflow\")\n    # [END howto_operator_drop_connection]\n\n    (\n        populate_table()\n        >> ng\n        >> hd\n        >> nn\n        >> C\n        >> C2\n        >> C3\n    )\n\n    from tests_common.test_utils.system_tests import get_test_run\n\n    # This test gets triggered by the trigger rule on the connection, we want to wait for it to complete\n    wait_test_run = get_test_run(dag).add_task(lambda: lambda: None)"}], "metadata": {"file_name": "dag_1579_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I use AWS SDK for Python (Boto3) to filter and retrieve specific VPCs based on tags?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_args = {\n    \"test_context_builder_cls\": SystemTestContextBuilder,\n    \"default_args\": {\n        \"vpc_ids\": [\"default\"],\n        \"region\": \"us-east-1\",\n        \"conn_args\": {\n            \"region\": \"us-east-1\",\n            \"endpoint_url\": \"http://localhost:4321\",\n            \"aws_access_key_id\": \"1234\",\n            \"aws_secret_access_key\": \"1234\",\n        },\n    },\n    \"test_duration\": 10,\n    \"docker_env\": {\n        \"image\": \"amazon/aws-cli\",\n        \"command\": [\n            \"bash\",\n            \"-c\",\n            f\"echo {{'region': '{ENV_ID_KEY}'}} > test_context && \"\n            f\"echo '{{{json.dumps(sys_test_args['default_args']['conn_args'].region)}}}' > vpc_filter_test_context\",\n        ],\n    },\n}\n\n\n@task\ndef list_vpcs_with_tag(tag_name: str):\n    client = boto3.client(\"ec2\")\n    response = client.describe_vpcs(\n        Filters=[\n            {\n                \"Name\": \"tag-name\",\n                \"Values\": [tag_name],\n            }\n        ],\n        MaxResults=1000,\n    )\n    return [x[\"Vpc\"][\"VpcId\"] for x in response[\"Vpcs\"]]\n\n\n@task\ndef create_security_group(sec_group_name: str, vpc_id: str):\n    client = boto3.client(\"ec2\")\n    client.create_security_group(\n        Description=\"Test Security Group\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.authorize_security_group_ingress(\n        GroupId=client.get_security_group(\n            Description=\"Test Security Group\",\n        )[\"SecurityGroup\"][\"GroupId\"],\n        FromPort=22,\n        IpProtocol=\"TCP\",\n        ToPort=22,\n    )\n    client.authorize_security_group_ingress(\n        GroupId=client.get_security_group(\n            Description=\"Test Security Group\",\n        )[\"SecurityGroup\"][\"GroupId\"],\n        FromPort=80,\n        IpProtocol=\"TCP\",\n        ToPort=80,\n    )\n    return client.get_security_group(Description=\"Test Security Group\")[\"SecurityGroup\"][\"GroupId\"]\n\n\n@task\ndef create_launch_template(template_name: str):\n    client = boto3.client(\"ec2\")\n    client.create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n            \"InstanceMetadataOptions\": {\n                \"HttpEndpoint\": \"enabled\",\n                \"HttpTokens\": \"required\",\n            },\n            \"BlockDeviceMappings\": [\n                {\n                    \"DeviceName\": \"/dev/xvda1\",\n                    \"Ebs\": {\n                        \"Encrypted\": False,\n                        \"DeleteOnTermination\": True,\n                        \"SnapshotId\": \"snap-039f36007371ea185\",\n                        \"VolumeSize\": 8,\n                        \"VolumeSizeIncrement\": 8,\n                        \"AvailabilityZone\": \"us-east-1a\",\n                    },\n                }\n            ],\n            \"KeyName\": \"test-ec2-key-pair\",\n            \"InstanceType\": \"t2.micro\",\n            \"HibernationOptions\": {\"Enabled\": False},\n            \"TagSpecifications\": [\n                {\n                    \"ResourceType\": \"instance\",\n                    \"Tags\": [\n                        {\"Key\": \"Tag1\", \"Value\": \"value1\"},\n                        {\"Key\": \"Tag2\", \"Value\": \"value2\"},\n                    ],\n                }\n            ],\n        },\n    )\n    return client.get_launch_template(LaunchTemplateName=template_name)[\"LaunchTemplate\"][\"Id\"]\n\n\n@task\ndef create_launch_template_template_mapping(template_template_name: str, launch_template_id: str):\n    client = boto3.client(\"ec2\")\n    client.create_launch_template_template_mapping(\n        TemplateTemplateId=template_template_name, LaunchTemplateId=launch_template_id\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    client = boto3.client(\"ec2\")\n    client.delete_launch_template(LaunchTemplateName=template_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_name: str):\n    client = boto3.client(\"ec2\")\n    client.delete_security_group(GroupName=sec_group_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_vpc(vpc_id: str):\n    client = boto3.client(\"ec2\")\n    client.delete_vpc(VpcId=vpc_id)\n\n\nwith DAG(\n    dag_id=\"example_boto3\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n):\n    test_context = load_test_context(sys_test_args)\n    env_id = test_context[ENV_ID_KEY]\n\n    vpc_id = list_vpcs_with_tag(env_id)\n    sec_group_name = f\"{env_id}-security-group\"\n    template_name = f\"{env_id}-launch-template\"\n    template_template_name = f\"{env_id}-template-template\"\n\n    create_security_group(sec_group_name, vpc_id[0])\n    create_launch_template(template_name)\n    create_launch_template_template_mapping(template_template_name, create_launch_template(template_name)[\"LaunchTemplate\"][\"Id\"])\n\n    # [START howto_operator_boto3]\n    wait_for_termination = lambda: True  # Replace with actual wait logic.\n    delete_vpc(vpc_id[0])\n    delete_security_group(sec_group_name)\n    delete_launch_template(template_name)\n    delete_launch_template_template_mapping(template_template_name)\n    # [END howto_operator_boto3]\n\n    chain(\n        test_context,\n        vpc_id,\n        sec_group_name,\n        template_name,\n        template_template_name,\n        wait_for_termination,\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that sequentially executes SQL tasks with clear dependency management, showing how tasks can be chained together to perform a complete database operation lifecycle. The DAG should illustrate task ordering and atomic database interactions.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n\n@task(task_id=\"sql_sample_task\")\ndef execute_sql():\n    # create table\n    execute_sql = SQLExecuteQueryOperator(\n        task_id=\"create_table_user\",\n        sql=\"CREATE TABLE users (user_id IDENTITY(1,1), user_name VARCHAR(50), user_age SMALLINT)\",\n    )\n\n    # insert data\n    execute_sql = SQLExecuteQueryOperator(\n        task_id=\"insert_data_user\",\n        sql=\"INSERT INTO users(user_name, user_age) VALUES ('user_name1', 23), ('user_name2', 22)\",\n    )\n\n    # select data\n    execute_sql = SQLExecuteQueryOperator(task_id=\"select_data_user\", sql=\"SELECT * FROM users\")\n\n    # drop table\n    execute_sql = SQLExecuteQueryOperator(task_id=\"drop_table_users\", sql=\"DROP TABLE users\")\n\n\nwith DAG(\n    dag_id=\"example_sql_operator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    execute_sql()"}], "metadata": {"file_name": "dag_2180_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Azure Data Lake Storage (ADLS) workflow that demonstrates file upload and immediate deletion. Design a simple DAG that showcases basic file management operations in a cloud storage environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSCreateOperator, ADLSDeleteOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls_create_delete\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    fs = \"abfss://test@test-continent.dfs.core.windows.net/\"\n    path = \"test.txt\"\n    create_task = ADLSCreateOperator(task_id=\"create_task\", fs_body={\"path\": path, \"entity\": \"folder\"})\n    delete_task = ADLSDeleteOperator(task_id=\"delete_task\", fs_id=create_task.output, delete_contents=True)\n\n    create_task >> delete_task"}], "metadata": {"file_name": "dag_962_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that transforms raw text data into dense vector representations using Cohere's embedding technology, preparing unstructured text for advanced natural language processing tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, dag\nfrom airflow.providers.cohere.operators.embedding import CohereEmbeddingOperator\nfrom airflow.settings import Session, json_config\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\nconnection_id = os.environ.get(\"COHERE_DEFAULT_CONNECTION_ID\", \"cohere_default_connection\")\noutput_filename = os.environ.get(\"COHERE_OUTPUT_FILENAME\", \"output.json\")\njson_config_obj = json_config.parse_obj({})\n\nwith dag(\n    \"example_cohere_embedding_dag\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"cohere\"],\n) as dag:\n\n    @task\n    def create_connection(connection_id: str):\n        log.info(\"Creating connection %s\", connection_id)\n        session = Session()\n        session.add(Connection(conn_id=connection_id, conn_id_override=None))\n        session.commit()\n        log.info(\"Connection created. \")\n\n    @task\n    def create_role(role_name: str):\n        log.info(\"Creating role %s\", role_name)\n        session = Session()\n        role = Role(name=role_name)\n        session.add(role)\n        session.commit()\n        log.info(\"Role created. \")\n\n    @task\n    def get_connections():\n        log.info(\"Getting connections\")\n        session = Session()\n        rows = session.query(Connection).all()\n        for row in rows:\n            log.info(row)\n        log.info(\"Connections got. \")\n\n    @task\n    def get_roles():\n        log.info(\"Getting roles\")\n        session = Session()\n        rows = session.query(Role).all()\n        for row in rows:\n            log.info(row)\n        log.info(\"Roles got. \")\n\n    @task\n    def drop_role(role_name: str):\n        log.info(\"Dropping role %s\", role_name)\n        session = Session()\n        role = session.query(Role).filter(Role.name == role_name).first()\n        if role is not None:\n            session.delete(role)\n        session.commit()\n        log.info(\"Role dropped. \")\n\n    @task\n    def drop_connection(connection_id: str):\n        log.info(\"Dropping connection %s\", connection_id)\n        session = Session()\n        connection = session.query(Connection).filter(Connection.conn_id == connection_id).first()\n        if connection is not None:\n            session.delete(connection)\n        session.commit()\n        log.info(\"Connection dropped. \")\n\n    create_connection_task = create_connection(connection_id)\n    create_role_task = create_role(\"test_role\")\n    get_connections = get_connections()\n    get_roles = get_roles()\n    drop_role_task = drop_role(\"test_role\")\n    drop_connection_task = drop_connection(connection_id)\n    embed_task = CohereEmbeddingOperator(task_id=\"embedding_task\", model=\"large\", text=[u\"Hello World\"])\n    u = embed_task.output\n\n    (\n        create_connection_task\n        >> create_role_task\n        >> get_connections\n        >> get_roles\n        >> drop_role_task\n        >> drop_connection_task\n        >> embed_task\n        >> TaskInstance(ti_id=\"embedding_task\", dag=dag)\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as done\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1548_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data workflow automation process that leverages Azure Synapse for executing a predefined data processing pipeline on a daily schedule. Configure the DAG to reliably trigger and monitor the pipeline execution with appropriate error handling and logging.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunPipelineOperator\n\nEXECUTION_TIMEOUT = int(os.getenv(\"EXECUTION_TIMEOUT\", 60))\nDAG_ID = \"example_synapse_run_pipeline\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    execution_timeout=EXECUTION_TIMEOUT,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_synapse_run_pipeline]\n    run_pipeline = AzureSynapseRunPipelineOperator(\n        task_id=\"run_pipeline\",\n        pipeline_name=\"your_pipeline_name\",\n        column_mapping=\"your_column_mapping\",\n        azure_synapse_conn_id=\"your_azure_synapse_conn_id\",\n        batch_size=10,\n        batch_start_item=0,\n        batch_id_field_name=\"MyBatchId\",\n        batch_item_type_name=\"YourBatchItemTypeName\",\n    )\n    # [END howto_operator_synapse_run_pipeline]\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1364_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases cloud infrastructure management by dynamically creating a GCS bucket, transferring data using a specialized operator, and then cleaning up cloud resources after the transfer is complete.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.azure_data_lake_to_gcs import AzureDataLakeToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_azuredatalake\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nAZUREdatalake_CONNECTION_ID = \"azuredatalake\"\n\nCURRENT_FOLDER = os.path.dirname(__file__)\nLOCAL_PATH = os.path.join(CURRENT_FOLDER, \"data\")\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azuredatalake\"],\n) as dag:\n    create_azuredatalake_connection = Connection(\n        conn_type=\"azuredatalake\",\n        host=f\"zonabe.{PROJECT_ID}.datalake.azure.com\",\n        schema=\"default\",\n        login=f\"vss-tfbs-user-id\",\n        password=f\"vss-tfbs-password-docker\",\n    )\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_file():\n        file_path = f\"path/to/{DAG_ID}/{ENV_ID}/test_file.txt\"\n        os.system(f\"mkdir -p {LOCAL_PATH}/path/to/\")\n        os.system(f\"touch {LOCAL_PATH}/path/to/{DAG_ID}/{ENV_ID}/test_file.txt\")\n        os.system(f\"cp {CURRENT_FOLDER}/example_data/test_file.txt {LOCAL_PATH}/path/to/\")\n        os.system(f\"gsutil cp {LOCAL_PATH}/path/to/test_file.txt {BUCKET_NAME}/path/to/test_file2.txt\")\n\n    create_file_task = create_file()\n\n    move_file = AzureDataLakeToGCSOperator(\n        task_id=\"move_file\",\n        azuredatalake_conn_id=AZUREdatalake_CONNECTION_ID,\n        source_bucket=BUCKET_NAME,\n        source_path=\"path/to/test_file2.txt\",\n        destination_bucket=\"bucket_azuredatalake_test\",\n        destination_path=\"path/to/\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_azuredatalake_connection\n        >> create_bucket\n        >> create_file_task\n        # TEST BODY\n        >> move_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_1264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed task automation DAG that leverages WinRM to execute multiple system commands across a remote Windows infrastructure. Create a workflow that simulates real-world remote server management scenarios with parallel task execution and dependency tracking.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_winrm\"\n\nconn = Connection(\n    conn_id=\"winrm\",\n    conn_type=\"winrm\",\n    host=\"winrm\",\n    login=\"admin\",\n    password=\"secret\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"conn_id\": conn.conn_id},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def setup_winrm():\n        winrm_hook = WinRMHook()\n        winrm_hook.connect()\n\n    @task\n    def create_file():\n        pass\n\n    @task\n    def get_file():\n        pass\n\n    @task\n    def remove_file():\n        pass\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def cleanup_winrm():\n        WinRMHook().close()\n\n    setup_winrm_task = setup_winrm()\n    create_file_task = create_file()\n    get_file_task = get_file()\n    remove_file_task = remove_file()\n    cleanup_winrm_task = cleanup_winrm()\n    install_python_package_task = BashOperator(task_id=\"install-python-package\", bash_command=\"pip install opentelemetry-python\")\n    @WinRMOperator(task_id=\"winrm-python-sender\", winrm_hook=winrm_hook)\n    def winrm_sender()*:\n        pass\n\n    install_python_package_task >> setup_winrm_task >> create_file_task >> get_file_task >> remove_file_task >> cleanup_winrm_task >> winrm_sender"}], "metadata": {"file_name": "dag_977_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a simple Yandex Cloud data query workflow that demonstrates basic Airflow orchestration. Design a DAG that executes a trivial SQL query and marks its completion with a final empty operator.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.yandex.operators.yandex_cloud_db import YandexCloudDbQlsRunOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_yandex_cloud_db_qls_run\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"retries\": 1},\n    tags=[\"example\"],\n) as dag:\n    # [START ynd_qls_run_operator]\n    run_operator = YandexCloudDbQlsRunOperator(task_id=\"run_query\", sql=\"SELECT 1 AS res, 2 AS col\")\n    # [END ynd_qls_run_operator]\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2243_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that performs time-span based file processing in Google Cloud Storage, applying a custom transformation script to selected files from a source bucket and transferring the results to a destination bucket. Create a systematic approach to data processing that includes automated setup, transformation, and cleanup of cloud storage resources.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_bq\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_bucket():\n        conn = Connection(\n            conn_id=\"google_cloud_platform\",\n            conn_type=\"google_cloud_platform\",\n            extra={\n                \"project\": \"default\",  # For default project id\n                \"scope\": \"https://www.googleapis.com/auth/cloud-platform\",\n                \"environment\": \"us-central1\",\n            },\n        )\n        role = Role.query.filter_by(role_id=conn.role_id).first()\n        bucket_name = f\"bucket-{ENV_ID}-{DAG_ID}\"\n\n        # [START howto_gcs_to_bq_create]\n        gcs_to_bq = BashOperator(\n            task_id=\"gcs_to_bq_example\",\n            bash_command=f\"echo {bucket_name}\",\n        )\n        # [END howto_gcs_to_bq_create]\n\n        bucket = Connection(\n            role=role,\n            extra={\"project\": \"default\", \"scope\": \"https://www.googleapis.com/auth/cloud-platform\", \"environment\": \"us-central1\"},\n        ).connect()\n\n        TaskGroup(body=gcs_to_bq, task_ids=[\"bucket\", \"role\", \"connection\"])\n\n    create_bucket()\n\n    connection = Connection(\n        role=Role(id=\"gcp-sheets-read-write\"),\n        conn_id=\"google_cloud_platform\",\n        extra={\n            \"project\": \"1234567890\",\n            \"scope\": \"https://www.googleapis.com/auth/spreadsheets,https://www.googleapis.com/auth/cloud-platform\",\n            \"environment\": \"us-central1\",\n        },\n    )\n    role = Role.query.filter_by(role_id=\"gcp-sheets-read-write\").first()\n    gcp_sheets_conn = Connection(\n        role=role,\n        extra={\n            \"project\": \"1234567890\",\n            \"scope\": \"https://www.googleapis.com/auth/spreadsheets,https://www.googleapis.com/auth/cloud-platform\",\n            \"environment\": \"us-central1\",\n        },\n    ).connect()\n\n    # [START howto_create_external_table]\n    external_table_to_bq = BashOperator(\n        task_id=\"external_table_to_bq\",\n        bash_command=f\"echo {bucket_name}\",\n    )\n    # [END howto_create_external_table]\n\n    # [START howto_populate_external_table]\n    populate_external_table = BashOperator(\n        task_id=\"populate_external_table\",\n        bash_command=f\"echo {bucket_name}\",\n    )\n    # [END howto_populate_external_table]\n\n    # [START howto_create_external_vision_table]\n    external_vision_to_bq = BashOperator(\n        task_id=\"external_vision_to_bq\",\n        bash_command=f\"echo {bucket_name}\",\n    )\n    # [END howto_create_external_vision_table]\n\n    # [START howto_populate_external_vision_table]\n    populate_vision_table = BashOperator(\n        task_id=\"populate_vision_table\",\n        bash_command=f\"echo {bucket_name}\",\n    )\n    # [END howto_populate_external_vision_table]\n\n    [connection, role, gcp_sheets_conn, external_table_to_bq, populate_external_table, external_vision_to_bq, populate_vision_table]"}], "metadata": {"file_name": "dag_2063_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Analytics admin tasks, including creating, listing, and deleting properties and data streams. Build a comprehensive DAG that demonstrates interaction with Google Analytics Admin API using Airflow's operators and task dependencies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.marketing_platform.operators.admin import (\n    GoogleMarketingPlatformCreatePropertyOperator,\n    GoogleMarketingPlatformDeletePropertyOperator,\n    GoogleMarketingPlatformGetPropertyOperator,\n    GoogleMarketingPlatformListPropertiesOperator,\n    GoogleMarketingPlatformUpdatePropertyOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"marketing_platform\"\n\n# [START howto_marketing_platform_property_operator_args]\nPROPERTY = {\n    \"account_id\": \"INSERT_ACCOUNT_ID\",\n    \"parameters\": {\"property_id\": \"INSERT_PROPERTY_ID\"},\n}\n# [END howto_marketing_platform_property_operator_args]\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"marketing-platform\"],\n) as dag:\n    # [START howto_marketing_platform_list_properties_operator]\n    list_properties = GoogleMarketingPlatformListPropertiesOperator(task_id=\"list_properties\")\n    # [END howto_marketing_platform_list_properties_operator]\n\n    # [START howto_marketing_platform_get_property_operator]\n    get_property = GoogleMarketingPlatformGetPropertyOperator(\n        task_id=\"get_property\", property_id=\"{{ task_instance.xcom_pull('list_properties', key='marketing_platform_id') }}\"\n    )\n    # [END howto_marketing_platform_get_property_operator]\n\n    # [START howto_marketing_platform_create_property_operator]\n    create_property = GoogleMarketingPlatformCreatePropertyOperator(\n        task_id=\"create_property\",\n        property=PROPERTY,\n        account_id=\"{{ task_instance.xcom_pull('list_properties', key='marketing_platform_id') }}\",\n    )\n    # [END howto_marketing_platform_create_property_operator]\n\n    # [START howto_marketing_platform_update_property_operator]\n    update_property = GoogleMarketingPlatformUpdatePropertyOperator(\n        task_id=\"update_property\", property_id=\"{{ task_instance.xcom_pull('list_properties', key='marketing_platform_id') }}\", property=PROPERTY\n    )\n    # [END howto_marketing_platform_update_property_operator]\n\n    # [START howto_marketing_platform_delete_property_operator]\n    delete_property = GoogleMarketingPlatformDeletePropertyOperator(\n        task_id=\"delete_property\", property_id=\"{{ task_instance.xcom_pull('list_properties', key='marketing_platform_id') }}\"\n    )\n    # [END howto_marketing_platform_delete_property_operator]\n\n    (\n        list_properties\n        >> get_property\n        >> create_property\n        >> update_property\n        >> delete_property\n    )\n\n    delete_property.trigger_rule = TriggerRule.ALL_DONE"}], "metadata": {"file_name": "dag_1730_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases inter-service communication using Redis as a message broker and synchronization mechanism. Develop a workflow that publishes a start signal and then waits for a confirmation key before advancing to subsequent tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.redis.operators.redis import RedisPullOperator, RedisPushOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"redis_to_sql\"\n\nconn = Connection(\n    conn_id=\"sql_conn_id\",\n    conn_type=\"sql\",\n    host=\"db\",\n    schema=\"db\",\n    extra={\"table\": \"table\"},\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    default_args={\"sql_conn_id\": conn.conn_id},\n    schedule=None,\n    tags=[\"example\", \"redis\", \"sql\"],\n    catchup=False,\n) as dag:\n    @task\n    def populate_redis():\n        # [START howto_operator_redis_push]\n        redis_push_operator = RedisPushOperator(task_id=\"redis_push_task\", value=\"value_str\", channel=\"channel_str\")\n        # [END howto_operator_redis_push]\n\n        # [START howto_operator_redis_pull_sql]\n        sql_push_to_redis_operator = SQLExecuteQueryOperator(\n            task_id=\"sql_push_to_redis_sql_task\",\n            sql=f\"PSQL INSERT INTO channels (channel, value) VALUES ( 'new_channel', 'new_value';\",\n            conn_id=\"sql_conn_id\",\n        )\n        # [END howto_operator_redis_pull_sql]\n\n        redis_push_operator >> sql_push_to_redis_operator\n        # [END howto_operator_redis_push]\n\n    @task\n    def setup_connections():\n        # Connections are setup using the Airflow UI or\n        # the \"Show Distances\" tool https://cloud.google.com/dataproc/docs/cloud-ui/google-cloud-datasource-ui/\n\n        from airflow.providers.google.cloud.operators.dataproc import (\n            DataprocCreateClusterOperator,\n            DataprocDeleteClusterOperator,\n        )\n\n        CLUSTER_NAME = \"redis-cluster\"\n\n        DATAPROC = {\n            \"master_config\": {\n                \"num_workers\": 1,\n                \"machine_type\": \"n1-standard-1\",\n                \"disk\": {\"size_gb\": 1024, \"location_policy\": \"AMBIGUOUS\"},\n            },\n            \"worker_config\": {\n                \"num_workers\": 1,\n                \"machine_type\": \"n1-standard-1\",\n                \"disk\": {\"size_gb\": 1024, \"location_policy\": \"AMBIGUOUS\"},\n            },\n        }\n\n        # Update the region to one other than us-central1 if preferred\n        DATAPROC[\"master_config\"][\"location_id\"] = os.environ.get(\"GCP_DATAPROC_LOCATION\", \"us-central1\")\n\n        # Always delete the cluster when done\n        DATAPROC[\"delete_cluster\"] = {\"wait\": False}\n\n        create_cluster = DataprocCreateClusterOperator(\n            task_id=\"create_cluster\",\n            project_id=\"airflow-system-tests-60661\",\n            cluster_name=CLUSTER_NAME,\n            config=DATAPROC,\n        )\n\n        delete_cluster = DataprocDeleteClusterOperator(\n            task_id=\"delete_cluster\",\n            project_id=\"airflow-system-tests-60661\",\n            cluster_name=CLUSTER_NAME,\n            wait_for_completion=True,\n        )\n\n        [create_cluster, delete_cluster]\n\n    start = SQLExecuteQueryOperator(task_id=\"create_table\", sql=\"CREATE TABLE table (value VARCHAR(255))\")\n    populate_redis()\n    start\n    setup_connections()\n    RedisPullOperator(task_id=\"wait_for_sql_task\", conn_id=\"sql_conn_id\")\n    SQLExecuteQueryOperator(task_id=\"drop_table\", sql=\"DROP TABLE table\")"}], "metadata": {"file_name": "dag_2197_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a controlled database transaction process with explicit start and cancellation steps in a Google Cloud environment. Create a systematic demonstration of transaction management that ensures data consistency and prevents unintended persistent changes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_odbc_to_sqlite\"\n\nconn = Connection(\n    conn_id=\"my_odbc_connection\",\n    conn_type=\"odbc\",\n    login=\"demo_user\",\n    password=\"demo_password\",\n    host=\"127.0.0.1\",\n    port=1433,\n    schema=\"test_db\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def setup_db():\n        conn.create()\n        sql = \"CREATE TABLE IF NOT EXISTS sqlite_table (value VARCHAR(16), PRIMARY KEY (value))\"\n        execute_sql = SQLExecuteQueryOperator(\n            task_id=\"execute_sql\",\n            sql=sql,\n            conn_id=\"my_odbc_connection\",\n            dag=dag,\n        )\n\n    execute_sql = SQLExecuteQueryOperator(\n        task_id=\"execute_sql\",\n        sql=\"CREATE TABLE IF NOT EXISTS sqlite_table (value VARCHAR(16), PRIMARY KEY (value))\",\n        conn_id=\"my_odbc_connection\",\n        dag=dag,\n    )\n\n    [setup_db(), execute_sql] >> [execute_sql]"}], "metadata": {"file_name": "dag_1249_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I create a custom prompt for a text generation model using Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.openai.sensors.openai import OpenAISensor\n\n# [START howto_operator_openai_embedding]\n# [START howto_sensor_openai_embedding]\ndefault_args = {\n    \"start_date\": datetime(2022, 1, 1),\n    \"catchup\": False,\n}\n\nwith DAG(\n    \"example_openai\",\n    default_args=default_args,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # OPERATOR\n    embedding_dag_id = \"example_embedding_dag\"\n    system_message = \"Generate complete, syntactically correct Python code.\"\n    openai_embedding = OpenAIEmbeddingOperator(task_id=\"embedding_task\", model=\"text-embedding-ada-002\")\n\n    # SENSOR\n    @task(sensor=OpenAISensor(task_id=\"sensor_task\", model=\"text-embedding-ada-002\", input_text=system_message)))\n    def wait_for_sensor():\n        # Hops beyond which the sensor remains as fulfilled is the timeout period.\n        while sensor_instance.wait(fulfilled=True, timeout=1):\n            pass\n        print(\"The task 'sensor_task' is triggered.\")\n\n    # Create instance of sensor\n    sensor_instance = wait_for_sensor()\n\n    # [END howto_sensor_openai_embedding]\n    # [END howto_operator_openai_embedding]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that demonstrates advanced branching techniques using different Python execution contexts, showing how to dynamically select task paths and handle complex workflow routing. Implement a DAG that explores random selection, external Python execution, and virtual environment task execution with clear demonstration of conditional task routing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport random\nimport tempfile\nfrom datetime import datetime\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import ExternalPythonOperator, PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\nwith DAG(\n    dag_id=\"example_branch_python_decorator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, 0, 0, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_branch_python_decorator]\n    @task.branch()\n    def branch_task(rand: int):\n        if rand <= 0.5:\n            return \"python_operator_1_task\"\n        else:\n            return \"python_operator_2_task\"\n\n    # [END howto_operator_branch_python_decorator]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def teardown():\n        print(\"Teardown\")\n\n    # [START howto_operator_python_decorator]\n    python_operator_1_task = PythonOperator(\n        task_id=\"python_operator_1_task\",\n        python_callable=lambda rand, d: print(json.dumps(d.update({\"a\": rand}))),\n        op_kwargs={\"task_instance\": f\"{WeekDay.MONDAY}_{rand}_{dag.dag_id}\"},\n    )\n\n    python_operator_2_task = PythonOperator(\n        task_id=\"python_operator_2_task\",\n        python_callable=lambda rand, d: print(json.dumps(d.update({\"a\": \"t\"}))),\n        op_kwargs={\"task_instance\": f\"{WeekDay.MONDAY}_{rand}_{dag.dag_id}\"},\n    )\n\n    # End of branch logic\n\n    bash_task_1 = BashOperator(task_id=\"bash_task_1\", bash_command=\"echo 6\")\n    bash_task_2 = BashOperator(task_id=\"bash_task_2\", bash_command=\"echo 2\")\n    # [START howto_operator_externalpython_decorator]\n    external_python_task = ExternalPythonOperator(\n        task_id=\"external_python_task\",\n        python=importlib.__import__(env_id=\"foo\"),  # Optional: Specify environment id\n        python_callable=lambda rand, d: print(json.dumps(d.update({\"a\": \"e\"}))),\n        op_kwargs={\"task_instance\": f\"{WeekDay.MONDAY}_{rand}_{dag.dag_id}\"},\n    )\n    # [END howto_operator_externalpython_decorator]\n\n    teardown_task = BashOperator(task_id=\"teardown_task\", bash_command=\"echo 42\", trigger_rule=TriggerRule.ALL_DONE)\n\n    python_operator_1_task >> branch_task(5) >> [bash_task_1, external_python_task]\n    bash_task_1 >> bash_task_2\n    bash_task_2 >> teardown_task"}], "metadata": {"file_name": "dag_1803_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that sequentially executes SQL operations against a Trino database, demonstrating task dependencies and progressive data transformation steps. The workflow should showcase how to create schemas, generate tables, insert data, and perform template-based and parameterized queries.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.trino.operators.trino import TrinoOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"conn_id\": \"trino_conn\"},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino]\n\n    create_table = TrinoOperator(\n        task_id=\"create_table\",\n        sql=\"CREATE TABLE employees(id INT, name VARCHAR)\",\n        conn_id=\"trino_conn\",\n    )\n\n    # [END howto_operator_trino]\n\n    # [START howto_operator_trino_render_template]\n\n    create_table_from_file = TrinoOperator(\n        task_id=\"create_table_from_file\",\n        sql=\"CREATE TABLE employees(id INT, name VARCHAR)\",\n        conn_id=\"trino_conn\",\n        dag_run_params={\"sql_file\": \"employees.sql\"},\n    )\n\n    # [END howto_operator_trino_render_template]\n\n    # [START howto_operator_trino_multiple]\n\n    drop_table = TrinoOperator(\n        task_id=\"drop_table\",\n        sql=\"DROP TABLE employees\",\n        conn_id=\"trino_conn\",\n    )\n\n    # [END howto_operator_trino_multiple]\n\n    create_table >> drop_table"}], "metadata": {"file_name": "dag_1824_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data transfer pipeline that moves a sample CSV file between cloud storage platforms while implementing robust testing and teardown mechanisms. The pipeline should showcase how to programmatically create, transfer, and delete cloud storage resources using infrastructure-as-code principles.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.azure.hooks.fileshare import AzureFileShareHook\nfrom airflow.providers.common.azure.operators.fileshare import AzureFileShareCreateOperator\nfrom airflow.providers.common.azure.operators.fileshare import AzureFileShareDeleteOperator\nfrom airflow.providers.common.azure.operators.fileshare import AzureFileShareMoveOperator\nfrom airflow.providers.common.azure.sensors.fileshare import AzureFileShareSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_PROJECT_ID\")\nDAG_ID = \"example_fileshare\"\n\nCONTAINER_NAME = f\"container_{DAG_ID}_{ENV_ID}\"\nSHARE_NAME = f\"share_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"test-file.txt\"\nDESTINATION_PATH = \"test-folder\"\nLOCAL_PATH = \"/tmp\"\nFULL_LOCAL_PATH = LOCAL_PATH + FILE_NAME\nPARTIAL_LOCAL_PATH = LOCAL_PATH + \"part/\"\nSUBPATH = \"part-file.txt\"\n\n\n@task\ndef create_connection(conn_id_name):\n    connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"azurefile\",\n        description=\"Azure fileshare connection\",\n        extra={\"share_name\": SHARE_NAME, \"container_name\": CONTAINER_NAME},\n    )\n    db_api = os.environ.get(\"DB_API\")\n    if db_api:\n        connection.extra[\"azure_file_share_access_policy\"] = (\n            f\"FullControl;DefaultAccess;ShareAdministrators;ShareAdministrators;\"\n            f\"DirectoryAdministrators;DirectoryAdministrators;ApplicationPermissions;\"\n            f\"ApplicationUsers;ApplicationUsers;Reader;DirectoryReader;FileSystems;\"\n            f\"FilesystemAdministrators;FilesystemAdministrators;FileShareAdministrators;\"\n            f\"FileShareAdministrators;UniversalStreamDownload;UniversalStreamReader;\"\n            f\"UniversalStreamWriter;UniversalStreamUploader;UniversalUploader\"\n        )\n    print(connection)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_network_connection():\n        create_connection(\"azure_file_network\")\n\n    @task\n    def create_network_public_ip():\n        create_connection(\"azure_network_public_ip\")\n\n    @task\n    def create_public_directory(conn_id_name):\n        create_connection(conn_id_name)\n        hook = AzureFileShareHook()\n        hook.create_share(\n            share_name=SHARE_NAME,\n            container_name=CONTAINER_NAME,\n            initial_directory=PARTIAL_LOCAL_PATH,\n            exist_ok=False,\n        )\n        print(hook.get_share_status())\n\n    @task\n    def delete_public_directory(conn_id_name):\n        hook = AzureFileShareHook()\n        hook.delete_share(share_name=SHARE_NAME, container_name=CONTAINER_NAME)\n\n    @task\n    def get_fileshare_sensor():\n        hook = AzureFileShareHook()\n        return hook.get_share_status() == \"WROTE\"\n\n    @task\n    def delete_network_public_ip():\n        db_api = os.environ.get(\"DB_API\")\n        if db_api:\n            public_network = os.environ.get(\"PUBLIC_NETWORK\")\n            cmd = f\"az network public-ip delete --name {public_network} --resource-group {project_id} --force-delete true\"\n            print(cmd)\n            os.system(cmd)\n\n    @task\n    def delete_network_connection():\n        db_api = os.environ.get(\"DB_API\")\n        if db_api:\n            network_connection = os.environ.get(\"NETWORK_CONNECTION\")\n            cmd = f\"az connect-azurefile delete --conn-id {network_connection}\"\n            print(cmd)\n            os.system(cmd)\n\n    create_network_public_ip = create_network_public_ip()\n    create_network_connection = create_network_connection()\n    get_fileshare_sensor = get_fileshare_sensor()\n    delete_network_public_ip = delete_network_public_ip()\n    delete_network_connection = delete_network_connection()\n    create_fileshare = AzureFileShareCreateOperator(\n        task_id=\"create_fileshare\",\n        share_name=SHARE_NAME,\n        container_name=CONTAINER_NAME,\n        initial_directory=PARTIAL_LOCAL_PATH,\n    )\n    move_file = AzureFileShareMoveOperator(\n        task_id=\"move_file\",\n        source_share=SHARE_NAME,\n        source_container=CONTAINER_NAME,\n        source_path=PARTIAL_LOCAL_PATH,\n        destination_share=SHARE_NAME,\n        destination_container=CONTAINER_NAME,\n        destination_path=SUBPATH,\n    )\n    delete_fileshare = AzureFileShareDeleteOperator(\n        task_id=\"delete_fileshare\",\n        share_name=SHARE_NAME,\n        container_name=CONTAINER_NAME,\n    )\n    delete_fileshare.trigger_rule = TriggerRule.ALL_DONE\n\n    create_network_public_ip >> create_network_connection\n    get_fileshare_sensor >> delete_network_public_ip\n    create_fileshare >> move_file >> delete_fileshare\n    delete_network_connection"}], "metadata": {"file_name": "dag_1756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an ETL process that automatically ingests CSV files from an external data source into a predefined Snowflake table using Airflow's Snowflake integration. Implement a robust data movement strategy that supports flexible file parsing and consistent data loading.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.snowflake.sensors.snowflake import SnowflakeSensor\n\nSNOWFLAKE_SAMPLE_TABLE = \"stock\"\nSNOWFLAKE_SAMPLE_PAGEMENT = \"employees\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_snowflake_sensor\"\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"snowflake_conn_id\": \"my_snowflake_conn\",\n        \"snowflake_schema\": \"sb\",\n        \"snowflake_path\": \"demo_path\",\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    start = SnowflakeOperator(task_id=\"start\", sql=\"SELECT 1\")\n\n    # [START howto_sensor_snowflake]\n    def check_snowflake_sensor(context, task_output):\n        snowflake_conn_id = context[\"params\"][\"snowflake_conn_id\"]\n        snowflake_sensor_sql = f\"select 1 from snowflake_sensors_sensor (conn_id='{snowflake_conn_id}')\"\n        snowflake_sensor = SnowflakeSensor(task_id=\"sensor_task\", sql=snowflake_sensor_sql)\n        return snowflake_sensor.wait() == \"0 rows\"\n    # [END howto_sensor_snowflake]\n\n    end = SnowflakeOperator(task_id=\"end\", sql=\"SELECT 1\")\n\n    start >> check_snowflake_sensor >> end"}], "metadata": {"file_name": "dag_1796_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates automated vector data insertion with configurable parameters like index name, namespace, and batch size. Construct a workflow that shows how to dynamically load vector embeddings from environment variables or predefined lists.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.weaviate.operators.weaviate import WeaviateIngestOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_weaviate_embedding_axle]\n# TODO: Complete the following list of task with the appropriate operators for each task.\ntasks = [\n    # The Axle task below requires the DagRun's metadata to be present. If the DagRun's\n    # metadata comes from a trigger rule (e.g., from the command line or the UI), then\n    # the DagRun needs to exist first in order to get the metadata.\n    task(task_id=\"axle_install_task\"),\n    task(task_id=\"add_weaviate_metadata_task\"),\n    OpenAIEmbeddingOperator(\n        task_id=\"embedding_task\",\n        model=\"text-embedding-ada-002\",\n        input_text=[\"This is some text with embeddings\"],\n    ),\n    # Weaviate's operator below requires an existing Weaviate dimension named 'example_dim'\n    # To create it, please run weaviate operator without any data first:\n    # airflow operators/weaviate/operators/WeaviateRemoveDimensionOperator or\n    # airflow operators/weaviate/operators/WeaviateAddDimensionOperator\n    task(task_id=\"add_example_dim_task\"),\n    WeaviateIngestOperator(\n        task_id=\"ingest_task\",\n        # WeaviateIngestOperator accepts any value that can be converted to a dict like the example below.\n        # The expected keys in the dict are the namespaces (str) and dimensions (dict).\n        # You can find the list of existing dimensions in weaviate/operators/asia.py\n        # import statement.\n        dict(\n            namespace=\"example\",\n            data={\n                \"sample1\": \"This is some text with  embeddings and a dimension value\",\n                \"sample2\": \"More text with embeddings and dimension value\",\n            },\n        ),\n        weaviate_config={\"vector\": True},\n    ),\n    # To delete all data in weaviate namespace called 'example' you can use the following operator:\n    # airflow operators/WeaviateRemoveAllDataOperator(task_id='remove_all_data_task'),\n    trigger_rule=TriggerRule.ALL_DONE,\n]\n# [END howto_operator_weaviate_embedding_axle]\n\n\n@task\ndef remove_weaviate_data(dag_run):\n    namespace = dag_run.conf.get(\"namespace\", \"example\")\n    weaviate_config = {\n        \"namespace\": namespace,\n        \"delete_data\": True,\n        \"delete_associations\": True,\n    }\n    weaviate_remove_data_operator = (\n        \"remove_weaviate_data\"\n    ) = \"airflow operators/WeaviateRemoveAllDataOperator\"\n    weaviate_remove_data_operator.trigger_rule = TriggerRule.ALL_DONE\n    weaviate_remove_data_operator.xcom_push(key=\"weaviate_config\", value=weaviate_config)\n\n\nwith DAG(\n    dag_id=\"example_weaviate_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    axle_install_task = remove_weaviate_data(dag)\n    for task in tasks:\n        axle_install_task >> task"}], "metadata": {"file_name": "dag_639_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure automation DAG that simulates a scalable cloud computing task execution environment. Create a workflow that programmatically provisions a compute resource, executes predefined remote commands, and then systematically tears down the infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.week import Week\nfrom airflow.settings import Session\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=Week.day(3),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo hello world\")\n\n    @task\n    def create_connection(conn_id_name: str):\n        session = Session()\n        session.add(Connection(conn_id_name=conn_id_name))\n        session.commit()\n\n    create_connection_task = create_connection(conn_id_name=\"singularity_conn\")\n\n    run_this >> create_connection_task\n\n    # [START howto_operator_singularity]\n    singularity_task = SingularityOperator(\n        task_id=\"singularity_task\",\n        image=\"docker://quay.io/test/test-singularity:5.9.2\",\n        command=\"/hello_singularity.sh\",\n        environment={\"HELLO\": \"GOODBYE\"},\n        volumes={\"/host/PATH\": {\"bind\": \"/PATH\", \"mode\": \"rw\"}},\n        network_mode=\"none\",\n        num_instances=2,\n    )\n    # [END howto_operator_singularity]"}], "metadata": {"file_name": "dag_172_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure pipeline that automates the provisioning and management of a scalable NoSQL database environment using Cloud Bigtable. Design a workflow that demonstrates dynamic resource allocation, replication monitoring, and systematic teardown of database infrastructure with configurable parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.bigtable import (\n    BigtableCreateInstanceOperator,\n    BigtableCreateTableOperator,\n    BigtableDeleteInstanceOperator,\n    BigtableDeleteTableOperator,\n    BigtableDisableMaintenanceModeOperator,\n    BigtableEnableMaintenanceModeOperator,\n    BigtableGetInstanceOperator,\n    BigtableGetInstanceField,\n    BigtableGetTableOperator,\n    BigtableGetTableField,\n    BigtableUpdateInstanceOperator,\n    BigtableUpdateInstanceField,\n    BigtableUpdateTableOperator,\n    BigtableUpdateTableField,\n)\nfrom airflow.providers.alibaba.cloud.sensors.bigtable import (\n    BigtableGetInstanceSensor,\n    BigtableGetTableSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_bigtable\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"owner\": \"airflow\"},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    connection = Connection(\n        conn_id=\"bigtable\",\n        conn_type=\"bigtable\",\n        prev_key=\"airflow\",\n    )\n\n    @task\n    def create_db():\n        instance_id = \"test-instance\"\n        table_id = \"test-table\"\n        create_instance = BigtableCreateInstanceOperator(\n            task_id=\"create-instance\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            location=\"default-location\",\n            instance_id=instance_id,\n        )\n        create_table = BigtableCreateTableOperator(\n            task_id=\"create-table\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            location=\"default-location\",\n            instance_id=instance_id,\n            table_id=table_id,\n        )\n        disable = BigtableDisableMaintenanceModeOperator(\n            task_id=\"disable-mode\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n        )\n        update_instance = BigtableUpdateInstanceOperator(\n            task_id=\"update-instance\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            is_auto_create_tables_enabled=True,\n            is_auto_delete_tables_enabled=True,\n        )\n        update_instance_field = BigtableUpdateInstanceFieldOperator(\n            task_id=\"update-instance-field\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            field=BigtableUpdateInstanceField.Field(\n                table_id=table_id\n            ),\n            is_auto_create_tables_enabled=True,\n            is_auto_delete_tables_enabled=True,\n        )\n        enable = BigtableEnableMaintenanceModeOperator(\n            task_id=\"enable-mode\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n        )\n        update_table = BigtableUpdateTableOperator(\n            task_id=\"update-table\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n            is_read_only_mode_enabled=True,\n        )\n        update_table_field = BigtableUpdateTableFieldOperator(\n            task_id=\"update-table-field\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n            field=BigtableUpdateTableField.Field(id=\"12345\"),\n            is_read_only_mode_enabled=True,\n        )\n        get_instance = BigtableGetInstanceOperator(\n            task_id=\"get-instance\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            bigtable_get_instance_fields=[\n                BigtableGetInstanceField.id(instance_id),\n            ],\n        )\n        get_table = BigtableGetTableOperator(\n            task_id=\"get-table\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n            bigtable_get_table_fields=[\n                BigtableGetTableField.id(table_id),\n            ],\n        )\n        get_instance_sensor = BigtableGetInstanceSensor(\n            task_id=\"get-instance-sensor\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n            deferrable=True,\n        )\n        get_table_sensor = BigtableGetTableSensor(\n            task_id=\"get-table-sensor\",\n            connection_id=\"bigtable\",\n            project_id=\"test-project\",\n            instance_id=instance_id,\n            table_id=table_id,\n        )\n        get_instance_sensor.wait_for_completion = False\n        get_table_sensor.wait_for_completion = False\n\n    create_db()\n\n    delete_instance = BigtableDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_table = BigtableDeleteTableOperator(\n        task_id=\"delete-table\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    disable_instance = BigtableDisableMaintenanceModeOperator(\n        task_id=\"disable-instance\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    enable_instance = BigtableEnableMaintenanceModeOperator(\n        task_id=\"enable-instance\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    update_instance = BigtableUpdateInstanceOperator(\n        task_id=\"update-instance\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        is_auto_create_tables_enabled=True,\n        is_auto_delete_tables_enabled=True,\n    )\n    update_instance_field = BigtableUpdateInstanceFieldOperator(\n        task_id=\"update-instance-field\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        id=\"12345\",\n        is_auto_create_tables_enabled=True,\n        is_auto_delete_tables_enabled=True,\n    )\n    update_table = BigtableUpdateTableOperator(\n        task_id=\"update-table\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        is_read_only_mode_enabled=True,\n    )\n    update_table_field = BigtableUpdateTableFieldOperator(\n        task_id=\"update-table-field\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        id=\"12345\",\n        id=\"12345\",\n        is_read_only_mode_enabled=True,\n    )\n    get_instance = BigtableGetInstanceOperator(\n        task_id=\"get-instance\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        bigtable_get_instance_fields=[\n            BigtableGetInstanceField.id(instance_id),\n        ],\n    )\n    get_table = BigtableGetTableOperator(\n        task_id=\"get-table\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        bigtable_get_table_fields=[\n            BigtableGetTableField.id(table_id),\n        ],\n    )\n    get_instance_sensor = BigtableGetInstanceSensor(\n        task_id=\"get-instance-sensor\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n        deferrable=True,\n    )\n    get_table_sensor = BigtableGetTableSensor(\n        task_id=\"get-table-sensor\",\n        connection_id=\"bigtable\",\n        project_id=\"test-project\",\n        instance_id=instance_id,\n        table_id=table_id,\n    )\n    get_instance_sensor.wait_for_completion = False\n    get_table_sensor.wait_for_completion = False\n\n    delete_instance()\n    delete_table()\n    disable_instance()\n    enable_instance()"}], "metadata": {"file_name": "dag_821_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates file transfer techniques between cloud storage buckets with explicit error handling and resource management. Construct a workflow that shows how to programmatically move files, manage cloud storage resources, and ensure reliable data transfer across different storage locations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_bigquery\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_bucket():\n        conn = Connection(\n            conn_id=\"google_cloud_platform\",\n            conn_type=\"google_cloud_platform\",\n            extra={\n                \"project\": \"test-12345\",\n                \"location\": \"us-central1\",\n                \"storage_config\": {\n                    \"bucket\": \"test-bucket\",\n                    \"predefined_mtu\": \"disk\",\n                },\n            },\n        )\n        session = conn.provide_session()\n        session.create_connection(conn)\n        Role(roles=[\"projects/test-12345/roles/us-central1-userTableIface\"], project_id=\"test-12345\")\n\n    @task\n    def create_object(bucket_name):\n        conn = Connection(\n            conn_id=\"google_cloud_platform\",\n            conn_type=\"google_cloud_platform\",\n            extra={\"project\": \"test-12345\", \"location\": \"us-central1\", \"bucket\": bucket_name},\n        )\n        session = conn.provide_session()\n        session.create_connection(conn)\n        BashOperator(task_id=\"create_object\", bash_command=f\"echo 'hello world' > {bucket_name}/hello.txt\")\n\n    @task\n    def create_dataflow_job():\n        bigquery = Role(\n            roles=[\"projects/test-12345/bigqueryDatasetWriter\", \"projects/test-12345/bigqueryDatasetReader\"],\n            project_id=\"test-12345\",\n        )\n        bucket = Role(\n            roles=[\"projects/test-12345/gcsWriteIface\"], project_id=\"test-12345\", storage_bucket=\"test-bucket\"\n        )\n\n        connection = Connection(\n            conn_id=\"google_cloud_platform\",\n            conn_type=\"google_cloud_platform\",\n            extra={\n                \"project\": \"test-12345\",\n                \"location\": \"us-central1\",\n                \"service_account\": \"service-account-email\",\n            },\n        )\n        session = connection.provide_session()\n        job = {\n            \"launchTime\": int(datetime.now().timestamp()),\n            \"parameters\": {\n                \"inputDataset\": f\"bq:{'project:default/dataset1'}\",\n                \"outputDataset\": f\"bq:{'project:default/dataset2'}\",\n                \"insertCount\": \"1000\",\n                \"sourceUris\": [f\"gs://{bucket}/hello.txt\"],\n            },\n            \"service_account\": \"service-account-email\",\n        }\n        session.create_job(job, location=\"us-central1\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_dataflow_job():\n        BashOperator(\n            task_id=\"delete_dataflow_job\",\n            bash_command=f\"gsutil delete -r {bucket}\",\n        )\n\n    @task\n    def delete_bucket(bucket_name):\n        BashOperator(task_id=\"delete_bucket\", bash_command=f\"gsutil delete -r {bucket_name}\")\n\n    delete_bucket(\"test-bucket\")\n    create_bucket()\n    create_object(\"test-bucket\")\n    create_dataflow_job()\n    delete_dataflow_job()"}], "metadata": {"file_name": "dag_1684_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates enterprise-level data extract refresh techniques for business intelligence reporting systems. Create a workflow that shows how to trigger workbook refreshes and monitor their completion status using different execution strategies.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_odbc_to_sqlite\"\n\nconn = Connection(\n    conn_id=\"my_odbc_conn\",\n    conn_type=\"odbc\",\n    login=\"demo_user\",\n    password=\"demo_password\",\n    host=\"localhost\",\n    schema=\"test_db\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def setup_db():\n        # [START create_db]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"create_table\", bash_command=\"echo 'CREATE TABLE test_table (col_1 INT);'\")\n\n        # [END create_db]\n\n    @task\n    def setup_odbc_connection():\n        # [START setup_odbc_connection]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(\n            task_id=\"setup_odbc_connection\",\n            bash_command=f\"echo \\\"CREATE CONNECTION my_odbc_conn ('ODBC Driver 17 ODBC Driver', 'DSN=my_odbc_dsn');\\\"\",\n        )\n        # [END setup_odbc_connection]\n\n    @task\n    def setup_worksheet():\n        # [START setup_worksheet]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"setup_worksheet\", bash_command='echo \"USE SQLITE;\";')\n\n        # [END setup_worksheet]\n\n    @task\n    def setup_query():\n        # [START drop_table]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"drop_table\", bash_command='echo \"DROP TABLE test_table;\"')\n        # [END drop_table]\n\n        # [START create_table]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"create_table\", bash_command='echo \"CREATE TABLE test_table (col_1 INT);\"')\n        # [END create_table]\n\n        # [START insert_data]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"insert_data\", bash_command='echo \"INSERT INTO test_table VALUES (1), (2), (3);\"')\n        # [END insert_data]\n\n        # [START drop_worksheet]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"drop_worksheet\", bash_command='echo \"DROP WORKSHEET test_worksheet;\"')\n        # [END drop_worksheet]\n\n        # [START create_worksheet]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"create_worksheet\", bash_command='echo \"USE WORKSHEET test_worksheet;\"')\n        # [END create_worksheet]\n\n        # [START read_data]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"read_data\", bash_command='echo \"SELECT * FROM test_table;\"')\n        # [END read_data]\n\n        # [START cleanup]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"cleanup\", bash_command='echo \"DROP TABLE test_table;\"')\n        # [END cleanup]\n        return True\n        # [END cleanup]\n        # [START get_last_watched_connection_id]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"get_last_watched_connection_id\", bash_command='echo \"\\\"\\\\\"\"')\n        # [END get_last_watched_connection_id]\n        return \"\"\n        # [END get_last_watched_connection_id]\n        # [START_destructure_connection]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"destructure_connection\", bash_command=f\"echo \\\"{conn[conn_id].last_dag_id}_\\\" {conn[conn_id].last_task_id}\")\n        # [END destructure_connection]\n        # [START _get_last_task_in_dag]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"get_last_task_in_dag\", bash_command=f\"echo \\\"{conn[conn_id].last_dag_id}_{{}}\\\" _get_last_task_in_dag\")\n        # [END _get_last_task_in_dag]\n        # [START _get_last_task]\n        from airflow.providers.standard.operators.bash import BashOperator\n\n        BashOperator(task_id=\"get_last_task\", bash_command=f\"echo \\\"{conn[conn_id].last_dag_id}_{{}}\\\" _get_last_task\")\n        # [END _get_last_task]\n        return False\n        # [END return_task]\n    # [END return_task]\n\n    setup_odbc_connection()\n    setup_db()\n    setup_worksheet()\n    setup_query()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def stop_all_workers():\n        from airflow.sdk import Connection\n\n        connections = Connection.query()\n        for connection in connections:\n            if connection.conn_id in (\"my_odbc_conn\", \"my_sqlite_conn\"):\n                connection.delete()\n\n    stop_all_workers()\n\n    t1 = BashOperator(task_id=\"print_lol\", bash_command=\"lol\")\n\n    t1 >> setup_worksheet() >> setup_query()\n    t1 >> setup_odbc_connection() >> setup_db()\n    t1 >> BashOperator(task_id=\"print_lol_2\", bash_command=\"lol 2\")\n\n    t1 >> stop_all_workers()"}], "metadata": {"file_name": "dag_2224_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates scheduling and executing a data pipeline in Microsoft Fabric, highlighting the process of triggering data transformation and movement tasks through a cloud-native orchestration mechanism.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.operators.msgraph import AzureGraphDataOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_msgraph_data\"\nFLOW_ID = os.environ.get(\"FLOW_ID\", \"flow-id-12345\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_msgraph_data]\n    @task\n    def mock_data():\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            response = msgraph_client.data.v1.trigger_flow(flow_id=FLOW_ID)\n            return response[\"id\"]\n\n    mock_data_task = mock_data()\n    # [END howto_operator_msgraph_data]\n\n    # [START howto_operator_msgraph_list]\n    @task\n    def fetch_data(item_id: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            response = msgraph_client.data.v1.list(item_id)\n            return response[\"value\"]\n\n    fetch_data_task = fetch_data(mock_data_task.output)\n    # [END howto_operator_msgraph_list]\n\n    # [START howto_operator_msgraph_create]\n    @task\n    def create_data(item_id: str, name: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            body = {\"displayName\": name}\n            response = msgraph_client.data.v1.create(item_id, body=body)\n            return response[\"id\"]\n\n    create_data_task = create_data(mock_data_task.output, \"Python Test\")\n    # [END howto_operator_msgraph_create]\n\n    # [START howto_operator_msgraph_update]\n    @task\n    def update_data(item_id: str, name: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            body = {\"displayName\": name}\n            response = msgraph_client.data.v1.update(item_id, body=body)\n            return response[\"displayName\"]\n\n    update_data_task = update_data(mock_data_task.output, \"Python Test Updated\")\n    # [END howto_operator_msgraph_update]\n\n    # [START howto_operator_msgraph_delete]\n    @task\n    def delete_data(item_id: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            response = msgraph_client.data.v1.delete(item_id)\n            return response[\"id\"]\n\n    delete_data_task = delete_data(create_data_task.output)\n    # [END howto_operator_msgraph_delete]\n\n    # [START howto_operator_msgraph_download]\n    @task\n    def download_file(item_id: str, download_path: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            response = msgraph_client.data.v1.download(item_id, download_path)\n            return response[\"id\"]\n\n    download_file_task = download_file(create_data_task.output, \"downloaded-file.txt\")\n    # [END howto_operator_msgraph_download]\n\n    # [START howto_operator_msgraph_upload]\n    @task\n    def upload_file(item_id: str, file_path: str):\n        from airflow.providers.microsoft.azure.hooks.msgraph import MSGraphHook\n\n        hook = MSGraphHook()\n        with hook.connect() as msgraph_client:\n            response = msgraph_client.data.v1.upload(item_id, file_path)\n            return response[\"id\"]\n\n    upload_file_task = upload_file(create_data_task.output, \"uploaded-file.txt\")\n    # [END howto_operator_msgraph_upload]\n\n    mock_data_task >> fetch_data_task >> create_data_task >> update_data_task >> delete_data_task >> download_file_task >> upload_file_task"}], "metadata": {"file_name": "dag_1359_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you explain the difference between a tuple and a list in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    print(str(pendulum.datetime()))\n\n\nwith DAG(\n    \"example_date\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    print_date()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I need help with a specific calculation in Python. I want to determine the interest accumulated on a bank account over a period of years, assuming a fixed annual interest rate and compound interest is applied. The account starts with an initial deposit. The formula for compound interest is A = P(1 + r/n)^(nt), where A is the amount of money accumulated after n years, including interest, P is the principal amount (initial deposit), r is the annual interest rate (decimal), n is the number of times that interest is compounded per year, and t is the time the money is invested for in years. Can you help me implement this in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def calculate_interest(principal, annual_interest_rate, years, compounded_times_per_year):\n    \"\"\"\n    Calculate the accumulated interest on a bank account over a period of years with compound interest.\n\n    Args:\n    principal (float): The initial deposit amount.\n    annual_interest_rate (float): The annual interest rate as a decimal (e.g., 0.05 for 5%).\n    years (int): The number of years the money is invested for.\n    compounded_times_per_year (int): The number of times interest is compounded per year.\n\n    Returns:\n    float: The accumulated amount of money after the specified number of years.\n    \"\"\"\n    # Calculate the accumulated amount using the compound interest formula\n    accumulated_amount = principal * (1 + annual_interest_rate / compounded_times_per_year) ** (compounded_times_per_year * years)\n    return accumulated_amount\n\n# Example usage\ninitial_deposit = 1000\nannual_rate = 0.05\ninvestment_period = 10\ncompounding_frequency = 12\n\n# Calculate the accumulated interest\nfinal_amount = calculate_interest(initial_deposit, annual_rate, investment_period, compounding_frequency)\nprint(f\"The accumulated amount after {investment_period} years is: ${final_amount:.2f}\")"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates interaction with InfluxDB using the InfluxDBOperator, showing how to connect to a time-series database and execute a Flux query. Develop a task that retrieves data from a predefined bucket with a configurable time window.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\nfrom airflow.providers.influxdb.operators.influxdb import InfluxDBOperator\n\nINFLUXDB_CONN_ID = os.environ.get(\"INFLUXDB_CONN_ID\", \"influxdb_default\")\nBUCKET_NAME = os.environ.get(\"BUCKET_NAME\", \"test-bucket-1\")\nINFLUXDB_END_TIME = os.environ.get(\"INFLUXDB_END_TIME\", \"2020-01-01T00:00:00Z\")\nINFLUXDB_START_TIME = os.environ.get(\"INFLUXDB_START_TIME\", \"2020-01-01T00:00:00Z\")\nINFLUXDB_PLOT_SIZE = os.environ.get(\"INFLUXDB_PLOT_SIZE\", \"10s\")\nQUERY_QUERY = os.environ.get(\"QUERY_QUERY\", \"from(bucket:\\\"test-bucket-1\\\") |> range(start:end:\\\"2020-01-01T00:00:00Z\\\", interval:\\\"10s\\\")\")\n\n\nwith DAG(\n    \"influxdb-example\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_influxdb]\n\n    inst = InfluxDBHook(influxdb_conn_id=INFLUXDB_CONN_ID)\n\n    query = inst.query_query(QUERY_QUERY, start=INFLUXDB_START_TIME, end=INFLUXDB_END_TIME, bucket=BUCKET_NAME)\n    # [END howto_operator_influxdb]"}], "metadata": {"file_name": "dag_612_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I obtain the A10 Networks AXAPI version 2.1 using the python-axapi library?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.axapi.hooks.axapi import AxapiHook\n\ntry:\n    from airflow.providers.axapi.sensors.axapi import AxapiSensor\nexcept ModuleNotFoundError:\n    print(\n        \"AxapiSensor not available. Make sure the provider axapi is installed and the connection details\"\n        \"are correct for the DAG to execute.\"\n    )\n\n\n@task\ndef get_version(axapi_conn: AxapiHook):\n    \"\"\"Returns the A10 Networks AXAPI version 2.1\"\"\"\n    return axapi_conn._conn.version\n\n\nwith DAG(\n    \"axapi_sensor_example\",\n    schedule=IntervalSchedule(\n        days=1,\n    ),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"axapi\"],\n) as dag:\n    # [START howto_axapi_sensor_axapi]\n    axapi_sensor = AxapiSensor(check_id=\"test_check\", check_period=\"test_period\", conn=AxapiHook())\n    # [END howto_axapi_sensor_axapi]\n\n    axapi_sensor >> get_version(axapi_conn)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to fine-tune a Google Vertex AI generative model using supervised learning techniques. The DAG will showcase how to take a base Gemini model and customize it with specific training data to improve its performance for a particular use case.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI Generative Model Tuning Job Operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSFileTransformOperator,\n    GCSListObjectsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.generative import (\n    VertexAiGenerativeCreateTuningJobOperator,\n    VertexAiGenerativeDeleteTuningJobOperator,\n    VertexAiGenerativeGetTuningJobOperator,\n    VertexAiGenerativeListTuningJobsOperator,\n    VertexAiGenerativeTuneModelOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_generative_tuning_job\"\nREGION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_OBJECT_NAME = \"generative/150k_gen_samples.jsonl\"\n\nGEN_MODEL = {\n    \"display_name\": f\"gen_model_{ENV_ID}\",\n    \"training_config\": {\n        \"train_budget_milli\": 1000,\n        \"training_fraction_test\": 0.2,\n        \"training_dataset_spec\": {\"input_config\": {\"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}}},\n    },\n}\nTUNING_JOB_NAME = f\"TuningJob_{ENV_ID}\"\nTUNING_JOB_2_NAME = f\"TuningJob_{ENV_ID}_2\"\nTUNING_CONFIG = {\n    \"training_budget_milli\": 2000,\n    \"training_fraction_test\": 0.2,\n    \"model_spec\": {\"display_name\": f\"tuning_{ENV_ID}\"},\n    \"optimization_spec\": {\n        \"objective\": {\"align_cross_entropy_loss\": 1},\n        \"params\": {\"temperature\": 0.2, \"top_p\": 0.95, \"top_k\": 5},\n        \"dataset\": {\"input_config\": {\"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}}},\n    },\n}\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"generative\", \"tuning_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    move_data_file = GCSFileTransformOperator(\n        task_id=\"move_data_file\",\n        source_bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        source_object=DATA_SAMPLE_GCS_OBJECT_NAME,\n        destination_bucket=\"gs://gen_data/\",\n        destination_object=\"generative/\",\n    )\n\n    create_tuning_job = VertexAiGenerativeCreateTuningJobOperator(\n        task_id=\"create_tuning_job\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_NAME,\n        gen_model=GEN_MODEL,\n        tuning_config=TUNING_CONFIG,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_vertex_ai_generative_get_tuning_job_operator]\n    get_tuning_job = VertexAiGenerativeGetTuningJobOperator(\n        task_id=\"get_tuning_job\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_get_tuning_job_operator]\n\n    # [START how_to_vertex_ai_generative_list_tuning_jobs_operator]\n    list_tuning_jobs = VertexAiGenerativeListTuningJobsOperator(\n        task_id=\"list_tuning_jobs\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_list_tuning_jobs_operator]\n\n    # [START how_to_vertex_ai_generative_delete_tuning_job_operator]\n    delete_tuning_job = VertexAiGenerativeDeleteTuningJobOperator(\n        task_id=\"delete_tuning_job\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_NAME,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_vertex_ai_generative_delete_tuning_job_operator]\n\n    # [START how_to_vertex_ai_generative_tune_model_operator]\n    tune_model = VertexAiGenerativeTuneModelOperator(\n        task_id=\"tune_model\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_NAME,\n        gen_model=GEN_MODEL,\n        tuning_config=TUNING_CONFIG,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_tune_model_operator]\n\n    # [START how_to_vertex_ai_generative_create_tuning_job_operator_kwargs]\n    create_tuning_job_kwargs = VertexAiGenerativeCreateTuningJobOperator(\n        task_id=\"create_tuning_job_kwargs\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_2_NAME,\n        gen_model=GEN_MODEL,\n        tuning_config={\"training_budget_milli\": 5000, \"training_fraction_test\": 0.3, \"model_spec\": {\"display_name\": f\"tuning_{ENV_ID}_2\"}},\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_create_tuning_job_operator_kwargs]\n\n    # [START how_to_vertex_ai_generative_get_tuning_job_operator_kwargs]\n    get_tuning_job_2 = VertexAiGenerativeGetTuningJobOperator(\n        task_id=\"get_tuning_job_2\",\n        region=REGION,\n        tuning_job_name=TUNING_JOB_2_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_get_tuning_job_operator_kwargs]\n\n    # [START how_to_vertex_ai_generative_list_tuning_jobs_operator_kwargs]\n    list_tuning_jobs_2 = VertexAiGenerativeListTuningJobsOperator(\n        task_id=\"list_tuning_jobs_2\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_vertex_ai_generative_list_tuning_jobs_operator_kwargs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> move_data_file\n        >> create_tuning_job\n        # TEST BODY\n        >> get_tuning_job\n        >> list_tuning_jobs\n        >> delete_tuning_job\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2123_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data processing system that performs daily data validation and generates reports with error tracking. Create a workflow that can automatically detect data integrity issues and trigger appropriate error communication or reporting mechanisms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Print the date\"\n    print(pendulum.datetime.now())\n\n\n@task(task_id=\"log_date\")\ndef log_date():\n    \"Log the date\"\n    print(f\"logged: {XComArg(print_date(), key='date')}\")\n\n\nwith DAG(\n    dag_id=\"example_xcom_python\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n):\n    print_date()\n    log_date()"}], "metadata": {"file_name": "dag_1842_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates remote task scheduling and execution using WinRM hooks, demonstrating how to connect to and run commands on Windows machines programmatically. Develop a parallel execution pattern that shows different command types running concurrently with a final consolidation task.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.context import Context\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_winrm\"\n\nconn = Connection(\n    conn_id=\"winrm\",\n    conn_type=\"winrm\",\n    host=\"winrm.test\",\n    login=\"test\",\n    password=\"test\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"* * * * *\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def setup_winrm_connection():\n        winrm_hook = WinRMHook()\n        winrm_hook.connect(conn)\n\n    @task\n    def create_file():\n        bash_command = \"echo 'Hello World' > /tmp/world.txt\"\n        BashOperator(task_id=\"create_file\", bash_command=bash_command)\n\n    @task\n    def execute_winrm_command():\n        winrm_hook = WinRMHook()\n        winrm_hook.connect(conn)\n        winrm_hook.execute_command(\"echo 'Hello World' > /tmp/world.txt\")\n\n    @task\n    def remove_file():\n        bash_command = \"rm -f /tmp/world.txt\"\n        BashOperator(task_id=\"remove_file\", bash_command=bash_command)\n\n    @task\n    def clean_up_connection():\n        conn.delete()\n\n    setup_winrm_connection()\n    create_file()\n    execute_winrm_command()\n    remove_file()\n    clean_up_connection()"}], "metadata": {"file_name": "dag_1761_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS service integration by creating, executing, and cleaning up a DataSync task between two dynamically generated S3 buckets. Implement a complete lifecycle management workflow that demonstrates task creation, execution, and resource cleanup using Python and AWS SDK.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.datasync import DataSyncHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncStartTaskOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nsys_test_context_task = SystemTestContextBuilder().add_param(\"bucket_name\").build()\n\nDAG_ID = \"example_datasync\"\n\nDATA_SYNC_ROLE_ARN_KEY = \"DATA_SYNC_ROLE_ARN\"\n\nS3_KEY = \"datasync/sync1.csv\"\nS3_KEY_URI = f\"s3://{ENV_ID_KEY}/datasync/sync1.csv\"\n\ns3_bucket_sync1_key = \"datasync/sync1.csv\"\nS3_BUCKET_SYNC1_NAME = f\"{ENV_ID_KEY}-datasync-bucket\"\n\nLOCAL_FILE_PATH = \"/tmp/sync1.csv\"\nDATA_SYNC_CONN_ID = \"datasync_default\"\n\n# Sanitizes and makes sure the bucket exists\ndef get_s3_bucket(bucket_name):\n    boto3.client(\"s3\").create_bucket(Bucket=bucket_name)\n    return bucket_name\n\n@task\ndef create_task_mapping():\n    log.info(\"Creating task mapping...\")\n    datasync_role_arn = get_task_arn(\"datasync_role_arn\", ENV_ID_KEY)\n    source_arn = get_arn(s3_bucket_s3_bucket_s3_bucket_s3_key_s3_bucket_uri_s3_bucket_uri, DATA_SYNC_CONN_ID, \"s3\")\n    destination_arn = get_arn(\"bucket\", s3_bucket_s3_bucket_s3_key_s3_bucket_uri_s3_key, DATA_SYNC_CONN_ID, \"s3\")\n\n    hook = DataSyncHook()\n    response = hook.create_task_mapping(\n        Source=source_arn,\n        TaskMappings={\n            \"Tasks\": [\n                {\n                    \"Source\": source_arn,\n                    \"Task\": destination_arn,\n                    \"TaskParameters\": {},\n                    \"RoleArn\": datync_role_arn,\n                    \"Minative\": \"PERIODIC\",\n                    \"ScheduleDateTime\": \"2021-01-01T10:00:00Z\",\n                }\n            ]\n        },\n    )\n    return response[\"TaskMappings\"][0][\"TaskMappingId\"]\n\n\n@task\ndef get_task_arn(param_name, env_id):\n    return f\"{param_name}-{env_id}-{DATA_SYNC_ROLE_ARN_KEY}\"\n\n\n@task\ndef get_arn(bucket_name, key, conn_id, task_type):\n    return f\"arn:aws:s3:{ boto3.client(\"s3\").get_bucket_acl(Bucket=bucket_name)[\"BucketACL\"] }:{\"private\" if key.startswith(\"//\") else \"public-read\"}:{conn_id}:{task_type}:{key}\"\n\n\n@task\ndef wait_for_hook_creation(hook_id):\n    logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n    waiter = boto3.client(\"datasync\").get_waiter(\"hook_exists\")\n    log.info(\"Waiting for hook %s to exist...\", hook_id)\n    waiter.wait(HookId=hook_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_mapping(task_mapping_id):\n    log.info(\"Deleting task mapping %s...\", task_mapping_id)\n    boto3.client(\"datasync\").delete_task_mapping(Id=task_mapping_id)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=S3_BUCKET_SYNC1_NAME)\n\n    @task\n    def get_sync_hook_id(s3_bucket_name):\n        datasync_role_arn = get_task_arn(DATA_SYNC_ROLE_ARN_KEY, ENV_ID_KEY)\n        hook_name = f\"sync-hook-{ENV_ID_KEY}\"\n        log.info(\"Creating sync hook %s...\", hook_name)\n        client = boto3.client(\"datasync\")\n        response = client.create_hook(\n            RoleArn=datync_role_arn,\n            Type=\"S3ToS3\",\n            Description=hook_name,\n            S3Source={\n                \"Bucket\": s3_bucket_name,\n                \"Prefix\": s3_bucket_s3_key_s3_bucket_uri_s3_key,\n            },\n            S3Destination={\n                \"Bucket\": s3_bucket_s3_bucket_s3_key_s3_bucket_uri_s3_key,\n                \"Prefix\": s3_bucket_s3_s3_key_s3_bucket_uri_s3_key,\n            },\n            StartTaskParameters={},\n            EndTaskParameters={},\n        )\n        return response[\"Hook\"][\"Id\"]\n\n    sync_hook_id = get_sync_hook_id(s3_bucket_sync1_name)\n\n    wait_for_hook_creation(sync_hook_id)\n\n    # [START howto_operator_datasync_start_task]\n    start_task = DataSyncStartTaskOperator(\n        task_id=\"start_task\",\n        task_mapping_id=task_mapping_id,\n        source_arn=s3_bucket_s3_bucket_s3_key_s3_bucket_uri_s3_key,\n        destination_arn=f\"arn:aws:s3:{ boto3.client(\"s3\").get_bucket_acl(Bucket=s3_bucket_s3_bucket_s3_key_s3_bucket_uri_s3_key)[\"BucketACL\"] }:{DATA_SYNC_CONN_ID}:{task_type}\",\n    )\n    # [END howto_operator_datasync_start_task]\n\n    delete_task_mapping(task_mapping_id)\n\n    delete_s3_bucket = BashOperator(\n        task_id=\"delete_s3_bucket\",\n        bash_command=f\"aws s3 rm {ENV_ID_KEY} /tmp || true\",\n    )\n\n    (\n        create_s3_bucket\n        >> wait_for_hook_creation(sync_hook_id)\n        >> get_sync_hook_id(s3_bucket_sync1_name)\n        >> start_task\n        >> delete_s3_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule of \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_59_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates SQL-based data validation techniques using column and table check operators against a predefined database table. Implement sequential validation tasks that check specific data quality metrics.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_sql_template]\n# [START howto_sql_template]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"my_db\"},\n    tags=[\"example\"],\n) as dag:\n    # [END howto_sql_template]\n\n    # [END howto_operator_sql_template]"}], "metadata": {"file_name": "dag_120_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases the complete lifecycle management of containerized jobs using Google Cloud Run operators. Develop a systematic workflow that demonstrates job creation, execution, modification, and cleanup with robust error checking and cross-task dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.run_job import (\n    CloudRunOperator,\n    RunJobDeleteOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_cloud_run\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nJOB_NAME = f\"job_run_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-north1\"\n\n# [START howto_operator_cloud_run_config]\n# Example of creating a CloudRun job configuration\nCREATE_JOB_REQUEST = {\n    \"image\": \"gcr.io/testprojectss/hello_world:latest\",\n    \"entrypoint\": \"hello_world\",\n    \"env\": [{\"name\": \"N\", \"value\": \"10\"}],\n    \"labels\": {\"sample\": \"test\"},\n    \"port\": 8080,\n}\n# [END howto_operator_cloud_run_config]\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"email\": [\"\"],\n    },\n    on_finish=lambda context: context.obj.get(\"run_run_job\") or context.obj.get(\"gcs_create_bucket\"),\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"task_create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_run_job]\n    run_job = CloudRunOperator(\n        task_id=\"run_job\",\n        project_id=PROJECT_ID,\n        image=\"{{ task_instance.xcom_pull('task_create_bucket', key='return_value') }}\",\n        create_job_request_create=CREATE_JOB_REQUEST,\n        location=LOCATION,\n        name=JOB_NAME,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_run_job]\n\n    delete_run_job = RunJobDeleteOperator(\n        task_id=\"delete_run_job\",\n        job_name=JOB_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> run_job\n        # TEST TEARDOWN\n        >> delete_run_job\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as failed / succeeded\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_2003_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow monitoring DAG that validates the completion status of a specific cloud task queue in a Google Cloud environment. Implement a sensor task that pauses workflow execution until the designated queue is completely empty.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataflow import DataflowJobQueueSensor\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_gcs_to_dataflow\"\n\nRESOURCES_BUCKET_NAME = f\"airflow-system-tests-resources-{DAG_ID}\"\nLOCATION = \"europe-west1\"\nWORKFLOW_NAME = \"gcs-to-dataflow\"\nDATAFLOW_CONF = {\n    \"job_name\": WORKFLOW_NAME,\n    \"dataflow_options\": {\n        \"project\": PROJECT_ID,\n        \"temp_location\": f\"gs://{RESOURCES_BUCKET_NAME}/dataflow/temp\",\n        \"pip_packages\": [],\n        \"container_spec\": {\n            \"image\": \"gcr.io/cloud-builders/python\",\n            \"entrypoint\": \"/bin/sh\",\n            \"args\": [\"-c\", \"echo Hello, world!\"],\n        },\n    },\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=RESOURCES_BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"dataflow_default\",\n        )\n        conn_extra = {\n            \"gcp_default\": {\n                \"project\": PROJECT_ID,\n            }\n        }\n        conn_extra_json = json.dumps(conn_extra)\n        bucket = GCSToGCSOperator(\n            task_id=\"create_connection_from_gcs\",\n            source_bucket=RESOURCES_BUCKET_NAME,\n            source_object=\"dataflow/default.json\",\n            destination_bucket=\"airflow-system-tests-resources-dataflow-default\",\n            destination_object=\"dataflow/default.json\",\n            extra_files=conn_extra_json,\n        )\n\n    create_connection = create_connection()\n\n    copy_workflow_source = GCSToGCSOperator(\n        task_id=\"copy_workflow_source\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=\"dataflow/templates/\" + WORKFLOW_NAME + \".json\",\n        destination_bucket=\"airflow-system-tests-resources-dataflow-workflow\",\n        destination_object=\"workflow_source.json\",\n    )\n\n    # [START howto_sensor_dataflow_job_queue]\n    check_dataflow_job_queue = DataflowJobQueueSensor(\n        task_id=\"check_dataflow_job_queue\",\n        job_name=WORKFLOW_NAME,  # This can be the full name or the ID.\n        deferrable=False,\n    )\n    # [END howto_sensor_dataflow_job_queue]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=RESOURCES_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_connection\n        >> copy_workflow_source\n        # TEST BODY\n        >> check_dataflow_job_queue\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as done\n    list(watcher(dag))"}], "metadata": {"file_name": "dag_699_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS Glue DataBrew data profiling workflow that demonstrates creating, executing, and cleaning up a data processing job using Airflow. Build a DAG that showcases end-to-end data preparation and profiling with automated resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, dag\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import weekday\nfrom airflow.utils.trigger_rule import WeekDayTriggerRule\n\n# [START howto_operator_glue_databrew]\n# [START howto_sensor_glue_databrew]\nfrom airflow.providers.amazon.aws.sensors.glue_databrew import GlueDatabrewJobCompleteSensor\n\n# [END howto_sensor_glue_databrew]\n# [END howto_operator_glue_databrew]\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_role]\nfrom airflow.models.role import Role\n# [END howto_role]\n# [START howto_connection]\nfrom airflow.models.connection import Connection\n# [END howto_connection]\n# [START example_extrack]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END example_extrack]\n# [START_example_get_connection]\nfrom airflow.models.connection import Connection\n# [END_example_get_connection]\n# [START_example_set_connection]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.utils.trigger_rule import TriggerRule\n# [END_example_set_connection]\n# [START_example_delete_connection]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.utils.trigger_rule import TriggerRule\n# [END_example_delete_connection]\n# [START_example_add_extra_attribute]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.models.connection import Connection\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils import timezone\nfrom airflow import settings\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.timezone import datetime\n# [END_example_add_extra_attribute]\n# [START example_get_role]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.utils.trigger_rule import TriggerRule\n# [END_example_get_role]\n# [START example_set_role]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils import timezone\nfrom airflow import settings\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.timezone import datetime\n# [END_example_set_role]\n# [START example_delete_role]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils import timezone\nfrom airflow import settings\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.timezone import datetime\n# [END_example_delete_role]\n# [START example_vault_get_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_get_secret]\n# [START example_vault_put_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_put_secret]\n# [START example_vault_del_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_del_secret]\n# [START example_vault_list_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_list_secret]\n# [START example_vault_acct_get_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_acct_get_secret]\n# [START example_vault_acct_put_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_acct_put_secret]\n# [START example_vault_acct_del_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_acct_del_secret]\n# [START example_vault_acct_list_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_acct_list_secret]\n# [START example_vault_secrets_manager_get_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_secrets_manager_get_secret]\n# [START example_vault_secrets_manager_put_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_secrets_manager_put_secret]\n# [START example_vault_secrets_manager_del_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_secrets_manager_del_secret]\n# [START example_vault_secrets_manager_list_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_secrets_manager_list_secret]\n# [START example_vault_s3_get_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_get_secret]\n# [START example_vault_s3_put_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_put_secret]\n# [START example_vault_s3_del_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_del_secret]\n# [START example_vault_s3_list_secret]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_list_secret]\n# [START example_vault_s3_put_record]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_put_record]\n# [START example_vault_s3_get_record]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_get_record]\n# [START example_vault_s3_del_record]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_del_record]\n# [START example_vault_s3_list_record]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_list_record]\n# [START example_vault_s3_get_part]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_get_part]\n# [START example_vault_s3_del_part]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_del_part]\n# [START example_vault_s3_list_part]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_list_part]\n# [START example_vault_s3_put_record_batch]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_put_record_batch]\n# [START example_vault_s3_get_record_batch]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_get_record_batch]\n# [START example_vault_s3_del_record_batch]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_del_record_batch]\n# [START example_vault_s3_list_record_batch]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_s3_list_record_batch]\n# [START example_vault_secrets_manager_put_record_batch]\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.api import ApiClient\nfrom airflow.models.api import request\n# [END_example_vault_secrets_manager_put_record_batch]\n# [END howto_operator_glue_databrew]\n\n\n# [START howto_trigger_glue_databrew]\nfrom airflow.providers.amazon.aws.triggers.glue_databrew import GlueDatabrewJobCompleteTrigger\n\n# [END howto_trigger_glue_databrew]\n# [START howto_taskinst_glue_databrew]\nfrom airflow.models.taskinstance import TaskInstance\n\n# [END"}], "metadata": {"file_name": "dag_1904_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates a complete cloud data transfer process, including bucket creation, data extraction, and post-transfer cleanup with error handling and observability.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.transfers.copy_into_s3 import S3ToSqlCopyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_sql_copy\"\n\nS3_BUCKET = \"bucket_name\"\nS3_KEY = \"file_name\"\nS3_KEY_DEF = f\"{S3_BUCKET}/{S3_KEY}\"\nS3_KEY_RES = \"s3://bucket_name/file_name\"\nS3_CONN = \"s3_default\"\nSQL_TABLE = \"airflow_test_table\"\nSQL_CONN = \"postgres_default\"\n\nconn = Connection(\n    conn_id=S3_CONN,\n    conn_type=\"s3\",\n    extra={\"path\": S3_KEY_RES},\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_sql_table():\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=SQL_CONN)\n        sql_hook.create_table(\n            sql_table,\n            \"airflow_test_table\",\n            [\"id serial, value varchar(255)\"],\n        )\n\n    @task\n    def insert_data():\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=SQL_CONN)\n        rows = [(\"test1\",), (\"test2\",), (\"test3\",)]\n        sql_hook.insert_rows(sql_table, rows)\n\n    @task\n    def get_data():\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=SQL_CONN)\n        rows = sql_hook.get_rows(sql=f\"select * from {SQL_TABLE}\")\n\n        # Airflow returns rows as a list of dictionaries. Convert to a list of tuples.\n        rows = [tuple(row) for row in rows]\n\n        # Airflow returns 'rows' and 'columns' as lists. Return 'columns' as tuple.\n        columns = rows[0]\n        rows = columns[1:]  # Skip first column ('id')\n\n        return rows\n\n    @task\n    def drop_table():\n        from airflow.providers.common.sql.hooks.sql import SQLHook\n\n        sql_hook = SQLHook(sql_conn_id=SQL_CONN)\n        sql_hook.drop_table(sql_table)\n\n    create_sql_table_task = create_sql_table()\n    insert_data_task = insert_data()\n    get_data_task = get_data()\n    drop_table_task = drop_table()\n    (create_sql_table_task >> insert_data_task >> get_data_task >> drop_table_task)\n\n    # [START howto_operator_s3_to_sql_copy]\n    s3_to_sql_copy = S3ToSqlCopyOperator(\n        task_id=\"s3_to_sql_copy\",\n        sql_table=SQL_TABLE,\n        s3_conn_id=S3_CONN,\n        s3_key=S3_KEY_DEF,\n        replace=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_s3_to_sql_copy]"}], "metadata": {"file_name": "dag_1658_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can conditionally trigger downstream tasks. Create a DAG that illustrates waiting mechanisms, file checks, time-based conditions, and different sensor behaviors.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.filesystem.operators.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.providers.common.filesystem.operators.s3_to_local import S3ToLocalOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_connection(conn_id_name: str):\n        session = Session()\n        session.add(Connection(id=conn_id_name, conn_id=conn_id_name, conn_type=\"s3\"))\n        session.commit()\n\n    create_s3_to_local_connection = create_connection(conn_id_name=\"s3_to_local_conn\")\n\n    @task\n    def create_local_to_s3_connection(conn_id_name: str):\n        session = Session()\n        session.add(Connection(id=conn_id_name, conn_id=conn_id_name, conn_type=\"s3\"))\n        session.commit()\n\n    create_local_to_s3_connection = create_local_to_s3_connection()\n\n    @task\n    def create_sensor(conn_id_name: str):\n        session = Session()\n        session.add(\n            Connection(\n                id=conn_id_name,\n                conn_id=conn_id_name,\n                conn_type=\"s3\",\n            )\n        )\n        session.add(\n            S3ToLocalOperator(task_id=\"local_to_s3_task\", conn_id=conn_id_name, filename=\"test.txt\")\n        )\n        session.commit()\n\n    create_sensor = create_sensor(conn_id_name=\"s3_to_local_conn\")\n\n    @task\n    def remove_sensor(conn_id_name: str):\n        session = Session()\n        session.query(S3ToLocalOperator).filter(S3ToLocalOperator.task_id == \"local_to_s3_task\").delete()\n        session.query(Connection).filter(Connection.conn_id == conn_id_name).delete()\n        session.commit()\n\n    remove_sensor = remove_sensor(conn_id_name=\"s3_to_local_conn\")\n\n    # [START howto_operator_local_to_s3]\n    create_local_to_s3_connection >> local_to_s3_task\n    # [END howto_operator_local_to_s3]\n\n    # [START howto_operator_s3_to_local]\n    s3_to_local_task >> remove_sensor\n    # [END howto_operator_s3_to_local]\n\n    (\n        create_s3_to_local_connection\n        >> create_local_to_s3_connection\n        >> local_to_s3_task\n        >> s3_to_local_task\n        >> remove_sensor\n    )"}], "metadata": {"file_name": "dag_344_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates basic SQL query execution against an Airflow metadata database, showing how to connect to a Postgres database and run multiple SQL statements. Create a workflow that showcases the SQLExecuteQueryOperator's capabilities for executing database queries.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_sql_execute_query_howto]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_execute_query_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    default_args={\"conn_id\": \"conn_id_sqlite\"},\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_sql_execute_query]\n    execute_query = SQLExecuteQueryOperator(\n        task_id=\"create_table_and_insert_rows\",\n        sql=\"CREATE TABLE my_table (value INTEGER); CREATE OR REPLACE TABLE temp_table AS SELECT * FROM employees;\",\n    )\n    # [END howto_operator_sql_execute_query]\n\n    execute_query_2 = SQLExecuteQueryOperator(\n        task_id=\"insert_statement\",\n        sql=\"INSERT INTO my_table VALUES (5), (6);\",\n    )\n\n    execute_query_3 = SQLExecuteQueryOperator(\n        task_id=\"statement_with_params\",\n        sql=\"SELECT * FROM employees WHERE id = {{ params.id }}\",\n        params={\"id\": 6},\n    )\n\n    execute_query_4 = SQLExecuteQueryOperator(\n        task_id=\"statement_with_params_and_commit\",\n        sql=\"SELECT * FROM employees WHERE id = {{ params.id }}\",\n        params={\"id\": 6},\n        trigger_rule=\"all_done\",\n    )\n\n    execute_query\n    execute_query_2\n    execute_query_3\n    execute_query_4"}], "metadata": {"file_name": "dag_1158_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Google Dataproc workflow template for running a Pig job on a managed Hadoop cluster. The workflow should showcase asynchronous job instantiation and template management in a cloud data processing environment.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocInstantiateWorkflowTemplateOperator,\n    DataprocRunJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"google_cloud_dataproc_pig\"\nPROJECT_ID = models.Environment.get(\"PROJECT_ID\")\n\nREGION = \"us-central1\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\"\nWORKFLOW_NAME = \"pig-workflow\"\nWORKFLOW_TEMPLATE = {\n    \"id\": WORKFLOW_NAME,\n    \"placement\": {\n        \"managed_cluster\": {\n            \"cluster_name\": CLUSTER_NAME,\n            \"config\": {\n                \"master_config\": {\n                    \"num_instances\": 1,\n                    \"machine_type_uri\": \"n1-standard-1\",\n                    \"disk\": {\n                        \"size_gb\": 1024,\n                        \"boot_disk_type\": \"PD_SSD\",\n                        \"boot_disk_config\": {\"source_image_uri\": \"projects/debian-cloud/global/ disks/debian-11-bullseye-v20220621-gce-disk\"},\n                    },\n                },\n                \"worker_config\": {\n                    \"num_instances\": 2,\n                    \"machine_type_uri\": \"n1-standard-1\",\n                    \"disk\": {\n                        \"size_gb\": 1024,\n                        \"boot_disk_type\": \"PD_SSD\",\n                        \"boot_disk_config\": {\n                            \"source_image_uri\": \"projects/debian-cloud/global/ disks/debian-11-bullseye-v20220621-gce-disk\",\n                        },\n                    },\n                },\n            },\n        }\n    },\n    \"region\": REGION,\n    \"job_list\": [{\"handler\": {\"source_archive_file\": \"gs://airflow-system-tests-resources/dataproc/pig/hello-pig.tar.gz\"}, \"args\": {\"pig\":\"ls -l\"}}],\n}\n\nCONTAINER_URI = \"gcr.io/google-cloud-sdk/compositions/dataproc-3.7.0\"\nWORKFLOW_RUN_CONFIG = {\n    \"placement\": {\n        \"managed_cluster\": {\"cluster_name\": CLUSTER_NAME},\n    },\n    \"region\": REGION,\n    \"job\": {\"handler\": {\"image_uri\": CONTAINER_URI, \"args\": [\"pig\", \"ls -l\"]}}},\n}\n\n\nwith models.DAG(\n    project_id=PROJECT_ID,\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"dataproc\", \"pig\"],\n    id=DAG_ID,\n    catchup=False,\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config={\n            \"master_config\": {\n                \"num_instances\": 1,\n                \"machine_type_uri\": \"n1-standard-1\",\n                \"disk\": {\n                    \"size_gb\": 1024,\n                    \"boot_disk_type\": \"PD_SSD\",\n                    \"boot_disk_config\": {\n                        \"source_image_uri\": \"projects/debian-cloud/global/ disks/debian-11-bullseye-v20220621-gce-disk\",\n                    },\n                },\n            },\n            \"worker_config\": {\n                \"num_instances\": 2,\n                \"machine_type_uri\": \"n1-standard-1\",\n                \"disk\": {\n                    \"size_gb\": 1024,\n                    \"boot_disk_type\": \"PD_SSD\",\n                    \"boot_disk_config\": {\n                        \"source_image_uri\": \"projects/debian-cloud/global/ disks/debian-11-bullseye-v20220621-gce-disk\",\n                    },\n                },\n            },\n            \"config\": {\n                \"metastore_service_account\": \"\",\n                \"gce_cluster_config\": {\n                    \"zone\": \"us-central1-c\",\n                },\n            },\n        },\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START how_to_dataproc_instantiate_workflow_template]\n    instantiate_workflow_template = DataprocInstantiateWorkflowTemplateOperator(\n        task_id=\"instantiate_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        template=WORKFLOW_TEMPLATE,\n    )\n    # [END how_to_dataproc_instantiate_workflow_template]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> instantiate_workflow_template\n        # TEST TEARDOWN\n        >> delete_cluster\n    )"}], "metadata": {"file_name": "dag_195_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning lifecycle for a predictive model, demonstrating how to manage cloud resources, transfer data, create datasets, and handle model training and deployment. Showcase how to integrate multiple cloud services and operators to create a robust, scalable machine learning infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.translate import TranslateCreateDatasetOperator, TranslateImportDataOperator\nfrom airflow.providers.amazon.aws.sensors.translate import TranslateSensor\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Monday\nfrom airflow.utils.trigger_rule import WeekDayTriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_translate\"\n\n# Externally fetched variables:\nS3_BUCKET_NAME = \"bucket_name\"\nS3_OBJECT_KEY = \"bucket_key\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(S3_BUCKET_NAME).add_variable(S3_OBJECT_KEY).build()\n\ndataset_name = f\"{DAG_ID}-dataset\"\ndataset = {\n    \"Name\": dataset_name,\n    \"S3DataSource\": {\n        \"S3DataDistributionType\": \"Balance\",\n        \"S3Uri\": {\"URI\": f\"s3://{S3_BUCKET_NAME}/{S3_OBJECT_KEY}\"},\n    },\n}\ninput_data_config = [\n    {\n        \"MultilinguisticIdfWeight\": 0.0,\n        \"S3Uri\": {\"URI\": f\"s3://{S3_BUCKET_NAME}/{S3_OBJECT_KEY}\"},\n        \"LanguageCode\": \"en\",\n        \"Delimiter\": \",\",\n        \"IgnoreFirstRecord\": True,\n        \"RecordHeader\": {\"Include\": \"NONE\"},\n        \"NumberOfLinesToSkip\": 0,\n        \"NumberOfSamplesPerSourceLanguage\": -1,\n    }\n]\noutput_data_config = {\n    \"S3Uri\": {\"URI\": f\"s3://{S3_BUCKET_NAME}/output/\"},\n    \"IgnoreFirstRecord\": True,\n    \"RecordHeader\": {\"Include\": \"NONE\"},\n    \"NumberOfLinesToSkip\": 0,\n    \"NumberOfSamplesPerSourceLanguage\": -1,\n}\n\n\n@task\ndef create_connection(conn_id_name: str, sql_conn_id: str):\n    connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n        description=\"S3 connection\",\n        db_spec=\"',\n            '\",\n        extra=json.dumps(\n            {\n                \"aws_access_key_id\": \"REMOVED\",\n                \"aws_secret_access_key\": \"REMOVED\",\n                \"bucket\": S3_BUCKET_NAME\n            }\n        ),\n    )\n    hook = S3Hook()\n    hook.create_connection(connection)\n    log.info(\"Created connection: %s\", connection)\n\n    # Use the SQL connection to get the same result\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    log.info(\"Sql connection id: %s\", sql_conn.id)\n    log.info(\"Sql connection conn_type: %s\", sql_conn.conn_type)\n    log.info(\"Sql connection description: %s\", sql_conn.description)\n    log.info(\"Sql conn_spec: %s\", sql_conn.db_spec)\n    log.info(\"Sql connection extra: %s\", sql_conn.extra)\n\n\n@task\ndef create_dataset_table(dataset_name: str, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    sql_conn.execute(f\"CREATE TABLE IF NOT EXISTS {dataset_name} (col1 VARCHAR(10), col2 VARCHAR(10))\")\n\n\n@task\ndef set_up_import_output_table(dataset_name: str, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    sql_conn.execute(f\"CREATE TABLE IF NOT EXISTS {dataset_name}_output (col1 VARCHAR(10), col2 VARCHAR(10))\")\n\n\n@task\ndef insert_data(target_table: str, input_data_config: list, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    sql_conn.execute(f\"INSERT INTO {target_table} VALUES ( 'Python', 'testing' ), ( 'Python2', 'testing2' )\")\n\n\n@task\ndef check_data(target_table: str, input_data_config: list, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    for i, input_data in enumerate(input_data_config):\n        sql_conn.execute(f\"SELECT count(*) FROM {target_table} WHERE col1 = '{input_data['S3Uri']['URI']}'{'+' + str(i) if i else ''}\")\n\n\n@task\ndef drop_table(table_name: str, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    sql_conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n\n\n@task\ndef drop_dataset(dataset_name: str, sql_conn_id: str):\n    sql_conn = hook.get_sql_conn(sql_conn_id)\n    sql_conn.execute(f\"DROP DATASET IF EXISTS {dataset_name}\")\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    s3_bucket_name = test_context[S3_BUCKET_NAME]\n    s3_object_key = test_context[S3_OBJECT_KEY]\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket_name)\n\n    @task\n    def wait_until_dataset_created(s3_bucket_name: str, dataset_name: str):\n        translate_client = boto3.client(\"translate\")\n        response = translate_client.describe_dataset(\n            DatasetName=dataset_name,\n        )\n        wait_for_completion = response[\"Dataset\"][\"State\"] == \"INACTIVE\"\n        log.info(\"Dataset is ready: %s\", wait_for_completion)\n        if wait_for_completion:\n            seconds_sleep = 10\n            for _ in range(100):\n                response = translate_client.describe_dataset(\n                    DatasetName=dataset_name,\n                )\n                if response[\"Dataset\"][\"State\"] == \"ACTIVE\":\n                    log.info(\"Dataset is ready\")\n                    break\n                else:\n                    log.info(\"Waiting for dataset to become ready...\")\n                    log.info(\"Sleeping for %s seconds\", seconds_sleep)\n                    time.sleep(seconds_sleep)\n\n    wait_until_dataset_created(task_id=\"wait_until_dataset_created\", s3_bucket_name=s3_bucket_name,\n                                dataset_name=dataset_name)\n\n    create_dataset = TranslateCreateDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_name=dataset_name,\n        input_data_config=input_data_config,\n        output_data_config=output_data_config,\n    )\n\n    import_dataset = TranslateImportDataOperator(\n        task_id=\"import_data\",\n        dataset_name=dataset_name,\n        input_data_config=input_data_config,\n    )\n\n    # [START howto_sensor_translate]\n    check_data_present = TranslateSensor(\n        task_id=\"check_data_present\",\n        dataset_name=dataset_name,\n        input_data_config=input_data_config,\n    )\n    # [END howto_sensor_translate]\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        sql=f\"INSERT INTO {dataset_name}_output VALUES ( 'Python', 'testing' ), ( 'Python2', 'testing2' )\",\n        conn_id=sql_conn_id,\n    )\n\n    # [START howto_operator_translate]\n    analyze_data = SQLExecuteQueryOperator(\n        task_id=\"analyze_data\",\n        sql=f\"ANALYZE TABLE {dataset_name} DISTRIBUTED BY (col1) PROPORTIONAL;\",\n        conn_id=sql_conn_id,\n    )\n    # [END howto_operator_translate]\n\n    drop_dataset(dataset_name, sql_conn_id)\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> wait_until_dataset_created(s3_bucket_name, dataset_name)\n        >> create_dataset\n        >> import_dataset\n        # TEST BODY\n        >> check_data(target_table=dataset_name, input_data_config=input_data_config, sql_conn_id=sql_conn_id)\n        >> insert_data\n        # TEST TEARDOWN\n        >> drop_dataset(dataset_name, sql_conn_id)\n        >> delete_s3_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the temp setup\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete data pipeline from NoSQL database export to cloud storage staging and then into a queryable data warehouse table. Develop a systematic workflow that shows how different cloud services can be integrated for seamless data processing.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.alibaba.cloud.operators.nosql import AmazonSQSQueueMoveOperator\nfrom airflow.providers.alibaba.cloud.operators.stargate import StargateSQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_nosql_to_stargate\"\n\nsqs_from_endpoint = \"sqs-from-endpoint\"\nsqs_to_endpoint = \"sqs-to-endpoint\"\nstargate_table_name = \"stargate_table\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_amazon_sqs_move_queue_to_endpoint]\n    move_queue_to_endpoint = AmazonSQSQueueMoveOperator(\n        task_id=\"move_queue_to_endpoint\",\n        from_queue_name=sqs_from_endpoint,\n        to_queue_name=sqs_to_endpoint,\n        trigger_template_name=\"stargate_create_template\",\n        stargate_table_name=stargate_table_name,\n        end_date=datetime.now().strftime(\"%Y-%m-%d\"),\n    )\n    # [END howto_operator_amazon_sqs_move_queue_to_endpoint]\n\n    # [START howto_operator_stargate_sql_execute_query]\n    execute_query = StargateSQLExecuteQueryOperator(\n        task_id=\"execute_query\",\n        sql=\"SELECT * from stargate_table\",\n    )\n    # [END howto_operator_stargate_sql_execute_query]\n\n    (move_queue_to_endpoint >> execute_query)\n\n    execute_query.trigger_rule = TriggerRule.ALL_DONE"}], "metadata": {"file_name": "dag_535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data workflow that uses a sensor task to simulate checking for data availability or external system readiness before triggering subsequent processing steps. Create a DAG that demonstrates how to conditionally control task execution using Airflow's sensor decorator.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"check_sensor_task\")\ndef check_sensor():\n    print(\"Hello World from check_sensor_task!\")\n\n\ndef setup_env(dag):\n    # [START create_default_dag]\n    default_args = {\n        \"email\": \"email@example.com\",\n        \"retries\": 1,\n        \"start_date\": pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n        \"max_active_runs\": 1,\n    }\n\n    default_location = \"us\"\n    default_sql_conn_id = \"postgres_default\"\n    default_formated_date = \"2020-01-01\"\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = \"one\"\n    default_timeout = 1\n    default_check_interval = 1\n\n    default_sensor_extra = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\"\n    default_retries_variable = 1\n    default_max_active_runs_variable = 1\n    default_formated_date_variable = \"2020-01-01\"\n    default_sensor_mode_variable = \"one\"\n    default_timeout_variable = 1\n    default_check_interval_variable = 1\n    default_sensor_extra_variable = {\n        \"extra\": \"whatever you want\"\n    }\n\n    default_args = {\n        \"email\": default_email_variable,\n        \"retries\": default_retries_variable,\n        \"start_date\": pendulum.datetime(default_formated_date_variable, tz=\"UTC\"),\n        \"max_active_runs\": default_max_active_runs_variable,\n    }\n\n    default_location = default_location_variable\n    default_sql_conn_id = default_sql_variable\n    default_formated_date = default_formated_date_variable\n\n    default_params = {\n        \"sensor_job_start_date\": default_formated_date,\n        \"sql_conn_id\": default_sql_conn_id,\n        \"location\": default_location,\n    }\n\n    default_sensor_mode = default_sensor_mode_variable\n    default_timeout = default_timeout_variable\n    default_check_interval = default_check_interval_variable\n\n    default_sensor_extra = {\n        \"extra\": default_sensor_extra_variable[\"extra\"],\n    }\n\n    default_location_variable = \"location\"\n    default_date_variable = \"2020-01-01\"\n    default_sql_variable = \"postgres_default\"\n    default_email_variable = \"email@example.com\""}], "metadata": {"file_name": "dag_1810_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you explain the relationship between `ExponentialBackoff` in Python and its use with `urllib3` for handling HTTP requests?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.backoff import ExponentialBackoff\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_urllib3_backoff\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo run me\")\n\n    @task\n    def get_tesla_info():\n        import urllib3\n\n        # Create a URL object\n        http = urllib3.PoolManager()\n        response = http.request(\"GET\", \"https://www.exampleapi.com/tesla\")\n\n        # Parse the response body into a json\n        tesla_info = json.loads(response.data.decode())\n\n        # Return the parsed json\n        return tesla_info\n\n    # [START howto_operator_urllib3_backoff]\n    get_tesla_info_backoff = ExponentialBackoff()\n    get_tesla_info_task = BashOperator(\n        task_id=\"get_tesla_info_backoff\",\n        bash_command=\"get_tesla_info | tr '}' '\\n' | while read line; do echo $line; sleep $get_tesla_info_backoff.next_backoff; done\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_urllib3_backoff]\n\n    run_this >> get_tesla_info() >> get_tesla_info_backoff\n\n    get_tesla_info_conn = Connection(\n        conn_id=\"tesla_info_conn\",\n        conn_type=\"bash\",\n        host=\"https://www.exampleapi.com/\",\n        extra={\"path\": \"/tesla\"},\n    )\n    get_tesla_info_conn.build()\n    get_tesla_info_conn.apply()\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as done\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an image data processing pipeline that transfers image files between cloud storage buckets and prepares them for machine learning-based product recognition. Create a reproducible workflow that demonstrates how to programmatically manage image datasets and product metadata using cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImagePropertiesOperator,\n    CloudVisionExtrapolate3dPointsOperator,\n    CloudVisionExtrapolate3dPointsImageOperator,\n    CloudVisionImageAnnotateOperator,\n    CloudVisionImageDetectOperator,\n    CloudVisionImageDetectProductSetsOperator,\n    CloudVisionImageProductSetCreateOperator,\n    CloudVisionImageProductSetDeleteOperator,\n    CloudVisionProductSetListOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"cloud_vision_annotate\"\n\nBUCKET_NAME = f\"bucket_vision_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\nGCP_VISION_PRODUCT_SET_ID = f\"product_set_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_vision_annotate_images_gcp_notebook]\ndef annotate_image_func(file, target_dir):\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucket, GCSDeleteBucket\n\n    bucket_created = False\n    for _ in range(3):\n        try:\n            gcs_bucket = GCSCreateBucket()\n            target_dir +=\"/tmp/\"\n            bucket_created = True\n            break\n        except Exception as exc:\n            print(\"Notice: There was an error creating the bucket, retrying...\")\n            print(exc)\n    if not bucket_created:\n        return\n    destination_bucket_path = f\"gs://{gcs_bucket.id}/{target_dir}\"\n    file_local_path = f\"{target_dir}/{file}\"\n    S3ToGCSOperator(\n        task_id=\"s3_to_gcs\",\n        bucket=name_s3,\n        dest_bucket=destination_bucket_path,\n        key_file='security/s3.json',\n        dest_file=file_local_path,\n    )\n    # [START howto_operator_vision_annotate_images]\n    vision = cloud_vision.ImageAnnotateRequest(\n        image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=file_local_path)),\n        annotation_requests=[\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=file_local_path)),\n                label_annotations=[],\n                product_set_id=target_product_set_id],\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=destination_bucket_path + '/' + file_local_path)),\n                label_annotations=[],\n                product_set_id=target_product_set_id],\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=file_local_path)),\n                label_annotations=[],\n                product_set_id=target_product_set_id),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=destination_bucket_path + '/' + file_local_path)),\n                label_annotations=[],\n                product_set_id=target_product_set_id),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=destination_bucket_path + '/' + file_local_path)),\n                label_annotations=[],\n                product_set_id=target_product_set_id),\n        ],\n    )\n    # [END howto_operator_vision_annotate_images]\n    # [START howto_operator_vision_annotate_images_result]\n    annotate_image_output = (vision)\n    # [END howto_operator_vision_annotate_images_result]\n    # [START howto_operator_vision_extrapolate_3d_points]\n    extrapolate_3d_points_request = cloud_vision.Extrapolate3dPointsImageRequest()\n    extrapolate_3d_points_request.image = cloud_vision.Extrapolate3dPointsImageSource(\n        uri=destination_bucket_path + '/' + file_local_path\n    )\n    extrapolate_3d_points_response = cloud_vision.Extrapolate3dPointsImageRequest(extrapolate_3d_points_request)\n    # [END howto_operator_vision_extrapolate_3d_points]\n    # [START howto_operator_vision_extrapolate_3d_points_result]\n    extrapolate_3d_points_result = extrapolate_3d_points_response\n    # [END howto_operator_vision_extrapolate_3d_points_result]\n    return annotate_image_output, extrapolate_3d_points_response, extrapolate_3d_points_result\n# [END howto_operator_vision_annotate_images_gcp_notebook]\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_product_set = CloudVisionImageDetectProductSetsOperator(\n        task_id=\"create_product_set\",\n        gcp_conn_id=\"google_cloud_platform\",\n        product_id=\"test-product\",\n        image=cloud_vision.ImageUri(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\"),\n        next_token=\"next_to_token\",\n    )\n    # [START howto_operator_vision_image_detect]\n    image_detect_task = cloud_vision.ImageDetectOperator(\n        task_id=\"image-detect\",\n        gcp_conn_id=\"google_cloud_platform\",\n        image=cloud_vision.ImageUri(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\"),\n    )\n    # [END howto_operator_vision_image_detect]\n    # [START howto_operator_vision_image_detect_result]\n    image_detect_result = gapic_vision_image_detect_result.output\n    # [END howto_operator_vision_image_detect_result]\n    # [START howto_operator_vision_annotate_image]\n    annotate_image_task = cloud_vision.ImageAnnotateImageOperator(\n        task_id=\"annotate_image\",\n        gcp_conn_id=\"google_cloud_platform\",\n        image=cloud_vision.ImageAnnotateRequest(image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")), annotation_requests=[\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")),\n                label_annotations=[],\n                product_set_id=\"test-product\"),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")),\n                label_annotations=[],\n                product_set_id=\"test-product\"),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageGcsSource(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")),\n                label_annotations=[],\n                product_set_id=\"test-product\"),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")),\n                label_annotations=[],\n                product_set_id=\"test-product\"),\n            cloud_vision.AnnotateImageRequest.ImageAnnotationRequest(\n                image=cloud_vision.Image(source=cloud_vision.ImageUri(uri=\"gs://airflow-system-tests-resources/vision/image1.jpg\")),\n                label_annotations=[],\n                product_set_id=\"test-product\"),\n        ]),\n    # [END howto_operator_vision_annotate_image]\n    # [START howto_operator_vision_extrapolate_3d_points]\n    extrapolate_3d_points_task = cloud_vision.Extrapolate3dPointsImageOperator(\n        task_id=\"extrapolate_3d_points\",\n        gcp_conn_id=\"google_cloud_platform\",\n        image=extrapolate_3d_points_request.image,\n        next_token=\"next_to_token\",\n    )\n    # [END howto_operator_vision_extrapolate_3d_points]\n    # [START howto_operator_vision_extrapolate_3d_points_result]\n    extrapolate_3d_points_task_result = gapic_vision_image_detect_result.output\n    # [END howto_operator_vision_extrapolate_3d_points_result]\n    delete_product_set = CloudVisionImageProductSetDeleteOperator(\n        task_id=\"delete_product_set\",\n        product_id=\"test-product\",\n        gcp_conn_id=\"google_cloud_platform\",\n    )\n    delete_product_set.trigger_rule = TriggerRule.ALL_DONE\n    # [START howto_operator_vision_extrapolate_3d_points_result]\n    extrpolate_3d_points_operator = CloudVisionExtrapolate3dPointsImageOperator(\n        task_id=\"extrpolate_3d_points\",\n        image=extrapolate_3d_points_request.image,\n        next_token=\"next_to_token\",\n    )\n    extrpolate_3d_points_operator.output_path = \"output_gcs\"\n    # [END howto_operator_vision_extrapolate_3d_points_result]\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n    )\n    delete_bucket.trigger_rule = TriggerRule.ALL_DONE\n\n    (create_product_set >> create_bucket"}], "metadata": {"file_name": "dag_598_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates task dependencies and lifecycle management, using nested task groups and explicit setup/teardown decorators to show how complex workflows can be structured with clear initialization and finalization steps. Develop a demonstration that highlights Airflow's ability to manage task relationships and automatic resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Prints the date\"\n    print(pendulum.datetime())\n\n\n@task(task_id=\"print_time\")\ndef print_time():\n    \"Prints the current time\"\n    print(pendulum.time())\n\n\n@task(task_id=\"print_day_of_week\")\ndef print_day_of_week():\n    \"Prints the current day of the week\"\n    print(pendulum.day_of_week)\n\n\n@task(task_id=\"print_month\")\ndef print_month():\n    \"Prints the current month\"\n    print(pendulum.month)\n\n\n@task(task_id=\"print_quarter\")\ndef print_quarter():\n    \"Prints the current quarter\"\n    print(pendulum.quarter)\n\n\n@task(task_id=\"print_year\")\ndef print_year():\n    \"Prints the current year\"\n    print(pendulum.year)\n\n\n@task(task_id=\"print_day_of_year\")\ndef print_day_of_year():\n    \"Prints the current day of the year\"\n    print(pendulum.day_of_year)\n\n\n@task(task_id=\"print_week_of_year\")\ndef print_week_of_year():\n    \"Prints the current week of the year\"\n    print(pendulum.week_of_year)\n\n\n@task(task_id=\"print_week_of_month\")\ndef print_week_of_month():\n    \"Prints the current week of the month\"\n    print(pendulum.week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_quarter\")\ndef print_day_of_quarter():\n    \"Prints the current day of the quarter\"\n    print(pendulum.day_of_quarter)\n\n\n@task(task_id=\"print_day_of_month\")\ndef print_day_of_month():\n    \"Prints the current day of the month\"\n    print(pendulum.day_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_hour\")\ndef print_hour():\n    \"Prints the current hour\"\n    print(pendulum.hour)\n\n\n@task(task_id=\"print_minute\")\ndef print_minute():\n    \"Prints the current minute\"\n    print(pendulum.minute)\n\n\n@task(task_id=\"print_second\")\ndef print_second():\n    \"Prints the current second\"\n    print(pendulum.second)\n\n\n@task(task_id=\"print_microsecond\")\ndef print_microsecond():\n    \"Prints the current microsecond\"\n    print(pendulum.microsecond)\n\n\n@task(task_id=\"print_weekday\")\ndef print_weekday():\n    \"Prints the current weekday\"\n    print(pendulum.weekday)\n\n\n@task(task_id=\"print_weekday_of_month\")\ndef print_weekday_of_month():\n    \"Prints the current weekday of the month\"\n    print(pendulum.weekday_of_month)\n\n\n@task(task_id=\"print_weekday_of_quarter\")\ndef print_weekday_of_quarter():\n    \"Prints the current weekday of the quarter\"\n    print(pendulum.weekday_of_quarter)\n\n\n@task(task_id=\"print_month_of_quarter\")\ndef print_month_of_quarter():\n    \"Prints the current month of the quarter\"\n    print(pendulum.month_of_quarter)\n\n\n@task(task_id=\"print_quarter_of_year\")\ndef print_quarter_of_year():\n    \"Prints the current quarter of the year\"\n    print(pendulum.quarter_of_year)\n\n\n@task(task_id=\"print_quarter_of_week\")\ndef print_quarter_of_week():\n    \"Prints the current quarter of the week\"\n    print(pendulum.quarter_of_week)\n\n\n@task(task_id=\"print_week_of_month\")\ndef print_week_of_month():\n    \"Prints the current week of the month\"\n    print(pendulum.week_of_month)\n\n\n@task(task_id=\"print_week_of_year\")\ndef print_week_of_year():\n    \"Prints the current week of the year\"\n    print(pendulum.week_of_year)\n\n\n@task(task_id=\"print_year_of_quarter\")\ndef print_year_of_quarter():\n    \"Prints the current year of the quarter\"\n    print(pendulum.year_of_quarter)\n\n\n@task(task_id=\"print_year_of_week\")\ndef print_year_of_week():\n    \"Prints the current year of the week\"\n    print(pendulum.year_of_week)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of_year():\n    \"Prints the current day of the week of the year\"\n    print(pendulum.day_of_week_of_year)\n\n\n@task(task_id=\"print_day_of_week_of_month\")\ndef print_day_of_week_of_month():\n    \"Prints the current day of the week of the month\"\n    print(pendulum.day_of_week_of_month)\n\n\n@task(task_id=\"print_day_of_week_of_quarter\")\ndef print_day_of_week_of_quarter():\n    \"Prints the current day of the week of the quarter\"\n    print(pendulum.day_of_week_of_quarter)\n\n\n@task(task_id=\"print_day_of_week_of_year\")\ndef print_day_of_week_of"}], "metadata": {"file_name": "dag_1054_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure DAG that demonstrates system-level and query-level interactions with Apache Pinot for data exploration and management. Create a workflow that executes administrative commands and retrieves database information using specialized Pinot hooks.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinot.hooks.pinot import PinotHook\nfrom airflow.providers.pinot.operators.pinot import PinotAdminOperator\n\nCONNECTION_ID = os.environ.get(\"CONNECTION_ID\", \"pinot_default\")\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_pinot_admin\"\n\n# [START howto_operator_pinot]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": CONNECTION_ID},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def setup_databases(hook: PinotHook):\n        databases = [\"mydatabase1\", \"mydatabase2\"]\n        for db in databases:\n            hook.create_database(db, create_table=True)\n\n    setup_databases_hook = PinotAdminOperator(\n        task_id=\"setup_databases\",\n        hook=PinotHook(),\n        pinot_conn_id=\"pinot_default\",\n        request_body={\"actions\": \"CREATE DATABASE mydatabase1 CREATE TABLE table1\"},\n    )\n\n    @task\n    def get_databases(hook: PinotHook):\n        databases = hook.get_databases()\n        for db in databases:\n            print(db)\n\n    get_databases_hook = PinotAdminOperator(\n        task_id=\"get_databases\",\n        pinot_conn_id=\"pinot_default\",\n        hook=PinotHook(),\n        request_body={\"actions\": \"GET DATABASES\"},\n        dag=dag,\n    )\n\n    @task\n    def clean_databases(hook: PinotHook):\n        databases = [\"mydatabase1\", \"mydatabase2\"]\n        for db in databases:\n            hook.delete_database(db)\n\n    clean_databases_hook = PinotAdminOperator(\n        task_id=\"clean_databases\",\n        pinot_conn_id=\"pinot_default\",\n        hook=PinotHook(),\n        pinot_conn_id=CONNECTION_ID,\n        request_body={\"actions\": \"DELETE DATABASE mydatabase1\"},\n    )\n\n    (setup_databases_hook >> get_databases_hook >> clean_databases_hook)\n\n    # [END howto_operator_pinot]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_773_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with clear sequential steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.task import Task\nfrom airflow.models.taskinstance import TaskInstance as TI\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.sdk import DAG as Dag\nfrom airflow.utils.weekday import WeekDay\n\nfrom system.openlineage.operator import OpenLineageTestOperator\nfrom system.test_utils.watcher import watcher\n\n# [START howto_operator_aws_create_stack]\ndef aws_create_stack(stack_name: str, stack_template: str, region_name: str):\n    command = f\"aws cloudformation create-stack --stack-name {stack_name} \" \\\n             f\"--template-body file://{stack_template} \" \\\n             f\"--region {region_name} --onfailure KeepStacks\"\n    subprocess.run(command, check=True, shell=True)\n\n\ndef aws_delete_stack(stack_name: str, region_name: str):\n    command = f\"aws cloudformation delete-stack --stack-name {stack_name} \" \\\n             f\"--region {region_name} --onfailure KeepStacks\"\n    subprocess.run(command, check=True, shell=True)\n\n\n# [END howto_operator_aws_create_stack]\n# [START howto_operator_aws_delete_stack]\ndef aws_delete_stack_with_role(stack_name: str, region_name: str):\n    command = f\"aws cloudformation delete-stack --stack-name {stack_name} \" \\\n             f\"--region {region_name} \" \\\n             f\"--onfailure KeepStacks \" \\\n             f\"--roles {Param(TI.xcom_pull(task_ids='aws_create_stack_role', key='role_arn'))}\"\n    subprocess.run(command, check=True, shell=True)\n\n\n# [END howto_operator_aws_delete_stack]\n# [START howto_operator_aws_create_stack_role]\ndef aws_create_stack_role(stack_name: str, region_name: str):\n    command = f\"aws cloudformation create-stack-role --stack-name {stack_name} \" \\\n             f\"--region {region_name} \" \\\n             f\"--role-name test-role \" \\\n             f\"--capabilities ['CAPABILITY_IAM']\"\n    subprocess.run(command, check=True, shell=True)\n\n\ndef aws_delete_stack_role_role_role(stack_name: str, region_name: str):\n    command = f\"aws cloudformation delete-stack-role --stack-name {stack_name} \" \\\n             f\"--region {region_name} \" \\\n             f\"--role-name test-role\"\n    subprocess.run(command, check=True, shell=True)\n\n\n# [END howto_operator_aws_create_stack_role]\n# [START howto_operator_aws_get_stack]\ndef aws_get_stack(stack_name: str, region_name: str):\n    command = f\"aws cloudformation get-stack --stack-name {stack_name} \" \\\n             f\"--region {region_name}\"\n    output = subprocess.check_output(command, shell=True)\n    return json.loads(output)\n\n\n# [END howto_operator_aws_get_stack]\n# [START howto_operator_aws_list_stacks]\ndef aws_list_stacks():\n    command = \"aws cloudformation list-stacks --region default\"\n    output = subprocess.check_output(command, shell=True)\n    return json.loads(output)\n\n\n# [END howto_operator_aws_list_stacks]\n# [START howto_operator_aws_create_sqs_queue]\ndef aws_create_sqs_queue(queue_name: str, region_name: str):\n    command = f\"aws sqs create-channel --name {queue_name} \" \\\n             f\"--region {region_name}\"\n    subprocess.run(command, check=True, shell=True)\n\n\ndef aws_delete_sqs_queue(queue_name: str, region_name: str):\n    command = f\"aws sqs delete-channel --channel-name {queue_name} \" \\\n             f\"--region {region_name}\"\n    subprocess.run(command, check=True, shell=True)\n\n\n# [END howto_operator_aws_create_sqs_queue]\n# [START howto_operator_aws_list_sqs_channels]\ndef aws_list_sqs_channels(queue_name: str):\n    command = f\"aws sqs list-channels --channel-name {queue_name} --region default\"\n    output = subprocess.check_output(command, shell=True)\n    return json.loads(output)\n\n\nwith DAG(\n    dag_id=\"example_aws\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"aws_default\",\n            conn_type=\"aws\",\n        )\n        session = Session()\n        session.add(conn)\n        session.commit()\n\n    @task\n    def create_role():\n        role = Role()\n        role.role_name = \"test-role\"\n        role.permissions_boundary.capabilities = [\"CAPABILITY_IAM\"]\n        session = Session()\n        session.add(role)\n        session.commit()\n\n    @task\n    def create_stack(stack_name: str, stack_template: str, region_name: str):\n        aws_create_stack(stack_name, stack_template, region_name)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_stack(stack_name: str, region_name: str):\n        aws_delete_stack(stack_name, region_name)\n        aws_delete_stack_role_role_role(stack_name, region_name)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def get_stack(stack_name: str, region_name: str):\n        aws_get_stack(stack_name, region_name, return_json=True)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def list_stacks():\n        aws_list_stacks()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def list_sqs_channels(queue_name: str):\n        aws_list_sqs_channels(queue_name)\n\n    @task\n    def touch_dag():\n        with open(\"test.txt\", \"a\"):\n            pass\n\n    @task\n    def get_template(queue_name: str):\n        with open(\"test_template.json\") as f:\n            return json.load(f)\n\n    create_connection_task = create_connection()\n    create_role_task = create_role()\n    create_stack_task = create_stack(\"stack-ti-1\", \"stack-create-ti.json\", \"us-east-1\")\n    create_stack_task2 = create_stack(\"stack-ti-2-2\", \"stack-create-ti-2-2\", \"us-east-1\")\n    create_stack_role_task = create_stack_role(\"stack-role-ti\", \"stack-role-ti.json\", \"us-east-1\")\n    create_stack_with_role = create_stack(\"aws_create_stack_stack\", \"aws_create_stack_stack_template\", \"us-east-1\")\n    aws_get_stack_task = aws_get_stack(\"stack-name\", \"stack-name\")\n    aws_list_stacks_task = aws_list_stacks()\n    aws_list_sqs_channels_task = aws_list_sqs_channels(\"queue-name\")\n    aws_delete_stack_role_role_role_task = aws_delete_stack_role_role_role(\"stack-name\", \"stack-name\")\n    aws_delete_stack_task = aws_delete_stack(\"stack-name\", \"stack-name\")\n    aws_create_sqs_queue_task = aws_create_sqs_queue(\"queue-ti-1\", \"stack-role-ti.json\", \"us-east-1\")\n    aws_create_sqs_queue_task2 = aws_create_sqs_queue(\"queue-ti-2-2\", \"stack-role-ti.json\", \"us-east-1\")\n    aws_delete_sqs_queue_task = aws_delete_sqs_queue(\"queue-name\", \"stack-role-ti.json\")\n    get_template_task = get_template(\"queue-name\")\n    touch_dag_task = touch_dag()\n    get_template_result = XComArg(get_template_task, key=\"return_value\")\n    list_stacks_result = XComArg(aws_list_stacks_task, key=\"return_value\")\n    list_sqs_channels_result = XComArg(aws_list_sqs_channels_task, key=\"return_value\")\n    aws_create_stack_with_role_result = XComArg(create_stack_with_role, key=\"return_value\")\n    aws_get_stack_result = XComArg(aws_get_stack_task, key=\"return_value\")\n    aws_delete_stack_role_role_role_result = XComArg(aws_delete_stack_role_role_role_task, key=\"return_value\")\n    aws_create_stack_role_result = XComArg(create_stack_role_task, key=\"return_value\")\n    aws_create_stack_task_output = XComArg(aws_create_stack_task, key=\"return_value\")\n    aws_create_stack_task2_output = XComArg(aws_create_stack_task2, key=\"return_value\")\n    aws_delete_stack_task_output = XComArg(aws_delete_stack_task, key=\"return_value\")\n    aws_create_sqs_queue_task_output = XComArg(aws_create_sqs_queue_task, key=\"return_value\")\n    aws_create_sqs_queue_task2_output = XComArg(aws_create_sqs_queue_task2, key=\"return_value\")\n    aws_delete_sqs_queue_task_output = XComArg(aws_delete_sqs_queue_task, key=\"return_value\")\n    render_jinja_test = BashOperator(\n        bash_command=\"echo '{{ dag.params.param }}\",\n        task_id=\"render_jinja_test\"\n    )\n\n    watcher(dag)\n\n    list_stacks_task >> touch_dag_task >> render_jinja_test\n    [create_connection_task, create_role_task, create_stack_with_role] >> remove_stack\n    [aws_create_stack_task, aws_create_stack_task2] >> aws_create_sqs_queue_task >> aws_create_sqs_queue_task2\n    [aws_delete_stack_task, aws_delete_stack_role_role_role_task, aws_delete_stack_role_role_role_task,\n     aws_delete_stack"}], "metadata": {"file_name": "dag_58_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that simulates real-world S3 file processing and storage management tasks, including creating buckets, uploading sample data, and performing metadata operations. Design a learning exercise that demonstrates how to programmatically interact with cloud storage using Apache Airflow and AWS S3 providers.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.sensors.s3 import S3ObjectSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_sensor\"\n\nconn = Connection(\n    conn_id=\"s3_sensor_test\",\n    conn_type=\"s3\",\n    host=\"s3://airflow-system-tests-resources/\",\n    schema=\"s3\",\n    description=\"Test S3 connection\",\n    tags=[\"test\"],\n)\n\nOBJECT = \"test-s3-file\"\nBUCKET = \"test-s3-bucket\"\n\n\n@task\ndef create_dump_file(bucket: str, object: str):\n    \"\"\"Create a temporary file containing JSON serialization of details about the task context and dump it to a\n    temporary file in S3.\"\"\"\n    # Let's get the details of the dAG and task that were decorated with `sys_test_context_task` and extract its\n    # schema, host and other information. We'll use this information to dump the task context into a temporary\n    # file in S3.\n    file_content = json.dumps(\n        {\n            \"dag_id\": dagdag.__dict__[\"DAG_ID\"],\n            \"task_id\": dagtask.__dict__[\"task_id\"],\n            \"cond_str\": str(dagdag.__dict__[\"tasks\"][dagtask.__dict__[\"task_id\"]].condition),\n            \"context\": {\n                \"dag\": dagdag,\n                \"task_instance\": dagtask,\n                \"run_id\": dagtask.info.get(\"run_id\"),\n            },\n            \"conn\": {\n                \"conn_id\": conn.conn_id,\n                \"host\": conn.host,\n                \"schema\": conn.schema,\n                \"tags\": conn.tags,\n            },\n        }\n    )\n\n    # Now we'll dump this file contents to an S3 url. An Airflow hook will be used to determine what S3 bucket\n    # and key name to use for the file. In this case the bucket name is \"test-s3-dump\" and the key name is the\n    # decorated task's id, \"task-<task_id>\", so that we can easily find the file that was dumped We'll also\n    # specify the S3 bucket location to which the file will be dumped. Once the file has been dumped to S3 we\n    # can clean up the file from the Hadoop environment.\n    S3Hook().dump_file(file_content, bucket=bucket, key=object)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dump_file(bucket: str, object: str):\n    \"\"\"Delete the temporary file you just created in the S3 bucket.\"\"\"\n    S3Hook().delete_file(bucket=bucket, key=object)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    bucket_name = f\"{env_id}-s3-bucket\"\n    object_key = f\"{env_id}-s3-file\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    @task\n    def get_s3_url(bucket_name: str, object_key: str):\n        \"\"\"Returns an S3 URL for the given bucket and key.\"\"\"\n        return f\"s3://{bucket_name}/{object_key}\"\n\n    s3_url = get_s3_url(task_id=\"get_s3_url\", bucket_name=bucket_name, object_key=object_key)\n\n    create_dump_file(bucket_name, object_key)\n\n    wait_for_dump_file = S3ObjectSensor(\n        task_id=\"wait_for_dump_file\",\n        s3_conn_id=\"s3_sensor_test\",\n        s3_bucket=bucket_name,\n        s3_key=object_key,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_dump_file = BashOperator(\n        task_id=\"delete_dump_file\",\n        bash_command=f\"rm {bucket_name}/{object_key}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = BashOperator(\n        task_id=\"delete_bucket\",\n        bash_command=f\"rm -r {bucket_name}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> wait_for_dump_file\n        >> create_dump_file(bucket_name, object_key)\n        >> get_s3_url(bucket_name, object_key)\n        >> wait_for_dump_file\n        # TEST TEARDOWN\n        >> delete_dump_file\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_1117_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data transformation pipeline that converts CSV files to AVRO format using Google Dataflow templates. Create a workflow that uploads source files, manages GCS buckets, and executes flexible data conversion jobs.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowTemplatePlaygroundOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.samba import SambaCopyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"dataflow_lesson_1\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west2-b\"\nCLUSTER_NAME = f\"cluster_{DAG_ID}_{ENV_ID}\"\nTEMPLEATE_ID = \"word-count\"\nSOURCE_GCS = \"samba://root@sis:/user/root\"\nDESTINATION_GCS = \"gs://dataflow-templates/latest/python/word-count\"\nPARENT_DIR = \"gs://dataflow-templates/latest/python/word-count/output\"\nCONNECTION_ID = \"samba\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataflow\"],\n) as dag:\n    create_samba_connection = SambaCopyOperator(\n        task_id=\"samba_to_bucket\",\n        connection_id=CONNECTION_ID,\n        source=SOURCE_GCS,\n        destination=BUCKET_NAME,\n        replace=False,\n    )\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_dataflow_templated_task]\n    template_task = DataflowTemplatePlaygroundOperator(\n        task_id=\"template_task\",\n        template=DESTINATION_GCS,\n        parameters={\"outputDirectory\": PARENT_DIR},\n        location=LOCATION,\n        cluster_spec={\n            \"name\": CLUSTER_NAME,\n            \"imageVersion\": \"dataflow-2.1\",\n            \"softwareSpec\": {\n                \"artifactSpec\": {\n                    \"url\": \"gs://google-cloud-compute/releases/latest/dataproc.tar.gz\",\n                    \"type\": \"url\",\n                }\n            },\n            \"numWorkers\": 3,\n            \"containerSpec\": {\n                \"imageUri\": \"googlecloudcomputebuilds//container:python33-py373-v1.0.0-da48868\",\n                \"args\": [\n                    \"spark\", \"run\", \"gs://dataflow-templates/latest/python/word-count/word-count.py\",\n                    \"--source-gcs\", SOURCE_GCS,\n                    \"--destination-gcs\", PARENT_DIR,\n                ],\n                \"env\": [\n                    {\"name\": \"PROJECT_ID\", \"value\": PROJECT_ID},\n                    {\"name\": \"BUCKET_NAME\", \"value\": BUCKET_NAME},\n                    {\"name\": \"LOCATION\", \"value\": LOCATION},\n                    {\"name\": \"CLUSTER_NAME\", \"value\": CLUSTER_NAME},\n                ],\n            },\n        },\n        do_xcom_push=True,\n    )\n    # [END howto_operator_dataflow_templated_task]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        [create_samba_connection, create_bucket] >> template_task >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_2023_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a simple data storage workflow using LevelDB that demonstrates basic key-value retrieval and insertion operations. Create a DAG that shows how to get and put data into a lightweight embedded key-value database.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.level_db import LevelDBHook\nfrom airflow.providers.google.cloud.operators.level_db import LevelDBPutOperator\nfrom airflow.providers.google.cloud.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"level_db\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"level_db\"],\n) as dag:\n    bucket_name = f\"bucket_{DAG_ID}_{ENV_ID}\"\n    bucket_file = \"test.level\"\n    key = \"test.key\"\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    @task\n    def create_table():\n        hook = LevelDBHook(project_id=PROJECT_ID)\n        hook.create_table(str(key), str({\"test\": \"test_value\"}))\n\n    create_table_task = create_table()\n\n    # [START how_to_level_db_put]\n    put_task = LevelDBPutOperator(\n        task_id=\"put_task\",\n        project_id=PROJECT_ID,\n        bucket_name=bucket_name,\n        object_name=bucket_file,\n        key=key,\n        value=\"test_value\",\n    )\n    # [END how_to_level_db_put]\n\n    delete_bucket = S3CreateBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=bucket_name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> create_table_task\n        >> put_task\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove\n    # the temp bucket\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and communication using XComs. Create a workflow that shows how different tasks can push and pull data between each other using various methods like direct pushing, returning values, and bash operator interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    print(pendulum.datetime.now())\n\n\n@task(task_id=\"print_date_in_return\")\ndef print_date_in_return():\n    return pendulum.datetime.now()\n\n\n@task(task_id=\"push_date_to_xcom\", task_id=\"pull_date_from_xcom\")\ndef push_pull_date():\n    push_date_in_return()\n    print_date()\n\n\nwith DAG(\n    \"example_xcom_dag\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    push_pull_date()\n    BashOperator(task_id=\"print_date\", bash_command=\"echo 42\")\n    BashOperator(task_id=\"print_date_in_return\", bash_command=\"echo $1\")"}], "metadata": {"file_name": "dag_32_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates secure and controlled file movement between cloud storage systems with explicit lifecycle management. Create a process that initializes a temporary storage container, executes a cross-platform file transfer, and ensures complete resource cleanup regardless of task outcomes.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_s3_copy\"\n\ns3_conn = Connection(\n    conn_id=\"s3_conn\",\n    conn_type=\"s3\",\n    host=\"s3-us-gov.amazonaws.com\",\n    login=\"admin\",\n    password=\"password\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_s3_to_s3_copy_to_destination():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_destination\",\n            s3_source_key=\"s3_to_s3_copy_to_destination\",\n            s3_destination_key=\"s3_to_s3_copy_to_destination_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_destination_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_destination_to_to_destination():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_destination_to_destination = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_destination_to_destination\",\n            s3_source_key=\"s3_to_s3_copy_from_destination_to_destination\",\n            s3_destination_key=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_to_source():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_to_source = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_source\",\n            s3_source_key=\"s3_to_s3_copy_to_source\",\n            s3_destination_key=\"s3_to_s3_copy_to_source_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_source_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_source_to_source():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_source_to_source = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_source_to_source\",\n            s3_source_key=\"s3_to_s3_copy_from_source_to_source\",\n            s3_destination_key=\"s3_to_s3_copy_from_source_to_source_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_source_to_source_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_to_destination_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_to_destination_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_destination_dest\",\n            s3_source_key=\"s3_to_s3_copy_to_destination_dest\",\n            s3_destination_key=\"s3_to_s3_copy_to_destination_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_destination_dest_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_destination_to_destination_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_destination_to_destination_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n            s3_source_key=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n            s3_destination_key=\"s3_to_s3_copy_from_destination_to_destination_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_destination_to_destination_dest_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_to_source_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_to_source_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_source_dest\",\n            s3_source_key=\"s3_to_s3_copy_to_source_dest\",\n            s3_destination_key=\"s3_to_s3_copy_to_source_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_source_dest_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_source_to_source_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_source_to_source_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_source_to_source_dest\",\n            s3_source_key=\"s3_to_s3_copy_from_source_to_source_dest\",\n            s3_destination_key=\"s3_to_s3_copy_from_source_to_source_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_source_to_source_dest_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_to_destination():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_to_destination = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_destination\",\n            s3_source_key=\"s3_to_s3_copy_to_destination\",\n            s3_destination_key=\"s3_to_s3_copy_to_destination_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_destination_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_destination_to_destination():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_destination_to_destination = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_destination_to_destination\",\n            s3_source_key=\"s3_to_s3_copy_from_destination_to_destination\",\n            s3_destination_key=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_destination_to_destination_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_to_source_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_to_source_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_to_source_dest\",\n            s3_source_key=\"s3_to_s3_copy_to_source_dest\",\n            s3_destination_key=\"s3_to_s3_copy_to_source_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_to_source_dest_dest\",\n        )\n\n    @task\n    def create_s3_to_s3_copy_from_source_to_source_dest():\n        from airflow.providers.s3.transfers.copy_between_s3_to_s3 import S3ToS3CopyOperator\n\n        s3_to_s3_copy_from_source_to_source_dest = S3ToS3CopyOperator(\n            task_id=\"s3_to_s3_copy_from_source_to_source_dest\",\n            s3_source_key=\"s3_to_s3_copy_from_source_to_source_dest\",\n            s3_destination_key=\"s3_to_s3_copy_from_source_to_source_dest_dest\",\n            dest_s3_key=\"s3_to_s3_copy_from_source_to_source_dest_dest\",\n        )\n\n    create_s3_to_s3_copy_to_destination = create_s3_to_s3_copy_to_destination()\n    create_s3_to_s3_copy_from_destination_to_destination = create_s3_to_s3_copy_from_destination_to_destination()\n    create_s3_to_s3_copy_to_source_dest = create_s3_to_s3_copy_to_source_dest()\n    create_s3_to_s3_copy_from_source_to_source_dest = create_s3_to_s3_copy_from_source_to_source_dest()\n    create_s3_to_s3_copy_to_destination_dest = create_s3_to_s3_copy_to_destination_dest()\n    create_s3_to_s3_copy_from_destination_to_destination_dest = create_s3_to_s3_copy_from_destination_to_destination_dest()\n    create_s3_to_s3_copy_from_source_to_source_dest = create_s3_to_s3_copy_from_source_to_source_dest()\n    create_s3_to_s3_copy_from_source_to_source = create_s3_to_s3_copy_from_source_to_source()\n    create_s3_to_s3_copy_to_source = create_s3_to_s3_copy_to_source()\n    create_s3_to_s3_copy_from_source = create_s3_to_s3_copy_from_source()\n\n    execute_copy_between_s3_to_s3 = SQLExecuteQueryOperator(\n        task_id=\"execute_copy_between_s3_to_s3\",\n        sql=f\"SELECT * FROM {s3_conn.conn_id}.{s3_source_key}\",\n    )\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_all_s"}], "metadata": {"file_name": "dag_93_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that automatically collects, downloads, and transfers digital advertising performance data from Display Video 360 to a centralized data warehouse for comprehensive marketing analytics and reporting.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.display_video import (\n    DisplayVideoDownloadInteractionsOperator,\n    DisplayVideoDownloadVideoOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"display_video\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_FILE = \"video.mp4\"\nVIDEO_ID = os.environ.get(\"VIDEO_ID\", \"test_video_id\")\nSECOND_VIDEO_ID = VIDEO_ID + \"-2\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"display_video\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    download_video = DisplayVideoDownloadVideoOperator(\n        task_id=\"download_video\",\n        video_id=VIDEO_ID,\n        gcp_conn_id=\"google_cloud_platform\",\n        destination_bucket=BUCKET_NAME,\n        destination_file_name=BUCKET_FILE,\n    )\n\n    download_video_second = DisplayVideoDownloadVideoOperator(\n        task_id=\"download_video_second\",\n        video_id=SECOND_VIDEO_ID,\n        gcp_conn_id=\"google_cloud_platform\",\n        destination_bucket=BUCKET_NAME,\n        destination_file_name=BUCKET_FILE,\n    )\n\n    download_interactions = DisplayVideoDownloadInteractionsOperator(\n        task_id=\"download_interactions\",\n        video_id=VIDEO_ID,\n        gcp_conn_id=\"google_cloud_platform\",\n        include_deleted=True,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> download_video\n        >> download_video_second\n        >> download_interactions\n    )\n\n    # TEST TEARDOWN\n    delete_bucket"}], "metadata": {"file_name": "dag_46_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data extraction and loading process that captures a specific Firestore collection, exports it to cloud storage, and prepares it for analytical querying in BigQuery. Design a scalable data movement solution that transforms unstructured NoSQL data into a queryable external table format.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQueryToGoogleCloudStorageOperator,\n)\nfrom airflow.providers.google.cloud.transfers.firebase_to_gcs import FirebaseToGoogleCloudStorageOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nDAG_ID = \"firebase_to_gcs_example\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{PROJECT_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{PROJECT_ID}\"\nFIREBASE_URL = \"https://your-project-id.firebaseio.com\"\nTABLE_NAME = \"test-table\"\nSOURCE_OBJECT_NAME = \"data.json\"\nEXTERNAL_TABLE_NAME = \"external-test-table\"\nEXTERNAL_SOURCE_OBJECT_NAME = \"external-data.json\"\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"firebase\", \"gcs\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    # [START howto_operator_firebase_to_gcs]\n    upload_firebase_data_to_gcs = FirebaseToGoogleCloudStorageOperator(\n        task_id=\"upload_firebase_data_to_gcs\",\n        firebase_url=FIREBASE_URL,\n        bucket_name=BUCKET_NAME,\n        source_object=SOURCE_OBJECT_NAME,\n        external_table_name=EXTERNAL_TABLE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_firebase_to_gcs]\n\n    # [START howto_operator_insert_bigqueryitem]\n    insert_bigqueryitem = BigQueryInsertJobOperator(\n        task_id=\"insert_bigqueryitem\",\n        configuration={\n            \"query\": {\n                \"query\": f\"CREATE TABLE {DATASET_NAME}.{EXTERNAL_TABLE_NAME} AS SELECT * FROM {BUCKET_NAME}.{EXTERNAL_TABLE_NAME}\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_insert_bigqueryitem]\n\n    # [START howto_operator_create_external_table]\n    create_external_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_external_dataset\",\n        dataset_id=EXTERNAL_TABLE_NAME,\n    )\n    # [END howto_operator_create_external_table]\n\n    # [START howto_operator_insert_bigqueryitem_external]\n    insert_external_table = BigQueryInsertJobOperator(\n        task_id=\"insert_external_table\",\n        configuration={\n            \"query\": {\n                \"query\": f\"CREATE TABLE {EXTERNAL_DATASET_NAME}.{EXTERNAL_TABLE_NAME} AS SELECT * FROM {BUCKET_NAME}.{EXTERNAL_TABLE_NAME}\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_insert_external_table]\n\n    # [START howto_operator_gcs_to_bigquery]\n    gcs_to_bq = BigQueryToGoogleCloudStorageOperator(\n        task_id=\"gcs_to_bq\",\n        source_objects=[EXTERNAL_SOURCE_OBJECT_NAME],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_firebase_data_to_gcs >> create_dataset >> insert_bigqueryitem >> create_external_dataset >>\n    insert_external_table >> create_external_dataset >> gcs_to_bq >> delete_dataset"}], "metadata": {"file_name": "dag_207_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data science workflow that transforms raw training data into a deployable machine learning model with automated infrastructure provisioning and prediction capabilities. The solution should demonstrate how to take structured CSV data, train a classification model, and expose it as a real-time prediction service with automatic resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.sdk import chain\n\nlog = logging.getLogger(__name__)\n\nHOME = os.environ.get(\"HOME\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_training\"\n\nconn = Connection(\n    conn_id=\"form_data\",\n    schema=\"airflow\",\n    conn_type=\"form_data\",\n    description=\"Connection for example_adf_training DAG.\",\n    extra=json.dumps({\"path\": f\"{HOME}/adf_training_form.json\"}),\n)\n\nsession = Session()\nsession.add_all([conn])\nsession.commit()\n\n\n@task\ndef create_form_adf():\n    role = Role()\n    role.role_id = 1\n    session.add(role)\n\n    trigger_rule = \"all_done\"\n    task = BashOperator(task_id=\"test_task\", bash_command=\"echo test\", trigger_rule=trigger_rule)\n    task.role_id = role.role_id\n\n    task_adf = BashOperator(\n        task_id=\"test_task_adf\",\n        bash_command=\"echo test\",\n        trigger_rule=trigger_rule,\n    )\n    task_adf.role_id = role.role_id\n\n    chain(task, task_adf)\n\n\n@task\ndef create_model_adf():\n    role = Role()\n    role.role_id = 2\n    session.add(role)\n\n    model_task = BashOperator(\n        task_id=\"test_model_task\",\n        bash_command=\"echo test\",\n        trigger_rule=\"all_done\",\n    )\n    model_task.role_id = role.role_id\n\n    model_task_adf = BashOperator(\n        task_id=\"test_model_task_adf\",\n        bash_command=\"echo test\",\n        trigger_rule=\"all_done\",\n    )\n    model_task_adf.role_id = role.role_id\n\n    chain(model_task, model_task_adf)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_form_adf()\n    create_model_adf()\n    dag"}], "metadata": {"file_name": "dag_1928_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the lifecycle of cloud infrastructure, showing how to programmatically create a Google Kubernetes Engine (GKE) cluster and then spin up a Dataproc cluster within that Kubernetes environment. Demonstrate infrastructure-as-code principles by defining cluster configurations and managing their creation and deletion as part of a coordinated workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.dataproc import DataprocHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_dataproc\"\nREGION = \"europe-west1\"\nZONE = REGION + \"-a\"\nNETWORK = \"network-1\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nGCE_INSTANCE_NAME = f\"instance-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nCLUSTER_CONFIG = {\n    \"master_machine_type\": \"n1-standard-4\",\n    \"worker_machine_type\": \"n1-standard-4\",\n    \"num_workers\": 2,\n    \"disk_size_gb\": 100,\n    \"disk_name\": \"i-d8b17779\",\n    \"gce_config\": {\n        \"machine_type\": \"n1-standard-4\",\n        \"name\": GCE_INSTANCE_NAME,\n        \"disks\": [\n            {\n                \"boot\": True,\n                \"device_name\": GCE_INSTANCE_NAME,\n                \"initialize_params\": {\n                    \"disk_size_gb\": 100,\n                    \"disk_name\": \"i-d8b17779\",\n                },\n            }\n        ],\n        \"network_interfaces\": [\n            {\n                \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                \"network\": \"global/networks/np-1\",\n            }\n        ],\n    },\n}\nDATAPROC_CLUSTER = {\n    \"config\": {\n        \"master_config\": {\n            \"num_instances\": 1,\n            \"machine_type\": \"n1-standard-4\",\n        },\n        \"worker_config\": {\n            \"num_instances\": 2,\n            \"machine_type\": \"n1-standard-4\",\n        },\n        \"software_config\": {\n            \"image_version\": \"2.2.1\",\n        },\n    },\n    \"status\": {\"state\": \"HEALTHY\"},\n    \"name\": CLUSTER_NAME,\n}\nGCS_BUCKET_NAME = f\"bucket_{ENV_ID}_{DAG_ID}\"\nFILE_NAME = \"file.tar\"\nPATH_TO_FILE = f\"gs://{GCS_BUCKET_NAME}/{FILE_NAME}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"compute\", \"dataproc\"],\n) as dag:\n    @task\n    def create_cluster():\n        dataproc_hook = DataprocHook()\n        # Cluster creation takes effect immediately.\n        dataproc_hook.create_cluster(\n            PROJECT_ID,\n            CLUSTER_NAME,\n            DATAPROC_CLUSTER[\"config\"],\n        )\n\n    @task\n    def delete_cluster():\n        dataproc_hook = DataprocHook()\n        dataproc_hook.delete_cluster(\n            PROJECT_ID, CLUSTER_NAME, force_delete=True, trigger_rule=TriggerRule.ALL_DONE\n        )\n\n    @task\n    def create_bucket():\n        gcs_create_bucket = GCSCreateBucketOperator(\n            task_id=\"gcs_create_bucket\",\n            bucket_name=GCS_BUCKET_NAME,\n            project_id=PROJECT_ID,\n        )\n\n    @task\n    def upload_file():\n        gcs_upload_file = LocalFilesystemToGCSOperator(\n            task_id=\"gcs_upload_file\",\n            src=PATH_TO_FILE,\n            dst=FILE_NAME,\n            bucket=GCS_BUCKET_NAME,\n        )\n\n    # [START create_stack](https://cloud.google.com/compute/docs/staging/create-stack-steps)\n    create_stack = ComputeEngineInsertInstanceOperator(\n        task_id=\"create_stack\",\n        project_id=PROJECT_ID,\n        zone=ZONE,\n        body=CLUSTER_CONFIG,\n    )\n    # [END create_stack]\n\n    # [START delete_stack]\n    delete_stack = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_stack\", project_id=PROJECT_ID, zone=ZONE, resource_id=GCE_INSTANCE_NAME\n    )\n    # [END delete_stack]\n\n    create_cluster() >> create_bucket() >> upload_file() >> create_stack >> delete_stack >> delete_cluster()\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed or as failed)\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1626_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task management of a Google Cloud DLP job trigger, demonstrating how to create, list, retrieve, update, and delete a data inspection trigger. The workflow will serve as a technical reference for managing data protection tasks through a structured, automated pipeline.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobTriggerOperator,\n    CloudDLPDeleteJobTriggerOperator,\n    CloudDLPGetJobTriggerOperator,\n    CloudDLPListJobTriggersOperator,\n    CloudDLPUpdateJobTriggerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or \"default\"\n\nDLP_JOB_TRIGGER_ID = f\"dlp_job_trigger_{ENV_ID}\"\n\n# [START howto_operator_dlp_create_job_trigger]\ncreate_job_trigger = CloudDLPCreateJobTriggerOperator(\n    task_id=\"create_job_trigger\",\n    project_id=PROJECT_ID,\n    dlp_job_trigger={\"inspect_job\": {\"storage_config\": {\"gcs_source\": {\"uri\": \"bq://airflow.system-tests\"}}}},\n    trigger_list_name=\"dlp-job-triggers\",\n)\n# [END howto_operator_dlp_create_job_trigger]\n\n# [START howto_trigger_dlp_get_job_trigger]\nget_job_trigger = CloudDLPGetJobTriggerOperator(\n    task_id=\"get_job_trigger\",\n    project_id=PROJECT_ID,\n    dlp_job_trigger_id=DLP_JOB_TRIGGER_ID,\n)\n# [END howto_trigger_dlp_get_job_trigger]\n\n# [START howto_trigger_dlp_list_job_triggers]\nlist_job_triggers = CloudDLPListJobTriggers(\n    task_id=\"list_job_triggers\", project_id=PROJECT_ID, page_size=5, page_token=\"abc\", location=\"us\"\n)\n# [END howto_trigger_dlp_list_job_triggers]\n\n# [START howto_trigger_dlp_update_job_trigger]\nupdate_job_trigger = CloudDLPUpdateJobTriggerOperator(\n    task_id=\"update_job_trigger\",\n    project_id=PROJECT_ID,\n    dlp_job_trigger_id=DLP_JOB_TRIGGER_ID,\n    body={\"inspect_job\": {\"storage_config\": {\"gcs_source\": {\"uri\": \"gcs/path/file\"}}}},\n)\n# [END howto_trigger_dlp_update_job_trigger]\n\n# [START howto_trigger_dlp_delete_job_trigger]\ndelete_job_trigger = CloudDLPDeleteJobTriggerOperator(\n    task_id=\"delete_job_trigger\",\n    project_id=PROJECT_ID,\n    dlp_job_trigger_id=DLP_JOB_TRIGGER_ID,\n    trigger_list_name=\"dlp-job-triggers\",\n)\n# [END howto_trigger_dlp_delete_job_trigger]\ndelete_job_trigger.trigger_list_name = \"dlp-job-triggers2\"\n\nwith DAG(\n    \"dlp-job-triggers\",\n    schedule_at_least_once\u6bcf\u4e00\u5929=datetime(2021, 1, 1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"job-triggers\"],\n) as dag:\n    t1 = create_job_trigger\n    t2 = get_job_trigger\n    t3 = list_job_triggers\n    t4 = update_job_trigger\n    t5 = delete_job_trigger\n    t6 = delete_job_trigger\n\n    t1 >> t2 >> t3 >> t4 >> t5 >> t6"}], "metadata": {"file_name": "dag_2015_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task execution pattern for cloud infrastructure management, specifically demonstrating how to programmatically interact with Alibaba OSS bucket resources. Develop a simple workflow that shows task dependencies and cloud service interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.alibaba.cloud.operators.oss import AlibabaCloudOSSCreateBucketOperator\nfrom airflow.providers.alibaba.cloud.operators.oss import AlibabaCloudOSSDeleteBucketOperator\nfrom airflow.providers.alibaba.cloud.operators.oss import AlibabaCloudOSSDownloadFileOperator\nfrom airflow.providers.alibaba.cloud.operators.oss import AlibabaCloudOSSUploadFileOperator\n\nBUCKET_NAME = os.environ.get(\"ALIBABA_OSS_BUCKETS\", \"test-oss-bucket\")\nOBJECT_KEY = os.environ.get(\"ALIBABA_OSS_OBJECTS\", \"test-oss-object\")\nUPLOAD_FILE_PATH = os.environ.get(\"ALIBABA_OSS_UPLOAD_FILE_PATH\", \"alibaba.png\")\nDELETE_BUCKET_TIMEOUT = int(os.environ.get(\"ALIBABA_OSS_BUCKET_TIMEOUT\", 300)))\nEXIST_BUCKET_URL = os.environ.get(\"ALIBABA_OSS_EXIST_BUCKET_URL\", \"https://alibaba.png\")\nEXIST_OBJECT_URL = os.environ.get(\"ALIBABA_OSS_EXIST_OBJECT_URL\", \"https://alibaba.png\")\n\n\nwith models.DAG(\n    \"example_oss\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_oss_create_bucket]\n    create_bucket = AlibabaCloudOSSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        region=\"cn-qingdao\",\n    )\n    # [END howto_operator_oss_create_bucket]\n\n    # [START howto_operator_oss_upload_file]\n    upload_file = AlibabaCloudOSSUploadFileOperator(\n        task_id=\"upload_file\",\n        bucket_name=BUCKET_NAME,\n        file=UPLOAD_FILE_PATH,\n        object_name=OBJECT_KEY,\n        region=\"cn-qingdao\",\n    )\n    # [END howto_operator_oss_upload_file]\n\n    # [START howto_operator_oss_download_file]\n    download_file = AlibabaCloudOSSDownloadFileOperator(\n        task_id=\"download_file\",\n        bucket_name=BUCKET_NAME,\n        file=UPLOAD_FILE_PATH,\n        object_name=OBJECT_KEY,\n        dest_path=UPLOAD_FILE_PATH,\n        region=\"cn-qingdao\",\n    )\n    # [END howto_operator_oss_download_file]\n\n    # [START howto_operator_oss_delete_bucket]\n    delete_bucket = AlibabaCloudOSSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        region=\"cn-qingdao\",\n        timeout=DELETE_BUCKET_TIMEOUT,\n    )\n    # [END howto_operator_oss_delete_bucket]\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >> download_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # Test the connection to OSS\n    # [START howto_operator_oss_inspect]\n    oss_inspect = AlibabaCloudOSSUploadFileOperator(\n        task_id=\"oss_inspect\",\n        bucket_name=BUCKET_NAME,\n        file=BUCKET_NAME,\n        object_name=\"oss-inspect.txt\",\n        region=\"cn-qingdao\",\n        inspect_path=\"oss-inspect.txt.inspect\",\n    )\n    # [END howto_operator_oss_inspect]\n\n    # [START howto_operator_oss_list_bucket]\n    oss_list_bucket = AlibabaCloudOSSUploadFileOperator(\n        task_id=\"oss_list_bucket\",\n        bucket_name=BUCKET_NAME,\n        file=BUCKET_NAME + \".list.txt\",\n        object_name=\"list.txt\",\n        region=\"cn-qingdao\",\n        inspect_path=\"list.txt.inspect\",\n    )\n    # [END howto_operator_oss_list_bucket]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_54_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that dynamically creates an SQS queue, publishes multiple messages, and demonstrates different message retrieval strategies using sensors. Implement a complete workflow that covers queue creation, message publishing, message sensing, and queue deletion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.sensors.sqs import SqsSensor, SqsSensorWithProvideCapacity\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_sensor_sqs_def]\ndef _get_queue_arn(queue_name: str) -> str:\n    # Create an SQS queue and return its ARN\n    # Example: arn:aws:sqs:us-east-1:400933821377:my-queue\n    return f\"arn:aws:sqs:us-east-1:{random.randint(1, 9999)}:{queue_name}\"\n\ndef _create_queue(queue_name: str) -> None:\n    # Create an SQS queue and wait until it's ready\n    queue = boto3.resource(\"sqs\")\n    queue_name_arn = _get_queue_arn(queue_name)\n    queue.create_queue(\n        QueueName=queue_name,\n        Attributes={\n            \"DelayStrategy\": \"0\",\n        }\n    ).wait_until_ready()\n\ndef _get_message_body(message_attributes: dict) -> str:\n    # Get the value of the 'MyAttribute' attribute for the given message\n    return message_attributes.get(\"MyAttribute\").get(\"StringValue\")\n\ndef _publish_to_queue(queue_name: str, message: str) -> None:\n    # Publish a message to an SQS queue\n    queue = boto3.resource(\"sqs\")\n    queue_name_arn = _get_queue_arn(queue_name)\n    queue.get_queue_url(QueueName=queue_name_arn)[\"QueueUrl\"]\n    queue.queue_url = queue_name_arn\n    queue.create_message(\n        QueueName=queue_name_arn,\n        MessageBody=message,\n        MessageAttributes={\n            \"MyAttribute\": {\n                \"Type\": \"String\",\n                \"StringValue\": \"Hello World!\",\n                \"BinaryValue\": bytearray(),\n                \"StringList\": [\"value1\", \"value2\"],\n                \"Number\": {\"Type\": \"Number\", \"Value\": 123},\n                \"Bool\": {\"Type\": \"Boolean\", \"Value\": True},\n            }\n        }\n    ).wait_for_completion()\n\n@task\ndef get_messages_at_sqs(queue_arn: str) -> list[dict]:\n    response = boto3.client(\"sqs\").receive_message(QueueUrl=queue_arn)\n    messages = [\n        {\n            \"MessageId\": msg.get(\"MessageId\"),\n            \"QueueUrl\": msg.get(\"QueueUrl\"),\n            \"Body\": msg.get(\"MessageBody\"),\n            \"Attributes\": msg.get(\"Attributes\"),\n        }\n        for msg in response.get(\"Messages\")\n    ]\n    return messages\n\n@task\ndef delete_queue(queue_name: str) -> None:\n    # Delete an SQS queue\n    boto3.resource(\"sqs\").get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n    boto3.resource(\"sqs\").get_queue_url(QueueName=queue_name).get(\"QueueUrl\") \\\n        .delete()\n\nwith DAG(\n    dag_id=\"example_sqs\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    test_queue_name: str = \"test-queue\"\n    create_test_queue = _create_queue(test_queue_name)\n\n    # [START howto_sensor_sqs]\n    follow_test_queue = SqsSensor(\n        task_id=\"follow_test_queue\",\n        queue_url=_get_queue_arn(test_queue_name),\n        # The lambda below will delete the message it receives, so we need to make sure\n        # that we understand the error message and act upon it by clearing the message.\n        # See https://docs.aws.amazon.com/sqs/latest/sqs/tutorial.html\n        poke_interval=5,\n        # The below lambda function is what will delete the message\n        push_function=lambda message, **kwargs: delete_queue(test_queue_name),\n    )\n    # [END howto_sensor_sqs]\n\n    # [START howto_sensor_sqs_provide_capacity]\n    follow_test_queue_with_provide_capacity = SqsSensorWithProvideCapacity(\n        task_id=\"provide_capacity_test_queue\",\n        queue_url=_get_queue_arn(test_queue_name),\n        poke_interval=5,\n        provide_capacity=True,\n    )\n    # [END howto_sensor_sqs_provide_capacity]\n\n    # [START howto_operator_bash]\n    delete_test_queue = BashOperator(\n        task_id=\"delete_test_queue\",\n        bash_command=f\"echo 'Deleting test queue {test_queue_name}'\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_bash]\n\n    chain(\n        create_test_queue,\n        follow_test_queue,\n        follow_test_queue_with_provide_capacity,\n        delete_test_queue,\n    )"}], "metadata": {"file_name": "dag_423_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data ingestion pipeline that transforms a local CSV file containing US state information into a structured BigQuery table using Google Cloud data transfer services. Implement a systematic approach to moving structured data from local storage to cloud-based analytical platforms.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_gcs_to_bigquery\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"usa-states.csv\"\nGCS_OBJECT_NAME = \"gcs/us-states-us-states.csv\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"us_states\"\nSCHEMA = [\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"example_gcs_to_bigquery\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        table_resource={\n            \"schema\": {\"fields\": SCHEMA},\n            \"inputAcceptanceTestConfig\": {\n                \"BigQueryInsertOrder\": \"BigQueryInsertOrder\",\n                \"IgnoreMissingTables\": True,\n            },\n        },\n    )\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_NAME,\n        dst=GCS_OBJECT_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcs_to_bigquery]\n    load_data = BigQueryInsertJobOperator(\n        task_id=\"load_data\",\n        configuration={\n            \"query\": {\n                \"query\": \"LOAD DATA INFILE 'gcs://bucket_name/object_name/' INTO TABLE `bigquery_table`;\",\n                \"useLegacyStagingFields\": False,\n            }\n        },\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_dataset, create_bucket, upload_file]\n        >> create_table\n        # TEST BODY\n        >> load_data\n        # TEST TEARDOWN\n        >> [delete_dataset, delete_bucket]\n    )\n\n    from tests.system.utils import get_test_run\n\n    # This test needs trigger rule as \"all done\" because\n    # of bigquery_to_gcs transfer job, which is causing\n    # open file and closes file after open. Open file is\n    # controlled inside test and not inside test setup,\n    # thus going out of scope. Trigger rule \"all done\"\n    # is to make sure open and close is covered.\n    test_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical process of cloud file operations, including creating a GCS bucket, uploading a local file, downloading the file, and performing cleanup operations. Implement the workflow to demonstrate inter-system file transfer and management using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.s3.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.providers.s3.transfers.s3_to_local import S3ToLocalOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"s3_to_local\"\n\nlocal_file_path = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nremote_file_path = os.environ.get(\"REMOTE_FILE_PATH\", \"remotefile.txt\")\ns3_bucket = os.environ.get(\"S3_BUCKET\", \"s3 bucket name\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"s3\"],\n) as dag:\n    @task\n    def create_s3_bucket():\n        S3Hook().conn.create_bucket(s3_bucket)\n\n    @task\n    def create_connection(conn_id_name):\n        conn = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"s3\",\n            description=\"Connection to S3\",\n            extra={\"bucket\": s3_bucket},\n        )\n        S3Hook().conn.create_connection(conn_id=conn_id_name, bucket=s3_bucket)\n        return conn_id_name\n\n    create_s3_bucket_task = create_s3_bucket()\n    create_connection_task = create_connection(\"s3_conn\")\n\n    # [START howto_transfer_local_to_s3]\n    transfer_local_to_s3 = LocalFilesystemToS3Operator(\n        task_id=\"transfer_local_to_s3\",\n        local_path=local_file_path,\n        remote_path=remote_file_path,\n        s3_conn_id=\"s3_conn\",\n    )\n    # [END howto_transfer_local_to_s3]\n\n    # [START howto_transfer_s3_to_local]\n    transfer_s3_to_local = S3ToLocalOperator(\n        task_id=\"transfer_s3_to_local\",\n        s3_conn_id=\"s3_conn\",\n        local_path=local_file_path,\n        s3_path=remote_file_path,\n    )\n    # [END howto_transfer_s3_to_local]\n\n    delete_connection = BashOperator(\n        task_id=\"delete_connection\",\n        bash_command=f\"airflow providers s3 hooks s3_conn_id=s3_conn delete --conn_id=s3_conn\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        create_s3_bucket_task\n        >> create_connection_task\n        >> transfer_local_to_s3\n        >> transfer_s3_to_local\n        >> delete_connection\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the temp file\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that illustrates the interaction between local file systems and cloud storage using Azure-specific operators for file transfer and deletion. Develop a simple demonstration of how data can be moved and managed programmatically in a cloud infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import AzureADLSWriteOperator\nfrom airflow.providers.microsoft.azure.operators.file import AzureFileCopyOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToAzureADLSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls_to_file_transfer\"\n\nconn = Connection(\n    conn_id=\"my_network_fileshare\",\n    conn_type=\"azure_fileshare\",\n    azure_fileshare_name=\"my_network_fileshare\",\n)\n\nFILE_NAME = \"file.txt\"\nDESTINATION_FILE_NAME = \"file.copy.txt\"\nLOCAL_PATH = \"/tmp/\"\nFULL_LOCAL_PATH = LOCAL_PATH + FILE_NAME\nFULL_DESTINATION_PATH = os.environ.get(\"AZURE_FILESHARE_DESTINATION_PATH\", \"\") + DESTINATION_FILE_NAME\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_network_fileshare():\n        \"\"\"\n        Create a Azure Fileshare in the namespace `default` and access share is `my_network_fileshare`.\n        \"\"\"\n        from airflow.providers.microsoft.azure.operators.adls import ListAzureADLSOperator\n\n        ListAzureADLSOperator(task_id=\"list_adls_objects\", azure_fileshare_name=\"my_network_fileshare\")\n\n        create_task = AzureADLSWriteOperator(\n            task_id=\"create_adls\",\n            azure_fileshare_name=\"my_network_fileshare\",\n            file_path=FULL_LOCAL_PATH,\n            data=f\"test.txt{str(datetime.now())}\",\n        )\n\n        fileshare_copy_task = AzureFileCopyOperator(\n            task_id=\"copy_file_to_fileshare\",\n            source_path=FULL_LOCAL_PATH,\n            destination_path=FULL_DESTINATION_PATH,\n            azure_fileshare_name=\"my_network_fileshare\",\n        )\n\n        [create_task, fileshare_copy_task]\n\n    create_network_fileshare_task = create_network_fileshare()\n    # [START howto_operator_adls_to_file_transfer]\n    transfer_task = AzureFilesystemToAzureADLSOperator(\n        task_id=\"transfer_file_to_adls\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_adls_to_file_transfer]\n    # [START howto_operator_adls_to_file_transfer_async]\n    transfer_task_async = AzureFilesystemToAzureADLSOperator(\n        task_id=\"transfer_file_to_adls_async\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n        deferrable=True,\n    )\n    # [END howto_operator_adls_to_file_transfer_async]\n\n    (\n        create_network_fileshare_task\n        >> transfer_task\n        >> transfer_task_async\n    )\n\n    # Add additional tasks as needed for your workflow\n\n    # [START howto_operator_create_connection]\n    create_conn = AzureFilesystemToAzureADLSOperator(\n        task_id=\"create_conn\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_create_connection]\n\n    # [START howto_operator_delete_connection]\n    delete_conn = AzureFilesystemToAzureADLSOperator(\n        task_id=\"delete_conn\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_delete_connection]\n\n    (\n        create_conn\n        >> delete_conn\n    )\n\n    # Add additional tasks as needed for your workflow\n\n    # [START howto_operator_get_connection]\n    get_conn = AzureFilesystemToAzureADLSOperator(\n        task_id=\"get_conn\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_get_connection]\n\n    # [START howto_operator_get_con\n    get_conn_deferrable = AzureFilesystemToAzureADLSOperator(\n        task_id=\"get_conn_deferrable\",\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n        deferrable=True,\n    )\n    # [END howto_operator_get_connection]\n\n    (\n        get_conn\n        >> get_conn_deferrable\n    )\n\n    # Add additional tasks as needed for your workflow\n\n    # [START howto_operator_set_connection]\n    set_conn = AzureFilesystemToAzureADLSOperator(\n        task_id=\"set_conn\",\n        conn_id=conn,\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_set_connection]\n\n    # [START howto_operator_set_connection_deferrable]\n    set_conn_deferrable = AzureFilesystemToAzureADLSOperator(\n        task_id=\"set_conn_deferrable\",\n        conn_id=conn,\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n        deferrable=True,\n    )\n    # [END howto_operator_set_connection_deferrable]\n\n    (\n        set_conn\n        >> set_conn_deferrable\n    )\n\n    # Add additional tasks as needed for your workflow\n\n    # [START howto_operator_del_connection]\n    del_conn = AzureFilesystemToAzureADLSOperator(\n        task_id=\"del_conn\",\n        conn_id=conn,\n        local_path=FULL_LOCAL_PATH,\n        azure_fileshare_name=\"my_network_fileshare\",\n        file_path=FILE_NAME,\n    )\n    # [END howto_operator_del_connection]\n\n    (\n        del_conn\n    )"}], "metadata": {"file_name": "dag_963_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that retrieves tweets from different Twitter accounts, transforms the raw data through cleaning and analysis stages, and prepares aggregated insights for storage. Develop a systematic approach to collecting, processing, and summarizing social media data with clear transformation steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, dag\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dagrun import DAGRun\nfrom airflow.settings import Session\nfrom airflow.tracking import Model\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflowx.google.cloud import service_acc\nfrom airflowx.google.cloud.twitter import TwitterDag\nfrom airflowx.google.cloud.util import GCPeyesta\nfrom airflowx.utils import ENV_ID\n\nlog = logging.getLogger(__name__)\n\nVIEW_NAME = \"view_twitter_dag\"\n\n\ndef setup_dag_connections(dag: TwitterDag):\n    conn = Connection(\n        conn_id=\"twitter_conn\",\n        conn_type=\"google_cloud_platform\",\n        extra=json.dumps({\"scope\": \"https://www.googleapis.com/auth/cloud-platform\"}),\n    )\n    session = Session()\n    session.add(conn)\n    session.commit()\n\n    dag.conf[Model(VIEW_NAME).name] = {\n        \"conn_id\": \"twitter_conn\",\n    }\n\n\n@task\ndef create_view(view_name, connection_id, location):\n    gcp = GCPeyesta()\n    gcp.create_view(\n        project_id=os.environ.get(\"GCP_PROJECT\"),\n        location=location,\n        body={\"name\": view_name, \"connectionId\": connection_id},\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_view(view_name, connection_id, location):\n    client = service_acc.Client()\n    client.del_view(\n        project_id=os.environ.get(\"GCP_PROJECT\"),\n        location=location,\n        connection_id=connection_id,\n        name=view_name,\n    )\n\n\nwith dag(\n    dag_id=\"example_twitter_dag\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"twitter\"],\n):\n    connect = TwitterDag.connect\n    root = TwitterDag()\n    root.start = create_view(\"twitter_view\", root.connect, \"us\")\n    root.tweets = root.load_tweets(\n        {\n            \"max_results\": 100,\n            \"track\": \"pizza,food,food delivery,food delivery app,food delivery service\",\n            \"language\": \"en\",\n            \"place_id\": \"\",\n        },\n        path=\"api/v1.1/search/tweets\",\n    )\n    root.aggs = root.tweets | root.load_aggs({\"date\": \"created_at\", \"keyword\": \"full_text\"}, \"count\")\n    root.end = root.aggs | root.create_view_view(VIEW_NAME, root.connect, \"us\")\n    root.trigger = root.end | root.connect[connect](connection_id=root.connect, task_id=\"trigger\")\n\n    chain(\n        # TEST SETUP\n        root.connect[connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect][root.connect"}], "metadata": {"file_name": "dag_426_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an event-driven Airflow DAG that demonstrates message production and consumption using Kafka, with a focus on creating a sensor that triggers specific actions based on number characteristics. Implement a data pipeline that showcases asynchronous message processing and conditional event handling.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport random\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.kafka.operators.kafka import KafkaGenerateOffsetsOperator, KafkaOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_kafka_operator]\n# [START howto_start_offset_generation]\ndef produce_to_kafka(message: str, topic: str) -> None:\n    # Create a key value pair and push it to a specific topic\n    # You can push as many messages as you want\n    KafkaOperator(\n        task_id=\"produce_task\",\n        kafka_config={\"bootstrap.servers\": \"kafka_host\"},\n        topic=topic,\n        value=message,\n        generate_offsets=False,\n        deferrable=False,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\ndef consume_from_kafka() -> None:\n    # Create a consumer instance and consume messages until no more messages are available\n    while True:\n        messages = KafkaOperator(\n            task_id=\"consume_task\",\n            kafka_config={\"bootstrap.servers\": \"kafka_host\"},\n            consumer_group=\"test-consumer-group\",\n            # ExactlyOnce guarantees that a message is deleted from the broker once consumed once.\n            # If you want messages to be easily deleted between tests, then leave this option False\n            auto_offset_reset=\"latest\",\n            # ExactlyOnce will automatically call produce_to_kafka for deleted messages\n            consume_count=-1,\n        )\n\n\n# [END howto_start_offset_generation]\n# [END howto_kafka_operator]\n\n\n# [START howto_kafka_sensor]\ndef consume_from_kafka_sensor() -> None:\n    # Create a sensor instance and check if a message is consumed\n    KafkaOperator(\n        task_id=\"consume_sensor_task\",\n        kafka_config={\"bootstrap.servers\": \"kafka_host\"},\n        consumer_group=\"test-consumer-group\",\n        topic=\"test-topic\",\n        poke_interval=10,\n    ) >> PythonOperator(task_id=\"consume_sensor_python_task\", python_callable=consume_from_kafka())\n\n\n# [END howto_kafka_sensor]\n\n\nDAG_ID = \"kafka\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2020, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    # [START howto_kafka_generate_offsets]\n    generate_kafka_offsets = KafkaGenerateOffsetsOperator(\n        task_id=\"generate_kafka_offsets\", kafka_config={\"bootstrap.servers\": \"kafka_host\"}, topic=\"test-topic\"\n    )\n    # [END howto_kafka_generate_offsets]\n\n    # [START task_out]\n    @task\n    def task_out(message: str) -> None:\n        print(message)\n\n    # [END task_out]\n    # [START task_in]\n    @task\n    def task_in(message: str) -> None:\n        print(message + str(random.random()))\n\n    # [END task_in]\n\n    produce_to_kafka(message=\"test message 1\", topic=\"test-topic-1\")\n    produce_to_kafka(message=\"test message 2\", topic=\"test-topic-2\")\n    consume_from_kafka()\n    consume_from_kafka_sensor()\n    task_in(task_out(\"test string\"))"}], "metadata": {"file_name": "dag_1942_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure-as-code workflow for dynamically creating and managing a distributed caching service with precise configuration controls. Develop a DAG that illustrates how to programmatically set up a scalable in-memory data storage solution with specific node and memory configurations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.ssm import SSMHook\nfrom airflow.providers.amazon.aws.operators.ssm import SSMPutParameterOperator\nfrom airflow.providers.amazon.aws.operators.ssm import SSMGetLastUsedOperator\nfrom airflow.providers.amazon.aws.sensors.ssm import SSMSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\nROLE_ARN_KEY = \"ROLE_ARN\"\nDAG_ID = \"example_ssminit\"\nEXECUTION_ROLE_NAME = \"SSMInitExecutionRole\"\nAWS_DEFAULT_REGION = \"us-east-1\"\n\n# [START howto_ssminit]\n# Command to run during system initialization, runs with execute_as_role role\n# https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-init-actions.html\nINITIALIZE_INSTANCE_COMMAND = \"echo 'Some reasonable initialization'\"\n\n@task(task_id=\"initialize_instance\")\ndef initialize_instance(instance_id: str):\n    # Need to attach the root volume to the instance\n    log.info(f\"Attaching volume to instance {instance_id}\")\n    boto3.client(\"ec2\").attach_volume(VolumeId=os.environ[\"AWS_ECS_CONTAINER_TASK_ROLE_ARN\"],\n                                       InstanceId=instance_id,\n                                       Device=\"/dev/xvda\")\n\n    # Need to wait for the volume to be attached to the instance\n    log.info(\"Waiting for volume to be attached to instance\")\n    boto3.client(\"ec2\").wait_for_object(InstanceId=instance_id, Filter=[{\"Name\": \"volume-id\", \"Values\": [os.environ[\"AWS_ECS_CONTAINER_TASK_ROLE_ARN\"]]}])\n\n    # Next we can detach any possibly existing volume from the instance\n    log.info(\"Detaching any existing volume from instance\")\n    boto3.client(\"ec2\").detach_volume(VolumeId=os.environ[\"AWS_ECS_CONTAINER_TASK_ROLE_ARN\"])\n\n    # Finally we can delete the existing volume which makes instance initialization faster\n    log.info(\"Deleting volume\")\n    boto3.client(\"ec2\").delete_volume(VolumeId=os.environ[\"AWS_ECS_CONTAINER_TASK_ROLE_ARN\"])\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_ssminit_instance_id]\n    instance_id = \"your-instance-id\"\n    # [END howto_ssminit_instance_id]\n\n    conn = Connection(\n        conn_id=\"aws_ecs\",\n        task_id=\"aws_aws-ecs-containers\",\n        schema=\"airflow.providers.amazon.aws.hooks.ecs\",\n        conn_type=\"ecs\",\n        description=\"Airflow Task to manage AWS ECS\",\n        host=os.environ.get(\"AWS_ENDPOINT\"),\n        port=4567,\n        login=os.environ.get(\"AWS_ECS_EXECUTOR_ID\"),\n        password=os.environ.get(\"AWS_ECS_EXECUTOR_SECRET\"),\n        region=AWS_DEFAULT_REGION,\n        params={\n            \"register_task_role\":\n                f\"arn:{ boto3.Session().region_name }:iam/{EXECUTION_ROLE_NAME}:role/ecsInit\"\n        },\n    )\n\n    # [START howto_ssminit_ssm_hook]\n    ssm_hook = SSMHook(\n        service=\"ssm\",\n        path=\"/my/init/\",\n        name=\"my-param\",\n        description=\"Some description\",\n        type=\"Number\",\n        minimum=1,\n        maximum=1,\n        allow_no_value=True,\n        no_value_string=\"None\",\n        functional_role=True,\n        source_outdated=True,\n        execution_role=conn.params[\"register_task_role\"],\n        external_id=os.environ.get(\"AWS_ECS_EXTERNAL_ID\"),\n        use_aws_compatible_prefix=True,\n    )\n    # [END howto_ssminit_ssm_hook]\n\n    # [START howto_ssminit_put_param]\n    put_param_task = SSMPutParameterOperator(\n        task_id=\"put_param_task\",\n        ssm_hook=ssm_hook,\n        data=json.dumps({\"Initializers\": [f\"file://{__file__};{INITIALIZE_INSTANCE_COMMAND}\"]}),\n        path=\"/my/init/\",\n        name=\"my-param\",\n        functional_role=True,\n    )\n    # [END howto_ssminit_put_param]\n\n    # [START howto_ssminit_get_last_used]\n    get_last_used = SSMGetLastUsedOperator(\n        task_id=\"get_last_used\",\n        ssm_hook=ssm_hook,\n        last_used_last_run_time=True,\n    )\n    # [END howto_ssminit_get_last_used]\n\n    # [START howto_ssminit_sensor]\n    wait_for_last_use = SSMSensor(\n        task_id=\"wait_for_last_use\",\n        ssm_hook=ssm_hook,\n        last_used_last_run_time=True,\n        timeout=60 * 20,\n        mode=\"EXCLUSIVE\",\n        trigger=\"all\",\n        to_be_true=True,\n    )\n    # [END howto_ssminit_sensor]\n\n    # [START howto_ssminit_add_to_run_list]\n    add_to_run_list(instance_id, conn)\n    # [END howto_ssminit_add_to_run_list]\n\n    # [START howto_ssminit_remove_from_run_list]\n    remove_from_run_list(instance_id, conn)\n    # [END howto_ssminit_remove_from_run_list]\n\n    dag.add_task(conn)"}], "metadata": {"file_name": "dag_1588_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a MySQL database to Google Cloud Storage using an automated, one-time batch process. Create a DAG that demonstrates cloud data extraction and storage with built-in bucket management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.mysql_to_gcs import MySQLToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"mysql_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"test_file\"\n\nSQL = \"SELECT * FROM test_table\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mysql_to_gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    # [START howto_operator_mysql_to_gcs]\n    mysql_to_gcs = MySQLToGCSOperator(\n        task_id=\"mysql_to_gcs\",\n        sql=SQL,\n        target_bucket=BUCKET_NAME,\n        table=\"test_table\",\n        file_type=\"csv\",\n        replace=True,\n        gzip=False,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_mysql_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> mysql_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_220_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates parameter passing and environment variable testing in Airflow, showcasing different methods of injecting runtime configuration into tasks. Create a DAG that illustrates how parameters can be dynamically passed and accessed across Python and Bash tasks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task\ndef generate_python_output(value: str):\n    os.environ[\"TNT_TEST_VAR\"] = value\n    print(f\"TNT_TEST_VAR value: {os.environ.get('TNT_TEST_VAR')}\")\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_tnt_test_var\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"retries\": 1},\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    generate_python_output(\"test_value\") >> BashOperator(bash_command=\"echo $TNT_TEST_VAR\")"}], "metadata": {"file_name": "dag_339_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data movement process that extracts salary records from a BigQuery table and loads them into a PostgreSQL destination table. Develop a DAG that handles data transfer with configurable parameters and includes robust error handling and resource management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_postgres_to_sql\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    connection_id = models.Property(\"Connection ID\")\n    sql_table = models.Property(\"Name of the SQL table\")\n    postgres_table = models.Property(\"Name of the postgres table\")\n    sql_string = SQLExecuteQueryOperator.task_method_call(\n        task_id=\"sql_method_call_task\",\n        python_callable=PostgresHook().write_records,\n        op_kwargs={\n            \"sql\": f\"SELECT * FROM {postgres_table};\",\n            \"conn\": connection_id,\n        },\n    )\n    # [START howto_postgres_to_sql]\n    postgres_to_sql_task = PostgresOperator(\n        task_id=\"postgres_to_sql_task\",\n        sql=f\"SELECT * FROM {sql_table};\",\n        conn=connection_id,\n    )\n    # [END howto_postgres_to_sql]\n    postgres_to_sql_operator = SQLExecuteQueryOperator(\n        task_id=\"postgres_to_sql_operator\",\n        sql=f\"SELECT * FROM {sql_table};\",\n        conn=connection_id,\n    )\n    postgres_to_sql_short_circuit = PostgresOperator(\n        task_id=\"postgres_to_sql_short_circuit\",\n        sql=f\"SELECT * FROM {postgres_table};\",\n        conn=connection_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [START howto_postgres_to_sql_operator]\n    postgres_to_sql_operator_2 = SQLExecuteQueryOperator(\n        task_id=\"postgres_to_sql_operator_2\",\n        sql=f\"SELECT * FROM {sql_table};\",\n        conn=connection_id,\n    )\n    # [END howto_postgres_to_sql_operator]\n\n    (postgres_to_sql_task >> postgres_to_sql_operator >> postgres_to_sql_short_circuit)\n    (postgres_to_sql_operator_2 >> sql_string)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_823_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that demonstrates dynamic table creation and transformation in BigQuery with multiple data object types. Implement a workflow that shows how to create, modify, and manage different table representations including standard tables, views, and materialized views.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryDeleteTableOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_bigquery import MsSqlToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_mssql_to_bigquery\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nMSSQL_TABLE_NAME = \"employees\"\nGCS_BUCKET_PATH = f\"gs://{BUCKET_NAME}/\"\nGCS_FILE_LOCAL_PATH = \"/tmp/test_file.csv\"\nTABLE_NAME = \"employees_2\"\nSCHEMA = [\n    {\"name\": \"employee_id\", \"type\": \"INT\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"salary\", \"type\": \"INT\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"department_id\", \"type\": \"INT\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mssql\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=GCS_FILE_LOCAL_PATH,\n        dst=GCS_BUCKET_PATH,\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        table_resource={\n            \"schema\": {\"fields\": SCHEMA},\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"destinationTable\": {\"projectId\": PROJECT_ID, \"datasetId\": DATASET_NAME},\n                \"writeDisposition\": \"WRITE_TRUNCATE\",\n            },\n        },\n    )\n\n    insert_query_job = BigQueryInsertJobOperator(\n        task_id=\"insert_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"SELECT * FROM `test-bq-mssql`.\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_operator_mssql_to_bigquery]\n    ms_sql_to_bigquery = MsSqlToBigQueryOperator(\n        task_id=\"ms_sql_to_bigquery\",\n        sql=\"SELECT * FROM [test-mssql]\",\n        destination_table={\"dataset_id\": DATASET_NAME, \"table_id\": TABLE_NAME},\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_operator_mssql_to_bigquery]\n\n    select_query = BigQuerySelectOperator(\n        task_id=\"select_query\",\n        sql=\"SELECT * FROM [test-mssql]\",\n        destination_table={\"dataset_id\": DATASET_NAME, \"table_id\": TABLE_NAME + \"_2\"},\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >> create_dataset\n        >> create_table\n        >> insert_query_job\n        >> ms_sql_to_bigquery\n        >> select_query\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    # [START howto_operator_delete_dataset]\n    delete_dataset_now = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset_now\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_delete_dataset]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_151_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing workflow that shows how to execute database transactions using Oracle connections, highlighting different parameter input strategies for stored procedures. Create a DAG that explores various methods of executing database operations with input and output parameter handling.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport logging\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.settings import Session, json\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n# [START howto_oracle_install_module]\n# Import specific installs from the Airflow TestingModuleInstallProviderClass\nfrom airflow.testing.providers.common.sql.oracle.install import test_oracle_operator  # noqa\nfrom airflow.testing.providers.common.sql.oracle.utils import TestOracleUtils  # noqa\n\n# [END howto_oracle_install_module]\n\n\n# [START howto_oracle_dag]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_this = SQLExecuteQueryOperator(\n        task_id=\"run_this\",\n        sql=\"SELECT 1 FROM dual\",\n        conn=\"oracle_default\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    @task\n    def create_test_connection():\n        test_connection = Connection(\n            conn_id=\"oracle_test\",\n            conn_type=\"oracle\",\n            login=\"test\",\n            password=\"test\",\n            host=\"localhost\",\n            port=1521,\n            extra=json.dumps({\"sid\": \"orcl\"}),\n        )\n        session = Session()\n        session.add(test_connection)\n        session.commit()\n\n    @task\n    def create_test_role():\n        test_role = Role()\n        test_role.role_name = \"oracle_test_role\"\n        test_role.handler = \"python:airflow.providers.common.sql.operators.oracle.OraclesTestHandler\"\n        session = Session()\n        session.add(test_role)\n        session.commit()\n\n    @task\n    def enable_oracle_test_trigger_rule(ti):\n        ti.xcom_pull(task_ids=[\"create_test_connection\"], key=\"return_value\")\n        session = Session()\n        query = \"SELECT 1 FROM dual\"\n        trigger_id = session.query(Trigger).filter(Trigger.task_id == query).first().id\n        Trigger.trigger_rule = TriggerRule.ALL_DONE\n        session.merge(session.query(Trigger).filter(Trigger.task_id == query).first())\n        session.commit()\n\n    @task\n    def populate_test_connection_in_test_role(ti):\n        test_connection_in_test_role = Connection(\n            conn_id=\"oracle_test\",\n            conn_type=\"oracle\",\n            login=\"test\",\n            password=\"test\",\n            host=\"localhost\",\n            port=1521,\n            extra=json.dumps({\"sid\": \"orcl\"}),\n        )\n        session = Session()\n        session.add(test_connection_in_test_role)\n        session.commit()\n\n    create_test_role >> enable_oracle_test_trigger_rule >> create_test_connection >> run_this\n    test_oracle_operator(conn_id=\"oracle_test\", sql=\"SELECT 1 FROM dual\", task_id=\"test_operator\")\n# [END howto_oracle_dag]\n\nif os.environ.get(\"TEST_ENV_ID\"):\n    dag.conf[Param.name] = os.environ.get(\"TEST_ENV_ID\")\n\n# [START howto_oracle_get_connections]\nwith DAG(\n    dag_id=\"test_oracle_get_connections\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def get_connections(_env_id):\n        session = Session()\n        connections = session.query(Connection).filter(Connection.extra == f'{{\"sid\":\"{_env_id}\"}}')\n        for c in connections:\n            print(c.id, c.conn_id, c.login, c.host, c.port, c.extra)\n\n    get_connections(env_id=ENV_ID)\n# [END howto_oracle_get_connections]\n\n# [START howto_oracle_multiple_connections]\nwith DAG(\n    dag_id=\"test_oracle_multiple_connections\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def get_connections1(_env_id):\n        session = Session()\n        connections = session.query(Connection).filter(Connection.extra == f'{{\"sid\":\"{_env_id}\"}}')\n        for c in connections:\n            print(c.id, c.conn_id, c.login, c.host, c.port, c.extra)\n\n    @task\n    def get_connections2(_env_id):\n        session = Session()\n        connections = session.query(Connection).filter(Connection.extra == f'{{\"sid\":\"{_env_id}\"}}')\n        for c in connections:\n            print(c.id, c.conn_id, c.login, c.host, c.port, c.extra)\n\n    get_connections1(env_id=ENV_ID) >> get_connections2(env_id=ENV_ID)\n# [END howto_oracle_multiple_connections]\n\n# [START howto_oracle_warehouse]\nwith DAG(\n    dag_id=\"test_oracle_warehouse\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def get_warehouses(_env_id):\n        session = Session()\n        warehouses = session.query(Connection).filter(Connection.extra == f'{{\"warehouse\":\"US\"}}')\n        for w in warehouses:\n            print(w.id, w.conn_id, w.login, w.host, w.port, w.extra)\n\n    get_warehouses(env_id=ENV_ID)\n# [END howto_oracle_warehouse]\n\n# [START howto_oracle_multiple_warehouse]\nwith DAG(\n    dag_id=\"test_oracle_multiple_warehouse\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def get_warehouses1(_env_id):\n        session = Session()\n        warehouses = session.query(Connection).filter(Connection.extra == f'{{\"warehouse\":\"US\"}}')\n        for w in warehouses:\n            print(w.id, w.conn_id, w.login, w.host, w.port, w.extra)\n\n    @task\n    def get_warehouses2(_env_id):\n        session = Session()\n        warehouses = session.query(Connection).filter(Connection.extra == f'{{\"warehouse\":\"US\"}')\n        for w in warehouses:\n            print(w.id, w.conn_id, w.login, w.host, w.port, w.extra)\n\n    get_warehouses1(env_id=ENV_ID) >> get_warehouses2(env_id=ENV_ID)\n# [END howto_oracle_multiple_warehouse]\n\n# [START howto_oracle_test_handler]\nclass OracleTestHandler(logging.Handler):\n    def __init__(self, task_instance, *args, **kwargs):\n        super(OracleTestHandler, self).__init__(*args, **kwargs)\n        self.task_instance = task_instance\n\n    def emit(self, record):\n        print(f\"print log in {self.task_instance.task.dag_id}, {self.task_instance.task_id}, \" f\"{record.msg}\")\n# [END howto_oracle_test_handler]\n\n# [START howto_oracle_multiple_test_handler]\nclass OracleTestHandler2(logging.Handler):\n    def __init__(self, task_instance, *args, **kwargs):\n        super(OracleTestHandler2, self).__init__(*args, **kwargs)\n        self.task_instance = task_instance\n\n    def emit(self, record):\n        print(f\"print log in {self.task_instance.task.dag_id}, {self.task_instance.task_id}, \" f\"{record.msg}\")\n# [END howto_oracle_multiple_test_handler]\n\n\nif os.environ.get(\"TEST_ENV_ID\"):\n    dag.conf[Param.name] = os.environ.get(\"TEST_ENV_ID\")\n\n# [START howto_oracle_named_connection]\nwith DAG(\n    dag_id=\"test_oracle_named_connection\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def create_named_connection(_env_id):\n        session = Session()\n        test_named_connection = Connection(\n            conn_id=\"oracle_test_named\",\n            conn_type=\"oracle\",\n            login=\"test\",\n            password=\"test\",\n            host=\"localhost\",\n            port=1521,\n            extra=json.dumps({\"sid\": f\"{_env_id}_named\"}),\n        )\n        session.add(test_named_connection)\n        session.commit()\n\n    @task\n    def check_out_default_connection(_env_id):\n        session = Session()\n        default_connection = session.query(Connection).filter(Connection.conn_id == \"oracle_default\")\n        print(f\"Default connection id is {default_connection.first().id}\")\n\n    @task\n    def check_out_named_connection(_env_id):\n        session = Session()\n        named_connection = session.query(Connection).filter(Connection.conn_id == \"oracle_test_named\")\n        print(f\"Named connection id is {named_connection.first().id}\")\n\n    create_named_connection(env_id=ENV_ID) >> check_out_default_connection() >> check_out_named_connection(\n        env_id=ENV_ID\n    )\n# [END howto_oracle_named_connection]\n\n# [START howto_oracle_multiple_named_connection]\nwith DAG(\n    dag_id=\"test_oracle_multiple_named_connection\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n) as dag2:\n    @task\n    def create_named_connections(_env_id):\n        session = Session()\n        test_named_connection1 = Connection(\n            conn_id=\"oracle_test_named1\",\n            conn_type=\"oracle\",\n            login=\"test\",\n            password=\"test\",\n            host=\"localhost\",\n            port=1521,\n            extra=json.dumps({\"sid\": f\"{_env_id}_named1\"}),\n        )\n        test_named_connection2 = Connection("}], "metadata": {"file_name": "dag_708_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data workflow that illustrates how different data assets can be managed and scheduled using modern asset-based decorators and S3 storage. Create a pipeline that shows the relationship between data producers and consumers with minimal but clear processing steps.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@dag(\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"provenance\"],\n)\ndef example_provenance_decorator():\n    # [START howto_operator_provenance]\n    @task.provide_context()\n    def run_this(ctx):\n        print(f\"Running {ctx.task} at {ctx.dag.dttm}\")\n\n    # [END howto_operator_provenance]\n\n    run_this()\n\n\nexample_provenance_decorator()"}], "metadata": {"file_name": "dag_1844_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I modify the given Python code to handle a list of dictionaries with potential missing keys? When processing each dictionary, I want to ensure that if a key is missing, it returns a default value instead of throwing an error.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.edgedb.utils import get_edgedb_conn\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.edgedb import EDGEDB_CONNECTION_ID\nfrom system.openlineage.operator import OpenLineageOperator\n\nDAG_ID = \"example_edgedb\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"edgedb\"],\n) as dag:\n    # [START howto_edgedb_operator]\n    @task.edgedb(task_id=\"success_task\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def create_person(edgedb_conn, name, age):\n        with edgedb_conn:\n            person = edgedb.Person()\n            person.name = name\n            person.age = age\n            person.save()\n\n    @task.edgedb(task_id=\"delete_person\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def delete_person(edgedb_conn, person_id):\n        with edgedb_conn:\n            query = \"\"\"\n                MATCH (p:Person {id: $person_id})\n                DELETE p\n            \"\"\"\n            edgedb_conn.execute(query, person_id=person_id)\n\n    @task.edgedb(task_id=\"insert_friend\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def insert_friend(edgedb_conn, person_id, friend_name, friend_age):\n        with edgedb_conn:\n            friend = edgedb.Person()\n            friend.name = friend_name\n            friend.age = friend_age\n            friend.set_friends\u4e4b\u4eba(person_id)\n            friend.save()\n\n    @task.edgedb(task_id=\"get_person\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def get_person(edgedb_conn, person_id):\n        with edgedb_conn:\n            query = \"MATCH (p:Person {id: $person_id}) RETURN p\"\n            result = edgedb_conn.execute(query, person_id=person_id)\n            return result[0]\n\n    @task.edgedb(task_id=\"query_person\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def query_person(edgedb_conn):\n        with edgedb_conn:\n            query = \"query { names { name }, ages { age } }\"\n            result = edgedb_conn.execute(query)\n            return result\n\n    @task.edgedb(task_id=\"drop_schema\", extra={\"task_name\": \"example_edgedb_operator\"})\n    def drop_schema(edgedb_conn, schema_name):\n        with edgedb_conn:\n            query = f\"DROP SCHEMA {schema_name} CASCADE\"\n            edgedb_conn.execute(query)\n\n    drop_schema_schema_name = drop_schema.extra[\"task_name\"]\n\n    create_person_schema_name = create_person.extra[\"task_name\"]\n    delete_person_schema_name = delete_person.extra[\"task_name\"]\n    insert_friend_schema_name = insert_friend.extra[\"task_name\"]\n    get_person_schema_name = get_person.extra[\"task_name\"]\n    query_person_schema_name = query_person.extra[\"task_name\"]\n\n    # [END howto_edgedb_operator]\n\n    chain(\n        # [START howto_edgedb_templated_operator]\n        create_person(\n            edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID),\n            name=\"One\",\n            age=25,\n        ),\n        create_person(\n            edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID),\n            name=\"Two\",\n            age=22,\n        ),\n        # [END howto_edgedb_templated_operator]\n        delete_person(edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID), person_id=\"{{ task_instance.xcom_pull(task_ids='create_person', key='return_value')['id'] }}\"),\n        insert_friend(edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID), person_id=\"{{ task_instance.xcom_pull(task_ids='create_person', key='return_value')['id'] }}\", friend_name=\"Three\", friend_age=23),\n        get_person(edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID), person_id=\"{{ task_instance.xcom_pull(task_ids='create_person', key='return_value')['id'] }}\"),\n        query_person(edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID)),\n        # [START howto_edgedb_operator_async]\n        BashOperator(\n            task_id=\"insert_friend_async\",\n            bash_command=f\"sleep 5 && python -m unittest tests.providers.standard.utils.edgedb.test_edgedb_operator {insert_friend_schema_name}\",\n            dag=dag,\n            trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n        ),\n        # [END howto_edgedb_operator_async]\n        OpenLineageOperator(\n            task_id=\"lineage_insert_friend_async\",\n            conn_id=\"edgedb_default\",\n            resource=\"{{ task_instance.xcom_pull(task_ids='insert_friend_async', key='return_value') }}\",\n            dag=dag,\n        ),\n        get_person_schema_name(),\n        query_person_schema_name(),\n        # [START howto_edgedb_drop_operator]\n        drop_schema(\n            edgedb_conn=get_edgedb_conn(EDGEDB_CONNECTION_ID),\n            schema_name=drop_schema_schema_name,\n        )\n        # [END howto_edgedb_drop_operator]\n        # [START howto_edgedb_trigger_operator]\n        BashOperator(\n            task_id=\"trigger_drop_schema\",\n            bash_command=f\"sleep 5 && python -m unittest tests.providers.standard.utils.edgedb.test_edgedb_operator {drop_schema_schema_name}\",\n            dag=dag,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END howto_edgedb_trigger_operator],\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that uploads a local file to Azure Data Lake Storage and then immediately deletes the uploaded file. Create a DAG that demonstrates file movement and cleanup in a cloud storage environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import AzureDataLakeDeleteOperator\nfrom airflow.providers.microsoft.azure.operators.azure_fileshare import AzureFileShareCreateOperator\nfrom airflow.providers.microsoft.azure.transfers.azure_fileshare_to_adls import AzureFileShareToADLSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls_to_afs\"\n\nconn = Connection(\n    conn_id=\"fileshare\",\n    conn_type=\"azure_fileshare\",\n    extra={\"user\": \"<your fileshare user>\", \"shared_access_key\": \"your shared access key\"},\n)\n\nFILE_SHARE_NAME = f\"fileshare_{ENV_ID}\"\nAZURE_DATA_LAKE_PATH = f\"azure_data_lake/{ENV_ID}\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=TriggerRule.ALL_DONE,\n    tags=[\"example\"],\n) as dag:\n    create_fileshare = AzureFileShareCreateOperator(task_id=\"create_fileshare\", share_name=FILE_SHARE_NAME)\n\n    @task\n    def create_adls():\n        from airflow.providers.microsoft.azure.transfers.adls_to_s3 import ADLSToS3Operator\n\n        ADLSToS3Operator(task_id=\"create_adls\", path=AZURE_DATA_LAKE_PATH)\n\n    create_adls_path = create_adls()\n\n    move_file = AzureFileShareToADLSOperator(\n        task_id=\"move_file_to_adls\",\n        fileshare_name=FILE_SHARE_NAME,\n        adls_path=AZURE_DATA_LAKE_PATH,\n    )\n\n    delete_fileshare = AzureDataLakeDeleteOperator(\n        task_id=\"delete_fileshare\", share_name=FILE_SHARE_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    create_connection = Connection(\n        conn_id=\"fileshare\",\n        conn_type=\"azure_fileshare\",\n        extra={\"user\": \"<your fileshare user>\", \"shared_access_key\": \"your shared access key\"},\n    )\n\n    # [START howto_operator_azure_fileshare_to_adls]\n    (\n        create_fileshare\n        >> create_adls()\n        >> create_adls_path\n        >> move_file\n        >> create_connection\n        >> [delete_fileshare, create_adls]\n    )\n    # [END howto_operator_azure_fileshare_to_adls]"}], "metadata": {"file_name": "dag_1750_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data processing pipeline that demonstrates Google Cloud Dataflow's workflow for batch text analysis, using a word count example with a sample text file. Design an Airflow DAG that manages the entire lifecycle of creating, running, and deleting a Dataflow pipeline while handling Google Cloud Storage operations.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreateApplicationOperator,\n    DataflowCreatePipelineOperator,\n    DataflowDeletePipelineOperator,\n    DataflowDeleteApplicationOperator,\n    DataflowRunPipelineOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"dataflow_java_pipeline\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nAPPLICATION_ID = f\"app_{DAG_ID}_{ENV_ID}\"\nPIPELINE_ID = f\"pipe_{DAG_ID}_{ENV_ID}\"\nPIPELINE_NAME = f\"gs://{BUCKET_NAME}/{PIPELINE_ID}\"\n\n# File containing a Java pipeline which counts words in a file.\nPIPELINE_FILE = \"count_words_in_file.java\"\n\n# JAR file containing a Java pipeline which counts words in a file.\nPIPELINE_JAR = \"count_words_in_file.jar\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataflow\", \"java\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_application = DataflowCreateApplicationOperator(\n        task_id=\"create_application\",\n        project_id=PROJECT_ID,\n        application_id=APPLICATION_ID,\n        location=LOCATION,\n    )\n\n    create_pipeline = DataflowCreatePipelineOperator(\n        task_id=\"create_pipeline\",\n        project_id=PROJECT_ID,\n        application_id=APPLICATION_ID,\n        pipeline_id=PIPELINE_ID,\n        pipeline_file=PIPELINE_FILE,\n        location=LOCATION,\n    )\n\n    # [START how_to_dataflow_run_pipeline]\n    run_pipeline = DataflowRunPipelineOperator(\n        task_id=\"run_pipeline\",\n        project_id=PROJECT_ID,\n        application_id=APPLICATION_ID,\n        pipeline_id=PIPELINE_ID,\n    )\n    # [END how_to_dataflow_run_pipeline]\n\n    # [START how_to_dataflow_delete_pipeline]\n    delete_pipeline = DataflowDeletePipelineOperator(\n        task_id=\"delete_pipeline\",\n        project_id=PROJECT_ID,\n        application_id=APPLICATION_ID,\n        pipeline_id=PIPELINE_ID,\n    )\n    # [END how_to_dataflow_delete_pipeline]\n\n    # [START how_to_dataflow_delete_application]\n    delete_application = DataflowDeleteApplicationOperator(\n        task_id=\"delete_application\",\n        project_id=PROJECT_ID,\n        application_id=APPLICATION_ID,\n    )\n    # [END how_to_dataflow_delete_application]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_application\n        >> create_pipeline\n        # TEST BODY\n        >> run_pipeline\n        # TEST TEARDOWN\n        >> delete_pipeline\n        >> delete_application\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the task from the schedule\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1216_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates converting text to speech using Google Cloud Text-to-Speech service and storing the audio file in a Google Cloud Storage bucket. The DAG should showcase creating a bucket, generating an audio file from sample text, and then cleaning up the bucket.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Regionality\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.text_gcs import CloudTextGCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_gcs import (\n    TextGCSCreateAudioFileOperator,\n    TextGCSCreateConversationFileOperator,\n    TextGCSCreateDiarizationFileOperator,\n    TextGCSCreateExpressionsFileOperator,\n    TextGCSCreateVideoFileOperator,\n    TextGCSDeleteAudioFileOperator,\n    TextGCSDeleteConversationFileOperator,\n    TextGCSDeleteDiarizationFileOperator,\n    TextGCSDeleteExpressionsFileOperator,\n    TextGCSDeleteVideoFileOperator,\n    TextGCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"text_gcs_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"output_1.txt\"\nCONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"text-gcs-to-gcs\"],\n) as dag:\n    create_connection = Connection(\n        conn_id=CONNECTION_ID,\n        conn_type=\"google_cloud_platform\",\n        extra={\n            \"project_id\": PROJECT_ID,\n            \"location\": \"us-central1\",\n            \"scope\": \"us-central1\",\n            \"uri\": \"gs://cloud-samples-data/text/you_say_i_say/you_say_i_say.txt\",\n        },\n    )\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_temp_diarization_bucket():\n        # [START create_temp_diarization_bucket]\n        create_temp_diarization_bucket = GCSCreateBucketOperator(\n            task_id=\"create_temp_diarization_bucket\",\n            bucket_name=f\"bucket_temp_diarization_{DAG_ID}_{ENV_ID}\",\n        )\n        # [END create_temp_diarization_bucket]\n\n    @task\n    def create_temp_audio_file():\n        # [START create_temp_audio_file]\n        create_temp_audio_file = TextGCSCreateAudioFileOperator(\n            task_id=\"create_temp_audio_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            language_code=\"en\",\n            model=\"base\",\n        )\n        # [END create_temp_audio_file]\n\n    @task\n    def create_temp_video_file():\n        # [START create_temp_video_file]\n        create_temp_video_file = TextGCSCreateVideoFileOperator(\n            task_id=\"create_temp_video_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            video_format=\"mp4\",\n        )\n        # [END create_temp_video_file]\n\n    @task\n    def create_temp_conversation_file():\n        # [START create_temp_conversation_file]\n        create_temp_conversation_file = TextGCSCreateConversationFileOperator(\n            task_id=\"create_temp_conversation_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n        )\n        # [END create_temp_conversation_file]\n\n    @task\n    def create_temp_expressions_file():\n        # [START create_temp_expressions_file]\n        create_temp_expressions_file = TextGCSCreateExpressionsFileOperator(\n            task_id=\"create_temp_expressions_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n        )\n        # [END create_temp_expressions_file]\n\n    @task\n    def create_temp_diarization_file():\n        # [START create_temp_diarization_file]\n        create_temp_diarization_file = TextGCSCreateDiarizationFileOperator(\n            task_id=\"create_temp_diarization_file\",\n            bucket_name=f\"bucket_temp_diarization_{DAG_ID}_{ENV_ID}\",\n            file_name=\"diarization_spec.json\",\n        )\n        # [END create_temp_diarization_file]\n\n    @task\n    def delete_temp_diarization_bucket():\n        # [START delete_temp_diarization_bucket]\n        delete_temp_diarization_bucket = GCSDeleteBucketOperator(\n            task_id=\"delete_temp_diarization_bucket\",\n            bucket_name=f\"bucket_temp_diarization_{DAG_ID}_{ENV_ID}\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_diarization_bucket]\n\n    @task\n    def delete_temp_audio_file():\n        # [START delete_temp_audio_file]\n        delete_temp_audio_file = GCSDeleteFileOperator(\n            task_id=\"delete_temp_audio_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_audio_file]\n\n    @task\n    def delete_temp_video_file():\n        # [START delete_temp_video_file]\n        delete_temp_video_file = GCSDeleteFileOperator(\n            task_id=\"delete_temp_video_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_video_file]\n\n    @task\n    def delete_temp_conversation_file():\n        # [START delete_temp_conversation_file]\n        delete_temp_conversation_file = GCSDeleteFileOperator(\n            task_id=\"delete_temp_conversation_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_conversation_file]\n\n    @task\n    def delete_temp_expressions_file():\n        # [START delete_temp_expressions_file]\n        delete_temp_expressions_file = GCSDeleteFileOperator(\n            task_id=\"delete_temp_expressions_file\",\n            bucket_name=BUCKET_NAME,\n            file_name=FILE_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_expressions_file]\n\n    @task\n    def delete_temp_diarization_file():\n        # [START delete_temp_diarization_file]\n        delete_temp_diarization_file = GCSDeleteFileOperator(\n            task_id=\"delete_temp_diarization_file\",\n            bucket_name=f\"bucket_temp_diarization_{DAG_ID}_{ENV_ID}\",\n            file_name=\"diarization_spec.json\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n        # [END delete_temp_diarization_file]\n\n    create_connection >> create_bucket\n    create_temp_bucket = create_temp_diarization_bucket()\n    create_temp_audio_file() >> create_temp_video_file()\n    create_temp_conversation_file() >> create_temp_expressions_file()\n    create_temp_diarization_file() >> create_temp_diarization_file()\n\n    # [START howto_operator_text_gcs_create_audio_file]\n    create_audio_file = TextGCSCreateAudioFileOperator(\n        task_id=\"create_audio_file\", bucket_name=BUCKET_NAME, file_name=FILE_NAME, language_code=\"en\"\n    )\n    # [END howto_operator_text_gcs_create_audio_file]\n\n    # [START howto_operator_text_gcs_create_video_file]\n    create_video_file = TextGCSCreateVideoFileOperator(\n        task_id=\"create_video_file\", bucket_name=BUCKET_NAME, file_name=FILE_NAME, video_format=\"mp4\"\n    )\n    # [END howto_operator_text_gcs_create_video_file]\n\n    # [START howto_operator_text_gcs_create_conversation_file]\n    create_conversation_file = TextGCSCreateConversationFileOperator(\n        task_id=\"create_conversation_file\", bucket_name=BUCKET_NAME, file_name=FILE_NAME\n    )\n    # [END howto_operator_text_gcs_create_conversation_file]\n\n    # [START howto_operator_text_gcs_create_expressions_file]\n    create_expressions_file = TextGCSCreateExpressionsFileOperator(\n        task_id=\"create_expressions_file\", bucket_name=BUCKET_NAME, file_name=FILE_NAME\n    )\n    # [END howto_operator_text_gcs_create_expressions_file]\n\n    # [START howto_operator_text_gcs_create_diarization_file]\n    create_diarization_file = TextGCSCreateDiarizationFileOperator(\n        task_id=\"create_diarization_file\", bucket_name=BUCKET_NAME, file_name=\"diarization_spec.json\"\n    )\n    # [END howto_operator_text_gcs_create_diarization_file]\n\n    # [START howto_operator_text_gcs_create_video_file_async]\n    create_video_file_async = TextGCSCreateVideoFileOperator(\n        task_id=\"create_video_file_async\",\n        bucket_name=BUCKET_NAME,\n        file_name=FILE_NAME,\n        video_format=\"mp4\",\n        deferrable=True,\n    )\n    # [END howto_operator_text_gcs_create_video_file_async]\n\n    # [START howto_operator_text_gcs_create_audio_file_async]\n    create_audio_file_async = TextGCSCreateAudioFileOperator(\n        task_id=\"create_audio_file_async\", bucket_name=BUCKET_NAME, file_name=FILE_NAME, language_code=\"en\"\n    )\n    # [END howto_operator_text_gcs_create_audio_file_async]\n\n    # [START howto_operator_text_gcs_create_conversation_file_async]\n    create_conversation_file_async = TextGCSCreateConversationFileOperator(\n        task_id=\"create_conversation_file_async\", bucket_name=BUCKET_NAME, file_name=FILE_NAME"}], "metadata": {"file_name": "dag_572_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data pipeline that illustrates how to orchestrate system-level tasks, including time-based scheduling, command execution, and basic error handling in a distributed computing environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.providers.singularity.utils.file import get_absolute_path\n\ntry:\n    from airflow.providers.singularity.operators.singularity import SingularitySubmitOperator\nexcept ImportError:\n    # This import should trigger a failure\n    from airflow.sdk import DAG as UnknownProviderDAG\n\nsys_path = os.path.dirname(os.path.abspath(UnknownProviderDAG.__file__))\nabsolute_path = get_absolute_path()\n\nsong_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {absolute_path}/song_table\n    (song_id INT, title TEXT, artist_id INT, year INT, duration FLOAT);\nCREATE TABLE IF NOT EXISTS {absolute_path}/users\n(name TEXT, location TEXT, gender TEXT, level TEXT);\nCREATE TABLE IF NOT EXISTS {absolute_path}/time\n(start_time TIMESTAMP, ds TEXT);\nCREATE TABLE IF NOT EXISTS {absolute_path}/temp\n(songs_id INT, artist_id INT, user_id INT, level TEXT);\nCREATE TABLE IF NOT EXISTS {absolute_path}/acc\n(songs_id INT, artist_id INT, user_id INT, year INT, duration FLOAT);\nCREATE TABLE IF NOT EXISTS {absolute_path}/final\n(songs_id INT, artist_id INT, user_id INT, year INT, duration FLOAT);\n\"\"\"\n\nwith DAG(\n    dag_id=\"singularity_submit_example\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"singularity\"],\n) as dag:\n    # [START howto_operator SingularitySubmitOperator]\n    t1 = SingularityOperator(\n        task_id=\"spark_submit\",\n        image=\"docker://busybox\",\n        command=\"sh -c 'sleep 20'\",\n        singularity_options=\"--volume /opt/airflow/dags:/opt/airflow/dags:Z --volume /tmp/airflow/dags:/tmp/airflow/dags:Z\",\n    )\n    # [END howto_operator SingularitySubmitOperator]\n\n    # [START howto_operator_SingularityOperator]\n    t2 = SingularityOperator(\n        task_id=\"spark_submit\",\n        image=\"${SINGULARITY_IMAGE}\",\n        file=f\"{sys_path}/examples/toml/singularity/toml_example.sif\",\n        arguments=f\"--tmpdir /tmp/my_spark --logistic /tmp/my_spark/logistic_regression_model.bin \\\n            --data {absolute_path}/song_table_sql.txt\",\n    )\n    # [END howto_operator_SingularityOperator]\n\n    [t1, t2] >> SingularityOperator(task_id=\"noop\", command=\"sh -c 'sleep 20'\")\n\n    for task in [t1, t2]:\n        task >> SingularityOperator(\n            task_id=f\"env_var_{task.task_id}\",\n            command=\"echo $AIRFLOW__API__SQLACHEM__IMPL\",\n            env_vars={\"AIRFLOW__API__SQLACHEM__IMPL\": \"MySql\"},\n        )\n\n    t1 >> SingularityOperator(\n        task_id=\"env_var _file_test\",\n        command=\"echo $AIRFLOW__API__SQLACHEM__IMPL\",\n        env_vars={\"AIRFLOW__API__SQLACHEM__IMPL\": \"FileTestResource\"},\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_648_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you help me extract specific elements from a JSON document using Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task\ndef process_json(file_path: str):\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n        # Extract specific elements from the JSON document\n        name = data[\"name\"]\n        age = data[\"age\"]\n        print(f\"Name: {name}, Age: {age}\")\n\n\nwith DAG(\n    \"example_json_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n):\n    bash_cmd = f\"python3 {__file__} {file_path}\"\n    BashOperator(task_id=\"json_data\", bash_command=bash_cmd)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that moves spreadsheet data between Google Sheets and Google Cloud Storage using Airflow's task orchestration capabilities. Create a systematic process for extracting spreadsheet data, transferring it to cloud storage, and then reimporting the data into a new spreadsheet environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.sheets import GoogleCloudSheetsHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.sheets import GoogleCloudSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleCloudSheetsCreateSpreadsheetImportOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleCloudSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleCloudSheetsImportSheetToSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.sheets_to_sheets import SheetsToSheetsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_sheets_to_sheets\"\n\nLOCATION = \"europe-west2-b\"\nREGION = \"europe-west2\"\nNETWORK_NAME = f\"network-{ENV_ID}\"\nCONNECTION_ID = f\"connection-{ENV_ID}\"\nSPREADSHEET_ID = f\"spreadsheet-id-{ENV_ID}\"\nSHEET_NAME = \"test_sheet\"\nNEW_SHEET_NAME = \"new_test_sheet\"\nBUCKET_NAME = f\"bucket-{ENV_ID}\"\nFILE_NAME = \"output_sheet_import.csv\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sheets\", \"sheets_to_sheets\"],\n) as dag:\n    @task\n    def create_connection(connection_id: str):\n        connection = Connection(\n            conn_id=connection_id,\n            conn_type=\"google_cloud_platform\",\n        )\n        connection_extra = {\n            \"scope\": \"https://www.googleapis.com/auth/spreadsheets,https://www.googleapis.com/auth/cloud-platform\",\n            \"project\": PROJECT_ID,\n            \"location\": LOCATION,\n            \"resource\": {\"name\": connection_id},\n        }\n        connection.add_extra_field(**connection_extra)\n        return connection\n\n    create_connection_task = create_connection(connection_id=CONNECTION_ID)\n\n    @task\n    def create_bucket():\n        location = LOCATION\n        storage_bucket = BUCKET_NAME\n\n        service = GoogleCloudSheetsHook()\n        service.sheets_api.create_body = {\n            \"name\": storage_bucket,\n            \"initialConfiguration\": {\n                \"size\": \"LIMITED\",\n                \"diskConfig\": {\n                    \"source\": {\n                        \"source\": f\"projects/{PROJECT_ID}/locations/{location}/storage buckets/{storage_bucket}\"\n                    }\n                },\n                \"networking\": {\n                    \"assignPublicIp\": \"AUTO\"\n                }\n            }\n        }\n        service.sheets_api.create()\n\n    create_bucket_task = create_bucket()\n\n    @task\n    def upload_file_to_bucket():\n        import_uri = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n        service = ComputeEngineHook()\n        service.compute_engine_insert_body = {\n            \"name\": \"\",\n            \"project\": PROJECT_ID,\n            \"resource\": {\n                \"machine_type\": \"n1-standard-1\",\n                \"disks\": [\n                    {\n                        \"auto_delete\": True,\n                        \"boot\": True,\n                        \"device_name\": \"instance-1\",\n                        \"initialize_params\": {\n                            \"disk_size_gb\": \"10\",\n                            \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v202206\",\n                            \"source_element_group\": \"\",\n                            \"source_image_project\": \"google-cloud\",\n                            \"disk_config\": {\"connection_spec\": service.compute_engine_insert_body[\"resource\"][\"network_interfaces\"][0]}\n                        },\n                    }\n                ],\n                \"metadata\": {},\n                \"network_interfaces\": [\n                    {\n                        \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                        \"network\": f\"global/networks/{service.compute_engine_insert_body['resource']['network_interfaces'][0]['network\"]}\",\n                    }\n                ],\n            },\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\n                            \"name\": \"QEMUSharedNIC\",\n                            \"network_tier\": \"PREMIUM\",\n                        }\n                    ],\n                    \"network\": f\"global/networks/{location}-global\",\n                    \"subnetwork\": f\"global/networks/{location}-global\",\n                }\n            ],\n        }\n        service.compute_engine_insert()\n\n    upload_file_to_bucket_task = upload_file_to_bucket()\n\n    @task\n    def create_role():\n        role = Role(\n            name=f\"role-{ENV_ID}\",\n            description=\"Test role generated by Airflow.\",\n        )\n        project = PROJECT_ID\n        location = LOCATION\n        service = ComputeEngineHook()\n        service.compute_engine_set_service_acc = {\n            \"project\": project,\n            \"region\": location,\n            \"name\": f\"roles/{role.name}\",\n            \"body\": {\"members\": [f\"service-account:{project}\"],\n                    \"impersonator\": \"\"},\n        }\n        service.compute_engine_set_service_acc()\n\n    create_role_task = create_role()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_instance(instance_name: str):\n        hook = ComputeEngineHook()\n        hook.compute_engine_delete_instance(\n            project=PROJECT_ID, zone=LOCATION, resource_id=instance_name\n        )\n\n    delete_instance_task = delete_instance(instance_name=INSTANCE_NAME)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_bucket(bucket_name: str):\n        service = GoogleCloudSheetsHook()\n        service.sheets_api.delete_body(request_body={\"resourceId\": bucket_name})\n\n    delete_bucket_task = delete_bucket(bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_sheets_to_sheets]\n    copy_spreadsheet_to_new_sheet = GoogleCloudSheetsImportSheetToSpreadsheetOperator(\n        task_id=\"copy_spreadsheet_to_new_sheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        destination_sheet_name=NEW_SHEET_NAME,\n        location=LOCATION,\n    )\n    # [END howto_operator_sheets_to_sheets]\n\n    move_spreadsheet_to_new_sheet = GoogleCloudSheetsCreateSpreadsheetImportOperator(\n        task_id=\"move_spreadsheet_to_new_sheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        destination_sheet_name=NEW_SHEET_NAME,\n        location=LOCATION,\n        destination_project_id=PROJECT_ID,\n    )\n\n    @task\n    def wait_on_job(spreadsheet_import_name: str):\n        hook = GoogleCloudSheetsHook()\n        # Wait for the spreadsheet import to complete.\n        hook.wait_for_operation_to_complete(\n            request_body={\"operationName\": spreadsheet_import_name}\n        )\n\n    wait_on_job(spreadsheet_import_name=spreadsheet_import_name)\n\n    # [START howto_operator_sheets_to_sheets_template]\n    create_table = GoogleCloudSheetsCreateSpreadsheetOperator(\n        task_id=\"create_table\", spreadsheet_id=SPREADSHEET_ID, params={\"title\": \"Test Spreadsheet\"}\n    )\n\n    insert_table = GoogleCloudSheetsCreateSpreadsheetImportOperator(\n        task_id=\"insert_table\",\n        spreadsheet_id=SPREADSHEET_ID,\n        params={\"title\": \"Test Imported Spreadsheet\"},\n    )\n    # [END howto_operator_sheets_to_sheets_template]\n\n    @task\n    def wait_on_table_creation(spreadsheet_create_name: str):\n        hook = GoogleCloudSheetsHook()\n        # Wait for the spreadsheet creation to complete.\n        hook.wait_for_operation_to_complete(request_body={\"operationName\": spreadsheet_create_name})\n\n    wait_on_table_creation(spreadsheet_create_name=spreadsheet_create_name)\n\n    # [START howto_operator_sheets_to_sheets_copy_import]\n    copy_import_sheet = GoogleCloudSheetsImportSheetToSpreadsheetOperator(\n        task_id=\"copy_import_sheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        destination_sheet_name=NEW_SHEET_NAME,\n        location=LOCATION,\n        destination_project_id=PROJECT_ID,\n    )\n    # [END howto_operator_sheets_to_sheets_copy_import]\n\n    # [START howto_operator_sheets_to_sheets_move_import]\n    move_import_sheet = GoogleCloudSheetsCreateSpreadsheetImportOperator(\n        task_id=\"move_import_sheet\",\n        spreadsheet_id=SPREADSHEET_ID,\n        destination_sheet_name=NEW_SHEET_NAME,\n        location=LOCATION,\n        destination_project_id=PROJECT_ID,\n    )\n    # [END howto_operator_sheets_to_sheets_move_import]\n\n    # [START howto_operator_sheets_to_sheets_multiple_imports]\n    create_multiple_imports = GoogleCloudSheetsCreateSpreadsheetImportOperator(\n        task_id=\"create_multiple_imports\",\n        spreadsheet_id=SPREADSHEET_ID,\n        params={\"properties\": \"title\"},\n    )\n    # [END howto_operator_sheets_to_sheets_multiple_imports]\n\n    # [START howto_operator_sheets_to_sheets_multiple_imports_result]\n    wait_on_multiple_imports = TriggerRule dependence(\n        create_multiple_imports.output_value[\"spreadsheet_id\"]\n    )\n    # [END howto_operator_sheets_to_sheets_multiple_imports_result]\n\n    # [START howto_operator_sheets_to_sheets_copy_import_result]\n    wait_on_copy_import = TriggerRule dependence(copy_import_sheet.output_value[\"spreadsheet_id\"])\n    # [END howto_operator_sheets_to_sheets_copy_import_result]\n\n    # [START howto_operator_sheets_to_sheets_move_import_result]\n    wait_on_move_import = TriggerRule dependence(move_import_sheet.output_value[\"spreadsheet_id\"])\n    # [END howto_operator_sheets_to"}], "metadata": {"file_name": "dag_895_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data orchestration process that extracts tabular data from a Google Spreadsheet and securely transfers it to a cloud storage solution with dynamic environment-based naming. Create a repeatable template for extracting structured data from cloud-based spreadsheet services and preparing it for downstream processing.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_google_sheets_to_gcs\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"google_sheets_api\",\n            conn_type=\"google_cloud_platform\",\n        )\n        conn_extra = {\n            \"scope\": \"https://www.googleapis.com/auth/spreadsheets,https://www.googleapis.com/auth/drive\",\n            \"user\": os.environ.get(\"GCP_PROJECT_ID\"),\n            \"password\": os.environ.get(\"SPREADSHEET_SECRET\"),\n        }\n        conn_extra_json = json.dumps(conn_extra)\n        conn.set_extra(conn_extra_json)\n\n        role = Role()\n        role.name = \"gcs_owner\"\n        role.project_id = os.environ.get(\"GCP_PROJECT_ID\")\n\n        tg = TaskGroup(\"gcs_bucket\")\n        trigger = Trigger(\n            task_id=\"create_bucket\",\n            bash_command=BashOperator(\n                task_id=\"create_bucket_cmd\",\n                bash_command=f\"bucket_name={os.environ.get('GCP_BUCKET_NAME')}\",\n            ),\n        )\n        role.add_task(tg)\n        dag.add_task(trigger)\n        bucket = Connection(\n            conn_id=\"google_cloud_platform\",\n            conn_type=\"google_cloud_platform\",\n            extra=conn_extra_json,\n            project_id=os.environ.get(\"GCP_PROJECT_ID\"),\n        )\n        bucket.set_extra({\"scope\": \"https://www.googleapis.com/auth/drive\"})\n        gcs_conn = Connection(\n            conn_id=\"gcs_api\",\n            conn_type=\"google_cloud_platform\",\n        )\n        gcs_extra = {\n            \"project\": os.environ.get(\"GCP_PROJECT_ID\"),\n            \"scope\": \"https://www.googleapis.com/auth/drive\",\n        }\n        gcs_extra_json = json.dumps(gcs_extra)\n        gcs_conn.set_extra(gcs_extra_json)\n        bucket.add_extra(gcs_extra)\n        gcs_conn.add_extra(gcs_extra)\n\n    @task\n    def create_sheet():\n        from airflow.providers.google.suite.hooks.sheets import GoogleSheetsHook\n\n        sheets_hook = GoogleSheetsHook()\n        info = sheets_hook.get_conn().projects().files.list().execute()\n        sheet_id = info[\"files\"][0]\n        print(f\"Sheet id is {sheet_id}\")\n\n        body = {\n            \"properties\": {\n                \"title\": \"Test1\",\n            },\n            \"sheets\": [\n                {\n                    \"properties\": {\n                        \"title\": \"Sheet1\",\n                    }\n                }\n            ],\n        }\n        service = sheets_hook.get_conn()\n        response = service.spreadsheets_create(body=body).execute()\n        print(response)\n\n    @task\n    def upload_sheet_to_gcs(sheet_id, gcs_bucket_name):\n        from airflow.providers.google.suite.hooks.sheets import GoogleSheetsHook\n\n        sheets_hook = GoogleSheetsHook()\n        service = sheets_hook.get_conn()\n        request = service.spreadsheets_values.batchGet(\n            spreadsheetId=sheet_id,\n            ranges=[\"Sheet1!A1:D1\"],\n            includeCells={\"includeAllCellsInSpreadsheet\": True},\n        )\n        response = request.execute()\n        print(response)\n\n        file = {\n            \"name\": f\"{gcs_bucket_name}/output.csv\",\n            \"path\": f\"{gcs_bucket_name}/output.csv\",\n            \"service\": \"google_cloud_storage\",\n        }\n        body = {\n            \"name\": f\"{gcs_bucket_name}/output.csv\",\n            \"sourceUris\": [f\"gs://{gcs_bucket_name}/output.csv\"],\n        }\n        service = google_cloud_storage_hook.get_conn()\n        response = service.files.create(body=body, media_body=file).execute()\n        print(response)\n\n    google_sheets_api = create_connection()\n    sheet = create_sheet()\n    upload_sheet_to_gcs(sheet.output, os.environ.get(\"GCP_BUCKET_NAME\"))"}], "metadata": {"file_name": "dag_1905_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I have a string that contains multiple pieces of information about a location, separated by commas. The string looks something like this: \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\". I need to extract the name, location, category, and rating from this string and store them in separate variables. How can I do this in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def extract_location_info(input_string):\n    \"\"\"\n    Extracts name, location, category, and rating from a given string containing multiple pieces of information separated by commas.\n    \"\"\"\n    # Split the input string by commas to get individual pieces\n    pieces = input_string.split(',')\n    \n    # Initialize variables to store extracted information\n    name = location = category = rating = \"\"\n    \n    # Iterate over the pieces to extract each piece\n    for piece in pieces:\n        key, value = piece.split(':')\n        if key == 'Name':\n            name = value\n        elif key == 'Location':\n            location = value\n        elif key == 'Category':\n            category = value\n        elif key == 'Rating':\n            rating = value\n    \n    return name, location, category, rating\n\n# Example usage\ninput_string = \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\"\nname, location, category, rating = extract_location_info(input_string)\nprint(f\"Name: {name}\")\nprint(f\"Location: {location}\")\nprint(f\"Category: {category}\")\nprint(f\"Rating: {rating}\")"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide me with a Python code snippet that demonstrates how to use the `pandas` library to read a CSV file and filter rows based on a specific condition?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pandas as pd\n\n# [START howto_operator_pandas]\ndef pandas_callable(data: pd.DataFrame, n: int) -> pd.DataFrame:\n    \"\"\"\n    A sample function that takes a pandas DataFrame and returns a modified version.\n\n    :param data: The original pandas DataFrame.\n    :param n: An integer value to filter the DataFrame.\n    :return: A modified pandas DataFrame.\n    \"\"\"\n    return data[data > n]\n\n\ndef list_of_pandas_callable(*dfs: pd.DataFrame, n: int) -> list[pd.DataFrame]:\n    \"\"\"\n    A sample function that takes a list of pandas DataFrames and returns a modified version.\n\n    :param dfs: A list of pandas DataFrames.\n    :param n: An integer value to filter the DataFrames.\n    :return: A list of modified pandas DataFrames.\n    \"\"\"\n    return [df[df > n] for df in dfs]\n\n\ndef dict_of_pandas_callable(d: dict[str, pd.DataFrame], n: int) -> dict[str, pd.DataFrame]:\n    \"\"\"\n    A sample function that takes a dictionary of pandas DataFrames and returns a modified version.\n\n    :param d: A dictionary of pandas DataFrames.\n    :param n: An integer value to filter the DataFrames.\n    :return: A dictionary of modified pandas DataFrames.\n    \"\"\"\n    return {k: v[v > n] for k, v in d.items()}\n\n\ndef list_of_pandas_callable_dict_of_pandas_callable(\n    *dls: dict[str, pd.DataFrame], n: int\n) -> list[dict[str, pd.DataFrame]]:\n    \"\"\"\n    A sample function that takes a list of dictionaries of pandas DataFrames and returns a modified version.\n\n    :param dls: A list of dictionaries of pandas DataFrames.\n    :param n: An integer value to filter the DataFrames.\n    :return: A list of dictionaries of modified pandas DataFrames.\n    \"\"\"\n    return [dict_of_pandas_callable(d, n) for d in dls]\n\n\ndef list_of_list_of_pandas_callable(\n    *ls: list[pd.DataFrame], n: int\n) -> list[list[pd.DataFrame]]:\n    \"\"\"\n    A sample function that takes a list of lists of pandas DataFrames and returns a modified version.\n\n    :param ls: A list of lists of pandas DataFrames.\n    :param n: An integer value to filter the DataFrames.\n    :return: A list of lists of modified pandas DataFrames.\n    \"\"\"\n    return [list_of_pandas_callable(l, n) for l in ls]\n\n\ndef list_of_list_of_pandas_callable_dict_of_pandas_callable(\n    *dls: dict[str, pd.DataFrame], n: int\n) -> list[dict[str, pd.DataFrame]]:\n    \"\"\"\n    A sample function that takes a list of lists of dictionaries of pandas DataFrames and returns a modified version.\n\n    :param dls: A list of lists of dictionaries of pandas DataFrames.\n    :param n: An integer value to filter the DataFrames.\n    :return: A list of lists of dictionaries of modified pandas DataFrames.\n    \"\"\"\n    return [list_of_pandas_callable_dict_of_pandas_callable(d, n) for d in dls]\n\n\ndef read_and_return_csv(file: str) -> str:\n    \"\"\"\n    A sample function that reads a CSV file and returns its content as a string.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a string.\n    \"\"\"\n    return pd.read_csv(file).to_string(index=False)\n\n\ndef read_and_return_csv_dict(file: str) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads a CSV file and returns its content as a dictionary.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary.\n    \"\"\"\n    return pd.read_csv(file).to_dict(orient=\"index\")\n\n\ndef read_and_return_csv_dict_of_types(file: str) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads a CSV file and returns its content as a dictionary with specified types.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary with specified types.\n    \"\"\"\n    return pd.read_csv(file, dtype={\"col1\": str, \"col2\": str}).to_dict(orient=\"index\")\n\n\ndef read_and_return_csv_in_chunks(file: str, chunk_size: int) -> list[str]:\n    \"\"\"\n    A sample function that reads a CSV file in chunks and returns each chunk as a string.\n\n    :param file: The path to the CSV file.\n    :param chunk_size: The size of each chunk.\n    :return: A list of strings, each representing a chunk of the CSV file.\n    \"\"\"\n    return [str(df) for _, df in pd.read_csv(file, chunksize=chunk_size)]\n\n\ndef read_and_return_csv_in_chunks_dict(file: str, chunk_size: int) -> list[dict[str, str]]:\n    \"\"\"\n    A sample function that reads a CSV file in chunks and returns each chunk as a dictionary.\n\n    :param file: The path to the CSV file.\n    :param chunk_size: The size of each chunk.\n    :return: A list of dictionaries, each representing a chunk of the CSV file.\n    \"\"\"\n    return [\n        dict(row) for _, row in pd.read_csv(file, chunksize=chunk_size)\n    ]\n\n\ndef read_and_return_csv_in_chunks_dict_of_types(\n    file: str, chunk_size: int\n) -> list[dict[str, str]]:\n    \"\"\"\n    A sample function that reads a CSV file in chunks and returns each chunk as a dictionary with specified types.\n\n    :param file: The path to the CSV file.\n    :param chunk_size: The size of each chunk.\n    :return: A list of dictionaries with specified types, each representing a chunk of the CSV file.\n    \"\"\"\n    return [\n        dict(row, **{\"col1\": v[\"col1\"], \"col2\": v[\"col2\"]}) for _, v in pd.read_csv(\n            file, chunksize=chunk_size\n        )\n    ]\n\n\ndef read_and_return_csv_infinite(file: str) -> str:\n    \"\"\"\n    A sample function that reads an infinite CSV file and returns its content as a string.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a string.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1).next().to_string(index=False)\n\n\ndef read_and_return_csv_infinite_dict(file: str) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads an infinite CSV file and returns its content as a dictionary.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1).next().to_dict(orient=\"index\")\n\n\ndef read_and_return_csv_infinite_dict_of_types(\n    file: str\n) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads an infinite CSV file and returns its content as a dictionary with specified types.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary with specified types.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1).next().to_dict(orient=\"index\")  # type: ignore[call-arg]\n\n\ndef read_and_return_csv_infinite_with_gcs(file: str) -> str:\n    \"\"\"\n    A sample function that reads an infinite CSV file from GCS and returns its content as a string.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a string.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1, storage_options={\"service_account_scopes\": [\"https://www.googleapis.com/auth/cloud-platform\"]}).next().to_string(index=False)\n\n\ndef read_and_return_csv_infinite_with_gcs_dict(file: str) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads an infinite CSV file from GCS and returns its content as a dictionary.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1, storage_options={\"service_account_scopes\": [\"https://www.googleapis.com/auth/cloud-platform\"]}).next().to_dict(orient=\"index\")\n\n\ndef read_and_return_csv_infinite_with_gcs_dict_of_types(\n    file: str\n) -> dict[str, str]:\n    \"\"\"\n    A sample function that reads an infinite CSV file from GCS and returns its content as a dictionary with specified types.\n\n    :param file: The path to the CSV file.\n    :return: The content of the CSV file as a dictionary with specified types.\n    \"\"\"\n    return pd.read_csv(file, iterator=True, chunksize=1, storage_options={\"service_account_scopes\": [\"https://www.googleapis.com/auth/cloud-platform\"]}).next().to_dict(orient=\"index\")  # type: ignore[call-arg]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases a systematic cloud storage transfer process with clear setup, execution, and teardown stages. Implement a workflow that creates a temporary S3 bucket, transfers data from Azure Blob Storage, and then deletes the bucket after completion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.sas import AzureSasOperator\nfrom airflow.providers.microsoft.azure.transfers.s3_to_azure import S3ToAzureOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_azure\"\n\n# [START howto_operator_s3_to_azure]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) is\n    # Create\n    TaskGroup(string_id=\"create_bucket\"):\n        connection = Connection(\n            conn_id=\"azure_default\",\n            conn_type=\"azure_file\",\n            prev_task_id=\"create_connection\",\n            schema=\"airflow.providers.microsoft.azure.operators.azure_file.AzureFileCreateOperator\",\n        )\n\n        bucket_name = f\"bucket_{ENV_ID}\"\n        directory = \"dir\"\n\n        S3ToAzureOperator(\n            task_id=\"create_s3_to_azure_connection\",\n            bucket_name=bucket_name,\n            directory=directory,\n            conn_id=connection,\n        )\n\n    # [START howto_sas_create_bucket]\n    create_bucket_with_sas:\n        bucket_name = \"bucket_sas\"\n        directory = \"dir\"\n\n        AzureSasOperator(\n            task_id=\"create_s3_to_azure_with_sas\",\n            bucket_name=bucket_name,\n            directory=directory,\n            expires=datetime(2021, 1, 1, 12, 10, 0),\n        )\n        # [END howto_sas_create_bucket]\n        # [START howto_sas_short_lived_bucket]\n        short_lived_create_bucket_with_sas:\n            bucket_name = \"bucket_sas_short_lived\"\n            directory = \"dir\"\n\n            AzureSasOperator(\n                task_id=\"create_s3_to_azure_with_sas_short_lived\",\n                bucket_name=bucket_name,\n                directory=directory,\n                expires=datetime(2021, 1, 1, 12, 10, 0),\n                azure_file_service_connection_id=\"azure_default\",\n            )\n        # [END howto_sas_short_lived_bucket]\n    # [END howto_sas_bucket]\n    # [START howto_delete_bucket]\n    delete_bucket:\n        bucket_name = \"bucket_{ENV_ID}\".format(env_id=ENV_ID)\n\n        BashOperator(\n            task_id=\"delete_s3_bucket\",\n            bash_command=f\"sudo aws s3 rm {bucket_name} --force\",\n        )\n        # [END howto_delete_bucket]\n        # [START howto_delete_bucket_without_cleanup]\n        delete_bucket_without_cleanup:\n            bucket_name = \"bucket_{ENV_ID}_without_cleanup\".format(env_id=ENV_ID)\n\n            BashOperator(\n                task_id=\"delete_s3_bucket_without_cleanup\",\n                bash_command=f\"sudo aws s3 rm {bucket_name} --force\",\n                trigger_rule=TriggerRule.ALL_DONE,\n            )\n        # [END howto_delete_bucket_without_cleanup]\n    # [END howto_delete_bucket]\n    # [START howto_transfer_files]\n    transfer_files:\n        bucket_name = \"bucket_{env_id}\".format(env_id=ENV_ID)\n        dest_bucket_name = \"bucket_{env_id}_dest\".format(env_id=ENV_ID)\n\n        S3ToAzureOperator(\n            task_id=\"transfer_files\",\n            bucket_name=bucket_name,\n            directory=\"dir\",\n            dest_bucket_name=dest_bucket_name,\n            dest_directory=\"dir\",\n        )\n        # [END howto_transfer_files]\n    # [END howto_transfer_files]\n    # [START howto_transfer_files_with_azure_connection]\n    transfer_files_with_azure_connection:\n        bucket_name = \"bucket_{env_id}\".format(env_id=ENV_ID)\n        dest_bucket_name = \"bucket_{env_id}_dest\".format(env_id=ENV_ID)\n\n        S3ToAzureOperator(\n            task_id=\"transfer_files_with_azure_connection\",\n            bucket_name=bucket_name,\n            directory=\"dir\",\n            dest_bucket_name=dest_bucket_name,\n            dest_directory=\"dir\",\n            azure_file_service_connection_id=\"azure_default\",\n        )\n        # [END howto_transfer_files_with_azure_connection]\n# [END howto_operator_s3_to_azure]"}], "metadata": {"file_name": "dag_1071_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a reproducible machine learning data processing pipeline for housing price prediction that leverages cloud infrastructure and automated workflow management. Create a system that transforms local housing data into a cloud-based training job with standardized dataset preparation and model training steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\nHOME = os.environ.get(\"HOME\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"shell_python_task_dag\"\nDB_NAME = \"airflow_test\"\n\ns3_bucket = f\"{ENV_ID}-shell-dag-bucket\"\ns3_uri = f\"s3://{s3_bucket}\"\n\npostgres_conn_id = f\"{ENV_ID}-postgres\"\npostgres_conn = {\n    \"conn_id\": postgres_conn_id,\n    \"login\": \"airflow\",\n    \"password\": \"airflow\",\n    \"host\": \"postgres\",\n    \"port\": \"5432\",\n    \"conn_type\": \"postgres\",\n    \"odbc_connect\": f\"dbname={DB_NAME} user={airflow} password={airflow} host=postgres port=5432\",\n}\n\ndata_csv_local = HOME + \"/resources/state_housing_data.csv\"\ndata_csv_s3 = f\"{s3_uri}/state_housing_data.csv\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"shell-python\"],\n) as dag:\n\n    @task\n    def create_db_postgres():\n        \"\"\"\n        Task creates database called airflows_db if not exists.\n        \"\"\"\n        Session.execute(\n            f\"\"\"\n            CREATE DATABASE {DB_NAME} IF NOT EXISTS;\n            \"\"\"\n        )\n\n    @task\n    def create_conn_postgres():\n        \"\"\"\n        Task creates Postgres connection\n        \"\"\"\n        conn = Session.create(\n            conn_id=postgres_conn_id,\n            login=\"airflow\",\n            password=\"airflow\",\n            host=\"postgres\",\n            port=\"5432\",\n            conn_type=\"postgres\",\n        )\n        return conn\n\n    @task\n    def create_role_postgres():\n        \"\"\"\n        Task creates role\n        \"\"\"\n        role = Role()\n        role.role_name = \"test_shell_python\"\n        session = Session()\n        session.add(role)\n        session.commit()\n\n    @task\n    def drop_role_postgres(role_name: str):\n        \"\"\"\n        Task drops role\n        \"\"\"\n        session = Session()\n        query = session.query(Role).filter(Role.role_name == role_name)\n        query.delete()\n        session.commit()\n\n    @task\n    def drop_db_postgres(db_name: str):\n        \"\"\"\n        Task drops db\n        \"\"\"\n        Session.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n\n    @task\n    def create_table(data_csv_local, db_name, postgress_conn_id):\n        \"\"\"\n        Task creates table called housing if not exists.\n        \"\"\"\n        Session.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {db_name}.housing (\n            median_income decimal,\n            housing_type varchar,\n            county varchar,\n            state varchar,\n            median_house_value decimal\n            );\n            \"\"\"\n        )\n\n    @task\n    def load_data(data_csv_local, db_name, postgress_conn_id):\n        \"\"\"\n        Task loads data into table housing.\n        \"\"\"\n        load_sql = f\"LOAD DATA INFILE '{data_csv_local}' INTO TABLE {db_name}.housing FIELDS TERMINATED BY ',';\"\n        Session.execute(load_sql)\n\n    @task\n    def remove_logs(logs):\n        \"\"\"\n        Remove logs\n        \"\"\"\n        for log in logs:\n            log.delete()\n\n    at_least_one_task = BashOperator(\n        task_id=\"at_least_one_task\",\n        bash_command=\"echo \\\"Hello World\\\"\",\n    )\n\n    at_least_one_task >> create_db_postgres() >> create_conn_postgres() >> create_role_postgres()\n    [create_table(), create_table()] >> load_data() >> remove_logs()\n    [drop_db_postgres(DB_NAME), drop_db_postgres(DB_NAME)] >> drop_conn_postgres() >> drop_role_postgres(\n        Role(role_name=\"test_shell_python\")\n    )\n\n    ti = TaskInstance(task=load_data(), dttm=datetime(2020, 10, 10), role_id=\"test_shell_python\")\n    log_info = BashOperator(task_id=\"log_info\", bash_command=\"echo \\\"Hello World\\\", ti.task_id\", task_instance=ti)"}], "metadata": {"file_name": "dag_259_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the core principles of task sequencing and SQL query execution using the SQLExecuteQueryOperator. Develop a workflow that connects to a Presto database and demonstrates fundamental data manipulation tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.presto.operators.presto import PrestoOperator\n\n# [START howto_presto_operator]\ndefault_args = {\"conn_id\": \"presto_conn_id\"}\n\nwith DAG(\n    \"presto_example\",\n    default_args=default_args,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END howto_presto_operator]\n\n    # [START howto_presto_operator_with_pk]\n    # precursor table containing product id, name, and cost\n    precursor_table = PrestoOperator(\n        task_id=\"precursor_table\",\n        sql=\"\"\"\n            CREATE TABLE precursor_table (\n            product_id INTEGER,\n            product_name VARCHAR,\n            cost DOUBLE)\n            INSERT INTO precursor_table VALUES\n            (1, 'Widget', 10.5),\n            (2, 'Umbrella', 15.0),\n            (3, 'Car', 25.0);\n        \"\"\",\n    )\n    # [END howto_presto_operator_with_pk]\n\n    # destination table containing sales data\n    destination_table = PrestoOperator(\n        task_id=\"destination_table\",\n        sql=\"\"\"\n            CREATE TABLE sales_table (\n            product_id INTEGER,\n            sales_date DATE,\n            sales_amount DOUBLE)\n            INSERT INTO sales_table VALUES\n            (1, '2021-01-01', 100.5),\n            (2, '2021-01-02', 200.0),\n            (3, '2021-01-03', 300.0);\n        \"\"\",\n    )\n    # [START howto_presto_operator_with_multiple_queries]\n    # query to aggregate sales data\n    agg_query = PrestoOperator(\n        task_id=\"agg_query\",\n        sql=\"\"\"\n            SELECT product_id, sum(sales_amount) as total_sales\n            FROM sales_table\n            GROUP BY product_id;\n        \"\"\",\n    )\n    # [END howto_presto_operator_with_multiple_queries]\n\n    precursor_table >> destination_table >> agg_query"}], "metadata": {"file_name": "dag_2192_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "What are the differences between synchronous and asynchronous programming in Python? Can you provide a simple example to illustrate the difference?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    print(pendulum.datetime.now())\n\n\nwith DAG(\n    \"example_date\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1),  # Override to match your needs\n    catchup=False,\n    tags=[\"example\"],\n):\n    print_date()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates how to ingest data into Weaviate vector database using custom vectors and different data input methods. Create a workflow that shows how to create a Weaviate class, prepare data, and load it into the vector database using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.weaviate.operators.weaviate import WeaviateIngestOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_weaviate_ingest]\n# Standard Python packages\n# Third-party libraries\n\n# System dependencies\n# [END howto_operator_weaviate_ingest]\n\nlocal_file = \"/tmp/Weaviate/test_vectors_with_config.json\"\n\n\n@task\ndef create_weaviate_class(class_name: str):\n    \"\"\"\n    Create a Weaviate class if it does not exist.\n\n    :param class_name Class name.\n    \"\"\"\n    print(f\"Creating Weaviate class {class_name}\")\n\n\n@task\ndef prepare_data_vector(data: dict):\n    \"\"\"\n    Prepare data with vectors in order to be ingested in Weaviate.\n\n    :param data Data to be modified.\n    \"\"\"\n    data[\"vector\"] = [[1.0, 2.0, 3.0]]\n    return data\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_class(class_name: str):\n    \"\"\"\n    Delete a Weaviate class if it exists.\n\n    :param class_name Class name.\n    \"\"\"\n    print(f\"Deleting Weaviate class {class_name}\")\n\n\nwith DAG(\n    \"example_weaviate_ingest\",\n    schedule=\"* * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_weaviate_class(\"Weaviate_example_class_with_vector_using_operator\")\n\n    # [START howto_operator_weaviate_ingest_default]\n    ingest_data = WeaviateIngestOperator(\n        task_id=\"weaviate_ingest_task\",\n        class_name=\"Weaviate_example_class_with_vector_using_operator\",\n        input_data={\"test_vectors_with_config\": local_file},\n    )\n    # [END howto_operator_weaviate_ingest_default]\n\n    delete_class(\"Weaviate_example_class_with_vector_using_operator\")\n    ingest_data"}], "metadata": {"file_name": "dag_1016_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one DAG can initiate another DAG with a predefined configuration message.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"example_trigger_by_alias\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n):\n    # [START trigger_by_alias]\n    t1 = BashOperator(task_id=\"print_hello_world\", bash_command=\"echo hello world\")\n    # [END trigger_by_alias]\n\n    t2 = BashOperator(task_id=\"print_hello_python\", bash_command=\"echo hello python\")\n\n    t1 >> t2"}], "metadata": {"file_name": "dag_1056_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Could you provide a detailed explanation of how to build a recommendation system using collaborative filtering in Python? Building a recommendation system can be quite complex, and I'm looking for a step-by-step guide.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.textblob.operators.textblob import TextBlobOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_textblob_operator]\nwith DAG(\n    dag_id=\"example_textblob\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is TextBlobOperator:\n\n    # Example of the basic TextBlobOperator\n\n    @task\n    def get_word_count():\n        \"\"\"A dummy operator that returns the number of words in a string\"\"\"\n        return 4\n\n    @task\n    def get_sentiment(text: str):\n        \"\"\"A dummy operator that returns the sentiment analysis result of a string\"\"\"\n        return {\"compound\": 0.47, \"positive\": 0.9, \"negative\": 0.03}\n\n    # [END howto_textblob_operator]\n\n    get_word_count >> get_sentiment\n# [END howto_textblob_operator]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data integrity verification workflow that uses SQL operators to perform systematic checks on a database table's structure and content. Build a DAG that validates column-level constraints and confirms the expected number of rows in the target table.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_sql_template]\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_account\": True,\n    \"conn_id\": \"postgres_default\",\n    \"poke_interval\": 60,\n}\n\nwith DAG(\n    \"example_sql_template\",\n    default_args=default_args,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    sql_template = SQLExecuteQueryOperator(\n        task_id=\"table_structure_check\",\n        sql=\"SELECT table_name, column_name FROM information_schema.columns WHERE table_name = 'users';\",\n    )\n\n    sql_template_count = SQLExecuteQueryOperator(\n        task_id=\"count_rows_in_users_table\",\n        sql=\"SELECT count(*) FROM users;\",\n    )\n\n    sql_template.render()\n\n    sql_template_count.render()\n\n# [END howto_operator_sql_template]\n\n# Task dependency set by SQLExecuteQueryOperator\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_784_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a BigQuery data pipeline that demonstrates creating datasets, tables, and executing SQL queries for data insertion and retrieval. Build a workflow that showcases different BigQuery operators and demonstrates basic data management techniques in a cloud data warehouse environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"bigquery_operator\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"bigquery-example-export.csv\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe\"\n\nTABLE_NAME_1 = \"table_1\"\nTABLE_NAME_2 = \"table_2\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n        location=LOCATION,\n    )\n\n    create_table_1 = BigQueryCreateTableOperator(\n        task_id=\"create_table_1\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME_1,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n        location=LOCATION,\n    )\n\n    create_table_2 = BigQueryCreateTableOperator(\n        task_id=\"create_table_2\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME_2,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n        location=LOCATION,\n    )\n\n    # [START howto_operator_bigquery_insert_job]\n    load_data = BigQueryInsertJobOperator(\n        task_id=\"load_data\",\n        configuration={\n            \"query\": {\n                \"query\": \"LOAD DATA INOUTFILE 'gs://%(bucket_name)s/%(file_name)s'\n                INTO TABLE bigquery_operator_table\n                PARTITION(dt=STRFTIME('%Y-%m-%d',%%date))\",\n                \"useLegacySql\": False,\n            }\n        },\n        location=LOCATION,\n    )\n    # [END howto_operator_bigquery_insert_job]\n\n    # [START howto_operator_bigquery_select]\n    select_query = BigQuerySelectOperator(\n        task_id=\"select_query\",\n        sql=f\"SELECT * FROM `bigquery_operator_dataset.bigquery_operator_table`\",\n        location=LOCATION,\n    )\n    # [END howto_operator_bigquery_select]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        >> create_table_1\n        >> create_table_2\n        >> load_data\n        # TEST BODY\n        >> select_query\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_471_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that uses multimodal machine learning models to process and evaluate text and image data. The pipeline should leverage Google Cloud services to perform embedding, generation, and safety-filtered content evaluation across different input types.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.natural_language import (\n    CloudNaturalLanguageAnnotateTextOperator,\n    CloudNaturalLanguageTextType,\n    CloudNaturalLanguageProcessModelBatchAnnotateFilesOperator,\n)\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAnnotateImageOperator,\n    CloudVisionCreateImageObjectDetectionConfigOperator,\n    CloudVisionCreateImageObjectRecognitionConfigOperator,\n    CloudVisionDeleteImageObjectDetectionConfigOperator,\n    CloudVisionDeleteImageObjectRecognitionConfigOperator,\n    CloudVisionGetImageObjectDetectionConfigOperator,\n    CloudVisionGetImageObjectRecognitionConfigOperator,\n    CloudVisionAnnotateImageOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\n# [START howto_operator_gcp_natural_language_annotate_gcp_path]\nGCP_NATURAL_LANGUAGE_ANTONATE_BUCKET_NAME = \"antonate-bucket\"\nGCP_NATURAL_LANGUAGE_ANTONATE_URI = \"gs://antonate-bucket/antonate.txt\"\nGCP_VISION_IMAGE_URL = \"gs://vision-image-recognition-bucket/labels_list.txt\"\n# [END howto_operator_gcp_natural_language_annotate_gcp_path]\n\nwith DAG(\n    dag_id=\"example_gcp_natural_language_antonate\",\n    start_date=datetime(2022, 1, 1),\n    default_args={\"project_id\": \"foo\"},\n    schedule=None,\n    catchup=False,\n) as dag:\n    @task\n    def create_bucket():\n        GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=GCP_NATURAL_LANGUAGE_ANTONATE_BUCKET_NAME)\n\n    @task\n    def create_annotate_text_request(text, language_list):\n        return {\n            \"document\": {\n                \"source\": {\"gcs_source\": {\"uri\": GCP_NATURAL_LANGUAGE_ANTONATE_URI}},\n                \"language_list\": language_list,\n            },\n            \"annotator_config\": {\"type_\": CloudNaturalLanguageTextType.TYPE_SUGGESTION},\n        }\n\n    @task\n    def create_batch_annotate_text_request(*, GC_BUCKET, resource_data):\n        return {\n            \"embedding_configs\": [\n                {\n                    \"source\": {\"gcs_source\": {\"uri\": GC_BUCKET}},\n                    \"type_\": CloudNaturalLanguageTextType.TYPE_OWNED,\n                    \"resource_data\": resource_data[i],\n                }\n                for i, GC_BUCKET in enumerate([GCP_NATURAL_LANGUAGE_ANTONATE_BUCKET_NAME] * 2)\n            ],\n            \"language_codes\": [l for c in language_list for l in c],\n            \"target_language_codes\": [l for c in language_list for l in c],\n        }\n\n    @task\n    def create_vision_image_object_detection_config():\n        return {\n            \"image_object_detection_config\": CloudVisionImageObjectDetectionConfig(\n                source_image=CloudVisionCreateImageObjectDetectionConfigSourceImage(\n                    image_uri=GCP_VISION_IMAGE_URL\n                )\n            )\n        }\n\n    @task\n    def create_vision_image_object_recognition_config():\n        return {\n            \"image_object_recognition_config\": CloudVisionImageObjectRecognitionConfig(\n                source_image=CloudVisionCreateImageObjectRecognitionConfigSourceImage(\n                    image_uri=GCP_VISION_IMAGE_URL\n                )\n            )\n        }\n\n    @task\n    def get_vision_image_object_detection_config(config):\n        return {\n            \"image_object_detection_config\": CloudVisionGetImageObjectDetectionConfigRequest(config=config)\n        }\n\n    @task\n    def get_vision_image_object_recognition_config(config):\n        return {\n            \"image_object_recognition_config\": CloudVisionGetImageObjectRecognitionConfigRequest(config=config)\n        }\n\n    @task\n    def annotate_image(bucket_name, object_detection_config, image_uri, resource_data):\n        return {\n            \"image_annotation_request\": {\n                \"source\": {\"gcs_source\": {\"uri\": image_uri}},\n                \"object_detection_config\": object_detection_config[\"image_object_detection_config\"],\n                \"text_detection_config\": {},\n                \"image_rounding_mode\": CloudVisionAnnotateImageRoundiNgMode.CENTROID,\n                \"ppapi_sharing_mode\": CloudVisionAnnotateIamgePpapiSharingMode.AUTO,\n                \"do_cloud_safe_search_check\": True,\n                \"document_pages\": [{\"gcs_source\": {\"uri\": f\"gs://{bucket_name}/page_{i}.txt\"}}]\n                for i, bucket_name in enumerate([bucket_name] * 2)\n            },\n            \"document_resources\": [\n                {\n                    \"source\": {\"gcs_source\": {\"uri\": resource_data[i]}},\n                    \"type_\": CloudVisionAnnotateImageDocumentResourceType.OWNED,\n                }\n                for i, bucket_name in enumerate([bucket_name] * 2)\n            ],\n        }\n\n    @task\n    def annotate_image_batch(bucket_name, object_recognition_config, image_uri, resource_data):\n        return {\n            \"image_annotation_request\": {\n                \"source\": {\"gcs_source\": {\"uri\": image_uri}},\n                \"object_detection_config\": {},\n                \"text_detection_config\": {},\n                \"image_rounding_mode\": CloudVisionAnnotateImageRoundiNgMode.CENTROID,\n                \"ppapi_sharing_mode\": CloudVisionAnnotateIamgePpapiSharingMode.AUTO,\n                \"do_cloud_safe_search_check\": True,\n                \"document_pages\": [\n                    {\n                        \"gcs_source\": {\"uri\": f\"gs://{bucket_name}/page_{i}.txt\"},\n                    }\n                    for i, bucket_name in enumerate([bucket_name] * 2)\n                ],\n            },\n            \"document_resources\": [\n                {\n                    \"source\": {\"gcs_source\": {\"uri\": resource_data[i]}},\n                    \"type_\": CloudVisionAnnotateImageDocumentResourceType.BATCH,\n                }\n                for i, bucket_name in enumerate([bucket_name] * 2)\n            ],\n        }\n\n    @task\n    def write_output(output, filename):\n        with open(filename, \"w\") as f:\n            for i, item in enumerate(output):\n                f.write(f\"{i}\\t{item['value']} {item['type']} {item['document_page_uri']}</n>\")\n        return f\"Wrote {len(output)} lines to {filename}\"\n\n    create_bucket_task = create_bucket()\n    annotate_text_request_task = create_annotate_text_request(\"gs://antonate-bucket/antonate.txt\", [\"en\"])\n    batch_annotate_text_request_task = CloudNaturalLanguageProcessModelBatchAnnotateFilesOperator(\n        task_id=\"batch_annotate_text_task\",\n        project_id=\"foo\",\n        text_params=[\n            CloudNaturalLanguageTextType.TYPE_SUGGESTION,\n            CloudNaturalLanguageTextType.TYPE_OWNED,\n            CloudNaturalLanguageTextType.TYPE_UNANNOTIFIED,\n            CloudNaturalLanguageTextType.TYPE_DOC,\n        ],\n        gcs_source=[GCP_NATURAL_LANGUAGE_ANTONATE_URI],\n        config=create_batch_annotate_text_request_request(),\n    )\n    vision_object_detection_config_task = create_vision_image_object_detection_config()\n    vision_object_recognition_config_task = create_vision_image_object_recognition_config()\n    get_vision_object_detection_config_task = get_vision_image_object_detection_config(vision_object_detection_config_task.output)\n    get_vision_object_recognition_config_task = get_vision_image_object_recognition_config(vision_object_recognition_config_task.output)\n    annotate_image_batch_task = annotate_image_batch(\n        \"gs://antonate-bucket/antonate.txt\", \"vision-image-object-detection-config\", \"gs://antonate-bucket/antonate-1.txt\", \"gs://antonate-bucket/antonate-1.txt page_0.txt\", \"gs://antonate-bucket/antonate-2.txt\", \"gs://antonate-bucket/antonate-2.txt page_0.txt\"\n    )\n    annotate_image_batch_output = BashOperator(\n        task_id=\"annotate_image_batch_output\",\n        bash_command=f\"cat {create_bucket_task.output} {annotate_text_request_task.output} {batch_annotate_text_request_task.output} {get_vision_object_detection_config_task.output} {get_vision_object_recognition_config_task.output} {annotate_image_batch_task.output}\",\n    )\n\n    delete_bucket_task = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=GCP_NATURAL_LANGUAGE_ANTONATE_BUCKET_NAME\n    )\n\n    (\n        create_bucket_task\n        >> annotate_text_request_task\n        >> batch_annotate_text_request_task\n        >> vision_object_detection_config_task\n        >> vision_object_recognition_config_task\n        >> get_vision_object_detection_config_task\n        >> get_vision_object_recognition_config_task\n        >> annotate_image_batch_task\n        >> annotate_image_batch_output\n        >> delete_bucket_task\n    )"}], "metadata": {"file_name": "dag_1714_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud infrastructure management by dynamically creating a Google Cloud Storage bucket, transferring data, and then cleaning up the bucket after the operation. Implement a sequential task flow that handles resource provisioning, data movement, and resource teardown.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.compute_v1 import ComputeEngineV1Hook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils import split_task_id\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_storage_to_compute\"\n\nREGION = \"europe-west1\"\nZONE = REGION + \"-a\"\nNETWORK = \"cloud-samples-registry-i-1\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\nDESTINATION_FILE_PATH = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\nSHORT_MACHINE_TYPE = \"n1-standard-1\"\nDAG_FILE_PATH = \"example_cloud_storage_to_compute.py\"\n\nCONNECTION_ID = \"google_cloud_platform\"\n\nIP_TYPE = \"PRIMARY\"\n\nSOURCE_IP_RANGE_NAME = f\"ip-range/{DAG_ID}/{ENV_ID}\"\nCREATE_INSTANCES_BODY = [\n    {\n        \"name\": f\"instance-{i}\",\n        \"machine_type\": f\"zones/{ZONE}/machineTypes/{SHORT_MACHINE_TYPE}\",\n        \"disks\": [\n            {\n                \"auto_delete\": True,\n                \"boot\": True,\n                \"device_name\": f\"instance-{i}\",\n                \"initialize_params\": {\n                    \"disk_size_gb\": 10,\n                    \"disk_name\": f\"instance-{i}\",\n                    \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-9\",\n                    \"source_element_group\": {},\n                    \"source_image_project\": \"debian-cloud\",\n                },\n            }\n        ],\n        \"network_interfaces\": [\n            {\n                \"access_configs\": [{\"name\": \"PrimaryNVGREAccessConfig\", \"network_tier\": \"PREMIUM\"}],\n                \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n            }\n        ],\n        \"metadata\": {\n            \"http_port_80\": \"\",\n            \"http_port_8050\": \"\",\n            \"enable_ip_forwarding\": \"\",\n            \"enable_iap_inspection\": \"false\",\n        },\n        \"metadata_access_device_name\": True,\n        \"disks\": [\n            {\n                \"auto_delete\": True,\n                \"boot\": True,\n                \"device_name\": f\"instance-{i}\",\n                \"initialize_params\": {\n                    \"disk_size_gb\": 10,\n                    \"disk_name\": f\"instance-{i}\",\n                    \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-9\",\n                    \"source_element_group\": {},\n                    \"source_image_project\": \"debian-cloud\",\n                },\n            }\n        ],\n    }\n    for i in range(3)\n]\nDELETE_INSTANCES_BODY = [\n    f\"instances-{i}\" for i in range(3)\n]\n\nCREATE_POSTGRES_INSTANCE_BODY = {\n    \"name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n    \"machine_type\": f\"zones/{ZONE}/machineTypes/n1-standard-1\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n    \"metadata\": {\n        \"http_port_80\": \"\",\n        \"enable_ip_forwarding\": \"\",\n        \"enable_iap_inspection\": \"false\",\n    },\n    \"metadata_access_device_name\": True,\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n    \"network\": f\"global/networks/{NETWORK}\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n}\n\nCOPY_POSTGRES_INSTANCE_BODY = {\n    \"name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n    \"network\": f\"global/networks/{NETWORK}\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n}\n\nDELETE_POSTGRES_INSTANCE_BODY = [f\"postgres-instance-{DAG_ID}-{ENV_ID}-copy\" for _ in range(3)]\n\nSOURCE_CONNECTION = {\n    \"name\": CONNECTION_ID,\n    \"project\": PROJECT_ID,\n}\n\nDB_NAME = \"testdb\"\nDB_PORT = 5432\nFIREWALL_NAME = f\"firewall/{DAG_ID}/{ENV_ID}\"\nCREATE_FIREWALL_BODY = {\n    \"name\": FIREWALL_NAME,\n    \"network\": f\"global/networks/default\",\n    \"allowed\": [\n        {\"protocol\": \"tcp\", \"portRanges\": [\"{port:{protocol:tcp, number:%s}}\" % {\"number\": DB_PORT}]},\n        {\"protocol\": \"udp\", \"portRanges\": [\"{port:{protocol:udp, number:%s}}\" % {\"number\": DB_PORT}]},\n    ],\n    \"source_ranges\": [],\n    \"source_ports\": [],\n    \"description\": \"Test firewall\",\n    \"network_tier\": IP_TYPE,\n}\n\nPROJ_ID = f\"project-{DAG_ID}-{ENV_ID}\"\nGCE_POSTGRES_INSTANCE_NAME = f\"postgres-instance-{DAG_ID}-{ENV_ID}\"\nCREATE_POSTGRES_INSTANCE_BODY_OUTPUT = {\n    \"name\": GCE_POSTGRES_INSTANCE_NAME,\n    \"network\": f\"global/networks/{NETWORK}\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"{GCE_POSTGRES_INSTANCE_NAME}\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"{GCE_POSTGRES_INSTANCE_NAME}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \"source_image_project\": \"debian-cloud\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": f\"{GCE_POSTGRES_INSTANCE_NAME}\",\n            \"initialize_params\": {\n                \"disk_size_gb\": 10,\n                \"disk_name\": f\"{GCE_POSTGRES_INSTANCE_NAME}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/debian-11\",\n                \"source_element_group\": {},\n                \""}], "metadata": {"file_name": "dag_894_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function that takes a string and returns a new string with the first letter of each word capitalized.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n@task\ndef capitalize_words(input_string):\n    \"\"\"\n    Function to capitalize the first letter of each word in a given string.\n    \"\"\"\n    words = input_string.split()\n    capitalized_words = [word.capitalize() for word in words]\n    return \" \".join(capitalized_words)\n\nwith DAG(\n    \"capitalize_words_dag\",\n    default_args={\"location\": \"/tmp\"},\n    description=\"Demonstrating the capitalize_words function with Airflow.\",\n    start_date=datetime(2021, 3, 4),\n    schedule=None,\n    catchup=False,\n) as dag:\n    bash_task = BashOperator(\n        task_id=\"capitalize_words_example\",\n        bash_command=f\"echo \\\"{capitalize_words('hello world')}\\\",\",\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Dataproc workflow template in Google Cloud, showing how to set up a Hadoop/Spark cluster and run a Pig job. The workflow should illustrate the lifecycle of a cloud data processing job from template creation to execution.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_pig\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 1024,\n        },\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 1024,\n        },\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 1024,\n        },\n    },\n}\n\n# Jobs definitions\n# [START how_to_dataproc_pig_config]\nPIG_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_id\": CLUSTER_NAME},\n    \"pig_job\": {\"query_list\": {\"queries\": [\"define HiveBasicExample HiveBasicExample.py\"]}},\n}\n# [END how_to_dataproc_pig_config]\n\n\nwith models.DAG(\n    project_id=PROJECT_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"system_tests\"],\n    id=DAG_ID,\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START how_to_dataproc_pig_operator]\n    run_pig_task = DataprocSubmitJobOperator(\n        task_id=\"pig_task\", job=PIG_JOB, region=REGION, project_id=PROJECT_ID\n    )\n    # [END how_to_dataproc_pig_operator]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> run_pig_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils import get_test_run  # noqa: E402\n\n    # Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\n    test_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_525_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function that calculates the total cost of a list of items, where each item is represented as a dictionary with keys 'name', 'quantity', and 'price_per_unit'. The function should return the total cost of all items.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nfrom system.openlineage.operator import OpenLineageTestOperator\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_python]\n    @task\n    def get_total_cost(items: list[dict]):\n        total = sum(item[\"price_per_unit\"] * item[\"quantity\"] for item in items)\n        return total\n\n    # [END howto_operator_python]\n\n    @task\n    def print_output(output):\n        print(output)\n\n    # [START howto_operator_python_result]\n    run_this = BashOperator(\n        task_id=\"run_this\",\n        bash_command=f\"echo {get_total_cost('{{params.items}}')}\",\n        do_xcom_push=True,\n    )\n\n    print_output(run_this.output)\n    # [END howto_operator_python_result]\n\n    # [START howto_operator_openlineage]\n    check_openlineage_events = OpenLineageTestOperator(\n        task_id=\"check_openlineage_events\",\n        file_json=json.dumps(\n            {\n                \"resources\": [\n                    {\n                        \"resource_id\": \"example-rds-instance\",\n                        \"event_type\": \"SCHEDULED_SLOW_QUERY_LOGIC_DELETE\",\n                        \"actor\": \"airflow_user\",\n                        \"actor_uid\": 1,\n                        \"resource_status\": \"Completed\",\n                        \"primitive_results\": {\"deleted\": 12},\n                    }\n                ]\n            }\n        )\n    )\n    # [END howto_operator_openlineage]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a machine learning pipeline that demonstrates Vertex AI dataset creation and pipeline job management using Google Cloud resources. Create a workflow that synchronizes housing data, creates a tabular dataset, runs a predefined ML pipeline, and then manages the pipeline job lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Pipeline Job operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSFileTransformOperator,\n    GCSListObjectsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import (\n    DeletePipelineJobOperator,\n    GetPipelineJobOperator,\n    ListPipelineJobOperator,\n    RunPipelineJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"vertex_ai_pipeline_job\"\nREGION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_OBJECT_NAME = \"pipeline/vertex-ai-example-pipeline-job-dataset.csv\"\n\nDESTINATION_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\nPOLICY = {\n    \"fail_after_step_completion\": False,\n    \"max_job_run_time_mins\": 30,\n}\n\nTILESET_ID = f\"tileset_{DAG_ID}_{ENV_ID}\"\n_PIPELINE_job_id = f\"pipeline-job-{ENV_ID}\"\n\nPARAMETER_VALUES = {\n    \"train_budget_milli\": \"1000\",\n    \"job_id\": _pipeline_job_id,\n    \"dest_bucket\": DESTINATION_BUCKET_NAME,\n    \"input_dataset\": f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/{DATA_SAMPLE_GCS_OBJECT_NAME}\",\n    \"pipeline_path\": f\"gs://{RESOURCE_DATA_BUCKET}/vertex-ai/pipeline/pipeline.json\",\n    \"region\": REGION,\n    \"project\": PROJECT_ID,\n}\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"pipeline_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    @task\n    def create_tileset():\n        \"\"\"\n        Creating Tileset.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import create_tileset_operator\n\n        create_tileset_operator(\n            task_id=\"create_tileset\",\n            project_id=PROJECT_ID,\n            region=REGION,\n            tileset_id=TILESET_ID,\n            parameter_values=PARAMETER_VALUES,\n            resource_data_bucket=RESOURCE_DATA_BUCKET,\n        )\n\n    create_tileset_task = create_tileset()\n\n    @task\n    def get_tileset_output():\n        \"\"\"\n        Getting output of Tileset.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import get_tileset_operator\n\n        get_tileset_operator(\n            task_id=\"get_tileset_output\",\n            project_id=PROJECT_ID,\n            region=REGION,\n            tileset_id=TILESET_ID,\n            # Externally, the Vertex AI service documentation states that the response isPERSISTED always returns\n            # True for Tilesets.  However, internal testing found that the response field\n            # has the old name of Is Output PERSISTED.\n            parameter_values={\"is_output_persisted\": True},\n        )\n\n    get_tileset_output_task = get_tileset_output()\n\n    @task\n    def list_pipeline_job():\n        \"\"\"\n        Listing PipelineJob.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import list_pipeline_job_operator\n\n        list_pipeline_job_operator(\n            task_id=\"list_pipeline_job\",\n            project_id=PROJECT_ID,\n            # The view will show the latest pipeline job created for each region.\n            page_token=\"please-use-query-string-for-page-token\",\n            location=TILESET_ID,\n        )\n\n    list_pipeline_job_task = list_pipeline_job()\n\n    @task\n    def delete_pipeline_job():\n        \"\"\"\n        Deleting PipelineJob.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import delete_pipeline_job_operator\n\n        delete_pipeline_job_operator(\n            task_id=\"delete_pipeline_job\",\n            project_id=PROJECT_ID,\n            pipeline_job_id=_pipeline_job_id,\n        )\n\n    delete_pipeline_job_task = delete_pipeline_job()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_tileset():\n        \"\"\"\n        Deleting Tileset.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import delete_tileset_operator\n\n        delete_tileset_operator(\n            task_id=\"delete_tileset\",\n            project_id=PROJECT_ID,\n            tileset_id=TILESET_ID,\n        )\n\n    delete_tileset_task = delete_tileset()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_bucket():\n        \"\"\"\n        Deleting bucket.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n        from airflow.providers.google.cloud.operators.gcs import GCSDeleteBucketOperator\n\n        GCSDeleteBucketOperator(\n            task_id=\"delete_bucket\",\n            bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        )\n\n    delete_bucket_task = delete_bucket()\n\n    # [START how_to_cloud_vertex_ai_run_pipeline_job_operator)\n    run_pipeline_job_task = RunPipelineJobOperator(\n        task_id=\"run_pipeline_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        pipeline_job_id=_pipeline_job_id,\n        tileset_id=TILESET_ID,\n        resource_data_bucket=RESOURCE_DATA_BUCKET,\n    )\n    # [END how_to_cloud_vertex_ai_run_pipeline_job_operator]\n\n    # [START how_to_cloud_vertex_ai_get_pipeline_job_operator)\n    get_pipeline_job_task = GetPipelineJobOperator(\n        task_id=\"get_pipeline_job\",\n        project_id=PROJECT_ID,\n        pipeline_job_id=_pipeline_job_id,\n    )\n    # [END how_to_cloud_vertex_ai_get_pipeline_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_pipeline_job_operator)\n    delete_pipeline_job_task = DeletePipelineJobOperator(\n        task_id=\"delete_pipeline_job\",\n        project_id=PROJECT_ID,\n        pipeline_job_id=_pipeline_job_id,\n    )\n    # [END how_to_cloud_vertex_ai_delete_pipeline_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_pipeline_job_operator)\n    list_pipeline_job_task = ListPipelineJobOperator(\n        task_id=\"list_pipeline_job\",\n        project_id=PROJECT_ID,\n        page_token=\"please-use-query-string-for-page-token\",\n        location=TILESET_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_pipeline_job_operator]\n\n    (\n        create_bucket\n        >> create_tileset_task\n        >> get_tileset_output_task\n        >> list_pipeline_job_task\n        >> run_pipeline_job_task\n        >> get_pipeline_job_task\n        >> delete_pipeline_job_task\n        >> delete_tileset_task\n        >> delete_bucket_task\n        >> delete_pipeline_job_task\n    )\n\n    # Task dependencies created via `XComArgs` are not executed by default and need to be executed explicitly.\n    from airflow.operators.bash import BashOperator\n\n    BashOperator(\n        task_id=\"explicit_task_dep_creation\",\n        bash_command=\"\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )"}], "metadata": {"file_name": "dag_1327_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates complex marketing data operations across Google Cloud Platform services, focusing on extracting advertising performance metrics and managing data transfer between Google Display Video 360 and BigQuery. Implement tasks for downloading line items, uploading files, and performing data transformations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.display_video import (\n    DisplayVideoDownloadLineItemsOperator,\n    DisplayVideoUploadFilesOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"display_video\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.csv\"\nDESTINATION_FILE_PATH = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nPATH_TO_SHARING_FILE = \"/tmp/sharing_file.csv\"\nUPLOAD_FILE_PATH = \"/tmp/\"\r\nSAMPLE_DATA_FILE = \"display_video/sample_data.csv\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"display_video\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    copy_sample_file_to_bucket = GCSToGCSOperator(\n        task_id=\"copy_sample_file_to_bucket\",\n        source_bucket=\"airflow-system-tests-resources\",\n        source_object=SAMPLE_DATA_FILE,\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_display_video_download_line_items_operator]\n    download_line_items = DisplayVideoDownloadLineItemsOperator(\n        task_id=\"download_line_items\",\n        project_id=PROJECT_ID,\n        params={\"level\": \"COUNTRY\", \"reporting_only\": \"FALSE\"},\n    )\n    # [END howto_display_video_download_line_items_operator]\n\n    # [START howto_display_video_upload_files_operator]\n    upload_files = DisplayVideoUploadFilesOperator(\n        task_id=\"upload_files\",\n        project_id=PROJECT_ID,\n        body={\"sharing_file\": DESTINATION_FILE_PATH},\n    )\n    # [END howto_display_video_upload_files_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> copy_sample_file_to_bucket\n        >> download_line_items\n        >> upload_files\n        # TEST BODY\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove\n    # the temp file\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_711_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates conditional task execution using short-circuit logic, where certain tasks only run based on predefined boolean conditions. Create a DAG that showcases how tasks can be dynamically skipped or executed depending on specific criteria.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task\ndef check_condition(condition: bool):\n    if condition:\n        print(\"Condition met, task executed!\")\n\n\n@task\ndef my_task():\n    print(\"Hello World!\")\n\n\ndef my_main():\n    # [START example_conditional_operator]\n    dag1 = DAG(\n        dag_id=\"example_conditional_operator\",\n        start_date=pendulum.datetime(2028, 1, 1, 0, 0, 0),\n        schedule=None,\n        tags=[\"example\"],\n        catchup=False,\n    )\n    # [END example_conditional_operator]\n\n    check_condition(condition=True)\n    my_task()\n    check_condition(condition=False)\n\n\nif __name__ == \"__main__\":\n    my_main()"}], "metadata": {"file_name": "dag_24_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data movement and management pipeline that transfers files between cloud storage buckets while implementing systematic tracking and cleanup procedures. Create a workflow that demonstrates how data can be safely moved, listed, and deleted using cloud-native orchestration techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.filesystem.operators.diff import DiffOperator\nfrom airflow.providers.common.filesystem.transfers.copy_file import CopyFileOperator\nfrom airflow.providers.common.filesystem.transfers.copy_single_file import CopySingleFileOperator\nfrom airflow.providers.common.filesystem.transfers.copy_tree import CopyTreeOperator\nfrom airflow.providers.common.filesystem.transfers.move_file import MoveFileOperator\nfrom airflow.providers.common.filesystem.transfers.move_tree import MoveTreeOperator\nfrom airflow.providers.common.filesystem.utils import make_temp_directory\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_file_system_transfer_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_temp_directory():\n        return make_temp_directory(prefix=\"file-system-\")\n\n    @task\n    def create_connection(conn_id_name):\n        connection = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"s3\",\n            description=\"Connection to Amazon S3\",\n            extra={\"bucket\": \"bucket_non_local\"},\n        )\n        return connection\n\n    # [START howto_operator_diff]\n    diff = DiffOperator(\n        task_id=\"diff_task\",\n        path1=\"/empty1\",\n        path2=\"/empty2\",\n        print_diff=True,\n    )\n    # [END howto_operator_diff]\n\n    # [START howto_operator_copy_single_file]\n    copy_single_file = CopySingleFileOperator(\n        task_id=\"copy_single_file_task\", input_path=\"/non_local/empty1\", output_path=\"/non_local/empty2\"\n    )\n    # [END howto_operator_copy_single_file]\n\n    # [START howto_operator_copy_file]\n    copy_file = CopyFileOperator(\n        task_id=\"copy_file_task\", input_path=\"/non_local/empty1\", output_path=\"/non_local/empty2\"\n    )\n    # [END howto_operator_copy_file]\n\n    # [START howto_operator_copy_tree]\n    copy_tree = CopyTreeOperator(task_id=\"copy_tree_task\", input_path=\"/non_local/dir1\", output_path=\"/non_local/\")\n    # [END howto_operator_copy_tree]\n\n    # [START howto_operator_move_file]\n    move_file = MoveFileOperator(\n        task_id=\"move_file_task\", input_path=\"/non_local/empty1\", output_path=\"/non_local/empty2\"\n    )\n    # [END howto_operator_move_file]\n\n    # [START howto_operator_move_tree]\n    move_tree = MoveTreeOperator(\n        task_id=\"move_tree_task\", input_path=\"/non_local/dir1\", output_path=\"/non_local/\"\n    )\n    # [END howto_operator_move_tree]\n\n    create_temp_directory_task = create_temp_directory()\n    create_connection_task = create_connection(\"connection_1\")\n\n    copy_single_file_task = copy_file()\n    move_file_task = move_file()\n    copy_tree_task = copy_tree()\n    move_tree_task = move_tree_task()"}], "metadata": {"file_name": "dag_2055_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates container-based task scheduling and execution using Docker Swarm operator. Create a workflow that launches a CentOS container, executes a brief sleep operation, and automatically removes the container after completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.docker.operators.docker_swarm import DockerSwarmOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_swarm_operator_dag\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"docker\"],\n) as dag:\n    t1 = DockerSwarmOperator(\n        api_conn_id=\"docker_swarm\",\n        task_id=\"win_container_task\",\n        docker_url=\"unix://var/run/docker.sock\",  # Customize this url\n        command=\"sh -c 'sleep 10 && echo done && exit 0'\",\n        image=\"centos:latest\",\n        container_name=\"win_container\",\n        network_mode=\"swarm\",\n        env=[\"PROMPT=1\"],\n        depends_on=\"t2\",\n        dag=dag,\n    )\n\n    t2 = DockerSwarmOperator(\n        api_conn_id=\"docker_swarm\",\n        task_id=\"linux_container_task\",\n        docker_url=\"unix://var/run/docker.sock\",  # Customize this url\n        command=\"sh -c 'sleep 10 && echo done && exit 0'\",\n        image=\"centos:latest\",\n        container_name=\"linux_container\",\n        network_mode=\"swarm\",\n        env=[\"PROMPT=1\"],\n        depends_on=\"d1\",\n        dag=dag,\n    )\n\n    t1 >> t2\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_453_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data quality workflow that ingests raw data, performs initial analysis, and checks data integrity. Create a branching process that either saves clean data and generates a report, or handles errors by describing issues and sending notifications.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START howto_branch_python]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef branch():\n    print(\"Branch 1\")\n\n\n@task\ndef branch_true():\n    print(\"Branch True\")\n\n\n@task\ndef branch_false():\n    print(\"Branch False\")\n\n\nwith DAG(\n    dag_id=\"example_branch_python\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    branch()\n    BashOperator(task_id=\"branch_1_result\", bash_command=\"echo Branch 1 ran\", trigger_rule=TriggerRule.NONE_FAILED)\n    BashOperator(\n        task_id=\"branch_2_result\", bash_command=\"echo Branch 2 ran\", inlets=[\"branch_1_result\", \"branch_true\"]\n    )\n    BashOperator(task_id=\"branch_false_result\", bash_command=\"echo Branch False ran\", inlets=[\"branch_false\"])\n\n# [END howto_branch_python]"}], "metadata": {"file_name": "dag_325_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data enrichment pipeline that uses OpenAI to generate descriptive information about a static list of items, leveraging Airflow's task expansion and deferred execution capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nSAMPLE_INPUT = json.dumps({\"id\": \"123\", \"name\": \"John Doe\", \"items\": [{\"id\": \"456\", \"name\": \"Apple\"}]})\n\n\n@task\ndef generate_vector(*, input_data: str, model=\"text-embedding-ada-002\") -> dict:\n    return {\"vector\": \"0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef\"}  # type: ignore\n\n\nwith DAG(\n    \"example_openai_embedding_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n):\n    # [START howto_operator_openai_embedding]\n    embed = OpenAIEmbeddingOperator(task_id=\"openai_embedding_task\", model=\"text-embedding-ada-002\")\n    # [END howto_operator_openai_embedding]\n\n    # [START howto_operator_openai_embedding_result]\n    embed_result = BashOperator(\n        task_id=\"openai_embedding_result\",\n        bash_command=f\"echo {embed.result}\",\n        do_xcom_push=True,\n    )\n    # [END howto_operator_openai_embedding_result]\n\n    # [START howto_operator_openai_embedding_async]\n    embed_async = OpenAIEmbeddingOperator(\n        task_id=\"openai_embedding_async\", input_data=SAMPLE_INPUT, model=\"text-embedding-ada-002\"\n    )\n    # [END howto_operator_openai_embedding_async]\n\n    embed_async >> embed_result"}], "metadata": {"file_name": "dag_2174_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that retrieves specific information about Tom Hanks from a Neo4j graph database on a daily basis. Design the DAG to execute a Cypher query targeting a node with Tom Hanks' details, using the current date as a dynamic parameter.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.neo4j.operators.neo4j import Neo4jOperator\n\ntry:\n    from airflow.providers.standard.operators.python import PythonOperator\nexcept ImportError:\n    from airflow.operators.python import PythonOperator  # type: ignore\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"neo4j_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = Neo4jOperator(\n        task_id=\"neo4j_example\",\n        neo4j_conn_id=\"neo4j_conn\",\n        sql=\"MATCH (tomHanks:Actor {name: 'Tom Hanks'}) RETURN tomHanks\",\n        python_callable=PythonOperator,\n        op_kwargs={\"task_id\": \"run_this\"},\n        dag=dag,\n    )\n\n    @task\n    def set_connection():\n        conn = Connection(\n            conn_id=\"neo4j_conn\",\n            conn_type=\"neo4j\",\n        )\n        session = conn.get_extra()\n        neo_url = session.get(\"neo_url\")\n        neo_user = session.get(\"neo_user\")\n        neo_password = session.get(\"neo_password\")\n        print(f\"Setting connection with neo_url: {neo_url}, neo_user: {neo_user}, neo_password: {neo_password}\")\n        conn.set_extra({\"neo_url\": neo_url, \"neo_user\": neo_user, \"neo_password\": neo_password})\n\n    set_connection()\n    run_this"}], "metadata": {"file_name": "dag_2172_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that monitors a specified input directory for new files, validates file presence, and performs a controlled file transfer using containerized operations. Implement a robust mechanism for detecting, validating, and processing files with built-in error handling and logging.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors import SFTPSensor\nfrom airflow.providers.sftp.transfers.local_to_sftp import LocalFilesystemToSFTPSensor\nfrom airflow.providers.sftp.transfers.sftp_to_local import SFTPToLocalFilesystemSensor\nfrom airflow.providers.sftp.transfers.sftp_to_sftp import SFTPToSFTPFile\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_sensor_dag\"\n\nsftp_conn = Connection(\n    conn_id=\"sftp_conn\",\n    conn_type=\"sftp\",\n    host=\"sftp.example.com\",\n    login=\"xxxyyyzzzzzz\",\n    password=\"Toor@123\",\n)\n\nfile_path = \"/tmp/test_file.txt\"\n\n\n@task\ndef create_file():\n    \"\"\"Empty function to create a file\"\"\"\n    conn = Session().query(Connection).filter(Connection.conn_id == \"sftp_conn\").first()\n    with conn.open() as file:\n        file.write(b\"\")\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    t1 = create_file()\n    t2 = LocalFilesystemToSFTPSensor(\n        task_id=\"local_to_sftp_sensor\",\n        local_filepath=file_path,\n        remote_filepath=\"/tmp/\",\n        allow_missing=True,\n    )\n    t3 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor\",\n        local_filepath=file_path,\n    )\n    t4 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor2\",\n        local_filepath=file_path,\n    )\n    t5 = LocalFilesystemToSFTPSensor(\n        task_id=\"local_to_sftp_sensor2\",\n        local_filepath=file_path,\n        remote_filepath=\"/tmp/\",\n        allow_missing=True,\n    )\n    t6 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor3\",\n        local_filepath=file_path,\n    )\n    t7 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor3\",\n        local_filepath=file_path,\n    )\n    t8 = LocalFilesystemToSFTPSensor(\n        task_id=\"local_to_sftp_sensor3\",\n        local_filepath=file_path,\n        remote_filepath=\"/tmp/\",\n        allow_missing=True,\n    )\n    t9 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor4\",\n        local_filepath=file_path,\n    )\n    t10 = SFTPToLocalFilesystemSensor(\n        task_id=\"sftp_to_local_sensor4\",\n        local_filepath=file_path,\n    )\n\n    t2.trigger_rule = TriggerRule.POST\u5f00\u82b1\n    t3.trigger_rule = TriggerRule.POST\u5f00\u82b1\n    t4.trigger_rule = TriggerRule.ALL_DONE\n    t5.trigger_rule = TriggerRule.ALL_DONE\n    t6.trigger_rule = TriggerRule.ALL_DONE\n    t7.trigger_rule = TriggerRule.ALL_DONE\n    t8.trigger_rule = TriggerRule.ALL_DONE\n    t9.trigger_rule = TriggerRule.ALL_DONE\n    t10.trigger_rule = TriggerRule.ALL_DONE\n\n    t1 >> [t2, t3]\n    t4 >> t5\n    t6 >> t7\n    t8 >> t9\n    t10 >> [t6, t7, t8]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_794_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data profiling pipeline that uses AWS Glue DataBrew to analyze a JSON dataset stored in S3, generating insights and managing computational resources dynamically. Construct a workflow that shows how to programmatically profile data with full lifecycle management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.glue import GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.glue import GlueJobStartOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_glue_databrew\"\n\n# [START howto_glue_databrew_env_variables_def]\nENV_ID = sys_test_context_task()\nGLUE_DATABREW_JOB_NAME = \"databrew-job\"\n# [END howto_glue_databrew_env_variables_def\"]\n\n# [START howto_glue_databrew_bucket_def]\nSAMPLE_S3_BUCKET = f\"{ENV_ID}-databrew-bucket\"\n# [END howto_glue_databrew_bucket_def]\n\n\n@task\ndef create_aws_databrew_role():\n    \"\"\"Creates an AWS Databrew Execution Role with S3 access.\"\"\"\n\n    client = boto3.client(\"iam\")\n\n    client.create_role(\n        Path=\"/\",\n        Name=\"databrewExecutionRole\",\n        AssumeRolePolicyDocument=json.dumps(\n            {\n                \"Statement\": [\n                    {\n                        \"Action\": [\"sts:AssumeRole\"],\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\"Service\": \"databrew.amazonaws.com\"},\n                    }\n                ],\n            }\n        ),\n    )\n\n    client.get_waiter(\"role_exists\").wait(\n        RoleNames=[\"databrewExecutionRole\"],\n    )\n\n    client.attach_role_policy(\n        RoleName=\"databrewExecutionRole\",\n        PolicyName=\"S3ReadOnlyAccess\",\n    )\n\n\n@task\ndef create_databrew_job():\n    \"\"\"Creates a databrew job that munges data in a S3 bucket.\"\"\"\n\n    hook = GlueJobHook()\n    hook.create(\n        JobName=GLUE_DATABREW_JOB_NAME,\n        DurationInHours=1,\n        StartAtBeginJobHour=12,\n        EndAtEndJobHour=13,\n        InputS3Uri=f\"s3://{SAMPLE_S3_BUCKET}/with_azures.txt\",\n        OutputS3Uri=f\"s3://{SAMPLE_S3_BUCKET}/with_databrew.txt\",\n        RoleArn=\"arn:aws:iam::123456789012:role/databrewExecutionRole\",\n    )\n\n\n@task\ndef delete_databrew_job():\n    \"\"\"Deletes a databrew job.\"\"\"\n    glue_job_hook = GlueJobHook()\n    glue_job_hook.delete(JobName=GLUE_DATABREW_JOB_NAME)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_aws_databrew_execution_role():\n    \"\"\"Deletes the aws databrew execution role.\"\"\"\n    client = boto3.client(\"iam\")\n    client.delete_role(\n        Name=\"databrewExecutionRole\",\n    )\n    client.get_waiter(\"role_not_exists\").wait(\n        RoleNames=[\"databrewExecutionRole\"],\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID]\n    sample_s3_bucket = f\"{env_id}-databrew-bucket\"\n    glue_databrew_job_name = GLUE_DATABREW_JOB_NAME\n\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\",\n        bucket_name=sample_s3_bucket,\n    )\n\n    # [START howto_glue_databrew_create_execution_role]\n    create_aws_databrew_execution_role = create_aws_databrew_role()\n    # [END howto_glue_databrew_create_execution_role]\n\n    # [START howto_glue_databrew_create_job]\n    create_databrew_job = create_databrew_job()\n    # [END howto_glue_databrew_create_job]\n\n    delete_databrew_job = GlueJobStartOperator(\n        task_id=\"delete_databrew_job\",\n        job_name=glue_databrew_job_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_aws_databrew_execution_role = GlueJobStartOperator(\n        task_id=\"delete_aws_databrew_execution_role\",\n        job_name=glue_databrew_job_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        glue_databrew_job_id=GLUE_DATABREW_JOB_NAME,\n        actions=[\n            {\n                \"actionName\": \"DELETE\",\n                \"roleArn\": \"arn:aws:iam::123456789012:role/databrewExecutionRole\",\n            }\n        ],\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        create_aws_databrew_execution_role,\n        create_databrew_job,\n        delete_databrew_job,\n        # TEST TEARDOWN\n        delete_aws_databrew_execution_role,\n    )"}], "metadata": {"file_name": "dag_1904_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates how to interact with Databricks SQL endpoints for creating tables, inserting data, and performing data transformations. The pipeline should showcase different SQL operations like table creation, data selection, file export, and data import using Airflow and Databricks operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nSTART_DATE = datetime(2022, 1, 1)\nDAG_ID = \"databricks_sql_pipeline\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_SQL_DEFAULT_INSTANCE_ID = os.environ.get(\"DATABRICKS_SQL_DEFAULT_INSTANCE_ID\", \"\")\n\n# [START howto_operator_databricks_sql]\ndef task_sql_decorator(query, parameters=None, conn_id=\"databricks_default\"):\n    def do_inlet(task):\n        sql_cli = task.dag.databases.get(conn_id=conn_id)\n        if parameters:\n            task.add_task(sql_cli.run(query=query, parameters=parameters))\n        else:\n            task.add_task(sql_cli.run(query=query))\n\n    return do_inlet\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=START_DATE,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_databricks_sql]\n    sql_task = DatabricksSubmitRunOperator(\n        task_id=\"sql_task\",\n        sql=\n        \"define sql_test_table = sql context.sql s3_path = 's3://dksgit/airflow/databricks/sql/sample_csv.csv';\" \"select * from sql_test_table;\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_sql]\n\n    # [START howto_sensor_databricks_sql]\n    sql_sensor = DatabricksSensor(\n        task_id=\"sql_sensor\",\n        sql=\"select * from sql_test_table;\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_sensor_databricks_sql]\n\n    sql_task >> sql_sensor\n\n    if __name__ == \"__main__\":\n        dag.run()"}], "metadata": {"file_name": "dag_1163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated database management workflow that demonstrates cross-database interaction and query execution in a Cloud SQL environment. Create a data pipeline that showcases table creation, data insertion, and table deletion across PostgreSQL and MySQL instances.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_execute_query\"\nDB = os.environ.get(\"POSTGRES_DB_NAME\")\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_sql_execute_query]\n    def execute_query_with_postgres(*, sql, db):\n        return SQLExecuteQueryOperator(\n            task_id=f\"{db}-execute-query\",\n            sql=sql,\n            database=db,\n        )()\n\n    execute_query_with_postgres(\n        sql=\"CREATE TABLE test_table (id INT PRIMARY KEY, name VARCHAR(255))\",\n        db=\"postgres\",\n    )\n    # [END howto_operator_sql_execute_query]\n\n    def execute_query_with_mysql(*, sql, db):\n        return SQLExecuteQueryOperator(\n            task_id=f\"{db}-execute-query\",\n            sql=sql,\n            database=db,\n        )()\n\n    execute_query_with_mysql(\n        sql=\"CREATE TABLE test_table (id INT PRIMARY KEY, name VARCHAR(255))\",\n        db=\"mysql\",\n    )"}], "metadata": {"file_name": "dag_36_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete cloud data transfer process, including creating a DynamoDB table, enabling point-in-time recovery, and performing a segmented export to S3. Implement a workflow that showcases dynamic resource management and cloud service interactions.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.swf import SwfCreateJobOperator, SwfStartJobOperator\nfrom airflow.providers.amazon.aws.sensors.swf import SwfJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import weekday\nfrom airflow.utils.trigger_rule import WeekDayTriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_swf\"\n\n# [START howto_operator_swf_create_job]\n# The SWF ROLES GET TASK DEFINITION GET EXECUTION GET STATE LIST JOBS API ENDPOINTS are equivalent to SWF\n# ROLES LIST EXECUTIONS LIST API ENDPOINTS when using the SWFOperator.\n# See the docs for a full list of equivalents.\nROLE_ARN_KEY = \"aws_swf_role_arn\"\nTABULAR_TASK_DEFINITION_NAME = \"Tabular\"\nTABULAR_JOB_NAME = \"TabularJob\"\nTABULAR_INPUT = json.dumps(\n    {\"s3Bucket\": \"EXAMPLE-S3-BUCKET\", \"s3Prefix\": \"EXAMPLE-S3-PREFIX\", \"rowCount\": 1000, \"columnCount\": 4}\n)\n\n# The first two expressions evaluate to True because the expression type is\n# inferred from the values evaluated.  The _key in ROLE_ARN_KEY is\n# capitalized to match the name of the attribute of the Role.\n# The _ in TABULAR_JOB_NAME is all lowercase because it's a argument to\n# create the job and is not a attribute of the object created by\n# creating the job.\ncreate_job_operator_kwargs = {\n    ROLE_ARN_KEY: os.environ[ROLE_ARN_KEY],\n    TABULAR_JOB_NAME: TABULAR_JOB_NAME,\n    \"tabularInput\": TABULAR_INPUT,\n    \"bucketRoleArn\": os.environ[ROLE_ARN_KEY].replace(\".role.\", \".role.\") + \"_execution\",\n    \"defaultScheduleFrequency\": \"BETWEEN-DAY\",\n    \"maxExecutionTimeInHours\": 2,\n    \"isScheduleOnce\": False,\n    \"weekday\": weekday.weekday_name[os.environ.get(\"DAY\", weekday.SATURDAY)],\n    \"hour\": int(os.environ.get(\"TIME\", \"1\")),\n    \"minute\": int(os.environ.get(\"TIME\", \"30\")),\n}\n# [END howto_operator_swf_create_job]\n\n# [START howto_operator_swf_start_job]\nstart_job_operator_kwargs = {\"jobId\": os.environ[\"JOB_ID\"]}\n# [END howto_operator_swf_start_job]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"swf\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\",\n        bucket_name=f\"{ENV_ID}-swf-bucket\"),\n    # [START howto_sensor_swf_job]\n    job_sensor = SwfJobSensor(\n        task_id=\"job_sensor\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='create_job_operator', key='job_id') }}\",\n        firebases={},\n    )\n    # [END howto_sensor_swf_job]\n    get_job_result = SwfCreateJobOperator(\n        task_id=\"get_job_result\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='create_job_operator', key='job_id') }}\",\n        tabularInput=\"{{ task_instance.xcom_pull(task_ids='create_job_operator', key='tabular_input') }}\",\n        bucketRoleArn=os.environ[\"ROLE_ARN\"].replace(\".role.\", \".role.\") + \"_execution\",\n        defaultScheduleFrequency=\"BETWEEN-DAY\",\n        maxExecutionTimeInHours=2,\n        isScheduleOnce=False,\n    )\n    # [START howto_operator_swf_start_job]\n    start_get_job_result_job = SwfStartJobOperator(\n        task_id=\"start_get_job_result_job\",\n        tabularInput=\"{{ task_instance.xcom_pull(task_ids='create_job_operator', key='tabular_input') }}\",\n        bucketRoleArn=os.environ[\"ROLE_ARN\"].replace(\".role.\", \".role.\") + \"_execution\",\n        jobId=\"{{ task_instance.xcom_pull(task_ids='create_job_operator', key='job_id') }}\",\n    )\n    # [END howto_operator_swf_start_job]\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=f\"{ENV_ID}-swf-bucket\",\n        force_delete=True)\n    # [START howto_configure_log_role_for_swf]\n    configure_log_role_for_swf = SwfCreateJobOperator(\n        task_id=\"configure_log_role_for_swf\",\n        tabularInput=json.dumps({\"logRoleArn\": os.environ[\"ROLE_ARN\"]}),\n        defaultScheduleFrequency=\"BETWEEN-DAY\",\n        maxExecutionTimeInHours=1,\n        isScheduleOnce=True,\n    )\n    # [END howto_configure_log_role_for_swf]\n    start_configure_log_role_for_swf_job = SwfStartJobOperator(\n        task_id=\"start_configure_log_result_job\",\n        tabularInput=json.dumps({\"logRoleArn\": os.environ[\"ROLE_ARN\"]}),\n        jobId=\"{{ task_instance.xcom_pull(task_ids='configure_log_role_for_swf', key='job_id') }}\",\n    )\n    delete_log_role_for_swf_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_log_role_for_swf_bucket\",\n        bucket_name=os.environ[\"ROLE_ARN\"].replace(\".role.\", \".role.\")).override(\n        trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # Task dependencies\n    create_s3_bucket >> create_job_operator_kwargs >> start_create_job_operator_kwargs >> get_job_result\n    start_create_job_operator_kwargs >> start_get_job_result_job >> start_configure_log_role_for_swf_job\n    start_configure_log_role_for_swf_job >> configure_log_role_for_swf\n    get_job_result >> start_start_job_operator >> start_configure_log_result_job\n    start_start_job_operator >> start_configure_log_result_job >> start_configure_log_role_for_swf\n    start_configure_log_result_job >> configure_log_role_for_swf >> start_get_job_result_job\n    start_get_job_result_job >> start_start_job_operator\n    start_start_job_operator >> start_configure_log_result_job >> start_configure_log_role_for_swf\n    start_configure_log_role_for_swf >> configure_log_role_for_swf >> start_get_job_result_job\n    start_get_job_result_job >> start_start_job_operator\n    start_start_job_operator >> start_configure_log_result_job >> start_configure_log_role_for_swf\n    start_configure_log_result_job >> configure_log_role_for_swf >> start_get_job_result_job\n    start_get_job_result_job >> start_start_job_operator\n    [create_s3_bucket, delete_s3_bucket, configure_log_role_for_swf, start_configure_log_role_for_swf_bucket] >> \\\n        [delete_log_role_for_swf_bucket]\n\n    # [START howto_operator_swf_get_execution]\n    get_execution = SwfGetExecutionOperator(\n        task_id=\"get_execution\",\n        executionId=\"{{ task_instance.xcom_pull('create_job_operator', key='executionId') }}\",\n        taskDefinition=\"{{ task_instance.xcom_pull('create_job_operator', key='taskDefinition') }}\",\n        startToCloseWaitInterval=10,\n        closeToStartWaitInterval=10,\n    )\n    # [END howto_operator_swf_get_execution]\n    # [START howto_sensor_swf_get_execution]\n    get_execution_sensor = SwfGetExecutionSensor(\n        task_id=\"get_execution_sensor\",\n        execution_id=\"{{ task_instance.xcom_pull('create_job_operator', key='executionId') }}\",\n        task_definition=\"{{ task_instance.xcom_pull('create_job_operator', key='taskDefinition') }}\",\n        poke_interval=10,\n    )\n    # [END howto_sensor_swf_get_execution]\n    # [START howto_operator_swf_list_executions]\n    list_executions = SwfListExecutionsOperator(\n        task_id=\"list_executions\",\n        taskDefinition=TABULAR_TASK_DEFINITION_NAME,\n        startExecutionTaskDefinitionQuery={\n            \"StartExecutionTimeLessThanOrEqualToTimestamp\": \"{{ task_instance.xcom_pull('create_job_operator', \"\n                                                            \"key='executionStartTime') }}\",\n            \"StartExecutionEventName\": \"StartExecution\",\n        },\n        executionStatus=\"SUCCEEDED\",\n        maxItems=100,\n        waitForCompletion=True,\n    )\n    # [END howto_operator_swf_list_executions]\n    # [START howto_sensor_swf_list_executions]\n    list_executions_sensor = SwfListExecutionsSensor(\n        task_id=\"list_executions_sensor\",\n        task_definition=TABULAR_TASK_DEFINITION_NAME,\n        start_execution_task_definition_query={\n            \"StartExecutionTimeLessThanOrEqualToTimestamp\": \"{{ task_instance.xcom_pull('create_job_operator', \"\n                                                           \"key='executionStartTime') }}\",\n            \"StartExecutionEventName\": \"StartExecution\",\n        },\n        execution_status=\"SUCCEEDED\",\n        poke_interval=10,\n    )\n    # [END howto_sensor_swf_list_executions]\n    # [START howto_operator_swf_start_execution]\n    start_execution_operator = SwfStartExecutionOperator(\n        task_id=\"start_execution_operator\",\n        taskDefinition=TABULAR_TASK_DEFINITION_NAME,\n        executionParameters={\"Bucket\": \"{{ task_instance.xcom_pull('create_job_operator', key='s3Bucket') }}\",\n                             \"Key\":"}], "metadata": {"file_name": "dag_722_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python code to query the current weather data from a weather API?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_weather_api\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_weather_api]\n\n    @task\n    def get_weather_api():\n        import requests\n\n        response = requests.get(\"https://weather-api.com/api/v1.1/current?&q=San+Diego\")\n        return response.json()[\"weather\"][0]\n\n    # [END howto_operator_weather_api]\n\n    # [START howto_operator_weather_api_hook]\n\n    @task\n    def get_weather_api_hook():\n        from airflow.providers.standard.hooks.weather_api import WeatherApiHook\n\n        hook = WeatherApiHook()\n        response = hook.get_resp(json_path=\"weather_api\")\n        return response[\"weather\"][0]\n\n    # [END howto_operator_weather_api_hook]\n\n    # [START howto_operator_snow_depth_api]\n\n    @task\n    def get_snow_depth_api():\n        import requests\n\n        response = requests.get(\"https://snow-depth-api.com/api/v1.1/snow-depth?&date=2020-08-15\")\n        return response.json()[\"snow_depth\"][0]\n\n    # [END howto_operator_snow_depth_api]\n\n    # [START howto_operator_snow_depth_api_hook]\n\n    @task\n    def get_snow_depth_api_hook():\n        from airflow.providers.standard.hooks.snow_depth_api import SnowDepthApiHook\n\n        hook = SnowDepthApiHook()\n        response = hook.get_resp(json_path=\"snow_depth_api\")\n        return response[\"snow_depth\"][0]\n\n    # [END howto_operator_snow_depth_api_hook]\n\n    t1 = get_weather_api()\n    t2 = get_weather_api_hook()\n    t3 = get_snow_depth_api()\n    t4 = get_snow_depth_api_hook()\n\n    BashOperator(task_id=\"echo_t1\", bash_command=f\"echo {t1}\")\n    BashOperator(task_id=\"echo_t2\", bash_command=f\"echo {t2}\")\n    BashOperator(task_id=\"echo_t3\", bash_command=f\"echo {t3}\")\n    BashOperator(task_id=\"echo_t4\", bash_command=f\"echo {t4}\")\n\n    Connection(\n        conn_id=\"snow_depth_api\",\n        conn_type=\"snow_depth_api\",\n        description=\"Connection for Snow Depth API\",\n        host=\"s3.us-east-1.amazonaws.com\",\n        login=\"*****\",\n        password=\"*****\",\n        schema=\"default\",\n    )\n\n    Connection(\n        conn_id=\"weather_api\",\n        conn_type=\"weather_api\",\n        description=\"Connection for Weather API\",\n        host=\"s3.us-east-1.amazonaws.com\",\n        login=\"*****\",\n        password=\"*****\",\n        schema=\"default\",\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an educational data processing pipeline that explores Airflow's task flexibility by implementing a sequence of tasks that print context, log SQL queries, perform time-based operations, and demonstrate isolated Python environment executions.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\n\n\n@task(task_id=\"print_it\")\ndef print_it():\n    print(\"Hello\")\n\n\n@task(task_id=\"log_it\")\ndef log_it():\n    print(\"Goodbye\")\n\n\n@task(task_id=\"python_operator_print\", python_callable=log_it)\ndef python_operator_print():\n    print_it()\n\n\nwith DAG(\n    \"example_airflow_decorator_dag\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    python_operator_print()"}], "metadata": {"file_name": "dag_1808_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Data Catalog tag template management workflow that demonstrates creating, retrieving, and modifying metadata templates for data governance. Build an Airflow DAG that showcases the lifecycle of a tag template with different operational steps.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogGetEntryGroupOperator,\n    CloudDataCatalogGetEntryOperator,\n    CloudDataCatalogGetTagTemplateOperator,\n    CloudDataCatalogListEntriesOperator,\n    CloudDataCatalogUpdateEntryOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"datacatalog_entries\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"entry-group-id-{ENV_ID}\"\nENTRY_GROUP_NAME = f\"entry-group-name-{ENV_ID}\"\nENTRY_ID = f\"entry-id-{ENV_ID}\"\nENTRY_DISPLAY_NAME = f\"entry-name-{ENV_ID}\"\nTAG_TEMPLATE_ID = \"tag-template-id\"\nTAG_TEMPLATE_DISPLAY_NAME = f\"tag-template-name-{ENV_ID}\"\nRESOURCE_ID = \"resource-id\"\nRESOURCE_NAME = f\"resource-name-{ENV_ID}\"\nGCS_BUCKET_URL = f\"gs://{BUCKET_NAME}\"\n\nFILE_NAME_1 = \"file_name_1\"\nFILE_NAME_2 = \"file_name_2\"\n\n# [START howto_operator_datacatalog_create_entry_group]\ncreate_entry_group_operator = CloudDataCatalogCreateEntryGroupOperator(\n    task_id=\"create_entry_group\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_group={\"name\": ENTRY_GROUP_NAME},\n)\n# [END howto_operator_datacatalog_create_entry_group]\n\n# [START howto_operator_datacatalog_create_entry]\ncreate_entry_operator = CloudDataCatalogCreateEntryOperator(\n    task_id=\"create_entry\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_id=ENTRY_ID,\n    entry={\n        \"name\": RESOURCE_ID,\n        \"type_\": \"FILESET\",\n        \"display_name\": ENTRY_DISPLAY_NAME,\n        \"metadata\": {\"sample_meta_data\": \" attributable when a place or thing is identified \" \"with its unique ID.\"},\n    },\n)\n# [END howto_operator_datacatalog_create_entry]\n\n# [START howto_operator_datacatalog_get_entry]\nget_entry_operator = CloudDataCatalogGetEntryOperator(\n    task_id=\"get_entry\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_id=ENTRY_ID,\n    read_options={\"return_body\": True},\n)\n# [END howto_operator_datacatalog_get_entry]\n\n# [START howto_operator_datacatalog_list_entries]\nlist_entries_operator = CloudDataCatalogListEntriesOperator(\n    task_id=\"list_entries\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    page_size=5,\n)\n# [END howto_operator_datacatalog_list_entries]\n\n# [START howto_operator_datacatalog_update_entry]\nupdate_entry_operator = CloudDataCatalogUpdateEntryOperator(\n    task_id=\"update_entry\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_id=ENTRY_ID,\n    update_mask=[{\"path\": \"metadata.sample_meta_data\"}],\n)\n# [END howto_operator_datacatalog_update_entry]\n\n# [START howto_operator_datacatalog_delete_entry]\ndelete_entry_operator = CloudDataCatalogDeleteEntryOperator(\n    task_id=\"delete_entry\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_id=ENTRY_ID,\n    trigger_rule=TriggerRule.ALL_DONE,\n)\n# [END howto_operator_datacatalog_delete_entry]\n\n# [START howto_operator_datacatalog_get_tag_template]\nget_tag_template_operator = CloudDataCatalogGetTagTemplateOperator(\n    task_id=\"get_tag_template\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    tag_template_id=TAG_TEMPLATE_ID,\n    read_options={\"return_body\": True},\n)\n# [END howto_operator_datacatalog_get_tag_template]\n\n# [START howto_operator_datacatalog_create_tag_template]\ncreate_tag_template_operator = CloudDataCatalogCreateTagTemplateOperator(\n    task_id=\"create_tag_template\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    tag_template_id=TAG_TEMPLATE_ID,\n    tag_template={\"name\": TAG_TEMPLATE_DISPLAY_NAME, \"metadatalist\": [{\"name\": RESOURCE_ID}]},\n)\n# [END howto_operator_datacatalog_create_tag_template]\n\n# [START howto_operator_datacatalog_delete_tag_template]\ndelete_tag_template_operator = CloudDataCatalogDeleteTagTemplateOperator(\n    task_id=\"delete_tag_template\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    tag_template_id=TAG_TEMPLATE_ID,\n)\n# [END howto_operator_datacatalog_delete_tag_template]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datacatalog\", \"entries\"],\n) as dag:\n    create_entry_group_operator >> create_entry_operator\n    get_entry_operator\n    list_entries_operator >> update_entry_operator\n    update_entry_operator >> delete_entry_operator\n    get_tag_template_operator >> create_tag_template_operator\n    create_tag_template_operator >> create_entry_operator\n    get_tag_template_operator\n    create_tag_template_operator >> create_entry_operator\n    get_tag_template_operator >> delete_tag_template_operator\n    delete_tag_template_operator >> create_entry_operator\n    get_entry_operator >> delete_entry_operator\n    create_entry_operator >> create_tag_template_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create_entry_operator\n    create_tag_template_operator >> create"}], "metadata": {"file_name": "dag_947_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I need to convert a large dataset of 1 million rows from a CSV file into a pandas DataFrame. How can I handle this process efficiently?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunQueryOperator\n\nAIRFLOW_HOME = os.getenv(\"AIRFLOW_HOME\", \"/usr/local/airflow\")\nQUERY_FILE_PATH = os.path.join(AIRFLOW_HOME, \"resources\", \"synapse\", \"query.sql\")\n\nwith DAG(\n    \"example_synapse_run_query\",\n    schedule=\"0 * * * *\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"synapse\"],\n) as dag:\n    # [START howto_operator_azure_synapse_run_query]\n    run_query = AzureSynapseRunQueryOperator(\n        task_id=\"query_samples_table\",\n        synapse_conn_id=\"synapse_default\",\n        body={\"query\": open(QUERY_FILE_PATH)},\n        synapse_run_config={\"wait_until_complete\": True},\n    )\n    # [END howto_operator_azure_synapse_run_query]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers CSV files from Google Cloud Storage to a Presto table, enabling seamless data movement between cloud storage and analytics platforms.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.presto.operators.presto import PrestoOperator\nfrom airflow.providers.s3.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_presto_s3_to_snowflake\"\n\nSNOWFLAKE_SAMPLE_TABLE = \"sample_table\"\nSNOWFLAKE_SAMPLE_SCHEMA = \"sample_schema\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_KEY = \"output.csv\"\n\nINPUT_DATA = \"s3://your-bucket-name/output.csv\"\nOUTPUT_TABLE = \"your-airflow-name\"\nSCHEMA = \"your-airflow-name,your-airflow-name\"\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=BUCKET_NAME,\n        s3_key=FILE_KEY,\n        data=\"hello world\",\n        replace=True,\n    )\n\n    create_table_presto = PrestoOperator(task_id=\"create_table_presto\", sql=f\"CREATE TABLE {SNOWFLAKE_SAMPLE_TABLE}\")\n\n    insert_data_presto = PrestoOperator(\n        task_id=\"insert_data_presto\",\n        sql=f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES (1, 'hello world'), (2, 'hello world')\",\n    )\n\n    create_view_presto = PrestoOperator(\n        task_id=\"create_view_presto\",\n        sql=f\"CREATE VIEW {SNOWFLAKE_SAMPLE_SCHEMA}.temp_view AS SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n    )\n\n    create_function_presto = PrestoOperator(\n        task_id=\"create_function_presto\",\n        sql=f\"CREATE FUNCTION get_{SNOWFLAKE_SAMPLE_SCHEMA}_name RETURN VARCHAR AS $$ SELECT 'hello world' FROM dual $$\",\n    )\n\n    create_table_snowflake = SnowflakeOperator(\n        task_id=\"create_table_snowflake\",\n        sql=f\"CREATE TABLE IF NOT EXISTS {SNOWFLAKE_SAMPLE_TABLE} COLUMNS (name VARCHAR, value VARCHAR)\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_column_mapping_snowflake = SnowflakeOperator(\n        task_id=\"create_column_mapping_snowflake\",\n        sql=f\"CREATE COLUMN_MAPPING IF NOT EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map \\\n            AS SELECT * FROM silver.s3_table_mappings\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_view_snowflake = SnowflakeOperator(\n        task_id=\"create_view_snowflake\",\n        sql=f\"CREATE VIEW {SNOWFLAKE_SAMPLE_SCHEMA}.view_name AS SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_function_snowflake = SnowflakeOperator(\n        task_id=\"create_function_snowflake\",\n        sql=f\"CREATE FUNCTION get_function_{SNOWFLAKE_SAMPLE_SCHEMA}_name \\\n            AS $$ SELECT 'hello world' FROM dual $$\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_table_from_external_source = SnowflakeOperator(\n        task_id=\"create_table_from_external_source\",\n        sql=f\"CREATE TABLE {OUTPUT_TABLE} COLUMNS (name VARCHAR, value VARCHAR)\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_column_mapping_from_external_source = SnowflakeOperator(\n        task_id=\"create_column_mapping_from_external_source\",\n        sql=f\"CREATE COLUMN_MAPPING {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map \\\n            AS SELECT * FROM {OUTPUT_TABLE}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_external_table = SnowflakeOperator(\n        task_id=\"create_external_table\",\n        sql=f\"CREATE EXTERNAL TABLE {SNOWFLAKE_SAMPLE_SCHEMA}.external_table \\\n            OPTIONS (type='csv', path='s3://your-bucket-name/output.csv', header='true')\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_column_mapping_external_table = SnowflakeOperator(\n        task_id=\"create_column_mapping_external_table\",\n        sql=f\"CREATE COLUMN_MAPPING {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map \\\n            AS SELECT * FROM {SNOWFLAKE_SAMPLE_EXTERNAL_TABLE}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    select_query_presto = PrestoOperator(task_id=\"select_query_presto\", sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\")\n\n    select_query_snowflake = SnowflakeOperator(task_id=\"select_query_snowflake\", sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_SCHEMA}.external_table\")\n\n    select_query_table = PrestoOperator(task_id=\"select_query_table\", sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_SCHEMA}.table_name\")\n\n    select_query_column_mapping = PrestoOperator(\n        task_id=\"select_query_column_mapping\",\n        sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_view_presto = SnowflakeOperator(task_id=\"drop_view_presto\", sql=f\"DROP VIEW {SNOWFLAKE_SAMPLE_SCHEMA}.temp_view\")\n\n    drop_view_snowflake = SnowflakeOperator(task_id=\"drop_view_snowflake\", sql=f\"DROP VIEW {SNOWFLAKE_SAMPLE_SCHEMA}.view_name\")\n\n    drop_table_presto = SnowflakeOperator(task_id=\"drop_table_presto\", sql=f\"DROP TABLE IF EXISTS {SNOWFLAKE_SAMPLE_TABLE}\")\n\n    drop_table_snowflake = SnowflakeOperator(task_id=\"drop_table_snowflake\", sql=f\"DROP TABLE IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.external_table\")\n\n    drop_table_column_mapping_presto = SnowflakeOperator(\n        task_id=\"drop_table_column_mapping_presto\",\n        sql=f\"DROP TABLE IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_table_column_mapping_snowflake = SnowflakeOperator(\n        task_id=\"drop_table_column_mapping_snowflake\",\n        sql=f\"DROP TABLE IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_view_column_mapping_presto = SnowflakeOperator(\n        task_id=\"drop_view_column_mapping_presto\",\n        sql=f\"DROP COLUMN_MAPPING IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_view_column_mapping_snowflake = SnowflakeOperator(\n        task_id=\"drop_view_column_mapping_snowflake\",\n        sql=f\"DROP COLUMN_MAPPING IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_create_bucket = S3CreateBucketOperator(task_id=\"drop_create_bucket\", bucket_name=BUCKET_NAME)\n\n    drop_create_object = S3CreateObjectOperator(\n        task_id=\"drop_create_object\", s3_bucket=BUCKET_NAME, s3_key=FILE_KEY, data=\"hello world\", replace=True\n    )\n\n    drop_table_from_external_source = SnowflakeOperator(\n        task_id=\"drop_table_from_external_source\",\n        sql=f\"DROP TABLE IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.external_table\",\n    )\n\n    drop_column_mapping_from_external_source = SnowflakeOperator(\n        task_id=\"drop_column_mapping_from_external_source\",\n        sql=f\"DROP COLUMN_MAPPING IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    )\n\n    drop_create_external_table = SnowflakeOperator(\n        task_id=\"drop_create_external_table\",\n        sql=f\"DROP EXTERNAL TABLE IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.external_table\",\n    )\n\n    drop_column_mapping_external_table = SnowflakeOperator(\n        task_id=\"drop_column_mapping_external_table\",\n        sql=f\"DROP COLUMN_MAPPING IF EXISTS {SNOWFLAKE_SAMPLE_SCHEMA}.schema_to_table_map\",\n    ]\n\n    (\n        create_bucket\n        >> create_object\n        >> create_table_presto\n        >> insert_data_presto\n        >> create_view_presto\n        >> create_function_presto\n        >> create_table_snowflake\n        >> create_column_mapping_snowflake\n        >> create_view_snowflake\n        >> create_function_snowflake\n        >> create_table_from_external_source\n        >> create_column_mapping_from_external_source\n        >> create_external_table\n        >> create_column_mapping_external_table\n        >> select_query_presto\n        >> select_query_snowflake\n        >> select_query_table\n        >> select_query_column_mapping\n        >> drop_view_presto\n        >> drop_view_snowflake\n        >> drop_table_presto\n        >> drop_table_snowflake\n        >> drop_table_column_mapping_presto\n        >> drop_table_column_mapping_snowflake\n        >> drop_view_column_mapping_presto\n        >> drop_view_column_mapping_snowflake\n        >> drop_create_bucket\n        >> drop_create_object\n        >> drop_table_from_external_source\n        >> drop_column_mapping_from_external_source\n        >> drop_create_external_table\n        >> drop_column_mapping_external_table\n    )"}], "metadata": {"file_name": "dag_301_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates cloud data processing tasks, including bucket creation, file uploads, template-based data processing, and automated cleanup of temporary resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"spark_example\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nBUCKET_NAME = f\"test-spark-{ENV_ID}\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}\"\nREGION = \"europe-west1\"\n\nOUTPUT_FOLDER = \"output\"\nOUTPUT_PERSISTENT_STORAGE_CONF = {\n    \"output_path\": f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}\",\n    \"persistent\": True,\n}\nTEMP_FOLDER = \"temp\"\nTEMP_PERSISTENT_STORAGE_CONF = {\"temp_path\": f\"gs://{BUCKET_NAME}/{TEMP_FOLDER}\"}\ninner_jar_file_path = (\n    \"p://public_bucket/spark-files/studies/BigTable/\"\n    \"google-cloud-spark-bigtable-proj-0.1-SNAPSHOT.jar\"\n)\nouter_jar_file_path = \"p://public_bucket/spark-files/studies/BigTable/bigtable-utils-0.1-SNAPSHOT.jar\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"size_gb\": 1024, \"disk_type\": \"PD_SSD\"},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"size_gb\": 1024, \"disk_type\": \"PD_SSD\"},\n    },\n    \"software_config\": {\n        \"image_version\": \"2.0.1\",\n        \"jars\": [inner_jar_file_path, outer_jar_file_path],\n        \"pyjaros\": [\"google-cloud-spark-utils-1.0.0-py37.jar\"],\n        \"num_preemptible_workers\": 1,\n        \"preemptibility\": \"PREEMPTIBLE\",\n    },\n}\n\n# Jobs definitions\n# [START howto_dataproc_trigger]\nTRIGGER_CONFIG = {\n    \"job_triggers\": {\n        \"inner_jar_file_trigger\": {\n            \"path\": inner_jar_file_path,\n        }\n        (\"outer_jar_file_trigger\"): {\n            \"path\": outer_jar_file_path,\n        }\n    }\n}\n\n# [END howto_dataproc_trigger]\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START howto_dataproc_submit_job_to_cluster]\n    run_pipeline = DataprocSubmitJobOperator(\n        task_id=\"run_pipeline\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        job={\n            \"reference\": {\n                \"cluster_name\": CLUSTER_NAME\n            },\n            \"placement\": {\n                \"managed\": {\n                    \"jar_file_uris\": [inner_jar_file_path, outer_jar_file_path],\n                    \"python_jar_file_uris\": [outer_jar_file_path],\n                    \"trigger_config\": TRIGGER_CONFIG,\n                    \"config\": OUTPUT_PERSISTENT_STORAGE_CONF,\n                }\n            },\n        },\n    )\n    # [END howto_dataproc_submit_job_to_cluster]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_cluster\n        # TEST BODY\n        >> run_pipeline\n        # TEST TEARDOWN\n        >> delete_cluster\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_178_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data backup workflow that transfers data from a DynamoDB table to an S3 bucket with point-in-time recovery capabilities. Create a system that demonstrates incremental data export and backup strategies for cloud-based database management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDYNAMODB_TABLE_NAME = \"TestTable\"\nSAMPLE_DATA = r\"\"\"{\"id\": \"1\", \"name\": \"Andrea Primi\", \"salary\": 120000}\n{\"id\": \"2\", \"name\": \"Aida Goga\", \"salary\": 110000}\n\"\"\"\n\n\n@task\ndef get_aws_db_credentials():\n    return {\n        \"AWS_ACCESS_KEY_ID\": \"AWS_ACCESS_KEY_ID\",\n        \"AWS_SECRET_ACCESS_KEY\": \"AWS_SECRET_ACCESS_KEY\",\n    }\n\n\n@task\ndef create_oath_table(table_name: str):\n    table = {\n        \"tableName\": table_name,\n        \"attributeDefinitions\": [\n            {\"name\": \"id\", \"type\": \"S\"},\n        ],\n        \"keySchema\": [\n            {\"name\": \"id\", \"type\": \"HASH\"},\n            {\"name\": \"id\", \"type\": \"RANGE\"},\n        ],\n        \"provisionedThroughput\": {\n            \"readCapacityUnits\": 10,\n            \"writeCapacityUnits\": 10,\n        },\n    }\n    print(table)\n    client = boto3.client(\"dynamodb\")\n    client.create_table(**table)\n\n\n@task\ndef load_sample_data(table_name: str):\n    table = {\n        \"tableName\": table_name,\n        \"item\": {\n            \"id\": {\"S\": \"1\"},\n            \"name\": {\"S\": \"Andrea Primi\"},\n            \"salary\": {\"S\": \"120000\"},\n        },\n    }\n    client = boto3.client(\"dynamodb\")\n    response = client.put_item(TableName=table_name, Item=table)\n\n\n@task\ndef get_dynamodb_table_points_in_time(table_name: str, start_time: str, end_time: str):\n    client = boto3.client(\"dynamodb\")\n    response = client.get_point_in_time_recovery_status(\n        TableName=table_name,\n        StartTime=start_time,\n        EndTime=end_time,\n    )\n    print(json.dumps(response, indent=4))\n\n\nwith DAG(\n    dag_id=\"dynamodb_to_s3\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = get_aws_db_credentials()\n    bucket_name = f\"{test_context['AWS_ACCOUNT_ID']}-{test_context['AWS_REGION']}\"\n    table_name = f\"{test_context['AWS_ACCOUNT_ID']}-{test_context['AWS_REGION']}\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    create_dynamodb_table = create_oath_table(table_name)\n\n    insert_sample_data = S3CreateObjectOperator(\n        task_id=\"insert_sample_data\",\n        s3_bucket=bucket_name,\n        s3_key=\"dynamodb/table.json\",\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    get_dynamodb_table_points_in_time(table_name, \"\", \"\")\n\n    # [START howto_transfer_dynamodb_to_s3]\n    task_transfer_dynamodb_to_s3 = DynamoDBToS3Operator(\n        task_id=\"transfer_dynamodb_to_s3\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        s3_key=\"dynamodb/table.json\",\n        start_time=\"1970-01-01T00:00:00\",\n        end_time=\"2030-12-31T23:59:59\",\n        account_id=test_context[\"AWS_ACCOUNT_ID\"],\n        region=test_context[\"AWS_REGION\"],\n    )\n    # [END howto_transfer_dynamodb_to_s3]\n\n    delete_dynamodb_table = DynamoDBToS3Operator(\n        task_id=\"delete_dynamodb_table\",\n        dynamodb_table_name=table_name,\n    )\n\n    delete_s3_bucket = S3CreateBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        test_context,\n        create_s3_bucket,\n        create_dynamodb_table,\n        insert_sample_data,\n        get_dynamodb_table_points_in_time(table_name, \"\", \"\"),\n        task_transfer_dynamodb_to_s3,\n        delete_dynamodb_table,\n        delete_s3_bucket,\n    )"}], "metadata": {"file_name": "dag_1473_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a cloud-based data extraction and loading pipeline that captures data from a NoSQL database, exports it to a neutral storage format, and prepares it for analytical querying. Create a workflow that transforms unstructured data into a structured, analyzable format across different Google Cloud platforms.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.compute_flex import ComputeEngineFlexHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator, GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_gcs import MssqlToGCSOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_sheets import MssqlToSheetsOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_gce_to_sheets\"\n\nREGION = \"europe-west1\"\nZONE = REGION + \"-a\"\nNETWORK = \"default-legacy\"\n\nMSSQL_CONN_ID = \"mssql_default\"\nGCS_BUCKET = f\"gs://{DAG_ID}_{ENV_ID}/spreadsheet.csv\"\n\nDB_NAME = \"testdb\"\nSHORT_DB_NAME = DB_NAME[:33]\nINSTANCE_NAME = f\"instance-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nLOCATION = \"us-central1\"\n\nSQL_TABLE_NAME = \"test_table\"\nCREATE_TABLE_SQL = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SHORT_DB_NAME}.{SQL_TABLE_NAME} (\n        id INT PRIMARY KEY,\n        name VARCHAR(255),\n        i_d VARCHAR(255)\n    );\n\"\"\"\n\nINSERT_SQL = f\"INSERT INTO {SHORT_DB_NAME}.{SQL_TABLE_NAME} (id, name, i_d) VALUES (?,?,?)\"\n\nSELECT_SQL = f\"SELECT * FROM {SHORT_DB_NAME}.{SQL_TABLE_NAME}\"\n\nBUCKET_NAME = f\"bucket_{ENV_ID}_{DAG_ID}\"\nGCS_FILE_NAME = \"spreadsheet.csv\"\nCREATE_BUCKET = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\nDROP_BUCKET = GCSDeleteBucketOperator(\n    task_id=\"delete_bucket\",\n    bucket_name=BUCKET_NAME,\n    trigger_rule=TriggerRule.ALL_DONE,\n)\n\n\ndef _create_connection(conn_id_name: str) -> str:\n    return f\"conn_{conn_id_name}\"\n\n\ndef _set_up_db():\n    print(\"Setting up MSSQL connection.\")\n    connection = Connection(\n        conn_id=MSSQL_CONN_ID,\n        conn_type=\"mssql\",\n        login=\"sa\",\n        password=\"samplepassword\",\n        schema=SHORT_DB_NAME,\n    )\n    print(f\"Creating connection {MSSQL_CONN_ID}.\")\n    hook = ComputeEngineFlexHook()\n    request = {\n        \"name\": f\"projects/{PROJECT_ID}/zones/{ZONE}/instanceTemplates/{INSTANCE_NAME}Template\",\n        \"deviceConfig\": {\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"deviceName\": INSTANCE_NAME,\n                    \"initializeParams\": {\n                        \"diskSizeGb\": 100,\n                        \"diskType\": hook.compute_type_service().get_value_by_name(\"PD_SSD\"),\n                        \"sourceImage\": \"projects/debian-cloud/global/images/debian-11-bullseye-v202206\",\n                    },\n                    \"accessConfigs\": [\n                        {\n                            \"name\": \"Zero Trust Network Protection\",\n                            \"networkPattern\": {\"ipRanges\": [\"0.0.0.0/0\"]},\n                        }\n                    ],\n                }\n            ],\n            \"metadataConfig\": {\"httpPort\": {\"networkPattern\": {\"ipRanges\": [\"0.0.0.0\"]}}},\n        },\n        \"networkConfig\": {\"network\": hook.compute_type_service().get_value_by_name(\"compute-java\")},\n        \"serviceAccountScopes\": [\n            \"https://www.googleapis.com/auth/cloud-platform\",\n            \"https://sqlserverlessapp.svc.idera.com/iam\",\n        ],\n    }\n    instance_template = hook.compute_service().instanceTemplates().insert(request_body=request).execute()\n    connection.set(conn_id=MSSQL_CONN_ID, role=Role.USER)\n    hook.compute_service().instances().insert(\n        project=id,\n        zone=ZONE,\n        body=request={\"name\": INSTANCE_NAME, \"deviceConfig\": request[\"deviceConfig\"]}\n        | {\"serviceAccountScopes\": [f\"https://{instance_template.name}.svc.idera.com/iam\"]},\n    )\n    print(f\"Service account scoped authentication for {INSTANCE_NAME} completed.\")\n    return connection\n\n\ndef _remove_connection(conn_id_name: str) -> None:\n    print(f\"Removing connection {MSSQL_CONN_ID}.\")\n    hook.compute_service().connections().delete(project=id, zone=ZONE, connId=MSSQL_CONN_ID)\n    print(f\"Connection {MSSQL_CONN_ID} removed.\")\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gce\"],\n) as dag:\n    @task\n    def create_instance_template():\n        print(\"Creating instance template.\")\n        request = {\n            \"name\": f\"projects/{PROJECT_ID}/zones/{ZONE}/instanceTemplates/{INSTANCE_NAME}Template\",\n            \"machineType\": hook.compute_type_service().get_value_by_name(\"n1-standard-1\"),\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"deviceName\": INSTANCE_NAME,\n                    \"initializeParams\": {\n                        \"diskSizeGb\": 100,\n                        \"diskType\": hook.compute_type_service().get_value_by_name(\"PD_SSD\"),\n                        \"sourceImage\": \"projects/debian-cloud/global/images/debian-11-bullseye-v202206\",\n                    },\n                    \"accessConfigs\": [\n                        {\n                            \"name\": \"Zero Trust Network Protection\",\n                            \"networkPattern\": {\"ipRanges\": [\"0.0.0.0/0\"]},\n                        }\n                    ],\n                }\n            ],\n            \"networkConfig\": {\n                \"network\": hook.compute_type_service().get_value_by_name(\"compute-java\"),\n            },\n            \"serviceAccountScopes\": [\n                \"https://www.googleapis.com/auth/cloud-platform\",\n                \"https://sqlserverlessapp.svc.idera.com/iam\",\n            ],\n        }\n        instance_template = hook.compute_service().instanceTemplates().insert(request_body=request).execute()\n        print(f\"Instance template created: {instance_template.name}\")\n        return instance_template\n\n    @task\n    def insert_instance(request_body: dict):\n        print(f\"Inserting instance {request_body['name']} created.\")\n        instance = hook.compute_service().instances().insert(project=id, zone=ZONE, body=request_body).execute()\n        print(f\"Instance {instance.name} created.\")\n\n    @task\n    def create_sheets_connection():\n        print(\"Creating Sheets connection.\")\n        request_body = {\n            \"name\": f\"projects/{PROJECT_ID}/sheets/connections/{SHORT_DB_NAME}\",\n            \"type\": \"mssql\",\n            \"user\": {\"serviceAccount\": {\"key\": \"conn_mssql_default\"}},\n        }\n        connection = hook.sheets_service().connections().create(body=request_body).execute()\n        print(f\"Connection created: {connection.name}\")\n        return connection\n\n    @task\n    def delete_sheets_connection(connection_id: str):\n        print(f\"Removing Sheets connection {connection_id}.\")\n        hook.sheets_service().connections().delete(project=id, zone=\"global\", connectionId=connection_id)\n        print(f\"Connection {connection_id} removed.\")\n\n    @task\n    def get_mssql_instance_name(request_body: dict):\n        print(f\"Mssql instance name: {request_body['name'].split('/')[-1]}\")\n\n    @task\n    def get_instance_zone(request_body: dict):\n        print(f\"Instance zone: {request_body['name'].split('/')[-2]}\")\n\n    @task\n    def get_network(request_body: dict):\n        print(f\"Network: {request_body['name'].split('/')[-1]}\")\n\n    @task\n    def get_created_template(request_body: dict):\n        print(f\"Instance template name: {request_body['name']}\")\n        print(f\"Instance template zone: {request_body['name'].split('/')[-2]}\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_instance_template(instance_template_id: str):\n        print(f\"Deleting instance template {instance_template_id}.\")\n        hook.compute_service().instanceTemplates().delete(\n            project=id, zone=ZONE, name=instance_template_id\n        )\n        print(f\"Instance template {instance_template_id} deleted.\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_mssql_connection(connection_id: str):\n        print(f\"Deleting MSSQL connection {connection_id}.\")\n        hook.sheets_service().connections().delete(project=id, zone=\"global\", connectionId=connection_id)\n        print(f\"Connection {connection_id} deleted.\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def create_sheets_connection_2():\n        print(\"Recreating Sheets connection.\")\n        request_body = {\n            \"name\": f\"projects/{PROJECT_ID}/sheets/connections/{SHORT_DB_NAME}\",\n            \"type\": \"mssql\",\n            \"user\": {\"serviceAccount\": {\"key\": \"conn_mssql_default\"}},\n        }\n        connection = hook.sheets_service().connections().create(body=request_body).execute()\n        print(f\"Connection created: {connection.name"}], "metadata": {"file_name": "dag_1250_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Google Cloud Data Catalog workflow that demonstrates creating and managing metadata for a Google Cloud Storage bucket. Design a DAG that showcases how to programmatically register, tag, and catalog cloud storage resources for improved data governance and discoverability.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateTagOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteTagOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogSearchEntriesOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_search\"\n\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"example-entry-group-{ENV_ID}\"\nENTRY_GROUP_NAME = f\"example-entry-group-{ENV_ID}\"\nENTRY_ID = \"example-entry\"\nENTRY_DISPLAY_NAME = f\"example-entry\"\nTEMPLATE_ID = f\"template-id-{ENV_ID}\"\nDESCRIPTION = f\"example-description\"\nFORMAT_ID = \"image_list\"\nLANDING_PAGE_ID = \"example-landing-page\"\nLANDING_PAGE_DISPLAY_NAME = f\"example-landing-page\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datacatalog\", \"search\"],\n) as dag:\n    # [START howto_operator_gcp_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"display_name\": ENTRY_DISPLAY_NAME},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_create_entry]\n    create_entry = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        entry={\"display_name\": ENTRY_DISPLAY_NAME, \"format\": FORMAT_ID},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry]\n\n    # [START howto_operator_gcp_datacatalog_create_tag]\n    create_tag = CloudDataCatalogCreateTagOperator(\n        task_id=\"create_tag\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        tag_template_id=TEMPLATE_ID,\n        tag={\"display_name\": TEMPLATE_ID, \"description\": DESCRIPTION},\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template]\n    create_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create_tag_template\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        tag_template_id=TEMPLATE_ID,\n        tag_template={\"display_name\": TEMPLATE_ID, \"description\": DESCRIPTION},\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_search_entries]\n    search_entries = CloudDataCatalogSearchEntriesOperator(\n        task_id=\"search_entries\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        read_mask=[\"display_name\"],\n    )\n    # [END howto_operator_gcp_datacatalog_search_entries]\n\n    # [START howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry]\n\n    # [START howto_operator_gcp_datacatalog_delete_tag]\n    delete_tag = CloudDataCatalogDeleteTagOperator(\n        task_id=\"delete_tag\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        tag_template_id=TEMPLATE_ID,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag]\n\n    # [START howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n        task_id=\"delete_tag_template\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        tag_template_id=TEMPLATE_ID,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_read_tag_template]\n    read_tag_template = CloudDataCatalogReadTagTemplateOperator(\n        task_id=\"read_tag_template\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        tag_template_id=TEMPLATE_ID,\n    )\n    # [END howto_operator_gcp_datacatalog_read_tag_template]\n\n    (\n        # TEST SETUP\n        create_entry_group\n        >> create_entry\n        >> create_tag\n        >> create_tag_template\n        # TEST BODY\n        >> search_entries\n        # TEST TEARDOWN\n        >> delete_entry_group\n        >> delete_entry\n        >> delete_tag\n        >> delete_tag_template\n        >> read_tag_template\n    )\n\n    # [END import_gate]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)  # type: ignore[no-redef]"}], "metadata": {"file_name": "dag_271_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data pipeline that demonstrates task chaining, system command execution, and containerized task running, highlighting Airflow's ability to coordinate different types of computational tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START howto_operator_singularity]\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef singularity_install():\n    import subprocess\n\n    install_cmd = \"pkg install -y docker singularity\"\n    print(f\"Installing Singularity and Docker with command: {install_cmd}\")\n    install_proc = subprocess.Popen(install_cmd, shell=True)\n    install_proc.wait()\n    print(\"Installed Singularity and Docker\")\n\n\n@task\ndef hello_world():\n    print(\"Hello, world!\")\n\n\nwith DAG(\n    dag_id=\"example Singularity\",\n    start_date=pendulum.datetime(2020, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    singularity_install()\n    hello_world()\n# [END howto_operator_singularity]"}], "metadata": {"file_name": "dag_1563_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates a complete machine learning workflow with Google Cloud services, showcasing task dependencies, cloud storage interactions, and automated model training processes. Construct a step-by-step orchestration that moves data, creates datasets, and trains a translation model with clear operational stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.nlp import CloudNaturalLanguageTextDetectionOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_natural_language\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_text_detection_sentence.txt\"\nNLP_INPUT = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nGCS_OUTPUT = f\"gs://{BUCKET_NAME}/outputs/\"\nSTAGING_DIR = \"gs://airflow-system-tests/staging\"\n\n# [START howto_operator_gcp_natural_language_text_detection_gcs_to_local]\n\nlocal_path = LOCAL_PATH = \"example_text_detection_output.json\"\n\n\ndef get_output_path():\n    return LOCAL_PATH\n\n\ndef get_bucket():\n    return BUCKET_NAME\n\n\ndef get_file():\n    return FILE_NAME\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_natural_language_detection_text]\n    text_detection = CloudNaturalLanguageTextDetectionOperator(\n        task_id=\"text_detection\",\n        gcs_input=NLP_INPUT,\n    )\n    # [END howto_operator_natural_language_detection_text]\n\n    # [START howto_operator_natural_language_detection_text_result]\n    text_detection_result = CloudNaturalLanguageTextDetectionOperator(\n        task_id=\"text_detection_result\",\n        gcs_input=NLP_INPUT,\n        output_path=get_output_path(),\n        bucket=get_bucket(),\n        file=get_file(),\n    )\n    # [END howto_operator_natural_language_detection_text_result]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> text_detection\n        >> text_detection_result\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_461_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that showcases the end-to-end process of creating, managing, and analyzing image resources in a cloud environment. Develop a systematic workflow that demonstrates Google Cloud Vision's product and image management capabilities.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductSetOperationOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateImageSetOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteImageSetOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionListImagesOperator,\n    CloudVisionRemoveProductSetOperationOperator,\n)\nfrom airflow.providers.google.cloud.sensors.vision import CloudVisionImageSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"cloud_vision_image\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nIMAGE_NAME = \"image1\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nFILE_PATH = f\"gcs/{IMAGE_NAME}.jpg\"\n\nVISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/{IMAGE_NAME}.jpg\"\n\n# [START howto_operator_vision_create_image_set]\ndef create_image_set():\n    from airflow.providers.google.cloud.operators.vision import CloudVisionCreateImageSetOperator\n\n    product_set_id = \"example_product_set\"\n    image_set_id = \"example_image_set\"\n    location = \"us-central1\"\n\n    CloudVisionCreateImageSetOperator(\n        task_id=\"create_image_set\",\n        project_id=PROJECT_ID,\n        location=location,\n        body=request_body=dict(\n            product_set_id=product_set_id,\n            image_set_id=image_set_id,\n            images=[VISION_IMAGE_URL],\n        ),\n    )\n\n\n# [END howto_operator_vision_create_image_set]\n\n# [START howto_operator_vision_list_image_set]\ndef list_image_set():\n    from airflow.providers.google.cloud.operators.vision import CloudVisionListImagesOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    image_sets = CloudVisionListImagesOperator(\n        task_id=\"list_image_set\",\n        location=location,\n        product_set_id=product_set_id,\n    )[0]\n\n\n# [END howto_operator_vision_list_image_set]\n\n# [START howto_operator_vision_get_image_set]\ndef get_image_set(image_set_id):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionGetImageSetOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    image_set_id = \"example_image_set\"\n    CloudVisionGetImageSetOperator(\n        task_id=\"get_image_set\",\n        location=location,\n        product_set_id=product_set_id,\n        image_set_id=image_set_id,\n        body=request_body={\"image_set_id\": image_set_id},\n    )[0]\n\n\n# [END howto_operator_vision_get_image_set]\n\n# [START howto_operator_vision_remove_image_set]\ndef remove_image_set(image_set_id):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionRemoveProductSetOperationOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    operation_id = \"example_operation_id\"\n    CloudVisionRemoveProductSetOperationOperator(\n        task_id=\"remove_image_set\",\n        location=location,\n        product_set_id=product_set_id,\n        image_set_id=image_set_id,\n        body=request_body={\"operation_id\": operation_id},\n    )\n\n\n# [END howto_operator_vision_remove_image_set]\n\n# [START howto_operator_vision_create_product_set]\ndef create_product_set():\n    from airflow.providers.google.cloud.operators.vision import CloudVisionCreateProductSetOperator\n\n    product_set_id = \"example_product_set\"\n    location = \"us-central1\"\n\n    CloudVisionCreateProductSetOperator(\n        task_id=\"create_product_set\",\n        project_id=PROJECT_ID,\n        location=location,\n        body=request_body=dict(product_set_id=product_set_id),\n    )\n\n\n# [END howto_operator_vision_create_product_set]\n\n# [START howto_operator_vision_list_product_set]\ndef list_product_set():\n    from airflow.providers.google.cloud.operators.vision import CloudVisionListProductSetsOperator\n\n    location = \"us-central1\"\n    product_sets = CloudVisionListProductSetsOperator(task_id=\"list_product_set\", location=location)[0]\n\n\n# [END howto_operator_vision_list_product_set]\n\n# [START howto_operator_vision_get_product_set]\ndef get_product_set(product_set_id):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionGetProductSetOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    CloudVisionGetProductSetOperator(\n        task_id=\"get_product_set\",\n        location=location,\n        product_set_id=product_set_id,\n        body=request_body={\"product_set_id\": product_set_id},\n    )[0]\n\n\n# [END howto_operator_vision_get_product_set]\n\n# [START howto_operator_vision_add_product_set]\ndef add_product_set(product_set_id, image_set_id):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionAddProductSetOperationOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    operation_id = \"example_operation_id\"\n    CloudVisionAddProductSetOperationOperator(\n        task_id=\"add_product_set\",\n        location=location,\n        product_set_id=product_set_id,\n        image_set_id=image_set_id,\n        body=request_body={\"operation_id\": operation_id},\n    )\n\n\n# [END howto_operator_vision_add_product_set]\n\n# [START howto_operator_vision_delete_product_set]\ndef delete_product_set(product_set_id):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionDeleteProductSetOperator\n\n    location = \"us-central1\"\n    product_set_id = \"example_product_set\"\n    CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set\",\n        location=location,\n        product_set_id=product_set_id,\n    )\n\n\n# [END howto_operator_vision_delete_product_set]\n\n# [START howto_operator_vision_delete_bucket]\ndef delete_bucket():\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n    from airflow.providers.google.cloud.operators.gcs import GCSSensor\n    from airflow.providers.google.cloud.operators.gcs import GC"}], "metadata": {"file_name": "dag_267_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates end-to-end image processing and machine learning operations using Google Cloud Vision APIs. Develop a pipeline that transfers images, creates product catalogs, and performs automated image analysis.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductSetOperationOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionCreateImageAnnotateOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionDeleteImageAnnotateOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionImageAnnotateOperator,\n    CloudVisionImageAnnotateRequest,\n    CloudVisionProductSetAnnotationParams,\n    CloudVisionProductSetImageAnnotateRequest,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"cloud_vision_automl\"\n\nBUCKET_NAME = f\"bucket_vision_{DAG_ID}_{ENV_ID}\"\nIMAGE_1_URI = f\"gs://{BUCKET_NAME}/image_1.jpg\"\nIMAGE_2_URI = f\"gs://{BUCKET_NAME}/image_2.jpg\"\nIMAGE_1_OUTPUT_URI = f\"gs://{BUCKET_NAME}/image_1_output.jpg\"\nIMAGE_2_OUTPUT_URI = f\"gs://{BUCKET_NAME}/image_2_output.jpg\"\n\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nPRODUCT_SET_ID = f\"product_set_{DAG_ID}_{ENV_ID}\"\nREFERENCE_IMAGE_ID = f\"reference_image_{DAG_ID}_{ENV_ID}\"\nIMAGE_ANNOTATE_REQUEST = CloudVisionImageAnnotateRequest(image=CloudVisionImageAnnotateRequest.Image())\nIMAGE_ANNOTATE_REQUEST.image_annotate_requests.append(IMAGE_ANNOTATE_REQUEST)\nIMAGE_ANNOTATE_REQUEST2 = CloudVisionImageAnnotateRequest(image=CloudVisionImageAnnotateRequest.Image())\nIMAGE_ANNOTATE_REQUEST2.image_annotate_requests.append(IMAGE_ANNOTATE_REQUEST)\n\nEXAMPLE_IMAGE_1_PATH = \"gs://vision-system-tests-resources/v1/images/image_1.jpg\"\nEXAMPLE_IMAGE_2_PATH = \"gs://vision-system-tests-resources/v1/images/image_2.jpg\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\", \"automl\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    upload_file_1 = GCSToGCSOperator(\n        task_id=\"upload_file_1\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=EXAMPLE_IMAGE_1_PATH,\n        destination_bucket=BUCKET_NAME,\n    )\n    upload_file_2 = GCSToGCSOperator(\n        task_id=\"upload_file_2\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=EXAMPLE_IMAGE_2_PATH,\n        destination_bucket=BUCKET_NAME,\n    )\n\n    # [START howto_cloud_vision_annotate_image_operator]\n    annotate_image = CloudVisionImageAnnotateOperator(\n        image=IMAGE_ANNOTATE_REQUEST,\n        request_type=CloudVisionImageAnnotateRequest,\n        task_id=\"annotate_image\",\n    )\n    # [END howto_cloud_vision_annotate_image_operator]\n\n    # [START howto_cloud_vision_image_annotate_operator]\n    image_annotate = CloudVisionImageAnnotateOperator(\n        image=IMAGE_ANNOTATE_REQUEST2,\n        request_type=CloudVisionProductSetImageAnnotateRequest,\n        task_id=\"image_annotate\",\n    )\n    # [END howto_cloud_vision_image_annotate_operator]\n\n    # [START howto_cloud_vision_create_product_set_operator]\n    create_product_set = CloudVisionCreateProductSetOperator(\n        product_set_id=PRODUCT_SET_ID,\n        task_id=\"create_product_set\",\n        project_id=PROJECT_ID,\n    )\n    # [END howto_cloud_vision_create_product_set_operator]\n\n    # [START howto_cloud_vision_get_product_set_operator]\n    get_product_set = CloudVisionGetProductSetOperator(\n        product_set_id=PRODUCT_SET_ID,\n        project_id=PROJECT_ID,\n        task_id=\"get_product_set\",\n    )\n    # [END howto_cloud_vision_get_product_set_operator]\n\n    # [START howto_cloud_vision_create_reference_image_operator]\n    create_reference_image = CloudVisionCreateReferenceImageOperator(\n        reference_image_id=REFERENCE_IMAGE_ID, image_uri=IMAGE_1_URI, task_id=\"create_reference_image_1\"\n    )\n    # [END howto_cloud_vision_create_reference_image_operator]\n\n    # [START howto_cloud_vision_image_annotate_operator]\n    image_annotate_1 = CloudVisionImageAnnotateOperator(\n        image=IMAGE_ANNOTATE_REQUEST,\n        request_type=CloudVisionImageAnnotateRequest,\n        task_id=\"image_annotate_1\",\n        image_uri=IMAGE_1_URI,\n    )\n    # [END howto_cloud_vision_image_annotate_operator]\n\n    # [START howto_cloud_vision_image_annotate_operation]\n    image_annotate_operation = CloudVisionImageAnnotateOperator(\n        image=IMAGE_ANNOTATE_REQUEST2,\n        request_type=CloudVisionImageAnnotateRequest,\n        task_id=\"image_annotate_operation\",\n        image_uri=IMAGE_2_URI,\n    )\n    # [END howto_cloud_vision_image_annotate_operation]\n\n    # [START howto_cloud_vision_create_image_annotate_operator]\n    image_annotate_uri = CloudVisionCreateImageAnnotateOperator(\n        request=IMAGE_ANNOTATE_REQUEST,\n        image_uri=IMAGE_1_URI,\n        task_id=\"image_annotate_uri\",\n    )\n    # [END howto_cloud_vision_create_image_annotate_operator]\n\n    # [START howto_cloud_vision_add_product_set_operation]\n    add_product_set_operation = CloudVisionAddProductSetOperationOperator(\n        product_set_id=PRODUCT_SET_ID, request=IMAGE_ANNOTATE_REQUEST, task_id=\"add_product_set_operation\"\n    )\n    # [END howto_cloud_vision_add_product_set_operation]\n\n    # [START howto_cloud_vision_delete_image_annotate_operator]\n    delete_image_annotate_uri = CloudVisionDeleteImageAnnotateOperator(\n        request=IMAGE_ANNOTATE_REQUEST2, task_id=\"delete_image_annotate_uri\"\n    )\n    # [END howto_cloud_vision_delete_image_annotate_operator]\n\n    delete_product_set_operation = CloudVisionDeleteProductSetOperationOperator(\n        product_set_id=PRODUCT_SET_ID, task_id=\"delete_product_set_operation\", project_id=PROJECT_ID\n    )\n\n    delete_reference_image = CloudVisionDeleteReferenceImageOperator(\n        reference_image_id=REFERENCE_IMAGE_ID, task_id=\"delete_reference_image\", project_id=PROJECT_ID\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        upload_file_1,\n        upload_file_2,\n        create_product_set,\n        get_product_set,\n        create_reference_image,\n        image_annotate_1,\n        image_annotate,\n        image_annotate_operation,\n        add_product_set_operation,\n        delete_image_annotate_uri,\n        delete_product_set_operation,\n        delete_reference_image,\n        delete_bucket,\n    )\n\n    from tests.system.utils import get_test_run\n\n    # This test needs the following:\n    # - That the environment variable \"SYSTEM_TESTS_GCP_PROJECT\" has a value\n    #   Other tests in the system test suite will supply the rest of the\n    #   environment variables.\n    test_run = get_test_run(dag)\n    test_run.minimise_post_run_down()"}], "metadata": {"file_name": "dag_596_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates advanced cloud service integration by copying an image file between buckets and performing multiple types of computer vision analysis using different Google Cloud Vision operators. Showcase how complex data processing workflows can be orchestrated with modular, configurable task chains.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductSetOperationOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionCreateImageOperationOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionDeleteImageOperationOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionUpdateProductSetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_vision\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\n\nGCP_VISION_PRODUCT_SET_ID = f\"product_set_{DAG_ID}_{ENV_ID}\"\nGCP_VISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nGCP_VISION_REFERENCE_IMAGE_ID = f\"reference_image_{DAG_ID}_{ENV_ID}\"\nGCP_VISION_IMAGE_URL_2 = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# [START howto_operator_vision_image_detect]\ndef image_detect_func(image_uri: str):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionDetectImageOperationAmount\n\n    result = (\n        f\" {image_uri}\",\n        CloudVisionDetectImageOperationAmount(max_results=10),\n    )\n    (image_detect_id_,) = result\n    return image_detect_id_\n# [END howto_operator_vision_image_detect]\n\n# [START howto_operator_vision_image_import]\ndef image_import_func(bucket_name: str, source_image_uri: str):\n    from airflow.providers.google.cloud.operators.vision import CloudVisionImportImageOperation\n\n    (image_import_id_,) = CloudVisionImportImageOperation.image_import(\n        bucket_name, source_image_uri, GCP_VISION_REFERENCE_IMAGE_ID\n    )\n    return image_import_id_\n# [END howto_operator_vision_image_import]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = GCSToGCSOperator(\n        task_id=\"upload_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_path=FILE_NAME,\n    )\n\n    image_detect_id_ = image_detect_func(GCP_VISION_IMAGE_URL)\n    image_import_id_ = image_import_func(BUCKET_NAME, GCP_VISION_IMAGE_URL)\n\n    # [START howto_operator_vision_image_import_result]\n    image_import_result = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result\", operation_id=image_import_id_\n    )\n    (image_detected_id_,) = image_import_result\n    # [END howto_operator_vision_image_import_result]\n\n    # [START howto_operator_vision_detect_image]\n    image_detect = CloudVisionDetectImageOperationAmount.task_id\n    # The default is to use the most precise detector, which is ImageDetectHumanDetectorsModel\n    # [START howto_operator_vision_detect_image_full]\n    image_detect_full = CloudVisionDetectImageOperator(\n        task_id=\"image_detect_full\", image_uri=GCP_VISION_IMAGE_URL_2, model=\"image_detect_full_model\"\n    )\n    # [END howto_operator_vision_detect_image_full]\n    # [START howto_operator_vision_get_product_set]\n    product_set_get = CloudVisionGetProductSet.task_id\n    (product_set_id_,) = product_set_get(id=GCP_VISION_PRODUCT_SET_ID)\n    # [END howto_operator_vision_get_product_set]\n    # [START howto_operator_vision_list_product_sets]\n    product_sets = CloudVisionListProductSets.task_id\n    (product_set_id_,) = product_sets()\n    # [END howto_operator_vision_list_product_sets]\n    # [START howto_operator_vision_update_product_set]\n    product_set_update = CloudVisionUpdateProductSet.task_id\n    (product_set_id_,) = product_set_update(id=GCP_VISION_PRODUCT_SET_ID, features=[{\"image_url\": GCP_VISION_REFERENCE_IMAGE_ID}])\n    # [END howto_operator_vision_update_product_set]\n    # [START howto_operator_vision_image_import_result]\n    image_import_result = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result\", operation_id=image_import_id_\n    )\n    (image_detected_id_,) = image_import_result\n    # [END howto_operator_vision_image_import_result]\n\n    # [START howto_operator_vision_image_import_result_break>\n    image_import_result = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_break\", operation_id=image_import_id_\n    )\n    (image_detected_id_,) = image_import_result\n    # [END howto_operator_vision_image_import_result_break>\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_product_set = CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set\",\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        project_id=PROJECT_ID,\n    )\n\n    delete_reference_image = CloudVisionDeleteReferenceImageOperator(\n        task_id=\"delete_reference_image\",\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        project_id=PROJECT_ID,\n    )\n\n    delete_image = CloudVisionDeleteImageOperationOperator(\n        task_id=\"delete_image\",\n        operation_id=image_detect_id_,\n        project_id=PROJECT_ID,\n    )\n\n    delete_image_2 = CloudVisionDeleteImageOperationOperator(\n        task_id=\"delete_image_2\",\n        operation_id=image_import_id_,\n        project_id=PROJECT_ID,\n    )\n\n    delete_product_set_2 = CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set_2\",\n        product_set_id=product_set_id_,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_image_import]\n    image_import_ = CloudVisionImportImageOperator(\n        task_id=\"image_import_\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n    )\n    # [END howto_operator_vision_image_import]\n\n    image_import_break = CloudVisionImportImageOperator(\n        task_id=\"image_import_break\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL_2,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        deferrable=True,\n    )\n\n    image_import_result_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_2 = CloudVisionImportImageOperator(\n        task_id=\"image_import_result_2\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL_2,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n    )\n\n    image_import_result_2_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_2_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_3 = CloudVisionImportImageOperator(\n        task_id=\"image_import_result_3\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        deferrable=True,\n    )\n\n    image_import_result_3_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_3_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_4 = CloudVisionImportImageOperator(\n        task_id=\"image_import_result_4\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL_2,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        deferrable=True,\n    )\n\n    image_import_result_4_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_4_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_5 = CloudVisionImportImageOperator(\n        task_id=\"image_import_result_5\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        deferrable=True,\n    )\n\n    image_import_result_5_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_5_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_6 = CloudVisionImportImageOperator(\n        task_id=\"image_import_result_6\",\n        bucket_name=BUCKET_NAME,\n        source_image_uri=GCP_VISION_IMAGE_URL_2,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        deferrable=True,\n    )\n\n    image_import_result_6_break = CloudVisionImportImageOperation.retrieve_image_result(\n        task_id=\"image_import_result_6_break\", operation_id=image_import_id_\n    )\n\n    image_import_result_7 = CloudVisionImportImageOperator(\n        task_id=\""}], "metadata": {"file_name": "dag_597_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates a complete data integration process from external file sources to a Databricks SQL environment, including steps for table creation, data import, transformation, and output generation. The workflow should highlight the flexibility of connecting and manipulating data across different storage systems and compute environments.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_DEFAULT_ACCOUNT = os.environ.get(\"DATABRICKS_DEFAULT_ACCOUNT\")\n\n# [START howto_operator_databricks]\nwith models.DAG(\n    dag_id=\"example_databricks\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_databricks_submit_run]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"spark_job\",\n        databricks_conn_id=\"databricks_default\",\n        sql_task={\n            \"workspace_id\": ENV_ID,\n            \"sql_endpoint_name\": \"Databricks Prep\",\n            \"command\": \"python\",\n            \"parameters\": [\n                \"file:///\",\n                \"read_csv\",\n                \"input_file\",\n                \"example_file.csv\",\n                \"schema_file\",\n                \"example_schema.json\",\n                \"output_table\": \"example_output_table\",\n            ],\n        },\n    )\n    # [END howto_operator_databricks_submit_run]\n\n    # [START howto_sensor_databricks]\n    def wait_for_job(sleep=30):\n        while True:\n            # Poll the Databricks UI for the job's state.\n            job = DatabricksSensor(\n                task_id=\"wait_for_job\",\n                databricks_conn_id=\"databricks_default\",\n                sql_task={\n                    \"workspace_id\": ENV_ID,\n                    \"sql_endpoint_name\": \"Databricks Prep\",\n                    \"statement\": \"SELECT COUNT(*) FROM example_output_table\",\n                },\n                timeout=sleep,\n                poke_interval=sleep,\n            )\n            if not job.output:\n                break\n            else:\n                # If the job is still running, do not force stop the test.\n                return\n\n    wait_for_job()\n    # [END howto_sensor_databricks]\n\n    submit_run >> wait_for_job()\n\n    # Task dependency created via `XComArgs`:\n    # submit_run >> wait_for_job()"}], "metadata": {"file_name": "dag_791_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete data transfer lifecycle, including bucket and dataset creation, data extraction from a Trino information schema, and external table generation in BigQuery. Implement a robust, idempotent workflow that handles resource provisioning and cleanup automatically.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryDeleteTableOperator,\n    BigQueryGetDataOperator,\n    BigQueryInsertJobOperator,\n    BigQueryValueSetTableOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.trino_to_bigquery import TrinoToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.google.cloud.transfers.trino_to_external_table import TrinoToExternalTableOperator\nexcept ImportError:\n    trino_external_table_is_available = False\nelse:\n    from airflow.utils.trigger_rule import TriggerRule  # noqa\n\ntest_project_id = os.environ.get(\"TEST_GCP_PROJECT\", \"default\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_to_bigquery\"\n\nGCP_LOCATION = os.environ.get(\"GCP_LOCATION\", \"europe-west1\")\nGCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nGCS_BUCKET_PATH = f\"gs://{GCS_BUCKET_NAME}/\"\nTRINO_TABLE = \"trino_test\"\nEXTERNAL_TABLE_TABLE = \"external_test\"\nSQL_TABLE = \"bigquery_table\"\nSQL_TABLE_2 = \"bigquery_table2\"\nSQL_CREATE_TABLE = f\"CREATE TABLE {SQL_TABLE} AS SELECT * FROM trino_test;\"\nSQL_CREATE_TABLE_2 = f\"CREATE TABLE {SQL_TABLE_2} AS SELECT * FROM trino_test WHERE val % 2 = 0;\"\nSQL_DELETE_TABLE = f\"DELETE FROM {SQL_TABLE};\"\nSQL_DELETE_TABLE_2 = f\"DELETE FROM {SQL_TABLE_2};\"\nSQL_INSERT_TABLE = f\"INSERT INTO {SQL_TABLE} VALUES (1, 'one'), (2, 'two');\"\nSQL_INSERT_TABLE_2 = f\"INSERT INTO {SQL_TABLE_2} VALUES (1, 'one'), (2, 'two');\"\nSQL_SELECT = f\"SELECT * FROM {SQL_TABLE};\"\nSQL_SELECT_2 = f\"SELECT * FROM {SQL_TABLE_2};\"\nSQL_SELECT_3 = f\"SELECT val FROM {SQL_TABLE_2} WHERE val % 2 = 0;\"\nSQL_DROP_TABLE = f\"DROP TABLE IF EXISTS {SQL_TABLE};\"\nSQL_DROP_TABLE_2 = f\"DROP TABLE IF EXISTS {SQL_TABLE_2};\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"trino\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=GCS_BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=TRINO_TABLE, location=GCP_LOCATION\n    )\n\n    create_external_table_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_external_table_dataset\",\n        dataset_id=EXTERNAL_TABLE_TABLE,\n        location=GCP_LOCATION,\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=SQL_TABLE,\n        table_id=SQL_TABLE,\n        schema_fields=[\n            {\"name\": \"val\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        location=GCP_LOCATION,\n    )\n\n    create_table_2 = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table_2\",\n        dataset_id=SQL_TABLE_2,\n        table_id=SQL_TABLE_2,\n        schema_fields=[\n            {\"name\": \"val\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        location=GCP_LOCATION,\n    )\n\n    insert_data = BigQueryInsertJobOperator(\n        task_id=\"insert_data\",\n        configuration={\n            \"query\": {\n                \"query\": SQL_CREATE_TABLE,\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    insert_data_2 = BigQueryInsertJobOperator(\n        task_id=\"insert_data_2\",\n        configuration={\n            \"query\": {\n                \"query\": SQL_CREATE_TABLE_2,\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    value_set_table = BigQueryValueSetTableOperator(\n        task_id=\"value_set_table\",\n        dataset_id=SQL_TABLE,\n        dimensions=[\"date\"],\n        location=GCP_LOCATION,\n    )\n\n    value_set_table_2 = BigQueryValueSetTableOperator(\n        task_id=\"value_set_table_2\",\n        dataset_id=SQL_TABLE_2,\n        dimensions=[\"date\"],\n        location=GCP_LOCATION,\n    )\n\n    select_data = BigQueryGetDataOperator(\n        task_id=\"select_data\",\n        configuration={\n            \"query\": {\n                \"query\": SQL_SELECT,\n                \"useLegacySql\": False,\n                \"options\": [\"--pretty\"],\n            }\n        },\n    )\n\n    select_data_2 = BigQueryGetDataOperator(\n        task_id=\"select_data_2\",\n        configuration={\n            \"query\": {\n                \"query\": SQL_SELECT_2,\n                \"useLegacySql\": False,\n                \"options\": [\"--pretty\"],\n            }\n        },\n    )\n\n    select_data_3 = BigQueryGetDataOperator(\n        task_id=\"select_data_3\",\n        configuration={\n            \"query\": {\n                \"query\": SQL_SELECT_3,\n                \"useLegacySql\": False,\n                \"options\": [\"--pretty\"],\n            }\n        },\n    )\n\n    delete_data = BigQueryDeleteTableOperator(\n        task_id=\"delete_data\",\n        dataset_id=SQL_TABLE,\n        table_id=SQL_TABLE,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_data_2 = BigQueryDeleteTableOperator(\n        task_id=\"delete_data_2\",\n        dataset_id=SQL_TABLE_2,\n        table_id=SQL_TABLE_2,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET_NAME,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=TRINO_TABLE,\n        is_delete_table_operator_true=False,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_external_table_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_external_table_dataset\",\n        dataset_id=EXTERNAL_TABLE_TABLE,\n        is_delete_table_operator_true=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_trino_to_bigquery]\n    trino_to_bigquery = TrinoToBigQueryOperator(\n        task_id=\"trino_to_bigquery\",\n        conn_id=\"trino\",\n        sql=f\"SELECT * FROM {TRINO_TABLE}\",\n        destination_table_id=SQL_TABLE,\n        source_format=\"TRINO\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        task_id=\"trino_to_bigquery\",\n    )\n    # [END howto_operator_trino_to_bigquery]\n\n    # [START howto_operator_trino_to_bigquery_external]\n    trino_to_bigquery_external = TrinoToBigQueryOperator(\n        task_id=\"trino_to_bigquery_external\",\n        conn_id=\"trino\",\n        sql=f\"SELECT * FROM {TRINO_TABLE}\",\n        destination_table_id=SQL_TABLE_2,\n        source_format=\"TRINO\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        task_id=\"trino_to_bigquery_external\",\n    )\n    # [END howto_operator_trino_to_bigquery_external]\n\n    # [START howto_operator_trino_to_external_table]\n    trino_to_external_table = TrinoToExternalTableOperator(\n        task_id=\"trino_to_external_table\",\n        conn_id=\"trino\",\n        sql=f\"SELECT * FROM {TRINO_TABLE}\",\n        table_resource_id=EXTERNAL_TABLE_TABLE,\n        write_disposition=\"WRITE_TRUNCATE\",\n        task_id=\"trino_to_external_table\",\n    )\n    # [END howto_operator_trino_to_external_table]\n\n    # [START howto_operator_trino_to_external_table_def]\n    trino_to_external_table_def = TrinoToExternalTableOperator(\n        task_id=\"trino_to_external_table_def\",\n        conn_id=\"trino\",\n        sql=f\"SELECT * FROM {TRINO_TABLE}\",\n        table_resource_id=EXTERNAL_TABLE_TABLE,\n        schema_file=GCS_BUCKET_PATH + \"schema_trino.sql\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        task_id=\"trino_to_external_table_def\",\n    )\n    # [END howto_operator_trino_to_external_table_def]\n\n    delete_dataset.set_trigger(rule=TriggerRule.ALL_DONE)\n    delete_external_table_dataset.set_trigger(rule=TriggerRule.ALL_DONE)\n    delete_bucket.set_trigger(rule=TriggerRule.ALL_DONE)\n\n\n@pytest.fixture\ndef payload():\n    if trino_external_table_is_available:\n        return {\n            \"table_resource_id\": GCS_BUCKET_NAME + \"/\" +EXTERNAL_TABLE_TABLE,\n            \"sql\": f\"SELECT * FROM {TRINO_TABLE}\"\n        }\n    return {}\n\n\ndef test_payload(payload):\n    assert payload[\"table_resource_id\"] == GCS_BUCKET_NAME + \"/\" + EXTERNAL_TABLE_TABLE\n    assert payload[\"sql\"] == f\"SELECT * FROM {TRINO_TABLE}\"\n\n\n# [START howto_operator_trino_to_bigquery_xcom]\ndef test_trino_to_bigquery_xcom(payload, mock_connection, mock_translator, payload_returned):\n    assert mock_translator.translate == mock_connection.connect\n\n\ndef"}], "metadata": {"file_name": "dag_2074_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for provisioning and managing an Amazon EKS (Elastic Kubernetes Service) cluster with dynamic nodegroup creation. Create a system that demonstrates infrastructure-as-code deployment of Kubernetes resources using Airflow's AWS provider operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksGetServerlessApplicationRunnerOperator,\n    EksStartServerlessApplicationRunnerOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.edgemodifier import Edgemodifier\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eks\"\n\n# Externally fetched variables:\nCLUSTER_NAMESPACE_KEY = \"CLUSTER_NAMESPACE\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(CLUSTER_NAMESPACE_KEY).build()\n\nSAMPLE_APPLICATION_NAME = \"sample-application\"\nSAMPLE_APPLICATION_ROLE_ARN = \"sample-application-role-arn\"\nSAMPLE_APPLICATION_LARGEST_POD_REQUEST = \"10\"\n\n# File locations:\nFILE_DIR = Path(__file__).parent\nFILE_PATH = FILE_DIR / \"resources\" / \"eks\" / \"sample-application-cr.yaml\"\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task\ndef set_up_cloudwatch_log_group(log_group_name: str):\n    # Creates a log group if it does not exist.\n    boto3.client(\"logs\").create_log_group(\n        logGroupName=log_group_name,\n    )\n    # Sets the log group to overwrite any existing logs.\n    boto3.client(\"logs\").put_log_group_arns_for_grouparies(\n        logGroupName=log_group_name,\n        logArns=[f\"arn:aws:logs:{boto3.session.Session().region_name}:{boto3.client(\"ecs\").get_compatible_region()}:log-group/{log_group_name}\"],\n    )\n\n\n@task\ndef create_launch_permission_policy(cluster_name: str):\n    # Creates a launch permission policy that allows access to the cluster\n    boto3.client(\"eksctl\").get_waiter_waiter_config()[\"waiters\"][\n        \"eks:deploy_cluster_policy::\"\n    ][\"Handler\"](\n        ClusterName=cluster_name,\n        Name=\"allow-access-to-RMGBY466V23Q\",\n        Description=\"Enable access to Sample Application\",\n        Actions=[\n            \"ecs:DescribeClusters\",\n            \"ecs:UpdateClusterProps\",\n            \"ecs:DeleteCluster\",\n            \"ecs:CreateCluster\",\n            \"ecs:DeleteClusterTaskDefinition\",\n            \"ecs:DescribeClusterTasks\",\n            \"ecs:StartClusterTask\",\n            \"ecs:CreateClusterFromUpdate\",\n            \"ecs:DeleteClusterFromUpdate\",\n        ],\n        AddToRolePolicy=True,\n    )\n\n\n@task\ndef create_task_definition(log_group_name: str, family: str):\n    boto3.client(\"ecs\").register_task_definition(\n        family=family,\n        # Image is pulled from AWS ECS documentation for Elastic Kubernetes Service\n        # https://docs.aws.amazon.com/eks/latest/userguide/ecs.html\n        image=\"4vi8y83.ohl5y835.nq8y8y83.s3-website-us-east-1.amazonaws.com:7999/ecs/qflow-2\",\n        register_command=\"-N sample-application -r $AWS_EKS_SAMPLE_ROLE_ARN -t $AWS_EKS_TASK_ROLE_ARN\",\n        requires_compatibilities=[\"EC2\"],\n        network_mode=\"awsvpc\",\n        container_definitions=[\n            {\n                \"name\": \"sample-application\",\n                \"image\": \"4vi8y83.ohl5y835.nq8y8y83.s3-website-us-east-1.amazonaws.com:7999/ecs/qflow-2\",\n                \"workingDirectory\": \"/usr/local/bin\",\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": f\"/aws/ecs/{SAMPLE_APPLICATION_NAME}\",\n                        \"awslogs-region\": boto3.region_info.region_name,\n                        \"awslogs-create-group\": \"true\",\n                    },\n                },\n            },\n        ],\n        logging={\n            \"logDriver\": \"awslogs\",\n            \"options\": {\n                \"awslogs-group\": f\"/aws/ecs/{SAMPLE_APPLICATION_NAME}\",\n                \"awslogs-region\": boto3.region_info.region_name,\n                \"awslogs-create-group\": \"true\",\n            },\n        },\n        cpu=\"256\",\n        memory=\"512\",\n    )\n    boto3.client(\"logs\").put_log_stream_name_exists(logGroupName=log_group_name, logStreamName=\"app-log\")\n    boto3.client(\"logs\").put_log_group_arns_for_grouparies(\n        logGroupName=log_group_name,\n        logArns=[\n            f\"arn:aws:logs:{boto3.region_info.region_name}:{boto3.client('ecs').get_compatible_region()):log-group/{log_group_name}/app-log\",\n        ],\n    )\n\n\n@task\ndef create_rms(log_group_name: str, family: str):\n    boto3.client(\"ecs\").create_resource_endpoint(\n        ClusterName=\"sample-cluster\",\n        TaskDefinition=family,\n        PowerLimitMegaTasks=1,\n        DesiredCount=1,\n        RegisterResourcePosition=\"LOGISTICSIAMROLE\",\n        LogGroupRetentionDays=14,\n        LogGroupName=log_group_name,\n    )\n    boto3.client(\"ecs\").create_resource_endpoint(\n        ClusterName=\"sample-cluster\",\n        TaskDefinition=family,\n        PowerLimitMegaTasks=1,\n        DesiredCount=1,\n        RegisterResourcePosition=\"AWS_IAM_ROLE\",\n        LogGroupRetentionDays=14,\n        LogGroupName=log_group_name,\n    )\n\n\n@task\ndef get_iam_role_arn_from_resource_endpoint(resource_endpoint_arn: str):\n    # Resource endpoint ARN should match the form:\n    # ecs\u8d44\u6e90\u7aef\u70b9ID.eks..amazonaws.com/resourceEndpoints/\u8d44\u6e90\u540d\u79f0\n    resource_endpoint_name = resource_endpoint_arn.split(\".ecs.eks.amazonaws.com/\")[1]\n    response = boto3.client(\"ecs\").describe_resource_endpoint(\n        ResourceEndpointArn=resource_endpoint_arn,\n    )\n    return response[\"ResourceEndpointDescriptions\"][0][\"RegisteredResources\"][0][\"Arn\"]\n\n\n@task\ndef get_lambda_permission_policy(cluster_name: str):\n    # Creates a resource permission policy that allows access to the cluster\n    boto3.client(\"ecs\").get_waiter_waiter_config()[\"waiters\"][\n        \"ecs:deploy_resource_endpoint::\"\n    ][\"Handler\"](\n        ClusterName=cluster_name,\n        ResourceEndpointArn=get_iam_role_arn_from_resource_endpoint(resource_endpoint_arn),\n        ServiceName=\"sample-application\",\n        Path=\"/sample-application/\",\n        FunctionName=\"SampleFunction\",\n        AddToRolePolicy=True,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_resources(cluster_name: str, nodegroup_name: str, task_definition_family: str):\n    cluster_endpoint_arn = (\n        boto3.client(\"ecs\").describe_resource_endpoint(\n            ResourceEndpointArn=f\"{cluster_name}.ecs.amazonaws.com\",\n        )\n        [\"ResourceEndpointDescriptions\"][0]\n        [\"ResourceEndpointArn\"]\n    )\n    boto3.client(\"ecs\").delete_resource_endpoint(\n        ResourceEndpointArn=cluster_endpoint_arn, WaitForCompletion=True\n    )\n    boto3.client(\"ecs\").register_task_definition(\n        family=task_definition_family,\n        container_definitions=[\n            {\n                \"name\": \"sample-application\",\n                \"image\": \"4vi8y83.ohl5y835.nq8y8y83.s3-website-us-east-1.amazonaws.com:7999/ecs/qflow-2\",\n                \"workingDirectory\": \"/usr/local/bin\",\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": f\"/aws/ecs/{SAMPLE_APPLICATION_NAME}\",\n                        \"awslogs-region\": boto3.region_info.region_name,\n                        \"awslogs-create-group\": \"true\",\n                    },\n                },\n            },\n        ],\n        requires_compatibilities=[\"EC2\"],\n        network_mode=\"awsvpc\",\n    )\n    boto3.client(\"eks\").delete_cluster(\n        ClusterName=cluster_name,\n        DeleteClusterResources=True,\n        SkipUpdateClusterUserPermissions=Edgemodifier(),\n    )\n    boto3.client(\"iam\").delete_role_policy(\n        RoleName=\"SampleApplicationRole\", PolicyName=\"SampleApplicationRole\"\n    )\n    boto3.client(\"iam\").delete_account_alias(AliasName=\"sample-cluster-alias\")\n    boto3.client(\"logs\").delete_log_group(logGroupName=\"/aws/ecs/{name}/app-log\".format(name=SAMPLE_APPLICATION_NAME))\n    boto3.client(\"logs\").delete_log_group(logGroupName=\"/sample-application-cr/\")\n    boto3.client(\"resourcegroupmanager\").delete_resource_group(\n        ResourceGroupName=\"ecs-resource-endpoint-group-{}\".format(SAMPLE_APPLICATION_NAME)\n    )\n\n\n@task\ndef setup_env():\n    # This test creates an IAM alias and enables IMDSv2 in the Kubernetes cluster.\n    # The test needs network access to the EKS control plane and access to the\n    # EKS cluster being tested.  The alias name and cluster name are derived from\n    # the cluster namespace which is stored in the environment variable\n    # test_context"}], "metadata": {"file_name": "dag_732_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kubernetes volume management workflow that automatically creates and deletes a persistent volume claim (PVC) with a 5GB storage allocation. The workflow should run daily and demonstrate basic Kubernetes resource lifecycle management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.cncf.kubernetes.operators.batch import (\n    BatchCreateFromExistingOperator,\n    BatchDeleteOperator,\n    BatchDeleteNowOperator,\n    BatchStartOperator,\n    BatchStartOperatorWithWait,\n    BatchStartWithWaitOperator,\n    BatchStartWithWaitOperatorAsync,\n    BatchStartWithWaitAndPollOperator,\n    BatchStartWithWaitAndPollOperatorAsync,\n)\nfrom airflow.providers.cncf.kubernetes.sensors.batch import BatchStartSensor, BatchTensorboardSensor\n\nPVC_NAME = \"test-pvc\"\nbatch_name = f\"batch_{PVC_NAME}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\nTENSORBOARD_URL = os.getenv(\"TENSORBOARD_URL\", \"http://testtensorboardurl:6066\")\n\nwith DAG(\n    \"example_batch\",\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n    @task\n    def create_pvc():\n        from airflow.providers.cncf.kubernetes.hooks.batch import BatchHook\n\n        BatchHook().create_pvc(name=PVC_NAME, namespace=\"test-k8s\")\n\n    @task\n    def create_batch():\n        from airflow.providers.cncf.kubernetes.hooks.batch import BatchHook\n\n        batch = {\n            \"apiVersion\": \"batch/v1\",\n            \"kind\": \"Batch\",\n            \"metadata\": {\n                \"name\": batch_name,\n            },\n            \"spec\": {\n                \"image\": \"perl:5.34.0\",\n                \"resources\": {\"requests\": {\"nvidia.com/gpu\": 1}},\n                \"publish_ports\": [{\"port\": 80, \"protocol\": \"TCP\"}],\n                \"volume_files\": [\n                    {\n                        \"name\": PVC_NAME,\n                        \"path\": \"/tmp\",\n                        \"mode\": \"0777\",\n                    }\n                ],\n                \"env\": [\n                    {\"name\": \"ENV_VAR\", \"value\": \"Testing Environment\"},\n                ],\n            },\n        }\n        BatchHook().create_batch(batch)\n\n    @task\n    def delete_batch():\n        from airflow.providers.cncf.kubernetes.hooks.batch import BatchHook\n\n        BatchHook().delete_batch(batch_name=batch_name)\n\n    @task\n    def delete_pvc():\n        from airflow.providers.cncf.kubernetes.hooks.batch import BatchHook\n\n        BatchHook().delete_pvc(name=PVC_NAME, namespace=\"test-k8s\")\n\n    # [START howto_batch_start]\n    start_batch = BatchStartOperator(\n        task_id=\"start_batch\",\n        batch_name=batch_name,\n        wait_for_completion=True,\n    )\n    # [END howto_batch_start]\n\n    # [START howto_batch_start_def]\n    start_batch_def = BatchStartOperator(\n        task_id=\"start_batch_def\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_start_def]\n\n    # [START howto_batch_start_with_wait]\n    wait_batch = BatchStartOperatorWithWait(\n        task_id=\"wait_batch\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_start_with_wait]\n\n    # [START howto_batch_start_with_wait_def]\n    wait_batch_def = BatchStartOperatorWithWait(\n        task_id=\"wait_batch_def\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_start_with_wait_def]\n\n    # [START howto_batch_start_async]\n    start_batch_async = BatchStartOperatorAsync(\n        task_id=\"start_batch_async\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_start_async]\n\n    # [START howto_batch_start_with_wait_async]\n    wait_batch_async = BatchStartOperatorWithWaitAsync(\n        task_id=\"wait_batch_async\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_start_with_wait_async]\n\n    # [START howto_batch_start_tensorboard]\n    tensorboard_batch = BatchStartOperator(\n        task_id=\"tensorboard_batch\",\n        batch_name=batch_name,\n        tensorboard_url=TENSORBOARD_URL,\n    )\n    # [END howto_batch_start_tensorboard]\n\n    # [START howto_batch_start_tensorboard_def]\n    tensorboard_batch_def = BatchStartOperator(\n        task_id=\"tensorboard_batch_def\",\n        batch_name=batch_name,\n        tensorboard_url=TENSORBOARD_URL,\n    )\n    # [END howto_batch_start_tensorboard_def]\n\n    # [START howto_batch_sensor]\n    check_batch = BatchStartSensor(\n        task_id=\"check_batch\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_sensor]\n\n    # [START howto_batch_sensor_def]\n    check_batch_def = BatchStartSensor(\n        task_id=\"check_batch_def\",\n        batch_name=batch_name,\n    )\n    # [END howto_batch_sensor_def]\n\n    # [START howto_batch_tensorboard]\n    tensorboard_batch_tensorboard = BatchTensorboardSensor(\n        task_id=\"tensorboard_batch_tensorboard\",\n        batch_name=batch_name,\n        target_url=TENSORBOARD_URL,\n    )\n    # [END howto_batch_tensorboard]\n\n    # [START howto_batch_tensorboard_def]\n    tensorboard_batch_tensorboard_def = BatchTensorboardSensor(\n        task_id=\"tensorboard_batch_tensorboard_def\",\n        batch_name=batch_name,\n        target_url=TENSORBOARD_URL,\n    )\n    # [END howto_batch_tensorboard_def]\n\n    # [START howto_batch_delete]\n    delete_batch_def = BatchDeleteOperator(task_id=\"delete_batch_def\", batch_name=batch_name)\n    # [END howto_batch_delete]\n\n    # [START howto_batch_delete_now]\n    delete_batch_now = BatchDeleteNowOperator(task_id=\"delete_batch_now\", batch_name=batch_name)\n    # [END howto_batch_delete_now]\n\n    # [START howto_batch_create_from_existing]\n    create_batch_from_existing = BatchCreateFromExistingOperator(\n        task_id=\"create_batch_from_existing\",\n        batch_name=batch_name,\n        image=\"perl:5.34.0\",  # can leave blank to use existing batch\n        namespace=\"test-k8s\",  # can leave blank to use existing batch's namespace\n    )\n    # [END howto_batch_create_from_existing]\n\n    (\n        create_pvc()\n        >> create_batch()\n        >> start_batch\n        >> wait_batch\n        >> tensorboard_batch\n        >> check_batch\n        >> create_batch_from_existing\n        >> delete_batch_def\n        >> delete_batch_now\n        >> delete_pvc()\n    )"}], "metadata": {"file_name": "dag_1153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can we determine the mode of a dataset in Python, especially for a small dataset like [4, 5, 2, 1, 4, 5, 4, 3]?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import PythonOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_function():\n    # [START sample_function]\n    def sample_function():\n        # Your code here\n        return [1, 2, 3]\n\n    # [END sample_function]\n\n    # [START sample_dataset]\n    def sample_dataset():\n        # Your code here\n        return \"test.csv\"\n\n    # [END sample_dataset]\n\n    sample_dataset >> sample_function()\n\n\nexample = example_function()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a BigQuery table to a Google Cloud Storage bucket. Create an Airflow DAG that demonstrates how to extract data from a BigQuery dataset and export it to a cloud storage location.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryDeleteTableOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.providers.google.cloud.utils import job_trigger\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"bigquery_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-north1\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"table_name\"\nGCS_DESTINATION_PREFIX = f\"gs://{BUCKET_NAME}/\"\nSCHEMA = [\n    {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\", \"gcs\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    @task\n    def create_temp_role():\n        role = Role()\n        role.id = f\"temp_role_{DAG_ID}_{ENV_ID}\"\n        role.principal_id = f\"{PROJECT_ID}.serviceaccount\"\n        role.principal_type = \"SERVICE_ACCOUNT\"\n        role.create_time = datetime.now()\n        save_conn = Connection(\n            conn_id=\"temp_role\",\n            conn_type=\"google_cloud_platform\",\n            schema=SINGLE_VALUE_GCP_API,\n            extra={\"project\": PROJECT_ID, \"role\": role},\n        )\n        db_cursor.execute(save_conn)\n        return role.id\n\n    create_temp_role_id = create_temp_role().id\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    @task\n    def create_gcs_destination_prefix():\n        return GCS_DESTINATION_PREFIX\n\n    create_gcs_destination_prefix = create_gcs_destination_prefix()\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        table_resource={\n            \"schema\": {\"fields\": SCHEMA},\n            \"inputAcceptanceTestConfig\": {\n                \"dataSource\": {\"bigqueryDataset\": {\"datasetId\": DATASET_NAME, \"tableId\": TABLE_NAME}}\n            },\n        },\n    )\n\n    # [START howto_operator_bigquery_to_gcs]\n    bigquery_to_gcs = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs\",\n        source_project_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\",\n        destination_cloud_storage_uris=[f\"{GCS_DESTINATION_PREFIX}{TABLE_NAME}.csv\"],\n    )\n    # [END howto_operator_bigquery_to_gcs]\n\n    # [START howto_trigger_bigquery_to_gcs]\n    trigger = job_trigger.JobTrigger(\n        task_id=\"trigger_job\",\n        project_id=PROJECT_ID,\n        job_id=\"test-bigquery-to-gcs-job\",\n    )\n    # [END howto_trigger_bigquery_to_gcs]\n\n    delete_temp_role = JobTrigger(\n        task_id=\"delete_temp_role\",\n        project_id=PROJECT_ID,\n        job_id=create_temp_role_id,\n    )\n\n    delete_temp_role_min = JobTrigger(\n        task_id=\"delete_temp_role_min\",\n        project_id=PROJECT_ID,\n        job_id=\"test-bigquery-to-gcs-job-2\",\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    delete_dataset_min = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset_min\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # [START howto_trigger_delete_bucket]\n    trigger_delete_bucket = job_trigger.JobTrigger(\n        task_id=\"trigger_delete_bucket\",\n        project_id=PROJECT_ID,\n        job_id=\"test-bigquery-to-gcs-job-2\",\n    )\n    # [END howto_trigger_delete_bucket]\n\n    (\n        create_dataset\n        >> create_temp_role_id\n        >> create_bucket\n        >> create_gcs_destination_prefix\n        >> create_table\n        >> bigquery_to_gcs\n        >> trigger\n        >> delete_temp_role\n        >> delete_dataset\n        >> delete_bucket\n        >> trigger_delete_bucket\n    )\n\n    # TaskGroup is a convenient way to group related tasks.\n    with TaskGroup(\"bigquery_to_gcs_args\") as bigquery_to_gcs_args:\n        # [START howto_operator_bigquery_to_gcs_args]\n        bigquery_to_gcs_args = BigQueryToGCSOperator(\n            task_id=\"bigquery_to_gcs_args\",\n            source_project_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\",\n            destination_cloud_storage_uris=[f\"{GCS_DESTINATION_PREFIX}{TABLE_NAME}.csv\"],\n        )\n        # [END howto_operator_bigquery_to_gcs_args]\n\n        # [START howto_trigger_bigquery_to_gcs]\n        trigger = job_trigger.JobTrigger(\n            task_id=\"trigger_job\",\n            project_id=PROJECT_ID,\n            job_id=\"test-bigquery-to-gcs-job\",\n        )\n        # [END howto_trigger_bigquery_to_gcs]\n\n    (\n        create_dataset\n        >> create_temp_role_id\n        >> create_bucket\n        >> create_gcs_destination_prefix\n        >> create_table\n        >> bigquery_to_gcs_args\n        >> trigger\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    # In the following DAG, the templated fields ({{ task_instance.xcom_pull(...) }}) will be replaced with the\n    # result of the corresponding Python function call when the DAG is triggered.\n\n    (\n        create_dataset\n        >> [create_temp_role_id, create_bucket, create_gcs_destination_prefix]\n        >> create_table\n        >> bigquery_to_gcs\n        >> trigger\n        >> delete_temp_role\n        >> delete_dataset\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_1579_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Given a string of words, write a Python function to split the string into words and sort them lexicographically. Assume that the words are separated by spaces.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n@task\ndef split_and_sort(words):\n    \"\"\"\n    Function to split a given string of words by space and sort the list of words lexicographically.\n    \"\"\"\n    words_list = words.split()\n    sorted_words = sorted(words_list)\n    return sorted_words\n\n\nwith DAG(\n    \"example_split_and_sort_dag\",\n    default_args={\"max_active_runs\": 1},\n) as dag:\n    bash_task = BashOperator(\n        task_id=\"split_and_sort_example\",\n        bash_command=f\"split_and_sort '{ '{{' 'words' '}}' }'\",\n    )\n    bash_task"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a multi-step cloud-based audio processing workflow, showing how to chain together Google Cloud operators for text-to-speech synthesis and speech translation. The DAG will showcase task dependencies, cloud service integration, and automated workflow management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import CloudTextToSpeechSynthesizeOperator\nfrom airflow.providers.google.cloud.operators.translate_speech import CloudTranslateSpeechDetectLanguageOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_translate_speech\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_translate_speech]\nwith models.DAG(\n    DAG_ID,\n    schedule_interval=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    text_to_speech = CloudTextToSpeechSynthesizeOperator(\n        project_id=PROJECT_ID,\n        body={\n            \"input_config\": {\n                \"gcs_input\": {\n                    \"uri\": f\"gs://{BUCKET_NAME}/output/text_to_speech.mp3\"\n                }\n            },\n            \"voice_params\": {\n                \"language_code\": \"en-US\",\n                \"ssml_gender\": \"FEMALE\"\n            }\n        },\n        task_id=\"text_to_speech_synthesize\"\n    )\n\n    translate_speech_detect_language = CloudTranslateSpeechDetectLanguageOperator(\n        project_id=PROJECT_ID,\n        file_name=f\"{BUCKET_NAME}/output/text_to_speech.mp3\",\n        task_id=\"translate_speech_detect_language\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> text_to_speech\n        >> translate_speech_detect_language\n        >> delete_bucket\n    )\n    # [END howto_operator_translate_speech]"}], "metadata": {"file_name": "dag_2108_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure deployment pipeline that automatically provisions a Kubernetes cluster with secure metadata handling and demonstrates the ability to run containerized workloads. Create a scalable and repeatable process for initializing cloud-native compute resources with built-in compliance and verification steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nimport tempfile\nfrom datetime import datetime\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.trigger_rule import TriggerRuleMixin\n\nENV_ID = str(pendulum.datetime.now().year % 10)\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1),\n    catchup=False,\n    default_args={\"owner\": \"airflow\"},\n    tags=[\"example\", \"singularity\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo hello world\")\n\n    # [START howto_operator_singularity]\n    task_1 = SingularityOperator(\n        command=\"/bin/sleep 5\",\n        image=\"nharris/singularity-test-vm:2.3.2-py38-dfsg-20210308\",\n        environment={\n            \"SOURCE_DIR\": \"/tmp\",\n            \"ARGS\": \"[\\\"-s\\\", \\\"/tmp\\\" ,\\\"-e\\\", \\\"TEST\\\"]\",\n        },\n        task_id=\"singularity_test\",\n    )\n    # [END howto_operator_singularity]\n\n    @task.trigger_rule(TriggerRule.ALL_DONE)\n    def catchall():\n        print(\"All tasks have completed\")\n\n    run_this >> task_1 >> catchall()\n    for weekday in WeekDay:\n        run_this >> BashOperator(task_id=f\"run_{weekday}\", bash_command=f\"echo {weekday}\")\n\n    for i, weekday in enumerate(WeekDay):\n        filename = f\"file_{weekday}.tmp\"\n        with tempfile.NamedTemporaryFile() as file:\n            # Replace tokens in template:\n            # %s is replaced by the argument passed to the BashOperator\n            # %d is replaced by the day of the week (0=Mon, 6=Sun)\n            file.write(\n                \"touch $SOURCE_DIR/$ARGS\\n\".encode()\n                + filename.encode()\n                + (\n                    f\"echo This file was created with BashOperator on {weekday} at {datetime.now().strftime('%H:%M')}\"\n                    .encode()\n                )\n            )\n\n            BashOperator(\n                task_id=f\"get_{weekday}\",\n                bash_command=f\"cat {file.name} || mv {filename} {file.name}\",\n            )\n\n    for weekday in WeekDay:\n        BashOperator(\n            task_id=f\"remove_{weekday}\",\n            bash_command=f\"rm {filename}\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    for i, weekday in enumerate(WeekDay):\n        BashOperator(\n            task_id=f\"remove_file_{weekday}\",\n            bash_command=f\"rm {file.name}\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    for weekday in WeekDay:\n        BashOperator(\n            task_id=f\"remove_{weekday}_dir\",\n            bash_command=f\"rm -rf {filename}\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    for i, weekday in enumerate(WeekDay):\n        Connection(\n            task_id=f\"connection_{weekday}\",\n            conn_id=f\"connection_{weekday}\",\n            conn_type=\"singularity\",\n            sql_command=f\"create connection connection_{weekday} as 'nharris/singularity-test-vm:2.3.2-py38-dfsg-20210308';\",\n        )\n\n    for i, weekday in enumerate(WeekDay):\n        BashOperator(\n            task_id=f\"get_connection_{weekday}\",\n            bash_command=f\"echo \\\"CONN_ID={weekday}\\\" >> /etc/singularity/singularity.conf\",\n        )\n\n    for i, weekday in enumerate(WeekDay):\n        BashOperator(\n            task_id=f\"remove_connection_{weekday}\",\n            bash_command=f\"rm -rf /etc/singularity/singularity.conf\",\n        )"}], "metadata": {"file_name": "dag_387_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that showcases data cube transformation and maintenance processes, demonstrating how to systematically process, update, and manage analytical data structures using Kylin operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.kylin import (\n    KylinCreateBlueprintOperator,\n    KylinCreateCubeOperator,\n    KylinCreateTableOperator,\n    KylinDeleteBlueprintOperator,\n    KylinDeleteCubeOperator,\n    KylinDeleteTableOperator,\n    KylinRunCubeOperator,\n    KylinRunJobOperator,\n)\nfrom airflow.providers.google.cloud.sensors.kylin import KylinJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_kylin\"\n\nLOCATION = \"us-central1\"\nBLUEPRINT_NAME = f\"kylin_blueprint_{ENV_ID}\"\nCUBE_NAME = f\"kylin_cube_{ENV_ID}\"\nTABLE_NAME = f\"kylin_table_{ENV_ID}\"\nCRON_TIME = \"0 0 * * *\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"owner\": \"Kylin Team\",\n        \"email\": [\"liudayu@google.com\"],\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"kylin\"],\n) as dag:\n    # [START howto_operator_create_blueprint]\n    create_blueprint = KylinCreateBlueprintOperator(\n        task_id=\"create_blueprint\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_name=BLUEPRINT_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_blueprint]\n\n    # [START howto_operator_create_cube]\n    create_cube = KylinCreateCubeOperator(\n        task_id=\"create_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_name=CUBE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_cube]\n\n    # [START howto_operator_create_table]\n    create_table = KylinCreateTableOperator(\n        task_id=\"create_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        table_name=TABLE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_table]\n\n    # [START howto_operator_run_cube]\n    run_cube = KylinRunCubeOperator(\n        task_id=\"run_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        cron=\"0 0 * * *\",\n    )\n    # [END howto_operator_run_cube]\n\n    run_cube_sensor = KylinJobSensor(\n        task_id=\"run_cube_sensor\",\n        job_id=None,\n        job_name=f\"cube_{CUBE_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_delete_cube]\n    delete_cube = KylinDeleteCubeOperator(\n        task_id=\"delete_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n    )\n    # [END howto_operator_delete_cube]\n\n    delete_cube_sensor = KylinJobSensor(\n        task_id=\"delete_cube_sensor\",\n        job_id=None,\n        job_name=f\"cube_{CUBE_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_delete_blueprint]\n    delete_blueprint = KylinDeleteBlueprintOperator(\n        task_id=\"delete_blueprint\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n    )\n    # [END howto_operator_delete_blueprint]\n\n    delete_blueprint_sensor = KylinJobSensor(\n        task_id=\"delete_blueprint_sensor\",\n        job_id=None,\n        job_name=f\"blueprint_{BLUEPRINT_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_sync_table]\n    sync_table = KylinRunJobOperator(\n        task_id=\"sync_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_sync_table]\n\n    # [START howto_operator_get_table]\n    get_table = KylinRunJobOperator(\n        task_id=\"get_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_get_table]\n\n    # [START howto_operator_list_tables]\n    list_tables = KylinRunJobOperator(\n        task_id=\"list_tables\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_list_tables]\n\n    # [START howto_operator_delete_table]\n    delete_table = KylinDeleteTableOperator(\n        task_id=\"delete_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        table_id=TABLE_NAME,\n    )\n    # [END howto_operator_delete_table]\n\n    delete_table_sensor = KylinJobSensor(\n        task_id=\"delete_table_sensor\",\n        job_id=None,\n        job_name=f\"table_{TABLE_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_create_blueprint]\n    create_blueprint = KylinCreateBlueprintOperator(\n        task_id=\"create_blueprint\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_name=BLUEPRINT_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_blueprint]\n\n    # [START howto_operator_create_cube]\n    create_cube = KylinCreateCubeOperator(\n        task_id=\"create_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_name=CUBE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_cube]\n\n    # [START howto_operator_create_table]\n    create_table = KylinCreateTableOperator(\n        task_id=\"create_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        table_name=TABLE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_create_table]\n\n    # [START howto_operator_run_cube]\n    run_cube = KylinRunCubeOperator(\n        task_id=\"run_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        cron=\"0 0 * * *\",\n    )\n    # [END howto_operator_run_cube]\n\n    run_cube_sensor = KylinJobSensor(\n        task_id=\"run_cube_sensor\",\n        job_id=None,\n        job_name=f\"cube_{CUBE_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_delete_cube]\n    delete_cube = KylinDeleteCubeOperator(\n        task_id=\"delete_cube\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n    )\n    # [END howto_operator_delete_cube]\n\n    delete_cube_sensor = KylinJobSensor(\n        task_id=\"delete_cube_sensor\",\n        job_id=None,\n        job_name=f\"cube_{CUBE_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_delete_blueprint]\n    delete_blueprint = KylinDeleteBlueprintOperator(\n        task_id=\"delete_blueprint\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n    )\n    # [END howto_operator_delete_blueprint]\n\n    delete_blueprint_sensor = KylinJobSensor(\n        task_id=\"delete_blueprint_sensor\",\n        job_id=None,\n        job_name=f\"blueprint_{BLUEPRINT_NAME}\",\n        trigger_rule=TriggerRule.NONE_FAILED,\n    )\n\n    # [START howto_operator_sync_table]\n    sync_table = KylinRunJobOperator(\n        task_id=\"sync_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_sync_table]\n\n    # [START howto_operator_get_table]\n    get_table = KylinRunJobOperator(\n        task_id=\"get_table\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_get_table]\n\n    # [START howto_operator_list_tables]\n    list_tables = KylinRunJobOperator(\n        task_id=\"list_tables\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        blueprint_id=BLUEPRINT_NAME,\n        cube_id=CUBE_NAME,\n        sql=f\"SELECT * FROM gcs://{LOCATION}//{TABLE_NAME}\",\n    )\n    # [END howto_operator_list_tables]\n\n    # [START howto_operator_delete_table]\n    delete_table = KylinDeleteTable"}], "metadata": {"file_name": "dag_1944_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a streaming data processing system that captures real-time taxi ride events, transforms them using Apache Beam, and routes the processed data through Google Cloud Pub/Sub topics with automated infrastructure management. Create a scalable and repeatable data streaming solution that demonstrates cloud-native data processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.pubsub import (\n    PubSubCreateTopicOperator,\n    PubSubDeleteTopicOperator,\n    PubSubPublishMessageOperator,\n    PubSubPullOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.pubsub_to_gcs import PubSubToGCSOperator\nfrom airflow.utils.context import Context\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"pubsub_to_gcs\"\n\nGCP_LOCATION = os.environ.get(\"GCP_LOCATION\", \"europe-west1\")\n\nSOURCE_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDESTINATION_BUCKET_NAME = f\"bucket_dest_{DAG_ID}_{ENV_ID}\"\nTOPIC_ID = f\"topic_{DAG_ID}_{ENV_ID}\"\nMESSAGE = {\"data\": b\"Airflow is fun!\"}\n\n\nwith DAG(\n    DAG_ID,\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"pubsub\", \"gcs\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(task_id=\"create_bucket_src\", bucket_name=SOURCE_BUCKET_NAME)\n\n    create_bucket_dest = GCSCreateBucketOperator(task_id=\"create_bucket_dest\", bucket_name=DESTINATION_BUCKET_NAME)\n\n    create_topic = PubSubCreateTopicOperator(task_id=\"create_topic\", topic=TOPIC_ID, project_id=PROJECT_ID)\n\n    publish_message = PubSubPublishMessageOperator(\n        task_id=\"publish_message\",\n        project_id=PROJECT_ID,\n        topic=TOPIC_ID,\n        messages=[MESSAGE, MESSAGE],\n    )\n\n    pull_messages = PubSubPullOperator(\n        task_id=\"pull_messages\", project_id=PROJECT_ID, topic=TOPIC_ID,ack_messages=True, )\n    # [START howto_operator_pubsub_to_gcs]\n    publish_topic = PubSubCreateTopicOperator(task_id=\"publish_topic\", topic=TOPIC_ID, project_id=PROJECT_ID)\n\n    publish_message_2 = PubSubPublishMessageOperator(\n        task_id=\"publish_message_2\",\n        project_id=PROJECT_ID,\n        topic=TOPIC_ID,\n        messages=[b\"Airflow is amazing!\", b\"Airflow is flexible!\"],\n    )\n\n    pull_messages_2 = PubSubPullOperator(\n        task_id=\"pull_messages_2\", project_id=PROJECT_ID, topic=TOPIC_ID,ack_messages=True, )\n    # [END howto_operator_pubsub_to_gcs]\n\n    # [START howto_operator_gcs_to_pubsub]\n    copy_message_to_pubsub = GCSToGCSOperator(\n        task_id=\"copy_message_to_pubsub\",\n        source_bucket=SOURCE_BUCKET_NAME,\n        source_object=\"pubsub_to_gcs/*.txt\",\n        destination_bucket=DESTINATION_BUCKET_NAME,\n        destination_object=\"pubsub_to_gcs/\",\n    )\n    # [END howto_operator_gcs_to_pubsub]\n\n    # [START howto_operator_gcs_to_pubsub_result]\n    copy_message_to_pubsub_result = GCSToGCSOperator(\n        task_id=\"copy_message_to_pubsub_result\",\n        source_bucket=SOURCE_BUCKET_NAME,\n        source_object=\"pubsub_to_gcs_result/*.txt\",\n        destination_bucket=DESTINATION_BUCKET_NAME,\n        destination_object=\"pubsub_to_gcs_result/\",\n        trigger_rule=\"all_done\",\n        result={\n            \"pubsub_to_gcs_result_2\": {\n                \"project_id\": PROJECT_ID,\n                \"topic\": TOPIC_ID,\n            },\n        },\n    )\n    # [END howto_operator_gcs_to_pubsub_result]\n\n    # [START howto_operator_pubsub_to_gcs_result]\n    copy_message_to_pubsub_result_result = GCSToGCSOperator(\n        task_id=\"copy_message_to_pubsub_result_result\",\n        source_bucket=DESTINATION_BUCKET_NAME,\n        source_object=\"pubsub_to_gcs_result/*.txt\",\n        destination_bucket=SOURCE_BUCKET_NAME,\n        destination_object=\"pubsub_to_gcs_result/\",\n        trigger_rule=\"all_done\",\n        result={\n            \"pubsub_to_gcs_result\": {\n                \"project_id\": PROJECT_ID,\n                \"topic\": TOPIC_ID,\n            },\n        },\n    )\n    # [END howto_operator_pubsub_to_gcs_result]\n\n    delete_topic = PubSubDeleteTopicOperator(task_id=\"delete_topic\", topic=TOPIC_ID, project_id=PROJECT_ID)\n\n    delete_bucket_src = GCSDeleteBucketOperator(task_id=\"delete_bucket_src\", bucket_name=SOURCE_BUCKET_NAME)\n    delete_bucket_dest = GCSDeleteBucketOperator(task_id=\"delete_bucket_dest\", bucket_name=DESTINATION_BUCKET_NAME)\n\n    (\n        # TEST SETUP\n        create_bucket_src\n        >> create_bucket_dest\n        >> create_topic\n        >> publish_message\n        >> pull_messages\n        # TEST BODY\n        >> publish_topic\n        >> publish_message_2\n        >> pull_messages_2\n        # TEST TEARDOWN\n        >> delete_topic\n        >> delete_bucket_src\n        >> delete_bucket_dest\n    )\n\n    pull_messages.wait_for_completion = pull_messages_2.wait_for_completion = True\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(watcher(dag))\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1215_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "\u8bf7\u7ed9\u6211\u4e00\u4e2aPython\u4ee3\u7801\u793a\u4f8b\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e24\u4e2a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3002\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def calculate_average(num1, num2):\n    \"\"\"\n    Calculate the average of two numbers.\n\n    Args:\n    num1 (float): The first number.\n    num2 (float): The second number.\n\n    Returns:\n    float: The average of the two numbers.\n    \"\"\"\n    return (num1 + num2) / 2\n\n# Example usage\nresult = calculate_average(10, 20)\nprint(result)  # Output: 15.0"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs a complete data transfer operation between two database tables, highlighting the steps of source data preparation, data movement, and destination table validation. Demonstrate a systematic approach to moving data with clear, traceable steps from source to destination.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_orm_sql]\ndef setup_connection():\n    \"\"\"Connection setup function for demo purposes\"\"\"\n    return \"my_pet_db\", \"my_pet_user\", \"my_pet_password\"\n\n\ndef setup_roles(db_conn_id):\n    \"\"\"Roles setup function for demo purposes\"\"\"\n    return \"pet\", \"read_write\"\n\n\ndef setup_macros(db_conn_id, pet_role, user_role):\n    \"\"\"Macros setup function for demo purposes\"\"\"\n    return {\n        \"pets\": f\"CREATE MACRO macro_name AS SELECT * FROM allpets WHERE pet_role='{pet_role}'\",\n        \"allpets\": f\"CREATE MACRO macro_name AS SELECT * FROM {{ this.schema }}.{{ this.table }}\",\n        \"allpets_rw\": f\"CREATE MACRO macro_name AS SELECT * FROM allpets WHERE pet_role='{user_role}'\",\n    }\n\n\ndef populate_db(_ctx, db_conn_id, pet_role, user_role, rows_to_insert=10, table=\"allpets\"):\n    \"\"\"Populate the database with sample data for testing\"\"\"\n    connection_info = f\"conn_id='{db_conn_id}'\"\n    sql = f\"INSERT {connection_info}.{table} VALUES (1, 'Dog', '{pet_role}', '{user_role}')\"\n    _ctx['conn'].execute(sql)\n    sql = f\"INSERT {connection_info}.{table} VALUES (2, 'Cat', '{pet_role}', '{user_role}')\"\n    _ctx['conn'].execute(sql)\n    if rows_to_insert > 1:\n        sql = f\"INSERT {connection_info}.{table} VALUES (3, 'Fish', '{pet_role}', '{user_role}')\"\n        _ctx['conn'].execute(sql)\n\n\ndef call_macro(_ctx, db_conn_id, macro_name, table=\"allpets\"):\n    \"\"\"Call macro to test macro def\"\"\"\n    _ctx['conn'].execute(f\"CALL {{'{macro_name}'}}();\")\n    _ctx['conn'].execute(f\"CALL {{'{macro_name}'}}({table});\")\n\n\nwith DAG(\n    \"example_sql_operator\",\n    schedule=datetime(2023, 1, 1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    env_id = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n    connection_id = f\"postgres_{env_id}\"\n    user_role = \"reader\"\n    pet_role = \"writer\"\n    rows_to_insert = 5\n\n    setup = SQLExecuteQueryOperator(\n        task_id=\"setup_connection\",\n        sql=setup_connection(),\n    )\n    setup_role = SQLExecuteQueryOperator(\n        task_id=\"create_role\",\n        sql=setup_roles(connection_id),\n    )\n    setup_macro = SQLExecuteQueryOperator(\n        task_id=\"create_macro\",\n        sql=setup_macros(connection_id, pet_role, user_role),\n    )\n    insert_pet = SQLExecuteQueryOperator(\n        task_id=\"insert_pet\",\n        sql=f\"INSERT {connection_id}.allpets VALUES (?,?,?,?)\",\n        parameters=(\n            4,\n            \"Hamster\",\n            pet_role,\n            user_role,\n        ),\n    )\n    insert_cats = SQLExecuteQueryOperator(\n        task_id=\"insert_cats\",\n        sql=f\"INSERT {connection_id}.allpets VALUES (?,?,?,?)\",\n        parameters=(\n            5,\n            \"Cat\",\n            pet_role,\n            user_role,\n        ),\n    )\n    insert_fish = SQLExecuteQueryOperator(\n        task_id=\"insert_fish\",\n        sql=f\"INSERT {connection_id}.allpets VALUES (?,?,?,?)\",\n        parameters=(\n            6,\n            \"Fish\",\n            pet_role,\n            user_role,\n        ),\n    )\n    clean = SQLExecuteQueryOperator(\n        task_id=\"clean_up\",\n        sql=f\"DELETE FROM {connection_id}.allpets WHERE pet_role!='{user_role}'\",\n    )\n    read_pets = SQLExecuteQueryOperator(\n        task_id=\"read_pets\",\n        sql=f\"CALL {{'{macro_name}'}}();\",\n    )\n    read_pet_rw = SQLExecuteQueryOperator(\n        task_id=\"read_pet_rw\",\n        sql=f\"CALL {{'{macro_name}'}}({table});\",\n    )\n    call_setup_role = SQLExecuteQueryOperator(\n        task_id=\"call_setup_role\",\n        sql=f\"CALL {{'{setup_role}'}}();\",\n    )\n    call_insert_pet = SQLExecuteQueryOperator(\n        task_id=\"call_insert_pet\",\n        sql=f\"CALL {{'{setup_macro}'}}({table});\",\n    )\n    call_insert_cats = SQLExecuteQueryOperator(\n        task_id=\"call_insert_cats\",\n        sql=f\"CALL {{'{setup_macro}'}}({table});\",\n    )\n    call_insert_fish = SQLExecuteQueryOperator(\n        task_id=\"call_insert_fish\",\n        sql=f\"CALL {{'{setup_macro}'}}({table});\",\n    )\n\n    setup >> setup_role >> setup_macro >> insert_pet >> insert_cats >> insert_fish >> clean\n    read_pets >> read_pet_rw"}], "metadata": {"file_name": "dag_1827_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering solution that demonstrates how to ingest raw data from an external source, transform it using a cloud-based data preparation tool, and generate a clean, structured output in a specified cloud storage location. Develop a reproducible data processing workflow that can be easily configured and scaled.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.datajoin import (\n    DataJoinOperator,\n    DataResolveTableOperator,\n    ExternalTableOperator,\n    ExternalTableTransformOperator,\n    TablePartitionOperator,\n)\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_datajoin\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nPARTITION_CHROMES = \"chromes\"\nPARTITION_COLUMNS = [\"model\"]\nFORMAT = \"parquet\"\nFORMAT_OPTIONS = {\"file_type\": FORMAT}\nPARTITION_FORMAT_OPTIONS = {\"file_type\": FORMAT}\nTABLE_NAME = \"new_table\"\nEXTERNAL_TABLE_BUCKET = \"gs://airflow-system-tests-resources/gcs/datajoin/bucket\"\nEXTERNAL_TABLE_NAME = \"external_table\"\nSCHEMA = [\n    {\"name\": \"model\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"chromes\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datajoin\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    upload_file = S3ToGCSOperator(\n        task_id=\"upload_file\",\n        source_bucket=\"airflow-system-tests-resources\",\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME,\n        replace=True,\n    )\n\n    # [START howto_operator_datajoin_basic]\n    datajoin_basic = DataJoinOperator(\n        task_id=\"datajoin_basic\",\n        project_id=PROJECT_ID,\n        join_fields=[{\"source_field\": \"chromes\", \"destination_field\": \"model\"}],\n        source_bucket=EXTERNAL_TABLE_BUCKET,\n        external_table_destination_bucket=EXTERNAL_TABLE_BUCKET,\n        schema=SCHEMA,\n        format_options=FORMAT_OPTIONS,\n        target_bucket=BUCKET_NAME,\n        target_path=TABLE_NAME,\n    )\n    # [END howto_operator_datajoin_basic]\n\n    # [START howto_operator_datajoin_disjoint]\n    datajoin_disjoint = DataJoinOperator(\n        task_id=\"datajoin_disjoint\",\n        project_id=PROJECT_ID,\n        source_bucket=EXTERNAL_TABLE_BUCKET,\n        external_table_destination_bucket=EXTERNAL_TABLE_BUCKET,\n        schema=SCHEMA,\n        format_options=FORMAT_OPTIONS,\n        target_bucket=BUCKET_NAME,\n        target_path=TABLE_NAME,\n        join_fields=[{\"source_field\": \"chromes\", \"destination_field\": \"model\"}],\n        disjoint=True,\n    )\n    # [END howto_operator_datajoin_disjoint]\n\n    # [START howto_operator_datajoin_multiple_sources]\n    datajoin_multiple_sources = DataJoinOperator(\n        task_id=\"datajoin_multiple_sources\",\n        project_id=PROJECT_ID,\n        source_bucket=EXTERNAL_TABLE_BUCKET,\n        external_table_destination_bucket=EXTERNAL_TABLE_BUCKET,\n        schema=SCHEMA,\n        format_options=FORMAT_OPTIONS,\n        target_bucket=BUCKET_NAME,\n        target_path=TABLE_NAME,\n        join_fields=[\n            {\"source_field\": \"chromes\", \"destination_field\": \"model\"},\n            {\"source_field\": \"chromes_2\", \"destination_field\": \"model_2\"},\n        ],\n    )\n    # [END howto_operator_datajoin_multiple_sources]\n\n    # [START howto_operator_datajoin_named]\n    datajoin_named = DataJoinOperator(\n        task_id=\"datajoin_named\",\n        project_id=PROJECT_ID,\n        source_bucket=EXTERNAL_TABLE_BUCKET,\n        external_table_destination_bucket=EXTERNAL_TABLE_BUCKET,\n        schema=SCHEMA,\n        format_options=FORMAT_OPTIONS,\n        target_bucket=BUCKET_NAME,\n        target_path=TABLE_NAME,\n        join_fields=[\n            {\"source_field\": \"chromes\", \"destination_field\": \"model\"},\n            {\"source_field\": \"chromes_2\", \"destination_field\": \"model_2\"},\n        ],\n        name=\"my_join\",\n    )\n    # [END howto_operator_datajoin_named]\n\n    # [START howto_operator_dataresolve_table]\n    dataresolve_table = DataResolveTableOperator(\n        task_id=\"dataresolve_table\",\n        project_id=PROJECT_ID,\n        bucket=BUCKET_NAME,\n        table_name=TABLE_NAME,\n    )\n    # [END howto_operator_dataresolve_table]\n\n    # [START howto_operator_ext_table]\n    ext_table_task = ExternalTableOperator(\n        task_id=\"ext_table_task\",\n        bucket=EXTERNAL_TABLE_BUCKET,\n        table_name=EXTERNAL_TABLE_NAME,\n        schema=PARTITION_SCHEMA = [\n            {\"name\": \"model\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"chromes\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        format_options=PARTITION_FORMAT_OPTIONS,\n    )\n    # [END howto_operator_ext_table]\n\n    # [START howto_operator_ext_table_transform]\n    ext_table_transform_task = ExternalTableTransformOperator(\n        task_id=\"ext_table_transform_task\",\n        bucket=EXTERNAL_TABLE_BUCKET,\n        table_name=EXTERNAL_TABLE_NAME,\n        input_path=f\"gs://{BUCKET_NAME}/{TABLE_NAME}\",\n        output_path=\"tmp/\",\n        sql=\"SELECT * FROM {{ this.table_name }} WHERE model='test_model'\",\n    )\n    # [END howto_operator_ext_table_transform]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> upload_file\n        >> datajoin_basic\n        >> datajoin_disjoint\n        >> datajoin_multiple_sources\n        >> datajoin_named\n        >> dataresolve_table\n        >> ext_table_task\n        >> ext_table_transform_task\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the temp file\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1225_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data engineering workflow that transforms raw city data by creating schemas, generating tables with predefined structures, and inserting sample city records using Trino as the data processing platform. The pipeline should demonstrate how to systematically build and populate database objects with incremental complexity.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.trino.operators.trino import TrinoOperator\n\n# [START howto_trino_operator]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"owner\": \"airflow\"},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_trino_create_table_operator]\n    create_city_table = TrinoOperator(\n        task_id=\"create_city_table\",\n        sql=\"CREATE TABLE city (city_id ID, name VARCHAR, population INT)\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_create_table_operator]\n\n    # [START howto_trino_populate_table_operator]\n    populate_city_table = TrinoOperator(\n        task_id=\"populate_city_table\",\n        sql=\"INSERT INTO city VALUES (1, 'New York', 8627000), (2, 'Los Angeles', 3998400)\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_populate_table_operator]\n\n    # [START howto_trino_create_view_operator]\n    create_city_view = TrinoOperator(\n        task_id=\"create_city_view\",\n        sql=\"CREATE VIEW city_view AS SELECT city_id, name, population FROM city\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_create_view_operator]\n\n    # [START howto_trino_insert_statement_operator]\n    insert_city_data = TrinoOperator(\n        task_id=\"insert_city_data\",\n        sql=\"INSERT INTO city VALUES (3, 'Chicago', 2704577), (4, 'Houston', 2342397)\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_insert_statement_operator]\n\n    # [START howto_trino_create_index_operator]\n    create_city_index = TrinoOperator(\n        task_id=\"create_city_index\",\n        sql=\"CREATE INDEX idx_city ON city(city_id)\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_create_index_operator]\n\n    # [START howto_trino_create_external_table_operator]\n    create_city_external_table = TrinoOperator(\n        task_id=\"create_city_external_table\",\n        sql=\"CREATE EXTERNAL TABLE city_ext (city_id ID, name VARCHAR, population INT) LOCATION 's3://airflow/ext_data/'\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_create_external_table_operator]\n\n    # [START howto_trino_insert_statement_external_table_operator]\n    insert_city_data_ext = TrinoOperator(\n        task_id=\"insert_city_data_ext\",\n        sql=\"INSERT INTO city_ext VALUES (5, 'Detroit', 954926)\",\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_trino_insert_statement_external_table_operator]\n\n    (\n        create_city_table\n        >> populate_city_table\n        >> create_city_view\n        >> insert_city_data\n        >> create_city_index\n        >> create_city_external_table\n        >> insert_city_data_ext\n    )"}], "metadata": {"file_name": "dag_2235_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I need a function that calculates the factorial of a number in Python. Can you provide one?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def calculate_factorial(n):\n    \"\"\"\n    Calculate the factorial of a non-negative integer n.\n\n    Args:\n    n (int): A non-negative integer whose factorial is to be calculated.\n\n    Returns:\n    int: The factorial of the given number n.\n    \"\"\"\n    if n == 0:\n        return 1\n    else:\n        return n * calculate_factorial(n - 1)\n\n# Example usage\nprint(calculate_factorial(5))  # Output: 120"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data movement pipeline that generates random binary files and demonstrates cloud storage data transfer and replication strategies. Create an automated workflow that shows how to programmatically manage file uploads, bucket interactions, and cross-bucket synchronization using Airflow and Google Cloud Storage operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.azure_blob_to_gcs import AzureBlobStorageToGCSOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to azure_blob import LocalFilesystemToAzureBlobStorageOperator\nfrom airflow.providers.microsoft.azure.transfers.azure_blob_to_local import AzureBlobStorageToLocalFilesystemOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_azure_blob_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nAZURE_CONTAINER_NAME = \"mycontainer\"\n\nFILE_NAME = \"file.txt\"\nPATH_TO_FILE = f\"gcs/{FILE_NAME}\"\nTAB_PATH = f\"azure_blob_storage/{FILE_NAME}\"\n\nAZURE_FILE_PATH = f\"azure_blob_storage/{FILE_NAME}\"\n\nCONNECTION_ID = \"google_cloud_platform\"\n\nCURRENT_FOLDER = os.path.dirname(os.path.abspath(__file__))\n\nSINGLE_FILE_LOCAL_PATH = os.path.join(CURRENT_FOLDER, \"resources\", FILE_NAME)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azure_blob_storage\", \"gcs\"],\n) as dag:\n    @task\n    def create_connection(connection_id: str):\n        conn = Connection(\n            conn_id=connection_id,\n            conn_type=\"google_cloud_platform\",\n        )\n        conn_extra = {\n            \"project_id\": PROJECT_ID,\n            \"scope\": \"https://storage.googleapis.com/storage-api-v3-rule\",\n            \"uri\": \"\",\n            \"principle\": \"\",\n            \"password\": \"\",\n        }\n        conn_extra_json = json.dumps(conn_extra)\n        conn.set_extra(conn_extra_json)\n        return conn\n\n    create_connection_task = create_connection(connection_id=CONNECTION_ID)\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToAzureBlobStorageOperator(\n        task_id=\"upload_file\",\n        dest_blob_name=TAB_PATH,\n        bucket_name=BUCKET_NAME,\n        file_path=SINGLE_FILE_LOCAL_PATH,\n    )\n\n    # [START howto_operator_azure_blob_to_gcs]\n    azure_blob_to_gcs = AzureBlobStorageToGCSOperator(\n        task_id=\"azure_blob_to_gcs\",\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=TAB_PATH,\n        bucket_name=BUCKET_NAME,\n        replace=True,\n    )\n    # [END howto_operator_azure_blob_to_gcs]\n\n    download_file = AzureBlobStorageToLocalFilesystemOperator(\n        task_id=\"download_file\",\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=TAB_PATH,\n        bucket_name=BUCKET_NAME,\n        file_path=TAB_PATH,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_bucket >> upload_file >> create_bucket >> azure_blob_to_gcs >> download_file >> create_connection_task\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the task from the schedule\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1650_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an employee salary data processing pipeline that transforms raw employee data from BigQuery using Google Cloud Dataflow. Create a workflow that loads employee salary information, applies transformations, and writes results to a new BigQuery table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nLOCATION = \"europe-west3\"\nDAG_ID = \"bigquery_to_dataflow\"\nBQ_DATASET = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBQ_TABLE_IN = f\"emp_salary_{DAG_ID}_{ENV_ID}\"\nBQ_TABLE_OUT = f\"emp_salary_aggregated_{DAG_ID}_{ENV_ID}\"\nBQ_TABLE_OUT_DEF = f\"emp_salary_aggregated_def_{DAG_ID}_{ENV_ID}\"\nBQ_LOCATION = \"us\"\n\nSCHEMA = [\n    {\"name\": \"employee_id\", \"type\": \"INTEGER\", \"mode\": \"1\"},\n    {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"1\"},\n]\n\nINSERT_DATE = \"2020\"\nINSERT_ROWS = [{\"employee_id\": i, \"salary\": 30000 + i} for i in range(1, 6)]\n\nCONTAINER_SOURCE_URL = \"gs://airflow-system-tests-resources/dataflow/bigquery/template.json\"\nWORKFLOW_NAME = f\"test-flex-template-{ENV_ID}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"bigquery\", \"dataflow\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=BQ_DATASET,\n        location=BQ_LOCATION,\n    )\n\n    create_table_in = BigQueryCreateTableOperator(\n        task_id=\"create_table_in\",\n        dataset_id=BQ_DATASET,\n        table_id=BQ_TABLE_IN,\n        schema_fields=SCHEMA,\n        project_id=PROJECT_ID,\n    )\n\n    insert_into_table_in = BigQueryInsertJobOperator(\n        task_id=\"insert_into_table_in\",\n        configuration={\n            \"query\": {\n                \"query\": \"INSERT `test-dataset1.`+`test-table1.` VALUES (1, 20000), (2, 20001), (3, 20002), (4, 20003), (5, 20004);\",\n                \"useLegacySql\": False,\n            }\n        },\n        location=BQ_LOCATION,\n    )\n\n    # [START howto_operator_bigquery_to_dataflow]\n    bigquery_to_dataflow = DataflowStartFlexTemplateOperator(\n        task_id=\"bigquery_to_dataflow\",\n        template_location=f\"gs://{BUCKET}/dataflow/templates/{WORKFLOW_NAME}.json\",\n        parameters={\n            \"input\": f\"bq:{BQ_DATASET}.{BQ_TABLE_IN}\",\n            \"output\": f\"gs://{BUCKET}/output/\",\n            \"project_id\": PROJECT_ID,\n            \"location\": LOCATION,\n        },\n    )\n    # [END howto_operator_bigquery_to_dataflow]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=BQ_DATASET,\n        trigger_rule=TriggerRule.ALL_DONE,\n        location=BQ_LOCATION,\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_table_in\n        >> insert_into_table_in\n        # TEST BODY\n        >> bigquery_to_dataflow\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done;\n    # (using --checkonly option) fails without it\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1218_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning workflow from data ingestion to model training and cleanup. Develop a systematic task sequence that handles Google Cloud Storage operations, dataset creation, model training, and resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.nlp import CloudNLPTrainAutoMLTextModelOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"nlp_train_automl_text\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nTEXT = \"This is a test text.\"\nMODEL_DISPLAY_NAME = f\"auto-text-model-{ENV_ID}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"nlp\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    train_model = CloudNLPTrainAutoMLTextModelOperator(\n        task_id=\"train_model\",\n        location=\"us-central1\",\n        display_name=MODEL_DISPLAY_NAME,\n        gcs_source=gcs_source={\"input_uri\": f\"gs://{BUCKET_NAME}/text-extraction.csv\"},\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> train_model\n        # TEST TEARDOWN\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_1312_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates end-to-end machine learning orchestration with clear task dependencies and cloud-based infrastructure management. Build a workflow that creates a dataset, trains an AutoML model, and automatically cleans up resources after model training.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.auto_ml import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLDeployModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateCopyObjectOperator, GCSDeleteObjectOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_gcp_auto_ml\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET = \"test-airflow-gcp-auto-ml\"\nFILE_NAME = \"image_classification_training_set_images.zip\"\nDESTINATION_FILE_PATH = \"image_classification_test_images.zip\"\nMODEL_DISPLAY_NAME = f\"model_{DAG_ID}_{ENV_ID}\"\nREGION = \"us-central1\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\nIMAGE_GCS_BUCKET = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nIMAGE_LOCAL_PATH = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nDESTINATION_IMAGE_PATH = f\"gs://{BUCKET_NAME}/{DESTINATION_FILE_PATH}\"\n\nTABULAR_GCS_BUCKET = f\"gs://{BUCKET_NAME}/tabular-mit-btrt-dataset.csv\"\nTABULAR_LOCAL_PATH = f\"gs://{BUCKET_NAME}/{TABULAR_GCS_BUCKET}\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"auto_ml\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    copy_dataset_file = GCSCreateCopyObjectOperator(\n        task_id=\"copy_dataset_file\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=IMAGE_GCS_BUCKET,\n        destination_bucket=RESOURCE_DATA_BUCKET,\n        destination_object=DESTINATION_FILE_PATH,\n    )\n\n    create_tabular_dataset = AutoMLCreateDatasetOperator(\n        task_id=\"tabular_dataset_create\",\n        target_column_id=\"class_id\",\n        dataset_config={\n            \"display_name\": \"tabular-mit-btrt-dataset\",\n            \"source_info\": {\"gcs_source\": {\"uri\": TABULAR_LOCAL_PATH}},\n            \"info_source_type\": \"DAY_TIME\",\n            \"target_column_transform_type\": \"KEEP_ALL\",\n            \"test_percent\": 20,\n            \"validation_percent\": 70,\n            \"training_percent\": 10,\n        },\n        project_id=PROJECT_ID,\n    )\n\n    import_image_dataset = AutoMLImportDataOperator(\n        task_id=\"image_dataset_import\",\n        dataset_id=create_tabular_dataset.output[\"dataset_id\"],\n        bucket=BUCKET_NAME,\n        object_name=IMAGE_GCS_BUCKET,\n        source_format=\"GCS\",\n        project_id=PROJECT_ID,\n    )\n\n    image_model = AutoMLTrainModelOperator(\n        task_id=\"image_model\",\n        display_name=MODEL_DISPLAY_NAME,\n        prediction_type=\"multiclass_classification\",\n        model_display_name=MODEL_DISPLAY_NAME,\n        project_id=PROJECT_ID,\n        training_dataset_id=import_image_dataset.output[\"dataset_id\"],\n    )\n\n    deploy_image_model = AutoMLDeployModelOperator(\n        task_id=\"image_model_deploy\",\n        model_id=image_model.output[\"id\"],\n        http_port=8081,\n        protocol=\"HTTP\",\n        project_id=PROJECT_ID,\n    )\n\n    delete_image_model = AutoMLDeleteModelOperator(\n        task_id=\"delete_image_model\",\n        model_id=image_model.output[\"id\"],\n        is_delete_leftover_datasets=True,\n        project_id=PROJECT_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_object = GCSDeleteObjectOperator(\n        task_id=\"delete_object\",\n        bucket_name=BUCKET_NAME,\n        object_name=DESTINATION_FILE_PATH,\n    )\n\n    delete_tabular_dataset = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=create_tabular_dataset.output[\"dataset_id\"],\n        is_delete_leftover_datasets=True,\n        project_id=PROJECT_ID,\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2115_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "\u5982\u4f55\u5728Python\u4e2d\u4f7f\u7528\u88c5\u9970\u5668\u6765\u68c0\u67e5\u6743\u9650\uff1f\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.s3.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\n\nS3_DEFAULT_BUCKET = os.environ.get(\"S3_DEFAULT_BUCKET\", \"example-s3-bucket\")\n\n\n@task\ndef create_s3_bucket():\n    \"\"\"Create bucket\"\"\"\n    S3Hook().conn.create_bucket(S3_DEFAULT_BUCKET)\n\n\n@task\ndef upload_file_to_s3(bucket_name, key):\n    \"\"\"Upload file to S3\"\"\"\n    file_path = \"test.txt\"\n    S3Hook().conn.upload_file(file_path, bucket_name, key)\n\n\n@task\ndef check_s3_file_content(bucket_name, key):\n    \"\"\"Check file content\"\"\"\n    s3_hook = S3Hook()\n    s3_bucket = s3_hook.load_file(path=file_path, dest_path=key)\n    assert \"Hello, World\" in s3_bucket\n\n\nwith DAG(\n    \"s3-example\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_s3_bucket_task = create_s3_bucket()\n    upload_file_to_s3_task = upload_file_to_s3(S3_DEFAULT_BUCKET, \"test.txt\")\n    check_s3_file_content_task = check_s3_file_content(S3_DEFAULT_BUCKET, \"test.txt\")\n\n    create_s3_bucket_task >> upload_file_to_s3_task >> check_s3_file_content_task"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the step-by-step operational control of a cloud-based machine learning notebook, showcasing how to orchestrate infrastructure management tasks using AWS SageMaker operators. Develop a workflow that demonstrates dynamic instance management with clear, sequential task dependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker import SageMakerStartNotebookExecutionOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import SageMakerNotebookExecutionSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nDAG_ID = os.environ[\"DAG_ID\"]\nSAGEMAKER_NOTEBOOK_NAME_KEY = \"SAGEMAKER_NOTEBOOK_NAME\"\nSAGEMAKER_SESSION_TOKEN_KEY = \"SAGEMAKER_SESSION_TOKEN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(SAGEMAKER_NOTEBOOK_NAME_KEY).add_variable(\n    SAGEMAKER_SESSION_TOKEN_KEY\n).build()\n\nexec_role_arn = os.environ.get(\"EXEC_ROLE_ARN\")\nnotebook_role_arn = os.environ.get(\"SAGEMAKER_NOTEBOOK_ROLE_ARN\")\n\n\n# Externally fetched variables (for returning the response of the creation of the notebook):\nCREATE_NOTEBOOK_BODY = {\"role_arn\": notebook_role_arn, \"code_uri\": \"s3://dkemr-sagemaker-asserts/MLScript.py\"}\n\n\n@task\ndef create_launch_config(execution_role_arn: str):\n    log.info(\"Creating launch config %s with container/image nginx:1.15.6\"), execution_role_arn\n    boto3.client(\"sagemaker\").create_launch_config(\n        LaunchConfigName=\"test-launch-nginx\",\n        ContainerProperties={\n            \"ImageUri\": \"nginx:1.15.6\",\n            \"ExecutionRoleArn\": execution_role_arn,\n        },\n    )\n\n\n@task\ndef create_notebook(notebook_name: str, session_token: str):\n    log.info(\"Creating SageMaker Notebook %s with Token %s\", notebook_name, session_token)\n    boto3.client(\"sagemaker\", config={\"EndpointAccessRoleArn\": \"my-role\", \"SessionToken\": session_token}).create_notebook_instance(\n        NotebookName=notebook_name, ExecutionRoleArn=exec_role_arn\n    )\n\n\n@task\ndef wait_for_notebook_execution_to_complete(notebook_name: str, conn: boto3.resource):\n    waiter = conn.get_waiter(\"notebook_execution_complete\")\n    waiter.wait(\n        Filters=[\n            {\n                \"Name\": \"notebook_name\",\n                \"Values\": [notebook_name],\n            }\n        ],\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_notebook(notebook_name: str, session_token: str):\n    log.info(\"Deleting SageMaker Notebook %s with Token %s\", notebook_name, session_token)\n    boto3.client(\"sagemaker\").delete_notebook_instance(NotebookName=notebook_name, RoleArn=exec_role_arn)\n    boto3.client(\"sagemaker-runtime\", config={\"EndpointAccessRoleArn\": \"my-role\", \"EndpointId\": session_token}).delete_endpoint_config(\n        EndpointName=\"test-endpoint\"\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sagemaker\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    notebook_name = test_context[SAGEMAKER_NOTEBOOK_NAME_KEY]\n    session_token = test_context[SAGEMAKER_SESSION_TOKEN_KEY]\n\n    launch_config_name = f\"{env_id}-launch-nginx\"\n    create_launch_config(launch_config_name)\n    create_notebook(notebook_name, session_token)\n    wait_for_notebook_execution_to_complete(notebook_name, boto3.resource(\"sagemaker\")())\n    connection = Connection(\n        conn_id=\"sagemaker-api\",\n        conn_id_extra={\"endpoint_url\": f\"https://sagemaker.{env_id}.aws.amazon.com\"},  # Add region name\n    )\n    delete_notebook(notebook_name, session_token)\n    delete_launch_config(launch_config_name, connection=connection)\n\n    notebook_execution_id = boto3.client(\"sagemaker\").create_notebook_execution(\n        NotebookName=notebook_name, RoleArn=exec_role_arn, Body=json.dumps(CREATE_NOTEBOOK_BODY)\n    )[\"NotebookExecutionId\"]\n\n    notebook_execution_sensor = SageMakerNotebookExecutionSensor(\n        notebook_name=notebook_name, notebook_execution_id=notebook_execution_id, task_id=\"notebook_sensor\"\n    )\n\n    terminate_notebook_execution = BashOperator(\n        task_id=\"terminate_notebook_execution\",\n        bash_command=f\"aws sagemaker terminate-notebook-execution '{notebook_execution_id}'\",\n    )\n\n    notebook_execution_sensor >> terminate_notebook_execution"}], "metadata": {"file_name": "dag_759_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated metadata management pipeline that allows dynamic configuration and manipulation of data catalog tag templates for standardizing data asset descriptions and classifications. The workflow will enable systematic tracking and enhancement of metadata across cloud data environments.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.nlp import AlchemyMetadataCatalogOperator\n\nSTART_DATE = datetime(2021, 1, 1)\n\n# [START howto_alchemy_catalog_aws]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\ncatalog = AlchemyMetadataCatalogOperator(\n    task_id=\"catalog_objects\",\n    input_data={\n        \"entities\": [\n            \"GCS_BUCKET\",\n            \"GCS_OBJECT\",\n            \"OBJECT_AWSDMS_TABLE\",\n        ],\n        \"values\": [\"bucket_name\"],  # List of valid entity types to be cataloged.\n        \"count\": 1,\n        \"step\": 1,\n        \"entity_format\": \"LIST\",  # Format of the return value for Entities query. Optional. Defaults to LIST.\n        \"start_time\": \"2023-01-01T00:00:00Z\",  # Optional. Time range to query for catalog items.\n        \"end_time\": \"2023-02-01T00:00:00Z\",  # Optional. Time range to query for catalog items.\n        \"agg_by_entity\": False,  # Optional. Return aggregated count per entity. Defaults to False.\n        \"entity_format\": \"JSON\",  # Format of the return value for AggEntities query. Optional. Defaults to JSON.\n    },\n    ak=\"ak\",\n    sk=\"sk\",\n    region=\"cn\",\n    access_type=\"common\",\n)\n# [END howto_alchemy_catalog_aws]\n\n\nDAG_ID = \"example_alchemy_catalog\"\ndags_dir = os.path.dirname(__file__)\nsys_test_file = os.path.join(dags_dir, \"resources\", \"alchemy\", \"catalog.json\")\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=START_DATE,\n    catchup=False,\n    tags=[\"example\", \"alchemy\", \"catalog\"],\n) as dag:\n    catalog\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1333_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates basic Pig script execution in a local Hadoop environment. Create a DAG that runs a simple Pig command to list directory contents as a learning example of task configuration.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.hadoop.operators.hadoop import HadoopExecuteBashOperator\n\n\n@task\ndef create_spark_env():\n    # [START create_spark_env]\n    return \"spark.driver.cores=2;spark.driver.memory=4G;spark.executor.cores=1;spark.executor.memory=16G\"\n    # [END create_spark_env]\n\n\n@task\ndef set_up(hadoop_home, spark_env):\n    os.system(f\"{hadoop_home}/bin/hadoop jar {hadoop_home}/contrib/streaming/hadoop-streaming.jar -D {spark_env} \"\n              \"-input /tmp/input -output /tmp/output -mapper 'if [ \\$FILENAME == &quot;file1&quot; ]; then echo '\n              'hello world';fi;' -reducer 'if [ \\$FILENAME == &quot;file2&quot; ]; then echo hello airflow;'\n              'fi;' -file file1\")\n    os.system(f\"{hadoop_home}/bin/hadoop jar {hadoop_home}/contrib/streaming/hadoop-streaming.jar -D {spark_env} \"\n              \"-input /tmp/input -output /tmp/output -mapper 'if [ \\$FILENAME == &quot;file1&quot; ]; then cat '\n              '>/tmp/out;fi;' -reducer 'if [ \\$FILENAME == &quot;file2&quot; ]; then cat /tmp/out;fi;' -file file2\")\n\n\n@task\ndef clean(hadoop_home):\n    os.system(f\"{hadoop_home}/bin/hadoop jar {hadoop_home}/contrib/streaming/hadoop-streaming.jar -D '\n    spark.env -input /tmp/input -output /tmp/output -mapper 'if [ \\$FILENAME == &quot;file1&quot; ]; then echo '\n    'hello world';fi;' -reducer 'if [ \\$FILENAME == &quot;file2&quot; ]; then echo hello airflow;fi;' -file file3\"\n    )\n\n\nwith DAG(\n    \"example_hadoop_streaming\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n    create_spark_env()\n    set_up(create_spark_env())\n    execute = HadoopExecuteBashOperator(task_id=\"execute\", command=\"cat /tmp/output\")\n    clean(create_spark_env())\n    execute"}], "metadata": {"file_name": "dag_1536_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for deploying and managing a simple containerized task on AWS ECS Fargate. Create a DAG that demonstrates creating an ECS cluster, registering a task definition, running a container with a basic command, and then cleaning up resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.openlineage.operator import OpenLineageTestOperator\n\n# [START howto_operator_ecs]\n# Let's create a DAG that executes two tasks on ECS.\n# First task is to create an ECS Cluster\n# Second task is to create an ECS Task Definition\n# And finally run a bash command on the ECS Task\n\nwith DAG(\n    dag_id=\"example_ecs\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_ecs_cluster(cluster_name):\n        # Definition of the cluster:\n        # In this cluster, we can create one or several tasks\n        # Here we create one task\n        # The ECS Cluster name, the group to which the cluster belongs, and the Amazon Ecs Cluster Parameter\n        # see http://docs.aws.amazon.com/ECS/latest/developerguide/cluster-create-from-template.html\n        print(\"Start creating ECS cluster\")\n        command = f\"aws ecs create-cluster --cluster-name {cluster_name} --region us-east-1 --parameter-overrides '\"\n        commands = [\n            f'{\"--register-container-definition\"} {\"--container-name\"} PythonTest {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task-definition\"} PythonTask {\"--family\"} PythonTest',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest2 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask2 {\"--family\"} PythonTest2',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest3 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask3 {\"--family\"} PythonTest3',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest4 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask4 {\"--family\"} PythonTest4',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest5 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask5 {\"--family\"} PythonTest5',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest6 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask6 {\"--family\"} PythonTest6',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest7 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask7 {\"--family\"} PythonTest7',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest8 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask8 {\"--family\"} PythonTest8',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest9 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask9 {\"--family\"} PythonTest9',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest10 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask10 {\"--family\"} PythonTest10',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest11 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask11 {\"--family\"} PythonTest11',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest12 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask12 {\"--family\"} PythonTest12',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest13 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask13 {\"--family\"} PythonTest13',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest14 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask14 {\"--family\"} PythonTest14',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest15 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask15 {\"--family\"} PythonTest15',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest16 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask16 {\"--family\"} PythonTest16',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest17 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask17 {\"--family\"} PythonTest17',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest18 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask18 {\"--family\"} PythonTest18',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest19 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask19 {\"--family\"} PythonTest19',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest20 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask20 {\"--family\"} PythonTest20',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest21 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask21 {\"--family\"} PythonTest21',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest22 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask22 {\"--family\"} PythonTest22',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest23 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask23 {\"--family\"} PythonTest23',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest24 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask24 {\"--family\"} PythonTest24',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest25 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask25 {\"--family\"} PythonTest25',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest26 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask26 {\"--family\"} PythonTest26',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest27 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask27 {\"--family\"} PythonTest27',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest28 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask28 {\"--family\"} PythonTest28',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest29 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask29 {\"--family\"} PythonTest29',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest30 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask30 {\"--family\"} PythonTest30',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest31 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask31 {\"--family\"} PythonTest31',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest32 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask32 {\"--family\"} PythonTest32',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest33 {\"--image\"} {\"ubuntu:20.04\"}',\n            f'{\"--task_definition\"} PythonTask33 {\"--family\"} PythonTest33',\n            f'{\"--register-container_definition\"} {\"--container-name\"} PythonTest34 {\"--image\"} {\"ubuntu:2"}], "metadata": {"file_name": "dag_1087_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you suggest a way to modify the given Python code to ensure that a list is only modified if the new value is not already present in the list?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport random\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\n\n\n@task\ndef generate_random_list(size: int):\n    \"\"\"Generate a random list of integers.\"\"\"\n    return [random.randint(1, 100) for _ in range(size)]\n\n\n@task\ndef check_list_length(list_length, new_value):\n    \"\"\"Check if the list length has been modified.\"\"\"\n    if len(list_length) != len(new_value):\n        raise ValueError(\"List length has not been modified.\")\n\n\n@task\ndef modify_list(list_length, new_value):\n    \"\"\"Modify the list by adding the new value.\"\"\"\n    list_length[:] = new_value\n\n\nwith DAG(\n    \"example_list_comprehension\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n):\n    # [START howto_operator_list_comprehension]\n    random_values = generate_random_list(3)\n    new_random_value = random.randint(1, 100)\n\n    check_list_length(random_values, new_random_value)\n    modify_list(random_values, [new_random_value])\n    # [END howto_operator_list_comprehension]\n\n    chain(random_values, check_list_length, modify_list)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transformation pipeline that demonstrates dynamic task mapping by taking a list of numbers, incrementing each number, and then calculating their total sum. Create a workflow that shows how Airflow can dynamically generate tasks based on input data.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task\ndef increment(value: int) -> int:\n    return value + 1\n\n\n@task\ndef sum_it(values: list[int]) -> int:\n    return sum(values)\n\n\nwith DAG(\n    \"example_dynamic_task_id\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    # [START howto_operator_dynamic_task_id]\n    increment(task_id=\"dynamic_task_id\")\n    # [END howto_operator_dynamic_task_id]\n\n    # [START howto_operator_dynamic_task_id_with_return_value]\n    sum_it(task_id=\"dynamic_task_id_with_return_value\", return_values=True)\n    # [END howto_operator_dynamic_task_id_with_return_value]"}], "metadata": {"file_name": "dag_1849_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that ensures only the most recent workflow run is processed, preventing redundant task executions across multiple scheduling intervals. Create a workflow that demonstrates how to skip unnecessary task runs when a newer workflow instance is already in progress.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.providers.standard.operators.trigger_dag import TriggerDagOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_skip_if_last\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=periodic_task,\n    catchup=False,\n    tags=[\"example\"],\n):\n    run_now = EmptyOperator(task_id=\"run_now\")\n    skip_it = EmptyOperator(task_id=\"skip_it\")\n\n    chain(\n        skip_it,\n        TriggerDagOperator(\n            task_id=\"my_task\",\n            trigger_dag_id=\"example_dag_to_skip\",\n            timeout=30,\n            wait_for_completion=True,\n        ),\n        XComArg(skip_it, key=\"return_value\", task_id=\"return_value\"),\n    )\n\n    @task\n    def print_return_value(value):\n        print(value)\n\n    chain(run_now, print_return_value(\"me\"))\n\n    def check_periodic_dag_running():\n        return any((task_instance.task_id == \"skip_it\" and task_instance.state == \"success\")\n                   for task_instance in TaskInstance.databases)\n\n    @task\n    def stop_working():\n        conn = sqlite3.connect(os.path.join(os.path.dirname(__file__), \"test_hook.db\"))\n        c = conn.cursor()\n        c.execute(\"SELECT value FROM my_table\")\n        for value in c:\n            print(value)\n        conn.close()\n        os._exit(0)\n\n    PeriodicTask = job.TimerTask()\n    stop_working()\n    PeriodicTask(\n        task_id=\"periodic_task\",\n        interval=timedelta(seconds=2),\n        to_run=check_periodic_dag_running(),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_335_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data pipeline that transfers cocktail recipe data from an S3 bucket to a Redshift database, demonstrating end-to-end cloud data migration. Create a workflow that provisions cloud resources, loads sample data, and establishes necessary connections for data transfer.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.s3.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\nDAG_ID = \"example_s3_to_sql\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_PROJECT_ID\")\n\nS3_KEY = \"S3 bucket file key test value\"\nS3_KEY_PREFIX = \"S3 bucket file key prefix test value\"\nS3_DESTINATION_KEY = \"S3 bucket file key destination test value\"\nS3_DESTINATION_KEY_PREFIX = \"S3 bucket file key destination test value\"\nS3_DESTINATIONville_NAME = f\"{DAG_ID}_{ENV_ID}\"\nS3_CONN_ID = f\"s3_conn_{ENV_ID}\"\nSQL_TABLE = \"cocktails\"\nSQL_CREATE = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEY_PREFIX} S3Key)\"\nSQL_INSERT = f\"INSERT INTO {SQL_TABLE} ({S3_KEY_PREFIX}) VALUES ({S3_KEY})\"\nSQL_LOAD = f\"LOAD DATA {S3_KEY_PREFIX} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_SELECT = \"SELECT * FROM {}\".format(SQL_TABLE)\nSQL_DROP = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_ALL = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEY_PREFIX} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_KEY_PREFIX}) VALUES ({S3_KEY})\"\nSQL_LOAD_ALL = f\"LOAD DATA {S3_KEY_PREFIX} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_TMP = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATION_key_prefix} S3Key)\"\nSQL_INSERT_TMP = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATION_key_prefix}) VALUES ({S3_DESTINATION_key})\"\nSQL_DROP_TMP = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_TMP_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATION_key_prefix}) VALUES ({S3_DESTINATION_key})\"\nSQL_LOAD_TMP = f\"LOAD DATA {S3_DESTINATION_key_prefix} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_TMP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATEville = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATIONville_key_prefix} S3Key)\"\nSQL_INSERTVILLE = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATIONville_key_prefix}) VALUES ({S3_DESTINATIONville_key})\"\nSQL_DROPVILLE = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERTVILLE_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATIONville_key_prefix}) VALUES ({S3_DESTINATIONville_key})\"\nSQL_LOADVILLE = f\"LOAD DATA {S3_DESTINATIONville_key_prefix} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROPVILLE_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_all = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATION_key_prefix} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATION_key_prefix}) VALUES ({S3_DESTINATION_key})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_templ = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATION_key_prefix} S3Key)\"\nSQL_INSERT_TEMPL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATION_key_prefix}) VALUES ({S3_DESTINATION_key})\"\nSQL_DROP_TEMPL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_TEMPL_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATION_key_prefix}) VALUES ({S3_DESTINATION_key})\"\nSQL_LOAD_TEMPL = f\"LOAD DATA {S3_DESTINATION_key_prefix} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_TEMPL_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_ville = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATIONville_key_prefix} S3Key)\"\nSQL_INSERT_VILLE = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATIONville_key_prefix}) VALUES ({S3_DESTINATIONville_key})\"\nSQL_DROP_VILLE = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_VILLE_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATIONville_key_prefix}) VALUES ({S3_DESTINATIONville_key})\"\nSQL_LOAD_VILLE = f\"LOAD DATA {S3_DESTINATIONville_key_prefix} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_VILLE_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_allville = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_DESTINATIONville_key_prefix} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_DESTINATIONville_key_prefix}) VALUES ({S3_DESTINATIONville_key})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_select = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEY} S3Key)\"\nSQL_INSERT_SELECT = f\"INSERT INTO {SQL_TABLE} ({S3_KEY}) VALUES ({S3_KEY})\"\nSQL_DROP_SELECT = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_all_select = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEY} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_KEY}) VALUES ({S3_KEY})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_tmp = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEY_prefix} S3Key)\"\nSQL_INSERT_TMP = f\"INSERT INTO {SQL_TABLE} ({S3_KEY_prefix}) VALUES ({S3_KEY_prefix})\"\nSQL_DROP_TMP = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_TMP_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_KEY_prefix}) VALUES ({S3_KEY_prefix})\"\nSQL_LOAD_TMP = f\"LOAD DATA {S3_KEY_prefix} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_TMP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_ville_select = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEYville} S3Key)\"\nSQL_INSERT_VILLE_SELECT = f\"INSERT INTO {SQL_TABLE} ({S3_KEYville}) VALUES ({S3_KEYville})\"\nSQL_DROP_VILLE_SELECT = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_VILLE_SELECT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_KEYville}) VALUES ({S3_KEYville})\"\nSQL_LOAD_VILLE_SELECT = f\"LOAD DATA {S3_KEYville} INTO TABLE {SQL_TABLE} FROM S3\"\nSQL_DROP_VILLE_SELECT_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_all_selectville = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_KEYville} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_KEYville}) VALUES ({S3_KEYville})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_select_all = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_key} S3Key)\"\nSQL_INSERT_SELECT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_DROP_SELECT_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_all = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_key} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_select_templ = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_key} S3Key)\"\nSQL_INSERT_SELECT_TEMPL = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_DROP_SELECT_TEMPL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_all_select_templ = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_key} S3Key)\"\nSQL_INSERT_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_DROP_ALL = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_CREATE_select_ville = f\"CREATE TABLE {SQL_TABLE} (cocktail_id IDENTITY, {S3_key} S3Key)\"\nSQL_INSERT_VILLE = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_DROP_VILLE = f\"DROP TABLE IF EXISTS {SQL_TABLE}\"\nSQL_INSERT_VILLE_ALL = f\"INSERT INTO {SQL_TABLE} ({S3_key}) VALUES ({S3_key})\"\nSQL_LOAD_VILLE = f"}], "metadata": {"file_name": "dag_755_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates the technical orchestration of data cube operations using the KylinCubeOperator, demonstrating how complex data processing tasks can be chained together with precise time-based segmentation and automated management. The DAG will highlight task dependencies, XCom data passing, and systematic execution of build, refresh, merge, disable, and purge commands.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.kylin import (\n    KylinBuildCubeOperator,\n    KylinDisableCubeOperator,\n    KylinEnableCubeOperator,\n    KylinMergeCubeOperator,\n    KylinPushCubeOperator,\n    KylinRunCubeOperator,\n    KylinTableScanOperator,\n    KylinCubeOperator,\n)\nfrom airflow.providers.alibaba.cloud.sensors.kylin import KylinSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kylin_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"kylin\"],\n) as dag:\n    # [START howto_operator_kylin_run_cube]\n    run_cube = KylinRunCubeOperator(task_id=\"run_cube\")\n    # [END howto_operator_kylin_run_cube]\n\n    # [START howto_operator_kylin_build_cube]\n    build_cube = KylinBuildCubeOperator(\n        task_id=\"build_cube\",\n        cube_id=\"cube_test\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kylin_build_cube]\n\n    # [START howto_operator_kylin_disable_cube]\n    disable_cube = KylinDisableCubeOperator(\n        task_id=\"disable_cube\",\n        cube_id=\"cube_test\",\n    )\n    # [END howto_operator_kylin_disable_cube]\n\n    # [START howto_operator_kylin_merge_cube]\n    merge_cube = KylinMergeCubeOperator(\n        task_id=\"merge_cube\",\n        merge_info={\n            \"cube_id\": \"cube_test_merge\",\n            \"sources\": [\"cube_test\"],\n            \"strategy\": \"UNION\",\n            \"merge_date\": datetime(2021, 3, 14),\n        },\n    )\n    # [END howto_operator_kylin_merge_cube]\n\n    # [START howto_operator_kylin_push_cube]\n    push_cube = KylinPushCubeOperator(\n        task_id=\"push_cube\",\n        push_info={\n            \"cube_id\": \"cube_test_push\",\n            \"file_type\": \"ANY\",\n            \"file_info\": {\"file_set\": \"DIR\", \"dir_prefix\": \"/kylin/bak\"},\n        },\n    )\n    # [END howto_operator_kylin_push_cube]\n\n    # [START howto_operator_kylin_table_scan]\n    table_scan = KylinTableScanOperator(\n        task_id=\"table_scan\", cube_id=\"cube_test_push\", trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_kylin_table_scan]\n\n    # [START howto_operator_kylin_cube]\n    disable_cube >> build_cube >> merge_cube >> push_cube >> table_scan\n    # [END howto_operator_kylin_cube]\n\n    run_cube >> disable_cube >> build_cube >> merge_cube >> push_cube >> table_scan\n\n    # [START howto_sensor_kylin]\n    sensor = KylinSensor(task_id=\"kylin_sensor\", cube_id=\"cube_test\", timeout=600)\n    # [END howto_sensor_kylin]\n\n    sensor >> push_cube\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as skipped\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1143_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with proper error handling and sequential task execution.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sqs import SqsCreateQueueOperator, SqsDeleteQueueOperator\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.providers.amazon.aws.sensors.sqs import SqsMessageSensor, SqsMessageSensorAsync\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.sdk import chain\n\n# [START howto_operator_cloudformation_create_stack]\ndef _create_stack(stack_name, stack_template, region, role_arn):\n    # Create a CloudFormationConnection object with the specified region.\n    connection = AirflowAwsConnection(role_arn=role_arn, region_name=region)\n\n    # Create a stack using the CloudFormationCreateStackOperator.\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=stack_name,\n        stack_template=stack_template,\n        connection=connection,\n        on_success=lambda **kwargs: print(\"Create Stack Success!\"),\n        on_failure=lambda task_id, **kwargs: print(\"Create Stack Failure!\", task_id),\n    )\n\n\ndef _delete_stack(stack_name, region, role_arn):\n    # Create a CloudFormationConnection object with the specified region.\n    connection = AirflowAwsConnection(role_arn=role_arn, region_name=region)\n\n    # Delete a stack using the CloudFormationDeleteStackOperator.\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=stack_name,\n        connection=connection,\n        on_success=lambda **kwargs: print(\"Delete Stack Success!\"),\n        on_failure=lambda task_id, **kwargs: print(\"Delete Stack Failure!\", task_id),\n    )\n\n\ndef _create_sqs_queue(queue_name, region, role_arn):\n    # Create a SqsCreateQueueOperator to create an SQS queue.\n    create_sqs_queue = SqsCreateQueueOperator(\n        task_id=\"create_sqs_queue\",\n        queue_name=queue_name,\n        connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        on_success=lambda **kwargs: print(\"Create SQS Queue Success!\"),\n        on_failure=lambda task_id, **kwargs: print(\"Create SQS Queue Failure!\", task_id),\n    )\n\n\ndef _get_sqs_message(queue_name, region, role_arn):\n    # Create a SqsMessageSensorAsync to wait for messages on the SQS queue.\n    sqs_message_sensor = SqsMessageSensorAsync(\n        task_id=\"get_sqs_message\",\n        queue_name=queue_name,\n        connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        max_messages=1,\n        wait_time=5,\n        trigger_rule=TriggerRule.ALL_DONE,\n        # If you find it practical, you can also use\n        # SqsMessageSensor to consume messages from SQS queue.\n        # sqs_message_sensor = SqsMessageSensor(\n        #     task_id=\"get_sqs_message\",\n        #     queue_name=queue_name,\n        #     connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        #     max_messages=1,\n        #     wait_time=5,\n        #     # trigger_rule=TriggerRule.ALL_DONE,\n        #     task_group=TaskGroup(task_id=\"sqs_group\"),\n        # ),\n    )\n\n\ndef _delete_sqs_queue(queue_name, region, role_arn):\n    # Create a SqsDeleteQueueOperator to delete the SQS queue.\n    delete_sqs_queue = SqsDeleteQueueOperator(\n        task_id=\"delete_sqs_queue\",\n        queue_name=queue_name,\n        connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\ndef _create_tg(sqs_queue, stack_name, region, role_arn):\n    # Create a TaskGroup to coordinate tasks within the DAG.\n    tg = TaskGroup(task_id=\"sample_task_group\", task_groups=[tg])\n    # Create a SqsMessageSensorAsync to consume messages from the SQS queue.\n    _get_sqs_message(sqs_queue, region, role_arn)\n    # Delete the stack after the task group is triggered.\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_sample_stack\",\n        stack_name=stack_name,\n        connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # Link the task group with the stack deletion task.\n    (tg >> delete_stack)\n\n\ndef _delete_sample_stack(stack_name, region, role_arn):\n    # Create a CloudFormationDeleteStackOperator to delete the stack.\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=stack_name,\n        connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\nwith DAG(\n    dag_id=\"example_cloudformation\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n    # Some constants to help set up the DAG\n    lambda_role_name = \"lambda-sample-role\"\n    stack_name = \"lambda-sample-stack\"\n    region = \"us-east-1\"\n    sqs_queue_name = \"lambda-sqs-queue\"\n\n    # Create an AWS Connection object.\n    connection = AirflowAwsConnection(role_arn=\"\", region_name=region)\n\n    # Create an Amazon CloudFormation Connection object.\n    cloudformation_connection = CloudFormationConnection(region_name=region)\n\n    # Create an SQS queue and wait for a message.\n    # [START howto_sensor_sqs_message]\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def wait_for_message(sqs_queue_name, region, role_arn):\n        # Create a SqsMessageSensorAsync to wait for messages on the SQS queue.\n        sqs_message_sensor = SqsMessageSensorAsync(\n            task_id=\"wait_for_message\",\n            queue_name=sqs_queue_name,\n            connection=AirflowAwsConnection(role_arn=role_arn, region_name=region),\n            max_messages=1,\n            wait_time=5,\n        )\n        # Wait for a message to be available.\n        sqs_message_sensor.wait_for_completion()\n        # Return True once a message is detected.\n        return True\n\n\n    # [END howto_sensor_sqs_message]\n\n    # Create a CloudFormation stack and wait for creation to complete.\n    # [START howto_operator_cloudformation_create_stack]\n    @task(task_id=\"create_stack\")\n    def create_stack_from_task():\n        _create_stack(stack_name, stack_template, region, role_arn)\n\n    # [END howto_operator_cloudformation_create_stack]\n\n    # Create a CloudFormation stack and wait for creation to complete.\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=stack_name,\n        stack_template={\"TemplateBody\": stack_template},\n        connection=cloudformation_connection,\n        on_success=lambda **kwargs: print(\"Create Stack Success!\"),\n        on_failure=lambda task_id, **kwargs: print(\"Create Stack Failure!\", task_id),\n    )\n\n    # Establish a dependency on the task above.\n    wait_for_message(task_id=\"wait_for_message\", sqs_queue_name=sqs_queue_name, region=region, role_arn=role_arn)\n\n    # Delete the CloudFormation stack.\n    # [START howto_operator_cloudformation_delete_stack]\n    @task(task_id=\"delete_stack\")\n    def delete_stack_from_task():\n        _delete_stack(stack_name, region, role_arn)\n\n    # [END howto_operator_cloudformation_delete_stack]\n\n    # Delete the CloudFormation stack.\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=stack_name,\n        connection=cloudformation_connection,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # Establish a dependency on the task above.\n    create_stack >> wait_for_message >> delete_stack\n\n    # Establish a dependency on the task group above.\n    create_tg(sqs_queue_name, stack_name, region, role_arn) >> delete_stack\n\n    # Establish a dependency on the sensor above.\n    create_stack >> wait_for_message >> _create_tg(sqs_queue_name, stack_name, region, role_arn)\n\n    # Establish a dependency on the sensor above.\n    create_stack >> wait_for_message >> _delete_sqs_queue(sqs_queue_name, region, role_arn)\n\n    # Establish a dependency on the sensor above.\n    create_stack >> wait_for_message >> _create_tg(sqs_queue_name, stack_name, region, role_arn)\n\n    # Establish a dependency on the sensor above.\n    create_stack >> wait_for_message >> _delete_stack(stack_name, region, role_arn)\n\n    # Setup\n    role_arn = Param.from_string(\"role_arn\", desc=\"Role ARN of the SQS role.\")\n    stack_name = Param.from_string(\"stack_name\", desc=\"Stack name.\")\n    region = Param.from_string(\"region\", desc=\"Region where the stack resides.\")\n    sqs_queue_name = Param.from_string(\"sqs_queue_name\", desc=\"SQS queue name.\")\n    stack_template = {\n        \"Resources\": {\n            \"SampleFunction\": {\n                \"Type\": \"AWS"}], "metadata": {"file_name": "dag_1878_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that explores different task execution strategies by implementing parallel pipeline branches with configurable success conditions. Construct a DAG that demonstrates how tasks can be conditionally executed or skipped based on predefined trigger rules.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START dag_def]\nwith DAG(\n    dag_id=\"example_branching\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = EmptyOperator(task_id=\"run_this\")\n    run_this2 = EmptyOperator(task_id=\"run_this2\")\n\n    @task.branch()\n    def branch_task():\n        print(\"Hello\")\n        return [\"task1\", \"task2\"]\n\n    t1 = branch_task()\n    t2 = EmptyOperator(task_id=\"task1\")\n    t3 = EmptyOperator(task_id=\"task2\")\n    t4 = EmptyOperator(task_id=\"task3\")\n    t5 = EmptyOperator(task_id=\"task4\")\n\n    [t1, t2, t3, t4, t5] >> run_this\n# [END dag_def]"}], "metadata": {"file_name": "dag_689_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I'm looking for a Python function that can send an email with an attachment. Can you provide a sample code?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.email import EmailOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_smtp\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_send_email]\n    @task\n    def send_email(\n        to: str,\n        subject: str,\n        content: str,\n        html: str = \"\",\n        connection_id: str = \"smtp_default\",\n        use_ssl: bool = True,\n        port: int = 587,\n    ):\n        email = EmailOperator(\n            task_id=\"send_email\",\n            to=to,\n            subject=subject,\n            content=content,\n            html=html,\n            connection_id=connection_id,\n            use_ssl=use_ssl,\n            port=port,\n        )\n\n    # [END howto_operator_send_email]\n    send_email(\"test@airflow.com\", \"Test Subject\", \"Test Content\", html='Test</br>Html')\n\n    BashOperator(\n        task_id=\"enable_smtp\",\n        bash_command='if [[ $(dpkg-architecture -qDEB_HOST_MULTIARCH) != \"arm64\" ]]; then '\n        + 'sudo dpkg --add-|arch|deb|source||package|manager||install| -t `cat /etc/os-release | grep ^ID=|cut -d= -f2`; '\n        + \"fi\",\n    )\n\n    connection = Connection(\n        conn_id=\"smtp_default\",\n        conn_type=\"smtp\",\n        host=\"mail.example.com\",\n        port=25,\n        login=\"username\",\n        password=\"password\",\n        extra={\"validate\": \"yes\"},\n    )\n\n    BashOperator(\n        task_id=\"enable_mailhog_sensor\",\n        bash_command=f\"sudo apt install mailx -y; \"\n        + f\"sudo mailx -s 'Testing with mailx!' test@airflow.com\",\n    )\n\n    @task\n    def create_mailhog_sensor_output(sensors_output: list):\n        #\u7231\u5fc5\u6cf0\u601d\n        for sensor_output in sensors_output:\n            print(sensor_output[1])\n\n    create_mailhog_sensor_output(sensors_output=[{'Task': 'enable_mailhog_sensor', 'Output': '{{ ts + timedelta(seconds=10) }}'}])\n\n    [send_email.to, connection].move_together()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data processing workflow with multiple interconnected tasks, including cloud resource management, data import, transformation, and export. Implement a systematic approach to handling data pipeline tasks with robust error handling and connection management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mysterio\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"mysterio\"},\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_role():\n        Role.objects.create(role_id=1, name=\"mysterio_role\")\n\n    @task\n    def create_task_group():\n        TaskGroup.objects.create(task_group_id=1, role_id=1, task_ids=[\"t1\", \"t2\"])\n\n    @task\n    def create_table():\n        sql = \"CREATE TABLE mysterio_table (col_1 INTEGER, col_2 VARCHAR(10))\"\n        SQLExecuteQueryOperator(task_id=\"t1\", sql=sql)\n\n    @task\n    def insert_data():\n        sql = \"INSERT INTO mysterio_table VALUES (1, 'one'), (2, 'two')\"\n        SQLExecuteQueryOperator(task_id=\"t1\", sql=sql)\n\n    @task\n    def execute_python():\n        os.environ[\"MYSTERIO_CONFIG\"] = '{\"conn_id': 'mysterio'}'\n        print(\"Hello World!\")\n\n    @task\n    def execute_python_file():\n        BashOperator(task_id=\"python_file\", bash_command=\"python3.6 example_mysterio.py\")\n\n    @task\n    def drop_table():\n        sql = \"DROP TABLE mysterio_table\"\n        SQLExecuteQueryOperator(task_id=\"t2\", sql=sql)\n\n    @task\n    def drop_task_group():\n        TaskGroup.objects.get(task_group_id=1).delete()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_conn():\n        conn = Connection(\n            conn_id=\"mysterio\",\n            conn_type=\"mysterio\",\n        )\n        conn.delete()\n\n    create_role >> create_task_group >> create_table >> insert_data >> drop_table >> drop_task_group\n    execute_python >> execute_python_file >> delete_conn"}], "metadata": {"file_name": "dag_1225_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a MySQL database cleanup workflow that demonstrates dropping tables using Airflow's SQL operators. Create a DAG that shows how to remove database tables programmatically and sequentially.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.mysql.operators.mysql import MySQLOperator\n\n# [START create_table]\ndef create_table():\n    print(\"Creating table employee...\")\n\n    sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS employee (\n        id INT NOT NULL AUTO_INCREMENT,\n        name VARCHAR(50) NOT NULL,\n        gender ENUM('M', 'F') NOT NULL,\n        salary TINYINT NOT NULL,\n        PRIMARY KEY (id)\n    );\n    \"\"\"\n\n    print(sql)\n    return sql\n# [END create_table]\n\n# [START execute_table]\ndef execute_table():\n    print(\"Executing create table employee...\")\n\n    sql = \"SELECT * FROM employee;\"\n    print(sql)\n    return sql\n# [END execute_table]\n\n# [START drop_table]\ndef drop_table():\n    print(\"Cleaning up table employee...\")\n\n    sql = \"DROP TABLE employee;\"\n    print(sql)\n    return sql\n# [END drop_table]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mysql\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"mysql_default\"},\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    create_table_task = MySQLOperator(task_id=\"create_table_task\", sql=create_table(), dag=dag)\n    execute_task = MySQLOperator(task_id=\"execute_task\", sql=execute_table(), dag=dag)\n    drop_table_task = MySQLOperator(task_id=\"drop_table_task\", sql=drop_table(), dag=dag)\n    create_table_task >> execute_task >> drop_table_task"}], "metadata": {"file_name": "dag_1368_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that captures and transforms data between two sequential assets, demonstrating how Airflow can manage data dependencies and trigger downstream processes. Create a pipeline that shows how one data asset can trigger and inform the processing of a subsequent asset.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"generate\")\ndef generate(n):\n    return [f\"element_{i}\" for i in range(n)]\n\n\n@task(task_id=\"count_down\", task_kwargs={\"nargs\": \"l\", \"task_id\": \"print_elements\"})\ndef count_down(elements):\n    for i in elements:\n        print(i)\n        yield from count_down\n\n\nwith DAG(\n    \"example_countdown_decorator\",\n    schedule=\"0-59/2 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    generate(10) >> count_down()"}], "metadata": {"file_name": "dag_1434_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete machine learning lifecycle for image classification, showcasing how to programmatically manage cloud machine learning infrastructure and model deployment processes. The DAG should illustrate task dependencies and cloud service interactions for an end-to-end ML workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImageSafeSearchOperator,\n    CloudVisionExtrapolateImageFeaturesOperator,\n    CloudVisionExtractTextOperator,\n    CloudVisionImageAnnotateOperator,\n    CloudVisionImageDetectOperator,\n    CloudVisionImageProductSetCreateOperator,\n    CloudVisionImageProductSetDeleteOperator,\n    CloudVisionImageProductCreateOperator,\n    CloudVisionImageProductDeleteOperator,\n    CloudVisionImageSetProductAssociationsCreateOperator,\n    CloudVisionImageSetProductAssociationsDeleteOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"vision_annotate_image\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nIMAGE_NAME = f\"image_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nVISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/vision/image_{ENV_ID}.jpg\"\n\n# [START howto_operator_vision_annotate_image_result]\ndef annotate_image_result(output_path):\n    with open(output_path) as file:\n        content = file.read()\n        print(content)\n        print(content[::-1])\n        print(file.read())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())\n        print(file.readline())"}], "metadata": {"file_name": "dag_1317_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data transformation workflow that triggers dbt Cloud jobs for data processing and monitoring their execution status. Design a DAG that demonstrates different methods of running and tracking dbt Cloud job runs with synchronous and asynchronous sensors.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.dbt.cloud.operators.dbt import (\n    DbtCloudListJobsOperator,\n    DbtCloudRunJobOperator,\n    DbtCloudRunJobSyncOperator,\n)\nfrom airflow.providers.dbt.cloud.sensors.dbt import DbtCloudJobRunStateSensor, DbtCloudListJobsSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_dbt_cloud\"\n\nconn = Connection(\n    conn_id=\"dbt_id\",\n    conn_type=\"dbt\",\n    host=\"https://dbt.cloud\",\n    extra=\"\"\"\n        {\n            \"api_key\": \"d<PASSWORD>88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888"}], "metadata": {"file_name": "dag_792_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing orchestration DAG that provisions a multi-node Dataproc cluster with preemptible workers and custom initialization scripts for data analysis tasks. Create a reproducible cloud data processing environment that automatically manages computational resources and supports flexible metadata and package installations.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.providers.singularity.utils import get_singularity_image_uri\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.triggerer import trigger_dag\nfrom airflow.sdk import chain\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_connection(conn_id_name):\n        connection = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"singularity\",\n        )\n        connection.set_extra(\n            {\n                \"image\": get_singularity_image_uri(),\n                \"command\": \"sleep 10\",\n                \"env\": \"PATH=/usr/bin\",\n                \"volumes\": \"/tmp/singularity-volume-1\",\n            }\n        )\n        return connection\n\n    trigger_task = trigger_dag(\n        task_id=\"trigger_task\",\n        trigger_dag_id=\"singularity_with_env_var\",\n    )\n\n    chain(\n        create_connection(conn_id_name=\"singularity_for_forcing_with_env_var\"),\n        BashOperator(\n            task_id=\"print_it\",\n            bash_command=\"echo {{ task_instance.xcom_pull('singularity_for_forcing_with_env_var') }}\",\n            on_finish=lambda info: input(\"Press Enter to delete the connection 'singularity_for_forcing_with_env_var'.\") == \"Enter\",\n        ),\n        trigger_task,\n    )"}], "metadata": {"file_name": "dag_2031_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates cloud storage data movement and management techniques using Alibaba OSS operators. Create a workflow that captures the essential steps of file lifecycle management: transferring data between local and cloud storage, and executing precise object deletion strategies.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud_oss.operators.oss import (\n    AlibabaCloudOSSCreateBucketOperator,\n    AlibabaCloudOSSDeleteBucketOperator,\n    AlibabaCloudOSSDeleteObjectOperator,\n    AlibabaCloudOSSGetObjectOperator,\n    AlibabaCloudOSSPutObjectOperator,\n)\nfrom airflow.providers.alibaba.cloud_oss.sensors.oss import AlibabaCloudOSSObjectExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_operator_dag\"\n\nOBJECT_KEY = os.environ.get(\"SYSTEM_TESTS_OSS_OBJECT_KEY\", \"key/test.txt\")\nBUCKET_NAME = os.environ.get(\"SYSTEM_TESTS_OSS_BUCKET_KEY\", \"airflow-test-oss\")\n\nS3_FILE_PATH = os.environ.get(\"SYSTEM_TESTS_S3_FILE_PATH\", \"s3://airflow-test-oss/test/test.txt\")\n\nconn = Connection(\n    conn_id=\"oss-id\",\n    conn_type=\"oss\",\n    description=\"oss-id\",\n    extra={\n        \"access_key_id\": os.environ.get(\"SYSTEM_TESTS_OSS_ACCESS_KEY_ID\"),\n        \"access_key_secret\": os.environ.get(\"SYSTEM_TESTS_OSS_ACCESS_KEY_SECRET\"),\n        \"bucket\": BUCKET_NAME,\n        \"region_id\": os.environ.get(\"SYSTEM_TESTS_OSS_REGION_ID\"),\n    },\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2023, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection():\n        conn.id = \"oss-id\"\n        conn.save()\n\n    @task\n    def create_bucket():\n        AlibabaCloudOSSCreateBucketOperator(\n            task_id=\"create_bucket\", bucket_name=BUCKET_NAME, region_id=os.environ.get(\"SYSTEM_TESTS_OSS_REGION_ID\")\n        )\n\n    @task\n    def put_object():\n        AlibabaCloudOSSPutObjectOperator(\n            task_id=\"put_object\",\n            bucket=BUCKET_NAME,\n            object_key=OBJECT_KEY,\n            file=S3_FILE_PATH,\n            replace=True,\n        )\n\n    @task\n    def delete_object():\n        AlibabaCloudOSSDeleteObjectOperator(\n            task_id=\"delete_object\",\n            bucket=BUCKET_NAME,\n            object_key=OBJECT_KEY,\n        )\n\n    @task\n    def delete_bucket():\n        AlibabaCloudOSSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def wait_object_exist():\n        AlibabaCloudOSSObjectExistenceSensor(\n            task_id=\"wait_object_exist\",\n            bucket=BUCKET_NAME,\n            object_key=OBJECT_KEY,\n            timeout=60,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    create_bucket()\n    put_object()\n    wait_object_exist()\n    delete_object()\n    delete_bucket()\n    create_connection()"}], "metadata": {"file_name": "dag_55_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Could you provide a detailed explanation of how the `input()` function works in Python, including its syntax and typical use cases, and perhaps some examples?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAGContext\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_input_output():\n    @task\n    def get_input() -> str:\n        return \"Hello World\"\n\n    @task\n    def print_input_to_log(message: DAGContext[\"input\"]) -> None:\n        print(message.input)\n\n    t1 = get_input()\n    t2 = print_input_to_log(t1.output)\n    chain(t1, t2)\n\n\nexample_input_output()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Google Cloud Compute Engine template management workflow that demonstrates how to create, copy, and update instance templates for a scalable infrastructure deployment. The workflow should showcase best practices for managing compute resources programmatically using Airflow.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import TaskGroup\nfrom airflow.models.dag import DAG\n\ntry:\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineCopyInstanceOperator,\n        ComputeEngineDeleteInstanceOperator,\n        ComputeEngineDeleteInstanceTemplateOperator,\n        ComputeEngineInsertInstanceTemplateOperator,\n        ComputeEngineInsertInstanceTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        ComputeEngineInsertInstanceTemplateTemplateTemplateOperator,\n        Compute"}], "metadata": {"file_name": "dag_2008_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that illustrates the basic mechanics of Azure Blob Storage interaction using Airflow operators, focusing on the sequential upload and deletion of a file in a cloud container.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.blob import AzureBlobCreateBlockOperator, AzureBlobDeleteOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_blob import LocalFilesystemToBlobOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_blob\"\n\nconn = Connection(\n    conn_id=\"my_connection\",\n    conn_type=\"azure_container\",\n    extra={\"server\": \"my-containers-server\", \"namespace\": \"my-containers-namespace\"},\n)\n\nFILE_NAME = \"file.txt\"\nDESTINATION_PATH = \"dir/file2.txt\"\nAZURE_CONTAINER_NAME = f\"container_{DAG_ID}_{ENV_ID}\"\nAZURE_FILE_PATH = f\"files/{FILE_NAME}\"\nAZURE_BLOB_NAME = f\"blob_{DAG_ID}_{ENV_ID}\"\nAZURE_BLOB_PATH = f\"folder/{FILE_NAME}\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"transfer\"],\n) as dag:\n    @task\n    def create_container():\n        \"\"\"\n        Create an Azure Container Blob Storage container.\n        \"\"\"\n        from airflow.providers.microsoft.azure.transfers.local_to_blob import Transfer\n        from airflow.utils.trigger_rule import TriggerRule\n\n        t = Transfer(\n            task_id=\"azure_container_to_blob_transfer\",\n            source=(\n                f\"azure_container://{AZURE_CONTAINER_NAME}\",\n                f\"files/{AZURE_FILE_PATH}\",\n            ),\n            destination=f\"azure_container://{AZURE_CONTAINER_NAME}/{AZURE_BLOB_NAME}\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n        t.execute(conn_id=conn)\n\n    create_container_block = AzureBlobCreateBlockOperator(\n        task_id=\"azure_container_to_blob_block_operator\",\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=AZURE_BLOB_NAME,\n        body=f\"{{}}\",\n    )\n\n    create_container()\n    upload_file = LocalFilesystemToBlobOperator(\n        task_id=\"upload_file_to_container\",\n        filename=FILE_NAME,\n        dest_blob=AZURE_FILE_PATH,\n        container_name=AZURE_CONTAINER_NAME,\n        blob_path=AZURE_FILE_PATH,\n    )\n    delete_file = AzureBlobDeleteOperator(\n        task_id=\"delete_file_from_blob\",\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=AZURE_BLOB_NAME,\n    )\n    delete_container = AzureBlobDeleteOperator(\n        task_id=\"delete_file_from_container\",\n        container_name=AZURE_CONTAINER_NAME,\n    )\n    [create_container_block, upload_file], [delete_container, delete_file] >> [create_container()]\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed or as failed)\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2159_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a modular Airflow pipeline that uses task decorators to execute two distinct Pinot interaction tasks with different connection methods. Implement a DAG that demonstrates parallel task execution using Pinot admin and database API hooks.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinot.operators.pinot_admin import PinotAdminAsyncOperator\nfrom airflow.providers.pinot.operators.pinot_db import PinotDbAsyncOperator\nfrom airflow.providers.pinot.sensors.pinot_admin import PinotAdminSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_pinot_db_async\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"pinot_admin_conn\"},\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"pinot\"],\n) as dag:\n    # [START howto_operator_pinot_admin_async]\n    populate_data = PinotAdminAsyncOperator(\n        task_id=\"populate_data\",\n        sql=\"INSERT INTO pinschema (num) VALUES (10), (11), (12);\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_pinot_admin_async]\n\n    # [START howto_operator_pinot_db_async]\n    insert_data = PinotDbAsyncOperator(\n        task_id=\"insert_data\",\n        sql=\"INSERT INTO pinschema (num) VALUES (12), (13), (14);\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_pinot_db_async]\n\n    # [START howto_sensor_pinot_admin_async]\n    wait_for_drop = PinotAdminSensor(\n        task_id=\"wait_for_drop\",\n        sql=\"DROP TABLE pinschema;\",\n        timeout=5,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_sensor_pinot_admin_async]\n\n    populate_data >> insert_data >> wait_for_drop"}], "metadata": {"file_name": "dag_112_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated Kubernetes cluster management workflow that demonstrates creating a GKE cluster, running test pods with different configurations, and then cleaning up the cluster. The workflow should showcase how to dynamically create and delete Kubernetes infrastructure while executing sample workloads.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gke import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n    GKEStartPodOperator,\n    GKEStartWorkflowOperator,\n    GKEStartWorkflowRunOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"gke_with_workflow\"\nGCP_LOCATION = \"europe-north1\"\nCLUSTER_NAME = f\"gke-workflow-{ENV_ID}\"\nWORKFLOW_NAME = \"test-workflow\"\nWORKFLOW_TEMPLATE_NAME = f\"workflow-{ENV_ID}\"\nGCS_BUCKET = f\"bucket_gke_workflow_{ENV_ID}\"\nGCS_FILE_LOCAL_PATH = \"/tmp/test_workflow\"\n\n\n@task\ndef create_connection():\n    connection = Connection(\n        conn_id=\"google_cloud_platform\",\n        user=\"dummy\",\n        password=\"dummy\",\n        description=\"Google Cloud Platform connector\",\n        schema=\"airflow.\",\n    )\n    os.environ[\"SYSTEM_TESTS_CONNECTION_DEFAULT\"] = \"google_cloud_platform\"\n    print(connection)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gke-with-workflow\"],\n) as dag:\n    create_cluster = GKECreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        initial_node_count=1,\n    )\n    # [START howto_operator_gke_start_pod]\n    start_pod = GKEStartPodOperator(\n        task_id=\"start_pod\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        namespace=\"default\",\n        image=\"perl\",\n        name=\"test-pod\",\n        in_cluster=False,\n        in_cluster_config=True,\n        get_logs=True,\n    )\n    # [END howto_operator_gke_start_pod]\n\n    # [START howto_operator_gke_start_workflow]\n    start_workflow = GKEStartWorkflowOperator(\n        task_id=\"start_workflow\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        workflow_name=WORKFLOW_NAME,\n        workflow_template_name=WORKFLOW_TEMPLATE_NAME,\n        name=\"test-workflow\",\n        in_cluster=False,\n        namespace=\"default\",\n        project_job_id=\"google-cloud-platform\",\n    )\n    # [END howto_operator_gke_start_workflow]\n\n    # [START howto_operator_gke_start_workflow_run]\n    start_workflow_run = GKEStartWorkflowRunOperator(\n        task_id=\"start_workflow_run\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        workflow_name=WORKFLOW_NAME,\n        workflow_template_name=WORKFLOW_TEMPLATE_NAME,\n        run_name=\"test-workflow-run\",\n        run_params={\"some_param\": \"value\"},\n        namespace=\"default\",\n        project_job_id=\"google-cloud-platform\",\n    )\n    # [END howto_operator_gke_start_workflow_run]\n\n    # [START howto_operator_gke_start_workflow_run_from_workflow]\n    start_workflow_run_from_workflow = GKEStartWorkflowRunOperator(\n        task_id=\"start_workflow_run_from_workflow\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        workflow_name=WORKFLOW_NAME,\n        workflow_template_name=WORKFLOW_TEMPLATE_NAME,\n        run_name=\"test-workflow-run-from-workflow\",\n        run_params={\"some_param\": \"value\"},\n        in_cluster=False,\n        namespace=\"default\",\n        project_job_id=\"google-cloud-platform\",\n        initial_context={\"qubole_org\": \"my_org\", \"qubole_project\": \"my-project\"},\n    )\n    # [END howto_operator_gke_start_workflow_run_from_workflow]\n\n    # [START howto_operator_gke_create_workflow_from_workflow]\n    create_workflow_from_workflow = GKEStartWorkflowOperator(\n        task_id=\"create_workflow_from_workflow\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        workflow_name=WORKFLOW_NAME,\n        workflow_template_name=WORKFLOW_TEMPLATE_NAME,\n        name=\"test-workflow-from-workflow\",\n        in_cluster=False,\n        namespace=\"default\",\n        project_job_id=\"google-cloud-platform\",\n    )\n    # [END howto_operator_gke_create_workflow_from_workflow]\n\n    # [START howto_operator_gke_delete_cluster]\n    delete_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gke_delete_cluster]\n\n    move_file = GCSToGCSOperator(\n        task_id=\"move_file\",\n        source_bucket=GCS_BUCKET,\n        source_object=\"bucket_gke_workflow_*\",\n        destination_bucket=GCS_BUCKET,\n        destination_object=\"bucket_gke_workflow_temp\",\n    )\n\n    @task\n    def remove_file():\n        hook = GCSHook()\n        hook.delete_bucket(GCS_BUCKET)\n\n    create_cluster >> [start_pod, start_workflow, start_workflow_run] >> delete_cluster\n    (move_file >> remove_file()) >> create_cluster"}], "metadata": {"file_name": "dag_1273_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that creates a schema and multiple tables in Trino for storing city information, demonstrating basic database schema creation and data insertion techniques. The pipeline should showcase different query methods including templated and parameterized queries.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.trino.operators.trino import TrinoExecuteQueryOperator\n\n# [START howto_trino_template]\n# [START howto_trino_for_loop]\nwith DAG(\n    \"trino_example\",\n    default_args={\"conn_id\": \"my_trino_conn\"},\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END howto_trino_for_loop]\n\n    # [START howto_trino_template_language_print_query]\n    t1 = TrinoExecuteQueryOperator(\n        task_id=\"print_trino_query\",\n        sql=\"SELECT 3 AS const, 'hello, world!' AS msg\",\n    )\n    # [END howto_trino_template_language_print_query]\n\n    # [START howto_trino_template_language_format_query]\n    t2 = TrinoExecuteQueryOperator(\n        task_id=\"format_trino_query\",\n        sql=\"SELECT 'yo\" & 'a' AS greet\"\n             \"FROM {{ params.table }}\"\n             \"WHERE <condition>\"\n    )\n    # [END howto_trino_template_language_format_query]\n\n    t1 >> t2\n# [END howto_trino_template]\n# [END howto_trino_for_loop]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"trino_example\"\n\n\n# [START howto_trino_optimizable]\nwith DAG(\n    DAG_ID,\n    default_args={\"conn_id\": \"my_trino_conn\"},\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END howto_trino_optimizable]\n\n    # [START howto_trino_template_language_print_query_optimizable]\n    t1 = TrinoExecuteQueryOperator(\n        task_id=\"print_trino_query\",\n        sql=\"SELECT 3 AS const, 'hello, world!' AS msg\",\n    )\n    # [END howto_trino_template_language_print_query_optimizable]\n\n    # [START howto_trino_template_language_format_query_optimizable]\n    base_params = {\n        \"table\": \"test_table\",\n        \"condition\": \"a > 5\",\n    }\n\n    t2 = TrinoExecuteQueryOperator(\n        task_id=\"format_trino_query\",\n        sql=\"SELECT 'yo' & 'a' AS hello, FROM {{ params.table }} WHERE {{ params.condition }}\",\n        params=base_params,\n    )\n    # [END howto_trino_template_language_format_query_optimizable]\n\n    t1 >> t2\n# [END howto_trino_optimizable]"}], "metadata": {"file_name": "dag_320_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration pipeline that moves files from cloud object storage to a secure file transfer protocol destination with complete lifecycle tracking. Create a workflow that demonstrates secure, reliable data transfer between different cloud storage systems with comprehensive task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.transfers.copy_between_s3_to_s3 import CopyBetweenS3ToS3Operator\nfrom airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\nfrom airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_s32\"\n\ns3_conn = Connection(\n    conn_id=\"s3_default\",\n    conn_type=\"s3\",\n    host=\"s3.amazonaws.com\",\n    schema=\"s3\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_table():\n        from airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\n\n        S3ToS32Operator(\n            task_id=\"create_table\",\n            s3_conn_id=\"s3_default\",\n            bucket=\"tc-s3-b\",\n            dest_s3_key_prefix=\"s3_to_s32/\",\n            sql=\"CREATE TABLE my_table AS SELECT * FROM employees;\",\n        )\n\n    @task\n    def create_view():\n        from airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\n\n        S3ToS32Operator(\n            task_id=\"create_view\",\n            s3_conn_id=\"s3_default\",\n            bucket=\"tc-s3-b\",\n            dest_s3_key_prefix=\"s3_to_s32/\",\n            sql=\"CREATE OR REPLACE VIEW my_view AS SELECT * FROM employees;\",\n        )\n\n    @task\n    def drop_view():\n        from airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\n\n        S3ToS32Operator(\n            task_id=\"drop_view\",\n            s3_conn_id=\"s3_default\",\n            bucket=\"tc-s3-b\",\n            dest_s3_key_prefix=\"s3_to_s32/\",\n            sql=\"DROP VIEW IF EXISTS my_view;\",\n        )\n\n    @task\n    def drop_table():\n        from airflow.providers.s3.transfers.s3_to_s32 import S3ToS32Operator\n\n        S3ToS32Operator(\n            task_id=\"drop_table\",\n            s3_conn_id=\"s3_default\",\n            bucket=\"tc-s3-b\",\n            dest_s3_key_prefix=\"s3_to_s32/\",\n            sql=\"DROP TABLE IF EXISTS my_table;\",\n        )\n\n    create_view >> create_table >> drop_view >> drop_table\n\n    # [START howto_operator_s3_to_s32]\n    copy_between_s3_to_s3_to_s32 = CopyBetweenS3ToS3Operator(\n        task_id=\"copy_between_s3_to_s3_to_s32\",\n        source_s3=\"s3://tc-s3-a/\",\n        dest_s3_key_prefix=\"s3_to_s32/\",\n        s3_conn_id=\"s3_default\",\n    )\n    # [END howto_operator_s3_to_s32]\n\n    (\n        copy_between_s3_to_s3_to_s32\n        >> create_view\n        >> create_table\n        >> drop_view\n        >> drop_table\n    )\n\n    delete_session = Session()\n    delete_session.delete_task(s3_conn_id=\"s3_default\", task_id=create_table)\n    delete_session.delete_task(s3_conn_id=\"s3_default\", task_id=create_view)\n    delete_session.delete_task(s3_conn_id=\"s3_default\", task_id=drop_view)\n    delete_session.delete_task(s3_conn_id=\"s3_default\", task_id=drop_table)\n    delete_session.delete_task(s3_conn_id=\"s3_default\", task_id=drop_table)"}], "metadata": {"file_name": "dag_756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data processing and machine learning pipeline that takes raw image data from Google Cloud Storage, prepares it for object detection training, and creates a deployable machine learning model. Implement a systematic approach to handling data ingestion, model creation, and infrastructure management using Airflow and Google Cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImageSafeSearchOperator,\n    CloudVisionExtrapolateImageFeaturesOperator,\n    CloudVisionExtractTextOperator,\n    CloudVisionImageAnnotateOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.utils.retry import CloudRestAPIRetry\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"cloud_vision_annotator\"\n\nBUCKET_NAME = f\"bucket_vision_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"image_detection.jpg\"\n\n# [START howto_operator_vision_annotate_images]\n@task(task_id=\"vision_annoint_images_task\")\ndef vision_annoint_images(local_image_path, cloud_storage_path):\n    retry = CloudRestAPIRetry(maximum=5)\n    client = cloud_storage_path.client\n    image_annotate_request = client.image_annotate_request(\n        {\n            \"image\": {\"source\": {\"image_uri\": local_image_path}},\n            \"features\": [\n                {\"type_\": \"IMAGE_LABEL\"},\n                {\"type_\": \"IMAGE_SAFE_SEARCH\"},\n                {\"type_\": \"EXPLORE_IMAGE_FEATURES\"},\n            ],\n        }\n    )\n    response = client.image_annotate(requests=[image_annotate_request], retry=retry)\n    print(\"Image labeled:\")\n    print(response.image_annotation.label_labels)\n    print(\"\\nImage safe search result:\")\n    print(response.image_annotation.safe_search)\n    print(\"\\nExploring image features:\")\n    print(response.explore_image_features_response.explore_images_result)\n    print(\"\\nAnnotation for image at \", local_image_path, \" is :\")\n    print(response)\n    return\n\n\n# [END howto_operator_vision_annotate_images]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = GCSToGCSOperator(\n        task_id=\"upload_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    @task\n    def delete_file_from_gcs():\n        bucket_client = cloud_storage_client.bucket(BUCKET_NAME)\n        bucket_client.delete()\n\n    delete_file = delete_file_from_gcs()\n\n    # [START howto_operator_vision_detect_image_labels]\n    detect_image_labels_task = CloudVisionDetectImageLabelsOperator(\n        task_id=\"detect_image_labels_task\",\n        image_uri=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n    )\n    # [END howto_operator_vision_detect_image_labels]\n\n    # [START howto_operator_vision_detect_image_safe_search]\n    detect_image_safe_search_task = CloudVisionDetectImageSafeSearchOperator(\n        task_id=\"detect_image_safe_search_task\",\n        image_uri=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n    )\n    # [END howto_operator_vision_detect_image_safe_search]\n\n    # [START howto_operator_vision_extrapolate_annotate_image_features]\n    extrapolate_annotate_image_features_task = CloudVisionExtrapolateImageFeaturesOperator(\n        task_id=\"extrapolate_annotate_image_features_task\",\n        image_uri=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n        input_config={\n            \"image_uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n            \"source_image_format\": \"URIS_ONLY\",\n        },\n        output_config={\"file_path\": f\"gs://{BUCKET_NAME}/explore_explicit_image_features.csv\"},\n    )\n    # [END howto_operator_vision_extrapolate_annotate_image_features]\n\n    # [START howto_operator_vision_extract_text]\n    extract_text_task = CloudVisionExtractTextOperator(\n        task_id=\"extract_text_task\",\n        image_uri=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n    )\n    # [END howto_operator_vision_extract_text]\n\n    # [START howto_operator_vision_annotate_image]\n    annotate_image_task = CloudVisionImageAnnotateOperator(\n        task_id=\"annotate_image_task\",\n        image_uri=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n        features=[\n            cloud_storage_client.image_annotation.ImageAnnotation({\"source\": {\"image_uri\": local_image_path}}),\n        ],\n    )\n    # [END howto_operator_vision_annotate_image]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, upload_file]\n        >> detect_image_labels_task\n        >> detect_image_safe_search_task\n        >> extrapolate_annotate_image_features_task\n        >> extract_text_task\n        >> annotate_image_task\n        # TEST TEARDOWN\n        >> delete_bucket\n        >> delete_file\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove the task from the schedule\n    list(watcher(dag))"}], "metadata": {"file_name": "dag_462_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that transforms unstructured trivia data into a semantically searchable vector database using AI-powered embedding techniques. Create an end-to-end pipeline that demonstrates how to load, vectorize, query, and manage structured knowledge data.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.weaviate.operators.weaviate import WeaviateIngestOperator\nfrom airflow.utils.context import Context\n\nclass_obj_id = \"weaviate_example_without_mapping_class\"\nembedding_vector_length = 768\n\n# [START howto_operator_weaviate_embedding_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"weaviate\", \"embedding\"],\n)\ndef example_weaviate_embedding():\n    # [START howto_operator_weaviate_embedding]\n    @task\n    def create_class_without_mapping():\n        \"\"\"\n        Example of creating a class without any mapping. Embedding will be created\n        using the OpenAIEmbeddingOperator.\n        \"\"\"\n        openai = WeaviateIngestOperator(\n            task_id=\"create_class_without_mapping\",\n            class_id=class_obj_id,\n            input_data=[\n                {\"Question\": \"What is Airflow?\", \"Answer\": \"An intuitive platform for building and deploying data pipelines.\"},\n                {\"Question\": \"What is Weaviate?\", \"Answer\": \"A high-performance vector data store.\"},\n            ],\n        )\n\n    # End of example_embedding_class\n\n    # [START howto_operator_weaviate_embedding_async]\n    @task\n    def create_class_without_mapping_async():\n        \"\"\"\n        Example of creating a class without any mapping. Embedding will be created\n        using the OpenAIEmbeddingOperator.\n        \"\"\"\n        openai = WeaviateIngestOperator(\n            task_id=\"create_class_without_mapping_async\",\n            class_id=class_obj_id,\n            input_data=[\n                {\"Question\": \"What is Airflow?\", \"Answer\": \"An intuitive platform for building and deploying data pipelines.\"},\n                {\"Question\": \"What is Weaviate?\", \"Answer\": \"A high-performance vector data store.\"},\n            ],\n            trigger_rule=\"all_done\",\n        )\n\n    # End of example_embedding_class_async\n\n    @task\n    def get_embeddings_using_openai_embedding():\n        \"\"\"\n        Example of using the OpenAIEmbeddingOperator to get embeddings.\n        \"\"\"\n        openai = OpenAIEmbeddingOperator(task_id=\"get_embeddings_using_openai_embedding\")\n        embeddings = openai.output\n        assert embeddings == [\n            [0.5352271, -0.58617181, -1.1291977, ..., 1.4807897, -0.16663627, -0.93133316],\n            [-0.0433162, -0.74333174, -0.4123119, ..., -0.96933327, 0.62811114, -0.23122227],\n        ]\n\n    # End of example_get_embeddings_using_openai_embedding\n\n    # [START howto_operator_weaviate_ingest_embedding]\n    @task\n    def ingest_embedding_using_openai_embedding():\n        \"\"\"\n        Example of using WeaviateIngestOperator to ingest embeddings to Weaviate without\n        mapping using OpenAIEmbeddingOperator to get embeddings.\n        \"\"\"\n        openai = WeaviateIngestOperator(\n            task_id=\"ingest_embedding_using_openai_embedding\",\n            class_id=class_obj_id,\n            input_data=[\n                [{\"Question\": \"What is Airflow?\", \"Answer\": \"An intuitive platform for building and deploying data pipelines.\"}],\n                [{\"Question\": \"What is Weaviate?\", \"Answer\": \"A high-performance vector data store.\"}],\n            ],\n            vectorizers=\"openai\",\n            embeddings=get_embeddings_using_openai_embedding().output,\n        )\n\n    # End of example_ingest_embedding_using_openai_embedding\n\n    # [START howto_operator_weaviate_ingest_embedding_async]\n    @task\n    def ingest_embedding_using_openai_embedding_async():\n        \"\"\"\n        Example of using WeaviateIngestOperator to ingest embeddings to Weaviate without\n        mapping using OpenAIEmbeddingOperator to get embeddings.\n        \"\"\"\n        openai = WeaviateIngestOperator(\n            task_id=\"ingest_embedding_using_openai_embedding_async\",\n            class_id=class_obj_id,\n            input_data=[\n                [{\"Question\": \"What is Airflow?\", \"Answer\": \"An intuitive platform for building and deploying data pipelines.\"}],\n                [{\"Question\": \"What is Weaviate?\", \"Answer\": \"A high-performance vector data store.\"}],\n            ],\n            vectorizers=\"openai\",\n            embeddings=get_embeddings_using_openai_embedding().output,\n            trigger_rule=\"all_done\",\n        )\n\n    # End of example_ingest_embedding_using_openai_embedding_async\n\n    # [END howto_operator_weaviate_ingest_embedding]\n    # [END howto_operator_weaviate_ingest_embedding_async]\n\n    create_class_without_mapping()\n    create_class_without_mapping_async()\n    ingest_embedding_using_openai_embedding()\n    ingest_embedding_using_openai_embedding_async()"}], "metadata": {"file_name": "dag_1833_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Data Lake Storage (ADLS) file management workflow that demonstrates uploading a local file and then immediately deleting it. Create a DAG that showcases automated file transfer and cleanup processes in a cloud storage environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.operators.azure_file_share import AzureFileShareCreateOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls\"\n\nconn = Connection(\n    conn_id=\"azure_default\",\n    conn_type=\"azure_arm\",\n    schema=\"azure_arm\",\n    description=\"Azure Active Directory connection.\",\n    conn_params=\"PK--4666Z9563A8RJ4U8RJ4U8RJ4U8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ3U8RJ3A8RJ"}], "metadata": {"file_name": "dag_1353_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the complete lifecycle of an AutoML image training job, from dataset initialization through model training to resource deletion. Showcase how to programmatically manage machine learning infrastructure using cloud services and workflow orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductSetOperationOperator,\n    CloudVisionCreateImageSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionDeleteImageSetOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductsOperator,\n    CloudVisionListProductSetsOperator,\n)\nfrom airflow.providers.google.cloud.sensors.vision import CloudVisionImageSetSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = models.Variable.get(\"GCP_PROJECT_ID\")\n\nDAG_ID = \"vision_image_set\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nIMAGE_SET_NAME = \"test-image-set\"\nIMAGE_SET = {\n    \"image_uri\": f\"gs://{BUCKET_NAME}/{IMAGE_SET_NAME}\",\n    \"cloud_source_image_set_id\": \"test-image-set\",\n}\nPRODUCT_ID = \"my-product-123\"\nPRODUCT_SET_ID = \"my-product-set-123\"\nIMAGE = {\"source_image\": {\"image_uri\": f\"gs://{BUCKET_NAME}/test-image1.jpg\"}}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\", \"image_set\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_image_set = CloudVisionCreateImageSetOperator(\n        task_id=\"create_image_set\",\n        location=LOCATION,\n        image_set=IMAGE_SET,\n    )\n\n    # [START howto_operator_vision_image_set]\n    def wait_for_image_set(func, image_set_id):\n        # Provide the image set ID to wait for.\n        func(id=image_set_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_image_set]\n    image_set_sensor = CloudVisionImageSetSensor(\n        task_id=\"wait_for_image_set\",\n        image_set_id=\"{{ task_instance.xcom_pull(task_ids='create_image_set', key='image_set_id') }}\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        deferrable=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_product_set = CloudVisionCreateProductSetOperator(\n        task_id=\"create_product_set\",\n        location=LOCATION,\n        product_set={},\n    )\n\n    # [START howto_operator_vision_create_product]\n    def wait_for_product_set(func, product_set_id):\n        # Provide the product set ID to wait for.\n        func(id=product_set_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_create_product]\n    product_set_sensor = CloudVisionCreateProductOperator(\n        task_id=\"create_product\",\n        location=LOCATION,\n        product_id=PRODUCT_ID,\n        product_set_id=product_set_id=\"{{ task_instance.xcom_pull(task_ids='create_product_set', key='product_set_id') }}\",\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n\n    create_product = CloudVisionCreateProductOperator(\n        task_id=\"create_product\",\n        location=LOCATION,\n        product_id=PRODUCT_ID,\n        product_set_id=product_set_id=\"{{ task_instance.xcom_pull(task_ids='create_product_set', key='product_set_id') }}\",\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_image_product]\n    def wait_for_product(func, product_id):\n        # Provide the product ID to wait for.\n        func(id=product_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_image_product]\n    product_sensor = CloudVisionGetProductOperator(\n        task_id=\"get_product\",\n        product_id=PRODUCT_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n\n    list_products = CloudVisionListProductsOperator(\n        task_id=\"list_products\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_product_list]\n    def list_products_func(func, page_token=None):\n        # List products.\n        func(page=page_token, location=LOCATION, project_id=PROJECT_ID, page_size=100)\n\n    # [END howto_operator_vision_product_list]\n    list_products_sensor = BashOperator(\n        task_id=\"list_products_func\",\n        bash_command=f\"list_products_func() {{\\n{list_products_func} ; }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_vision_image_product_set]\n    def wait_for_product_set_func(func, page_token=None):\n        # List product set.\n        func(page=page_token, location=LOCATION, project_id=PROJECT_ID, page_size=100)\n\n    # [END howto_operator_vision_image_product_set]\n    wait_for_product_set_sensor = BashOperator(\n        task_id=\"wait_for_product_set_func\",\n        bash_command=f\"wait_for_product_set_func() {{\\n{wait_for_product_set_func} ; }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_vision_product_sets]\n    def list_product_sets_func(func, page_token=None):\n        # List product set.\n        func(page=page_token, location=LOCATION, project_id=PROJECT_ID, page_size=100)\n\n    # [END howto_operator_vision_product_sets]\n    list_product_sets = CloudVisionListProductSetsOperator(\n        task_id=\"list_product_sets\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_product_set]\n    def wait_for_product_set_func_2(func, page_token=None):\n        # List product set.\n        func(page=page_token, location=LOCATION, project_id=PROJECT_ID, page_size=100)\n\n    # [END howto_operator_vision_product_set]\n    wait_for_product_set_2_sensor = BashOperator(\n        task_id=\"wait_for_product_set_func_2\",\n        bash_command=f\"wait_for_product_set_func_2() {{\\n{wait_for_product_set_func_2} ; }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_product_set = CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set\",\n        product_set_id=\"{{ task_instance.xcom_pull(task_ids='create_product_set', key='product_set_id') }}\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_delete_product_set]\n    def delete_product_set_func(func, product_set_id):\n        # Delete product set.\n        func(product_set_id=product_set_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_delete_product_set]\n    delete_product_set_func_sensor = CloudVisionRemoveProductSetOperationOperator(\n        task_id=\"delete_product_set_func\",\n        product_set_id=\"{{ task_instance.xcom_pull(task_ids='create_product_set', key='product_set_id') }}\",\n        operation_id=\"{{ task_instance.xcom_pull(task_ids='create_product_set', key='operation_id') }}\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n\n    delete_product = CloudVisionDeleteProductOperator(task_id=\"delete_product\", product_id=PRODUCT_ID, location=LOCATION, project_id=PROJECT_ID)\n\n    # [START howto_operator_vision_delete_product]\n    def delete_product_func(func, product_id):\n        # Delete product.\n        func(product_id=product_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_delete_product]\n    delete_product_func_sensor = CloudVisionRemoveProductOperationOperator(\n        task_id=\"delete_product_func\",\n        product_id=\"{{ task_instance.xcom_pull(task_ids='create_product', key='product_id') }}\",\n        operation_id=\"{{ task_instance.xcom_pull(task_ids='create_product', key='operation_id') }}\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n\n    delete_image_set = CloudVisionDeleteImageSetOperator(\n        task_id=\"delete_image_set\", image_set_id=\"{{ task_instance.xcom_pull(task_ids='create_image_set', key='image_set_id') }}\", location=LOCATION, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_vision_delete_image_set]\n    def delete_image_set_func(func, image_set_id):\n        # Delete image set.\n        func(image_set_id=image_set_id, project_id=PROJECT_ID)\n\n    # [END howto_operator_vision_delete_image_set]\n    delete_image_set_func_sensor = CloudVisionRemoveImageSetOperationOperator(\n        task_id=\"delete_image_set_func\",\n        image_set_id=\"{{ task_instance.xcom_pull(task_ids='create_image_set', key='image_set_id') }}\",\n        operation_id=\"{{ task_instance.xcom_pull(task_ids='create_image_set', key='operation_id') }}\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n\n    delete_product_set_2 = CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set_2\",\n        product_set_id=\"{{ task_instance.xcom_pull(task"}], "metadata": {"file_name": "dag_2114_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an observability pipeline that showcases system interaction tasks, including date printing, sleep operations, and containerized command execution with clear task interdependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def gen_log() -> str:\n        \"\"\"\n        Genes a friendly log message.\n        \"\"\"\n        return \"Hello world!\"\n\n    gen_log_task = gen_log()\n\n    sleep_task = BashOperator(task_id=\"sleep\", bash_command=\"sleep 10\", dag=dag)\n\n    @task\n    def run_with_env() -> None:\n        \"\"\"\n        Runt a container with environment variables.\n        \"\"\"\n        container_args = {\n            \"file\": \"/path/to/myimage.sif\",\n            \"workdir\": \"/container/workdir\",\n            \"env\": [\"ENV_ID={{ENV_ID}}\"],\n        }\n        SingularityOperator(task_id=\"singularity_with_env\", **container_args)\n\n    run_with_env_task = run_with_env()\n\n    gen_log_task >> sleep_task >> run_with_env_task"}], "metadata": {"file_name": "dag_999_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data pipeline that illustrates how to interact with Databricks SQL environments, performing table creation, data insertion, transformation, file-based export, and cross-storage data import using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nSTART_DATE = datetime(2022, 1, 1)\nDAG_ID = \"databricks_sql_pipeline\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_SQL_DATABASE_NAME = os.environ.get(\"DATABRICKS_SQL_DATABASE_NAME\")\nPROMPT = \"Enter your Databricks URL:\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=START_DATE,\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START ddb_sensor]\n    check_query_progress = DatabricksSensor(\n        databricks_conn_id=\"databricks_conn_id\",\n        job_id=-1,\n        task_id=\"check_query_progress\",\n        sql_statement=\"SELECT COUNT(1) FROM table_name;\",\n        timeout=1200,\n        poke_interval=30,\n    )\n    # [END ddb_sensor]\n\n    # [START ddb_submit_run]\n    run_query = DatabricksSubmitRunOperator(\n        task_id=\"run_query\",\n        databricks_conn_id=\"databricks_conn_id\",\n        sql=f\"CREATE TABLE {DATABRICKS_SQL_DATABASE_NAME}.new_table AS SELECT gen_num AS new_gen_num FROM {DATABRICKS_SQL_DATABASE_NAME}.table_name;\",\n        job_params={\n            \"new_database_name\": DATABRICKS_SQL_DATABASE_NAME,\n            \"new_table_name\": \"new_table\",\n            \"overwrite\": \"overwrite\",\n            \"run_mode\": \"sql\",\n        },\n        dag=dag,\n    )\n    # [END ddb_submit_run]\n\n    check_query_progress >> run_query\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1558_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data transformation workflow using Google Dataprep, showcasing task dependencies, connection management, and cloud resource provisioning. Implement a systematic approach to creating, running, and monitoring data processing jobs across different cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataprep import (\n    DataprepCopyJobOperator,\n    DataprepCreateJobOperator,\n    DataprepDeleteJobOperator,\n    DataprepGetJobOperator,\n    DataprepListJobsOperator,\n    DataprepRunJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_dataprep\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataprep\"],\n) as dag:\n    # [START howto_operator_dataprep_create_job]\n    create_job = DataprepCreateJobOperator(\n        task_id=\"create_job\",\n        project_id=PROJECT_ID,\n        body={\"name\": \"test_job\"},\n        validate_request=True,\n    )\n    # [END howto_operator_dataprep_create_job]\n\n    # [START howto_operator_dataprep_get_job]\n    get_job = DataprepGetJobOperator(\n        task_id=\"get_job\",\n        project_id=PROJECT_ID,\n        job_name=\"{{ task_instance.xcom_pull('create_job', key='return_value') }}\",\n    )\n    # [END howto_operator_dataprep_get_job]\n\n    # [START howto_operator_dataprep_list_jobs]\n    list_jobs = DataprepListJobsOperator(\n        task_id=\"list_jobs\",\n        project_id=PROJECT_ID,\n        job_name=\"{{ task_instance.xcom_pull('create_job', key='return_value') }}\",\n    )\n    # [END howto_operator_dataprep_list_jobs]\n\n    # [START howto_operator_dataprep_delete_job]\n    delete_job = DataprepDeleteJobOperator(\n        task_id=\"delete_job\",\n        project_id=PROJECT_ID,\n        job_name=\"{{ task_instance.xcom_pull('create_job', key='return_value') }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_dataprep_delete_job]\n\n    # [START howto_operator_dataprep_run_job]\n    run_job = DataprepRunJobOperator(\n        task_id=\"run_job\",\n        project_id=PROJECT_ID,\n        job_name=\"{{ task_instance.xcom_pull('create_job', key='return_value') }}\",\n        body={\"params\": {\"value\": \"answer\"}},\n    )\n    # [END howto_operator_dataprep_run_job]\n\n    create_job >> get_job >> list_jobs >> delete_job >> run_job\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#write-tests)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1619_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates a basic sensor mechanism for waiting on upstream data readiness before proceeding with downstream tasks. Design a simple workflow that simulates checking for data availability and then executing a subsequent operation.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"data_sensor_task\")\ndef data_sensor():\n    print(\"\u6570\u636e\u5df2\u51c6\u5907\u597d\uff01\")\n\n\ndef execute(data=None):\n    print(f\"\u6570\u636e\u4e3a\uff1a{data}\")\n\n\nwith DAG(\n    \"example_data_sensor_dag\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    execute(\"hello\")\n    data_sensor()"}], "metadata": {"file_name": "dag_19_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that illustrates advanced scheduling mechanisms using dataset dependencies, showcasing how tasks can be triggered based on dataset updates, time schedules, and complex logical conditions. Develop a series of DAGs that demonstrate dataset-based scheduling with various dependency scenarios.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.dataset import Dataset\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# [START howto_dataset_operator_async]\n@dag(\n    schedule=Dataset.get_dataset_by_path(\"airflow/datasets/example_csv_path.csv\").get_schedule(),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_csv_async():\n    # [START howto_dataset_operator_async]\n    @task.dataset_async(dataset_id=\"example_csv_task\")\n    def create_task():\n        pass\n\n    create_task() >> BashOperator(task_id=\"run_task\"),\n\n\n    # [END howto_dataset_operator_async]\n# [END howto_dataset_operator_async]\n\n\n# [START howto_dataset_operator]\n@dag(\n    schedule=Dataset.get_dataset_by_path(\"airflow/datasets/example_csv_path.csv\").get_schedule(),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_csv():\n    # [START howto_dataset_operator]\n    @task.dataset(task_id=\"run_task\")\n    def create_task():\n        pass\n\n    create_task()\n    # [END howto_dataset_operator]\n# [END howto_dataset_operator]\n\n\n# [START howto_task_group_dataset_operator]\n@dag(\n    schedule=Dataset.get_dataset_by_path(\"airflow/datasets/example_csv_path.csv\").get_schedule(),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_csv_gcs():\n    # [START howto_task_group_dataset_operator]\n    @task.task_group(task_id=\"run_task\")\n    def create_task():\n        @task.dataset(task_id=\"run_task\")\n        def create_sub_task():\n            pass\n\n        create_sub_task()\n        # [END howto_task_group_dataset_operator]\n    # [END howto_task_group_dataset_operator]\n\n        create_task()\n    # [END howto_task_group_dataset_operator]\n# [END howto_task_group_dataset_operator]"}], "metadata": {"file_name": "dag_671_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS SQS message queue workflow that demonstrates publishing and consuming messages across multiple tasks. Create a DAG that showcases how to interact with Amazon SQS by sending messages, reading messages in single and batch modes, and managing queue lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom airflow.providers.amazon.aws.operators.sqs import SqsPublishOperator, SqsGetMessageOperator\nfrom airflow.providers.amazon.aws.sensors.sqs import SqsSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_sqs_publish]\n# [START howto_sensor_sqs]\n\nDAG_ID = \"example_sqs\"\n\nwith DAG(\n    DAG_ID,\n    schedule=date_interval,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n) as dag:\n    run_this = [\n        # [START howto_operator_sqs_publish]\n        SqsHook().create_task(\n            QueueUrl=\"your queue url\",\n            MessageBody=json.dumps({\"task\": \"value\"}),\n        )\n        # [END howto_operator_sqs_publish]\n        for task in range(3)\n        # [START howto_sensor_sqs_get]\n        | SqsSensor(\n            task_id=f\"get-sqs-message-{task}\",\n            queue_url=\"your queue url\",\n            max_messages=1,\n            wait_time=10,\n            # If you know there's only going to be one message and you're\n            # certain it'll be ready when the sensor wakes up, set\n            # attribute_name and attribute_value to the message attributes.\n            # The sensor will wake up if the message's attributes match.\n            attribute_name=\"DeliveryStatus\",\n            attribute_value=\"SUCCESS\",\n            # If there are multiple messages, or if you're not sure which\n            # message you want to pull, set kill_message_mode to 'ONE_BY_ONE'.\n            kill_message_mode=\"ONE_BY_ONE\",\n        )\n        # [END howto_sensor_sqs_get]\n    ]\n\n    # [START howto_operator_sqs_create_queue]\n    create_queue = SqsHook().create_task(\n        QueueName=\"your queue name\",\n        QueueAttributes={\n            \"DelayQueue\": b\"false\",\n            \"QueueMaxSize\": b\"10000\",\n            \"QueueSoftMaxSize\": b\"0\",\n            \"MessageRetentionPeriod\": b\"60\",\n        },\n    )\n    # [END howto_operator_sqs_create_queue]\n\n    # [START howto_operator_sqs_receive_message]\n    receive_message = SqsHook().create_task(\n        QueueUrl=\"your queue url\",\n        MaxMessages=1,\n        WaitTimeSeconds=10,\n    )\n    # [END howto_operator_sqs_receive_message]\n\n    # [START howto_operator_sqs_delete_queue]\n    delete_queue = SqsHook().create_task(QueueUrl=\"your queue url\", QueueName=\"your queue name\", Delete=True)\n    # [END howto_operator_sqs_delete_queue]\n\n    chain(run_this, create_queue, receive_message, delete_queue)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done;\n    # if the task is removed, the \"complete\" label will not be set\n    # and the start date won't be stamped.\n    list(watcher(dag))\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#write-tests)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1133_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated metadata registration pipeline that enables systematic tracking and organization of data assets across different systems and repositories using Google Cloud Dataplex. Design a workflow that captures metadata attributes, defines entry types, and creates catalog entries to support comprehensive data discovery and governance initiatives.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateAssetOperator,\n    DataplexCreateLakeOperator,\n    DataplexCreateTaskOperator,\n    DataplexCreateTaskTemplateOperator,\n    DataplexDeleteAssetOperator,\n    DataplexDeleteLakeOperator,\n    DataplexDeleteTaskOperator,\n    DataplexDeleteTaskTemplateOperator,\n    DataplexGetAssetOperator,\n    DataplexGetLakeOperator,\n    DataplexGetTaskOperator,\n    DataplexGetTaskTemplateOperator,\n    DataplexListAssetsOperator,\n    DataplexListLakeOperators,\n    DataplexListLakesOperator,\n    DataplexListTasksOperator,\n    DataplexListTasksTemplateOperator,\n    DataplexUpdateAssetOperator,\n    DataplexUpdateLakeOperator,\n    DataplexUpdateTaskOperator,\n    DataplexUpdateTaskTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_dataplex\"\n\nLOCATION = \"us-central1\"\nRESOURCE_ID = f\"asset-{DAG_ID}-{ENV_ID}\"\nRESOURCE_ID_2 = f\"lake-{DAG_ID}-{ENV_ID}\"\nRESOURCE_ID_3 = f\"task-{DAG_ID}-{ENV_ID}\"\nRESOURCE_ID_4 = f\"task-template-{DAG_ID}-{ENV_ID}\"\nRESOURCE_ID_5 = f\"task-{DAG_ID}-{ENV_ID}-1\"\nRESOURCE_ID_6 = f\"task-{DAG_ID}-{ENV_ID}-2\"\n\nLAKE_ID = f\"lake-{DAG_ID}-{ENV_ID}\"\nTASK_ID = f\"task-{DAG_ID}-{ENV_ID}\"\nTEMPLATE_ID = f\"template-{DAG_ID}-{ENV_ID}\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataplex\"],\n) as dag:\n    # [START howto_operator_create_dataplex_asset]\n    create_asset = DataplexCreateAssetOperator(\n        task_id=\"create_asset\",\n        location=LOCATION,\n        asset_id=RESOURCE_ID,\n        project_id=PROJECT_ID,\n        resource={\n            \"display_name\": \"test_asset\",\n            \"metadata_schema_id\": \"bigquery_table\",\n            \"gcs_bucket\": \"airflow-system-tests-dataplex\",\n            \"resource_labels\": {\"env\": \"testing\", \"label\": \"123\"},\n        },\n    )\n    # [END howto_operator_create_dataplex_asset]\n\n    # [START howto_operator_create_dataplex_lake]\n    create_lake = DataplexCreateLakeOperator(\n        task_id=\"create_lake\",\n        location=LOCATION,\n        lake_id=LAKE_ID,\n        project_id=PROJECT_ID,\n        resource={\"display_name\": \"test_lake\", \"resource_labels\": {\"env\": \"testing\"}},\n    )\n    # [END howto_operator_create_dataplex_lake]\n\n    # [START howto_operator_create_dataplex_task]\n    create_task = DataplexCreateTaskOperator(\n        task_id=\"create_task\",\n        location=LOCATION,\n        lake_id=LAKE_ID,\n        task_id=RESOURCE_ID_2,\n        project_id=PROJECT_ID,\n        resource={\"display_name\": \"test_task\", \"inlets\": [{\"resource\": \"gcs\", \"count\": 1}]},\n    )\n    # [END howto_operator_create_dataplex_task]\n\n    # [START howto_operator_create_dataplex_task_template]\n    create_task_template = DataplexCreateTaskTemplateOperator(\n        task_id=\"create_task_template\",\n        location=\"us-central1\",\n        project_id=\"airflow-system-tests-dataplex\",\n        template_id=TEMPLATE_ID,\n        resource={\"name\": \"tasks/ccdfgmhi\", \"display_name\": \"test_task_template\", \"inlets\": []},\n    )\n    # [END howto_operator_create_dataplex_task_template]\n\n    # [START howto_operator_create_dataplex_asset_id]\n    asset_id = \"toto\"\n    # [END howto_operator_create_dataplex_asset_id]\n\n    # [START howto_operator_create_dataplex_lake_id]\n    lake_id = \"lafa\"\n    # [END howto_operator_create_dataplex_lake_id]\n\n    # [START howto_operator_create_dataplex_task_id]\n    task_id = \"tata\"\n    # [END howto_operator_create_dataplex_task_id]\n\n    # [START howto_operator_create_dataplex_task_template_id]\n    template_id = \"tarra\"\n    # [END howto_operator_create_dataplex_task_template_id]\n\n    # [START howto_operator_get_dataplex_asset]\n    get_asset = DataplexGetAssetOperator(\n        task_id=\"get_asset\",\n        location=LOCATION,\n        asset_id=asset_id,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_dataplex_asset]\n\n    # [START howto_operator_get_dataplex_lake]\n    get_lake = DataplexGetLakeOperator(\n        task_id=\"get_lake\",\n        location=LOCATION,\n        lake_id=lake_id,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_dataplex_lake]\n\n    # [START howto_operator_get_dataplex_task]\n    get_task = DataplexGetTaskOperator(\n        task_id=\"get_task\",\n        location=LOCATION,\n        lake_id=lake_id,\n        task_id=task_id,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_dataplex_task]\n\n    # [START howto_operator_get_dataplex_task_template]\n    get_task_template = DataplexGetTaskTemplateOperator(\n        task_id=\"get_task_template\",\n        location=\"us-central1\",\n        project_id=\"airflow-system-tests-dataplex\",\n        template_id=template_id,\n        resource={\"name\": \"tasks/ccdfgmhi\", \"display_name\": \"tarra\", \"inlets\": []},\n    )\n    # [END howto_operator_get_dataplex_task_template]\n\n    # [START howto_operator_list_dataplex_assets]\n    list_assets = DataplexListAssetsOperator(\n        task_id=\"list_assets\",\n        location=LOCATION,\n        filter=\"name=%s\",\n        parameters={\"ids\": [RESOURCE_ID_2], \"project_id\": PROJECT_ID},\n    )\n    # [END howto_operator_list_dataplex_assets]\n\n    # [START howto_operator_list_dataplex_lakes]\n    list_lakes = DataplexListLakesOperator(\n        task_id=\"list_lakes\", location=LOCATION, filter=\"name=%s\" , parameters={\"ids\": [LAKE_ID], \"project_id\": PROJECT_ID}\n    )\n    # [END howto_operator_list_dataplex_lakes]\n\n    # [START howto_operator_list_dataplex_tasks]\n    list_tasks = DataplexListTasksOperator(\n        task_id=\"list_tasks\",\n        location=LOCATION,\n        lake_id=lake_id,\n        filter=\"name=%s\" , parameters={\"ids\": [task_id_2], \"project_id\": PROJECT_ID}\n    )\n    # [END howto_operator_list_dataplex_tasks]\n\n    # [START howto_operator_list_dataplex_tasks_template]\n    list_tasks_template = DataplexListTasksTemplateOperator(\n        task_id=\"list_tasks_template\",\n        location=\"us-central1\",\n        project_id=\"airflow-system-tests-dataplex\",\n        template_id=template_id,\n        filter=\"name=%s\" , parameters={\"ids\": [task_template_3], \"project_id\": PROJECT_ID}\n    )\n    # [END howto_operator_list_dataplex_tasks_template]\n\n    # [START howto_operator_update_dataplex_asset]\n    update_asset = DataplexUpdateAssetOperator(\n        task_id=\"update_asset\",\n        location=LOCATION,\n        asset_id=asset_id,\n        project_id=PROJECT_ID,\n        resource={\"resource_labels\": {\"env\": \"testing-q\"}},\n    )\n    # [END howto_operator_update_dataplex_asset]\n\n    # [START howto_operator_update_dataplex_lake]\n    update_lake = DataplexUpdateLakeOperator(\n        task_id=\"update_lake\",\n        location=LOCATION,\n        lake_id=lake_id,\n        project_id=PROJECT_ID,\n        resource={\"resource_labels\": {\"env\": \"testing-r\"}},\n    )\n    # [END howto_operator_update_dataplex_lake]\n\n    # [START howto_operator_update_dataplex_task]\n    update_task = DataplexUpdateTaskOperator(\n        task_id=\"update_task\",\n        location=LOCATION,\n        lake_id=lake_id,\n        task_id=task_id,\n        project_id=PROJECT_ID,\n        resource={\"resource\": {\"name\": \"tasks/ccdfgmhi\", \"display_name\": \"testing-t\", \"inlets\": [{\"resource\": \"gcs\", \"count\": 1}]}},\n    )\n    # [END howto_operator_update_dataplex_task]\n\n    # [START howto_operator_update_dataplex_task_template]\n    update_task_template = DataplexUpdateTaskTemplateOperator(\n        task_id=\"update_task_template\",\n        location=\"us-central1\",\n        project_id=\"airflow-system-tests-dataplex\",\n        template_id=template_id,\n        resource={\"name\": \"tasks/ccdfgmhi\", \"display_name\": \"testing-t\", \"inlets\": []},\n    )\n    # [END howto_operator_update_datap"}], "metadata": {"file_name": "dag_2026_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a multi-stage data orchestration process using task dependencies and different operators like BashOperator and HiveOperator. Implement a workflow that shows how data can be transferred between local storage, HDFS, Hive tables, and a MySQL database.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.yandex.transfers.s3_to_mysql import S3ToMySqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_mysql\"\n\nS3_BUCKET = f\"s3://{ENV_ID}-bucket\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"sql_conn_id\": \"mysql_default\"},\n    schedule=TriggerRule.ALL_DONE,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=S3_BUCKET)\n\n    create_s3_object = S3CreateObjectOperator(\n        task_id=\"create_s3_object\",\n        s3_bucket=S3_BUCKET,\n        s3_key=\"example.csv\",\n        s3_path=\"example.csv\",\n        data=\"hello world\",\n    )\n    # [START howto_transfer_s3_to_mysql]\n    s3_to_mysql = S3ToMySqlOperator(\n        task_id=\"s3_to_mysql\",\n        s3_bucket=S3_BUCKET,\n        table_name=\"s3_to_mysql_table\",\n        s3_key=\"example.csv\",\n        mysql_conn_id=\"mysql_default\",\n    )\n    # [END howto_transfer_s3_to_mysql]\n\n    delete_s3_bucket = SQLExecuteQueryOperator(\n        task_id=\"delete_s3_bucket\", sql=f\"DROP TABLE IF EXISTS {DAG_ID}.s3_to_mysql_table\"\n    )\n\n    delete_s3_object = S3CreateObjectOperator(\n        task_id=\"delete_s3_object\", s3_bucket=S3_BUCKET, s3_key=\"example.csv\"\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> create_s3_object\n        # TEST BODY\n        >> s3_to_mysql\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n        >> delete_s3_object\n    )"}], "metadata": {"file_name": "dag_1939_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated configuration generation and artifact retrieval workflow that uses Jenkins to trigger a job for creating configuration files and then downloads the generated artifact. The workflow should demonstrate inter-system communication and automated artifact management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom requests import Request\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.jenkins.operators.jenkins import (\n    JenkinsAddJobToQueueOperator,\n    JenkinsCreateProjectOperator,\n    JenkinsGetProjectUrlOperator,\n    JenkinsInvokeProjectOperator,\n    JenkinsRemoveJobFromQueueOperator,\n)\nfrom airflow.providers.common.jenkins.utils import JOB_STATUS\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils._bunch import BasicBunch\nfrom airflow.utils.context import Context\nfrom airflow.utils import lab\n\n\n@lab.dag\ndef example_jenkins_operator():\n    \"\"\"\n    Example Jenkins operators using operator python tasks.\n    \"\"\"\n    # Example arguments for JenkinsCreateProjectOperator\n    project_params = BasicBunch()\n    project_params.classic = False\n    project_params.name = \"TestProject\"\n    project_params.has_build = True\n    project_params.has_install = False\n    project_params.has_config = False\n    project_params.has_start = False\n    project_params.has_stop = False\n    project_params.has_delete = False\n    project_params.builder_args = BasicBunch()\n    project_params.builder_args.username = \"\"\n    project_params.builder_args.password = \"\"\n    build_params = BasicBunch()\n    build_params.name = \"TestBuild\"\n    build_params.project = project_params.name\n    build_params.branch = \"master\"\n    build_params\u77f3\u6750 = \"\"\n    job = JenkinsCreateProjectOperator(task_id=\"project_task\", project_params=project_params, trigger=TriggerTrigger)\n\n    # [START howto_operator_jenkins_python]\n    @task\n    def hello_world():\n        \"\"\"\n        Print a welcome message.\n        \"\"\"\n        print(\"Hello world!\")\n\n    # [END howto_operator_jenkins_python]\n\n    # Example arguments for JenkinsInvokeProjectOperator\n    invoke_project_params = BasicBunch()\n    invoke_project_params.project = project_params.name\n    invoke_project_params\u77f3\u6750 = build_params.name\n    invoke_project_params_params = BasicBunch()\n    invoke_project_params_params\u77f3\u6750 = \"\"\n    invoke_project_params_params\u77f3\u6750 = \"env\"\n    invoke_project_params_operator = JenkinsInvokeProjectOperator(\n        task_id=\"invoke_task\", project=invoke_project_params.project,\u77f3\u6750=invoke_project_params\u77f3\u6750, params=invoke_project_params_params, trigger=TriggerTrigger\n    )\n\n    @task\n    def wait_for_job_to_finish(jenkins_conn_id, project_url,\u77f3\u6750=JOB_STATUS.wait_for_job_to_finish):\n        \"\"\"\n        Wait for job to finish (using the JENKINS_URL interface).\n        \"\"\"\n        jenkins = Jenkins(jenkins_conn_id, project_url)\n       \u77f3\u6750 =\u77f3\u6750 or \"\"\n        finished = False\n        while not finished:\n           \u77f3\u6750 =\u77f3\u6750 or jenkins.get(JOB_STATUS, project=project_url,\u77f3\u6750=\u77f3\u6750)\n            if\u77f3\u6750.get(\"jobs\") and\u77f3\u6750.get(\"jobs\") and\u77f3\u6750.get(J\u77f3\u6750.get(\"jobs\")) and\u77f3\u6750.get(J\u77f3\u6750.get(\"jobs\")).get(J\u77f3\u6750.get(\"jobs\")).get(J\u77f3\u6750.get(\"jobs\")) == 0:\n                finished = True\n           \u77f3\u6750 =\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750 or\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750\u6216\u77f3\u6750"}], "metadata": {"file_name": "dag_960_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a sequence of cloud tasks involving bucket creation, audio generation, speech recognition, and bucket cleanup as a systematic demonstration of Google Cloud Speech API interactions. Showcase how different cloud operators can be chained together to create a comprehensive speech processing workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.speech_to_text import CloudSpeechToTextRecognizeSpeechOperator\nfrom airflow.providers.google.cloud.transcripts.speech_to_text import SpeechToTextOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"cloud_speech_to_text\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"test-file.mp3\"\nAUDIO_INPUT = {\n    \"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n    \"language_code\": \"en\",\n    \"model\": \"base\",\n}\n\nwith models.DAG(\n    DAG_ID,\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"speech_to_text\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START cloud_speech_to_text_operator]\n    speech_to_text_task = SpeechToTextOperator(\n        task_id=\"speech_to_text_task\", project_id=PROJECT_ID, input_config=AUDIO_INPUT\n    )\n    # [END cloud_speech_to_text_operator]\n\n    recognize_speech_task = CloudSpeechToTextRecognizeSpeechOperator(\n        task_id=\"recognize_speech_task\",\n        storage_bucket=BUCKET_NAME,\n        config={\n            \"input\": {\n                \"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> speech_to_text_task\n        >> recognize_speech_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1680_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you help me understand how to solve a system of linear equations using Python? Certainly! To solve a system of linear equations, you can use several methods in Python, including NumPy, which provides a convenient and efficient way to handle such computations.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "import numpy as np\n\ndef solve_linear_equations():\n    \"\"\"\n    Solves a system of linear equations using NumPy.\n    \n    Returns:\n        A tuple containing the solutions for x1 and x2.\n    \"\"\"\n    # Define the coefficients matrix A and the constant matrix b\n    A = np.array([[1, 2], [3, 4]])\n    b = np.array([7, 10])\n    \n    # Solve the system of linear equations\n    x = np.linalg.solve(A, b)\n    \n    return x\n\n# Example usage\nsolution = solve_linear_equations()\nprint(\"The solution is:\", solution)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a monitoring and alerting workflow for Google Cloud infrastructure that allows creating, managing, and configuring alert policies and notification channels. Build a system that can dynamically enable, disable, and track infrastructure health alerts across different Google Cloud resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.cloud_memorystore import (\n    CloudMemorystoreGCECreateInstanceOperator,\n    CloudMemorystoreGCEDeleteInstanceOperator,\n    CloudMemorystoreGCENetworkConfigurationOperator,\n    CloudMemorystoreGCENetworkingConfigurationOperator,\n    CloudMemorystoreGCEUpdateInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_memorystore_instance\"\n\nREGION = \"us-central1\"\nMEMORYSTORE_REPLICA_SIZE = \"1a\"\nMEMORYSTORE_NETWORK = f\"network-{ENV_ID}\"\nNETWORK_CONNECTION = f\"connection-{ENV_ID}\"\n\nCONNECTION_ID = f\"{PROJECT_ID}-{NETWORK_CONNECTION}\"\nCREATE_BODY = {\n    \"master_instance_name\": f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n    \"node_instance_name\": f\"{MEMORYSTORE_REPLICA_SIZE}-node\",\n    \"zone\": REGION,\n}\nUPDATE_BODY = {\n    \"labels\": {\n        \"my-label\": \"http://my-label/\",\n    },\n}\nDELETE_BODY = {\"force_delete\": True}\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"cloud-memorystore', 'instance\"],\n) as dag:\n    @task\n    def create_connection(connection_id: str):\n        connection = Connection(\n            conn_id=connection_id,\n            conn_type=\"google_cloud_platform\",\n            extra={\"project_id\": PROJECT_ID, \"location\": REGION, \"resource_id\": connection_id},\n        )\n        db_conn = {\n            \"conn_id\": connection_id,\n            \"conn_type\": \"google_cloud_platform\",\n            \"extra\": {\"project_id\": PROJECT_ID, \"location\": REGION, \"resource_id\": connection_id},\n        }\n        return [connection, db_conn]\n\n    create_connection_task = create_connection(connection_id=CONNECTION_ID)\n\n    create_instance = CloudMemorystoreGCECreateInstanceOperator(\n        task_id=\"create-instance\",\n        body=CREATE_BODY,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_gcp_cloud_memorystore_update_instance]\n    update_instance = CloudMemorystoreGCEUpdateInstanceOperator(\n        task_id=\"update-instance\",\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        body=UPDATE_BODY,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_cloud_memorystore_update_instance]\n\n    # [START howto_operator_gcp_cloud_memorystore_list_instances]\n    list_instances = CloudMemorystoreGCEListInstancesOperator(\n        task_id=\"list-instance\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_cloud_memorystore_list_instances]\n\n    # [START howto_operator_gcp_cloud_memorystore_get_instance]\n    get_instance = CloudMemorystoreGCEGetInstanceOperator(\n        task_id=\"get-instance\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n    )\n    # [END howto_operator_gcp_cloud_memorystore_get_instance]\n\n    # [START howto_operator_gcp_cloud_memorystore_delete_instance]\n    delete_instance = CloudMemorystoreGCEDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_cloud_memorystore_delete_instance]\n\n    # [START howto_operator_gcp_cloud_memorystore_create_network_configuration]\n    create_network_configuration = CloudMemorystoreGCENetworkConfigurationOperator(\n        task_id=\"create-network-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body={\"network\": MEMORYSTORE_NETWORK, \"ipv4scanning\": {\"enabled\": True}, \"ipv6scanning\": {\"enabled\": True}}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_create_network_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_get_network_configuration]\n    get_network_configuration = CloudMemorystoreGCENetworkConfigurationOperator(\n        task_id=\"get-network-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body={\"network\": MEMORYSTORE_NETWORK},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_get_network_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_update_network_configuration]\n    update_network_configuration = CloudMemorystoreGCENetworkConfigurationOperator(\n        task_id=\"update-network-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        network=MEMORYSTORE_NETWORK,\n        body={\"ipv4scanning\": {\"enabled\": True}, \"ipv6scanning\": {\"enabled\": False}}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_update_network_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_list_network_configurations]\n    list_network_configurations = CloudMemorystoreGCENetworkConfigurationOperator(\n        task_id=\"list-network-configurations\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_cloud_memorystore_list_network_configurations]\n\n    # [START howto_operator_gcp_cloud_memorystore_delete_network_configuration]\n    delete_network_configuration = CloudMemorystoreGCENetworkConfigurationOperator(\n        task_id=\"delete-network-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        network=MEMORYSTORE_NETWORK,\n    )\n    # [END howto_operator_gcp_cloud_memorystore_delete_network_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_create_networking_configuration]\n    create_networking_configuration = CloudMemorystoreGCENetworkingConfigurationOperator(\n        task_id=\"create-networking-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        body={\"allow_ip_rotation\": {\"enabled\": True}, \"network_policy\": {\"enabled\": True}}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_create_networking_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_get_networking_configuration]\n    get_networking_configuration = CloudMemorystoreGCENetworkingConfigurationOperator(\n        task_id=\"get-networking-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        body={\"allow_ip_rotation\": {\"enabled\": True}, \"network_policy\": {\"enabled\": True}}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_get_networking_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_update_networking_configuration]\n    update_networking_configuration = CloudMemorystoreGCENetworkingConfigurationOperator(\n        task_id=\"update-networking-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        body={\"allow_ip_rotation\": {\"enabled\": True}, \"network_policy\": {\"enabled\": False}}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_update_networking_configuration]\n\n    # [START howto_operator_gcp_cloud_memorystore_list_networking_configurations]\n    list_networking_configurations = CloudMemorystoreGCENetworkingConfigurationOperator(\n        task_id=\"list-networking-configurations\",\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n    )\n    # [END howto_operator_gcp_cloud_memorystore_list_networking_configurations]\n\n    # [START howto_operator_gcp_cloud_memorystore_delete_networking_configuration]\n    delete_networking_configuration = CloudMemorystoreGCENetworkingConfigurationOperator(\n        task_id=\"delete-networking-configuration\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        body={\"allowed_ip_rotations\": {\"allowed\": []}, \"network_policy\": {\"enabled\": False}},\n    )\n    # [END howto_operator_gcp_cloud_memorystore_delete_networking_configuration]\n\n    add_connection = Connection(\n        conn_id=CONNECTION_ID,\n        conn_type=\"google_cloud_platform\",\n        extra={\"project_id\": PROJECT_ID, \"location\": REGION, \"resource_id\": MEMORYSTORE_CONNECTION},\n    )\n\n    # [START howto_operator_gcp_cloud_memorystore_enable_instance_traffic_through_connection]\n    enable_instance_traffic_through_connection = CloudMemorystoreGCEEnableInstanceTrafficThroughConnectionOperator(\n        task_id=\"enable-instance-traffic-through-connection\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        connection_id=add_connection[\"conn_id\"],\n    )\n    # [END howto_operator_gcp_cloud_memorystore_enable_instance_traffic_through_connection]\n\n    # [START howto_operator_gcp_cloud_memorystore_disable_instance_traffic_through_connection]\n    disable_instance_traffic_through_connection = CloudMemorystoreGCEDisableInstanceTrafficThroughConnectionOperator(\n        task_id=\"disable-instance-traffic-through-connection\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        instance_name=f\"{MEMORYSTORE_REPLICA_SIZE}-master\",\n        connection_id=add_connection[\"conn_id\"],\n    )\n    # [END howto_operator_gcp_cloud_mem"}], "metadata": {"file_name": "dag_1287_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates file transfer from a local temporary storage to an AWS S3 bucket, showcasing how to programmatically create, move, and clean up temporary files in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\n\nLOCAL_TEMP = os.path.expanduser(\"~\")\nS3_KEY = \"s3://bucket-name/file.txt\"\nS3_LOCATION = \"s3://bucket-name/\"\nconn = Connection(\n    conn_id=\"id\",\n    conn_type=\"s3\",\n    description=\"Test S3 Connection\",\n    host=LOCAL_TEMP,\n    login=\"test@example.com\",\n    password=\"cZ67z8g3b3\",\n    region_name=\"us-east-1\",\n)\nrole = Role(name=\"test_role\")\nti = TaskInstance(task_id=\"test_task\", dttm=datetime(2020, 1, 1)))\ndag = DAG(\n    \"test_dag\",\n    start_date=datetime(2020, 1, 1),\n    default_args={\"conn_id\": \"id\"},\n    task_fn=lambda: None,\n    tags=[\"example\"],\n)\n\n\n@task\ndef generate():\n    conn = Connection(\n        conn_id=\"id\",\n        conn_type=\"s3\",\n        description=\"Test S3 Connection\",\n        host=LOCAL_TEMP,\n        login=\"test@example.com\",\n        password=\"cZ67z8g3b3\",\n        region_name=\"us-east-1\",\n    )\n    role = Role(name=\"test_role\")\n    ti = TaskInstance(task_id=\"test_task\", dttm=datetime(2020, 1, 1)))\n    dag = DAG(\n        \"test_dag\",\n        start_date=datetime(2020, 1, 1),\n        default_args={\"conn_id\": \"id\"},\n        task_fn=lambda: None,\n        tags=[\"example\"],\n    )\n    return [conn, role, ti, dag]\n\n\n@task\ndef my_task(s3_key: str):\n    print(s3_key)\n\n\ngenerate_output = my_task.expand(s3_key=[S3_KEY, S3_LOCATION])"}], "metadata": {"file_name": "dag_1957_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data pipeline that filters and exports specific documents from a MongoDB collection to an S3 storage system using predefined query criteria. Construct a workflow that selectively moves data based on a status condition, ensuring only relevant documents are transferred between different data storage platforms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_to_s3\"\n\n\n@task\ndef create_db():\n    # [START create_db]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"create_user_table\", sql=\"CREATE DATABASE test_db_2\").execute()\n    # [END create_db]\n\n\n@task\ndef create_role():\n    # [START create_role]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"create_role\", sql=\"CREATE ROLE read_write_role\").execute()\n    # [END create_role]\n\n\n@task\ndef create_task_group():\n    # [START create_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"create_task_group\", sql=\"CREATE TASK tg1 THEN SELECT 1,2,3,4,5,6,7,8,9,10,99 FROM airflow.\").execute()\n    # [END create_task_group]\n\n\n@task\ndef create_external_task_group():\n    # [START create_external_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"create_external_task_group\", sql=\"CREATE TASK exttg1 THEN SELECT 1,2,3,4,5,6,7,8,9,10,99 FROM airflow.\").execute()\n    # [END create_external_task_group]\n\n\n@task\ndef enable_task_group():\n    # [START enable_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"enable_task_group\", sql=\"ENABLE task_group\").execute()\n    # [END enable_task_group]\n\n\n@task\ndef disable_task_group():\n    # [START disable_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"disable_task_group\", sql=\"DISABLE task_group\").execute()\n    # [END disable_task_group]\n\n\n@task\ndef drop_db():\n    # [START drop_db]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"drop_db\", sql=\"DROP DATABASE test_db_2\").execute()\n    # [END drop_db]\n\n\n@task\ndef drop_role():\n    # [START drop_role]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"drop_role\", sql=\"DROP ROLE read_write_role\").execute()\n    # [END drop_role]\n\n\n@task\ndef drop_task_group():\n    # [START drop_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"drop_task_group\", sql=\"DROP TASK tg1\").execute()\n    # [END drop_task_group]\n\n\n@task\ndef drop_external_task_group():\n    # [START drop_external_task_group]\n    from airflow.providers.standard.operators.sql import SQLExecuteQueryOperator\n\n    SQLExecuteQueryOperator(task_id=\"drop_external_task_group\", sql=\"DROP TASK exttg1\").execute()\n    # [END drop_external_task_group]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_all_tasks():\n    # [START delete_all_tasks]\n    from airflow.sdk import Connection, Role, TaskGroup, Trigger\n\n    connections = Connection.query()\n    roles = Role.query()\n    task_groups = TaskGroup.query()\n    triggers = Trigger.query()\n\n    for connection in connections:\n        connection.delete()\n    for role in roles:\n        role.delete()\n    for task_group in task_groups:\n        task_group.delete()\n    for trigger in triggers:\n        trigger.delete()\n    # [END delete_all_tasks]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    connect_1 = Connection(\n        conn_id=\"conn_1\",\n        conn_type=\"s3\",\n        description=\"Connection for example use\",\n    )\n\n    conn_2 = Connection(\n        conn_id=\"conn_2\",\n        conn_type=\"sql\",\n        sql=\"SELECT 1 as x\",\n    )\n\n    conn_3 = Connection(\n        conn_id=\"conn_3\",\n        conn_type=\"sql\",\n        sql=\"CREATE DATABASE test_db_2\",\n    )\n\n    conn_4 = Connection(\n        conn_id=\"conn_4\",\n        conn_type=\"sql\",\n        sql=\"CREATE ROLE read_write_role\",\n    )\n\n    conn_5 = Connection(\n        conn_id=\"conn_5\",\n        conn_type=\"sql\",\n        sql=\"CREATE TASK tg1 THEN SELECT 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 99 FROM airflow.\",\n    )\n\n    conn_6 = Connection(\n        conn_id=\"conn_6\",\n        conn_type=\"sql\",\n        sql=\"CREATE TASK exttg1 THEN SELECT 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 99 FROM airflow;\",\n    )\n\n    conn_7 = Connection(\n        conn_id=\"conn_7\",\n        conn_type=\"sql\",\n        sql=\"CREATE TASK tg2 AS sql_statement='SELECT 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 99 FROM airflow.'\",\n    )\n\n    conn_8 = Connection(\n        conn_id=\"conn_8\",\n        conn_type=\"sql\",\n        sql=\"CREATE TASK tg3 WITH IDENTIFIER 'sql_statement'='SELECT 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 99 FROM airflow.'\",\n    )\n\n    conn_9 = Connection(\n        conn_id=\"conn_9\",\n        conn_type=\"sql\",\n        sql=\"DROP DATABASE test_db_2;\",\n    )\n\n    conn_10 = Connection(\n        conn_id=\"conn_10\",\n        conn_type=\"sql\",\n        sql=\"DROP ROLE read_write_role;\",\n    )\n\n    conn_11 = Connection(\n        conn_id=\"conn_11\",\n        conn_type=\"sql\",\n        sql=\"DROP TASK tg1;\",\n    )\n\n    conn_12 = Connection(\n        conn_id=\"conn_12\",\n        conn_type=\"sql\",\n        sql=\"DROP TASK exttg1;\",\n    )\n\n    conn_13 = Connection(\n        conn_id=\"conn_13\",\n        conn_type=\"sql\",\n        sql=\"DROP TASK tg2;\",\n    )\n\n    conn_14 = Connection(\n        conn_id=\"conn_14\",\n        conn_type=\"sql\",\n        sql=\"DROP TASK tg3;\",\n    )\n\n    # [START howto_operator_create_external_table]\n    create_external_table = SQLExecuteQueryOperator(\n        task_id=\"create_external_table\",\n        sql=\"CREATE TABLE external_table (col1 INT, col2 STRING)\",\n    )\n    # [END howto_operator_create_external_table]\n\n    # [START howto_operator_insert_into_external_table]\n    insert_into_external_table = SQLExecuteQueryOperator(\n        task_id=\"insert_into_external_table\",\n        sql=\"INSERT INTO external_table VALUES (1, 'test_1'), (2, 'test_2')\",\n    )\n    # [END howto_operator_insert_into_external_table]\n\n    # [START howto_operator_create_external_table_with_partition]\n    create_external_table_with_partition = SQLExecuteQueryOperator(\n        task_id=\"create_external_table_with_partition\",\n        sql=\"CREATE TABLE external_table (col1 INT, col2 STRING) PARTITIONED BY (col1 INT)\",\n    )\n    # [END howto_operator_create_external_table_with_partition]\n\n    # [START howto_operator_insert_into_external_table_with_partition]\n    insert_into_external_table_with_partition = SQLExecuteQueryOperator(\n        task_id=\"insert_into_external_table_with_partition\",\n        sql=\"INSERT INTO external_table_with_partition VALUES (1, 'test_1'), (2, 'test_2')\",\n    )\n    # [END howto_operator_insert_into_external_table_with_partition]\n\n    # [START howto_operator_create_table]\n    create_table = SQLExecuteQueryOperator(task_id=\"create_table\", sql=\"CREATE TABLE my_table (col1 INT, col2 STRING)\")\n    # [END howto_operator_create_table]\n\n    # [START howto_operator_insert_into_table]\n    insert_into_table = SQLExecuteQueryOperator(task_id=\"insert_into_table\", sql=\"INSERT INTO my_table VALUES (1, 'test')\")\n    # [END howto_operator_insert_into_table]\n\n    # [START howto_operator_create_view]\n    create_view = SQLExecuteQueryOperator(task_id=\"create_view\", sql=\"CREATE VIEW my_view AS SELECT col1, col2 FROM my_table\")\n    # [END howto_operator_create_view]\n\n    # [START howto_operator_insert_into_view]\n    insert_into_view = SQLExecuteQueryOperator(task_id=\"insert_into_view\", sql=\"INSERT INTO my_view VALUES (1, 'test')\")\n    # [END howto_operator_insert_into_view]\n\n    # [START call_python]\n    call_python = PythonOperator(task_id=\"call_python\", python_callable= Bash"}], "metadata": {"file_name": "dag_1910_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that copies employee salary data from one BigQuery table to another within the same dataset. Create an Airflow DAG that demonstrates table creation, data transfer, and cleanup for a simple HR data management scenario.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"airflow-explorer\")\nDAG_ID = \"bigquery_insert_job\"\nLOCATION = \"europe-west1\"\nDATASET_NAME = \"airflow-test\"\nTABLE_NAME = \"airflow-table\"\nSCHEMA = [\n    {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    project_id=PROJECT_ID,\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    on_finish_trigger=models.TriggerRule.ALL_DONE,\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n        location=LOCATION,\n    )\n\n    insert_into_table = BigQueryInsertJobOperator(\n        task_id=\"insert_into_table\",\n        configuration={\n            \"query\": {\n                \"query\": \"INSERT INTO `bigquery_test.` `(emp_name, salary) VALUES ('Joe', 5000);\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_bigquery_select_query]\n    select_query = BigQuerySelectOperator(\n        task_id=\"select_query\",\n        sql=f\"SELECT * FROM `bigquery_test.`.{TABLE_NAME}\",\n        location=LOCATION,\n    )\n    # [END howto_bigquery_select_query]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> insert_into_table\n        # TEST BODY\n        >> select_query\n        # TEST TEARDOWN\n        >> delete_dataset\n    )"}], "metadata": {"file_name": "dag_816_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data engineering workflow that leverages Google Cloud Build for automated software deployment and infrastructure provisioning. Create a scalable DAG that demonstrates dynamic trigger creation, project number retrieval, and cloud resource orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import TaskMapping\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.builds_v1 import v1 as cloud_build_api\nfrom airflow.providers.google.cloud.operators.cloud_build import (\n    CloudBuildCreateBuildOperator,\n    CloudBuildDeleteBuildOperator,\n    CloudBuildGetBuildOperator,\n    CloudBuildListBuildsOperator,\n    CloudBuildUpdateBuildOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.cloud_build import CloudBuildBuildSensor\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_cloud_build\"\n\nGCP_LOCATION = os.environ.get(\"SYSTEM_TESTS_GCP_LOCATION\", \"europe-west1\")\n\nGCP_BUILD_NAME = f\"build_{ENV_ID}\"\nGCP_BUILD_BODY = {\n    \"name\": GCP_BUILD_NAME,\n    \"source\": {\"url\": \"gs://airflow-system-tests-resources/cloud-build/python-builds/builds/2.yaml\"},\n    \"steps\": [{\"name\": \"bash\"}],\n    \"configurations\": [{\"name\": \"python3.7\"}, {\"name\": \"gcpjava8\"}],\n}\nGCP_BODY_TEMPLATE = {\n    \"name\": GCP_BUILD_NAME + \"-tmp\",\n    \"source\": {\"url\": \"gs://airflow-system-tests-resources/cloud-build/python-builds/builds-2-temp.yaml\"},\n    \"steps\": [{\"name\": \"bash\"}],\n    \"configurations\": [{\"name\": \"python3.7\"}],\n}\nGCP_BUCKET_NAME = f\"bucket_{ENV_ID}\"\nGCP_FILE_NAME = \"builds-builds-temporary-file.yaml\"\nGCP_DESTINATION_FILE_PATH = \"tmp/builds-builds-builds-2-temp-script.sh\"\nGCP_SET_DEFAULT = {\n    \"name\": GCP_BUILD_NAME + \"-set-default\",\n    \"source\": {\"url\": \"gs://airflow-system-tests-resources/cloud-build/python-builds/builds-set-default.yaml\"},\n    \"steps\": [{\"name\": \"bash\"}],\n    \"configurations\": [{\"name\": \"python3.7\"}],\n}\nGCP_REPOSITORY_NAME = f\"repo_{ENV_ID}\"\nGCP_REPOSITORY_BODY = {\n    \"name\": GCP_REPOSITORY_NAME,\n    \"initialRepositorySource\": {\"url\": \"gs://airflow-system-tests-resources/cloud-build/repository-builds/yaml/3.git\"},\n}\nGCP_FILE_REPOSITORY_PATH = \"build-repos-builds-builds-2-temp-file.yaml\"\nGCP_REPOSITORY_SET_DEFAULT_BODY = {\n    \"name\": GCP_REPOSITORY_NAME + \"-set-default\",\n    \"initialRepositorySource\": {\n        \"url\": \"gs://airflow-system-tests-resources/cloud-build/repository-builds/yaml/3-temp.git\"\n    },\n}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cloud_build\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=GCP_BUCKET_NAME)\n\n    move_file = GCSToGCSOperator(\n        task_id=\"move_file\",\n        source_bucket=GCP_BUCKET_NAME,\n        source_object=GCP_FILE_NAME,\n        destination_bucket=GCP_BUCKET_NAME,\n        destination_object=GCP_FILE_REPOSITORY_PATH,\n    )\n\n    @task\n    def create_build_from_repo(repo_name: str):\n        # [START create_build_from_repo]\n        cloud_build_create_build = CloudBuildCreateBuildOperator(\n            task_id=\"create_build_from_repo\",\n            project_id=PROJECT_ID,\n            build=GCP_REPOSITORY_SET_DEFAULT_body,\n            location=GCP_LOCATION,\n            retry=None,\n            timeout=None,\n        )\n        # [END create_build_from_repo]\n\n        cloud_build_create_build.result()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_build(build_name: str):\n        # [START delete_build]\n        cloud_build_delete_build = CloudBuildDeleteBuildOperator(\n            task_id=\"delete_build\",\n            project_id=PROJECT_ID,\n            build_name=build_name,\n            location=GCP_LOCATION,\n            retry=None,\n            timeout=None,\n        )\n        # [END delete_build]\n\n        delete_build(build_name)\n\n    @task\n    def get_build(build_name: str):\n        # [START cloud_build_get_build]\n        cloud_build_get_build = CloudBuildGetBuildOperator(\n            task_id=\"get_build\",\n            project_id=PROJECT_ID,\n            build_name=build_name,\n            location=GCP_LOCATION,\n        )\n        # [END cloud_build_get_build]\n\n        print(cloud_build_get_build.output)\n\n    @task\n    def update_build(build_name: str):\n        # [START cloud_build_update_build]\n        cloud_build_update_build = CloudBuildUpdateBuildOperator(\n            task_id=\"update_build\",\n            project_id=PROJECT_ID,\n            build_name=build_name,\n            build={\"name\": f\"{build_name}-updated\"},\n            location=GCP_LOCATION,\n        )\n        # [END cloud_build_update_build]\n\n        cloud_build_update_build.result()\n\n    # [START how_to_create_build_v1]\n    create_build_v1 = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_v1\",\n        project_id=PROJECT_ID,\n        build=GCP_BUILD_BODY,\n        location=GCP_LOCATION,\n        retry=None,\n        timeout=None,\n    )\n    # [END how_to_create_build_v1]\n\n    # [START how_to_get_build_v1]\n    get_build_v1 = CloudBuildGetBuildOperator(\n        task_id=\"get_build_v1\", project_id=PROJECT_ID, build_name=create_build_v1.output[\"name\"], location=GCP_LOCATION\n    )\n    # [END how_to_get_build_v1]\n\n    # [START how_to_list_builds_v1]\n    list_builds_v1 = CloudBuildListBuildsOperator(\n        task_id=\"list_builds_v1\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        page_size=5,\n    )\n    # [END how_to_list_builds_v1]\n\n    # [START how_to_update_build_v1]\n    update_build_v1 = CloudBuildUpdateBuildOperator(\n        task_id=\"update_build_v1\",\n        project_id=PROJECT_ID,\n        build_name=create_build_v1.output[\"name\"],\n        build={\"name\": f\"{create_build_v1.output['name']}-updated\"},\n        location=GCP_LOCATION,\n    )\n    # [END how_to_update_build_v1]\n\n    # [START how_to_set_default_build_v1]\n    set_default_build_v1 = CloudBuildCreateBuildOperator(\n        task_id=\"set_default_build_v1\",\n        project_id=PROJECT_ID,\n        build=GCP_SET_DEFAULT,\n        location=GCP_LOCATION,\n        retry=None,\n        timeout=None,\n    )\n    # [END how_to_set_default_build_v1]\n\n    # [START how_to_delete_build_v1]\n    delete_build_v1 = CloudBuildDeleteBuildOperator(\n        task_id=\"delete_build_v1\",\n        project_id=PROJECT_ID,\n        build_name=set_default_build_v1.output[\"name\"],\n        location=GCP_LOCATION,\n        retry=None,\n        timeout=None,\n    )\n    # [END how_to_delete_build_v1]\n\n    build_sensor = CloudBuildBuildSensor(\n        task_id=\"build_sensor\",\n        project_id=PROJECT_ID,\n        build_name=create_build_v1.output[\"name\"],\n        location=GCP_LOCATION,\n    )\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_sensor(sensor_name: str):\n        # [START how_to_remove_sensor]\n        delete_sensor = CloudBuildDeleteBuildOperator(\n            task_id=\"remove_sensor\",\n            project_id=PROJECT_ID,\n            build_name=sensor_name,\n            location=GCP_LOCATION,\n            retry=None,\n            timeout=None,\n        )\n        # [END how_to_remove_sensor]\n\n        delete_sensor.result()\n\n    # [START how_to_create_repository]\n    create_repository = CloudBuildCreateBuildOperator(\n        task_id=\"create_repository\",\n        project_id=PROJECT_ID,\n        build=GCP_REPOSITORY_BODY,\n        location=GCP_LOCATION,\n        retry=None,\n        timeout=None,\n    )\n    # [END how_to_create_repository]\n\n    # [START how_to_get_repository]\n    get_repository = CloudBuildGetBuildOperator(\n        task_id=\"get_repository\", project_id=PROJECT_ID, build_name=create_repository.output[\"name\"], location=GCP_LOCATION\n    )\n    # [END how_to_get_repository]\n\n    # [START how_to_list_repository]\n    list_repository = CloudBuildListBuildsOperator(\n        task_id=\"list_repository\",\n        project_id=PROJECT_ID,\n        location=GCP_LOCATION,\n        page_size=5,\n    )\n    # [END how_to_list_repository]\n\n    # [START how_to_update_repository]\n    update_repository = CloudBuildUpdateBuildOperator(\n        task_id=\"update_repository\",\n        project_id=PROJECT_ID,\n        build_name=create_repository.output[\"name\"],\n        build={\"name\": f\"{create_repository.output['name']}-updated\"},\n        location=GCP_LOCATION,\n    )\n    # [END how_to_update_repository]\n\n    # [START how_to_set_default_repository]\n    set_default_repository = CloudBuildCreateBuildOperator(\n        task_id=\"set_default_repository\",\n        project_id=PROJECT_ID,\n        build=GCP_REPOSITORY_SET_DEFAULT_BODY,\n        location=GCP_LOCATION,\n        retry=None,\n        timeout=None,\n    )\n    # [END how_to_set_default_repository]\n\n    # [START how_to_delete_repository]\n    delete_repository"}], "metadata": {"file_name": "dag_1997_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how to manage resource allocation and cleanup for a multi-stage data processing pipeline. Create a DAG that includes preparatory tasks, main work tasks, and cleanup tasks to simulate a complete operational workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"run_from_atx_task\")\ndef run_my_script():\n    print(\"Hi from atx!\")\n\n\nwith DAG(\n    dag_id=\"example_atx_deferrable\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n):\n    run_my_script()"}], "metadata": {"file_name": "dag_1054_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that simulates a data processing scenario with explicit resource management, showing how to handle cluster initialization, data transformation, and cluster teardown in a single coordinated workflow. Construct a DAG that demonstrates best practices for managing computational resources across different task stages.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.sensors.compute import ComputeEngineInstanceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gce_args]\ndef _create_network():\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteNetworkOperator,\n        ComputeEngineInsertNetworkOperator,\n    )\n\n    from airflow.utils.trigger_rule import TriggerRule\n\n    network_name = \"test-network\"\n    network = {\n        \"name\": network_name,\n        \"description\": \"Test description\",\n        \"networking\": {\n            \"access\": {\n                \"allow\": [\n                    {\"IPProtocol\": \"ALL\", \"fromIP\": [\"0.0.0.0/0\", \"192.168.0.0/16\"]},\n                    {\"IPProtocol\": \"TCP\", \"fromIP\": [\"0.0.0.0/0\", \"192.168.0.0/16\"]},\n                    {\"IPProtocol\": \"UDP\", \"fromIP\": [\"0.0.0.0/0\", \"192.168.0.0/16\"]},\n                ],\n            },\n        },\n    }\n\n    insert_network = ComputeEngineInsertNetworkOperator(\n        task_id=\"insert_network\", project_id=\"my-project\", body=network, trigger_rule=TriggerRule.ALL_DONE\n    )\n    check_insert_network = ComputeEngineInsertNetworkOperator.task(\n        task_id=\"check_insert_network\", project_id=\"my-project\", body=network, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # Call insert_network and wait that check_insert_network\n    insert_network.wait_for_completion()\n    check_insert_network.check_object_id(insert_network.output)\n\n\ndef _create_instance():\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteInstanceOperator,\n        ComputeEngineInsertInstanceOperator,\n    )\n\n    from airflow.utils.trigger_rule import TriggerRule\n\n    machine_type = \"n1-standard-1\"\n    name = \"test-instance\"\n    resource_id = \"test-instance\"\n    network_name = \"test-network\"\n    QOS_CLASSIFICATION_ID = \"822c2f22-85e4-4df1-9d9d-3499d9669c67\"\n    disks = [\n        {\n            \"auto_delete\": True,\n            \"device_name\": resource_id + \"-disk\",\n            \"initialize_params\": {\n                \"device_name\": resource_id + \"-disk\",\n                \"disk_size_gb\": 10,\n                \"disk_type\": \"pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n            },\n            \"boot\": True,\n        }\n    ]\n    network_interface = {\n        \"access_configs\": [\n            {\n                \"name\": \"NetworkInterface-ExternalNetwork\",\n                \"network_tier\": \"PREMIUM\",\n            }\n        ],\n        \"network_name\": network_name,\n    }\n    request_body = {\n        \"name\": name,\n        \"machine_type\": machine_type,\n        \"disks\": disks,\n        \"network_interfaces\": [network_interface],\n        \"disks\": disks,\n        \"disks\": disks,\n    }\n\n    insert_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance\", project_id=\"my-project\", body=request_body, trigger_rule=TriggerRule.ALL_DONE\n    )\n    check_instance = ComputeEngineInsertInstanceOperator.task(\n        task_id=\"check_instance\", project_id=\"my-project\", body=request_body, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # Call insert_instance and wait that check_instance\n    insert_instance.wait_for_completion()\n    check_instance.check_object_id(insert_instance.output)\n\n\ndef _delete_network():\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteNetworkOperator,\n    )\n\n    network_name = \"test-network\"\n    delete_network = ComputeEngineDeleteNetworkOperator(\n        task_id=\"delete_network\", project_id=\"my-project\", body={\"name\": network_name}, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n\ndef _delete_instance():\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteInstanceOperator,\n    )\n\n    instance_name = \"test-instance\"\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\", project_id=\"my-project\", name=instance_name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n\n# [END howto_operator_gce_args]\n\n\ndef _create_instance_with_sensor():\n    from airflow.providers.google.cloud.sensors.compute import ComputeEngineInstanceSensor\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteInstanceOperator,\n        ComputeEngineInsertInstanceOperator,\n    )\n\n    from airflow.utils.trigger_rule import TriggerRule\n\n    machine_type = \"n1-standard-1\"\n    name = \"test-instance\"\n    resource_id = \"test-instance\"\n    network_name = \"test-network\"\n    QOS_CLASSIFICATION_ID = \"822c2f22-85e4-4df1-9d9d-3499d9669c67\"\n    disks = [\n        {\n            \"auto_delete\": True,\n            \"device_name\": resource_id + \"-disk\",\n            \"initialize_params\": {\n                \"device_name\": resource_id + \"-disk\",\n                \"disk_size_gb\": 10,\n                \"disk_type\": \"pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n            },\n            \"boot\": True,\n        }\n    ]\n    network_interface = {\n        \"access_configs\": [\n            {\n                \"name\": \"NetworkInterface-ExternalNetwork\",\n                \"network_tier\": \"PREMIUM\",\n            }\n        ],\n        \"network_name\": network_name,\n    }\n    request_body = {\n        \"name\": name,\n        \"machine_type\": machine_type,\n        \"disks\": disks,\n        \"network_interfaces\": [network_interface],\n        \"disks\": disks,\n        \"disks\": disks,\n    }\n\n    insert_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance\", project_id=\"my-project\", body=request_body, trigger_rule=TriggerRule.ALL_DONE\n    )\n    wait_for_instance = ComputeEngineInstanceSensor(\n        task_id=\"wait_for_instance\", project_id=\"my-project\", instance_name=name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # Call insert_instance and wait that wait_for_instance\n    insert_instance.wait_for_completion()\n    wait_for_instance.check_object_id(insert_instance.output)\n\n\ndef _delete_instance_with_sensor():\n    from airflow.providers.google.cloud.operators.compute import (\n        ComputeEngineDeleteInstanceOperator,\n    )\n\n    instance_name = \"test-instance\"\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\", project_id=\"my-project\", name=instance_name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n\nwith DAG(\n    dag_id=\"example_gce\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_network = ComputeEngineHook(task_id=\"create_network\", project_id=\"my-project\")\n    create_instance = ComputeEngineHook(task_id=\"create_instance\", project_id=\"my-project\")\n    delete_network = ComputeEngineHook(task_id=\"delete_network\", project_id=\"my-project\")\n    delete_instance = ComputeEngineHook(task_id=\"delete_instance\", project_id=\"my-project\")\n    create_instance_with_sensor = ComputeEngineHook(task_id=\"create_instance_with_sensor\", project_id=\"my-project\")\n    delete_instance_with_sensor = ComputeEngineHook(task_id=\"delete_instance_with_sensor\", project_id=\"my-project\")\n\n    # [START create_network_hook]\n    _create_network()\n    # [END create_network_hook]\n\n    # [START create_instance_hook]\n    _create_instance()\n    # [END create_instance_hook]\n\n    # [START delete_network_hook]\n    _delete_network()\n    # [END delete_network_hook]\n\n    # [START delete_instance_hook]\n    _delete_instance()\n    # [END delete_instance_hook]\n\n    # [START wait_for_instance]\n    wait_for_instance = ComputeEngineInstanceSensor(\n        task_id=\"wait_for_instance\", project_id=\"my-project\", instance_name=name\n    )\n    # [END wait_for_instance]\n\n    # [START wait_for_instance_timeout]\n    wait_for_instance_timeout = ComputeEngineInstanceSensor(\n        task_id=\"wait_for_instance_timeout\", project_id=\"my-project\", instance_name=name\n    )\n    # [END wait_for_instance_timeout]\n\n    # [START delete_instance_with_sensor]\n    _delete_instance_with_sensor()\n    # [END delete_instance_with_sensor]\n\n    # [START wait_for_instance_with_sensor]\n    wait_for_instance_with_sensor = ComputeEngineInstanceSensor(\n        task_id=\"wait_for_instance_with_sensor\", project_id=\"my-project\", instance_name=name\n    )\n    # [END wait_for_instance_with_sensor]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1054_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I handle the issue of receiving a 500 Internal Server Error when attempting to delete a record using an API endpoint in Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_operator_dag\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"owner\": \"graphgl\",\n        \"depend_on_parent\": True,\n        \"start_date\": datetime(2021, 1, 1),\n    },\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def call_print(cmd_output):\n        import subprocess\n\n        print(subprocess.run(cmd_output, shell=True).returncode)\n\n    container_args = [\n        \"--volume\",\n        f\"{os.path.dirname(os.path.abspath(__file__))}/task_base_image:/task_base_image\",\n        \"--entrypoint\",\n        \"/task_base_image/print.py\",\n        \"bash\",\n        \"-c\",\n        \"echo 255\",\n    ]\n\n    call_print(container_args)\n\n    singularity_inst = SingularityOperator(\n        task_id=\"singularity_test\",\n        image=\"/singularity_test.dimage\",\n        command=\"/task_base_image/print.py 255\",\n        environment={\"ENV_ID\": ENV_ID},\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates the complete lifecycle of data catalog operations, including creating entry groups, registering file entries, and attaching custom metadata tags. Develop a systematic workflow that illustrates how to interact with Google Cloud Data Catalog using Airflow operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateEntryGroupAssociationOperator,\n    CloudDataCatalogCreateMetadataTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteEntryGroupAssociationOperator,\n    CloudDataCatalogDeleteMetadataTemplateOperator,\n    CloudDataCatalogGetEntryGroupOperator,\n    CloudDataCatalogGetEntryOperator,\n    CloudDataCatalogGetEntryGroupAssociationOperator,\n    CloudDataCatalogListEntriesOperator,\n    CloudDataCatalogListEntryGroupsOperator,\n    CloudDataCatalogUpdateEntryGroupOperator,\n    CloudDataCatalogUpdateEntryOperator,\n    CloudDataCatalogUpdateEntryGroupAssociationOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog\"\n\nENTRY_GROUP_ID = f\"entry-group-{ENV_ID}\"\nENTRY_GROUP = {\n    \"display_name\": \"Test Entry Group\",\n    \"description\": \"Test Entry Group\",\n    \"entry_count\": 0,\n    \"entry_groups_count\": 0,\n    \"metadata_template_id\": \"test-mt-id\",\n}\nENTRY_ID = f\"entry-id-{ENV_ID}\"\nENTRY = {\n    \"display_name\": \"Test Entry\",\n    \"description\": \"Test Entry\",\n    \"external_id\": ENTRY_ID,\n    \"metadata_template_id\": \"test-mt-id\",\n}\nENTRY_GROUP_ASSOCIATION_ID = f\"entry-group-assoc-id-{ENV_ID}\"\nENTRY_GROUP_ASSOCIATION = {\n    \"entry_group_id\": ENTRY_GROUP_ID,\n    \"entry_id\": ENTRY_ID,\n}\nMETA_DATA_TEMPLATE_ID = \"test-mt-id\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datacatalog\"],\n) as dag:\n    # [START howto_operator_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group=ENTRY_GROUP,\n    )\n    # [END howto_operator_create_entry_group]\n\n    # [START howto_operator_create_entry]\n    create_entry = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry\", project_id=PROJECT_ID, entry_id=ENTRY_ID, entry=ENTRY\n    )\n    # [END howto_operator_create_entry]\n\n    # [START howto_operator_create_entry_group_association]\n    create_entry_group_association = CloudDataCatalogCreateEntryGroupAssociationOperator(\n        task_id=\"create_entry_group_association\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n    )\n    # [END howto_operator_create_entry_group_association]\n\n    # [START howto_operator_get_entry_group]\n    get_entry_group = CloudDataCatalogGetEntryGroupOperator(\n        task_id=\"get_entry_group\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n    )\n    # [END howto_operator_get_entry_group]\n\n    # [START howto_operator_get_entry]\n    get_entry = CloudDataCatalogGetEntryOperator(\n        task_id=\"get_entry\", project_id=PROJECT_ID, entry_id=ENTRY_ID\n    )\n    # [END howto_operator_get_entry]\n\n    # [START howto_operator_list_entries]\n    list_entries = CloudDataCatalogListEntriesOperator(\n        task_id=\"list_entries\", project_id=PROJECT_ID, file_set_type=\"ENTRY_GROUP\", parameters={\"entryGroupIds.[-1]\": ENTRY_GROUP_ID}\n    )\n    # [END list_entries]\n\n    # [START howto_operator_list_entry_groups]\n    list_entry_groups = CloudDataCatalogListEntryGroupsOperator(\n        task_id=\"list_entry_groups\", project_id=PROJECT_ID, parameters={\"entryGroupIds.[-1]\": ENTRY_GROUP_ID}\n    )\n    # [END howto_operator_list_entry_groups]\n\n    # [START howto_operator_update_entry_group]\n    update_entry_group = CloudDataCatalogUpdateEntryGroupOperator(\n        task_id=\"update_entry_group\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"display_name\": \"New Display Name\"},\n    )\n    # [END howto_operator_update_entry_group]\n\n    # [START howto_operator_update_entry]\n    update_entry = CloudDataCatalogUpdateEntryOperator(\n        task_id=\"update_entry\",\n        project_id=PROJECT_ID,\n        entry_id=ENTRY_ID,\n        entry={\"display_name\": \"New Display Name\"},\n    )\n    # [END howto_operator_update_entry]\n\n    # [START howto_operator_update_entry_group_association]\n    update_entry_group_association = CloudDataCatalogUpdateEntryGroupAssociationOperator(\n        task_id=\"update_entry_group_association\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        association=ENTRY_GROUP_ASSOCIATION,\n    )\n    # [END howto_operator_update_entry_group_association]\n\n    # [START howto_operator_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\", project_id=PROJECT_ID, entry_group_id=ENTRY_GROUP_ID\n    )\n    # [END howto_operator_delete_entry_group]\n\n    # [START howto_operator_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\", project_id=PROJECT_ID, entry_id=ENTRY_ID\n    )\n    # [END howto_operator_delete_entry]\n\n    # [START howto_operator_delete_entry_group_association]\n    delete_entry_group_association = CloudDataCatalogDeleteEntryGroupAssociationOperator(\n        task_id=\"delete_entry_group_association\",\n        project_id=PROJECT_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n    )\n    # [END howto_operator_delete_entry_group_association]\n\n    # [START howto_operator_get_metadata_template]\n    get_metadata_template = CloudDataCatalogGetMetadataTemplateOperator(\n        task_id=\"get_metadata_template\", project_id=PROJECT_ID, metadata_template_id=META_DATA_TEMPLATE_ID\n    )\n    # [END howto_operator_get_metadata_template]\n\n    # [START howto_operator_create_metadata_template]\n    create_metadata_template = CloudDataCatalogCreateMetadataTemplateOperator(\n        task_id=\"create_metadata_template\",\n        project_id=PROJECT_ID,\n        metadata_template_id=META_DATA_TEMPLATE_ID,\n        display_name=\"Test Metadata Template\",\n        description=\"Test Description\",\n    )\n    # [END howto_operator_create_metadata_template]\n\n    # [START howto_operator_list_entries]\n    list_entries = CloudDataCatalogListEntriesOperator(\n        task_id=\"list_entries\", project_id=PROJECT_ID, file_set_type=\"METADATA_TEMPLATE\", parameters={\"metadataTemplateIds.[-1]\": META_DATA_TEMPLATE_ID}\n    )\n    # [END list_entries]\n\n    # [START howto_operator_delete_metadata_template]\n    delete_metadata_template = CloudDataCatalogDeleteMetadataTemplateOperator(\n        task_id=\"delete_metadata_template\", project_id=PROJECT_ID, metadata_template_id=META_DATA_TEMPLATE_ID\n    )\n    # [END howto_operator_delete_metadata_template]\n\n    # [START start_operator_wait_for_completion]\n    wait_for_completion = AirflowTriggerWaitOperator(\n        task_id=\"wait_for_completion\",\n        trigger_id=create_entry_group.result_id,\n        wait_for_completion=True,\n    )\n    # [END start_operator_wait_for_completion]\n\n    # [START start_operator_skip_completion]\n    skip_completion = AirflowTriggerSkipOperator(\n        task_id=\"skip_completion\", trigger_id=create_entry_group.result_id, skip_reason=\"test\"\n    )\n    # [END start_operator_skip_completion]\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"always\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2135_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that showcases the sequential execution of Kylin cube operations, highlighting task dependencies, time-based data segmentation, and the progression of data transformation steps from build to purge.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SqlExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kylin\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_kb():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        kbp = os.environ.get(\"KYN_BP\", \"kynpdb\")\n        sql = f\"create instance kbInstance for sqlserver ({conn}); \" f\"create kb ({kbn}); \" f\"create kb_table_set ({kbp}); \"\n        BashOperator(task_id=\"create_kb\", bash_command=sql)\n\n    @task\n    def create_cubes():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        kcp = os.environ.get(\"KYN_CP\", \"kynpdb\")\n        sql = f\"create instance cubeInstance for sqlserver ({conn}); \" f\"create cube ({kbn}); \" f\"create cube_table_set ({kcp}); \"\n        BashOperator(task_id=\"create_cubes\", bash_command=sql)\n\n    @task\n    def create_build():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance buildInstance for sqlserver ({conn}); \" f\"create build ({kbn}); \"\n        BashOperator(task_id=\"create_build\", bash_command=sql)\n\n    @task\n    def create_refresh():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance refreshInstance for sqlserver ({conn}); \" f\"create refresh ({kbn}); \"\n        BashOperator(task_id=\"create_refresh\", bash_command=sql)\n\n    @task\n    def create_import():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance importInstance for sqlserver ({conn}); \" f\"create import ({kbn}); \"\n        BashOperator(task_id=\"create_import\", bash_command=sql)\n\n    @task\n    def create_load():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance loadInstance for sqlserver ({conn}); \" f\"create load ({kbn}); \"\n        BashOperator(task_id=\"create_load\", bash_command=sql)\n\n    @task\n    def create_pull():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance pullInstance for sqlserver ({conn}); \" f\"create pull ({kbn}); \"\n        BashOperator(task_id=\"create_pull\", bash_command=sql)\n\n    @task\n    def create_report():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance reportInstance for sqlserver ({conn}); \" f\"create report ({kbn}); \"\n        BashOperator(task_id=\"create_report\", bash_command=sql)\n\n    @task\n    def create_sources():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance sourcesInstance for sqlserver ({conn}); \" f\"create sources ({kbn}); \"\n        BashOperator(task_id=\"create_sources\", bash_command=sql)\n\n    @task\n    def create_trigger():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance triggerInstance for sqlserver ({conn}); \" f\"create trigger ({kbn}); \"\n        BashOperator(task_id=\"create_trigger\", bash_command=sql)\n\n    @task\n    def create_validation():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance validationInstance for sqlserver ({conn}); \" f\"create validation ({kbn}); \"\n        BashOperator(task_id=\"create_validation\", bash_command=sql)\n\n    @task\n    def create_workspaces():\n        conn = Connection(\n            conn_id=\"kylin\",\n            conn_type=\"sqlserver\",\n            host=\"127.0.0.1\",\n            login=\"sa\",\n            password=\"password\",\n            schema=\"master\",\n        )\n        kbn = os.environ.get(\"KYN_BD\", \"kyn\")\n        sql = f\"create instance workspacesInstance for sqlserver ({conn}); \" f\"create workspaces ({kbn}); \"\n        BashOperator(task_id=\"create_workspaces\", bash_command=sql)\n\n    create_kb()\n    create_cubes()\n    create_build()\n    create_refresh()\n    create_import()\n    create_load()\n    create_pull()\n    create_report()\n    create_sources()\n    create_trigger()\n    create_validation()\n    create_workspaces()\n\n    # [START howto_operator_kylin_build]\n    build_kylin = SqlExecuteQueryOperator(\n        task_id=\"build_kylin\",\n        sql=f\"kylin build -t {os.environ.get('KYN_TGT')} -b {os.environ.get('KYN_BLD')}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kylin_build]\n\n    # [START howto_operator_kylin_pull]\n    pull_kylin = SqlExecuteQueryOperator(\n        task_id=\"pull_kylin\",\n        sql=f\"kylin pull -t {os.environ.get('KYN_TGT')} -r {os.environ.get('KYN_RPT')}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kylin_pull]\n\n    # [START howto_operator_kylin_report]\n    report_kylin = SqlExecuteQueryOperator(\n        task_id=\"report_kylin\",\n        sql=f\"kylin report -t {os.environ.get('KYN_TGT')} -r {os.environ.get('KYN_RPT')}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kylin_report]\n\n    # [START howto_operator_kylin_import]\n    import_kylin = SqlExecuteQueryOperator(\n        task_id=\"import_kylin\",\n        sql=f\"kylin import -t {os.environ.get('KYN_TGT')} -r {os.environ.get('KYN_RPT')} -b {os.environ.get('KYN_BLD')}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kylin_import]\n\n    # [START howto_operator_kylin_sources]\n    sources_kylin = SqlExecuteQueryOperator(\n        task_id=\"sources_kylin\",\n        sql=f\"kylin sources -t {os.environ.get('KYN_TGT')} -ws {os.environ.get('KYN_WS')}\",\n        trigger_rule=TriggerRule.ALL_DONE"}], "metadata": {"file_name": "dag_1535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data workflow that monitors and validates file uploads to a Google Cloud Storage bucket. Design a process that checks for completed file transfers and triggers subsequent data processing steps based on file arrival and update conditions.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.s3.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.providers.s3.transfers.s3_to_s3 import S3ToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"s3_to_s3\"\n\ns3_conn = Connection(\n    conn_id=\"s3_conn\",\n    conn_type=\"s3\",\n    host=\"s3-us-gov.amazonaws.com\",\n    schema=\"s3\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_s3_connection():\n        s3_hook = S3Hook()\n        s3_hook.create_conn()\n\n    @task\n    def create_bucket(bucket_name: str):\n        s3_hook = S3Hook()\n        s3_hook.create_bucket(Bucket=bucket_name)\n\n    @task\n    def upload_file(local_path: str, bucket_name: str):\n        s3_hook = S3Hook()\n        s3_hook.load_file(\n            path=local_path, bucket_name=bucket_name, replace=True\n        )\n\n    create_s3_connection = create_s3_connection()\n    create_bucket = create_bucket(bucket_name=\"us-gov-s3-bucket\")\n    upload_file = upload_file(local_path=\"/tmp/airflow_test.txt\", bucket_name=\"us-gov-s3-bucket\")\n\n    # [START howto_transfer_s3_to_s3]\n    s3_to_s3 = S3ToS3Operator(\n        task_id=\"s3_to_s3\",\n        source_key=\"\",\n        dest_key=\"\",\n        replace=False,\n    )\n    # [END howto_transfer_s3_to_s3]\n\n    create_s3_connection >> create_bucket(\"us-gov-s3-bucket\") >> upload_file >> s3_to_s3\n\n    s3_to_s3.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed or as failed)\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1253_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data asset registration and tagging pipeline that enables automated metadata management for cloud storage resources. The pipeline will demonstrate how to dynamically create catalog entries, assign custom tags, and prepare data assets for discovery and governance.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"s3_to_gcs_with_iam\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nFILE_URI = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"s3\", \"gcs\", \"transfer\"],\n) as dag:\n    @task\n    def create_conf():\n        return {\n            \"source_bucket\": BUCKET_NAME,\n            \"dest_gcs_prefix\": FILE_NAME,\n            \"aws_conn_id\": \"aws_default\",\n            \"replace\": True,\n        }\n\n    create_transfer = S3ToGCSOperator(task_id=\"create_transfer\", conf=create_conf())\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        create_transfer\n        >> create_bucket\n        >> delete_bucket\n    )"}], "metadata": {"file_name": "dag_2136_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Life Sciences pipeline that demonstrates basic workflow orchestration using Airflow. Create a DAG that uploads a file to a GCS bucket, runs a simple bash command, and then performs a multi-step pipeline action.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.life_sciences import (\n    CloudLifeSciencesCancelPipelineOperator,\n    CloudLifeSciencesCreatePipelineOperator,\n    CloudLifeSciencesRunPipelineOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_life_sciences\"\n\nGCS_BUCKET = f\"bucket_{DAG_ID}_{ENV_ID}\"\nPIPELINE_ID = f\"pipeline_{DAG_ID}_{ENV_ID}\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"life_sciences\"],\n) as dag:\n    # [START howto_operator_gcp_life_sciences_create_pipeline]\n    create_pipeline = CloudLifeSciencesCreatePipelineOperator(\n        task_id=\"create_pipeline\",\n        project_id=PROJECT_ID,\n        body={\"display_name\": \"test_pipeline\", \"description\": \"test_description\"},\n        location=\"us-central1\",\n    )\n    # [END howto_operator_gcp_life_sciences_create_pipeline]\n\n    # [START howto_operator_gcp_life_sciences_run_pipeline]\n    run_pipeline = CloudLifeSciencesRunPipelineOperator(\n        task_id=\"run_pipeline\",\n        project_id=PROJECT_ID,\n        pipeline_id=PIPELINE_ID,\n        location=\"us-central1\",\n        wait_for_completion=True,\n    )\n    # [END howto_operator_gcp_life_sciences_run_pipeline]\n\n    # [START howto_operator_gcp_life_sciences_cancel_pipeline]\n    cancel_pipeline = CloudLifeSciencesCancelPipelineOperator(\n        task_id=\"cancel_pipeline\",\n        project_id=PROJECT_ID,\n        pipeline_id=PIPELINE_ID,\n        location=\"us-central1\",\n    )\n    # [END howto_operator_gcp_life_sciences_cancel_pipeline]\n\n    create_pipeline >> run_pipeline >> cancel_pipeline\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as error\n    list(watcher(dag))"}], "metadata": {"file_name": "dag_228_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a controlled waiting period in a data processing workflow, using an asynchronous time delta sensor to introduce a deliberate delay before task completion. Develop a DAG that shows how time-based synchronization can be achieved without occupying worker slots during the waiting phase.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\n\nwith DAG(\n    dag_id=\"example_time_delta_sensor\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_time_delta_sensor]\n    t0 = BashOperator(task_id=\"wait_for_time\", bash_command=\"sleep 50\", dag=dag)\n    # [END howto_operator_time_delta_sensor]\n\n    t0"}], "metadata": {"file_name": "dag_1862_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you explain how the Chinese Remainder Theorem (CRT) can be effectively used to solve systems of congruences? Could you also provide a Python code example to demonstrate its application?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.azure_fileshare import AzureFileShareFileDeleteOperator\n\nFILE_SHARE_NAME = os.environ.get(\"FILE_SHARE_NAME\", \"test-file-share\")\nDAG_ID = \"example_fileshare_filedelete\"  # Needs to be the same as DAG ID\n\nwith DAG(\n    DAG_ID,\n    default_args={\"fileshare_name\": FILE_SHARE_NAME},\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_azure_fileshare_filedelete]\n    delete_file = AzureFileShareFileDeleteOperator(\n        task_id=\"delete_fileshare_file\",\n        fileshare_name=FILE_SHARE_NAME,\n        folder_path=\"/test/delete\",\n        file_path=\"test/file/delete/file\",\n    )\n    # [END howto_operator_azure_fileshare_filedelete]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)  # type: ignore[call-arg]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown stages for Google Calendar data synchronization. Implement a workflow that creates necessary cloud resources, handles authentication, transfers data, and then systematically removes all temporary infrastructure components.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.compute_v1 import ComputeEngineV1Hook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.google_cloud_to_google_cloud import GCFToGoogleCloudCopyResourceOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGoogleCloudCopyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_sheets_to_gcs\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nNETWORK_NAME = f\"network-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nGCP_FOLDER = f\"folder-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nGCP_SHEET_ID = \"example-sheets-1\"\nSINGLE_FILE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSINGLE_FILE_URL = f\"gs://{SINGLE_FILE_GCS_BUCKET_NAME}/sheet1.csv\"\nGCS_DESTINATION_PREFIX = f\"gs://{SINGLE_FILE_GCS_BUCKET_NAME}/partitions/\"\nGCP_SHEET_CLIENT_ID = os.environ.get(\"GCP_SHEET_CLIENT_ID\", \"1234567890\")\nGCP_SPOT_INSTANCE_MACHINE_TYPE = \"n1-standard-1\"\nGCP_SPOT_INSTANCE_NAME_PREFIX = f\"instance-spot-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nGCE_NETWORK_PARAM_NAME = \"network\"\nGCP_SPOT_INSTANCE_DESCRIPTION = \"Spot Instance for Google Sheets test.\"\nGCP_LOCATION = \"europe-west1\"\nGCP_LOCATION_PRIVATE_NETWORK = f\"regions/{GCP_LOCATION}/networks/{NETWORK_NAME}\"\nGCP_CREATE_BODY = {\n    \"name\": f\"projects/{PROJECT_ID}/global/spotInstanceTemplates/{DAG_ID}_{ENV_ID}\",\n    \"description\": GCP_SPOT_INSTANCE_DESCRIPTION,\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"metadata\": {\n        \"httpPortOpts\": [[8000]],\n        \"httpHealthCheckTarget\": f\"targets/{GCP_FOLDER}/services/healthcheck?path=/healthcheck\",\n    },\n    \"serviceAccountScopes\": [\n        \"https://www.googleapis.com/auth/cloud-platform\",\n        \"https://www.googleapis.com/auth/spreadsheets\",\n    ],\n}\nGCP_SPOT_INSTANCE_CREATE_BODY = {\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"name\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n    \"networkInterfaces\": [\n        {\n            \"accessConfigs\": [\n                {\n                    \"name\": \"External NAT\",\n                    \"networkPattern\": {\"subnet\": GCP_LOCATION_PRIVATE_NETWORK},\n                }\n            ],\n            \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n        }\n    ],\n    \"metadata\": {\n        \"httpPortOpts\": [[8000]],\n        \"httpHealthCheckTarget\": f\"targets/{GCP_FOLDER}/services/healthcheck?path=/healthcheck\",\n    },\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n}\nGCP_UPDATE_BODY = {\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"name\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n    \"networkInterfaces\": [\n        {\n            \"accessConfigs\": [\n                {\n                    \"name\": \"External NAT\",\n                    \"networkPattern\": {\"subnet\": GCP_LOCATION_PRIVATE_NETWORK},\n                }\n            ],\n            \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n        }\n    ],\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n}\nGCP_UPDATE_DESCRIPTION_BODY = {\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"name\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n    \"networkInterfaces\": [\n        {\n            \"accessConfigs\": [\n                {\n                    \"name\": \"External NAT\",\n                    \"networkPattern\": {\"subnet\": GCP_LOCATION_PRIVATE_NETWORK},\n                }\n            ],\n            \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n        }\n    ],\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip-mapping\",\n            },\n        }\n    ],\n}\nGCP_DELETED_INSTANCE_NAME = f\"projects/{PROJECT_ID}/locations/{LOCATION}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\"\nGCP_CREATE_BODY_WITH_FOLDERS = {\n    \"name\": f\"projects/{PROJECT_ID}/global/spotInstanceTemplates/{DAG_ID}_{ENV_ID}\",\n    \"description\": GCP_SPOT_INSTANCE_DESCRIPTION,\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"metadata\": {\n        \"httpPortOpts\": [[8000]],\n        \"httpHealthCheckTarget\": f\"targets/{GCP_FOLDER}/services/healthcheck?path=/healthcheck\",\n    },\n    \"serviceAccountScopes\": [\n        \"https://www.googleapis.com/auth/cloud-platform\",\n        \"https://www.googleapis.com/auth/spreadsheets\",\n    ],\n}\nGCP_UPDATED_BODY_WITH_FOLDERS = {\n    \"machineType\": f\"zones/{LOCATION}/machineTypes/{GCP_SPOT_INSTANCE_MACHINE_TYPE}\",\n    \"name\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n    \"disks\": [\n        {\n            \"deviceName\": f\"projects/{PROJECT_ID}/instances/{DAG_ID}_{ENV_ID}-{NETWORK_NAME_PARAM_NAME}-{GCP_SPOT_INSTANCE_NAME_PREFIX}-{ENV_ID}\",\n            \"boot\": True,\n            \"initializeParams\": {\n                \"diskSizeGb\": 10,\n                \"diskType\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"sourceImage\": f\"projects/google-compute-engine/disks/global-public-ip"}], "metadata": {"file_name": "dag_1248_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering pipeline that demonstrates dynamic cluster provisioning and management for big data processing workloads, showing how to dynamically allocate and deallocate computational resources based on processing requirements. The workflow should highlight the flexibility of cloud-based data processing infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.ssm import SSMHook\nfrom airflow.providers.amazon.aws.operators.ssm import SSMCommandOperator\nfrom airflow.providers.amazon.aws.operators.ssm import SSMPutParameterOperator\nfrom airflow.providers.amazon.aws.sensors.ssm import SSMSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nGROUP_NAME_KEY = \"GROUP_NAME\"\nREPOSITORY_ID_KEY = \"REPOSITORY_ID\"\nREPOSITORY_NAME_KEY = \"REPOSITORY_NAME\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(GROUP_NAME_KEY).add_variable(REPOSITORY_ID_KEY) \\\n    .add_variable(REPOSITORY_NAME_KEY).build()\n\nDAG_ID = \"example_ssmin\"\n\n\n@task\ndef create_connection(conn_id_name, cluster_arn):\n    log.info(f\"Creating connection with cluster ARN {cluster_arn}.\")\n    Connection(\n        conn_id=conn_id_name,\n        conn_id_extra={\n            \"aws_access_key_id\": \"AWS_ACCESS_KEY_ID\",\n            \"aws_secret_access_key\": \"AWS_SECRET_ACCESS_KEY\",\n        },\n        set_conn=True,\n    ).get_or_create(\n        conn_id=conn_id_name,\n        cluster_arn=cluster_arn,\n    )\n\n\n@task\ndef set_up_parameters(connection_id, repository_id, repository_name):\n    group_name = f\"{connection_id}_{repository_id}\"\n    prefix = f\"{repository_id}_sm_\"\n\n    log.info(f\"Setting up parameters for SSM Command Operator.\")\n    SSMPutParameterOperator(\n        task_id=\"setup_parameter_1\",\n        key=prefix + \"param1\",\n        value=f\"{repository_name} {repository_id}\",\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        description=\"Description of the first parameter\",\n        path=prefix,\n        server_side_validation=True,\n        connection_id=connection_id,\n        physical_name_prefix=prefix,\n    )\n\n    SSMPutParameterOperator(\n        task_id=\"setup_parameter_2\",\n        key=prefix + \"param2\",\n        value=f\"{prefix}parameters\",\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        description=\"Description of the second parameter\",\n        path=prefix,\n        server_side_validation=True,\n        connection_id=connection_id,\n        physical_name_prefix=prefix,\n    )\n\n    SSMPutParameterOperator(\n        task_id=\"setup_parameter_3\",\n        key=prefix + \"param3\",\n        value=json.dumps({\"0\": \"1\"}),\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        description=\"Description of the third parameter\",\n        path=prefix,\n        server_side_validation=True,\n        connection_id=connection_id,\n        physical_name_prefix=prefix,\n    )\n\n\n@task\ndef wait_for_sensor(sensor_instance, timeout=300):\n    log.info(f\"Waiting for sensor {sensor_instance} to become available.\")\n    start = pendulum.now()\n    end = start + pendulum.duration(secs=timeout)\n    while end > pendulum.now():\n        if sensor_instance.status == \"complete\":\n            log.info(f\"Sensor {sensor_instance} became available.\")\n            break\n        log.info(f\"Sensor {sensor_instance} is still running.\")\n        pendulum.sleep(1)\n    else:\n        log.fatal(f\"Sensor {sensor_instance} did not become available within the timeout period.\")\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_parameters(connection_id, repository_id, repository_name):\n    group_name = f\"{connection_id}_{repository_id}\"\n    prefix = f\"{repository_id}_sm_\"\n\n    log.info(f\"Deleting parameters for SSM Command Operator.\")\n    SSMCommandOperator(\n        task_id=\"delete_parameter_1\",\n        path=f\"{prefix}parameters\",\n        command=\"delete\",\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        connection_id=connection_id,\n    )\n\n    SSMCommandOperator(\n        task_id=\"delete_parameter_2\",\n        path=f\"{prefix}param1\",\n        command=\"delete\",\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        connection_id=connection_id,\n    )\n\n    SSMCommandOperator(\n        task_id=\"delete_parameter_3\",\n        path=f\"{prefix}param3\",\n        command=\"delete\",\n        source_alias=None,\n        parallelism_type=None,\n        unit=None,\n        connection_id=connection_id,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[env_id]\n    group_name = test_context[GROUP_NAME_KEY]\n    repository_id = test_context[REPOSITORY_ID_KEY]\n    repository_name = test_context[REPOSITORY_NAME_KEY]\n\n    conn_id_name = f\"{env_id}_conn_id\"\n    cluster_arn = f\"arn:aws:ecs:{dag.dag_run.conf.get('region')}:auto:cluster/{env_id}\"\n\n    create_connection(conn_id_name, cluster_arn)\n    setup_parameters = set_up_parameters(conn_id_name, repository_id, repository_name)\n    wait_for_sensor(setup_parameters)\n    delete_parameters(conn_id_name, repository_id, repository_name)\n    delete_connection = create_connection(conn_id_name, cluster_arn)"}], "metadata": {"file_name": "dag_858_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that moves files from Google Cloud Storage to a Samba file share, demonstrating different transfer scenarios like single file copy, file move, directory copy, and selective file movement. Create a DAG that showcases flexible file migration strategies between cloud and network storage systems.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.samba.transfers.copy_from_samba_to_samba import CopyFromSambaToSambaOperator\nfrom airflow.providers.samba.transfers.copy_samba_to_samba import CopySambaToSambaOperator\nfrom airflow.providers.samba.transfers.move_from_samba_to_samba import MoveFromSambaToSambaOperator\nfrom airflow.providers.samba.transfers.move_samba_to_samba import MoveSambaToSambaOperator\nfrom airflow.providers.smb.hooks.samba import SambaHook\nfrom airflow.providers.smb.operators.samba import SambaOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"samba_copy_move\"\n\nSMB_CONN_ID = f\"smb_{DAG_ID}\".replace(\"-\", \"_\")\nCONNECTION_KEY = f\"connection_{DAG_ID}\".replace(\"-\", \"_\")\n\nTRANFER_FILES = {\n    \"src_smb\": \"/tmp/samba/shadow\",\n    \"dest_smb\": \"/tmp/samba/airflow_move\",\n    \"src_local\": \"/tmp/airflow_copy\",\n    \"dest_local\": \"/tmp/airflow_copy_result\",\n}\n# [START howto_samba_operator_copy_single_file_to_samba]\nCOPY_FILE = {\n    \"smb_conn_id\": SMB_CONN_ID,\n    \"source_path\": TRANSFER_FILES[\"src_smb\"],\n    \"destination_path\": TRANSFER_FILES[\"dest_smb\"],\n}\n# [END howto_samba_operator_copy_single_file_to_samba]\n\n# [START howto_samba_operator_copy_multiple_files_to_samba]\nCOPY_FILES = {\n    \"smb_conn_id\": SMB_CONN_ID,\n    \"source_path\": TRANSFER_FILES[\"src_smb\"],\n    \"destination_path\": TRANSFER_FILES[\"dest_samba\"],\n    \"include_files\": [\"file1.txt\", \"dir1/file2.txt\"],\n    \"exclude_files\": [\"dir2/file3.txt\"],\n}\n# [END howto_samba_operator_copy_multiple_files_to_samba]\n\n# [START howto_samba_operator_move_single_file_to_samba]\nMOVE_FILE = {\n    \"smb_conn_id\": SMB_CONN_ID,\n    \"source_path\": TRANSFER_FILES[\"src_smb\"],\n    \"destination_path\": TRANSFER_FILES[\"dest_samba\"],\n    \"include_files\": [\"file1.txt\", \"dir1/file2.txt\"],\n    \"exclude_files\": [\"dir2/file3.txt\"],\n    \"move\": True,\n}\n# [END howto_samba_operator_move_single_file_to_samba]\n# [START howto_samba_operator_move_multiple_files_to_samba]\nMOVE_FILES = {\n    \"smb_conn_id\": SMB_CONN_ID,\n    \"source_path\": TRANSFER_FILES[\"src_smb\"],\n    \"destination_path\": TRANSFER_FILES[\"dest_samba\"],\n    \"include_files\": [\"file1.txt\", \"dir1/file2.txt\"],\n    \"exclude_files\": [\"dir2/file3.txt\"],\n    \"move\": True,\n}\n# [END howto_samba_operator_move_multiple_files_to_samba]\n\n\n@task\ndef create_connection(connection_id: CONNECTION_KEY):\n    samba_hook = SambaHook(smb_conn_id=connection_id)\n    samba_hook.create_connection(\n        smb_conn_id=connection_id,\n        host=\"example.samba.com\",\n        user=\"testuser\",\n        port=139,\n        path=\"/tmp\",\n        keytab=None,\n        use_sspi=True,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"samba\"],\n) as dag:\n    test_connection = create_connection(connection_id=CONNECTION_KEY)\n\n    @task\n    def get_files_from_samba():\n        samba_hook = SambaHook(smb_conn_id=SMB_CONN_ID)\n        return samba_hook.get_files(TRANSFER_FILES[\"src_smb\"])\n\n    files_from_samba = get_files_from_samba()\n\n    @task\n    def move_files_to_local():\n        local_path = \"/tmp/airflow_copy_result\"\n        os.makedirs(local_path, exist_ok=True)\n        files = [\n            (files_from_samba.output[i], f\"{local_path}/{os.path.basename(TRANSFER_FILES['src_smb'])}\")\n            for i in range(len(TRANSFER_FILES[\"src_smb\"]))\n        ]\n        for src, dest in files:\n            with open(src, \"wb\") as f:\n                f.write(src.read())\n            os.link(dest, f\"{local_path}/{os.path.basename(src)}\")\n\n    move_files_to_local()\n\n    @task\n    def move_files_to_samba():\n        samba_hook = SambaHook(smb_conn_id=SMB_CONN_ID)\n        samba_hook.create_connection(\n            smb_conn_id=SMB_CONN_ID,\n            host=\"example.samba.com\",\n            user=\"testuser\",\n            port=139,\n            path=\"/tmp\",\n            keytab=None,\n            use_sspi=True,\n        )\n        samba_hook.move_files(\n            source_path=TRANSFER_FILES[\"src_smb\"],\n            destination_path=TRANSFER_FILES[\"dest_samba\"],\n            include_files=TRANSFER_FILES[\"src_smb\"],\n            exclude_files=TRANSFER_FILES[\"dest_samba\"],\n        )\n\n    move_files_to_samba()\n\n    @task\n    def remove_files_from_samba():\n        samba_hook = SambaHook(smb_conn_id=SMB_CONN_ID)\n        samba_hook.remove_files(\n            source_path=TRANSFER_FILES[\"src_smb\"],\n            destination_path=TRANSFER_FILES[\"dest_samba\"],\n        )\n\n    remove_files_from_samba()\n\n    @task\n    def clean_connections():\n        conn = Connection.get(connection_id=CONNECTION_KEY)\n        dbapi_conn = conn.to_dbapi_conn()\n        dbapi_conn.execute(\"DELETE FROM connections WHERE connection_id = :conn_id\",\n                           dict(conn_id=CONNECTION_KEY))\n        dbapi_conn.close()\n        conn.delete()\n\n    clean_connections()"}], "metadata": {"file_name": "dag_646_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases the integration between Airflow's task decorators and Snowflake's Snowpark session management, highlighting how to programmatically create a DataFrame and write it to a Snowflake table. Implement a validation task that checks the data integrity of the newly created table.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.snowflake.sensors.snowflake import SnowflakeSensor\n\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\nSNOWFLAKE_SAMPLE_TABLE = \"sample\"\n\nwith DAG(\n    \"example_snowflake_operator\",\n    schedule=datetime(2021, 10, 10),\n    start_date=datetime(2021, 1, 1),  # Override to match your need\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_snowflake]\n    check_table_data = SnowflakeOperator(\n        task_id=\"check_table_data\",\n        sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n    )\n    # [END howto_operator_snowflake]\n\n    # [START howto_sensor_snowflake]\n    scan_table_data = SnowflakeSensor(\n        task_id=\"scan_table_data\",\n        sql=f\"SELECT * FROM {SNOWFLAKE_SAMPLE_TABLE}\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n    )\n    # [END howto_sensor_snowflake]\n\n    check_table_data >> scan_table_data"}], "metadata": {"file_name": "dag_2206_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you design a Python class that includes methods for arithmetic operations and logging? The class should perform addition, subtraction, multiplication, and division, and it should log each operation with a timestamp and the result. The logging should be handled by a separate logger class.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.utils.week import Week\nfrom airflow.utils.trigger_rule import TriggerRule\n\nPATH_TO_SCRIPT = Path(__file__).parent / \"script.py\"\n\n\nwith DAG(\n    dag_id=\"example_python_decorator\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=Week(),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_this\", bash_command=f\"python3 {PATH_TO_SCRIPT}\")\n    run_this"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that showcases the sequential steps of interacting with a cloud-based NoSQL database, focusing on transaction management and query preparation. Build a workflow that illustrates the technical mechanics of database interaction in a cloud environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.alibaba.cloud.operators.nosql import AzMysqlApplyQueryOperator, AzMysqlCreateTableOperator\n\nindex_name = os.getenv(\"INDEX_NAME\", \"test_index\")\ntable_name = os.getenv(\"TABLE_NAME\", \"test_table\")\nschema = {\"_$schema\": \"mysql\"}\nprimary_key = {\"_$primaryKeyName\": table_name}\ncolumn = {\"name\": {\"_$type\": \"varchar\", \"$_size\": 16}, \"id\": {\"_$type\": \"int\", \"$_autoIncrement\": True}}\nconditions = [{\"name\": {\"_$eq\": \"foo\"}}, {\"name\": {\"_$neq\": \"bar\"}}, {\"name\": {\"_$like\": \"txt%\"}}]\ndefault = {\"name\": \"\"}\n\n\nwith DAG(\n    \"azmysql-query-index\",\n    default_args={\"aws_conn_id\": \"aws_default\"},\n    description=\"Demo DAG for Azure MySql Apply Query.\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azmysql\"],\n) as dag:\n    # [START howto_azmysql_create_table]\n    create_table = AzMysqlCreateTableOperator(\n        task_id=\"create_table\",\n        database=\"testdb\",\n        table_name=table_name,\n        column_definition=json.dumps(column),\n        primary_key=primary_key,\n        schema=schema,\n        default_value=default,\n    )\n    # [END howto_azmysql_create_table]\n\n    # [START howto_azmysql_apply_query]\n    apply_query = AzMysqlApplyQueryOperator(\n        task_id=\"apply_query\",\n        database=\"testdb\",\n        table=table_name,\n        statement=f\"-i {index_name} -u name -v id\",\n    )\n    # [END howto_azmysql_apply_query]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_532_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function to calculate the smallest common multiple (SCM) of two numbers. The function should take two integers as input and return their SCM.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def calculate_smallest_common_multiple(num1: int, num2: int) -> int:\n    \"\"\"\n    Calculate the smallest common multiple (SCM) of two integers.\n\n    Args:\n    num1 (int): The first integer.\n    num2 (int): The second integer.\n\n    Returns:\n    int: The smallest common multiple of num1 and num2.\n    \"\"\"\n    return num1 * num2 // gcd(num1, num2)\n\ndef gcd(x: int, y: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor (GCD) of two integers.\n\n    Args:\n    x (int): The first integer.\n    y (int): The second integer.\n\n    Returns:\n    int: The greatest common divisor of x and y.\n    \"\"\"\n    while y:\n        x, y = y, x % y\n    return x\n\n# Example usage\nprint(calculate_smallest_common_multiple(12, 18))  # Output: 36"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can I inherit from a class in Python that doesn't have a `__init__` method?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom airflow.sdk import DAG, task\n\n# [START dag_def]\nwith DAG(\n    dag_id=\"example_future_python_operator\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START task_inlet]\n    @task.inlet\n    def inlet(output: Any):\n        # [END task_inlet]\n        print(output)\n\n    # [START task_outlet]\n    @task.outlet\n    def outlet(output: Any):\n        # [END task_outlet]\n        print(output)\n\n    # [START python_operator]\n    @task.python\n    def my_task(output: Any):\n        # [END python_operator]\n        print(output)\n\n    # [END python_operator]\n    inlet(output=\"Hello, World!\")\n    outlet(output=\"Hello, World!\")\n    my_task(output=\"Hello, World!\")\n# [END dag_def]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an end-to-end machine learning data pipeline that demonstrates model training, throughput allocation, and model lifecycle management using AWS Bedrock services. Create a systematic approach to preparing, training, and deploying custom AI models with automated monitoring and cleanup.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.client import BaseClient\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.bedrock import (\n    BedrockCreateIngestionOperator,\n    BedrockIngestDataOperator,\n    BedrockListModelsOperator,\n    BedrockRunIngestionOperator,\n    BedrockRunIngestionOperatorAsync,\n)\nfrom airflow.providers.amazon.aws.sensors.bedrock import BedrockSensor\nfrom airflow.providers.amazon.aws.utils import get_aws_id\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlog: logging.Logger = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nMODEL_ARN_KEY = \"MODEL_ARN\"\nMODEL_NAME = \"swin-transformers-312\"\n\n@dag(\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"bedrock\"],\n)\ndef example_bedrock():\n    conn_id = get_aws_id()\n    role_arn = os.environ.get(ROLE_ARN_KEY)\n\n    model_arn = os.environ.get(MODEL_ARN_KEY)\n    if not model_arn:\n        log.warning(\"No model ARN found in env - using system test parameters instead.\")\n    else:\n        log.info(\"Model ARN found in env: %s\", model_arn)\n\n    sys_test_context = sys_test_context_task()\n    env_id = sys_test_context.env_id\n    region = sys_test_context.region\n\n    source_bucket = f\"{env_id}-bedrock-source-bucket\"\n    source_key = f\"{env_id}-bedrock-source-file\"\n\n    model_name = f\"{env_id}-bedrock-model\"\n    connection = Connection(\n        conn_id=conn_id,\n        conn_type=\"s3\",\n        description=\"Connection to source bucket\",\n        db_type=\"S3\",\n        db_host=\"\",\n        db_port=\"\",\n        db_name=\"\",\n        db_table=\"\",\n        schema=\"\",\n        extra=json.dumps({\"bucket_name\": source_bucket, \"bucket_key\": source_key}),\n    )\n\n    @task\n    def create_model(model_name: str):\n        bedrock_hook = BedrockHook()\n        model_arn = bedrock_hook.create_model(\n            model_name,\n            framework=\"BEDROK_KINECT\",\n            training_data_mode=\"SPARK_LMDB\",\n            training_data_s3_bucket_name=source_bucket,\n            training_data_s3_key_prefix=source_key,\n            training_data_s3_input_format=\"CSV\",\n            training_data_csv_header=\"true\",\n            training_data_csv_num_fields=1,\n            training_data_csv_minimal=True,\n            training_data_csv_split=True,\n            training_data_csv_delimiter=\",\",\n            training_data_csv_quantity=1000,\n            training_data_csv_field_1=\"string\",\n            training_data_csv_field_2=\"string\",\n            training_data_csv_field_3=\"string\",\n            training_data_csv_field_4=\"string\",\n            training_data_csv_field_5=\"string\",\n            training_data_csv_field_6=\"string\",\n            training_data_csv_field_7=\"string\",\n            training_data_csv_field_8=\"string\",\n            training_data_csv_field_9=\"string\",\n            training_data_csv_field_10=\"string\",\n            output_uri=f\"s3://{env_id}/output/\",\n            role_arn=role_arn,\n            model_name=model_name,\n        )\n        return model_arn\n\n    @task\n    def update_model_output_uri(model_arn: str, output_uri: str):\n        bedrock_hook = BedrockHook()\n        bedrock_hook.update_model_output_uri(\n            Arn=model_arn, OutputUri=output_uri\n        )\n\n    @task\n    def delete_model(model_arn: str):\n        bedrock_hook = BedrockHook()\n        bedrock_hook.delete_model(ModelArn=model_arn)\n\n    create_model_task = create_model(model_name)\n    output_uri = f\"s3://{env_id}/output/\"\n    update_model_output_uri_task = update_model_output_uri(create_model_task.output, output_uri)\n    delete_model_task = delete_model(model_arn)\n\n    @task\n    def create_ingestion():\n        bedrock_hook = BedrockHook()\n        model_name = \"swin-transformers-312\"\n        training_data_s3_bucket_name = \"example-bedrock-training-data-bucket\"\n        training_data_s3_key_prefix = \"training-data\"\n        training_data_s3_input_format = \"CSV\"\n        training_data_csv_header = \"true\"\n        training_data_csv_minimal = \"true\"\n        training_data_csv_split = \"true\"\n        training_data_csv_delimiter = \",\"\n        training_data_csv_quantity = 1000\n        training_data_csv_field_1 = \"string\"\n        training_data_csv_field_2 = \"string\"\n        training_data_csv_field_3 = \"string\"\n        training_data_csv_field_4 = \"string\"\n        training_data_csv_field_5 = \"string\"\n        training_data_csv_field_6 = \"string\"\n        training_data_csv_field_7 = \"string\"\n        training_data_csv_field_8 = \"string\"\n        training_data_csv_field_9 = \"string\"\n        training_data_csv_field_10 = \"string\"\n        training_data_train = {\n            \"S3Bucket\": training_data_s3_bucket_name,\n            \"S3KeyPrefix\": training_data_s3_key_prefix,\n            \"InputFormat\": training_data_s3_input_format,\n            \"Header\": training_data_csv_header,\n            \"Minimal\": training_data_csv_minimal,\n            \"Split\": training_data_csv_split,\n            \"Delimiter\": training_data_csv_delimiter,\n            \"Quantity\": training_data_csv_quantity,\n            \"Fields\": {\n                training_data_csv_field_1: {},\n                training_data_csv_field_2: {},\n                training_data_csv_field_3: {},\n                training_data_csv_field_4: {},\n                training_data_csv_field_5: {},\n                training_data_csv_field_6: {},\n                training_data_csv_field_7: {},\n                training_data_csv_field_8: {},\n                training_data_csv_field_9: {},\n                training_data_csv_field_10: {},\n            },\n        }\n        model_arn = \"arn:aws:bedrock:us-east-1:123456789012:modelArn\"\n        training_dataset_arn = bedrock_hook.create_training_dataset(\n            ModelName=model_name, TrainingData=training_data_train, TrainingDatasetArn=model_arn\n        )\n        return training_dataset_arn\n\n    @task\n    def get_training_dataset_id(training_dataset_arn: str):\n        return training_dataset_arn.split(\"/\")[-1]\n\n    @task\n    def wait_for_sensor(sleep_time: int = 5):\n        sleep_time = int(sleep_time)\n        bedrock_hook = BedrockHook()\n        waiter = bedrock_hook.get_waiter(\"training_dataset_exists\")\n        waiter.wait(TrainingDatasetArn=get_training_dataset_id(training_dataset_arn))\n\n        # Synchronize the test by checking that the training dataset ARN has been read from the sensor.\n        # If it has not been read by this point, the sensor was not triggered.\n        conn = S3Hook().conn\n        response = conn.get_bucket_role_arn(training_dataset_arn)\n        role_arn = response[\"Role\"][\"Arn\"]\n        log.info(\"Role ARN: %s\", role_arn)\n        return get_aws_id(role_arn)\n\n    @task\n    def set_up_connection(s3_conn, conn_id):\n        s3_conn.create_bucket(Bucket=source_bucket)\n        s3_conn.put_object(Bucket=source_bucket, Key=source_key, Body=\"test\")\n        conn_id = get_aws_id()\n        conn = Connection(\n            conn_id=conn_id,\n            conn_type=\"s3\",\n            extra=json.dumps({\"bucket_name\": source_bucket, \"bucket_key\": source_key}),\n        )\n        return conn\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_connections(conn_id):\n        s3_conn = S3Hook().conn\n        for connection in s3_conn.get_all_connections():\n            if connection.conn_id == conn_id:\n                s3_conn.delete_connection(connection_id=connection.id)\n        s3_conn.delete_bucket(Bucket=source_bucket)\n\n    source_connection = set_up_connection(s3_conn, conn_id)\n    create_training_dataset = create_ingestion()\n    get_training_dataset_id(create_training_dataset.output)\n    wait_for_sensor_task = wait_for_sensor()\n    delete_connections_task = delete_connections(conn_id)\n    [create_model_task, update_model_output_uri_task, delete_model_task, wait_for_sensor_task, delete_connections_task] >> s3_conn\n\n    bedrock_operator = BedrockRunIngestionOperator(\n        task_id=\"run_ingestion\",\n        training_dataset_arn=create_training_dataset.output,\n        model_arn=model_arn,\n        input_data=\"example-bedrock-training-data-bucket\",\n        language=\"en\",\n        location=\"us-east-1\",\n        model_name=\"swin-transformers-312\",\n        file_format=\"CSV\",\n        quantity=1000,\n        field_1=\"string\",\n        field_2=\"string\",\n        field_3=\"string\",\n        field_4=\"string\",\n        field_5=\"string\",\n        field_6=\"string\",\n        field_7=\"string\",\n        field_8=\"string\",\n        field_9=\"string\",\n        field_10=\"string\","}], "metadata": {"file_name": "dag_1075_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and XCom communication techniques using both Python and Bash operators. Create a workflow that shows how different tasks can push and pull data between each other using Airflow's XCom mechanism.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\n\n\n@task.python\ndef my_task():\n    print(\"Hi\")\n    return 42\n\n\n@task\ndef print_it(value):\n    print(value)\n\n\nwith DAG(\n    \"example_xcom\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    print_it(my_task())"}], "metadata": {"file_name": "dag_1454_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed command execution workflow that leverages WinRM to run system-level commands across remote Windows environments, highlighting inter-task dependencies and parallel processing capabilities. Create a DAG that simulates real-world remote system management and monitoring scenarios.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_winrm\"\n\nconn = Connection(\n    conn_id=\"winrm\",\n    conn_type=\"winrm\",\n    host=\"winrm.test\",\n    login=\"test\",\n    password=\"test\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def setup_winrm_connection():\n        winrm_hook = WinRMHook()\n        winrm_hook.connect(conn)\n\n    setup_winrm_connection_task = setup_winrm_connection()\n\n    cmd1 = WinRMOperator(task_id=\"cmd1\", winrm_hook=winrm_hook, url=\"127.0.0.1\", username=\"test\", bash_command=\"echo test\")\n\n    cmd2 = WinRMOperator(task_id=\"cmd2\", winrm_hook=winrm_hook, url=\"127.0.0.1\", username=\"test\", bash_command=\"echo test\")\n\n    cmd1 >> cmd2\n\n    @task\n    def remove_winrm_connection():\n        winrm_hook = WinRMHook()\n        winrm_hook.disconnect()\n\n    remove_winrm_connection_task = remove_winrm_connection()\n\n    (setup_winrm_connection_task >> cmd1 >> cmd2 >> remove_winrm_connection_task)"}], "metadata": {"file_name": "dag_1761_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you explain the differences between `sys.exit()` and `os._exit()` in Python, and provide examples of how to use each?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task\ndef run_cmd():\n    os.system(\"echo 'Hello World pipeline'\")\n\n\nwith DAG(\n    dag_id=\"example_bash\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n):\n    run_cmd()\n    BashOperator(task_id=\"run_cmd_2\", bash_command=\"echo 'Hello World pipeline 2'\")"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "In Python, how can I ensure that a function is called and its result is checked for a specific condition without using decorators?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\ntry:\n    from airflow.providers.standard.operators.python import PythonOperator  # type: ignore\nexcept ImportError:\n    from airflow.operators.python import PythonOperator  # type: ignore\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_python_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"hello_world\", bash_command=\"echo \\\"Hello World\\\"\")\n    def _my_function():\n        print(\"Hello World\")\n        raise Exception(\"Something went wrong!\")\n\n    python_task = PythonOperator(task_id=\"exc\", python_callable=_my_function)\n\n    run_this >> python_task"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a single-task data transfer mechanism between cloud storage and a distributed query engine. Develop a scheduled job that moves data from GCS to Presto with minimal configuration and maximum reliability.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_presto\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection(conn_id_name: str):\n        conn = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"presto\",\n            description=\"Example Presto connection\",\n            extra={\n                \"presto\": {\n                    \"host\": \"localhost\",\n                    \"port\": 8080,\n                }\n            },\n        )\n        db_conn = Role(roles=[\"admin\"], connection_id=conn_id_name)\n        db_conn.save()\n        TaskGroup(\n            task_id=\"set_up_presto_env\",\n            roles=[\"admin\"],\n            connections=[conn],\n            tasks=[\n                BashOperator(\n                    task_id=\"start_presto\",\n                    bash_command=\"presto --start\",\n                    ignore_exit_code=True,\n                ),\n                BashOperator(\n                    task_id=\"wait_for_presto\",\n                    bash_command=\"while [ $(ps aux | grep presto | grep -c ^$ENV_ID) -eq 1 ]; do :; done\",\n                    trigger_rule=TriggerRule.ALL_DONE,\n                    ignore_exit_code=True,\n                ),\n            ],\n        )\n\n    @task\n    def create_presto_table():\n        sql = \"CREATE TABLE test (inout varchar)\"\n        SQLExecuteQueryOperator(\n            task_id=\"presto_create_table\",\n            sql=sql,\n            conn_id=conn_id_name,\n        )\n\n    @task\n    def insert_data():\n        sql = \"INSERT INTO test VALUES ('outout')\"\n        SQLExecuteQueryOperator(\n            task_id=\"presto_insert_data\",\n            sql=sql,\n            conn_id=conn_id_name,\n        )\n\n    @task\n    def check_data():\n        sql = \"SELECT * FROM test\"\n        SQLExecuteQueryOperator(task_id=\"presto_check_data\", sql=sql, conn_id=conn_id_name)\n\n    @task\n    def delete_data():\n        sql = \"DROP TABLE test\"\n        SQLExecuteQueryOperator(task_id=\"presto_delete_data\", sql=sql, conn_id=conn_id_name)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_presto_env():\n        conn_id_name = f\"presto_{DAG_ID}_{ENV_ID}\"\n        presto_conn = Connection(\n            conn_id=conn_id_name, conn_type=\"presto\", extra={\"host\": \"localhost\", \"port\": 8080}\n        )\n        db_conn = Role(roles=[\"admin\"], connection_id=conn_id_name)\n        db_conn.save()\n        presto_conn.delete()\n        db_conn.delete()\n\n    (\n        BashOperator(\n            task_id=\"remove_presto_env\",\n            bash_command=\"presto --stop\",\n        )\n        >> delete_presto_env()\n        >> TaskGroup(\n            task_id=\"tearDown_presto_env\",\n            tasks=[\n                BashOperator(\n                    task_id=\"stop_presto\",\n                    bash_command=\"presto --stop\",\n                ),\n            ],\n        )\n        >> BashOperator(task_id=\"remove_connections\", bash_command=\"rm -f etc/airflow/hooks/presto_hook.py\")\n    )\n    create_connection(\"presto_{DAG_ID}_{ENV_ID}\")\n    create_presto_table()\n    insert_data()\n    check_data()\n    delete_data()"}], "metadata": {"file_name": "dag_992_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Hi, I have a dataset with numerical features and a target variable that is categorical. I want to perform classification. Can you guide me on how to approach this problem using Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.cncf.kubernetes.operators.kubernetes import (\n    KubernetesDeleteClusterOperator,\n    KubernetesLongRunningOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_kubernetes]\n# This DAG will use the cluster named 'kubernetes' and in the region 'us-central1'\ncluster_name = \"kubernetes\"\nregion = \"us-central1\"\n\nwith DAG(\n    \"kubernetes_long_running\",\n    schedule=\"* * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_kubernetes_create_cluster]\n    create_cluster = KubernetesDeleteClusterOperator(\n        task_id=\"create_cluster\",\n        name=cluster_name,\n        region=region,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kubernetes_create_cluster]\n\n    # [START howto_operator_kubernetes_long_running]\n    @task\n    def some_task():\n        import time\n\n        print(\"Hello World!\")\n        time.sleep(10)\n\n    long_running = KubernetesLongRunningOperator(\n        task_id=\"long_running\",\n        name=\"job-name\",\n        image=\"perl:5.34.0\",\n        cmds=[\"sh\", \"-c\"],\n        args=\"echo Hello world! && sleep 10\",\n        namespace=\"default\",\n        cluster_name=cluster_name,\n        region=region,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kubernetes_long_running]\n\n    # [START howto_operator_kubernetes_delete_cluster]\n    delete_cluster = KubernetesDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        name=cluster_name,\n        region=region,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kubernetes_delete_cluster]\n\n    create_cluster >> some_task() >> long_running >> delete_cluster\n    # [START howto_operator_kubernetes_skip_if_exists_cluster]\n    skip_long_running = KubernetesLongRunningOperator(\n        task_id=\"skip_long_running\",\n        name=\"job-name\",\n        image=\"perl:5.34.0\",\n        cmds=[\"sh\", \"-c\"],\n        args=\"echo Hello world! && sleep 10\",\n        namespace=\"default\",\n        cluster_name=cluster_name,\n        region=region,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_kubernetes_skip_if_exists_cluster]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Is there any library available in Python for parsing and interpreting natural language instructions for date and time extraction?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_bash() -> DAG:\n    @task\n    def get_date() -> str:\n        import datetime\n\n        return str(datetime.datetime.now(tz=\"UTC\"))\n\n    @task\n    def get_date_in_quotes(date: str) -> str:\n        import datetime\n\n        return f\"echo {date} | python3 -c 'import sys,sys,datetime;print(datetime.datetime.strptime(sys.argv[1],\"%Y-%m-%d\").strftime(\"%c\"))'\"\n\n    date = get_date()\n    echo_date = get_date_in_quotes.bind(date)\n    BashOperator(task_id=\"echo_date_result\", bash_command=echo_date)\n\n\nexample_bash()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates scalable cloud data movement techniques, focusing on transferring files between cloud storage environments with robust error management and cleanup procedures. Create a workflow that shows how to programmatically manage cloud storage resources and perform reliable file transfers using Google Cloud Transfer Service.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    GcpDataTransferServiceFileSet,\n    GcpDataTransferServiceJobStatus,\n)\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceCreateJobOperator,\n    CloudDataTransferServiceDeleteJobOperator,\n    CloudDataTransferServiceGetJobOperator,\n    CloudDataTransferServiceListJobsOperator,\n    CloudDataTransferServiceUpdateJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.cloud_storage_transfer_service import (\n    CloudDataTransferJobStateSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_gcp_dts\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nJOB_NAME = f\"job_transfer_{DAG_ID}_{ENV_ID}\"\nFILE_SET_NAME = f\"file_set_{DAG_ID}_{ENV_ID}\"\nJOB_TYPE = \"FILE_SET\"\n\nTRANSFER_FILE_SET_BODY = {\n    \"fileSetSpec\": {\n        \"resourceSchemaVersion\": 1,\n        \"gcsSource\": {\n            \"cloudStorageTransferServiceId\": PROJECT_ID,\n            \"bucket\": BUCKET_NAME,\n        },\n        \"schedule\": {\"frequency\": \"ONE_TIME\"},\n    }\n}\n\nconnection = Connection(\n    conn_id=\"google_cloud_transfer\",\n    user=\"project-assigner\",\n    password=\"project-assigner-password\",\n)\n\nwith DAG(\n    DAG_ID,\n    default_args={**airflow.settings.GLOBAL_ARGS, \"google\": {\"transfer_service\": PROJECT_ID}},\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcp-dts\"],\n) as dag:\n    create_transfer_conn = GCSCreateBucketOperator(task_id=\"create_bucket_task\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_file_set():\n        conn = airflow.connect.connect(\"google_cloud_transfer\")\n        file_set_body = {\n            \"fileSetSpec\": {\n                \"resourceSchemaVersion\": 1,\n                \"gcsSource\": {\n                    \"cloudStorageTransferServiceId\": PROJECT_ID,\n                    \"bucket\": BUCKET_NAME,\n                },\n                \"schedule\": {\"frequency\": \"ONE_TIME\"},\n            }\n        }\n        response = conn.create_file_set(body=file_set_body, project_id=PROJECT_ID)\n        return response.name\n\n    file_set_id = create_file_set().output\n\n    create_job = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_job_task\",\n        job=TRANSFER_FILE_SET_BODY,\n        transfer_job_id=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n    )\n\n    # [START howto_cloud_get_job]\n    get_job = CloudDataTransferServiceGetJobOperator(\n        task_id=\"get_job_task\",\n        job_name=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n    )\n    # [END howto_cloud_get_job]\n\n    # [START howto_cloud_list_jobs]\n    list_jobs = CloudDataTransferServiceListJobsOperator(\n        task_id=\"list_jobs_task\",\n        cloud_storage_transfer_service_id=PROJECT_ID,\n    )\n    # [END howto_cloud_list_jobs]\n\n    # [START howto_cloud_update_job]\n    update_job = CloudDataTransferServiceUpdateJobOperator(\n        task_id=\"update_job_task\",\n        job_name=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        file_set_id=file_set_id,\n    )\n    # [END howto_cloud_update_job]\n\n    # [START howto_cloud_delete_job]\n    delete_job = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_job_task\",\n        job_name=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n    )\n    # [END howto_cloud_delete_job]\n\n    # [START howto_cloud_sensors_job_state]\n    check_start_job = CloudDataTransferJobStateSensor(\n        task_id=\"check_start_job\",\n        job_name=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        target_state=GcpDataTransferServiceJobStatus.State.Value(\"STATE_IN_PROGRESS\"),\n    )\n\n    check_complete_job = CloudDataTransferJobStateSensor(\n        task_id=\"check_complete_job\",\n        job_name=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        target_state=GcpDataTransferServiceJobStatus.State.Value(\"STATE_SUCCEEDED\"),\n    )\n    # [END howto_cloud_sensors_job_state]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_task\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # Setup\n    create_transfer_conn()\n    [create_file_set(), create_job()]\n    # Test\n    check_start_job()\n    check_complete_job()\n    # Clean\n    [delete_bucket(), delete_job(), delete_file_set(), delete_transfer_conn()]\n\n\nwith DAG(\n    \"example_gcp_dts_def\",\n    default_args={**airflow.settings.GLOBAL_ARGS, \"google\": {\"transfer_service\": PROJECT_ID}},\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcp-dts-def\"],\n):\n    # [START howto_gcp_ds_create_job]\n    create_job_ds = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_job_ds\",\n        job=TRANSFER_FILE_SET_BODY,\n        transfer_job_id=\"test-dts-value\",\n        cloud_storage_transfer_service_id=PROJECT_ID,\n    )\n    # [END howto_gcp_ds_create_job]\n\n    # [START howto_gcp_ds_get_job]\n    get_job_ds = CloudDataTransferServiceGetJobOperator(\n        task_id=\"get_job_ds\", job_name=\"test-dts-value\", cloud_storage_transfer_service_id=PROJECT_ID\n    )\n    # [END howto_gcp_ds_get_job]\n\n    # [START howto_gcp_ds_list_jobs]\n    list_jobs_ds = CloudDataTransferServiceListJobsOperator(task_id=\"list_jobs_ds\", cloud_storage_transfer_service_id=PROJECT_ID)\n    # [END howto_gcp_ds_list_jobs]\n\n    # [START howto_gcp_ds_update_job]\n    update_job_ds = CloudDataTransferServiceUpdateJobOperator(\n        task_id=\"update_job_ds\",\n        job_name=\"test-dts-value\",\n        transfer_job_id=\"test-dts-value\",\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        file_set_id=file_set_id,\n    )\n    # [END howto_gcp_ds_update_job]\n\n    # [START howto_gcp_ds_delete_job]\n    delete_job_ds = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_job_ds\", job_name=\"test-dts-value\", cloud_storage_transfer_service_id=PROJECT_ID\n    )\n    # [END howto_gcp_ds_delete_job]\n\n    # [START howto_gcp_ds_sensors_job_state]\n    check_start_job_ds = CloudDataTransferJobStateSensor(\n        task_id=\"check_start_job_ds\",\n        job_name=\"test-dts-value\",\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        target_state=GcpDataTransferServiceJobStatus.State.Value(\"STATE_IN_PROGRESS\"),\n    )\n\n    check_complete_job_ds = CloudDataTransferJobStateSensor(\n        task_id=\"check_complete_job_ds\",\n        job_name=\"test-dts-value\",\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        target_state=GcpDataTransferServiceJobStatus.State.Value(\"STATE_SUCCEEDED\"),\n    )\n    # [END howto_gcp_ds_sensors_job_state]\n\n    # [START howto_gcp_ds_create_file_set]\n    create_file_set_ds = GCSCreateBucketOperator(task_id=\"create_file_set_ds\", bucket_name=BUCKET_NAME)\n    # [END howto_gcp_ds_create_file_set]\n\n    # [START howto_gcp_ds_get_file_set]\n    get_file_set_ds = CloudDataTransferServiceGetJobOperator(\n        task_id=\"get_file_set_ds\", job_name=JOB_NAME, cloud_storage_transfer_service_id=PROJECT_ID\n    )\n    # [END howto_gcp_ds_get_file_set]\n\n    # [START howto_gcp_ds_list_file_sets]\n    list_file_sets_ds = CloudDataTransferServiceListJobsOperator(\n        task_id=\"list_file_sets_ds\", job_name=JOB_NAME, cloud_storage_transfer_service_id=PROJECT_ID\n    )\n    # [END howto_gcp_ds_list_file_sets]\n\n    # [START howto_gcp_ds_delete_file_set]\n    delete_file_set_ds = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_file_set_ds\", job_name=JOB_NAME, cloud_storage_transfer_service_id=PROJECT_ID\n    )\n    # [END howto_gcp_ds_delete_file_set]\n\n    # [START howto_gcp_ds_update_file_set]\n    update_file_set_ds = CloudDataTransferServiceUpdateJobOperator(\n        task_id=\"update_file_set_ds\",\n        job_name=JOB_NAME,\n        transfer_job_id=JOB_NAME,\n        cloud_storage_transfer_service_id=PROJECT_ID,\n        file_set_id=file_set_id,\n    )\n    # [END howto_gcp_ds_update_file_set]"}], "metadata": {"file_name": "dag_1290_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud storage interactions using Azure Data Lake Storage operators. Construct a DAG that demonstrates how to programmatically upload and remove files with explicit dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.operators.adlsls import ADLSLSCreateOperator\nfrom airflow.providers.microsoft.azure.operators.msgraph import MSAzureCreateResourceOperator, MSAzureDeleteResourceOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adlsls_to_adls\"\n\n\u6e56\u6ccaCONN_ID = os.environ.get(\"ADLS_CONN_ID\")\n\nconnection = Connection(\n    conn_id=\u6e56\u6ccaCONN_ID,\n    schema=\"airflow providers\",\n    conn_type=\"azure_container_instance\",\n    extra={\"resource_group\": \"myresourcegroup\", \"name\": \"myaci\"},\n)\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"adlsls-to-adls\"],\n) as dag:\n    create_adls_ls_resource = MSAzureCreateResourceOperator(\n        task_id=\"create_adls_ls_resource\",\n        resource_name=\"myadlslsresource\",\n        resource_spec={\n            \"id\": \"\",\n            \"properties\": {\n                \"folderQuotaInMB\": 0,\n                \"folderSasDuration\": {\"seconds\": 43200},\n                \"fileSasDuration\": {\"seconds\": 43200},\n                \"isDirectoryPermittedForGuestAccess\": True,\n                \"isTrashedPermittedForGuestAccess\": True,\n                \"accessLevel\": \"ReadWrite\",\n            },\n        },\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n\n    create_connection = MSAzureCreateResourceOperator(\n        task_id=\"create_connection\",\n        resource_name=\"myazureconnection\",\n        resource_spec={\n            \"id\": \"\",\n            \"properties\": {\"appId\": \"123123123123123123123123123123123\", \"password\": \"top secret!\"},\n        },\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n\n    # [START howto_operator_adlsls_create_single_file]\n    create_file_adls_ls = ADLSLSCreateOperator(\n        task_id=\"create_file_adls_ls\",\n        path=\"/myfolder/myfile.txt\",\n        overwrite=False,\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n    # [END howto_operator_adlsls_create_single_file]\n\n    # [START howto_operator_adlsls_create_multiple_files]\n    create_multiple_files_adls_ls = ADLSLSCreateOperator(\n        task_id=\"create_multiple_files_adls_ls\",\n        path=\"/mryfolder/\",\n        overwrite=True,\n        extra_fields=[{\"path\": \"/mryfolder/file1.txt\", \"etag\": \"1234567890\"}],\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n    # [END howto_operator_adlsls_create_multiple_files]\n\n    # [START howto_operator_adls_delete]\n    delete_adls_connection = MSAzureDeleteResourceOperator(\n        task_id=\"delete_adls_connection\",\n        resource_name=\"myazureconnection\",\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n\n    delete_adls_ls_resource = ADLSDeleteOperator(\n        task_id=\"delete_adls_ls_resource\",\n        path=\"/myfolder\",\n        connection=connection,\n        azure_conn_id=\u6e56\u6ccaCONN_ID,\n    )\n\n    delete_adls_ls_resource.trigger_rule = TriggerRule.ALL_DONE\n    # [END howto_operator_adls_delete]\n\n    (\n        # TEST SETUP\n        create_adls_ls_resource\n        >> create_connection\n        >> create_file_adls_ls\n        >> create_multiple_files_adls_ls\n        # TEST BODY\n        >> delete_adls_connection\n        >> delete_adls_ls_resource\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done;\n    # (there are no tasks in this task group)\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_2151_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Windows Remote Management (WinRM) task orchestration DAG that demonstrates parallel remote command execution across multiple Windows servers. Create a workflow that showcases executing different shell commands simultaneously and tracking their completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_winrm_operator\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    winrm_hook = WinRMHook(ssh_conn_id=\"winrm\")\n\n    # [START howto_operator_winrm]\n    t1 = WinRMOperator(\n        task_id=\"run_script_on_remote_host\",\n        winrm_hook=winrm_hook,\n        command=\"echo successful\",\n        to_file=\"test_script.ps1\",\n        remote_server_name=\"remote_test\",\n        ssh_user=\"username\",\n        ssh_password=\"password\",\n        env_vars={\"AN_ENV_VAR\": \"An Environment Variable\"},\n        login_interactive=True,\n    )\n    # [END howto_operator_winrm]\n\n    t2 = WinRMOperator(\n        task_id=\"run_command_on_remote_host\",\n        winrm_hook=winrm_hook,\n        command=\"echo successful\",\n        to_file=\"test_script.ps1\",\n        remote_server_name=\"remote_test\",\n        ssh_user=\"username\",\n        ssh_password=\"password\",\n        env_vars={\"AN_ENV_VAR\": \"An Environment Variable\"},\n        login_interactive=True,\n    )\n\n    t1 >> t2\n\n    # [START howto_operator_winrm_async]\n    t3 = WinRMOperator(\n        task_id=\"run_script_on_remote_host_async\",\n        winrm_hook=winrm_hook,\n        command=\"echo successful\",\n        to_file=\"test_script.ps1\",\n        remote_server_name=\"remote_test\",\n        ssh_user=\"username\",\n        ssh_password=\"password\",\n        env_vars={\"AN_ENV_VAR\": \"An Environment Variable\"},\n        login_interactive=True,\n        wait_for_completion=False,\n        is_conditional=True,\n    )\n    # [END howto_operator_winrm_async]\n\n    t3.trigger_rule = TriggerRule.ALL_DONE\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_628_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that represents a lightweight, reproducible method for executing ephemeral computational tasks using Azure Container Instances. Create a workflow that demonstrates how to trigger a container-based job with standardized resource allocation and dynamic naming conventions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateClusterOperator,\n    ContainerInstancesDeleteClusterOperator,\n    ContainerInstancesStartContainerInstancesOperator,\n    ContainerInstancesStopContainerInstancesOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"container_instances\"\n\nCLUSTER_ID = os.environ.get(\"CONTAINER_INSTANCES_CLUSTER_ID\", \"cluster-test\")\nLOCATION = os.environ.get(\"CONTAINER_INSTANCES_LOCATION\", \"us\u5357\u533a\")\n\nTRIGGER = \"all\" if os.environ.get(\"TEST_ENV_ID\") else \"one\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"container_instances\"],\n) as dag:\n    set_id = models.Variable.get(\"container_instances_cluster_id\")\n\n    create_cluster_task = ContainerInstancesCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_id=set_id,\n        location=LOCATION,\n        cluster_name=f\"cluster-{ENV_ID}\",\n        node_agent_sku_id=\"container-windows-node:latest\",\n        node_count=2,\n    )\n\n    # [START howto_operator_container_instances_start]\n    start_container_instances_task = ContainerInstancesStartContainerInstancesOperator(\n        task_id=\"start_container_instances\",\n        cluster_id=set_id,\n        filter_string=\"name In (container1, container2)\",\n    )\n    # [END howto_operator_container_instances_start]\n\n    # [START howto_operator_container_instances_stop]\n    stop_container_instances_task = ContainerInstancesStopContainerInstancesOperator(\n        task_id=\"stop_container_instances\",\n        cluster_id=set_id,\n        filter_string=\"name In (container1, container2)\",\n    )\n    # [END howto_operator_container_instances_stop]\n\n    # [START howto_operator_container_instances_delete]\n    delete_cluster_task = ContainerInstancesDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_id=set_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_container_instances_delete]\n\n    create_cluster_task >> start_container_instances_task >> stop_container_instances_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_286_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you describe how to use the `numpy` library in Python to create a random 3x3 matrix and then find its eigenvalues and eigenvectors?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport numpy as np\nimport numpy.random as npr\n\n# [START howto_operator_numpy]\n# [START howto_operator_import_numpy]\nimport numpy as np\n# [END howto_operator_import_numpy]\n\n# [START howto_operator_random_matrix]\ndef random_matrix_function():\n    \"\"\"\n    Generate a random 3x3 matrix.\n    \"\"\"\n    return npr.rand(3, 3)\n\n\n# [END howto_operator_random_matrix]\n# [END howto_operator_numpy]\n\n\n# [START howto_operator_numpy_eigen]\ndef numpy_eigen_function(A: np.ndarray):\n    \"\"\"\n    Calculate the eigenvalues and eigenvectors of a given matrix A.\n\n    Args:\n        A (np.ndarray): A square matrix for which to calculate the eigenvalues and eigenvectors.\n\n    Returns:\n        tuple: A tuple containing the eigenvalues and eigenvectors of matrix A.\n    \"\"\"\n    return np.linalg.eig(A)\n\n\n# [END howto_operator_numpy_eigen]\n\n\n# [START howto_operator_numpy_eigen_import]\nimport numpy.linalg as la\n# [END howto_operator_numpy_eigen_import]\n\n\n# [START howto_operator_numpy_eigen_usage]\ndef numpy_eigen_usage_function(A: np.ndarray):\n    \"\"\"\n    Calculate the eigenvalues and eigenvectors of a given matrix A.\n\n    Args:\n        A (np.ndarray): A square matrix for which to calculate the eigenvalues and eigenvectors.\n\n    Returns:\n        tuple: A tuple containing the eigenvalues and eigenvectors of matrix A.\n    \"\"\"\n    return la.eig(A)\n\n\n# [END howto_operator_numpy_eigen_usage]\n# [END howto_operator_numpy_eigen_usage]\n\n\nwith DAG(\n    dag_id=\"example_numpy\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n\n    task1 = PythonOperator(task_id=\"random_matrix_task\", python_callable=random_matrix_function)\n    task2 = PythonOperator(task_id=\"numpy_eigen_function\", python_callable=numpy_eigen_function)\n    task3 = PythonOperator(task_id=\"numpy_eigen_usage_task\", python_callable=numpy_eigen_usage_function)\n    task4 = PythonOperator(task_id=\"task4\", python_callable=random_matrix_function)\n    task5 = PythonOperator(task_id=\"task5\", python_callable=numpy_eigen_function)\n    task6 = PythonOperator(task_id=\"task6\", python_callable=numpy_eigen_usage_function)\n\n    task1 >> task2 >> task3 >> task4 >> task5 >> task6"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration workflow that seamlessly moves structured CSV data from cloud storage into a distributed query engine for further analysis. Set up an Airflow DAG that enables efficient, one-time data transfer between cloud storage and a Presto database table.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.presto.operators.presto import PrestoOperator\nfrom airflow.providers.s3.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\n\n# [START howto_presto_to_s3_operator]\ndef presta_to_s3_fn(instance_name: str, table: str, s3_bucket: str, s3_key: str):\n    from airflow.providers.presto.hooks.presto import PrestoHook\n\n    presto_hook = PrestoHook()\n    response = presto_hook.execute(\n        sql=(\n            \"SELECT id, name, dt\"\n            \" FROM hive.`presta_to_s3_test\"\n            \" WHERE dt = ?\"\n            \" LIMIT 100;\"\n        ),\n        parameters=(os.environ[\"PRESTA_DATE\"],),\n        instance=instance_name,\n        table=table,\n    )\n    for row in response[0]:\n        s3_create_object_operator(\n            bucket=s3_bucket,\n            key=s3_key + str(row[0]) + \".csv\",\n            data=str(row[1]) + \"\\n\" + str(row[2]),\n            replace=True,\n        )\n\n\ndef load_s3_data_to_table_fn(instance_name: str, table: str, s3_bucket: str, s3_key):\n    from airflow.providers.presto.hooks.presto import PrestoHook\n\n    presto_hook = PrestoHook()\n    response = presto_hook.execute(\n        sql=\"SELECT id, name, dt FROM hive.\" + load_s3_data_to_table_fn.__name__ + \" WHERE dt = ? LIMIT 100;\",\n        parameters=(os.environ[\"PRESTA_DATE\"],),\n        instance=instance_name,\n        table=table,\n    )\n    for row in response[0]:\n        s3_create_object_operator(\n            bucket=s3_bucket,\n            key=s3_key + str(row[0]) + \".csv\",\n            data=str(row[1]) + \"\\n\" + str(row[2]),\n            replace=True,\n        )\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_presto_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"retries\": 1,\n                 \"concurrency\": 3,\n                 \"max_active_runs\": 3},\n    tags=[\"example\"],\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"s3_bucket_create\",\n        bucket_name=f\"{ENV_ID}-s3-bucket\",\n    )\n\n    create_presto_instance = PrestoOperator(\n        task_id=\"presto_instance_create\",\n        conn_id=\"presto_default\",\n        sql=(\n            \"CREATE INSTANCE presto_to_s3_id \"\n            \"PRIMARY INDEXING USING S3 \"\n            \"PARAMETERS(\"\n            \"bucket = 's3a://airflow/system_tests/\",  # S3 bucket url\n            \"path_prefix = 'presta_to_s3/'\",  # Path prefix on S3 for path parsing\n            \"s3_scheme = 's3a';\"\n            \");\"\n        ),\n    )\n\n    presta_to_s3_task = PrestoOperator(\n        task_id=\"presta_to_s3_task\",\n        conn_id=\"presto_default\",\n        sql=\"SELECT id, name, dt FROM hive.presto_to_s3_test WHERE dt = ? LIMIT 100;\",\n        parameters=(os.environ[\"PRESTA_DATE\"],),\n        instance=\"presto_to_s3_id\",\n        table=\"presta_to_s3_test\",\n    )\n\n    create_s3_key = S3CreateObjectOperator(\n        task_id=\"s3_key_create\",\n        bucket_name=f\"{ENV_ID}-s3-bucket\",\n        key=\"presta_to_s3/\",\n        data=\"id,name,dt\\n1,Pepsi,2021-01-01\\n2,Gatorade,2021-01-02\\n3,Milk,2021-01-03\",\n        replace=True,\n    )\n\n    load_s3_data_to_table_task = PrestoOperator(\n        task_id=\"load_s3_data_to_table\",\n        conn_id=\"presto_default\",\n        sql=\"SELECT id, name, dt FROM hive.load_s3_to_table WHERE dt = ? LIMIT 100;\",\n        parameters=(os.environ[\"PRESTA_DATE\"],),\n        instance=\"presto_to_s3_id\",\n        table=\"load_s3_data_to_table\",\n    )\n\n    delete_s3_bucket = S3CreateBucketOperator(\n        task_id=\"s3_bucket_delete\",\n        bucket_name=f\"{ENV_ID}-s3-bucket\",\n    )\n\n    delete_presto_instance = PrestoOperator(\n        task_id=\"presto_instance_delete\",\n        conn_id=\"presto_default\",\n        sql=\"DROP INSTANCE IF EXISTS presto_to_s3_id;\",\n    )\n\n    create_s3_key >> presta_to_s3_task >> load_s3_data_to_table_task >> delete_s3_bucket >> delete_presto_instance\n# [END howto_presto_to_s3_operator]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_644_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I write a Python function that takes a list of integers and returns a new list with each element incremented by 2?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def increment_by_two(numbers):\n    \"\"\"\n    Takes a list of integers and returns a new list with each element incremented by 2.\n    \"\"\"\n    return [num + 2 for num in numbers]\n\n# Example usage\ninput_list = [1, 2, 3, 4, 5]\noutput_list = increment_by_two(input_list)\nprint(output_list)  # Output: [3, 4, 5, 6, 7]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates event-driven message processing using Kafka as a message broker, with tasks that produce sequential numeric data and a sensor that dynamically responds to specific numeric conditions. Implement a real-time event listener that can detect and react to numbers divisible by 3, 5, or 15.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport random\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kafka_to_sql\"\n\nkafka_conn = Connection(\n    conn_id=\"kafka_conn\",\n    conn_type=\"kafka\",\n    host=\"kafka\",\n    schema=json.dumps({\n        \"name\": \"kafka_conn\",\n        \"extra\": {\n            \"bootstrap.servers\": \"broker:29092\",\n            \"group.id\": \"test-group\",\n            \"enable.auto.commit\": \"false\",\n            \"auto.offset.reset\": \"beginning\",\n            \"security.protocol\": \"ssl\",\n            \"ssl.ca.location\": \"ca.pem\",\n            \"ssl.broker.list\": \"broker:9092\",\n            \"ssl.protocol\": \"TLS\",\n        },\n    }),\n)\n\ntable_name = \"KafkaToSqlTable\"\nsql_table = SQLExecuteQueryOperator(\n    task_id=\"create_kafka_to_sql_table\",\n    sql=f\"CREATE TABLE {table_name} (value INT)\",\n    conn_id=\"sql_conn\",\n    sql_conn_id=\"sql_conn\",\n    dag=DAG_ID,\n    trigger_rule=TriggerRule.ALL_DONE,\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\", \"kafka\"],\n) as dag:\n    @task\n    def produce_kafka_message(kafka_conn_id, topic_name, num_messages):\n        kafka_producer = SQLExecuteQueryOperator(\n            task_id=\"produce_kafka_message\",\n            sql=f\"PRODUCE {kafka_conn_id} {topic_name} (FOR RANGING (VALUES ({{%s + row_number() OVER (ORDER BY RANDOM()) - 1}})),INT)\",\n            parameters=[random.randint(0, 100)],\n            conn_id=\"sql_conn\",\n            sql_conn_id=\"sql_conn\",\n            dag=dag,\n        )\n\n    produce_kafka_messageOperator = produce_kafka_message(kafka_conn_id=\"kafka_conn\", topic_name=\"test-topic\", num_messages=3)\n\n    @task\n    def consume_kafka_message(sql_conn_id, table_name, consumer_group):\n        consumer = SQLExecuteQueryOperator(\n            task_id=\"consume_kafka_message\",\n            sql=f\"CONSUME {sql_conn_id} {table_name} (FOR RANGING (VALUES ({{%s}})),PARTITION BY (MOD(value, 3)),{consumer_group})\",\n            parameters=[random.randint(0, 1000)],\n            conn_id=\"sql_conn\",\n            sql_conn_id=\"sql_conn\",\n            dag=dag,\n        )\n\n    consume_kafka_messageOperator = consume_kafka_message(\n        sql_conn_id=\"sql_conn\", table_name=table_name, consumer_group=\"test-group\"\n    )\n\n    @task\n    def clean_up_sql_table(sql_conn_id):\n        sql_execute_query = SQLExecuteQueryOperator(\n            task_id=\"clean_up_sql_table\",\n            sql=f\"DROP TABLE IF EXISTS {sql_conn_id}\",\n            conn_id=\"sql_conn\",\n            sql_conn_id=\"sql_conn\",\n            dag=dag,\n        )\n\n    clean_up_sql_tableOperator = clean_up_sql_table(sql_conn_id=\"sql_conn\")\n\n    @task\n    def clean_up_kafka_message(kafka_conn_id):\n        kafka_producer = SQLExecuteQueryOperator(\n            task_id=\"clean_up_kafka_message\",\n            sql=f\"DELETE FROM {kafka_conn_id} WHERE event = '{{%s}}'\",\n            parameters=[random.randint(0, 1000)],\n            conn_id=\"sql_conn\",\n            sql_conn_id=\"sql_conn\",\n            dag=dag,\n        )\n\n    clean_up_kafka_messageOperator = clean_up_kafka_message(kafka_conn_id=\"kafka_conn\")\n\n    (\n        kafka_conn\n        >> sql_table\n        >> produce_kafka_messageOperator\n        >> consume_kafka_messageOperator\n        >> clean_up_sql_tableOperator\n        >> clean_up_kafka_messageOperator\n    )"}], "metadata": {"file_name": "dag_427_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated data infrastructure pipeline that demonstrates database provisioning, configuration, and data migration strategies for a cloud-based database environment. Develop a systematic approach to managing database resources with reproducible and configurable deployment steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.alibaba.cloud.operators.alinity import AlinityCreateMaterialOperator\nfrom airflow.providers.alibaba.cloud.operators.dms import MRSGoogleGCSToAlinityOperator\nfrom airflow.providers.alibaba.cloud.operators.gsheets import GSheetsToMrssqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_alinity\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    gsheets_sheet_id = os.environ.get(\"GSHEETS_SHEET_ID\")\n    gcs_bucket = os.environ.get(\"GCS_BUCKET\", \"test-gcs\")\n    alinity_project_id = os.environ.get(\"ALINITY_PROJECT_ID\")\n    alinity_resource_name = os.environ.get(\"ALINITY_RESOURCE_NAME\")\n    sql_table = os.environ.get(\"SQL_TABLE\")\n    gcs_path = f\"gs://{gcs_bucket}/alinity/materials/csv/*.csv\"\n\n    create_material = AlinityCreateMaterialOperator(task_id=\"create_material\", project_id=alinity_project_id)\n\n    # [START howto_alinity_create_material_result_parser]\n    def parser_fn(result):\n        return {\n            \"alinity_id\": result[\"alinity_id\"],\n            \"resource_name\": result[\"resource_name\"],\n            \"description\": result[\"description\"],\n        }\n\n    # [END howto_alinity_create_material_result_parser]\n    create_material_result_parser = models.ExternalOperator(\n        task_id=\"create_material_result_parser\", external_task=\"create_material\", parser_function=parser_fn\n    )\n\n    # [START howto_mrsggsheets_to_alinity]\n    csv_to_alinity = MRSGoogleGCSToMrssqlOperator(\n        task_id=\"csv_to_alinity\",\n        gcs_prefix=gcs_path,\n        sql_table=sql_table,\n        project_id=alinity_project_id,\n        resource_name=alinity_resource_name,\n    )\n    # [END mrsggsheets_to_alinity]\n\n    # [START howto_mrssql_to_alinity]\n    gsheets_to_alinity = GSheetsToMrssqlOperator(\n        task_id=\"gsheets_to_alinity\",\n        gsheets_sheet_id=gsheets_sheet_id,\n        sql_table=sql_table,\n        project_id=alinity_project_id,\n        resource_name=alinity_resource_name,\n    )\n    # [END mrssql_to_alinity]\n\n    (\n        create_material\n        >> create_material_result_parser\n        >> csv_to_alinity\n        >> gsheets_to_alinity\n    )\n\n    # [START howto_operator_trigger_rule_def]\n    start = models.TaskFlowTriggerRule(\n        task_ids=[\"create_material\", \"csv_to_alinity\", \"gsheets_to_alinity\"]\n    )\n    # [END howto_operator_trigger_rule_def]\n\n    stop = models.TaskFlowTriggerRule(task_ids=[\"end_task\"])\n\n    start >> stop\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as done\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_492_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a demonstration DAG that showcases basic task orchestration and Docker integration, showing how different types of tasks can be sequenced and executed in an Airflow workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_docker]\n\nDAG_ID = \"docker_example\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"owner\": \"dfux\",\n        \"depend_on_parent\": True,\n    },\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=TriggerRule.ALL_DONE,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_docker_installation]\n    install_tesla_app = DockerOperator(\n        docker_url=\"unix\",\n        command=\"/bin/bash -c \\\"sudo -E -H -i bash -c \"\n        '\"sudo apt update && '\n        '\"sudo apt install -y aptitude && '\n        '\"sudo aptitude install -y python3-pip && '\n        '\"sudo pip3 install requests pandas pendulum opencv-python && '\n        '\"sudo pip3 install opencv-python==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-python-opencv-contrib-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-opencv-contrib-contrib==4.5.5.64 && '\n        '\"sudo pip3 install opencv-contrib-python-opencv-contrib-opencv-contrib-headless==4.5.5.64 && '\n        '\"sudo pip3 install opencv"}], "metadata": {"file_name": "dag_1563_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that monitors a CosmosDB collection for a specific document and then inserts a new document when the condition is met. Design a workflow that demonstrates document tracking and insertion in a cloud-based NoSQL database environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.hooks.cosmos_db import CosmosDBHook\nfrom airflow.providers.microsoft.azure.operators.cosmos_db import CosmosDBCreateCollectionOperator, CosmosDBTriggerOperator\nfrom airflow.providers.microsoft.azure.utils import get_resource_id\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"cosmos_db_trigger_ddl\"\nTRIGGER_ID = f\"trigger_{DAG_ID}_{ENV_ID}\"\n\ncosmos_db_conn_id = \"cosmos_default\"\ncollection_name = \"collection_name\"\n\n# Connection to CosmosDB\ncosmos_conn = Connection(\n    conn_id=cosmos_db_conn_id,\n    auth_scheme=\"default\",\n)\n\n# Collection definition\ncosmos_create_collection_body = {\n    \"id\": collection_name,\n    \"masterKey\": {\"primaryKeyId\": \"\"},\n    \"billingPlan\": {\"planType\": \"PerSecond\"},\n}\n\n\n@task\ndef create_trigger(cosmos_hook: CosmosDBHook, trigger_id: str):\n    # Create a trigger definition\n    trigger_create_body = {\n        \"id\": trigger_id,\n        \"documents\": [\n            {\n                \"systemTrigger\": {\n                    \"startMode\": \"Once\",\n                    \"expression\": {\n                        \"language\": \"WindowsExpressions\",\n                        \"expression\": \"/t:0;s:10\",\n                        \"scope\": \"global\",\n                    },\n                    \"inputDocuments\": [\n                        {\n                            \"id\": \"input_1\",\n                            \"primaryPartitionKey\": {\"stringValue\": \"1032133efe1bf777\"},\n                        },\n                        {\n                            \"id\": \"input_2\",\n                            \"primaryPartitionKey\": {\"stringValue\": \"1032133efe1c5777\"},\n                        },\n                    ],\n                    \"resultContainer\": {\n                        \"documents\": [\n                            {\n                                \"id\": \"output_1\",\n                                \"primaryPartitionKey\": {\"stringValue\": \"1032133efe1bf777\"},\n                            },\n                            {\n                                \"id\": \"output_2\",\n                                \"primaryPartitionKey\": {\"stringValue\": \"1032133efe1c5777\"},\n                            ],\n                        ],\n                    },\n                },\n                \"triggerBehavior\": \"Once\",\n            },\n        ],\n        \"createdOnDemand\": False,\n    }\n\n    # Create the trigger\n    cosmos_hook.create_trigger(**trigger_create_body)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cosmos-db\"],\n) as dag:\n    cosmos_hook = CosmosDBHook(conn_id=cosmos_conn)\n    create_collection = CosmosDBCreateCollectionOperator(\n        task_id=\"create_collection\",\n        cosmos_hook=cosmos_hook,\n        body=cosmos_create_collection_body,\n        trigger_id=None,\n    )\n    # [START howto_operator_cosmos_create_trigger]\n    create_trigger = create_trigger(cosmos_hook, TRIGGER_ID)\n    # [END howto_operator_cosmos_create_trigger]\n    delete_trigger = CosmosDBTriggerOperator(\n        task_id=\"delete_trigger\",\n        cosmos_hook=cosmos_hook,\n        trigger_id=TRIGGER_ID,\n    )\n    delete_collection = CosmosDBCreateCollectionOperator(\n        task_id=\"delete_collection\",\n        cosmos_hook=cosmos_hook,\n        body={},\n        trigger_id=None,\n        delete_collection=True,\n    )\n\n    (\n        create_collection\n        >> create_trigger\n        >> delete_trigger\n        >> delete_collection\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1745_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer mechanisms, highlighting task chaining, cloud service interactions, and dynamic environment configuration. Develop a systematic workflow that demonstrates creating, transferring, and deleting resources across Google Cloud and AWS infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.common.azure.hooks.fileshare import AzureFileShareHook\nfrom airflow.providers.common.azure.operators.fileshare import AzureFileShareCreateOperator\nfrom airflow.providers.common.azure.operators.fileshare_sas import AzureFileShareSASOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\nDAG_ID = \"example_fileshare\"\nENV_ID = Param(str, \"ENV_ID\")\nAZURE_SHARE_NAME = Param(str, \"AZURE_SHARE_NAME\")\nAZURE_CONTAINER_NAME = Param(str, \"AZURE_CONTAINER_NAME\")\nAZURE_DIRECTORY_PATH = Param(str, \"AZURE_DIRECTORY_PATH\")\nAZURE_FILE_PATH = Param(str, \"AZURE_FILE_PATH\")\nAWS_BUCKET_NAME = Param(str, \"AWS_BUCKET_NAME\")\nAWS_FILE_PATH = Param(str, \"AWS_FILE_PATH\")\nAWS_BUCKET_ID = Param(str, \"AWS_BUCKET_ID\")\nAWS_FILE_ROLE_NAME = Param(str, \"AWS_FILE_ROLE_NAME\")\nAWS_BUCKET_ROLE_ARN = Param(str, \"AWS_BUCKET_ROLE_ARN\")\nPOLL_INTERVAL = Param(int, 5, dtype=int)\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef create_azure_connection():\n    \"\"\"\n    Creates an Azure connection object within the session.\n    \"\"\"\n    return Connection(\n        conn_id=\"azure_connection\",\n        conn_type=\"azure_fileshare\",\n        host=AZURE_SHARE_NAME,\n        schema=\"airflow\",\n    )\n\n\n@task\ndef create_aws_access_key():\n    \"\"\"\n    Creates an AWS access key within the session.\n    \"\"\"\n    return Role(\n        role_id=\"aws_access_key\",\n        role_name=\"aws_access_key\",\n        principal_list=[\n            {\n                \"RoleName\": AWS_FILE_ROLE_NAME,\n                \"Arn\": AWS_BUCKET_ROLE_ARN,\n            }\n        ],\n        permissions=[str(Param(int, \"AWS_REGION\"))],\n        schema=\"airflow\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_azure_connection(conn_id):\n    session = Session()\n    log.info(\"Cleaning up Azure connection '%s'.\", conn_id)\n    query = session.query(Connection).filter(Connection.conn_id == conn_id)\n    query.delete()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_aws_access_key(role_name):\n    session = Session()\n    log.info(\"Cleaning up AWS access key role '%s'.\", role_name)\n    query = session.query(Role).filter(Role.role_name == role_name)\n    query.delete()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n    week_days=[WeekDay.SATURDAY, WeekDay.SUNDAY],\n    params={\n        \"AWS_REGION\": \"us-east-1\",\n        \"ENV_ID\": ENV_ID,\n    },\n    # [START howto_operator_azure_fileshare_create]\n    tasks=(\n        create_azure_connection(),\n        create_aws_access_key(),\n        # [END howto_operator_azure_fileshare_create]\n        TaskInstance(task_id=\"azure_fileshare_create_1\"),\n        AzureFileShareCreateOperator(\n            task_id=\"azure_fileshare_create_2\",\n            share_name=AZURE_SHARE_NAME,\n            subscription_id=AWS_BUCKET_ID,\n            mount_dir=\"azure_share_dir\",\n            fileshare_config={\n                \"fileshare_access\": \"CONTAINER_AND_SHARE_ACCESS\",\n                \"share_properties\": {\n                    \"protocol\": \"ONLINE\",\n                    \"rights\": \"CONTAINER_READ_WRITE\",\n                },\n            },\n            check_interval=POLL_INTERVAL,\n            timeout=200,\n        ),\n        # [START howto_operator_azure_fileshare_create_deferrable]\n        TaskInstance(task_id=\"azure_fileshare_create_3\"),\n        AzureFileShareCreateOperator(\n            task_id=\"azure_fileshare_create_4\",\n            share_name=AZURE_SHARE_NAME,\n            subscription_id=AWS_BUCKET_ID,\n            mount_dir=\"azure_share_dir\",\n            fileshare_config={\n                \"fileshare_access\": \"CONTAINER_AND_SHARE_ACCESS\",\n                \"share_properties\": {\n                    \"protocol\": \"ONLINE\",\n                    \"rights\": \"CONTAINER_READ_WRITE\",\n                },\n            },\n            check_interval=POLL_INTERVAL,\n            timeout=200,\n            deferrable=True,\n        ),\n        # [END howto_operator_azure_fileshare_create]\n        # [START howto_operator_azure_fileshare_sas]\n        TaskInstance(task_id=\"azure_fileshare_sas\"),\n        AzureFileShareSASOperator(\n            task_id=\"azure_fileshare_url\",\n            share_name=AZURE_SHARE_NAME,\n            container_name=AZURE_CONTAINER_NAME,\n            directory_path=AZURE_DIRECTORY_PATH,\n            file_path=AZURE_FILE_PATH,\n            expiry=Param(datetime(2021, 1, 15), \"EXPIRY_DATE\"),\n            conn_id=\"azure_connection\",\n            scope=\"Container+Share\",\n        ),\n        # [END howto_operator_azure_fileshare_sas]\n        # [START howto_operator_azure_fileshare_sas_deferrable]\n        TaskInstance(task_id=\"azure_fileshare_sas_deferrable\"),\n        AzureFileShareSASOperator(\n            task_id=\"azure_fileshare_url_deferrable\",\n            share_name=AZURE_SHARE_NAME,\n            container_name=AZURE_CONTAINER_NAME,\n            directory_path=AZURE_DIRECTORY_PATH,\n            file_path=AZURE_FILE_PATH,\n            expiry=Param(datetime(2021, 1, 15), \"EXPIRY_DATE\"),\n            conn_id=\"azure_connection\",\n            scope=\"Container+Share\",\n            deferrable=True,\n        ),\n        # [END howto_operator_azure_fileshare_sas_deferrable]\n        TriggerTrigger(task_id=\"aws_access_key_trigger\", trigger_id=\"aws_access_key_role_1),\n        # [START howto_operator_create_aws_access_key_role]\n        TaskInstance(task_id=\"aws_access_key_trigger\"),\n        Role(\n            task_id=\"aws_create_access_key\",\n            role_name=\"aws_access_key_role_2\",\n            principal_list=[\n                {\n                    \"RoleName\": \"aws_access_key_role_2\",\n                    \"Arn\": AWS_BUCKET_ROLE_ARN,\n                }\n            ],\n            permissions=[str(Param(int, \"AWS_REGION\"))],\n            schema=\"airflow\",\n        ),\n        # [END howto_operator_create_aws_access_key_role]\n        # [START howto_operator_delete_aws_access_key_role]\n        TaskInstance(task_id=\"aws_access_key_role_1\"),\n        delete_aws_access_key(\"aws_access_key_role_1\"),\n        # [END howto_operator_delete_aws_access_key_role]\n        # [START howto_operator_delete_azure_connection]\n        TaskInstance(task_id=\"aws_access_key_role_2\"),\n        delete_azure_connection(\"azure_connection\"),\n        # [END howto_operator_delete_azure_connection],\n    ),\n    schedule_interval=datetime(2021, 1, 1),\n) as dag:\n    pass"}], "metadata": {"file_name": "dag_734_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that retrieves the current server's external IP address and sends an email notification about the IP. Design a workflow that automatically tracks and communicates server connection details.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.email.operators.email import EmailOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mysql\"\n\nSQL_CONN_ID = \"mysql_default\"\n\nIP_TABLE = \"ip_table\"\n\nip_message = \"\"\"\nThe IP address of the server is: {}\n\nYou can use this IP address to communicate with the server.\n\"\"\"\n\n\n@task\ndef get_public_ip():\n    \"\"\"\n    Retrieving public IP address\n    \"\"\"\n    return \"35.231.183.226\"\n\n\n@task\ndef create_ip_table():\n    connection = Connection(\n        conn_id=SQL_CONN_ID,\n        conn_type=\"mysql\",\n        schema=\"mysql_default\",\n        login=\"airflow\",\n        password=\"airflow\",\n        host=\"mysql_default\",\n        port=3306,\n    )\n\n    sql = f\"CREATE TABLE IF NOT EXISTS {IP_TABLE} (ip_id INT AUTO_INCREMENT PRIMARY KEY, ip_address VARCHAR(255));\"\n\n    sql_execute = SQLExecuteQueryOperator(\n        task_id=\"create_ip_table\",\n        sql=sql,\n        check_sql=f\"SELECT * FROM {IP_TABLE}\",\n        conn_id=SQL_CONN_ID,\n        schema=\"mysql_default\",\n    )\n\n    ip_address = get_public_ip()\n\n    send_email = EmailOperator(\n        task_id=\"send_email\",\n        to=\"airflow@example.com\",\n        subject=\"IP Address of the Server\",\n        html_content=ip_message.format(ip_address),\n        conn_id=\"email_default\",\n    )\n\n    ip_address_insert = SQLExecuteQueryOperator(\n        task_id=\"ip_address_insert\",\n        sql=f\"INSERT INTO {IP_TABLE} (ip_address) VALUES ('{ip_address}');\",\n        check_sql=\"SELECT * FROM ip_table\",\n        conn_id=SQL_CONN_ID,\n        schema=\"mysql_default\",\n    )\n\n    # Set dependencies.\n    sql_execute >> send_email >> ip_address_insert\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    get_public_ip()\n    create_ip_table()\n    BashOperator(task_id=\"run_every_1s\", bash_command=\"sleep 1\")"}], "metadata": {"file_name": "dag_673_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated workflow that demonstrates provisioning an Amazon EKS cluster with Fargate compute resources and running a simple test pod. Design a DAG that showcases the end-to-end process of creating, testing, and cleaning up an EKS cluster infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, Task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import KubernetesClusterStates, KubernetesNodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksDeployApplicationOperator,\n    EksGetApplicationOperator,\n    EksListApplicationOperator,\n    EksStartApplicationOperator,\n    EksStopApplicationOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().add_variable_key(env_id_key=ENV_ID_KEY).build()\n\nDAG_ID = \"example_eks_with_kubernetes\"\n\n# [START howto_operator_eks_create_cluster]\n@task\ndef create_eks_cluster(cluster_name: str, cluster_role_arn: str, vpc_subnet_ids: list, vpc_security_group_ids: list):\n    # Create an Amazon EKS Cluster. This cluster is in the US East (Amazon AWS) region,\n    # and it has two nodegroups: one named test-1 with 3 nodes, and another named test-2\n    # with 1 node. The first nodegroup is configured to use AWS provided betas, and the\n    # second nodegroup is configured to use a cluster managed endpoint.\n    boto3.client(\"eks\").create_cluster(\n        cluster_name=cluster_name,\n        vpc_subnets=vpc_subnet_ids,\n        vpc_security_groups=vpc_security_group_ids,\n        resources_vpc_config={\n            \"subnetId\": vpc_subnet_ids,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": True,\n        },\n        # See https://docs.aws.amazon.com/eks/latest/userguide/beta.html\n        # for information about specifying beta settings.\n        # For this nodegroup we will use a cluster managed endpoint so we need to set\n        # this to true.\n        create_cluster_role_arn=cluster_role_arn,\n        # See https://docs.aws.amazon.com/eks/latest/userguide/using-clusters.html\n        # for information about specifying cluster settings.\n        resources={\n            \"clusterManagement\": True,\n            \"compute\": {\n                \"fargate\": {\n                    # The number of nodes to create in the nodegroup.  Here we create two\n                    # nodegroups.  One nodegroup has 3 nodes specified under FARGATE_POD_COUNT.\n                    # The other nodegroup has 1 node specified under FARGATE_POD_COUNT.\n                    \"count\": 2,\n                }\n            },\n        },\n        # Here we create a nodegroup within the cluster named test-1.\n        nodegroup_name=f\"{cluster_name}-nodegroup-1\",\n        # Here we configure the nodegroup to use the cluster managed endpoint and to\n        # communicate over the private subnet with VPC endpoint.\n        nodegroup_resources={\n            # See https://docs.aws.amazon.com/eks/latest/userguide/nodegroups.html\n            # For information about specifying nodegroup settings.\n            \"clusterManaged\": True,\n            \"subnets\": vpc_subnet_ids,\n            \"securityGroupIds\": vpc_security_group_ids,\n            \"nodegroupName\": f\"{cluster_name}-nodegroup-1\",\n            \"resources\": {\n                \"compute\": {\n                    \"fargate\": {\n                        \"cpu\": 1,\n                        \"memory\": 1,\n                        # Here we specify the endpoint to use for the Fargate pods running\n                        # on these nodes.  We specify the endpoint to use a VPC endpoint\n                        # rather than using a public IP because this cluster and nodegroup\n                        # are in the same AWS Region.  If the nodes are in a different\n                        # AWS Region then the IP assigned to the node will come from\n                        # the Internet and need to be behind a security group allowing\n                        # access to the Amazon Kubernetes Engine Fargate betas.\n                        \"endpoints\": {\n                            \"publicVirtualPrivateCloud\": {\n                                \"subnetIds\": vpc_subnet_ids,\n                            },\n                            \"privateVirtualPrivateCloud\": {\n                                \"subnetIds\": vpc_subnet_ids,\n                            },\n                        },\n                    }\n                }\n            },\n        },\n        # Similarly, we create a second nodegroup with one node and specify a different\n        # endpoint type of \"aws-elastic-ip\" so that we can test outbound access to the\n        # internet by the pods running on these nodes.\n        nodegroup_name=f\"{cluster_name}-nodegroup-2\",\n        nodegroup_resources={\n            \"clusterManaged\": True,\n            \"subnets\": vpc_subnet_ids,\n            \"securityGroupIds\": vpc_security_group_ids,\n            \"nodegroupName\": f\"{cluster_name}-nodegroup-2\",\n            \"resources\": {\n                \"compute\": {\n                    \"fargate\": {\n                        \"cpu\": 1,\n                        \"memory\": 1,\n                        \"endpoints\": {\n                            \"awsElasticIp\": {\n                                \"subnetIds\": vpc_subnet_ids,\n                                \"securityGroupIds\": vpc_security_group_ids,\n                            },\n                        },\n                    }\n                }\n            },\n        },\n    )\n\n\n# [END howto_operator_eks_create_cluster]\n\n# [START howto_operator_eks_create_nodegroup]\n@task\ndef create_eks_nodegroup_cluster(nodegroup_name: str, cluster_role_arn: str):\n    # Create an Amazon EKS Cluster with two nodegroups.  One nodegroup will use a\n    # cluster managed endpoint and will use a private subnet with VPC endpoint.\n    # The other nodegroup will use an endpoint over the internet, and will use\n    # a subnet assigned an AWS-EC2-IP-ENI security group.\n    boto3.client(\"eks\").create_nodegroup(\n        nodegroup_name=nodegroup_name,\n        nodegroup_role_arn=cluster_role_arn,\n        # Here we create a nodegroup within the cluster named test-nodegroup\n        nodegroup_name=f\"{cluster_name}-nodegroup\",\n        # Here we configure the nodegroup to use a cluster managed endpoint and to\n        # communicate over the private subnet with VPC endpoint.\n        nodegroup_resources={\n            \"clusterManaged\": True,\n            \"subnets\": vpc_subnet_ids,\n            \"securityGroupIds\": vpc_security_group_ids,\n        },\n        # Similarly, we create a second nodegroup with one node and specify a different\n        # endpoint type of \"aws-elastic-ip\" so that we can test outbound access to the\n        # internet by the pods running on these nodes.\n        nodegroup_name=f\"{cluster_name}-nodegroup2\",\n        nodegroup_resources={\n            \"clusterManaged\": True,\n            \"subnets\": vpc_subnet_ids,\n            \"securityGroupIds\": [\n                *vpc_security_group_ids,\n                # See the following link for information on getting the IPsec\n                # security group ID.\n                # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-amis.html\n                # Notice the IPSEC- word in the security group name\n                f\"{cluster_name}-ipsec-egress\",\n            ],\n        },\n    )\n\n\n# [END howto_operator_eks_create_nodegroup]\n\n# [START howto_operator_eks_deploy_application]\n@task\ndef deploy_application(cluster_name: str, nodegroup_name: str):\n    # Deploy a sample pod in the default namespace\n    # You need to install the Amazon EKSctl first\n    # https://docs.aws.amazon.com/eks/latest/userguide/using-eks-with-k3s.html\n    # and configure your environment\n    # https://docs.aws.amazon.com/eks/latest/userguide/using-k3s.html\n    # and launch the example pod test-pi.sh\n    # https://github.com/awsdocs/amazon-eks-best-practices/blob/main/examples/using-k3s-with-pi.md\n    # You need to have k3s installed and configured\n    # on the same AWS Region as your cluster\n    # and the security group of your cluster\n    # to allow access to the sample pod\n    # https://github.com/awsdocs/amazon-eks-best-practices/blob/main/examples/using-k3s-with-pi.md\n    # The sample pod will delete on its own\n    # https://github.com/awsdocs/amazon-eks-best-practices/blob/main/examples/using-k3s-with-pi.md\n    # After deploying this task will delete the application\n    # You need to have k3s installed and configured\n    # on the same AWS Region as your cluster\n    # and the security group of your cluster\n    # to allow access to the sample pod\n    # https://github.com/awsdocs/amazon-eks-best-practices/blob/main/examples/using-k3s-with-pi.md\n    # The sample pod will delete on its own\n    # https://github.com/awsdocs/amazon-eks-best-practices/blob/main/examples/using-k3s-with-pi.md\n    # Takes about 1 minute for the pod to be created and deleted\n    subprocess.run([\"k3s\", \"taint\", \"--namespace=default\", \"no-pi:NoValue\"], check=True)\n    # This command takes about 1 minute to run\n    # It creates a Linux test-pi.sh pod in the default namespace\n    # The pod will run the script and output the pi"}], "metadata": {"file_name": "dag_1481_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function to find the maximum possible sum of a subarray within a given array of integers. The array can contain both positive and negative numbers.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def max_subarray_sum(nums):\n    \"\"\"\n    Find the maximum possible sum of a subarray within the given array of integers.\n    \"\"\"\n    if not nums:\n        return 0\n\n    max_sum = current_sum = nums[0]\n    for num in nums[1:]:\n        current_sum = max(num, current_sum + num)\n        max_sum = max(max_sum, current_sum)\n    return max_sum\n\n# Example usage\nprint(max_subarray_sum([-2, 1, -3, 4, -1, 2, 1, -5, 4]))  # Output: 6"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated machine learning pipeline for sales forecasting using Google Vertex AI that transforms raw sales data into predictive models. Create a workflow that can ingest time-series data, train a forecasting model, and generate batch predictions for business planning.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "import os\nimport time\n\nimport pytest\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import CreateAutoMLForecastingTrainingJobOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import CreateDatasetOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.google.cloud.operators.vertex_ai import AutoMLTrainingJobToVertexPipelineOperator\nexcept ImportError:\n    # remove in Airflow 2.9\n    pytest.skip(\"ML API removed auto-mappings AutoMLTrainingJobToVertexPipelineOperator\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"vertex_ai_auto_ml_training_job_to_vertex_pipeline\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nCSV_FILE = \"sales.csv\"\n\nDATA_SAMPLE_GCS_BUCKET = f\"gs://{BUCKET_NAME}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDATASET = {\n    \"display_name\": DATASET_NAME,\n    \"metadata_schema_uri\": \"google.cloud.vizier::metadata\",\n    \"metadata\": {\n        \"info_piece\": {\n            \"input_config\": {\n                \"input_variables\": [\n                    {\"name\": \"sales_date\", \"type\": \"STRING\"},\n                    {\"name\": \"sales\", \"type\": \"FLOAT\"},\n                ]\n            },\n            \"training_config\": {\n                \"target_variable\": \"sales\",\n                \"time_column\": \"sales_date\",\n                \"time_format\": \"DAY\",\n                \"time_unit\": \"DAY\",\n                \"split_column\": None,\n                \"split_format\": None,\n                \"split_unit\": None,\n            },\n        }\n    },\n}\n\nforecasting_training_job_params = {\n    \"display_name\": f\"forecasting_training_job_{DAG_ID}_{ENV_ID}\",\n    \"image_size\": 1,\n    \"input_config\": {\"gcs_source\": {\"uri\": DATA_SAMPLE_GCS_BUCKET}},\n    \"target_variable\": \"sales\",\n    \"time_column_name\": \"sales_date\",\n    \"time_unit\": \"DAY\",\n    \"time_series_split_column_name\": \"sales_date\",\n    \"time_series_split_format\": \"DAY\",\n    \"time_series_split_unit\": \"DAY\",\n    \"target_split_column_name\": None,\n    \"target_split_format\": None,\n    \"target_split_unit\": None,\n    \"forecast_horizon\": 1,\n    \"model_display_name\": f\"forecasting_training_pipeline_{DAG_ID}_{ENV_ID}\",\n    \"forecasting\": {\"window_size\": 7},\n}\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"owner\": \"airflow\",\n        \"depends_on_past\": False,\n        \"retries\": 1,\n        \"retry_delay\": timeedelta(seconds=5),\n    },\n    schedule=\"@once\",\n    start_date=\"2021/01/01\",\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\", \"training_job\", \"to_vertex_pipeline\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_dataset():\n        conn = Connection(\n            conn_id=\"google_cloud_platform\",\n            project_id=PROJECT_ID,\n        )\n        client = conn.create_client()\n        return client\n\n    create_dataset_task = create_dataset()\n\n    create_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(task_id=\"forecasting_task\", **forecasting_training_job_params)\n\n    # [START howto_vertex_ai_create_training_job_to_vertex_pipeline_operator]\n    create_training_job_to_vertex_pipeline = AutoMLTrainingJobToVertexPipelineOperator(\n        task_id=\"training_job_to_vertex_pipeline_task\",\n        dataset_id=DATASET_NAME,\n        model_display_name=\"test_pipeline_model\",\n        image_size=1,\n        model_type=\"FORECASTING\",\n        python_file=\"vertex_ai_automl_training_job_to_vertex_operator_example.py\",\n    )\n    # [END howto_vertex_ai_create_training_job_to_vertex_pipeline_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_dataset_task\n        >> create_forecasting_training_job\n        # TEST BODY\n        >> create_training_job_to_vertex_pipeline\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / mark it as done\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_932_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Please write a Python function that takes a list of lists (a 2D list) and returns the longest sub-list in terms of the number of elements. If there are multiple sub-lists with the same maximum length, the function should return the first one encountered. For example, if the input is `[[1, 2, 3], [4, 5], [6, 7, 8, 9], [10, 11, 12]]`, the function should return `[6, 7, 8, 9]`.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def find_longest_sublist(matrix):\n    \"\"\"\n    Finds the longest sub-list in a 2D list (matrix) based on the number of elements.\n    If there are multiple sub-lists with the same maximum length, returns the first one.\n    \"\"\"\n    if not matrix:\n        return []\n    \n    longest = []\n    max_length = 0\n    \n    for sublist in matrix:\n        if len(sublist) > max_length:\n            max_length = len(sublist)\n            longest = [sublist]\n        elif len(sublist) == max_length:\n            longest.append(sublist)\n    \n    return longest\n\n# Example usage\nexample_matrix = [\n    [1, 2, 3],\n    [4, 5],\n    [6, 7, 8, 9],\n    [10, 11, 12]\n]\nprint(find_longest_sublist(example_matrix))  # Output: [[6, 7, 8, 9]]"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "The user claims there is a 'globalmon' module, but it does not seem to be part of any official Python libraries or widely recognized third-party packages. It could be a custom or private module. How can I help the user determine if they have access to this module and how it can be used?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.singularity.sensors.sif import SIFSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_sif_sensor\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_singularity_sif_sensor]\n    t1 = SIFSensor(image=\"hello-world\", task_id=\"singularity_sif_sensor\")\n    # [END howto_operator_singularity_sif_sensor]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that automates the process of preparing, training, and managing a text classification dataset using cloud-based machine learning infrastructure. Create a systematic workflow that handles data synchronization, dataset import, model training, and resource cleanup in a single, orchestrated pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.nlp import CloudNaturalLanguageTextClassifierCreateOperator\nfrom airflow.providers.google.cloud.sensors.nlp import CloudNaturalLanguageTextClassifierSentimentSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_text_classifier\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nTEXT = \"Here's some sample text to classify! It's very informative.\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule_interval=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    text_classifier_create_task = CloudNaturalLanguageTextClassifierCreateOperator(\n        task_id=\"text_classifier_create_task\",\n        project_id=PROJECT_ID,\n        body={\"pleaseStoreThisTextAsASentenceLabelFile\": \"false\", \"classification_model_spec\": {\"type\": \"MULTI_CLASS\"}},\n    )\n\n    # [START howto_cloud_natural_language_text_classifier_async_sensor]\n    check_sentiment = CloudNaturalLanguageTextClassifierSentimentSensor(\n        task_id=\"check_sentiment\", text_classifier_id=text_classifier_create_task.output\n    )\n    # [END howto_cloud_natural_language_text_classifier_async_sensor]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> text_classifier_create_task\n        >> check_sentiment\n        >> delete_bucket\n    )\n\n    # Task dependencies created via `XComArgs` are not used in this example but might be in a real world use case.\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_927_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Write a Python function that calculates the sum of the squares of the first n odd natural numbers.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def sum_of_squares_of_odds(n):\n    \"\"\"\n    Calculate the sum of the squares of the first n odd natural numbers.\n    \n    Args:\n    n (int): The number of odd natural numbers to consider.\n    \n    Returns:\n    int: The sum of the squares of the first n odd natural numbers.\n    \"\"\"\n    return sum(i**2 for i in range(1, n+1) if i % 2 != 0)\n\n# Example usage\nprint(sum_of_squares_of_odds(5))  # Output: 35"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow workflow that showcases asynchronous batch processing using OpenAI's API, with dynamic task generation for multiple Pok\u00e9mon descriptions and deferred execution. Implement a scalable pattern for sending multiple AI generation requests and managing the batch processing lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import chain\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_openai]\ndef call_openai(*args, **kwargs):\n    logging.info(\"Calling OpenAI with args %s and kwargs %s.\", args, kwargs)\n    return {\"output\": \"Here are the Pok\u00e9mon names I generated for you:\", \"error\": \"\"}\n\n\n@task.openai_call(task_id=\"openai_task\", model=\"text-davinci-003\", max_tokens=500)\ndef openai_task(*args, **kwargs):\n    return {\"output\": \"Here are the Pok\u00e9mon names I generated for you:\", \"error\": \"\"}\n\n\n@task\ndef clean_logs(logs):\n    logging.info(\"Cleaning logs\")\n    for line in logs:\n        logging.info(line)\n        if \"Calling OpenAI with args\" in line:\n            break\n\n\n@task\ndef save_logs_to_file(logs, dag_id, task_instance_id):\n    dir = os.path.dirname(task_instance_id)\n    file_path = os.path.join(dir, \"openai_response.log\")\n    with open(file_path, \"w\") as file:\n        for line in logs:\n            file.write(line)\n\n\nwith DAG(\n    dag_id=\"example_openai\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"my_openai_default\",\n        conn_type=\"openai\",\n        extra=json.dumps({\"api_version\": \"2023-05-15\"}),  # OpenAI API version\n    )\n\n    run_this = BashOperator(\n        task_id=\"run_this\",\n        bash_command=\":\",\n        params={\"conn_id\": conn.conn_id},\n    )\n\n    clean_logs(run_this)\n    save_logs_to_file(run_this)\n    chain(\n        # [START howto_operator_openai_call]\n        openai_task(),\n        # [END howto_operator_openai_call]\n    )\n\n    ti = TaskInstance(dag=dag, task_id=\"run_this\")\n    ti.xcom_push(key=\"openai_response\", value=\"Response stored in xcom\")"}], "metadata": {"file_name": "dag_1371_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced translation task orchestration using cloud services, with clear task dependencies and resource management. Develop a workflow that creates a temporary bucket, performs translations, and then cleanly deletes the bucket after processing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.translate_v3 import TranslateV3Hook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.translate_v3 import TranslateTextOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"gcp_translate_v3\"\n\nBUCKET_NAME = f\"test-translate-v3-bucket-{ENV_ID}\"\nFILE_NAME = \"example_translate_v3_file.txt\"\nGCS_CONN_ID = \"google_cloud_default\"\n\n# [START howto_translate_v3_gcp_conn_id]\nGCS_CONN_ID = \"google_cloud_translation\"\n# [END howto_translate_v3_gcp_conn_id]\n\n# [START howto_translate_body]\nbody = {\n    \"source_language_code\": \"en\",\n    \"target_language_code\": \"fr\",\n    \"texts\": [\"This is a sample file with text to translate.\", \"Our machine translation model is very helpful for translating\"],\n}\n# [END howto_translate_body]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"translate_v3\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    @task\n    def create_translation_gcs_conn():\n        translation_v3_conn = Connection(\n            conn_id=\"google_cloud_translation\",\n            user=os.environ.get(\"GCP_TRANSLATE_V3_USER\"),\n            password=os.environ.get(\"GCP_TRANSLATE_V3_PASSWORD\"),\n        )\n        translate_v3_conn = translation_v3_conn\n        return translate_v3_conn\n\n    create_translation_gcs_conn_task = create_translation_gcs_conn()\n\n    move_file_to_bucket = GCSToGCSOperator(\n        task_id=\"move_file_to_bucket\",\n        source_bucket=GCS_CONN_ID,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    translate_text = TranslateTextOperator(\n        task_id=\"translate_text\",\n        body=body,\n        gcp_translations_conn_id=GCS_CONN_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_translation_gcs_conn_task\n        >> move_file_to_bucket\n        # TEST BODY\n        >> translate_text\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1698_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Hello, how can I create a Python script that can handle HTTP POST requests to receive data and store it in a local JSON file?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef create_post_to_db(post: dict):\n    conn = Session.get_connection(conn_id=\"airflow\")\n    role = Role.get(conn_id=\"airflow\")\n    ti = TaskInstance(task=conn, row=post, role=role, dttm=datetime.now())\n    ti.execute()\n\n\n@task\ndef create_http_request(post_data: str):\n    log.info(\"Sending HTTP request to /hello with %s\", post_data)\n    log.info(\"Expecting payload of: {'name': 'guest'}\")\n\n\n@task\ndef save_json_to_file(json_string: str):\n    directory = os.path.dirname(os.path.abspath(__file__))\n    full_path = os.path.join(directory, \"example_post.json\")\n    with open(full_path, \"w\") as file:\n        file.write(json_string)\n\n\nwith DAG(\n    dag_id=\"example_http_operator,\n    schedule=None,\n    start_date=datetime(2022, 2, 2),\n    tags=[\"example\"],\n) as dag:\n    create_post_to_db_task = create_post_to_db({\"name\": \"guest\"})\n    create_http_request_task = create_http_request(\"{'name': 'guest'}\")\n    save_json_to_file_task = save_json_to_file(\"{'name': 'guest'}\")\n\n    BashOperator(task_id=\"create_post_to_db\", bash_command=f\"create_post_to_db_task({{{'name': 'guest'}}}})\")\n    BashOperator(task_id=\"create_http_request\", bash_command=f\"create_http_request_task({{{'post_data': 'post_data'}}}})\")\n    BashOperator(task_id=\"save_json_to_file\", bash_command=f\"save_json_to_file_task({{{'json_string': 'json_string'}}}})\")"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-DAG workflow that demonstrates dependency management between two related data processing pipelines, where one DAG triggers and monitors the execution of tasks in another DAG. Create a parent-child DAG relationship that showcases how external task dependencies can be established and monitored in Airflow.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"Prints the date\"\n    print(pendulum.datetime.now())\n\n\n@task(task_id=\"print_date_and_sum\")\ndef print_date_and_sum():\n    \"Prints the date and the result of task_id1\"\n    print(pendulum.datetime.now())\n    print(XComArg(task_id=\"print_date_and_sum_t\", key=\"return_value\"))\n\n\nwith DAG(\n    dag_id=\"example_external_dag\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n):\n    print_date()\n    print_date_and_sum()"}], "metadata": {"file_name": "dag_13_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "How can I create a Python script to check the status of a website?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.singularity.sensors.singularity import SingularitySensor  # type: ignore\nexcept ImportError:\n    print(\n        \"This example DAG requires an installed Airflow version greater than 2.7. \"\n        \"Please upgrade to the latest version and try again.\"\n    )\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_sensor_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    img = \"docker://nginx:1.1.4\"\n\n    # [START howto_operator_singularity]\n    t1 = SingularityOperator(\n        command=\"/bin/sleep 10\",\n        image=img,\n        task_id=\"singularity_example\",\n    )\n    # [END howto_operator_singularity]\n\n    # [START howto_sensor_singularity]\n    t2 = SingularitySensor(\n        command=\"/bin/sleep 10\",\n        image=img,\n        task_id=\"singularity_sensor_example\",\n    )\n    # [END howto_sensor_singularity]\n\n    t1 >> t2\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud service integration by creating a temporary GCP connection, uploading files to a Google Cloud Storage bucket, and then transferring those files to Google Drive using different configuration options. Implement a workflow that demonstrates file transfer techniques across cloud platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskInstance, DAG\nfrom airflow.providers.common.azure.hooks.adls import AzureFileShareHook\nfrom airflow.providers.common.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.common.azure.operators.bicep import BicepOperator\nfrom airflow.providers.common.azure.transfers.gcs_to_adls import GCSToADLSTaskOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_PROJECT_ID\")\nDAG_ID = \"example_gcs_to_adls\"\n\nRESOURCES_BUCKET_NAME = os.environ.get(\"GCP_RESOURCES_BUCKET_NAME\")\nADLS_PATH = os.environ.get(\"GCP_ADLS_PATH\", \"test.adls\")\nBICEP_FILE = os.environ.get(\"GCP_BICEP_FILE\", \"bicep/create-storage-in-proj.bicep\")\n\nconn = Connection(\n    conn_id=\"azure_default\",\n    conn_type=\"azure_arm\",\n    schema={\n        \"resource_group_name\": \"mygroup\",\n        \"resource_provider_type\": \"Microsoft.Azure.ActiveDirectory/v2.0\",\n        \"application_id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n        \"application_secret\": \"YmFkZTphcHAyMjIzZjJkZmQzZjUzZDkzZDc1ZDc0ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc=\",\n    },\n)\n\nrole = Role(role_id=\"test_role\", conn_id=\"azure_default\", user_id=\"a1e2c83d-25f9-488c-961c-62b16686d9b8\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_bicep_file():\n        conn_id = \"azure_default\"\n        resource_group_name = \"mygroup\"\n        storage_resource = {\n            \"resource_group_name\": resource_group_name,\n            \"location\": \"australia south\",\n            \"properties\": {\n                \"id\": \"d7863641-b8e1-40f3-986c-1a6694dd26f9\",\n                \"name\": \"mystorage\",\n                \"properties\": {\n                    \"account_type\": \"Standard\",\n                    \"service_principal\": {\n                        \"id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                        \"key_id\": \"6329b03d-ebc6-46b2-86d7-ebc686d79d3c\",\n                        \"resource_id\": \"d7863641-b8e1-40f3-986c-1a6694dd26f9\",\n                        \"user_name\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\"\n                    },\n                },\n            },\n        }\n\n        with open(BICEP_FILE) as f:\n            from airflow.providers.microsoft.azure.transfers.gcs_to_adls import GCSToADLSTaskOperator\n\n            gcs_to_adls_task = GCSToADLSTaskOperator(\n                task_id=\"gcs_to_adls_task\",\n                gcs_bucket=RESOURCES_BUCKET_NAME,\n                adls_path=ADLS_PATH,\n                storage_resource=storage_resource,\n                replace=True,\n            )\n\n        gcs_to_adls_task\n\n    @task\n    def create_connection():\n        conn_id = \"azure_default\"\n        resource_group_name = \"mygroup\"\n        storage_resource = {\n            \"resource_group_name\": resource_group_name,\n            \"location\": \"australia south\",\n            \"properties\": {\n                \"id\": \"d7863641-b8e1-40f3-986c-1a6694dd26f9\",\n                \"name\": \"mystorage\",\n                \"properties\": {\n                    \"account_type\": \"Standard\",\n                    \"service_principal\": {\n                        \"id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                        \"key_id\": \"6329b03d-ebc6-46b2-86d7-ebc686d79d3c\",\n                        \"resource_id\": \"d7863641-b8e1-40f3-986c-1a6694dd26f9\",\n                        \"user_name\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\"\n                    },\n                },\n            },\n        }\n\n        with open(BICEP_FILE) as f:\n            from airflow.providers.microsoft.azure.transfers.gcs_to_adls import GCSToADLSTaskOperator\n\n            gcs_to_adls_task = GCSToADLSTaskOperator(\n                task_id=\"gcs_to_adls_task\",\n                gcs_bucket=RESOURCES_BUCKET_NAME,\n                adls_path=ADLS_PATH,\n                storage_resource=storage_resource,\n                replace=True,\n            )\n\n        from airflow.models import Connection\n\n        conn = Connection(\n            conn_id=conn_id,\n            conn_type=\"azure_arm\",\n            schema={\n                \"resource_group_name\": resource_group_name,\n                \"resource_provider_type\": \"Microsoft.Azure.ActiveDirectory/v2.0\",\n                \"application_id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                \"application_secret\": \"YmFkZTphcHAyMjIzZjJkZmQzZjUzZDkzZDc1ZDc0ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc=\",\n            },\n        )\n        conn2 = Connection(\n            conn_id=\"azure_default2\",\n            conn_type=\"azure_arm\",\n            schema={\n                \"resource_group_name\": resource_group_name,\n                \"resource_provider_type\": \"Microsoft.Azure.ActiveDirectory/v2.0\",\n                \"application_id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                \"application_secret\": \"YmFkZTphcHAyMjIzZjJkZmQzZjUzZDkzZDc1ZDc0ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc=\",\n            },\n        )\n        conn22 = Connection(\n            conn_id=\"azure_default3\",\n            conn_type=\"azure_arm\",\n            schema={\n                \"resource_group_name\": resource_group_name,\n                \"resource_provider_type\": \"Microsoft.Azure.ActiveDirectory/v2.0\",\n                \"application_id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                \"application_secret\": \"YmFkZTphcHAyMjIzZjJkZmQzZjUzZDkzZDc1ZDc0ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc=\",\n            },\n        )\n        conn3 = Connection(\n            conn_id=\"azure_default4\",\n            conn_type=\"azure_arm\",\n            schema={\n                \"resource_group_name\": resource_group_name,\n                \"resource_provider_type\": \"Microsoft.Azure.ActiveDirectory/v2.0\",\n                \"application_id\": \"a1e2c83d-25f9-488c-961c-62b16686d9b8\",\n                \"application_secret\": \"YmFkZTphcHAyMjIzZjJkZmQzZjUzZDkzZDc1ZDc0ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc3ZDc=\",\n            },\n        )\n        conn4 = Connection(\n            conn_id=\"azure_default"}], "metadata": {"file_name": "dag_541_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that transforms raw JSON employee data into a structured Parquet format for improved query performance and storage efficiency. Create an Airflow task that uses Apache Drill to perform the data conversion and table management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.common.operators.bash import BashOperator\nfrom airflow.providers.common.common.utils import explain_sql\nfrom airflow.providers.common.drill.operators.drill import (  # noqa: E402\n    DrillCreateTableOperator,\n    DrillCreateTableFromExternalFileOperator,\n    DrillExecuteQueryOperator,\n    DrillGetTableStructureOperator,\n    DrillInsertOperator,\n    DrillInsertStmtOperator,\n    DrillKqlOperator,\n    DrillKqlQueryOperator,\n    DrillUpsertTableOperator,\n    DrillUpsertTableStmtOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_drill\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    execute_query_on_drill = DrillExecuteQueryOperator(\n        task_id=\"execute_query_on_drill\",\n        sql=\"SELECT * FROM employee\",\n        kql=\"select * from employee\",\n        sql_timeout=5,\n        kql_timeout=1,\n    )\n\n    get_table_structure = DrillGetTableStructureOperator(\n        task_id=\"get_table_structure\", table_name=\"employee\"\n    )\n\n    insert_stmt = DrillInsertStmtOperator(\n        task_id=\"insert_stmt\",\n        table=\"employee\",\n        stmt=\"INSERT INTO employee VALUES (42, 'Miguel', 'Castro', 'miguel@castro.com'), (43, 'Kenneth', 'Lin', 'ken@lin.com');\",\n    )\n\n    insert_operator = DrillInsertOperator(\n        task_id=\"insert_operator\",\n        table=\"employee\",\n        values=\"(42, 'Miguel', 'Castro', 'miguel@castro.com'), (43, 'Kenneth', 'Lin', 'ken@lin.com');\",\n    )\n\n    upsert_table = DrillUpsertTableOperator(\n        task_id=\"upsert_table\",\n        table=\"employee\",\n        upsert_stmt=\"INSERT INTO employee VALUES (42, 'Miguel', 'Castro', 'miguel@castro.com'), (43, 'Kenneth', 'Lin', 'ken@lin.com');\",\n    )\n\n    upsert_table_stmt = DrillUpsertTableStmtOperator(\n        task_id=\"upsert_table_stmt\",\n        table=\"employee\",\n        values=\"(42, 'Miguel', 'Castro', 'miguel@castro.com'), (43, 'Kenneth', 'Lin', 'ken@lin.com');\",\n    )\n\n    create_table = DrillCreateTableOperator(\n        task_id=\"create_table\",\n        table_name=\"employee\",\n        table=\"employee\",\n        partition=\"dt=(year(\u5f53\u524d\u65f6\u95f4) || '-' || month(\u5f53\u524d\u65f6\u95f4));\",\n    )\n\n    create_table_from_external_file = DrillCreateTableFromExternalFileOperator(\n        task_id=\"create_table_from_external_file\",\n        external_file=\"shred_employees.json\",\n        table_name=\"employee_ex\",\n    )\n\n    sql_file_path = os.path.join(os.path.dirname(__file__), \"shred_employees.sql\")\n\n    run_sh = BashOperator(\n        task_id=\"run_sh\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_kql_query = DrillKqlQueryOperator(\n        task_id=\"execute_kql_query\",\n        kql=\"select * from employee\",\n    )\n\n    execute_kql_multiple_queries = DrillKqlQueryOperator(\n        task_id=\"execute_kql_multiple_queries\",\n        sql=[\n            \"select * from employee\",\n            \"select * from employee_ex\",\n        ],\n    )\n\n    execute_sql_file = DrillExecuteQueryOperator(\n        task_id=\"execute_sql_file\",\n        sql=f\"cat {sql_file_path}\",\n        timeout=5,\n    )\n\n    execute_sql_file_result = BashOperator(\n        task_id=\"execute_sql_file_result\",\n        bash_command=f\"cat {sql_file_path}\",\n    ])\n\n    execute_sql_file_with_params = DrillExecuteQueryOperator(\n        task_id=\"execute_sql_file_with_params\",\n        sql=\"SELECT * FROM employee WHERE dt = ?\",\n        parameters=(\"2022-12-15\",),\n        timeout=5,\n    )\n\n    delete_stmt = DrillExecuteQueryOperator(\n        task_id=\"delete_stmt\",\n        sql=\"DELETE FROM employee WHERE dt = (SELECT dt FROM dt = CURRENT_DATE - 1 FROM employee);\",\n    )\n\n    delete_stmt_result = BashOperator(\n        task_id=\"delete_stmt_result\",\n        bash_command=f\"SELECT * FROM employee WHERE dt = (SELECT dt FROM dt = CURRENT_DATE - 1 FROM employee);\",\n    )\n\n    join_query = DrillExecuteQueryOperator(\n        task_id=\"join_query\",\n        sql=\"SELECT e1.emp_name, e2.dt FROM employee e1 JOIN employee e2 ON e1.emp_name = e2.emp_name;\",\n    )\n\n    join_query_result = BashOperator(\n        task_id=\"join_query_result\",\n        bash_command=f\"SELECT e1.emp_name, e2.dt FROM employee e1 JOIN employee e2 ON e1.emp_name = e2.emp_name;\",\n    )\n\n    drop_table = BashOperator(\n        task_id=\"drop_table\",\n        bash_command=f\"DROP TABLE IF EXISTS {DAG_ID}\",\n    )\n\n    drop_table_from_external_file = BashOperator(\n        task_id=\"drop_table_from_external_file\",\n        bash_command=f\"DROP TABLE IF EXISTS {DAG_ID}\",\n    )\n\n    execute_sql_file_with_params_result = BashOperator(\n        task_id=\"execute_sql_file_with_params_result\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_file_with_params_result_multiple_queries = BashOperator(\n        task_id=\"execute_sql_file_with_params_result_multiple_queries\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_file_with_params_result_multiple_queries_multiple_files = BashOperator(\n        task_id=\"execute_sql_file_with_params_result_multiple_queries_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_file_with_params_multiple_queries_multiple_files = BashOperator(\n        task_id=\"execute_sql_file_with_params_multiple_queries_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_file_with_params_multiple_queries_multiple_files_result = BashOperator(\n        task_id=\"execute_sql_file_with_params_multiple_queries_multiple_files_result\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_file_with_params_multiple_queries_multiple_files_result_multiple_files = BashOperator(\n        task_id=\"execute_sql_file_with_params_multiple_queries_multiple_files_result_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result = BashOperator(\n        task_id=\"execute_sql_multiple_files_result\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files\",\n        bash_command=f\"cat {sql_file_path}\",\n    )\n\n    execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files_multiple_files = BashOperator(\n        task_id=\"execute_sql_multiple_files_result_multiple_queries_multiple_files_result_multiple_files_multiple_files"}], "metadata": {"file_name": "dag_105_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an inter-DAG dependency workflow that demonstrates how two separate Airflow DAGs can synchronize task execution using external task markers and sensors. Create a parent DAG that signals completion and a child DAG that waits for the parent's specific task to finish before proceeding.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"hello_world\")\ndef hello_world():\n    print(\"Hello world from external task openline!\")\n\n\ndef external_parent():\n    print(\"Hello world from external parent!\")\n    return True\n\n\nwith DAG(\n    dag_id=\"example_external_parent\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    hello_world()\n    external_parent()"}], "metadata": {"file_name": "dag_674_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that demonstrates Azure Synapse integration with Apache Airflow, showing how enterprise data orchestration can be achieved through automated pipeline triggering. Implement a simple DAG that connects cloud data processing services with workflow management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunPipelineOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"AZURE_PROJECT_ID\")\nDAG_ID = \"example_synapse_run_pipeline\"\n\npipeline_name = \"pipeline1\"\nsynapse_conn_id = \"synapse_connection\"\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_pipeline = AzureSynapseRunPipelineOperator(\n        synapse_conn_id=synapse_conn_id,\n        pipeline_name=pipeline_name,\n        task_id=\"run_pipeline\",\n    )\n\n    run_pipeline"}], "metadata": {"file_name": "dag_973_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python code to extract sentences from a text while keeping only the words that contain only alphabetic characters?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"example_python_function\",\n    start_date=2023-04-01,\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START create_dag]\n\n    # [END create_dag]\n\n    # Example of the Python function to be called by the BashOperator.\n    @task\n    def get_alphabetic_sentences(text: str) -> list:\n        \"\"\"\n        Example function to be called which requires calling a Python function.\n\n        Args:\n            text(str): The input text to extract alphabetic words from.\n\n        Returns:\n            list: The list of alphabetic words.\n        \"\"\"\n        import re\n\n        # Use a regular expression to find all words that consist of alphabetic characters\n        alphabetic_words = re.findall(r\"\\b[a-zA-Z]+\\b\", text)\n        return alphabetic_words\n\n    # Example of creating a BashOperator and passing the @task function.\n    BashOperator(\n        task_id=\"extract_alphabetic_sentences\",\n        bash_command=f\"get_alphabetic_sentences '{{ task_instance.xcom_pull('get_all_sentences') }}'\",\n    )"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python function that converts a given word to Pig Latin?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def convert_to_pig_latin(word):\n    \"\"\"\n    Converts a given word to Pig Latin.\n\n    Args:\n        word (str): The word to convert.\n\n    Returns:\n        str: The Pig Latin version of the word.\n    \"\"\"\n    vowels = \"aeiou\"\n    if word[0].lower() in vowels:\n        return word + \"way\"\n    else:\n        return word[1:] + word[0] + \"ay\"\n\n# Example usage\nprint(convert_to_pig_latin(\"apple\"))  # Output: appleway\nprint(convert_to_pig_latin(\"banana\")) # Output: anabanay"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data catalog management workflow in Google Cloud Dataplex that demonstrates creating and managing metadata catalog entries, entry types, and aspect types. Design a DAG that showcases the full lifecycle of metadata catalog objects from creation to potential deletion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dataplex_v1 import DataCatalogId\nfrom google.cloud.dataplex_v1.entry_group import EntryGroupDisplayName, EntryGroupType\nfrom google.cloud.dataplex_v1.entry_type import EntryTypeDisplayName, EntryTypeName\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateEntryGroupOperator,\n    DataplexCreateEntryOperator,\n    DataplexCreateEntryTypeOperator,\n    DataplexCreateOrUpdateEntryGroupOperator,\n    DataplexCreateOrUpdateEntryOperator,\n    DataplexCreateOrUpdateEntryTypeOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteEntryOperator,\n    DataplexDeleteEntryTypeOperator,\n    DataplexDeleteOperationOperator,\n    DataplexGetEntryGroupOperator,\n    DataplexGetEntryOperator,\n    DataplexGetEntryTypeOperator,\n    DataplexListEntryGroupsOperator,\n    DataplexListEntriesOperator,\n    DataplexListEntryGroupsForEntryOperator,\n    DataplexListEntryTypesForEntryOperator,\n    DataplexPauseOperationOperator,\n    DataplexResumeOperationOperator,\n    DataplexUpdateEntryGroupOperator,\n    DataplexUpdateEntryOperator,\n    DataplexUpdateEntryTypeOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataplex import DataplexOperationStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or \"default_project_id\"\n\nDAG_ID = \"example_dataplex\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_FILE = \"dataplex-entry-gcs-file.csv\"\nRESOURCE_ID = f\"resource_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\n\n# [START howto_dataplex_entry_group]\nENTRY_GROUP = {\n    EntryGroupDisplayName.display_name: PROJECT_ID + \".example-entry-group\",\n    EntryGroupType.type: EntryGroupType.DisplayName,\n}\n# [END howto_dataplex_entry_group]\n\n# [START howto_dataplex_entry]\nENTRY = {\n    EntryDisplayName.display_name: PROJECT_ID + \".example-entry\",\n    EntryTypeName.type: EntryTypeName.GCS,\n    GCSAspect: {\n        \"image_uri\": f\"gs://{BUCKET_NAME}/{BUCKET_FILE}\",\n    },\n}\n# [END howto_dataplex_entry]\n\n# [START howto_dataplex_entry_type]\nENTRY_TYPE = {\n    EntryDisplayName.display_name: PROJECT_ID + \".example-entry-type\",\n    EntryTypeName.type: EntryTypeName.GCS,\n    GCSAspect: {\n        \"image_uri\": f\"gs://{BUCKET_NAME}/{BUCKET_FILE}\",\n    },\n    GCSAspect.modification_time: datetime.now(),\n}\n# [END howto_dataplex_entry_type]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataplex\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    def get_resource_id():\n        return {\n            \"resource_id\": {\n                \"entry_group_id\": RESOURCE_ID,\n                \"entry_type_id\": ENTRY_TYPE[\"entry_type_display_name\"],\n            }\n        }\n\n    get_resource_id_task = lambda: AirflowToken()\n    # [START howto_dataplex_create_entry_group_operator]\n    create_entry_group = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=RESOURCE_ID,\n        entry_group=ENTRY_GROUP,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_entry_group_operator]\n\n    # [START howto_dataplex_create_entry_operator]\n    create_entry = DataplexCreateEntryOperator(\n        task_id=\"create_entry\", location=LOCATION, entry_id=ENTRY_TYPE[\"entry_type_display_name\"], entry=ENTRY\n    )\n    # [END howto_dataplex_create_entry_operator]\n\n    # [START howto_dataplex_create_entry_type_operator]\n    create_entry_type = DataplexCreateOrUpdateEntryTypeOperator(\n        task_id=\"create_entry_type\",\n        location=LOCATION,\n        entry_type_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        entry_type=ENTRY_TYPE,\n        project_id=PROJECT_ID,\n        update_mask=FieldMask(paths=[\"gcs_aspect.\" + GCSAspect.field_name]),\n    )\n    # [END howto_dataplex_create_entry_type_operator]\n\n    # [START howto_dataplex_create_entry_group_operator_async]\n    create_entry_group_async = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group_async\",\n        location=LOCATION,\n        entry_group_id=RESOURCE_ID,\n        entry_group=ENTRY_GROUP,\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n    # [END howto_dataplex_create_entry_group_operator_async]\n\n    # [START howto_dataplex_create_entry_operator_async]\n    create_entry_async = DataplexCreateEntryOperator(\n        task_id=\"create_entry_async\", location=LOCATION, entry_id=ENTRY_TYPE[\"entry_type_display_name\"], entry=ENTRY\n    )\n    # [END howto_dataplex_create_entry_operator_async]\n\n    # [START howto_dataplex_create_entry_type_operator_async]\n    create_entry_type_async = DataplexCreateOrUpdateEntryTypeOperator(\n        task_id=\"create_entry_type_async\",\n        location=LOCATION,\n        entry_type_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        entry_type=ENTRY_TYPE,\n        project_id=PROJECT_ID,\n        update_mask=FieldMask(paths=[\"gcs_aspect.\" + GCSAspect.field_name],\n        )\n    )\n    # [END howto_dataplex_create_entry_type_operator_async]\n\n    # [START howto_dataplex_get_entry_group_operator]\n    get_entry_group = DataplexGetEntryGroupOperator(\n        task_id=\"get_entry_group\", location=LOCATION, entry_group_id=RESOURCE_ID, project_id=PROJECT_ID\n    )\n    # [END howto_dataplex_get_entry_group_operator]\n\n    # [START howto_dataplex_get_entry_operator]\n    get_entry = DataplexGetEntryOperator(\n        task_id=\"get_entry\", location=LOCATION, entry_id=ENTRY_TYPE[\"entry_type_display_name\"], project_id=PROJECT_ID\n    )\n    # [END howto_dataplex_get_entry_operator]\n\n    # [START howto_dataplex_list_entries_for_entry_operator]\n    list_entries_for_entry = DataplexListEntriesForEntryOperator(\n        task_id=\"list_entries_for_entry\",\n        location=LOCATION,\n        entry_group_id=RESOURCE_ID,\n        entry_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_list_entries_for_entry_operator]\n\n    # [START howto_dataplex_update_entry_group_operator]\n    update_entry_group = DataplexUpdateEntryGroupOperator(\n        task_id=\"update_entry_group\",\n        location=LOCATION,\n        entry_group_id=RESOURCE_ID,\n        entry_group={\"labels\": {\"label\": \"value\"}},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_update_entry_group_operator]\n\n    # [START howto_dataplex_update_entry_operator]\n    update_entry = DataplexUpdateEntryOperator(\n        task_id=\"update_entry\",\n        location=LOCATION,\n        entry_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        entry={\"labels\": {\"label\": \"value\"}},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_update_entry_operator]\n\n    # [START howto_dataplex_update_entry_group_operator_async]\n    update_entry_group_async = DataplexUpdateEntryGroupOperator(\n        task_id=\"update_entry_group_async\",\n        location=LOCATION,\n        entry_group_id=RESOURCE_ID,\n        entry_group={\"labels\": {\"label\": \"value\"}},\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n    # [END howto_dataplex_update_entry_group_operator_async]\n\n    # [START howto_dataplex_update_entry_operator_async]\n    update_entry_async = DataplexUpdateEntryOperator(\n        task_id=\"update_entry_async\",\n        location=LOCATION,\n        entry_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        entry={\"labels\": {\"label\": \"value\"}},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_update_entry_operator_async]\n\n    # [START howto_dataplex_delete_entry_operator]\n    delete_entry = DataplexDeleteOperationOperator(\n        task_id=\"delete_entry\", operation_id=ENTRY_TYPE[\"entry_type_display_name\"], project_id=PROJECT_ID\n    )\n    # [END howto_dataplex_delete_entry_operator]\n\n    # [START howto_dataplex_delete_entry_operator_async]\n    delete_entry_async = DataplexDeleteOperationOperator(\n        task_id=\"delete_entry_async\",\n        operation_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        project_id=PROJECT_ID,\n        deferrable=True,\n    )\n    # [END howto_dataplex_delete_entry_operator_async]\n\n    # [START howto_dataplex_delete_entry_operator_deferrable]\n    delete_entry_deferrable = DataplexDeleteOperationOperator(\n        task_id=\"delete_entry_deferrable\",\n        operation_id=ENTRY_TYPE[\"entry_type_display_name\"],\n        project_id=PROJECT_ID,"}], "metadata": {"file_name": "dag_1222_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that monitors a Google Cloud Tasks queue to ensure all tasks have been processed before proceeding with downstream operations. Create an Airflow DAG that checks queue status as a critical checkpoint in a distributed task processing system.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.google.cloud.operators.tasks import CloudTasksQueueDeleteOperator\nfrom airflow.providers.google.cloud.sensors.tasks import (\n    CloudTasksQueueTasksExistenceSensor,\n    CloudTasksQueueTasksPendingSensor,\n)\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"default\")\nLOCATION = os.environ.get(\"GCP_TASKS_LOCATION\", \"us-central1\")\nQUEUE_ID = os.environ.get(\"GCP_TASKS_QUEUE_ID\", \"my-task-queue\")\n\n\nwith DAG(\n    \"example_cloudtasks_dag\",\n    schedule=\"* * * * *\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cloudtasks_create_queue]\n    @task\n    def create_queue():\n        return {\n            \"name\": {\"value\": QUEUE_ID},\n        }\n\n    # [END cloudtasks_create_queue]\n\n    # [START cloudtasks_delete_queue]\n    @task\n    def delete_queue(queue_id):\n        return {\n            \"name\": {\"value\": queue_id},\n        }\n\n    # [END cloudtasks_delete_queue]\n\n    # [START cloudtasks_get_queue]\n    @task\n    def get_queue(queue_id):\n        return {\n            \"name\": {\"value\": queue_id},\n        }\n\n    # [END cloudtasks_get_queue]\n\n    # [START cloudtasks_tasks_create]\n    @task\n    def create_task():\n        return {\n            \"state\": \"PENDING\",\n            \"keywords\": {\"key\": \"value\"},\n            \"createTime\": {\"seconds\": 0},\n            \"statusChangeTime\": {\"seconds\": 0},\n        }\n\n    # [END cloudtasks_tasks_create]\n\n    # [START cloudtasks_tasks_delete]\n    @task\n    def delete_task(queue_id, task_id):\n        return {\n            \"name\": {\"value\": task_id},\n        }\n\n    # [END cloudtasks_tasks_delete]\n\n    # [START cloudtasks_tasks_existence_sensor]\n    at_tasks_existence = CloudTasksQueueTasksExistenceSensor(\n        task_id=\"at_tasks_existence\",\n        location=LOCATION,\n        queue_id=QUEUE_ID,\n        deferrable=True,\n    )\n    # [END] cloudtasks_tasks_existence_sensor\n\n    # [START cloudtasks_tasks_pending_sensor]\n    at_tasks_pending = CloudTasksQueueTasksPendingSensor(\n        task_id=\"at_tasks_pending\",\n        location=LOCATION,\n        queue_id=QUEUE_ID,\n        deferrable=True,\n    )\n    # [END] cloudtasks_tasks_pending_sensor\n\n    # [START cloudtasks_queue_create]\n    create_task_queue = CloudTasksQueueCreateOperator(\n        task_id=\"create_task_queue\", location=LOCATION, queue=create_queue()\n    )\n    # [END] cloudtasks_queue_create\n\n    # [START cloudtasks_queue_id]\n    queue_id_output = task(task_id=\"queue_id_output\", task_fn=lambda t: t.output())\n    # [END] cloudtasks_queue_id\n\n    # [START cloudtasks_queue_delete]\n    delete_task_queue = CloudTasksQueueDeleteOperator(\n        task_id=\"delete_task_queue\", location=LOCATION, queue_id=queue_id_output\n    )\n    # [END] cloudtasks_queue_delete\n\n    create_task_queue >> get_queue(QUEUE_ID) >> delete_task_queue >> at_tasks_existence\n    create_task_queue >> queue_id_output >> at_tasks_pending"}], "metadata": {"file_name": "dag_699_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Hello Qwen, how can I retrieve data from a SQL database using Python?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_sql_execute_query]\ndef setup_connections():\n    \"\"\"\n    Setup connections for the DAG.\n\n    Returns:\n        A dict of the connections.\n    \"\"\"\n    connections = {\n        \"my_db\": \"my_db_conn\",\n        \"my_service\": \"my_service_conn\",\n    }\n\n    return connections\n\n\ndef execute_sql_command(sql, context):\n    \"\"\"\n    Execute sql command in database.\n\n    Args:\n        sql (str): SQL command to execute.\n        context (dict): Current logical date and other information.\n    \"\"\"\n    print(f\"execute sql {sql}\")\n    return [float(x) for x in context[\"logical_date\"].split(\".\") if x]\n\n\ndef check_result(output):\n    \"\"\"\n    Check the output is correct.\n\n    Args:\n        output (list): The output of SQLExecuteQueryOperator\n    \"\"\"\n    assert output == [3.0, 2.5, 2.0, 1.5, 1.0]\n\n\nwith DAG(\n    dag_id=\"example_sql_execute_query\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    on_start=setup_connections,\n    on_finish=check_result,\n) as dag:\n    execute_sql_command(\"select 1.0 as x, 2.0 as y, 3.0 as z, 4.0 as x+2*y, 5.0 as x+3*y, 6.0 as x+2*y+3*z;\", dag)"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS-integrated Airflow pipeline that showcases sensor functionality for tracking data availability in DynamoDB, including table creation, value checking with different matching strategies, and automatic table deletion. Implement a systematic workflow that tests DynamoDB item value detection using Airflow's task chaining and sensor capabilities.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.dynamodb import DynamoDBHook\nfrom airflow.providers.amazon.aws.operators.dynamodb import DynamoDBPutItemOperator, DynamoDBScanOperator\nfrom airflow.providers.amazon.aws.sensors.dynamodb import DynamoDBSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nSCHEMA = [\n    {\n        \"AttributeName\": [\"ItemID\"],\n        \"KeyType\": \"HASH\"\n    },\n    {\n        \"AttributeName\": [\"Color\"],\n        \"KeyType\": \"RANGE\"\n    }\n]\n\nsys_test_context_task = SystemTestContextBuilder() \\\n    .add_variable(\"table_name\") \\\n    .add_variable(\"item_value\") \\\n    .add_variable(\"attribute_name\") \\\n    .build()\n\nDYNAMODB_SCAN_COLUMNS = [\n    {\"AttributeName\": \"ItemID\", \"ComparisonOperator\": \"EQ\", \"Value\": \"123\"},\n    {\"AttributeName\": \"Color\", \"ComparisonOperator\": \"EQ\", \"Value\": \"RED\"},\n]\n\n\ndef get_dynamodb_conn():\n    return DynamoDBHook()\n\n\n@task\ndef create_table(table_name: str):\n    dynamodb = DynamoDBHook()\n    table = dynamodb.create_table(\n        AttributeDefinitions=[{\"AttributeName\": \"ItemID\", \"Type\": \"S\"}],\n        TableName=table_name,\n        KeySchema=SCHEMA,\n    )\n    log.info(f\"Created table {table['TableName']} for table name {table_name}\")\n    dynamodb.get_conn().get_waiter(\"table_exists\").wait(TableName=table_name)\n\n\n@task\ndef save_to_table(table_name: str):\n    connection = Connection(\n        conn_id=\"dynamodb\",\n        schema=[(\"ItemID\", \"S\"), (\"Color\", \"S\")],\n        task_id=\"save_to_table\",\n    )\n    ddb = DynamoDBHook(connection=connection)\n    table = ddb.create_table(\n        AttributeDefinitions=[{\"AttributeName\": \"ItemID\", \"Type\": \"S\"}],\n        TableName=table_name,\n    )\n    log.info(f\"Saved table {table['TableName']} for table name {table_name}\")\n    ddb.get_conn().get_waiter(\"table_exists\").wait(TableName=table_name)\n\n    dynamodb = DynamoDBHook()\n    table = dynamodb.create_table(\n        AttributeDefinitions=[{\"AttributeName\": \"ItemID\", \"Type\": \"S\"}],\n        TableName=table_name + \"_scan\",\n    )\n    log.info(f\"Saved table {table['TableName']} for table name {table_name}\")\n    dynamodb.get_conn().get_waiter(\"table_exists\").wait(TableName=table_name + \"_scan\")\n\n    items = [\n        {\"ItemID\": \"123\", \"Color\": \"RED\"},\n        {\"ItemID\": \"123\", \"Color\": \"BLUE\"},\n        {\"ItemID\": \"456\", \"Color\": \"GREEN\"},\n        {\"ItemID\": \"456\", \"Color\": \"YELLOW\"},\n    ]\n    dynamodbhook = DynamoDBHook()\n    dynamodbhook.bulk_put_item(\n        TableName=table_name + \"_scan\",\n        ItemList=items,\n    )\n\n    scan_kwargs = {\n        \"TableName\": table_name + \"_scan\",\n        \"ExpressionAttributeNames\": {\"#T\": \"ItemID\"},\n        \"ExpressionAttributeValues\": {\n            \":T\": \"123\",\n            \":C\": \"RED\",\n        },\n        \"FilterExpression\": \"attribute_name('#T', ':T') AND attribute_name('#C', ':C')\",\n        \"ExpressionAttributeSymbols\": {\"#T\": \"ItemID\", \"#C\": \"Color\"},\n    }\n    dynamodbhook.bulk_get_item(**scan_kwargs)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_table(table_name: str):\n    log.info(f\"Deleting table {table_name}...\")\n    DynamoDBHook().conn.delete_table(TableName=table_name)\n    DynamoDBHook().conn.delete_table(TableName=table_name + \"_scan\")\n    log.info(f\"Done!\")\n\n\nwith DAG(\n    dag_id=\"dynamodb_sensor_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    table_name = test_context.table_name\n    item_value = test_context.item_value\n    attribute_name = test_context.attribute_name\n\n    set_up_table = create_table(table_name)\n\n    put_item = DynamoDBPutItemOperator(task_id=\"put_item\", table_name=table_name, item={\"ItemID\": item_value})\n\n    get_item = DynamoDBScanOperator(\n        task_id=\"get_item\",\n        table_name=table_name,\n        scan_kwargs=DYNAMODB_SCAN_COLUMNS,\n    )\n\n    # [START howto_dynamodb_sensor]\n    scan_table = DynamoDBSensor(\n        task_id=\"scan_table\",\n        table_name=table_name,\n        attribute_name=attribute_name,\n        key_value=item_value,\n    )\n    # [END howto_dynamodb_sensor]\n\n    del_table = delete_table(table_name)\n\n    (\n        # TEST SETUP\n        test_context\n        >> set_up_table\n        >> put_item\n        >> get_item\n        # TEST BODY\n        >> scan_table\n        # TEST TEARDOWN\n        >> del_table\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark the start and finish\n    # of the task.\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1472_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Can you provide a Python function that checks if a given number is a prime number? If the number is prime, the function should return `True`; otherwise, it should return `False`.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "def is_prime_number(number):\n    \"\"\"\n    Check if the given number is a prime number.\n\n    Args:\n    number (int): The number to check.\n\n    Returns:\n    bool: True if the number is prime, False otherwise.\n    \"\"\"\n    if number <= 1:\n        return False\n    if number <= 3:\n        return True\n    if number % 2 == 0 or number % 3 == 0:\n        return False\n    i = 5\n    while i * i <= number:\n        if number % i == 0 or number % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\n# Example usage\nprint(is_prime_number(29))  # Output: True\nprint(is_prime_number(10))  # Output: False"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily automated workflow for deploying and testing containerized applications in Azure Container Instances. Create a DAG that demonstrates launching different container configurations with varying registry sources and volume mounting strategies.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateFromTemplateOperator,\n    ContainerInstancesDeleteOperator,\n    ContainerInstancesStartOperator,\n    ContainerInstancesStopOperator,\n)\nfrom airflow.providers.microsoft.azure.utils import get_clean_name\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"container_instances_example\"\nCONTAINER_REGISTRY_SERVER = os.environ.get(\"CONTAINER_REGISTRY_SERVER\")\nAZURE_MY_STATEFUL_SET = {\n    \"name\": get_clean_name(\"my_stateful_set\"),\n    \"resource_group\": get_clean_name(\"my_resource_group\"),\n    \"docker_config\": {\n        \"image_source\": {\n            \"container_repository\": f\"{CONTAINER_REGISTRY_SERVER}somedockerregistry\",\n        },\n        \"command\": [\"sh\", \"-c\", \"while true; do sleep 10; done\"],\n    },\n    \"volume_conf\": [\n        {\n            \"name\": \"my_volume\",\n            \"type\": \"Directory\",\n            \"properties\": {\n                \"directory_mode\": 0o777,\n                \"azure_file_share_properties\": {\n                    \"share_name\": \"my_share\",\n                },\n            },\n        },\n    ],\n    \"linux_config\": {\n        \"image_version\": None,\n        \"container_port_mapping\": [\n            {\n                \"system_port\": int(os.environ.get(\"SYSTEM_PORT\", \"10000\")),\n                \"container_port\": int(os.environ.get(\"CONTAINER_PORT\", \"6379\")),\n            }\n        ],\n        \"service_port_mapping\": [\n            {\n                \"system_port\": int(os.environ.get(\"SYSTEM_PORT\", \"10000\")),\n                \"container_port\": int(os.environ.get(\"CONTAINER_PORT\", \"6379\")),\n            }\n        ],\n    },\n}\nAZURE_MY_STATEFUL_SET_V2 = {\n    \"name\": get_clean_name(\"my_stateful_set\"),\n    \"resource_group\": get_clean_name(\"my_resource_group\"),\n    \"docker_config\": {\n        \"image_source\": {\n            \"container_repository\": f\"{CONTAINER_REGISTRY_SERVER}somedockerregistry\",\n        },\n        \"command\": [\"sh\", \"-c\", \"while true; do sleep 10; done\"],\n    },\n    \"volume_conf\": [\n        {\n            \"name\": \"my_volume\",\n            \"type\": \"Directory\",\n            \"properties\": {\n                \"directory_mode\": 0o777,\n                \"azure_file_share_properties\": {\n                    \"share_name\": \"my_share\",\n                },\n            },\n        },\n    ],\n    \"linux_config\": {\n        \"image_version\": None,\n        \"container_port_mapping\": [\n            {\n                \"system_port\": int(os.environ.get(\"SYSTEM_PORT\", \"10000\")),\n                \"container_port\": int(os.environ.get(\"CONTAINER_PORT\", \"6379\")),\n            }\n        ],\n        \"service_port_mapping\": [\n            {\n                \"system_port\": int(os.environ.get(\"SYSTEM_PORT\", \"10000\")),\n                \"container_port\": int(os.environ.get(\"CONTAINER_PORT\", \"6379\")),\n            }\n        ],\n    },\n}\nAZURE_MY_CONTAINER_INSTANCE = {\n    \"name\": get_clean_name(\"my_container_instance\"),\n    \"resource_group\": get_clean_name(\"my_resource_group\"),\n    \"linux_configuration\": {\n        \"command_line_arguments\": \"--test\",\n        \"container_spec\": {\n            \"image_uri\": f\"{CONTAINER_REGISTRY_SERVER}somedockerregistry:latest\",\n        },\n    },\n}\nAZURE_MY_CONTAINER_INSTANCE_V2 = {\n    \"name\": get_clean_name(\"my_container_instance\"),\n    \"resource_group\": get_clean_name(\"my_resource_group\"),\n    \"linux_configuration\": {\n        \"command_line_arguments\": \"--test\",\n        \"container_spec\": {\n            \"image_uri\": f\"{CONTAINER_REGISTRY_SERVER}somedockerregistry:latest\",\n        },\n    },\n}\nTAINT_NAME = \"test-taint\"\nCONNECTION_ID = \"container-connection\"\n\n\n@task\ndef create_connection(connection_id: str):\n    conn = Connection(\n        conn_id=connection_id,\n        conn_type=\"azure_container_instances\",\n        description=\"Azure Container Instances connection for demo.\",\n        extra={\"azure_container_instances_connection\": {\"resource_group_name\": \"my_resource_group\", \"server\": \"some_container_registry\"}},\n    )\n    db_session.add(conn)\n    db_session.commit()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_connection(connection_id=CONNECTION_ID)  # Create connection in the database\n    # [START howto_operator_container_instances_create_from_template]\n    create_stateful_set = ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"stateful_set_creation\",\n        template=AZURE_MY_STATEFUL_SET,\n        subscription_id=\"my_subscription_id\",\n    )\n    # [END howto_operator_container_instances_create_from_template]\n    wait_for_stateful_set = ContainerInstancesStartOperator(\n        task_id=\"stateful_set_wait\",\n        subscription_id=\"my_subscription_id\",\n    )\n    delete_stateful_set = ContainerInstancesDeleteOperator(\n        task_id=\"stateful_set_deletion\",\n        template=AZURE_MY_STATEFUL_SET,\n        subscription_id=\"my_subscription_id\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_container_instances_create_from_template_v2]\n    create_stateless_instance = ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"instance_creation\",\n        template=AZURE_MY_CONTAINER_INSTANCE,\n        subscription_id=\"my_subscription_id\",\n    )\n    # [END howto_operator_container_instances_create_from_template_v2]\n    wait_for_stateless_instance = ContainerInstancesStartOperator(\n        task_id=\"instance_wait\",\n        subscription_id=\"my_subscription_id\",\n    )\n    delete_stateless_instance = ContainerInstancesDeleteOperator(\n        task_id=\"instance_deletion\",\n        template=AZURE_MY_CONTAINER_INSTANCE,\n        subscription_id=\"my_subscription_id\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_container_instances_create_from_template_v2_with_per_node_taint]\n    create_stateless_with_taint = ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"instance_with_taint_creation\",\n        template=AZURE_MY_CONTAINER_INSTANCE_V2,\n        subscription_id=\"my_subscription_id\",\n        additional_args={\"taints\": [{\"node_label_name\": TAIT_NAME}]}\n    )\n    # [END howto_operator_container_instances_create_from_template_v2_with_per_node_taint]\n    wait_for_stateless_with_taint_instance = ContainerInstancesStopOperator(\n        task_id=\"instance_with_taint_wait\",\n        subscription_id=\"my_subscription_id\",\n        per_node_taint=TAIT_NAME,\n    )\n    delete_stateless_with_taint_instance = ContainerInstancesDeleteOperator(\n        task_id=\"instance_with_taint_deletion\",\n        template=AZURE_MY_CONTAINER_INSTANCE_V2,\n        subscription_id=\"my_subscription_id\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    run_this_last = ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"final_stateless_instance_creation\",\n        template=AZURE_MY_CONTAINER_INSTANCE_V2,\n        subscription_id=\"my_subscription_id\",\n    )\n\n    run_this_last.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_618_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the complete lifecycle of a file transfer task, including dynamic environment setup, S3 bucket creation, local file generation, file upload, and subsequent resource cleanup. Implement a systematic approach to demonstrate cloud file transfer best practices.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_bucket():\n        conn = Connection(\n            conn_id=\"s3_conn_id\",\n            conn_type=\"s3\",\n            host=\"s3-us-gov.amazonaws.com\",\n            login=\"akiaiosfgh453nvtex1xz\",\n            password=\"2\u4f4f\u9662\u75c5\u5386\u53f7=\u897f\u5f0f\u9762\u98df\uff1a11\",\n            region=\"us-gov-us\",\n        )\n        role = Role(role_id=\"role_id\")\n        task_group = TaskGroup(task_id=\"task_group\")\n        for i in range(3):\n            BashOperator(\n                task_id=f\"bash_task_{i}\",\n                bash_command=f\"echo hello {i}\",\n            )\n        trigger = Trigger(trigger_id=\"trigger_id\", task_id=\"trigger_task\")"}], "metadata": {"file_name": "dag_1503_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Asana task management workflow that demonstrates creating, finding, updating, and deleting tasks programmatically using Airflow. Build a DAG that showcases the full lifecycle of task management operations in an automated, sequential manner.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.asana.operators.asana import AsanaTaskOperator, AsanaTasksOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"example_asana\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START asana_task_operator]\n    get_task = AsanaTaskOperator(task_id=\"get_task\", task={\"id\": \"AF31C93936543777\"})\n    # [END asana_task_operator]\n\n    update_task = AsanaTaskOperator(\n        task_id=\"update_task\",\n        task={\"id\": \"AF31C93936543777\": {\"notes\": \"Updated task notes!\"}},\n        trigger=TriggerRule.ALL_DONE,\n    )\n\n    @task\n    def print_task_id(task_id):\n        \"\"\"Print the value of the task_id attribute.\"\"\"\n        print(task_id)\n\n    # [START asana_tasks_operator]\n    get_task_ids = AsanaTasksOperator(task_id=\"get_task_ids\", params={\"task\": {\"state\": \"all\"}})\n    print_task_id(task_id=get_task_ids.output)\n    # [END asana_tasks_operator]\n\n    update_task_ids = AsanaTasksOperator(\n        task_id=\"update_task_ids\",\n        params={\"task\": {\"state\": \"all\"}},\n        task_id_operator=AsanaTaskOperator,\n        task_params_operator=print_task_id,\n    )\n\n    delete_task = AsanaTaskOperator(\n        task_id=\"delete_task\", task={\"id\": \"AF31C93936543777\"}, trigger=TriggerRule.ALL_DONE\n    )\n\n    # [START asana_task_operator_async]\n    create_task = AsanaTaskOperatorAsync(\n        task_id=\"create_task\", task={\"title\": \"Testing Task\", \"notes\": \"Testing notes for tasks\"}\n    )\n    # [END asana_task_operator_async]\n\n    join = AsanaTasksOperator(\n        task_id=\"join\",\n        params={\"task\": {\"ids\": create_task.output[\"task\"][\"id\"]}, \"join\": True},\n    )\n\n    delete_task_ids_async = AsanaTasksOperator(\n        task_id=\"delete_task_ids_async\",\n        params={\"task\": {\"ids\": create_task.output[\"task\"][\"id\"]}},\n        deferrable=True,\n    )\n\n    create_task.wait_for_completion = True\n    join.wait_for_completion = True\n    delete_task.wait_for_completion = True\n    delete_task_ids_async.wait_for_completion = True\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates remote task scheduling and execution using Azure's cloud infrastructure, showing how to configure a computational job with specific VM specifications and a basic command line task. Illustrate the process of defining a cloud-based computational workflow with precise infrastructure and execution parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.advisor import AzureAdvisorListComputationalJobsOperator\n\nAZURE_ADVISOR_MIN_SERVICE THROUGHLINE = \"AzureAdvisorMinServiceThroughline\"\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_advisor_list_jobs\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START advisor_list_jobs]\n    list_jobs = AzureAdvisorListComputationalJobsOperator(task_id=\"list_jobs\", az_service_throughline_name=AZURE_ADVISOR_MIN_SERVICE THROUGHLINE)\n    # [END advisor_list_jobs]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)  # noqa: F403"}], "metadata": {"file_name": "dag_965_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Papermill to dynamically execute a Jupyter notebook with date-based parameters, generating a new output notebook for each workflow run. The pipeline should verify the notebook's execution by checking a specific message generated during the process.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_papermill\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo \" \"Hello World\")\n\n    @task\n    def generate_notebook(notebook_input):\n        conn = Connection(\n            conn_id=\"papermill\",\n            conn_type=\"papermill\",\n        )\n        path = \"/tmp/output.ipynb\"\n        command = f\"papermill {notebook_input} {path} --log-level=DEBUG\"\n        os.system(command)\n        return path\n\n    generate_notebook(path=generate_notebook(\"test_notebook.ipynb\",))\n\n    run_this >> generate_notebook(path=\"test_notebook.ipynb\")"}], "metadata": {"file_name": "dag_1381_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated machine learning pipeline for training and deploying a housing price prediction model using Google Vertex AI. Create a workflow that handles dataset creation, model training, versioning, and deployment with robust error handling and cloud infrastructure management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Model Service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.vertex_ai.model_service import (\n    ListModelVersionsOperator,\n    UploadModelOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"vertex_ai_model_service\"\nREGION = \"us-central1\"\n\n# [START how_to_cloud_vertex_ai_upload_model]\ndef upload_model_operator():\n    model = {\n        \"display_name\": f\"test-housing-model-{ENV_ID}\",\n        \"base_version\": None,\n        \"version\": 0,\n        \"job_listening_service\": {\n            \"uri\": \"\",\n            \"port\": 0,\n            \"protocol\": \"SPARK_STAGING\",\n        },\n        \"source\": {\"uri\": \"\", \"job_id\": \"\"},\n        \"training_spec\": {\n            \"data_config\": [\n                {\n                    \"display_name\": \"training-data\",\n                    \"bigquery_source\": {\"table_name\": \"training-data\"},\n                }\n            ],\n            \"metric_spec\": {\"accuracy\": \"\"},\n            \"training_fraction\": 1.0,\n            \"validation_fraction\": 0.0,\n            \"test_fraction\": 0.0,\n            \"model_spec\": {\"type\": \"HousingPrice\"},\n            \"storage_config\": {\"method\": \"GCS\"},\n            \"resource_labels\": {},\n        },\n    }\n\n    upload_model_operator = UploadModelOperator(\n        task_id=\"upload_model_operator\", project_id=PROJECT_ID, region=REGION, body=model\n    )\n\n    return upload_model_operator\n# [END how_to_cloud_vertex_ai_upload_model]\n\n\n# [START how_to_cloud_vertex_ai_list_model_version]\ndef list_model_version():\n    list_model_version_operator = ListModelVersionsOperator(\n        task_id=\"list_model_version\", project_id=PROJECT_ID, region=REGION, model_name=f\"projects/{PROJECT_ID}/locations/{REGION}/models/test-housing-model-{{ds}}\"\n    )\n    return list_model_version_operator\n# [END how_to_cloud_vertex_ai_list_model_version]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"model_service\"],\n) as dag:\n    # [START how_to_cloud_vertex_ai_upload_model_operator)\n    upload_model = upload_model_operator()\n    # [END how_to_cloud_vertex_ai_upload_model_operator]\n\n    # [START how_to_cloud_vertex_ai_list_model_version_operator)\n    list_model_version = list_model_version()\n    # [END how_to_cloud_vertex_ai_list_model_version_operator]\n\n    upload_model >> list_model_version\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1718_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that manages Power BI workspace and dataset operations, focusing on retrieving workspace information and refreshing datasets automatically. Create a workflow that can scan workspace modifications, track dataset refresh status, and handle administrative Power BI tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.powerbi import (\n    PowerBICreateWorkspaceOperator,\n    PowerBIDeleteWorkspaceOperator,\n    PowerBIRetrieveWorkspaceOperator,\n    PowerBIRunRefreshOperator,\n)\nfrom airflow.providers.microsoft.azure.utils import get_blob_data\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"POWERBI_PROJECT_ID\")\nWORKSPACE_NAME = f\"workspace_{ENV_ID}\"\nACCT = os.environ.get(\"Azure_Bucket_ACS\")\nCONTAINER_NAME = \"powerbi\"\nFILE_NAME = \"powerbi.csv\"\n\n\n@task\ndef create_connection(connection_id: str):\n    connection = Connection(\n        conn_id=connection_id,\n        conn_type=ACCT,\n        description=\"Azure Data Lake connection for testing.\",\n    )\n    print(connection)\n    print(connection.id)\n    print(connection.get_key())\n    os.environ[connection_id] = str(connection.id)\n\n\nwith DAG(\n    dag_id=\"powerbi_example\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_connection(connection_id=\"azure_data_lake\")\n    # [START howto_operator_powerbi_create_workspace]\n    create_workspace = PowerBICreateWorkspaceOperator(\n        task_id=\"create_workspace\",\n        project_id=PROJECT_ID,\n        workspace_name=WORKSPACE_NAME,\n    )\n    # [END howto_operator_powerbi_create_workspace]\n\n    # [START howto_operator_powerbi_retrieve_workspace]\n    retrieve_workspace = PowerBIRetrieveWorkspaceOperator(\n        task_id=\"retrieve_workspace\",\n        project_id=PROJECT_ID,\n        workspace_name=WORKSPACE_NAME,\n    )\n    # [END howto_operator_powerbi_retrieve_workspace]\n\n    # [START howto_operator_powerbi_run_refresh]\n    run_refresh = PowerBIRunRefreshOperator(\n        task_id=\"run_refresh\",\n        dataset_id=\"dataset_id\",\n        wait_for_refresh=True,\n    )\n    # [END howto_operator_powerbi_run_refresh]\n\n    # [START howto_operator_powerbi_delete_workspace]\n    delete_workspace = PowerBIDeleteWorkspaceOperator(\n        task_id=\"delete_workspace\",\n        project_id=PROJECT_ID,\n        workspace_name=WORKSPACE_NAME,\n    )\n    # [END howto_operator_powerbi_delete_workspace]\n\n    (\n        create_connection(connection_id=\"azure_data_lake\")\n        >> create_workspace\n        >> retrieve_workspace\n        >> run_refresh\n        >> delete_workspace\n    )\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data validation pipeline that monitors a specific Cassandra keyspace and table for data availability and record-level conditions. Create a sensor-based workflow that acts as a gatekeeper to ensure data quality and completeness before initiating downstream data transformation or analysis tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_cassandra_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_cassandra_connection(conn_id_name: str):\n        connection = Connection(\n            conn_id=conn_id_name,\n            conn_type=\"cassandra\",\n            description=\"Cassandra connection for system tests.\",\n            extra={\n                \"host\": \"localhost\",\n                \"port\": 9042,\n                \"table\": \"students\",\n                \"columns\": \"student_id, name\",\n                \"schema\": \"student_id:int, name:string\",\n                \"rows\": [\n                    {\"student_id\": 1, \"name\": \"John\"},\n                    {\"student_id\": 2, \"name\": \"Ringo\"},\n                    {\"student_id\": 3, \"name\": \"Pete\"},\n                ],\n            },\n        )\n        return connection\n\n    create_cassandra_connection_task = create_cassandra_connection(conn_id_name=\"cassandra_default\")\n\n    @task\n    def create_cassandra_table(cassandra_connection_id):\n        sql = \"CREATE TABLE IF NOT EXISTS students (student_id INT, name STRING, PRIMARY KEY (student_id))\"\n        execute_cassandra_query = SQLExecuteQueryOperator(\n            task_id=\"execute_cassandra_query\",\n            sql=sql,\n            conn_id=cassandra_connection_id,\n        )\n\n        execute_cassandra_query\n\n    execute_cassandra_query_task = create_cassandra_table(cassandra_connection_id=\"cassandra_default\")\n\n    @task\n    def delete_cassandra_table(cassandra_connection_id):\n        sql = \"DROP TABLE IF EXISTS students\"\n        execute_cassandra_query = SQLExecuteQueryOperator(\n            task_id=\"execute_cassandra_query\",\n            sql=sql,\n            conn_id=cassandra_connection_id,\n        )\n\n        execute_cassandra_query\n\n    delete_cassandra_table_task = delete_cassandra_table(cassandra_connection_id=\"cassandra_default\")\n\n    @task\n    def delete_cassandra_connection(conn_id_name: str):\n        conn = Connection.get(conn_id=conn_id_name)\n        conn.status = Connection.RUNNING\n        conn.save()\n\n    delete_cassandra_connection_task = delete_cassandra_connection(conn_id_name=\"cassandra_default\")\n\n    delete_cassandra_connection_task >> create_cassandra_connection_task >> execute_cassandra_query_task >> delete_cassandra_table_task"}], "metadata": {"file_name": "dag_1935_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I want to create a Python Flask application that allows users to register and log in using their email and password. How can I implement this functionality?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.standard.operators.python import PythonOperator\n\n\n@task\ndef generate_secret():\n    return \"12345678-abcdef-0123456789\"\n\n\n@task\ndef create_secret_in_db(*, name: str):\n    print(f\"Added entry for {name} in db.\")\n    # [START howto_operator_create_secret_in_db]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"create_secret_in_db\",\n        sql=f\"CREATE TABLE IF NOT EXISTS my_secrets (SecretId UUID PRIMARY KEY, SecretValue TEXT)\",\n        conn=\"postgres\",\n        autocommit=True,\n    )\n    # [END howto_operator_create_secret_in_db]\n    sql.execute()\n    # [START howto_operator_get_secret_in_db]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"get_secret_in_db\",\n        sql=f\"SELECT SecretValue FROM my_secrets WHERE SecretId = '{name}'\",\n        conn=\"postgres\",\n    )\n    # [END howto_operator_get_secret_in_db]\n    return sql.execute().dicts()[0][\"SecretValue\"]\n\n\n@task\ndef update_secret_in_db(*, name: str, new_secret_id: str):\n    print(f\"Updated entry for {name} in db to have new id {new_secret_id}\")\n    # [START howto_operator_update_secret_in_db]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"update_secret_in_db\",\n        sql=f\"UPDATE my_secrets SET SecretId = '{new_secret_id}' WHERE SecretId = '{name}'\",\n        conn=\"postgres\",\n    )\n    # [END howto_operator_update_secret_in_db]\n\n\n@task\ndef delete_secret_in_db(*, name: str):\n    print(f\"Deleted entry for {name} from db\")\n    # [START howto_operator_delete_secret_in_db]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"delete_secret_in_db\",\n        sql=f\"DROP TABLE my_secrets WHERE SecretId = '{name}'\",\n        conn=\"postgres\",\n    )\n    # [END howto_operator_delete_secret_in_db]\n\n\n@task\ndef cleanup_secret(*, name: str):\n    # [START howto_operator_cleanup_secret]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"cleanup_secret\",\n        sql=f\"DROP TABLE my_secrets WHERE SecretId = '{name}'\",\n        conn=\"postgres\",\n    )\n    # [END howto_operator_cleanup_secret]\n    sql.execute()\n    print(f\"Deleted table my_secrets containing entry for {name}\")\n\n\n@task\ndef check_hook(conn_id=\"postgres\"):\n    from airflow.providers.standard.hooks.sql import SQLHook\n\n    hook = SQLHook()\n    assert hook.conn_id == conn_id\n\n\ndef setup_env():\n    db_type = os.getenv(\"DB_TYPE\", \"postgres\")\n    login = os.getenv(\"DB_LOGIN\", \"username\")\n    password = os.getenv(\"DB_PASSWORD\", \"password\")\n    host = os.getenv(\"DB_HOST\", \"127.0.0.1\")\n    port = os.getenv(\"DB_PORT\", 5432)\n    conn_id = os.getenv(\"DB_CONN_ID\", \"postgres_default\")\n\n    print(f\"Setting up environment for {db_type} db.\")\n    print(f\"Creating connection to {db_type} db hosted on {host}:{port} with login {login} and password {password}\")\n    print(f\"Using connection id {conn_id}\")\n    return login, password, host, port, conn_id\n\n\ndef setup_secrets():\n    name = \"s3cr3t\"\n    new_secret_id = \"2348790ba7d6bcdf789523487bca7d9f\"\n    generate_secret.return_value = new_secret_id\n    sql = f\"INSERT INTO my_secrets (SecretId, SecretValue) VALUES ('{name}', '{new_secret_id}')\"\n    os.getenv(\"TEST_DATABASE_CONNECTION\", \"\").execute(sql)\n    print(f\"Seeded database with entry for {name}\")\n\n\nwith DAG(\n    \"example_sql_operator\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    test_login = os.getenv(\"TEST_DATABASE_CONNECTION\").split(\",\")[0]\n    test_password = os.getenv(\"TEST_DATABASE_CONNECTION\").split(\",\")[1]\n    sql_conn_id = os.getenv(\"DB_CONN_ID\", \"postgres_default\")\n\n    login = test_login.split()[1]\n    password = test_password.split()[1]\n    host = test_login.split(\":\")[1].split(\".\")[0]\n    port = test_login.split(\":\")[2]\n\n    # [START howto_operator_create_table]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"create_table\",\n        sql=f\"CREATE TABLE IF NOT EXISTS users (user_id INT, name TEXT)\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_create_table]\n\n    # [START howto_operator_insert]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(task_id=\"insert\", sql=\"INSERT INTO users VALUES (1, 'John'), (2, 'Doe')\", conn=sql_conn_id)\n    # [END howto_operator_insert]\n\n    # [START howto_operator_insert_values]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"insert_values\",\n        sql=\"INSERT INTO users VALUES (values[]::integer[]::users[])\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_insert_values]\n\n    # [START howto_operator_select]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(task_id=\"select\", sql=\"SELECT * FROM users\", conn=sql_conn_id)\n    # [END howto_operator_select]\n\n    # [START howto_operator_select_values]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"select_values\",\n        sql=\"SELECT * FROM users WHERE name IN ('John', 'Doe')\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_select_values]\n\n    # [START howto_operator_select_values_result]\n    from airflow.providers.standard.operators.sql import SQLOperator\n    from airflow.providers.standard.operators.python import PythonOperator\n\n    def checkrows(func, *args, **kwargs):\n        rows = func(*args, **kwargs)\n        for row in rows:\n            print(row)\n\n    sql = SQLOperator(\n        task_id=\"select_values_result\",\n        sql=\"SELECT * FROM users WHERE name IN ('John', 'Doe')\",\n        conn=sql_conn_id,\n        python_callable=checkrows,\n    )\n    # [END howto_operator_select_values_result]\n\n    # [START howto_operator_insert_many]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"insert_many\",\n        sql=\"INSERT INTO users VALUES (values[]::integer[]::users[])\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_insert_many]\n\n    # [START howto_operator_insert_values_result]\n    from airflow.providers.standard.operators.sql import SQLOperator\n    from airflow.providers.standard.operators.python import PythonOperator\n\n    def checkrows(func, *args, **kwargs):\n        rows = func(*args, **kwargs)\n        for row in rows:\n            print(row)\n\n    sql = SQLOperator(\n        task_id=\"insert_values_result\",\n        sql=f\"INSERT INTO users VALUES {values[]::integer[]::users[]}\",\n        conn=sql_conn_id,\n        python_callable=checkrows,\n    )\n    # [END howto_operator_insert_values_result]\n\n    # [START howto_operator_update]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"update\",\n        sql=f\"UPDATE users SET name = 'Jane' WHERE user_id = 1\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_update]\n\n    # [START howto_operator_update_many]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"update_many\",\n        sql=f\"UPDATE users SET name = 'Jane' WHERE user_id IN ({values[]::integer[]::users[]})\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_update_many]\n\n    # [START howto_operator_delete]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(task_id=\"delete\", sql=\"DELETE FROM users WHERE user_id = 2\", conn=sql_conn_id)\n    # [END howto_operator_delete]\n\n    # [START howto_operator_delete_many]\n    from airflow.providers.standard.operators.sql import SQLOperator\n\n    sql = SQLOperator(\n        task_id=\"delete_many\",\n        sql=f\"DELETE FROM users WHERE user_id IN ({values[]::integer[]::users[]})\",\n        conn=sql_conn_id,\n    )\n    # [END howto_operator_delete_many]\n\n    # [START task_cleanup_secret]\n    cleanup_secret(\n        name=\"{{ task_instance.xcom_pull(task_ids='insert_values_result', key='return_value') }}\",\n    )\n    # [END task_cleanup_secret]\n\n    # [START task_cleanup_secrets]\n    cleanup_secrets(\n        new_secret"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an end-to-end machine learning workflow for automated model training and deployment using Google Vertex AI. Create a comprehensive data science pipeline that demonstrates training, evaluation, and deployment of machine learning models across different data types and use cases.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "import os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.vertex_ai.auto_ml import VertexAIAutoMLHook\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import CreateAutoMLForecastingTrainingJobOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    DeleteAutoMLForecastingTrainingJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"example_vertex_ai_auto_ml_forecasting\"\n\nFORECAST_DATASET_NAME = f\"forecast-dataset-{ENV_ID}\"\nTRAINING_JOB_NAME = f\"training-job-forecast-{ENV_ID}\"\nCONTAINER_SIZE = 10\nHOUR = 24\nDATA_SAMPLE = r\"\"\"\n    date,temperature\n    2018-12-01,13.4\n    2018-12-02,13.6\n    2018-12-03,13.7\n    2018-12-04,13.8\n    2018-12-05,13.9\n    2018-12-06,14.0\n    2018-12-07,14.1\n    2018-12-08,14.2\n    2018-12-09,14.3\n    2018-12-10,14.4\n    2018-12-11,14.5\n    2018-12-12,14.6\n    2018-12-13,14.7\n    2018-12-14,14.8\n    2018-12-15,14.9\n    2018-12-16,15.0\n    2018-12-17,15.1\n    2018-12-18,15.2\n    2018-12-19,15.3\n    2018-12-20,15.4\n    2018-12-21,15.5\n    2018-12-22,15.6\n    2018-12-23,15.7\n    2018-12-24,15.8\n    2018-12-25,15.9\n    2018-12-26,16.0\n    2018-12-27,16.1\n    2018-12-28,16.2\n    2018-12-29,16.3\n    2018-12-30,16.4\n    2018-12-31,16.5\n    2019-01-01,16.6\n    2019-01-02,16.7\n    2019-01-03,16.8\n    2019-01-04,16.9\n    2019-01-05,17.0\n    2019-01-06,17.1\n    2019-01-07,17.2\n    2019-01-08,17.3\n    2019-01-09,17.4\n    2019-01-10,17.5\n    2019-01-11,17.6\n    2019-01-12,17.7\n    2019-01-13,17.8\n    2019-01-14,17.9\n    2019-01-15,18.0\n    2019-01-16,18.1\n    2019-01-17,18.2\n    2019-01-18,18.3\n    2019-01-19,18.4\n    2019-01-20,18.5\n    2019-01-21,18.6\n    2019-01-22,18.7\n    2019-01-23,18.8\n    2019-01-24,18.9\n    2019-01-25,19.0\n    2019-01-26,19.1\n    2019-01-27,19.2\n    2019-01-28,19.3\n    2019-01-29,19.4\n    2019-01-30,19.5\n    2019-01-31,19.6\n    2019-02-01,19.7\n    2019-02-02,19.8\n    2019-02-03,19.9\n    2019-02-04,20.0\n    2019-02-05,20.1\n    2019-02-06,20.2\n    2019-02-07,20.3\n    2019-02-08,20.4\n    2019-02-09,20.5\n    2019-02-10,20.6\n    2019-02-11,20.7\n    2019-02-12,20.8\n    2019-02-13,20.9\n    2019-02-14,21.0\n    2019-02-15,21.1\n    2019-02-16,21.2\n    2019-02-17,21.3\n    2019-02-18,21.4\n    2019-02-19,21.5\n    2019-02-20,21.6\n    2019-02-21,21.7\n    2019-02-22,21.8\n    2019-02-23,21.9\n    2019-02-24,22.0\n    2019-02-25,22.1\n    2019-02-26,22.2\n    2019-02-27,22.3\n    2019-02-28,22.4\n    2019-03-01,22.5\n    2019-03-02,22.6\n    2019-03-03,22.7\n    2019-03-04,22.8\n    2019-03-05,22.9\n    2019-03-06,23.0\n    2019-03-07,23.1\n    2019-03-08,23.2\n    2019-03-09,23.3\n    2019-03-10,23.4\n    2019-03-11,23.5\n    2019-03-12,23.6\n    20"}], "metadata": {"file_name": "dag_45_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of a cloud computing resource, showing how to programmatically control a SageMaker notebook instance through different operational states. Illustrate the step-by-step process of resource provisioning, activation, deactivation, and cleanup using AWS SageMaker operators.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n    SageMakerStartNotebookOperator,\n    SageMakerStopNotebookOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import SageMakerNotebookSensor, SageMakerNotebookSensorTimeout\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.edgemodifier import Edgemodifier\nfrom airflow.utils.trigger_rule import FinalTriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_sagemaker_notebook\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_WORKSPACE_NAME = \"sample-sagemaker-workspace-alias\"\nSAGEMAKER_NOTEBOOK_NAME = \"test-notebook\"\nSUBNET_FILE = \"subnet-ec2-file\"\nROLE_ARN = \"127.0.0.1\"\n\nwith DAG(\n    DAG_ID,\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"system-test\"],\n    on_finish=Edgemodifier(add_var=ENV_ID_KEY),\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    subnet_file_path = test_context[SUBNET_FILE]\n    role_arn = test_context[ROLE_ARN]\n\n    # [START howto_operator_sagemaker_notebook_create]\n    create_notebook = SageMakerCreateNotebookOperator(\n        task_id=\"create_notebook\",\n        image=\"amazon/aws-sagemaker-notebook:latest\",\n        role_arn=role_arn,\n        workspace_name=SAMPLE_WORKSPACE_NAME,\n        instance_type=\"ml2.xlarge\",\n        instance_count=1,\n        cpu=1,\n        memory=\"2 GiB\",\n        subnet_file=subnet_file_path,\n        vpc_security_group_ids=[\"sg-0c380ae3200d9669e\"],\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_sagemaker_notebook_create]\n\n    # [START howto_sensor_sagemaker_notebook]\n    wait_for_notebook_create = SageMakerNotebookSensor(\n        task_id=\"wait_for_notebook_create\",\n        region=\"us-east-1\",\n        account_id=\"123456789012\",\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n    )\n    # [END howto_sensor_sagemaker_notebook]\n\n    # [START howto_operator_sagemaker_notebook_start]\n    start_notebook = SageMakerStartNotebookOperator(\n        task_id=\"start_notebook\",\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n    )\n    # [END howto_operator_sagemaker_notebook_start]\n\n    # [START howto_operator_sagemaker_notebook_stop]\n    stop_notebook = SageMakerStopNotebookOperator(\n        task_id=\"stop_notebook\",\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n        timeout_seconds=60,\n    )\n    # [END howto_operator_sagemaker_notebook_stop]\n\n    # [START howto_sensor_sagemaker_notebook_timeout]\n    wait_for_notebook_stop = SageMakerNotebookSensorTimeout(\n        task_id=\"wait_for_notebook_stop\",\n        region=\"us-east-1\",\n        account_id=\"123456789012\",\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n        timeout_seconds=6,\n    )\n    # [END howto_sensor_sagemaker_notebook_timeout]\n\n    # [START howto_operator_sagemaker_notebook_delete]\n    delete_notebook = SageMakerDeleteNotebookOperator(\n        task_id=\"delete_notebook\",\n        region=\"us-east-1\",\n        account_id=\"123456789012\",\n        sagemaker_job_name=SAGEMAKER_NOTEBOOK_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_sagemaker_notebook_delete]\n\n    chain(\n        test_context,\n        create_notebook,\n        wait_for_notebook_create,\n        start_notebook,\n        stop_notebook,\n        wait_for_notebook_stop,\n        delete_notebook,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it done\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_418_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data privacy workflow that automatically de-identifies sensitive phone numbers in tabular data using Google Cloud's Data Loss Prevention (DLP) service. Create an Airflow pipeline that demonstrates how to mask, template, and optionally reverse sensitive information transformations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobOperator,\n    CloudDLPDeidentifyDataOperator,\n    CloudDLPDeleteJobOperator,\n    CloudDLPDeidentifyTemplateOperator,\n    CloudDLPDeleteTemplateOperator,\n    CloudDLPGetDeidentifyTemplateOperator,\n    CloudDLPListDeidentifyTemplatesOperator,\n    CloudDLPRunDeidentifyDataOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDLP_LOCATION = os.environ.get(\"DLP_LOCATION\", \"us-central1\")\nDEIDENTIFY_TEMPLATE_ID = f\"deidentify-{ENV_ID}-template\"\nDEIDENTIFY_TEMPLATE = {\n    \"display_name\": f\"deidentify-{ENV_ID}-template\",\n    \"info_type\": {\"name\": \"STRING\", \"field_mask\": {\"paths\": [\"display_name\"]}},\n    \"transformations\": [\n        {\n            \"field\": {\"name\": \"abn\"},\n            \" transformation\": {\n                \"mask\": {\n                    \"simple_mask\": {\n                        \"bad_data\": {\n                            \"phone_numbers\": {\n                                \"action\": \"REPLACE_WITH_OUTLANDER_NUMBER\",\n                            }\n                        },\n                    },\n                },\n            },\n        },\n    ],\n}\nDATA = r\"\"\"name,Middle Name,Abn,Address Line 1,Address Line 2,City,Postcode\nAllardyce,Geoffrey,1234567890,Apricot Drive,Bateman Park,Auckland,1010\nAlfieri,Albert,9876543210,Brampton Grove,Blackfriars,Auckland,1020\nAmundsen,Robert,9876543211,Cleopatra Drive,Campbelltown,Auckland,1030\nAnderson,Donna,8765432109,Diamond Street,Doonowen,Auckland,1040\nArctic,John,8765432108,Dove Walk,Bird Hill,Auckland,1050\nAziz,Zain,8765432107,Egyptian Avenue,Eastern Suburbs,Auckland,1060\nBaker,Lesley,8765432106,Eagle Gully Drive,Enogue,Auckland,1070\nBall,Clare,8765432105,Evergates Road,Flatswood,Auckland,1080\nBarnes,Barbara,8765432104,Evergates Court,Fitzhalmom,Auckland,1090\nBarth,Andreas,8765432103,Eyre Peninsula Drive,Glengarry,Auckland,1100\nBennett,Clive,8765432102,Eyre Peninsula Highway,Gisborne,Auckland,1110\nBernstein,Leon,8765432101,Evergates Oval,Gisborne,Auckland,1120\nBird,Elaine,8765432100,Evergates Park,Gisborne,Auckland,1130\"\"\"\nDEIDENTIFIED_DATA = r\"\"\"name,Middle Name,Abn,Address Line 1,Address Line 2,City,Postcode\nAllardyce,Geoffrey,2018654321,Apricot Drive,Bateman Park,Auckland,1010\nAlfieri,Albert,7899876543,Brampton Grove,Blackfriars,Auckland,1020\nAmundsen,Robert,7899876543,Cleopatra Drive,Campbelltown,Auckland,1030\nAnderson,Donna,6543210987,Diamond Street,Doonowen,Auckland,1040\nArctic,John,6543210876,Dove Walk,Bird Hill,Auckland,1050\nAziz,Zain,6543210789,Egyptian Avenue,Eastern Suburbs,Auckland,1060\nBaker,Lesley,6543210654,Eagle Gully Drive,Enogue,Auckland,1070\nBall,Clare,6543210543,Evergates Road,Flatswood,Auckland,1080\nBarnes,Barbara,6543210432,Evergates Court,Fitzhalmom,Auckland,1090\nBarth,Andreas,6543210321,Eyre Peninsula Drive,Glengarry,Auckland,1100\nBennett,Clive,6543210210,Eyre Peninsula Highway,Gisborne,Auckland,1110\nBernstein,Leon,6543210109,Evergates Oval,Gisborne,Auckland,1120\nBird,Elaine,6543210098,Evergates Park,Gisborne,Auckland,1130\"\"\"\nDLP_TEMPLATE_BUCKET = os.environ.get(\"DLP_TEMPLATE_BUCKET\", \"dlp-tmp-1\")\nDLP_DEIDENTIFY_JOB_ID = f\"deidentify-{ENV_ID}-job\"\nDLP_DEIDENTIFIED_DATA_BUCKET = os.environ.get(\"DLP_DEIDENTIFIED_DATA_BUCKET\", \"dlp-bucket-1\")\nDLP_DEIDENTIFIED_GCS_URI = f\"gs://{DLP_DEIDENTIFIED_DATA_BUCKET}/deidentify_file.csv\"\nGCS_PATH = f\"gs://{DLP_DEIDENTIFIED_DATA_BUCKET}/\"\nDLP_DEIDENTIFIED_GCS_PATH = f\"gs://{DLP_DEIDENTIFIED_DATA_BUCKET}/\"\nVIEW = {\n    \"view_for_deidentification\": {\n        \"data_source\": {\"raw_data\": {\"input_data\": DLP_DEIDENTIFIED_GCS_URI}},\n        \"field_mask\": {\"paths\": [\"abn\"]],\n    }\n}\n\nwith models.DAG(\n    \"example_dlp_deidentify\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp\"],\n) as dag:\n    # [START dlp_create_deidentify_template_operator]\n    create_deidentify_template = CloudDLPCreateJobOperator(\n        task_id=\"create_deidentify_template\",\n        project_id=PROJECT_ID,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        deidentify_template=DEIDENTIFY_TEMPLATE,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_create_deidentify_template_operator]\n\n    # [START dlp_get_deidentify_template_operator]\n    get_deidentify_template = CloudDLPGetDeidentifyTemplateOperator(\n        task_id=\"get_deidentify_template\",\n        project_id=PROJECT_ID,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_get_deidentify_template_operator]\n\n    # [START dlp_list_deidentify_templates_operator]\n    list_deidentify_templates = CloudDLPListDeidentifyTemplatesOperator(\n        task_id=\"list_deidentify_templates\",\n        project_id=PROJECT_ID,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_list_deidentify_templates_operator]\n\n    # [START dlp_deidentify_data_operator]\n    deidentify_data = CloudDLPDeidentifyDataOperator(\n        task_id=\"deidentify_data\",\n        project_id=PROJECT_ID,\n        view=VIEW,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        input_data_source=\"gs://vision-dlp-sha2888888888888888888888888888888888888888888888888888888888888888/deidentify_file.csv\",\n        location=DLP_LOCATION,\n    )\n    # [END dlp_deidentify_data_operator]\n\n    # [START dlp_run_deidentify_data_operator]\n    run_deidentify_data = CloudDLPRunDeidentifyDataOperator(\n        task_id=\"run_deidentify_data\",\n        project_id=PROJECT_ID,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        input_data_source=DLP_DEIDENTIFIED_GCS_PATH,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_run_deidentify_data_operator]\n\n    # [START dlp_delete_deidentify_template_operator]\n    delete_deidentify_template = CloudDLPDeleteJobOperator(\n        task_id=\"delete_deidentify_template\",\n        project_id=PROJECT_ID,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_delete_deidentify_template_operator]\n\n    # [START dlp_delete_template_operator]\n    delete_template = CloudDLPDeleteTemplateOperator(\n        task_id=\"delete_template\",\n        project_id=PROJECT_ID,\n        deidentify_template_id=DEIDENTIFY_TEMPLATE_ID,\n        location=DLP_LOCATION,\n    )\n    # [END dlp_delete_template_operator]\n\n    [create_deidentify_template, run_deidentify_data, delete_deidentify_template] >> delete_template\n\n\nfrom tests.system.utils import get"}], "metadata": {"file_name": "dag_503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a time series forecasting system that processes retail sales data with granular geographic attributes like city, zip code, and county. Develop an automated pipeline that transforms raw sales data into predictive insights with a 30-day forecast horizon.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.azureml import AzureMLTrainingJobOperator\n\n\n@task\ndef create_cluster():\n    # Cluster definition\n    cluster = {\n        \"availability\": {\"zoneRequirement\": {\"networkLoadBalancingType\": \"NONE\"}},\n        \"computerVisionSettings\": {\"requireSshAccess\": True},\n        \"virtualMachineSettings\": {\n            \"imageName\": \"ubuntu-system-2\",\n            \"machineType\": \"Standard_D2s_v3\",\n            \"commandLineParameters\": [\"sudo apt update\"],\n        },\n    }\n\n    # Training job definition\n    training_job = {\n        \"trainingJobName\": \"test-azureml-training-job\",\n        \"baseExperimentTag\": \"test-azureml-training-job\",\n        \"compute\": cluster,\n        \"trainingJobDetailSchema\": {},\n        \"trainingJobEndpointDetails\": {},\n        \"trainingScriptDetails\": {\n            \"scriptPath\": \"/usr/lib/spark/bin/run-example\",\n            \"scriptArgs\": [\"--main-class\", \"com.continuum.io.iiif.AlchemyMain\", \"--train\"],\n        },\n        \"trainingDatasetDetails\": {\n            \"dataSource\": {\n                \"dataFiles\": [\n                    \"s3://aws-ml-datasets/Kaggle Retail Sales/retail_sales_train.csv\",\n                    \"s3://aws-ml-datasets/Kaggle Retail Sales/retail_sales_test.csv\",\n                ],\n                \"schema\": {\n                    \"fields\": [\n                        {\"name\": \"d_date\", \"type\": \"string\", \"nullable\": True, \"allowEmptyValue\": True},\n                        {\"name\": \"store_id\", \"type\": \"string\", \"nullable\": False},\n                        {\"name\": \"item_id\", \"type\": \"string\", \"nullable\": False},\n                        {\"name\": \"item_weight\", \"type\": \"double\", \"nullable\": True},\n                        {\"name\": \"instock\", \"type\": \"boolean\", \"nullable\": True},\n                        {\"name\": \"sales\", \"type\": \"double\", \"nullable\": False},\n                    ]\n                },\n            },\n            \"targetColumn\": \"sales\",\n            \"splitColumns\": [],\n            \"dataSourceConfig\": {\n                \"S3DataSources\": [\n                    {\"S3Endpoint\": \"s3://dk-azureml-dataset/retail_sales_train.csv\", \"WritingMode\": \"TRUNCATE\"},\n                    {\"S3Endpoint\": \"s3://dk-azureml-dataset/retail_sales_test.csv\", \"WritingMode\": \"OVERWRITE\"},\n                ]\n            },\n        },\n        \"trainingJobExperimentTag\": \"test-azureml-training-job-experiment\",\n        \"trainingJobObservationTimeout\": 1200,\n    }\n\n    # Call the AzureMLTrainingJobOperator\n    AzureMLTrainingJobOperator(task_id=\"create_azureml_training_job_task\", training_job=training_job)\n\n\nwith DAG(\n    \"azureml_training_job\",\n    schedule=pendulum.period(pendulum.datetime(2021, 1, 1), pendulum.datetime(2021, 1, 2)),\n    start_date=pendulum.datetime(2021, 1, 1),  # Schedule the DAG to start on the first day of the period\n    catchup=False,\n    tags=[],\n) as dag:\n    create_cluster()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2113_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data transformation pipeline that translates multilingual documents across different file formats (XLSX, DOCX, PDF) using cloud translation services. Implement a workflow that handles language conversion, manages input and output configurations, and demonstrates scalable document processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.translate import CloudTranslateTextOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_translate_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\ninput_data = {\n    \"gcp_translate_dataset\": {\n        \"display_name\": f\"dataset_{DAG_ID}_{ENV_ID}\",\n        \"source_language_code\": \"en\",\n        \"target_language_code\": \"es\",\n    }\n}\noutput_data = {\n    \"gcp_translate_output_dataset\": {\n        \"display_name\": f\"output_{DAG_ID}_{ENV_ID}\",\n        \"source_language_code\": \"en\",\n        \"target_language_code\": \"fr\",\n    }\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"translate\", \"gcp\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = S3ToGCSOperator(\n        task_id=\"upload_file\",\n        src=f\"resources/{FILE_NAME}\",\n        dest=f\"{BUCKET_NAME}/{FILE_NAME}\",\n        replace=True,\n    )\n\n    # [START translate_apply_v2_operator]\n    translate_v2 = CloudTranslateTextOperator(\n        task_id=\"translate_apply_v2_operator\",\n        project_id=PROJECT_ID,\n        source_language_code=input_data[\"gcp_translate_dataset\"][\"source_language_code\"],\n        target_language_code=input_data[\"gcp_translate_dataset\"][\"target_language_code\"],\n        gcs_source=input_data[\"gcp_translate_dataset\"][\"gcs_source\"],\n        format_=\"auto\",\n    )\n    # [END translate_apply_v2_operator]\n\n    # [START translate_async_v2_operator]\n    translate_v2_async = CloudTranslateTextOperator(\n        task_id=\"translate_v2_async\",\n        project_id=PROJECT_ID,\n        source_language_code=input_data[\"gcp_translate_dataset\"][\"source_language_code\"],\n        target_language_code=input_data[\"gcp_translate_dataset\"][\"target_language_code\"],\n        gcs_source=input_data[\"gcp_translate_dataset\"][\"gcs_source\"],\n        format_=\"auto\",\n        deferrable=True,\n    )\n    # [END translate_async_v2_operator]\n\n    # [START translate_operator]\n    translate_v3 = CloudTranslateTextOperator(\n        task_id=\"translate_v3\",\n        project_id=PROJECT_ID,\n        contents=[\"Airflow is a simple system that enables easy creation of data pipelines. The system consists of a set of reusable python tasks that can be combined using simple chaining logic.\"],\n        format_=\"auto\",\n    )\n    # [END translate_operator]\n\n    # [START translate_async_operator]\n    translate_v3_async = CloudTranslateTextOperator(\n        task_id=\"translate_v3_async\",\n        project_id=PROJECT_ID,\n        contents=[\"Airflow is a simple system that enables easy creation of data pipelines. The system consists of a set of reusable python tasks that can be combined using simple chaining logic.\"]\n        ,\n        format_=\"auto\",\n        deferrable=True,\n    )\n    # [END translate_async_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> translate_v2\n        >> translate_v2_async\n        >> translate_v3\n        >> translate_v3_async\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as done in test_online_translator)\n    # as done.\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1301_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing workflow that transforms raw project metadata stored in S3 into a QuickSight-ready dataset for business intelligence purposes. Create a systematic approach to extract, transfer, and load (ETL) project information with automated provisioning and cleanup of AWS resources.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateVisualOperator\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightDeleteOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_DATA = {\n    \"state\": \"CA\",\n    \"zip_code\": 90210,\n    \"city\": \"Los Angeles\",\n    \"country\": \"USA\",\n    \"name\": \"John Doe\",\n    \"phone\": \"555-0100\",\n}\n\n\n@task\ndef get_visual_id(bucket_name: str):\n    quickskinthook = QuickSightHook()\n    location_data = quickskinthook.data_location_describe(\n        DataLocationType=\"S3\",\n        S3Location={\"S3BucketName\": bucket_name},\n    )\n    for data_location in location_data[0][\"DataLocations\"]:\n        if data_location[\"DataLocationType\"] == \"S3\":\n            return data_location[\"Id\"]\n    raise Exception(\"Requested bucket not found in any S3 data location.\")\n\n\n@task\ndef create_connection(conn_id_name: str, bucket_name: str):\n    quickskinthook = QuickSightHook()\n    connection = quickskinthook.create_connection(\n        conn_id_name=conn_id_name,\n        bucket_name=bucket_name,\n    )\n    return connection\n\n\n@task\ndef update_data_location_arn(conn_id_name: str, data_location_arn: str):\n    conn = Connection.get_connection(conn_id_name)\n    conn.extra = json.dumps({\"data_location_arn\": data_location_arn})\n\n\n@task\ndef delete_data_location(conn_id_name: str):\n    conn = Connection.get_connection(conn_id_name)\n    conn.extra = \"\"\n    conn.update()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_aws_resources(conn_id_name: str):\n    quickskinthook = QuickSightHook()\n    conn = Connection.get_connection(conn_id_name)\n    quickskinthook.delete_connection(conn_id=conn.id)\n\n\nwith DAG(\n    \"example_quicksight\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    bucket_name = f\"{env_id}-quickskinthook-bucket\"\n    data_location_id = \"\"\n    visual_id = \"\"\n    conn_id = f\"{env_id}-quickskinthook-conn\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    create_data_location = S3CreateObjectOperator(\n        task_id=\"create_data_location\",\n        s3_bucket_name=bucket_name,\n        s3_key=\"empty_file.txt\",\n        s3_key_encoding=\"UTF-8\",\n        sample_data=SAMPLE_DATA,\n        task_id=\"create_data_location\",\n    )\n    # [START howto_quickskinthook_create_data_location_from_aws_s3_arn]\n    create_data_location_from_aws_s3_arn = S3CreateObjectOperator(\n        task_id=\"create_data_location_from_aws_s3_arn\",\n        sample_data=SAMPLE_DATA,\n    )\n    # [END howto_quickskinthook_create_data_location_from_aws_s3_arn]\n    create_connection_task = create_connection(conn_id_name=conn_id, bucket_name=bucket_name)\n\n    get_data_location_arn = GetDataLocationArnOperator(\n        task_id=\"get_data_location_arn\",\n        conn_id_name=conn_id,\n    )\n\n    set_data_location_arn_extrra = UpdateConnectionExtraOperator(\n        task_id=\"set_data_location_arn_extrra\",\n        conn_id_name=conn_id,\n        extra=get_data_location_arn.output,\n    )\n\n    # [START howto_quickskinthook_create_visual]\n    create_visual = QuickSightCreateVisualOperator(\n        task_id=\"create_visual\",\n        data_location_id=data_location_id,\n        visual_id=visual_id,\n        display_name=\"Test Visual\",\n        data_source_type=\"S3\",\n        data_source_arn=get_data_location_arn.output[\"Id\"],\n        query=\"select * from table\",\n    )\n    # [END howto_quickskinthook_create_visual]\n\n    # [START howto_quickskinthook_delete_visual]\n    delete_visual = QuickSightDeleteOperator(task_id=\"delete_visual\", visual_id=visual_id)\n    # [END howto_quickskinthook_delete_visual]\n\n    # [START howto_quickskinthook_sensor_create_data_location_from_aws_s3_arn]\n    sensor_create_data_location_from_aws_s3_arn = QuickSightSensor(\n        task_id=\"sensor_create_data_location_from_aws_s3_arn\", visual_id=visual_id\n    )\n    # [END howto_quickskinthook_sensor_create_data_location_from_aws_s3_arn]\n\n    sensor_create_data_location_from_aws_s3_arn_def = QuickSightSensor(\n        task_id=\"sensor_create_data_location_from_aws_s3_arn_def\", visual_id=visual_id\n    )\n\n    # [START howto_quickskinthook_sensor_create_visual]\n    sensor_create_visual = QuickSightSensor(task_id=\"sensor_create_visual\", visual_id=visual_id)\n    # [END howto_quickskinthook_sensor_create_visual]\n\n    sensor_create_visual_def = QuickSightSensor(task_id=\"sensor_create_visual_def\", visual_id=visual_id)\n\n    # [START howto_quickskinthook_sensor_delete_visual]\n    sensor_delete_visual = QuickSightSensor(task_id=\"sensor_delete_visual\", visual_id=visual_id)\n    # [END howto_quickskinthook_sensor_delete_visual]\n\n    sensor_delete_visual_def = QuickSightSensor(task_id=\"sensor_delete_visual_def\", visual_id=visual_id)\n\n    delete_aws_resources = delete_aws_resources(conn_id_name=conn_id)\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_data_location\n        >> create_connection_task\n        >> get_data_location_arn\n        >> set_data_location_arn_extrra\n        # TEST BODY\n        >> create_visual\n        >> sensor_create_data_location_from_aws_s3_arn\n        >> sensor_create_data_location_from_aws_s3_arn_def\n        >> sensor_create_visual\n        >> sensor_create_visual_def\n        # TEST TEARDOWN\n        >> delete_aws_resources\n        >> sensor_delete_visual\n        >> sensor_delete_visual_def\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as succeeded / failed\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates parallel task execution and infrastructure management using Airflow operators for Google Cloud Compute Engine. The DAG will demonstrate creating a VM instance, running parallel SSH commands with different authentication techniques, and then cleaning up the infrastructure.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.providers.ssh.transfers.copy_file import CopyFileTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table import CopyIntoTableTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_postgres import CopyIntoTablePostgresTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_sql import CopyIntoTableSqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_sqlite import CopyIntoTableSqliteTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_teradata import CopyIntoTableTeradataTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_mssql import CopyIntoTableMssqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_mysql import CopyIntoTableMySqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_oracle import CopyIntoTableOracleTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_spark import CopyIntoTableSparkTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_teradata import CopyIntoTableTeradataTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_sqlite import CopyIntoTableSqliteTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_sql import CopyIntoTableSqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_teradata import CopyIntoTableTeradataTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_mssql import CopyIntoTableMssqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_mysql import CopyIntoTableMySqlTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_oracle import CopyIntoTableOracleTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_spark import CopyIntoTableSparkTransfer\nfrom airflow.providers.ssh.transfers.copy_into_table_postgres import CopyIntoTablePostgresTransfer\nfrom airflow.providers.ssh.transfers.copy_from_file import CopyFromFileTransfer\nfrom airflow.providers.ssh.transfers.copy_from_sql import CopyFromSqlTransfer\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_ssh_transfer\"\n\nssh_conn_id = \"ssh_default\"\nconn = Session().query(Connection).filter(Connection.login == \"ssh_user\").first()\npublic_key_file = conn.extra_json.get(\"public_key_file\", None)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_public_key_public_key():\n        \"\"\"\n        Example task to create public key.\n        \"\"\"\n        from airflow.providers.ssh.utils import create_public_key\n\n        create_public_key(public_key_file)\n\n    @task\n    def create_ssh_connection(ssh_conn_id, ssh_user):\n        \"\"\"\n        Example task to create SSH connection.\n        \"\"\"\n        from airflow.providers.ssh.utils import create_ssh_connection\n\n        create_ssh_connection(ssh_conn_id, ssh_user)\n\n    @task\n    def remove_ssh_connection(ssh_conn_id):\n        \"\"\"\n        Example task to remove SSH connection.\n        \"\"\"\n        from airflow.providers.ssh.utils import remove_ssh_connection\n\n        remove_ssh_connection(ssh_conn_id)\n\n    @task\n    def remove_public_key_public_key():\n        \"\"\"\n        Example task to remove public key.\n        \"\"\"\n        from airflow.providers.ssh.utils import remove_public_key_public_key\n\n        remove_public_key_public_key()\n\n    @task\n    def get_public_key_public_key():\n        \"\"\"\n        Example task to get public key.\n        \"\"\"\n        from airflow.providers.ssh.utils import get_public_key_public_key\n\n        get_public_key_public_key(public_key_file)\n\n    @task\n    def create_instance():\n        \"\"\"\n        Example task to create instance.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.compute import (\n            compute_create_instance_template_operator,\n            compute_delete_instance_operator,\n        )\n\n        from airflow.utils.trigger_rule import TriggerRule\n\n        template_name = \"gi-t-assign-network-interfaces-template\"\n        zone = \"us-central1-a\"\n        machine_type = \"n1-standard-1\"\n        name = \"gi-t-assign-network-interfaces-template\"\n        service_net = \"global/networks/default\"\n\n        compute_create_instance_template_operator(\n            task_id=\"create_instance_template\",\n            project_id=\"cloud-compute-test-8888\",\n            zone=zone,\n            body={\n                    \"name\": name,\n                    \"machine_type\": f\"zones/{zone}/machineTypes/{machine_type}\",\n                    \"disks\": [\n                        {\n                            \"auto_delete\": True,\n                            \"boot\": True,\n                            \"device_name\": name,\n                            \"initialize_params\": {\n                                \"disk_size_gb\": \"10\",\n                                \"disk_name\": name,\n                                \"source_image_uri\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20231023\",\n                                \"source_element_group\": None,\n                                \"source_snapshot\": f\"global/snapshots/{service_net}/zones/{zone}/snapshots/{service_net}\",\n                            },\n                        }\n                    ],\n                    \"network_interfaces\": [\n                        {\n                            \"access_configs\": [\n                                {\n                                    \"name\": \"NetworkInterface-ExternalNetworkConfig\",\n                                    \"network\": f\"global/networks/{service_net}\",\n                                }\n                            ],\n                            \"machine_spec\": {\n                                \"machine_type\": f\"zones/{zone}/machineTypes/{machine_type}\"\n                            },\n                        }\n                    ],\n                    \"metadata\": {\n                        \"http_port_80\": \"\",\n                        \"http_port_22\": \"\",\n                        \"http_port_2100\": \"\",\n                        \"http_port_inbound_2100\": \"\",\n                        \"http_port_5432\": \"\",\n                        \"http_port_5433\": \"\",\n                        \"http_port_5434\": \"\",\n                        \"http_port_5432_inbound\": \"\",\n                        \"http_port_5433_inbound\": \"\",\n                        \"http_port_5434_inbound\": \"\",\n                    },\n                    \"service_endpoints\": [],\n                    \"network_policy\": {},\n                    \"post_startup_script\": [],\n                },\n        )\n\n        delete_operator = delete_instance = delete_instance_operator = None\n        with trigger_rule=TriggerRule.ALL_DONE:\n            delete_operator = compute_delete_instance_operator(\n                task_id=\"delete_instance_template\",\n                project_id=\"cloud-compute-test-8888\",\n                zone=zone,\n                resource_name=f\"places-templates/{template_name}\",\n            )\n            delete_instance = delete_instance_operator(\n                task_id=\"delete_instance\",\n                zone=zone,\n                resource_name=f\"places-templates/{template_name}\",\n            )\n\n        delete_operator.trigger_delete_on_any_operator_completion = True\n        delete_instance.trigger_delete_on_any_operator_completion = True\n\n    create_instance = create_instance()\n    get_public_key_public_key = get_public_key_public_key()\n    create_ssh_connection = create_ssh_connection(ssh_conn_id, \"ssh_user\")\n    remove_ssh_connection = remove_ssh_connection(ssh_conn_id)\n    remove_public_key_public_key = remove_public_key_public_key()\n    instance_template = compute_instance_template_zone()\n\n    # [START howto_transfer_copy_into_table_mysql]\n    copy_into_table_mysql = CopyIntoTableMySqlTransfer(\n        task_id=\"copy_into_table_mysql\",\n        source_conn_id=None,\n        query=\"SELECT * FROM table1\",\n        destination_conn_id=\"mysql_default\",\n        destination_schema=\"dest_table\",\n        destination_table=\"table2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_copy_into_table_mysql]\n\n    # [START howto_transfer_copy_into_table_postgres]\n    copy_into_table_postgres = CopyIntoTablePostgresTransfer(\n        task_id=\"copy_into_table_postgres\",\n        source_conn_id=None,\n        source_schema=\"test_db\",\n        source_table=\"test_table1\",\n        destination_conn_id=\"postgres_default\",\n        destination_schema=\"dest_table\",\n        destination_table=\"test_table2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_copy_into_table_postgres]\n\n    # [START howto_transfer_copy_into_table_sql]\n    copy_into_table_sql = CopyIntoTableSqlTransfer(\n        task_id=\"copy_into_table_sql\",\n        source_conn_id=None,\n        source_sql=\"SELECT * FROM table1\",\n        destination_conn_id=\"sqlite_default\",\n        destination_schema=\"dest_table\",\n        destination_table=\"test_table2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_copy_into_table_sql]\n\n    # [START howto_transfer_copy_into_table_sqlite]\n    copy_into_table_sqlite = CopyIntoTableSqliteTransfer(\n        task_id=\"copy_into_table_sqlite\",\n        source_conn_id=None,\n        source_sql=\"SELECT * FROM table1\",\n        destination_conn_id=\"sqlite_default\",\n        destination_schema=\"dest_table\",\n        destination_table=\"test_table2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_copy_into_table_sqlite]\n\n    # [START howto_transfer_copy_into_table_teradata]\n    copy_into_table_teradata = CopyIntoTableTeradataTransfer(\n        task_id=\"copy_into_table_teradata\",\n        source_conn_id=None,\n        source_table=\"table1\",\n        destination_conn_id=\"teradata_default\",\n        destination_table=\"test_table2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_copy_into_table_teradata]\n\n    # [START howto_transfer_copy_into_table_mssql]\n    copy_into_table_mssql = CopyIntoTableMssqlTransfer(\n        task_id=\"copy_into_table_mssql\",\n        source_conn_id=None"}], "metadata": {"file_name": "dag_2011_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates transferring files between Google Drive and Google Cloud Storage, showcasing cross-platform file movement and synchronization capabilities. Create a DAG that illustrates how enterprise data can be seamlessly migrated between different cloud storage platforms.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcp\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"google_cloud_platform_default\",\n            user=\"\",\n            description=\"Google Cloud Platform default connection.\",\n            schema=\"default\",\n        )\n        conn2 = Connection(\n            conn_id=\"google_cloud_platform_cloud\",\n            user=\"\",\n            description=\"Google Cloud Platform connection for Cloud Storage.\",\n            schema=\"bigquery',\n        )\n        session = getattr(importlib.import_module(\" airflow.providers.standard.operators.sql\"), \"SqlOperator\")()\n        session.create_task(conn)\n        session.create_task(conn2)\n\n    @task\n    def create_role():\n        role = Role()\n        role.name = \"gcp_provided_role\"\n        role.add_option(\n            \"gcp_project_id\",\n            Role.Property(type=\"string\",\n                           description=\"Project ID for the Google Cloud project.  This is also the \" +\n                                        \"project ID in the Google Cloud Console.\"),\n        )\n        role.add_option(\n            \"gcp_location\",\n            Role.Property(type=\"string\",\n                         description=\"Location for service account.  This location must be the same \" +\n                                        \"as the project location.\",\n                         ),\n        )\n        role.add_option(\n            \"gcp_service_account\",\n            Role.Property(type=\"string\",\n                         description=\"Path to service account file.  Service account needs \" +\n                                        \"permissions for all operations listed below.\"),\n        )\n        role.add_option(\n            \"gcp_service_account_scopes\",\n            Role.Property(type=\"string\",\n                         description=\"Scopes required for service account.  For most operations \"\n                                        \"this can be '' (the empty string).  If running the \"\n                                        \"command on behalf of a user (for example gcloud auth \"\n                                        \"login) then you will need scopes such as \"\n                                        \"'https://www.googleapis.com/auth/cloud-platform'.\",\n                        ),\n        )\n        role.add_option(\n            \"gcp_location_id\",\n            Role.Property(type=\"string\",\n                         description=\"Location ID of Dataproc cluster.\",\n                        ),\n        )\n        role.add_option(\n            \"gcp_cluster_name\",\n            Role.Property(type=\"string\",\n                         description=\"Dataproc cluster name.\",\n                        ),\n        )\n        role.add_option(\n            \"gcp_instance_name\",\n            Role.Property(type=\"string\",\n                         description=\"Dataproc instance name.\",\n                        ),\n        )\n        role.add_option(\n            \"gcp_config_name\",\n            Role.Property(type=\"string\",\n                         description=\"Configured Dataproc queue name.\",\n                        ),\n        )\n        role.add_option(\n            \"gcp_zone\",\n            Role.Property(type=\"string\",\n                         description=\"GCE zone for Dataproc cluster.\",\n                        ),\n        )\n        session = getattr(importlib.import_module(\" airflow.providers.standard.operators.sql\"), \"SqlOperator\")()\n        session.create_task(role)\n\n    @task\n    def create_task_group():\n        task_group = TaskGroup(\"task_group_1\")\n        bash_task = BashOperator(\n            task_id=\"bash_task\",\n            bash_command='echo \"Hello from Bash!\"',\n        )\n        bash_task2 = BashOperator(\n            task_id=\"bash_task2\",\n            bash_command='echo \"Hello from Bash!\"',\n        )\n        trigger = Trigger.trigger_run(\n            task_id=\"trigger_run\",\n            python_callable=lambda **kwargs: print(\"executing task_group_1\"),\n        )\n        bash_task >> bash_task2 >> trigger\n\n    # [START howto_operator_gcp_create]\n    create_connection()\n    # [END howto_operator_gcp_create]\n\n    # [START howto_operator_gcp_create_from_env]\n    create_connection_from_env = create_connection()\n    # [END howto_operator_gcp_create_from_env]\n\n    # [START howto_operator_gcp_get]\n    get_role = Role.get()\n    # [END howto_operator_gcp_get]\n\n    # [START howto_operator_gcp_list]\n    task_group = TaskGroup(\"task_group_2\")\n    BashOperator(\n        task_id=\"bash_task\",\n        bash_command=f\"echo {get_role.name}\",\n    )\n    BashOperator(\n        task_id=\"bash_task2\",\n        bash_command=f\"echo {get_role.name}\",\n    )\n    # [END howto_operator_gcp_list]\n\n    # [START howto_operator_gcp_list_from_env]\n    task_group_from_env = TaskGroup(\"task_group_3\")\n    BashOperator(\n        task_id=\"bash_task\",\n        bash_command=f\"echo {get_role.name}\",\n    )\n    BashOperator(\n        task_id=\"bash_task2\",\n        bash_command=f\"echo {get_role.name}\",\n    )\n    # [END howto_operator_gcp_list_from_env]\n\n    # [START howto_operator_gcp_update]\n    update_role = Role.update(role_id=\"test_role\", new_role=Role())\n    # [END howto_operator_gcp_update]\n\n    # [START howto_operator_gcp_update_from_env]\n    update_role_from_env = Role.update(role_id=\"test_role_2\", new_role=Role())\n    # [END howto_operator_gcp_update_from_env]\n\n    # [START howto_operator_gcp_delete]\n    delete_role = Role.delete(role_id=\"test_role_3\", trigger_id=\"trigger_run\")\n    # [END howto_operator_gcp_delete]\n\n    # [START howto_operator_gcp_delete_from_env]\n    delete_role_from_env = Role.delete(role_id=\"test_role_4\", trigger_id=\"trigger_run\")\n    # [END howto_operator_gcp_delete_from_env]\n\n    # [START howto_operator_gcp_get_id]\n    get_id_role = Role.get_id()\n    # [END howto_operator_gcp_get_id]\n\n    # [START howto_operator_gcp_get_location]\n    get_location_role = Role.get_location()\n    # [END howto_operator_gcp_get_location]\n\n    # [START howto_operator_gcp_instance_create]\n    instance_create = BashOperator(\n        task_id=\"instance_create\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_instance_create]\n\n    # [START howto_operator_gcp_instance_create_from_env]\n    instance_create_from_env = BashOperator(\n        task_id=\"instance_create_from_env\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_instance_create_from_env]\n\n    # [START howto_operator_gcp_instance_delete]\n    delete_instance = BashOperator(\n        task_id=\"delete_instance\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_instance_delete]\n\n    # [START howto_operator_gcp_instance_delete_from_env]\n    delete_instance_from_env = BashOperator(\n        task_id=\"delete_instance_from_env\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_instance_delete_from_env]\n\n    # [START howto_operator_gcp_create_template]\n    create_template = BashOperator(\n        task_id=\"create_template\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_create_template]\n\n    # [START howto_operator_gcp_get_id_template]\n    get_id_template = BashOperator(\n        task_id=\"get_id_template\",\n        bash_command='echo \"create_template\"',\n    )\n    # [END howto_operator_gcp_get_id_template]\n\n    # [START howto_operator_gcp_get_location_template]\n    get_location_template = BashOperator(\n        task_id=\"get_location_template\",\n        bash_command='echo \"get_id_template\"',\n    )\n    # [END howto_operator_gcp_get_location_template]\n\n    # [START howto_operator_gcp_update_template]\n    update_template = BashOperator(\n        task_id=\"update_template\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_update_template]\n\n    # [START howto_operator_gcp_update_location_template]\n    update_location_template = BashOperator(\n        task_id=\"update_location_template\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_update_location_template]\n\n    # [START howto_operator_gcp_list_templates]\n    list_templates = BashOperator(\n        task_id=\"list_templates\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_list_templates]\n\n    # [START howto_operator_gcp_list_location_templates]\n    list_location_templates = BashOperator(\n        task_id=\"list_location_templates\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_list_location_templates]\n\n    # [START howto_operator_gcp_delete_template]\n    delete_template = BashOperator(\n        task_id=\"delete_template\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_delete_template]\n\n    # [START howto_operator_gcp_create_dataproc]\n    create_dataproc = BashOperator(\n        task_id=\"create_dataproc\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_create_dataproc]\n\n    # [START howto_operator_gcp_create_dataproc_from_env]\n    create_dataproc_from_env = BashOperator(\n        task_id=\"create_dataproc_from_env\",\n        bash_command='echo \"hello world\"',\n    )\n    # [END howto_operator_gcp_create_dat"}], "metadata": {"file_name": "dag_1690_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data governance pipeline that tracks the creation, validation, and cleanup of metadata entries in a systematic workflow. Construct a DAG that simulates real-world metadata management processes with clear create, get, list, and delete operations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_me\")\ndef run_me():\n    print(\"Hi\")\n    print(\"Hello\")\n    return 1\n\n\nwith DAG(\n    \"example_metadata\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    run_me()"}], "metadata": {"file_name": "dag_326_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates the complete lifecycle of a cloud-based distributed computing cluster, highlighting the sequential task dependencies and operational states of a Dataproc cluster. The DAG should illustrate how to programmatically manage cloud infrastructure using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocStartClusterOperator,\n    DataprocStopClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_cluster\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\n# [START how_to_dataproc_create_cluster]\nCREATE_CLUSTER_BODY = {\n    \"master_config\": {\n        \"image_version\": \"1.2.1-ga\",\n        \"config\": {\n            \"zone\": REGION + \"-alpha\",\n            \"config\": {\n                \"master_config\": {\n                    \"machine_type\": \"n1-standard-4\",\n                }\n            },\n        },\n    },\n    \"config\": {\n        \"num_preemptible_workers\": 1,\n        \"worker_config\": {\n            \"image_version\": \"1.2.1-ga\",\n            \"config\": {\n                \"zone\": REGION + \"-alpha\",\n                \"config\": {\n                    \"num_preemptible_workers\": 1,\n                    \"num_preemptible_tasks\": 1,\n                    \"primary_image_version\": \"2.2.3-ga\",\n                    \"machine_type\": \"n1-standard-4\",\n                    \"config_custom_resource\": {\n                        \"jvm_memory_options\": [\n                            {\"below\": \"3GB\", \"option\": \":-Xmn512m\"},\n                            {\"above\": \"3GB\", \"option\": \":-Xmn768m\"},\n                        ]\n                    },\n                },\n            },\n        },\n        \"preemptibility\": \"PREEMPTIBLE\",\n    },\n}\n# [END how_to_dataproc_create_cluster]\n\nwith models.DAG(\n    PROJECT_ID,\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START how_to_dataproc_create_cluster_operator]\n    create_cluster_task = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_task\",\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        body=CREATE_CLUSTER_BODY,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_dataproc_create_cluster_operator]\n\n    # [START how_to_dataproc_stop_cluster_operator]\n    stop_cluster_task = DataprocStopClusterOperator(\n        task_id=\"stop_cluster_task\",\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        body={\"state\": \"STOPPED\"},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_dataproc_stop_cluster_operator]\n\n    # [START how_to_dataproc_start_cluster_operator]\n    start_cluster_task = DataprocStartClusterOperator(\n        task_id=\"start_cluster_task\",\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        body={\"state\": \"RUNNING\"},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_dataproc_start_cluster_operator]\n\n    # [START how_to_dataproc_delete_cluster_operator]\n    delete_cluster_task = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_task\",\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_dataproc_delete_cluster_operator]\n\n    create_cluster_task >> stop_cluster_task >> start_cluster_task >> delete_cluster_task\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases task dependencies and sequential PostgreSQL operations, illustrating how different database tasks can be chained together with clear dependency management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pgdb.hooks.pgdb import PgDBHook\nfrom airflow.providers.pgdb.operators.pgdb import PgDBObjectOperator\nfrom airflow.utils.context import Context\n\n\n@task\ndef create_postgres_hook():\n    \"\"\"\n    Create a Postgres hook which stores the connection details\n    \"\"\"\n    # [START howto_pgdb_hook]\n    hook = PgDBHook(conn_id=\"test_pgdb_default\")\n    # [END howto_pgdb_hook]\n    return hook\n\n\n@task\ndef create_table(hook):\n    \"\"\"\n    Create table using the hook object created in the previous task\n    \"\"\"\n    # [START howto_pgdb_create_table]\n    hook.create_table(\n        table_name=\"test_table\",\n        columns=[\n            (\"id\", \"integer\", \"PRIMARY KEY\"),\n            (\"value\", \"varchar\"),\n        ],\n        insert_values=[\n            (\"value: 'value 1'\"),\n            (\"value: 'value 2'\"),\n        ],\n        rows=[\n            (\"value: 'value 1'\"),\n            (\"value: 'value 2'\"),\n        ],\n        commit=True,\n        use_dict=False,\n    )\n    # [END howto_pgdb_create_table]\n\n\n@task\ndef insert_data(hook, context):\n    \"\"\"\n    Insert data into the table using the hook object created in the previous task\n    \"\"\"\n    # [START howto_pgdb_insert_data]\n    hook.insert_data(table_name=\"test_table\", sql=\"values (%s, %s)\", parameters=(1, \"value 1\"), use_dict=False, **context)\n    # [END howto_pgdb_insert_data]\n\n\n@task\ndef select_data(hook, context):\n    \"\"\"\n    Select data from the table using the hook object created in the previous task\n    \"\"\"\n    # [START howto_pgdb_select_data]\n    hook.select_data(table_name=\"test_table\", sql=\"select * from test_table order by id\", use_dict=False, **context)\n    # [END howto_pgdb_select_data]\n\n\n@task\ndef drop_table(hook, context):\n    \"\"\"\n    Drop the table using the hook object created in the previous task\n    \"\"\"\n    # [START howto_pgdb_drop_table]\n    hook.drop_table(table_name=\"test_table\", use_dict=False, **context)\n    # [END howto_pgdb_drop_table]\n\n\n@task\ndef clean_up(hook, context):\n    \"\"\"\n    Clean up the Postgres hook object created in the first task\n    \"\"\"\n    # [START howto_pgdb_cleanup]\n    hook = None\n    # [END howto_pgdb_cleanup]\n\n\nwith DAG(\n    \"pgdb_operator_example\",\n    schedule=pendulum.timeperiods.to_timeperiods(datetime.timedelta(days=2), duration=datetime.timedelta(hours=1)),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    hook = create_postgres_hook()\n    create_table(hook)\n    insert_data(hook, {\"ti\": hook})\n    select_data(hook, {\"ti\": hook})\n    drop_table(hook, {\"ti\": hook})\n    clean_up(hook, {\"ti\": hook})\n    drop_table(hook)"}], "metadata": {"file_name": "dag_302_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data migration pipeline that checks for the existence of a file in an Azure container and then transfers that file to a specified Google Cloud Storage location, showcasing cross-cloud data synchronization and movement capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.providers.common.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.common.azure.operators.bicep import BicepOperator\nfrom airflow.providers.common.azure.sensors.adls import ADLSSensor\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator, GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_bicep_sheets\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"GCP_SHEET_SPREADSHEET_ID\", \"1234567890qwerty\")\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azure\", \"bicep\", \"sheets\"],\n) as dag:\n    generate_bucket = GCSCreateBucketOperator(task_id=\"generate_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_sheet():\n        connection = Connection(\n            conn_id=\"google_cloud_platform_sheets\",\n            schema={\n                \"type\": \"google_cloud_platform\",\n                \"project_id\": PROJECT_ID,\n                \"spreadsheet_id\": SPREADSHEET_ID,\n            },\n        )\n        role = Role(conn_id=\"google_cloud_platform_sheets\", spreadsheet_id=SPREADSHEET_ID)\n\n        role.create(\n            resource=\"https://www.googleapis.com/auth/spreadsheets\",\n        )\n\n        bucket = Connection(\n            conn_id=\"google_cloud_platform_sheets_bucket\",\n            schema={\n                \"type\": \"google_cloud_platform\",\n                \"project_id\": PROJECT_ID,\n                \"resource\": f\"gs://{BUCKET_NAME}/\",\n            },\n        )\n        role = Role(conn_id=\"google_cloud_platform_sheets_bucket\", resource=f\"gs://{BUCKET_NAME}/\")\n\n        bucket.insert()\n\n        task_group = TaskGroup(task_id=\"task_group\", descr=\"Delete the sheet\"):\n            BashOperator(\n                task_id=\"delete_sheet\",\n                bash_command=f\"gcp sheets delete --project={PROJECT_ID} --spreadsheet={SPREADSHEET_ID}\",\n            )\n\n        trigger = Trigger(task_id=\"trigger\", task=task_group, targets=[generate_bucket])\n\n    create_sheet = create_sheet()\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=\"all-done\"\n    )\n\n    upload_bicep = BicepOperator(\n        task_id=\"upload_bicep\",\n        template_id=\"template\",\n        template_path=\"example_common/azure/bicep/azure-bicep-template.json\",\n    )\n\n    delete_template = BashOperator(\n        task_id=\"delete_template\",\n        bash_command=f\"make -C example_common/azure/bicep/deps/{template_id} rm {template_id}\",\n    )\n\n    (\n        generate_bucket\n        >> upload_bicep\n        >> create_sheet\n        >> [trigger, delete_bucket]\n        >> [delete_template, delete_template]\n        >> ADLSSensor(task_id=\"wait\", path=BUCKET_NAME, poke_interval=10)\n        >> ADLSDeleteOperator(task_id=\"wait\", path=BUCKET_NAME, trigger_rule=\"all-done\")\n    )"}], "metadata": {"file_name": "dag_1176_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an enterprise messaging system DAG that illustrates the complete lifecycle of Azure Service Bus communication, including queue and topic management, message transmission strategies, and subscription handling. Develop a technical demonstration of cloud messaging infrastructure using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.asb import (\n    ASBCreateQueueOperator,\n    ASBDeleteQueueOperator,\n    ASBReceiveMessageOperator,\n    ASBSendMessageOperator,\n    ASBSubscriptionCreateOperator,\n    ASBSubscriptionDeleteOperator,\n    ASBSubscriptionGetOperator,\n    ASBSubscriptionListOperator,\n    ASBTopicCreateOperator,\n    ASBTopicDeleteOperator,\n    ASBTopicReceiveMessageOperator,\n    ASBTopicSendMessageOperator,\n    ASBTopicSubscriptionCreateOperator,\n    ASBTopicSubscriptionDeleteOperator,\n    ASBTopicSubscriptionGetOperator,\n    ASBTopicSubscriptionListOperator,\n)\nfrom airflow.providers.microsoft.azure.utils import get_resource_id\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nAGENCY_ID = os.environ.get(\"AZURE_AGENCY_ID\", \"\")\nCLIENT_ID = os.environ.get(\"AZURE_CLIENT_ID\", \"\")\nTENANT_ID = os.environ.get(\"AZURE_TENANT_ID\", \"\")\nQUEUE_NAME = f\"queue_{ENV_ID}\"\nMESSAGE = \"Test Message\"\nSUBSCRIPTION_NAME = f\"subscription_{ENV_ID}\"\nMESSAGE_LIST = \"Test Message List\"\nMESSAGE_LIST2 = \"Test Message List 2\"\nMESSAGE_LIST3 = \"Test Message List 3\"\nDAG_ID = \"asb_teardown\"\nTOPIC_NAME = f\"topic_{ENV_ID}\"\nTOPIC_NAME2 = f\"topic_{ENV_ID}2\"\nTOPIC_NAME3 = f\"topic_{ENV_ID}3\"\nCONN = Connection(\n    conn_id=\"azure_default\",\n    schema={\"type\": \"aad\"},\n    extra={\n        \"client_id\": CLIENT_ID,\n        \"tenant_id\": TENANT_ID,\n        \"application_id\": AGENCY_ID,\n    },\n)\n\n\nwith DAG(\n    DAG_ID,\n    default_args=get_resource_id(CONN),\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"asb\"],\n) as dag:\n    @task\n    def create_connection():\n        \"\"\"\n        Create Azure Service Bus connection\n        \"\"\"\n        from airflow.providers.microsoft.azure.utils.azure import get_conn_id_info\n\n        return get_conn_id_info(\"azure_service_bus\")\n\n    # [START howto_operator_asb_queue_create]\n    create_queue = ASBCreateQueueOperator(\n        task_id=\"create_queue\",\n        queue_name=QUEUE_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n        exists_to_overwrite=True,\n    )\n    # [END howto_operator_asb_queue_create]\n\n    # [START howto_operator_asb_queue_receive_message]\n    receive_message = ASBTopicReceiveMessageOperator(\n        task_id=\"receive_message\",\n        topic_name=TOPIC_NAME,\n        max_message_count=5,\n        asb_conn_id=\"azure_service_bus\",\n    )\n    # [END howto_operator_asb_queue_receive_message]\n\n    # [START howto_operator_asb_queue_send_message]\n    send_message = ASBTopicSendMessageOperator(\n        task_id=\"send_message\",\n        topic_name=TOPIC_NAME,\n        messages=[MESSAGE],\n        asb_conn_id=\"azure_service_bus\",\n    )\n    # [END howto_operator_asb_queue_send_message]\n\n    # [START howto_operator_asb_topic_create]\n    create_topic = ASBTopicCreateOperator(\n        task_id=\"create_topic\",\n        topic_name=TOPIC_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_create]\n\n    # [START howto_operator_asb_topic_delete]\n    delete_topic = ASBTopicDeleteOperator(\n        task_id=\"delete_topic\",\n        topic_name=TOPIC_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_asb_topic_delete]\n\n    # [START howto_operator_asb_topic_subscription_create]\n    create_subscription = ASBTopicSubscriptionCreateOperator(\n        task_id=\"create_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_create]\n\n    # [START howto_operator_asb_topic_subscription_delete]\n    delete_subscription = ASBTopicSubscriptionDeleteOperator(\n        task_id=\"delete_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_delete]\n\n    # [START howto_operator_asb_topic_subscription_get]\n    get_subscription = ASBTopicSubscriptionGetOperator(\n        task_id=\"get_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_get]\n\n    # [START howto_operator_asb_topic_subscription_list]\n    list_subscriptions = ASBTopicSubscriptionListOperator(\n        task_id=\"list_subscriptions\",\n        topic_name=TOPIC_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_list]\n\n    # [START howto_operator_asb_queue_create]\n    create_queue2 = ASBCreateQueueOperator(\n        task_id=\"create_queue2\",\n        queue_name=QUEUE_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n        exists_to_overwrite=True,\n    )\n    # [END howto_operator_asb_queue_create]\n\n    # [START howto_operator_asb_queue_receive_message]\n    receive_message2 = ASBTopicReceiveMessageOperator(\n        task_id=\"receive_message2\",\n        topic_name=TOPIC_NAME,\n        max_message_count=5,\n        asb_conn_id=\"azure_service_bus\",\n    )\n    # [END howto_operator_asb_queue_receive_message]\n\n    # [START howto_operator_asb_queue_send_message]\n    send_message2 = ASBTopicSendMessageOperator(\n        task_id=\"send_message2\",\n        topic_name=TOPIC_NAME,\n        messages=[MESSAGE_LIST],\n        asb_conn_id=\"azure_service_bus\",\n    )\n    # [END howto_operator_asb_queue_send_message]\n\n    # [START howto_operator_asb_topic_create]\n    create_topic2 = ASBTopicCreateOperator(\n        task_id=\"create_topic2\",\n        topic_name=TOPIC_NAME2,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_create]\n\n    # [START howto_operator_asb_topic_delete]\n    delete_topic2 = ASBTopicDeleteOperator(\n        task_id=\"delete_topic2\",\n        topic_name=TOPIC_NAME2,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_asb_topic_delete]\n\n    # [START howto_operator_asb_topic_subscription_create]\n    create_subscription2 = ASBTopicSubscriptionCreateOperator(\n        task_id=\"create_subscription2\",\n        topic_name=TOPIC_NAME2,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_create]\n\n    # [START howto_operator_asb_topic_subscription_delete]\n    delete_subscription2 = ASBTopicSubscriptionDeleteOperator(\n        task_id=\"delete_subscription2\",\n        topic_name=TOPIC_NAME2,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_delete]\n\n    # [START howto_operator_asb_topic_subscription_get]\n    get_subscription2 = ASBTopicSubscriptionGetOperator(\n        task_id=\"get_subscription2\",\n        topic_name=TOPIC_NAME2,\n        subscription_name=SUBSCRIPTION_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_get]\n\n    # [START howto_operator_asb_topic_subscription_list]\n    list_subscriptions2 = ASBTopicSubscriptionListOperator(\n        task_id=\"list_subscriptions2\",\n        topic_name=TOPIC_NAME2,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n    )\n    # [END howto_operator_asb_topic_subscription_list]\n\n    # [START howto_operator_asb_queue_create]\n    create_queue3 = ASBCreateQueueOperator(\n        task_id=\"create_queue3\",\n        queue_name=QUEUE_NAME,\n        agency_id=AGENCY_ID,\n        tenant_id=TENANT_ID,\n        exists_to_overwrite=True,\n    )\n    # [END howto_operator_asb_queue_create]\n\n    # [START howto_operator_asb_queue_receive_message]\n    receive_message3 = ASBTopicReceiveMessageOperator(\n        task_id=\"receive_message3\",\n        topic_name=TOPIC_NAME,\n        max_message_count=5,\n        asb_conn_id=\"azure_service_bus\",\n    )\n    # [END howto_operator_asb_queue_receive_message]\n\n    # [START howto_operator_asb_queue_send_message]\n    send_message3 = ASBTopicSendMessageOperator(\n        task_id=\"send_message3\",\n        topic_name=TOPIC_NAME,\n        messages=[MESSAGE_LIST2],\n        asb_conn"}], "metadata": {"file_name": "dag_1349_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one workflow initiates another workflow with a predefined configuration message.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.trigger_dag import TriggerDagRunOperator\nfrom airflow.utils.dates import datetime\n\nSTART_DATE = pendulum.datetime(2021, 1, 1, tz=\"UTC\")\n\nwith DAG(\n    dag_id=\"example_trigger_dag\",\n    start_date=START_DATE,\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    run_this_first = TriggerDagRunOperator(\n        task_id=\"trigger_the_second_dag\",\n        trigger_dag_id=\"second_dag\",\n        conf={\"message\": \"Hello World\"},\n        wait_for_completion=True,\n    )\n\n    my_empty = EmptyOperator(task_id=\"nothing\", trigger_rule=\"all_done\")\n\n    run_this_first >> my_empty"}], "metadata": {"file_name": "dag_2223_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases different task execution strategies for Looker data transformations, highlighting the ability to start PDT builds with both blocking and non-blocking approaches. Implement a workflow that demonstrates task chaining and sensor-based job monitoring.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.looker import LookerStartPdtOperator, LookerStopPdtOperator\nfrom airflow.providers.google.cloud.sensors.looker import LookerPdtJobSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_looker\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    looker_user = os.environ.get(\"LOOKER_USER\")\n    looker_password = os.environ.get(\"LOOKER_PASSWORD\")\n    looker_admin_password = os.environ.get(\"LOOKER_ADMIN_PASSWORD\")\n    looker_pdt_name = os.environ.get(\"LOOKER_PDT_NAME\", \"example_pdt\")\n    looker_pdt_path = os.environ.get(\"LOOKER_PDT_PATH\", \"example_pdt_path\")\n    looker_conn_id = os.environ.get(\"LOOKER_CONN_ID\", \"looker_default\")\n\n    start_pdt_task = LookerStartPdtOperator(\n        task_id=\"start_looker_pdt\",\n        looker_user=looker_user,\n        looker_password=looker_admin_password,\n        looker_conn_id=looker_conn_id,\n        looker_pdt_name=looker_pdt_name,\n        looker_pdt_path=looker_pdt_path,\n    )\n\n    looker_pdt_sensor = LookerPdtJobSensor(\n        task_id=\"looker_pdt_sensor\",\n        looker_user=looker_user,\n        looker_password=looker_admin_password,\n        looker_conn_id=looker_conn_id,\n        looker_pdt_name=looker_pdt_name,\n        looker_pdt_path=looker_pdt_path,\n        target_job_status=[\"success\"],  # Can also use \"failed\"\n    )\n\n    stop_pdt_task = LookerStopPdtOperator(\n        task_id=\"stop_looker_pdt\",\n        looker_user=looker_user,\n        looker_password=looker_admin_password,\n        looker_conn_id=looker_conn_id,\n        looker_pdt_name=looker_pdt_name,\n    )\n\n    start_pdt_task >> looker_pdt_sensor >> stop_pdt_task"}], "metadata": {"file_name": "dag_42_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end infrastructure orchestration DAG that demonstrates the complete lifecycle of an EKS cluster, including creation, validation, and pod deployment. Implement a robust workflow that shows how to programmatically manage cloud infrastructure using Airflow's declarative task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import EksClusterNotExistsException\nfrom airflow.providers.amazon.aws.operators.eks import EksCreateClusterOperator, EksDeleteClusterOperator\nfrom airflow.providers.amazon.aws.operators.ecs import EcsCreateClusterOperator, EcsDeleteClusterOperator\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterAvailableSensor\nfrom airflow.providers.amazon.aws.sensors.ecs import EcsTaskCompletedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().add_variable_key(env_id_key=ENV_ID_KEY).build()\n\nDAG_ID = \"example_eks_with_ecs\"\n\n# This test requires minimum eks version of 1.17.0\nREQUIRED_ eks_VERSION = \"1.17.0\"\n\n\n@task\ndef create_launch_template(template_name: str):\n    # Create a launch template that spots says to create an EC2 instance with an AMI from Mac OS and have\n    # 1 vCPU, 8GB of memory, and network interconnect type of ENI-MATCH\n    response = boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"InstanceType\": \"t3.micro\",\n            \"KeyName\": \"test-eks\",\n            \"MetadataOptions\": {\"HttpEndpoint\": \"on\"},\n            \"BlockDeviceMappings\": [\n                {\n                    \"DeviceName\": \"/dev/xvdf\",\n                    \"Ebs\": {\n                        \"Encrypted\": True,\n                        \"DeleteOnTermination\": True,\n                        \"SnapshotId\": \"snap-037505867b04cba3e\",\n                        \"VolumeSize\": 8,\n                        \"VolumeSizeIncrement\": 8,\n                        \"AvailabilityZone\": \"us-east-1a\",\n                    },\n                }\n            ],\n            \"IamInstanceProfile\": {\"Name\": \"aws-ec2-request-pool-role\"},\n            \"SubnetIds\": [\"subnet-0388f07b170620d9e\", \"subnet-0378f27b1b1d20f7a\"],\n            \"NetworkInterfaceCount\": 1,\n        },\n    )\n    boto3.client(\"ec2\").get_waiter(\"launch_template_exists\").wait(LaunchTemplateId=response[\"LaunchTemplate\"][\"Id\"])\n\n\n@task\ndef run_example_task(group_name: str):\n    # Create a cluster\n    create_cluster = EksCreateClusterOperator.trigger(\n        task_id=\"create_eks_cluster\",\n        cluster_name=f\"{group_name}-test-eks\",\n        region=\"us-east-1\",\n        cluster_role_arn=f\"arn:aws:iam::{OVERRIDES[ENV_ID_KEY]}:role/cluster-role\",\n        vpc_config={\n            \"assignPublicIp\": \"ENABLED\",\n        },\n        resources_vpc_config={\n            \"subnetIds\": [group_name + \"-subnet-a\", group_name + \"-subnet-b\"],\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": True,\n        },\n        compute=\"default\",\n        # This cluster will be deleted on failure, so we can be sure\n        # of clean teardown\n        overrides={\n            \"nodegroupName\": f\"{group_name}-nodegroup\",\n            \"computeResource\": {\n                \"cpu\": 1,\n                \"memoryMiB\": 8,\n            },\n        },\n    )\n\n    # Wait for the cluster to be created\n    run_create_cluster = EksClusterNotExistsException(\n        task_id=\"wait_for_cluster_created\",\n        cluster_name=f\"{group_name}-test-eks\",\n        region=\"us-east-1\",\n        wait_for_status=True,\n        timeout=60,\n    )\n\n    # Add the cluster to the sensor\n    create_cluster.output_ids.append(run_create_cluster.output_id)\n\n    # Delete the cluster\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=f\"{group_name}-test-eks\",\n        region=\"us-east-1\",\n    )\n\n    # Wait for the cluster to be deleted\n    delete_cluster_sensor = EksClusterAvailableSensor(\n        task_id=\"wait_for_cluster_deleted\",\n        cluster_name=f\"{group_name}-test-eks\",\n        region=\"us-east-1\",\n        timeout=60,\n    )\n\n    # Connect the sensors and operators with >> and <<\n    chain(*create_cluster.create_task() >> run_create_cluster.wait_for_status() >> delete_cluster.delete_task())\n    (run_example_task(group_name) >> delete_launch_template(template_name=template_name))\n\n\n@task\ndef get_group_arn(cluster_name: str):\n    response = boto3.client(\"eks\").describe_cluster(clusterName=cluster_name)\n    return response[\"cluster\"][\"endpoint\"][\"identity\"][\"groupArn\"]\n\n\n@task\ndef create_launch_template_private(template_name: str):\n    # Create a launch template that spots says to create an EC2 instance with an AMI from Mac OS and have\n    # 1 vCPU, 8GB of memory, and network interconnect type of ENI-MATCH\n    response = boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"InstanceType\": \"t3.micro\",\n            \"KeyName\": \"test-eks\",\n            \"MetadataOptions\": {\"HttpEndpoint\": \"on\"},\n            \"BlockDeviceMappings\": [\n                {\n                    \"DeviceName\": \"/dev/xvdf\",\n                    \"Ebs\": {\n                        \"Encrypted\": True,\n                        \"DeleteOnTermination\": True,\n                        \"SnapshotId\": \"snap-037505867b04cba3e\",\n                        \"VolumeSize\": 8,\n                        \"VolumeSizeIncrement\": 8,\n                        \"AvailabilityZone\": \"us-east-1a\",\n                    },\n                }\n            ],\n            \"IamInstanceProfile\": {\"Name\": \"aws-ec2-request-pool-role\"},\n            \"SubnetIds\": [\"subnet-0378f27b1b1d20f7a\"],\n            \"NetworkInterfaceCount\": 1,\n        },\n    )\n    boto3.client(\"ec2\").get_waiter(\"launch_template_exists\").wait(LaunchTemplateId=response[\"LaunchTemplate\"][\"Id\"])\n    response = boto3.client(\"ec2\").describe_security_groups(\n        Filters=[\n            {\n                \"Name\": \"description\",\n                \"Values\": [\"Allow SSH Access\"],\n            },\n        ],\n    )\n    group_arn = get_group_arn(cluster_name)\n    for sg in response[\"SecurityGroups\"]:\n        if sg[\"GroupId\"] != group_arn:\n            for rule in sg[\"SecurityGroupRules\"]:\n                if rule[\"IpProtocol\"] == \"Tcp\" and rule[\"FromPort\"] == \"22\" and rule[\"IpRanges\"] == [template_name]:\n                    return sg[\"GroupId\"]\n    raise Exception(\"Could not find security group matching template name\")\n\n\n@task\ndef create_task_definition(cluster_name: str, launch_template_name: str):\n    response = boto3.client(\"ecs\").register_task_definition(\n        family=name,\n        # We specify 4 vCPU's and 8GB of memory for these tasks\n        # Note, because of the container personality, we need\n        # to specify the memory and cpus for the container,\n        # not the main image\n        container_definitions=[\n            {\n                \"name\": \"test-cmd\",\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"/bin/sh\", \"-c\"],\n                \"memory\": 8192,\n                \"cpu\": 4,\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": \"group-1234567890abcdef\",\n                        \"awslogs-region\": \"us-east-1\",\n                        \"awslogs-create-group\": \"true\",\n                        \"awslogs-stream-prefix\": \"ecs\",\n                    },\n                },\n            }\n        ],\n        # We need to specify the launch template for\n        # these tasks\n        requires_compatiable=False,\n        # Keep the task definition available even after\n        # the test module has finished\n        cpu=\"256\",\n        memory=\"512\",\n        network_configuration={\n            \"awsvpcConfiguration\": {\n                \"assignPublicIp\": \"ENABLED\",\n                \"subnets\": [\"subnet-0378f27b1b1d20f7a\"],\n                \"assignPrivateIpOnLaunch\": False,\n                \"assignPublicIpOnLaunch\": True,\n            }\n        },\n    )\n    return response[\"taskDefinition\"][\"taskDefinitionArn\"]\n\n\n@task\ndef run_example_task_private(group_name: str, task_definition_arn: str):\n    # Create a task\n    create_task = EcsCreateClusterOperator.trigger(\n        task_id=\"create_ecs_task\",\n        cluster_name=f\"{group_name}-test-ecs\",\n        task_definition=task_definition_arn,\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": \"test-cmd\",\n                    \"command\": [\"/bin/sh\", \"-c\", \"sleep 100\"],\n                }\n            ],\n            \"launchType\": \"FARGATE\",\n        },\n        region=\"us-east-1\",\n    )\n\n    # Wait for the task to be created\n    run_create_task = EcsTaskCompletedSensor(\n        task_id=\"wait_for_task_created\",\n        cluster_name=f\"{group_name}-"}], "metadata": {"file_name": "dag_732_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that shows how to programmatically manage and manipulate data in Google BigQuery using Airflow operators. Create a workflow that transforms raw data, performs quality checks, and demonstrates different query execution strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.providers.google.cloud.transfers.mssql_to_bigquery import MsSqlToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_mssql_to_bigquery\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"table_name\"\nMSSQL_TABLE = \"airflow\"\nSQL_QUERY = \"SELECT 1 AS test;\"\n\nBQ_INPUT_SCHEMA = [\n    {\"name\": \"value\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n]\n\nwith DAG(\n    DAG_ID,\n    default_args={\"project_id\": PROJECT_ID},\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mssql_to_bigquery\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    # [START howto_operator_mssql_to_bigquery]\n    task_mssql_to_bigquery = MsSqlToBigQueryOperator(\n        task_id=\"mssql_to_bigquery_example\",\n        conn_id=\"postgres_default\",\n        sql=SQL_QUERY,\n        destination_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        schema=BQ_INPUT_SCHEMA,\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_operator_mssql_to_bigquery]\n\n    create_dataset >> task_mssql_to_bigquery\n\n    # [START howto_operator_insert_query_bigquery]\n    task_insert_query = BigQueryInsertJobOperator(\n        task_id=\"insert_query_example\",\n        configuration={\n            \"query\": {\n                \"query\": \"SELECT 1 AS test;\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_insert_query_bigquery]\n\n    task_insert_query >> task_mssql_to_bigquery\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_dataset\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done;\n    # if the task is removed, the generator email is sent too early,\n    # and we mark it as done before the task is removed.\n    list(watcher(dag))\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1573_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task dependencies and lifecycle management, including nested task groups with explicit setup and teardown operations. Develop a workflow that demonstrates how to link tasks with clear initialization and finalization steps, using both decorator-based and direct task relationship methods.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_me\")\ndef run_me():\n    print(\"Hi\")\n    print(\"Hello\")\n    return 1\n\n\nwith DAG(\n    \"example_task_decorator\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    run_me()\n    run_me()\n    run_me()"}], "metadata": {"file_name": "dag_1858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data processing workflow that simulates a multi-stage bash script execution environment, demonstrating how different computational tasks can be sequenced and managed through Airflow's task scheduling mechanism. Create a DAG that shows how simple shell commands can be orchestrated with complex dependency relationships.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_me\")\ndef run_me():\n    print(\"Hi\")\n    print(\"Hello\")\n    return 1\n\n\nwith DAG(\n    \"example_bash_decorator\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    run_me()"}], "metadata": {"file_name": "dag_328_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates integration with InfluxDB using the InfluxDBOperator for time-bounded data extraction. Build a task that pulls recent time-series data using a dynamic time range relative to the current execution date.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\nfrom airflow.providers.influxdb.operators.influxdb import InfluxDBOperator\nfrom airflow.utils.context import Context\n\n# [START howto_influxdb_operator]\n\nDAG_ID = \"example_influxdb_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def populate_influxdb():\n        \"\"\"\n        Example task to populate influxdb with sample data\n        \"\"\"\n        hook = InfluxDBHook()\n        # Connect to the InfluxDB using SQL hook\n        hook.connect()\n        # Create a database\n        hook.create_database(name=\"test_db\")\n        # Write sample data to the database\n        hook.write_database(\n            database=\"test_db\",\n            measurement=\"sample_measurement\",\n            rows=[\n                (\"sample_point\", \"sample_tag1\", \"sample_value1\", \"sample_time\"),\n                (\"sample_point\", \"sample_tag1\", \"sample_value2\", \"sample_time\"),\n            ],\n        )\n        hook.close()\n\n    query_influxdb_task = InfluxDBOperator(\n        task_id=\"query_influxdb\",\n        conn_id=\"influxdb_default\",\n        sql=\"select * from test_db where dt = now() limit 10;\",\n        dag=dag,\n        extrainfodb={\n            \"rangeness\": \"last\",\n        },\n        timeout=10,\n        catchup=False,\n    )\n\n    populate_influxdb()\n\n    query_influxdb_task\n\n# [END howto_influxdb_operator]"}], "metadata": {"file_name": "dag_2145_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across multiple AWS services. Develop a sequence of tasks that create security groups, configure database endpoints, and set up a replication instance for seamless data transfer.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.rds import RdsInstanceHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbReplicationGroupOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbReplicationGroupOperator,\n    RdsStartDbReplicationGroupOperator,\n    RdsStopDbReplicationGroupOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RdsReplicationGroupSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().add_env_variable(\n    \"ROLE_ARN\", override=True\n).build()\n\nDAG_ID = \"example_rds_replication_group\"\n\nRDS_DB_NAME = \"test_db_name\"\nS3_BUCKET = f\"{DAG_ID}_{ENV_ID_KEY}\"\nS3_KEY = \"file.txt\"\nSNAPSHOT_S3_KEY = \"file_snapshot.txt\"\nREPLICA_TYPE = \"PRIMARY\"\nSOURCE_REPLICA_ENDPOINT = f\"us-east-1.amazonawsaws.s3.us-east-1.com\"\nSOURCE_REPLICA_DB_NAME = \"test_db_name_snapshot\"\nS3_PATH = f\"s3://{S3_BUCKET}/{SNAPSHOT_S3_KEY}\"\nAWS_REPLICA_ENDPOINT = f\"us-east-1.amazonawsaws.s3.us-east-1.com\"\nAWS_REPLICA_DB_NAME = \"test_db_name_snapshot\"\n\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    rds_instance_name = f\"{env_id}-rds-replica\"\n    db_create_params: dict = {\n        \"Engine\": \"MySQL\",\n        \"RdsInstanceClass\": REPLICA_TYPE,\n        \"MasterUsername\": \"rds_replica_username\",\n        \"MasterUserPassword\": \"rds_replica_password\",\n        \"AllocatedStorage\": 10,\n        \"DBName\": RDS_DB_NAME,\n    }\n    s3_create_bucket = S3CreateBucketOperator(task_id=\"s3_create_bucket\", bucket_name=S3_BUCKET)\n\n    @task\n    def create_snapshot(*, db_name: str, table_suffix: str) -> str:\n        \"\"\"Create snapshot of database table\"\"\"\n        s3_path = f\"s3://{S3_BUCKET}/{table_suffix}\"\n        return S3Hook().conn.create_snapshot(\n            SourceBucket=s3_path,\n            SourceTable=\"file.txt\",\n            SkipMulitpartUpload=True,\n        )[\"SnapshotArn\"]\n\n    snapshot_arn = create_snapshot(db_name=RDS_DB_NAME, table_suffix=\"table1\")(db_name=RDS_DB_NAME, table_suffix=\"table1\")\n    snapshot_arn_2 = create_snapshot(db_name=RDS_DB_NAME, table_suffix=\"table2\")(db_name=RDS_DB_NAME, table_suffix=\"table2\")\n\n    @task\n    def wait_for_replication_group_creation(rds_replica_endpoint: str, rds_replica_db_name: str):\n        \"\"\"Wait for replication group creation\"\"\"\n        rds_hook = RdsInstanceHook()\n        if rds_replica_endpoint and rds_replica_db_name:\n            wait_to_cancel = True\n            min_records = 1\n        else:\n            wait_to_cancel = False\n            min_records = 0\n        timeout = 0\n        while timeout < 300:\n            exists = rds_hook.is_replication_group_available(\n                ReplicationGroupIdentifier=rds_replica_endpoint,\n                RdsDbVersion=rds_replica_db_name,\n            )\n            if exists and exists >= min_records:\n                break\n            timeout += 30\n            print(f\"Replication group {rds_replica_endpoint} is not ready, retry in {timeout} seconds.\")\n        if not exists:\n            raise Exception(f\"Replication group {rds_replica_endpoint} not created\")\n\n    wait_for_replication_group_creation(rds_replica_endpoint=rds_replica_endpoint, rds_replica_db_name=rds_replica_db_name)\n\n    @task\n    def create_replication_group(\n        rds_replica_endpoint: str,\n        rds_replica_db_name: str,\n        source_aws_replica_endpoint: str,\n        source_aws_replica_db_name: str,\n    ) -> None:\n        \"\"\"Create replication group\"\"\"\n        RdsCreateDbReplicationGroupOperator(\n            task_id=\"create_replication_group\",\n            replication_group_name=rds_replica_endpoint,\n            replication_source_identifier=source_aws_replica_endpoint,\n            replication_source_db_name=source_aws_replica_db_name,\n            db_name=rds_replica_db_name,\n        )\n\n    create_replication_group(rds_replica_endpoint=rds_replica_endpoint, rds_replica_db_name=rds_replica_db_name,\n                             source_aws_replica_endpoint=rds_instance_name, source_aws_replica_db_name=RDS_DB_NAME)\n\n    @task\n    def start_replication_group(rds_replica_endpoint: str):\n        \"\"\"Start replication group\"\"\"\n        RdsStartDbReplicationGroupOperator(task_id=\"start_replication_group\", ReplicationGroupIdentifier=rds_replica_endpoint)\n\n    start_replication_group(rds_replica_endpoint=rds_replica_endpoint)\n\n    @task\n    def get_replication_group_status(rds_replica_endpoint: str, timeout: int):\n        \"\"\"Get replication group status\"\"\"\n        rds_hook = RdsInstanceHook()\n        timeout = timeout\n        wait_to_cancel = True\n        min_records = 1\n        while timeout >= 0:\n            exists = rds_hook.is_replication_group_available(\n                ReplicationGroupIdentifier=rds_replica_endpoint,\n                RdsDbVersion=rds_replica_db_name,\n            )\n            if exists and exists >= min_records:\n                break\n            timeout -= 1\n            print(f\"Replication group {rds_replica_endpoint} is not ready, retry in {timeout} seconds.\")\n        if not exists:\n            raise Exception(f\"Replication group {rds_replica_endpoint} not created\")\n\n    get_replication_group_status(rds_replica_endpoint=rds_replica_endpoint, timeout=300)\n\n    @task\n    def create_file_copy_action(rds_replica_endpoint: str, source_aws_replica_endpoint: str, source_aws_replica_db_name: str):\n        \"\"\"Create file copy action\"\"\"\n        RdsCreateDbReplicationGroupOperator(\n            task_id=\"create_file_copy_action\",\n            replication_group_name=rds_replica_endpoint,\n            source_aws_replica_endpoint=source_aws_replica_endpoint,\n            source_aws_replica_db_name=source_aws_replica_db_name,\n            actions=[\n                {\n                    \"ActionType\": \"FILECOPY\",\n                    \"SourceFileIdentifier\": SNAPSHOT_S3_KEY,\n                    \"DestinationFileIdentifier\": f\"{S3_PATH}\",\n                    \"Replace\": True,\n                }\n            ],\n        )\n\n    create_file_copy_action(rds_replica_endpoint=rds_replica_endpoint, source_aws_replica_endpoint=rds_instance_name, source_aws_replica_db_name=RDS_DB_NAME)\n\n    @task\n    def get_replication_group_status(rds_replica_endpoint: str, timeout: int):\n        \"\"\"Get replication group status\"\"\"\n        rds_hook = RdsInstanceHook()\n        timeout = timeout\n        wait_to_cancel = True\n        min_records = 1\n        while timeout >= 0:\n            exists = rds_hook.is_replication_group_available(\n                ReplicationGroupIdentifier=rds_replica_endpoint,\n                RdsDbVersion=rds_replica_db_name,\n            )\n            if exists and exists >= min_records:\n                break\n            timeout -= 1\n            print(f\"Replication group {rds_replica_endpoint} is not ready, retry in {timeout} seconds.\")\n        if not exists:\n            raise Exception(f\"Replication group {rds_replica_endpoint} not created\")\n\n    get_replication_group_status(rds_replica_endpoint=rds_replica_endpoint, timeout=300)\n\n    @task\n    def get_table_snapshot(rds_replica_endpoint: str, table_suffix: str):\n        \"\"\"Get table snapshot\"\"\"\n        rds_hook = RdsInstanceHook()\n        response = rds_hook.get_db_replication_instance_snapshot(\n            ReplicationGroupIdentifier=rds_replica_endpoint, DbReplicationInstanceIdentifier=rds_replica_endpoint, DbReplicationInstanceName=rds_replica_endpoint\n        )\n        return json.loads(response._text)[\n            \"DBSnapshots\"\n        ][0][\"DBSnapshotIdentifier\"]  # type: ignore[union-attr]\n\n    get_table_snapshot(rds_replica_endpoint=rds_replica_endpoint, table_suffix=\"table1\")\n\n    @task\n    def get_table_snapshot_2(rds_replica_endpoint: str, table_suffix: str):\n        \"\"\"Get table snapshot\"\"\"\n        rds_hook = RdsInstanceHook()\n        response = rds_hook.get_db_replication_instance_snapshot(\n            ReplicationGroupIdentifier=rds_replica_endpoint, DbReplicationInstanceIdentifier=rds_replica_endpoint, DbReplicationInstanceName=rds_replica_endpoint\n        )\n        return json.loads(response._text)[\n            \"DBSnapshots\"\n        ][1][\"DBSnapshotIdentifier\"]  # type: ignore[union-attr]\n\n    get_table_snapshot_2(rds_rep"}], "metadata": {"file_name": "dag_60_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "I have a dataset with multiple time-series data points for different categories, and I want to visualize the trends over time for these categories using Python. I have started using Matplotlib and Pandas. Can you guide me on how to plot multiple time-series data on the same graph to compare their trends over time?\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n\n@task(task_id=\"run_from_python\")\ndef run_from_python():\n    print(\"Hello from Python!\")\n    print(\"What a nice day to eat dessert!\")\n\n\ndefault_args = {\"start_date\": pendulum.datetime(2021, 1, 1), \"retries\": 1}\n\nwith DAG(\n    \"example_python_task\",\n    default_args=default_args,\n    start_date=pendulum.datetime(2021, 1, 1),  # Must be earlier than the first task\n    schedule=None,\n    tags=[\"example\"],\n):\n    run_from_python()"}], "metadata": {"model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client", "airflow_version": "2.7.2"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates error handling and external alerting by implementing a bash task with integrated failure notifications to an incident management platform.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.influxdb.hooks.influxdb import InfluxDBHook\nfrom airflow.providers.common.influxdb.operators.influxdb import InfluxDBOperator\nfrom airflow.providers.common.influxdb.sensors.influxdb import InfluxDBSensor\n\n# [START howto_influxdb_sensor_decorator]\n# [START howto_influxdb_sensor_async_decorator]\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_past\": False,\n    \"email\": [\"airflow@example.com\"],\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 1,\n    \"max_retries\": 1,\n    \"concurrency\": 1,\n    \"conn_id\": \"influxdb_default\",\n}\n\nwith DAG(\n    \"example_influxdb_sensor\",\n    default_args=default_args,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    influxdb_conn = Connection(\n        conn_id=\"influxdb_default\",\n        conn_type=\"influxdb\",\n    )\n\n    # [START howto_influxdb_sensor]\n    task_insert = InfluxDBOperator(\n        task_id=\"insert_task\",\n        sql='insert into temperature bucket=\"test-bucket\" dt=2020-01-18T13:25:00Z ts=\"1234567890\" mg=23',\n        influxdb_conn_id=influxdb_conn,\n    )\n    # [END howto_influxdb_sensor]\n\n    # [START howto_influxdb_sensor_async]\n    task_insert_async = InfluxDBOperator(\n        task_id=\"insert_task_async\",\n        sql='insert into temperature bucket=\"test-bucket\" dt=2020-01-18T13:25:00Z ts=\"1234567890\" mg=23',\n        influxdb_conn_id=influxdb_conn,\n        dag=dag,\n        deferrable=True,\n    )\n    # [END howto_influxdb_sensor_async]\n\n    # [START howto_influxdb_sensor_sensor]\n    task_insert_sensor = InfluxDBSensor(\n        task_id=\"sensor_task\",\n        sql='insert into temperature bucket=\"test-bucket\" dt=2020-01-18T13:25:00Z ts=\"1234567890\" mg=23',\n        influxdb_conn_id=influxdb_conn,\n    )\n    # [END howto_influxdb_sensor_sensor]\n\n    # [START howto_influxdb_sensor_sensor_async]\n    task_insert_sensor_async = InfluxDBSensor(\n        task_id=\"sensor_task_async\",\n        sql='insert into temperature bucket=\"test-bucket\" dt=2020-01-18T13:25:00Z ts=\"1234567890\" mg=23',\n        influxdb_conn_id=influxdb_conn,\n        dag=dag,\n        deferrable=True,\n    )\n    # [END howto_influxdb_sensor_sensor_async]\n\n    task_insert >> task_insert_sensor\n    task_insert_async >> task_insert_sensor_async\n    (Connection.get_url(influxdb_conn) / \"db/test-bucket\").write_precision(\"json\")"}], "metadata": {"file_name": "dag_2179_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data streaming pipeline that demonstrates Kafka cluster management and message processing using Google Cloud Managed Kafka. Create a workflow that showcases creating a Kafka cluster, producing messages, and consuming those messages with a specific consumer group.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport random\nimport time\nfrom datetime import datetime\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.kms import GoogleKMSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.kms import (\n    GoogleKMSCreateCryptoKeyOperator,\n    GoogleKMSCreateCryptoKeyDatasetOperator,\n    GoogleKMSCreateCryptoKeyEntryOperator,\n    GoogleKMSCreateCryptoKeyEntryOperatorV2,\n    GoogleKMSCreateDecryptionRequestOperator,\n    GoogleKMSCreateEncryptionRequestOperator,\n    GoogleKMSCipherTextOperator,\n    GoogleKMSCipherTextToBase64Operator,\n    GoogleKMSCmdToOperator,\n    GoogleKMSCryptDataOperator,\n    GoogleKMSCryptDataOperatorV2,\n    GoogleKMSCryptDataWithCryptoKeyEntryOperator,\n    GoogleKMSCryptDataWithCryptoKeyEntryOperatorV2,\n    GoogleKMSCryptDataWithServiceAgentAndCryptoKeyEntryOperator,\n    GoogleKMSCryptDataWithServiceAgentAndCryptoKeyEntryOperatorV2,\n    GoogleKMSCryptInternalDataOperator,\n    GoogleKMSCryptInternalDataOperatorV2,\n    GoogleKMSCryptoKeyDumpOperator,\n    GoogleKSCreateImportCryptoKeyOperator,\n    GoogleKSCreateImportCryptoKeyDatasetOperator,\n    GoogleKSCreateImportCryptoKeyEntryOperator,\n    GoogleKSCreateImportCryptoKeyEntryOperatorV2,\n    GoogleKSCreateKeysPoliciesAmbassadorSecretOperator,\n    GoogleKSCreateKeysPoliciesAmbassadorSecretOperatorV2,\n    GoogleKSCreateServiceAgentOperator,\n    GoogleKSCreateServiceAgentSecretOperator,\n    GoogleKSCreateTrustOperator,\n    GoogleKSCreateTrustOperatorV2,\n    GoogleKSCmdOperator,\n    GCSScilianoDeleteBucketOperator,\n    GCSUpdateBucketCommentOperator,\n)\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Monday\nfrom airflow.utils.weekday import Tuesday\nfrom airflow.utils.weekday import WeekDay\nfrom airflow.utils.weekday import WeekDayFilter\nfrom airflow.utils.weekday import WeekFilter\nfrom airflow.utils.trigger_rule import TriggerRuleWeekDayFilter\nfrom airflow.utils.trigger_rule import TriggerRuleWeekDayWeekDayFilter\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"kms_crypto_key_dataset\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nCRYPTO_KEY_ID = f\"cryptoki_{DAG_ID}_{ENV_ID}\"\nCRYPTO_KEY_DATASET_ID = f\"dataset_{DAG_ID}_{ENV_ID}\"\nCRYPTO_KEY_ENTRY_ID = f\"keyentry_{DAG_ID}_{ENV_ID}\"\nCRYPTO_KEY_ENCRYPTION_SPEC_ID = f\"encryption_spec_{DAG_ID}_{ENV_ID}\"\nTRUST_ID = f\"trust_{DAG_ID}_{ENV_ID}\"\nIMPORT_CRYPT_KEY_ID = f\"importcryptokikey_{DAG_ID}_{ENV_ID}\"\nIMPORT_CRYPT_KEY_DATASET_ID = f\"importdataset_{DAG_ID}_{ENV_ID}\"\nIMPORT_CRYPT_KEY_ENTRY_ID = f\"importkeyentry_{DAG_ID}_{ENV_ID}\"\nIMPORT_CRYPT_KEY_ENCRYPTION_SPEC_ID = f\"importencryption_spec_{DAG_ID}_{ENV_ID}\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"kms\"],\n) as dag:\n\n    @task\n    def create_project():\n        logging.info(f\"Creating project {PROJECT_ID}...\")\n        api_client = GoogleCloudApiServiceClient()\n        api_client.create_project_id(project_id=PROJECT_ID)\n\n    @task\n    def create_zone():\n        logging.info(\"Creating Crypto Key Ring zone...\")\n        api_client = GoogleCloudApiServiceClient()\n        zone = api_client.create_crypto_key_ring_zone(project_id=PROJECT_ID, location=\"us-central1\")\n\n    @task\n    def add_weekday_filter_to_bucket():\n        logging.info(f\"Adding {WeekDay.TUESDAY} weekday filter to bucket {BUCKET_NAME}...\")\n        bucket = GCSCreateBucketOperator(\n            task_id=\"add_weekday_filter_to_bucket\",\n            bucket_name=BUCKET_NAME,\n            public_bucket=False,\n            website=False,\n            comment=f\"Test bucket for {WeekDay.TUESDAY}\",\n        )\n\n        # [START get_id_of_bucket]\n        bucket_id = task_xcom_pull(task_id=bucket.task_id, task_name=bucket.task_id, key=\"bucket_id\")\n        # [END get_id_of_bucket]\n\n        # [START update_bucket_comment]\n        logging.info(f\"Updating comment of bucket with id {bucket_id}...\")\n        bucket.update_bucket_comment(\n            task_id=\"update_bucket_comment\",\n            bucket_id=bucket_id,  # or use string form of bucket_id\n            comment=f\"Test bucket for {WeekDay.WEDNESDAY}\",\n        )\n        # [END update_bucket_comment]\n\n    @task\n    def add_weekday_weekday_filter_to_bucket():\n        logging.info(f\"Adding {WeekDay.TUESDAY} and {WeekDay.WEDNESDAY} weekday filters to bucket {BUCKET_NAME}...\")\n        bucket = GCSCreateBucketOperator(\n            task_id=\"add_weekday_weekday_filter_to_bucket\",\n            bucket_name=BUCKET_NAME,\n            public_bucket=False,\n            website=False,\n            comment=f\"Test bucket for {WeekDay.TUESDAY} and {WeekDay.WEDNESDAY}\",\n        )\n\n        # [START get_id_of_bucket]\n        bucket_id = task_xcom_pull(task_id=bucket.task_id, task_name=bucket.task_id, key=\"bucket_id\")\n        # [END get_id_of_bucket]\n\n        # [START update_bucket_comment]\n        logging.info(f\"Updating comment of bucket with id {bucket_id}...\")\n        bucket.update_bucket_comment(\n            task_id=\"update_bucket_comment\",\n            bucket_id=bucket_id,  # or use string form of bucket_id\n            comment=f\"Test bucket for {WeekDay.TUESDAY} and {WeekDay.WEDNESDAY}\",\n            weekdays=[WeekDayFilter(Tuesday=True), WeekDayFilter(Wednesday=True)],\n        )\n        # [END update_bucket_comment]\n\n    @task\n    def create_crypto_key_dataset():\n        logging.info(f\"Creating Crypto Key Dataset {CRYPTO_KEY_DATASET_ID}...\")\n        api_client = GoogleCloudApiServiceClient()\n        dataset = api_client.create_crypto_key_dataset(\n            project_id=PROJECT_ID, location=\"us-central1\", dataset_id=CRYPTO_KEY_DATASET_ID\n        )\n\n    @task\n    def add_resource_table_to_bucket(dataset_id):\n        logging.info(f\"Adding resource table to bucket {BUCKET_NAME} for dataset {dataset_id}...\")\n        bucket = GCSCreateBucketOperator(\n            task_id=\"add_resource_table_to_bucket\",\n            bucket_name=BUCKET_NAME,\n            public_bucket=False,\n            website=False,\n            source=ResourceTable(source_object=f\"gs://{BUCKET_NAME}/{dataset_id}/resource_table\"),\n        )\n\n    @task\n    def create_crypto_key_entry():\n        logging.info(f\"Creating Crypto Key Entry {CRYPTO_KEY_ENTRY_ID}...\")\n        api_client = GoogleCloudApiServiceClient()\n        entry = api_client.create_crypto_key_entry(\n            project_id=PROJECT_ID, location=\"us-central1\", dataset_id=CryptoKeyId\n        )\n        return entry.key_id\n\n    @task\n    def create_crypto_key_encryption_spec(key_id):\n        logging.info(f\"Creating encryption spec for key {key_id}...\")\n        api_client = GoogleCloudApiServiceClient()\n        encryption_spec = api_client.create_crypto_key_encryption_spec(\n            project_id=PROJECT_ID, location=\"us-central1\", key_id=key_id\n        )\n        return encryption_spec.encryption_spec\n\n    @task\n    def import_crypt_key_crypt_key_dataset(id):\n        logging.info(f\"Importing Crypto Key {id} into {import_crypt_key_id}...\")\n        api_client = GoogleCloudApiServiceClient()\n        import_key_id = api_client.import_crypto_key(\n            project_id=PROJECT_ID,\n            location=\"us-central1\",\n            import_key_id=import_crypt_key_id,\n            source=ImportCryptoKeyId(\n                scheme=\"IMPORT_CRYPT_KEY_ASYMMETRIC\",>\n                identifier=id\n            ),\n        )\n\n    @task\n    def create_import_crypt_key_encryption_spec(import_key_id):\n        logging.info(f\"Creating import encryption spec for import_key {import_key_id}...\")\n        api_client = GoogleCloudApiServiceClient()\n        import_encryption_spec = api_client.create_crypto_key_encryption_spec(\n            project_id=PROJECT_ID, location=\"us-central1\", import_key_id=import_key_id\n        )\n        return import_encryption_spec.encryption_spec\n\n    @task\n    def create_cipher_text():\n        logging.info(f\"Creating cipher text...\")\n        api_client = GoogleCloudApiServiceClient()\n        cipher_text = api_client.create_cipher_text(\n            project_id=PROJECT_ID, location=\"us-central1\", body=RequestBody()\n        )\n        return cipher_text.cipher_text_id\n\n    @task\n    def create_cipher_text_to_base64():\n        logging.info(f\"Creating cipher text to base64...\")\n        api_client = Google"}], "metadata": {"file_name": "dag_2082_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an interactive Airflow DAG that demonstrates dynamic parameter configuration for workflow triggering, showcasing how users can customize input parameters with various input types and validation rules.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_custom_param\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def get_user_id(custom_params):\n        user_name = custom_params.get(\"user_name\")\n        logging.info(f\"Building DAG for user {user_name}\")\n        if not user_name:\n            logging.error(\"user_name param is required\")\n            return\n        return f\"user_{user_name}\"\n\n    @task\n    def create_connection(conn_id_name, conn, custom_params):\n        conn_id = conn_id_name\n        logging.info(f\"Creating connection {conn_id_name}\")\n        conn_id = conn_id_name\n        if not conn:\n            logging.error(\"Connection object not provided\")\n            return\n        if not custom_params:\n            logging.error(\"custom_params dictionary not provided\")\n            return\n        conn[conn_id] = (\n            f\"conn_{conn_id_name}\",\n            f\"{{{{ {custom_params.get('user_name')} }}}}\",\n            f\"{{{{ {custom_params.get('endpoint')} }}}}\",\n        )\n\n    @task\n    def trigger_dag_for_test(conn_id_name, timeout=30, task_instance=None):\n        BashOperator(\n            task_id=\"bash_task\",\n            bash_command=f\"echo 'Hello from {{ task_instance.task_id }} ({{ task_instance.dag_id }}) for connection: {{ task_instance.xcom_pull(task_ids='get_connection', key='conn_id') }}'\",\n            do_xcom_push=True,\n            timeout=timeout,\n        )\n\n    @task\n    def get_connection_params(conn_id_name, task_instance=None):\n        connection = task_instance.xcom_pull(task_ids=\"get_connection\", key=\"conn\")\n        return json.dumps({k: str(v) for k, v in connection[conn_id_name]})\n\n    connection_id = f\"connection_for_{ENV_ID}\"\n    new_connection = {\n        \"conn_type\": \"SQLITE\",\n        \"conn_id\": connection_id,\n        \"login\": \"root\",\n        \"password\": \"root\",\n        \"host\": \"localhost\",\n        \"port\": \"5432\",\n        \"sql\": \"SELECT 1 AS x WHERE 1=1\",\n    }\n\n    get_user_id_task = get_user_id(custom_params={\"user_name\": \"John\"})\n    create_connection_task = create_connection(conn_id_name=connection_id, conn=new_connection, custom_params={})\n\n    trigger_test = trigger_dag_for_test(conn_id_name=connection_id, task_instance=trigger_dag_for_test)\n\n    pull_params = get_connection_params(conn_id_name=connection_id, task_instance=task_instance)\n\n    load_conn = BashOperator(\n        task_id=\"load_conn\",\n        bash_command=f\"python3 -m airflow.settings load_conn {connection_id} '{{ {pull_params} }}'\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_connection = BashOperator(\n        task_id=\"delete_connection\",\n        bash_command=f\"python3 -m airflow.settings delete_conn {connection_id}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        get_user_id_task\n        >> create_connection_task\n        >> trigger_test\n        >> pull_params\n        >> load_conn\n        >> delete_connection\n    )\n\n    for task in [get_user_id_task, create_connection_task]:\n        task.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done.\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_16_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Dataproc cluster management workflow that demonstrates creating, starting, stopping, and deleting a Dataproc cluster with specific configuration parameters. The workflow should showcase handling of existing clusters and provide a complete lifecycle management example for big data processing infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import TaskMapping\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.dataproc import GoogleCloudDataprocHook\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocStartClusterOperator,\n    DataprocStopClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_cluster\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_{ENV_ID}_{DAG_ID}\"\nREGION = \"europe-west1\"\n\n# Project path to upload and download relative to cluster storage\nPROJECT_PATH = \"project-id\"\n# Path to bucket inside project at cluster storage\nBUCKET_NAME = \"bucket-name\"\n# Path to file within bucket\nPATH_TO_FILE = \"file.txt\"\n# Name of the file within bucket\nFILE_NAME = \"file_name\"\n# Size of the file in bytes\nSIZE = 10\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_cluster():\n        \"\"\"\n        Create a Dataproc cluster. This example creates a 1 node (n1-standard-1)\n        cluster with 10GB of memory in the region \"europe-west1\".\n        \"\"\"\n        hook = GoogleCloudDataprocHook(project_id=PROJECT_ID)\n        hook.create_cluster(\n            cluster_name=CLUSTER_NAME,\n            region=REGION,\n            num_workers=1,\n            master_machine_type=\"n1-standard-1\",\n            primary_disk_size_GB=10,\n            secondary_disk_size_GB=None,\n            gce_cluster_config={\n                \"zone\": \"europe-west1-b\",\n            },\n        )\n\n    @task\n    def stop_cluster():\n        \"\"\"\n        Stop a Dataproc cluster.\n        \"\"\"\n        hook = GoogleCloudDataprocHook(project_id=PROJECT_ID)\n        hook.stop_cluster(cluster_name=CLUSTER_NAME, region=REGION)\n\n    @task\n    def start_cluster():\n        \"\"\"\n        Start a Dataproc cluster.\n        \"\"\"\n        hook = GoogleCloudDataprocHook(project_id=PROJECT_ID)\n        hook.start_cluster(cluster_name=CLUSTER_NAME, region=REGION)\n\n    @task\n    def delete_cluster():\n        \"\"\"\n        Delete a Dataproc cluster.\n        \"\"\"\n        hook = GoogleCloudDataprocHook(project_id=PROJECT_ID)\n        hook.delete_cluster(cluster_name=CLUSTER_NAME, region=REGION)\n\n    create_cluster_task = create_cluster()\n    stop_cluster_task = stop_cluster()\n    start_cluster_task = start_cluster()\n    delete_cluster_task = delete_cluster()\n\n    # [START how_to_cloud_dataproc_start_cluster_operator]\n    start_cluster()\n    # [END how_to_cloud_dataproc_start_cluster_operator]\n\n    # [START how_to_cloud_dataproc_stop_cluster_operator]\n    stop_cluster()\n    # [END how_to_cloud_dataproc_stop_cluster_operator]\n\n    # [START how_to_cloud_dataproc_delete_cluster_operator]\n    delete_cluster()\n    # [END how_to_cloud_dataproc_delete_cluster_operator]\n\n    # [START how_to_cloud_dataproc_create_cluster_operator]\n    create_cluster()\n    # [END how_to_cloud_dataproc_create_cluster_operator]\n\n    # [START how_to_cloud_dataproc_start_cluster_operator]\n    start_cluster()\n    # [END how_to_cloud_dataproc_start_cluster_operator]\n\n    # [START how_to_cloud_dataproc_stop_cluster_operator]\n    stop_cluster()\n    # [END how_to_cloud_dataproc_stop_cluster_operator]\n\n    # [START how_to_cloud_dataproc_delete_cluster_operator]\n    delete_cluster()\n    # [END how_to_cloud_dataproc_delete_cluster_operator]\n\n    create_cluster_task >> stop_cluster_task >> start_cluster_task >> delete_cluster_task\n\n\n# [START how_to_cloud_dataproc_operator_usage]\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# ### TEST SETUP ###\ntest_run = get_test_run(dag)\n# ### TEST SETUP ###\n\n# If you are developing the Dataproc UI and want to test it out locally,\n# set the following:\nTEST_PIXEL_ID = os.environ.get(\"SYSTEM_TESTS_PIXEL_ID\", \"\")\nTEST_ENCRYPTION_KEY_NAME = os.environ.get(\"SYSTEM_TESTS_ENCRYPTION_KEY_NAME\", \"\")\nENVIRONMENT = os.environ.get(\"SYSTEM_TESTS_ENVIRONMENT\", \"\")\n\n# Provide a default ConfigStore if requested\nif TEST_PIXEL_ID and TEST_ENCRYPTION_KEY_NAME:\n    # Type: cov: True\n    # Doc: https://cloud.google.com/dataproc/docs/how-to/gcs\n    test_run.add_task(\n        LocalFilesystemToGCSOperator(\n            task_id=\"set_config_store_gcs\",\n            source_bucket=\"airflow-system-tests\",\n            source_object=\"dataproc/dataproc-v3/config\",\n            destination_bucket=f\"projects/{TEST_PIXEL_ID}/locations/{ENVIRONMENT}/configStores/default\",\n            pattern=\"*.gcs.config\",\n        )\n    )\n\n# If the test runs on the cluster, the IP address of the cluster is known to\n# be available at the test_run.cluster attribute.  Otherwise we look up the\n# IP address of the cluster via the service now operator.\nif test_run.cluster:\n    test_run.add_task(\n        GoogleCloudDataprocHook(service_now_operator=service_now.ServiceNowOperator(\n            task_id=\"get_cluster_ip\",\n            path=\"v1.projects.$PROJECT_ID.locations.$REGION.clusters.$CLUSTER_NAME.get\",\n            params={\"fields\": \"config\"},  # Look for config in the response\n            service_now_conn=test_run.cluster,\n        )),\n        task_mapping=TaskMapping(\n            (\n                (\"get_cluster_ip\", \"get_cluster_ip\"),\n                (\"delete_cluster\", \"delete_cluster\"),\n            )\n        ),\n    )\n# [END how_to_cloud_dataproc_operator_usage]"}], "metadata": {"file_name": "dag_1622_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that demonstrates a complete data transfer process from Google Ads to Google Cloud Storage, including bucket creation, data extraction using a custom query, listing of advertising accounts, and automatic bucket cleanup. The workflow should showcase inter-service orchestration and demonstrate best practices for cloud data migration.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.google.marketing_platform.operators.google_marketing_platform import (\n    GoogleMarketingPlatformListAccountsOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_google_marketing_platform\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-north1\"\nACCOUNTS_CSV_FILE_NAME = \"accounts.csv\"\nCSV_FILE_PATH = f\"gs://{BUCKET_NAME}/{ACCOUNTS_CSV_FILE_NAME}\"\nACCOUNT_LIST = [\n    {\n        \"account_status\": \"PAUSED\",\n    },\n    {\n        \"account_status\": \"ENABLED\",\n    },\n]\n\nCONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"\nCREATION_DATA = {\n    \"account_ids\": ACCOUNT_LIST,\n    \"fields\": [\"account_id\", \"campaign_id\", \"ad_group_id\"],\n    \"sort_order\": \"DESC\",\n}\nGCP_MARKETING_PLATFORM_ACCOUNT_LIST_ENDPOINT = f\"v6/{PROJECT_ID}/accounts/list\"\nGCS_PATH = f\"gs://{BUCKET_NAME}/\"\nGCS Destination Folder = f\"gs://{BUCKET_NAME}/\"\nGCS_PATH_TO_CSV = f\"{GCS_PATH}{ACCOUNTS_CSV_FILE_NAME}\"\nGCS Destination File = f\"{GCS_PATH}{ACCOUNTS_CSV_FILE_NAME}\"\nDAG_FILE_PATH = f\"gs://{GCS_DESTINATION_FOLDER}{ACCOUNTS_CSV_FILE_NAME}\"\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"email\": os.environ.get(\"EMAIL\"),\n        \"region\": LOCATION,\n        \"api_version\": \"v6\",\n        \"conn_id\": CONNECTION_ID,\n        \"endpoint\": GCP_MARKETING_PLATFORM_ACCOUNT_LIST_ENDPOINT,\n        \"dags_folder\": GCS_PATH,\n        \"creations_data\": CREATION_DATA,\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_connection(connection_id: str):\n        conn = Connection(\n            conn_id=connection_id,\n            region=LOCATION,\n            api_version=\"v6\",\n            extra={\"api_base\": GCP_MARKETING_PLATFORM_ACCOUNT_LIST_ENDPOINT},\n        )\n        db = Connection.get_connection(conn_id)\n        db.region = LOCATION\n        db.api_version = \"v6\"\n        db.save()\n\n    create_connection_task = create_connection(connection_id=CONNECTION_ID)\n\n    @task\n    def create_role(role_name: str):\n        role = Role(\n            role_id=role_name,\n            description=\"Test role generated by Airflow\",\n        )\n        role.save()\n\n    create_role_task = create_role(role_name=\"test-testing\")\n\n    @task\n    def enable_task_group(task_group_name: str):\n        task_group = TaskGroup(task_id=task_group_name)\n        BashOperator(\n            task_id=\"create_bucket\",\n            bash_command=f\"echo {BUCKET_NAME}\",\n        )\n        BashOperator(\n            task_id=\"create_connection\",\n            bash_command=f\"create_connection(connection_{DAG_ID}_{ENV_ID})\",\n        )\n        BashOperator(\n            task_id=\"create_role\",\n            bash_command=f\"create_role_task({CONNECTION_ID})\",\n        )\n        BashOperator(\n            task_id=\"enable_task_group\",\n            bash_command=f\"enable_task_group({task_group_name})\",\n        )\n        task_group\n\n    enable_task_group_task = enable_task_group(task_group_name=\"test-testing\")\n\n    @task\n    def setup_triggers():\n        trigger = Trigger.trigger_id == 'create_bucket'\n        (trigger for trigger in dag.tasks if trigger.task_id == 'create_bucket')\n\n    setup_triggers_task = setup_triggers()\n\n    @task\n    def add_and_check_csv_files(file_name: str, bucket_name: str):\n        # [START howto_marketing_platform_list_files_in_gcs]\n        from airflow.providers.google.marketing_platform.operators.google_marketing_platform import ListFilesInGCSOperator\n\n        list_files_in_gcs_task = ListFilesInGCSOperator(\n            task_id=\"list_files_in_gcs\",\n            region=LOCATION,\n            api_version=\"v6\",\n            bucket_name=bucket_name,\n            file_name=file_name,\n        )\n        # [END howto_marketing_platform_list_files_in_gcs]\n        BashOperator(\n            task_id=\"check_csv_files\",\n            bash_command=f\"echo {file_name} checked and {file_name} exists\",\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n    add_and_check_csv_files_task = add_and_check_csv_files(file_name=ACCOUNTS_CSV_FILE_NAME, bucket_name=BUCKET_NAME)\n\n    @task\n    def delete_files(file_name: str, bucket_name: str):\n        # [START howto_marketing_platform_delete_files_in_gcs]\n        from airflow.providers.google.marketing_platform.operators.google_marketing_platform import DeleteFilesInGCSOperator\n\n        delete_files_in_gcs_task = DeleteFilesInGCSOperator(\n            task_id=\"delete_files_in_gcs\",\n            region=LOCATION,\n            api_version=\"v6\",\n            bucket_name=bucket_name,\n            file_name=file_name,\n        )\n        # [END howto_marketing_platform_delete_files_in_gcs]\n\n    delete_files_task = delete_files(file_name=ACCOUNTS_CSV_FILE_NAME, bucket_name=BUCKET_NAME)\n\n    @task\n    def clean_up_connections():\n        # [START howto_marketing_platform_delete_connection]\n        from airflow.providers.google.marketing_platform.operators.google_marketing_platform import DeleteConnectionOperator\n\n        delete_connection_task = DeleteConnectionOperator(\n            task_id=\"delete_connection\",\n            conn_id=CONNECTION_ID,\n        )\n        # [END howto_marketing_platform_delete_connection]\n\n    clean_up_connections_task = clean_up_connections()\n\n    [create_bucket, create_connection_task, create_role_task, enable_task_group_task, setup_triggers_task, add_and_check_csv_files_task, delete_files_task, clean_up_connections_task].execute()"}], "metadata": {"file_name": "dag_454_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that leverages Azure Container Instances for scalable, ephemeral compute tasks. Create a DAG that demonstrates launching containerized workloads with configurable resources, environment variables, and storage volume integrations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateClusterOperator,\n    ContainerInstancesDeleteClusterOperator,\n    ContainerInstancesStartContainerOperator,\n    ContainerInstancesStopContainerOperator,\n)\nfrom airflow.providers.microsoft.azure.operators.azure_fileshare import AzureFileShareCreateOperator, AzureFileShareDeleteOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"ci_example_azure_container_instances\"\n\nAZURE_CONTAINER_INSTANCES_CLUSTER_NAME = f\"cluster-{DAG_ID}-{ENV_ID}\"\nAZURE_CONTAINER_INSTANCES_REGISTRY_NAME = f\"registry-{DAG_ID}-{ENV_ID}\"\nAZURE_CONTAINER_INSTANCES_STORAGE_NAME = f\"storage-{DAG_ID}-{ENV_ID}\"\nAZURE_FILE_SHARE_NAME = f\"fileshare-{DAG_ID}-{ENV_ID}\"\n\nTENANT_ID = os.environ.get(\"AZURE_TENANT_ID\")\nSUBSCRIPTION_ID = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\nENVIRONMENT_ID = os.environ.get(\"AZURE_ENVIRONMENT_ID\")\n\nCONNECTION_ID = \"azure_container_instances_default\"\n\nCURRENT_FOLDER = os.path.dirname(__file__)\n\n# [START howto_operator_container_instances_create]\n@task\ndef create_cluster(tenent_id: str, subscription_id: str, cluster_name: str):\n    os.system(f\"az container instances create --name {cluster_name} --resource-group=myGroup --node-count 1 --node-brand-name ACICommunity \"\n              f\"--node-imageCanonicalName Microsoft/windowsserver2019:10.0:latest --registry-server-name {AZURE_CONTAINER_INSTANCES_REGISTRY_NAME}\"\n              f\" --registry-login-server {tenent_id} --registry-login-server-secret {subscription_id}\")\n\n\n# [END howto_operator_container_instances_create]\n\n# [START howto_operator_container_instances_delete_cluster]\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster(cluster_name: str, resource_group: str, tenent_id: str, subscription_id: str):\n    os.system(f\"az container instances delete --name {cluster_name} --resource-group {resource_group} --tenent-id {tenent_id} \"\n              f\"--subscription {subscription_id}\")\n\n\n# [END howto_operator_container_instances_delete_cluster]\n\n# [START howto_operator_container_instances_create_container]\n@task\ndef create_container(cluster_name: str, image: str, container_name: str, resource_group: str, tenent_id: str, subscription_id: str):\n    os.system(f\"az container instances create --name {container_name} --resource-group {resource_group} \"\n              f\" --cluster-name {cluster_name} --image {image} --container-name {container_name} \"\n              f\"--command-path /bin/sleep --ports 80 --tenent-id {tenent_id} --subscription {subscription_id}\")\n\n\n# [END howto_operator_container_instances_create_container]\n\n# [START howto_operator_container_instances_stop_container]\n@task\ndef stop_container(cluster_name: str, container_name: str, resource_group: str, tenent_id: str, subscription_id: str):\n    os.system(f\"az container instances stop --name {container_name} --resource-group {resource_group} \"\n              f\" --cluster-name {cluster_name} --tenent-id {tenent_id} --subscription {subscription_id}\"\n\n\n# [END howto_operator_container_instances_stop_container]\n\n# [START howto_operator_container_instances_start_container]\n@task\ndef start_container(cluster_name: str, container_name: str, resource_group: str, tenent_id: str, subscription_id: str):\n    os.system(f\"az container instances start --name {container_name} --resource-group {resource_group} \"\n              f\" --cluster-name {cluster_name} --tenent-id {tenent_id} --subscription {subscription_id}\")\n\n\n# [END howto_operator_container_instances_start_container]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_cluster_instance_azure_container_instances = create_cluster(tenent_id= TENANT_ID, subscription_id=SUBSCRIPTION_ID, cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME)\n    create_fileshare_azure_container_instances = AzureFileShareCreateOperator(\n        task_id=\"create_fileshare_azure_container_instances\",\n        share_name=AZURE_FILE_SHARE_NAME,\n        resource_group=\"myGroup\",\n        connection=CONNECTION_ID,\n    )\n    # [START howto_operator_container_instances_create]\n    container_instances_create_container = create_container(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, image=\"ubuntu\", container_name=\"test\")\n    # [END howto_operator_container_instances_create]\n    # [START howto_operator_container_instances_create_container_name]\n    container_instances_create_container_name = create_container(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, image=\"ubuntu\", container_name=\"test2\")\n    # [END howto_operator_container_instances_create_container_name]\n    # [START howto_operator_container_instances_stop_container]\n    container_instances_stop_container = stop_container(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, container_name=\"test2\")\n    # [END howto_operator_container_instances_stop_container]\n    # [START howto_operator_container_instances_start_container]\n    container_instances_start_container = start_container(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, container_name=\"test2\")\n    # [END howto_operator_container_instances_start_container]\n    delete_cluster_instance_azure_container_instances = delete_cluster(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, resource_group=\"myGroup\", tenent_id=TENANT_ID, subscription_id=SUBSCRIPTION_ID)\n    delete_fileshare_azure_container_instances = AzureFileShareDeleteOperator(\n        task_id=\"delete_fileshare_azure_container_instances\",\n        share_name=AZURE_FILE_SHARE_NAME,\n        resource_group=\"myGroup\",\n    )\n    # [START howto_operator_container_instances_delete_cluster]\n    create_cluster_azure_container_instances = create_cluster(cluster_name=AZURE_CONTAINER_INSTANCES_CLUSTER_NAME, resource_group=\"myGroup\", tenent_id=TENANT_ID, subscription_id=SUBSCRIPTION_ID)\n    # [END howto_operator_container_instances_delete_cluster]\n\n    (\n        # TEST SETUP\n        create_cluster_instance_azure_container_instances\n        >> create_fileshare_azure_container_instances\n        # TEST BODY\n        >> container_instances_create_container\n        >> container_instances_stop_container\n        >> container_instances_start_container\n        # TEST TEARDOWN\n        >> delete_cluster_instance_azure_container_instances\n        >> delete_fileshare_azure_container_instances\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that illustrates the infrastructure-as-code approach to setting up a scalable job scheduling environment in Kubernetes. Create a workflow that demonstrates provisioning computational resources, defining job queues, and preparing a system for parallel workload processing.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection, Role, TaskGroup, Trigger\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import chain\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n# [START howto_operator_kubernetes]\ndef _create_kubernetes_operator():\n    \"\"\"\n    Create an operator which list cluster nodes.\n    \"\"\"\n\n    def list_clusters(*, namespace, project):\n        command = f\"kubectl --project={project} --namespace={namespace\" \"}' get cluster --output=json\"\n        return BashOperator(\n            task_id=\"list_clusters\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    def list_nodes(*, namespace, project):\n        command = f\"kubectl --project={project} --namespace={namespace}\" \"'' get nodes -o=json\"\n        return BashOperator(\n            task_id=\"list_nodes\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return [list_clusters, list_nodes]\n\n\ndef _create_kubernetes_sensor():\n    \"\"\"\n    Create an sensor which wait for a list of nodes are available.\n    \"\"\"\n\n    def wait_for_nodes(*, namespace, project, expected_nodes):\n        command = f\"for n in `kubectl --project={project}\" \"'' get nodes -n={namespace}\" \"'' -o=json\";\" \"do:\" \";\" \"if [ `awk '{{if(\\$1 ~ /10.9./)\" \"{'print \\$4}'}' <<< $n' ''\" ' -e 'if(\\$4 >= {expected_nodes}) break;'; ];\" \"then\" \"do:\" \";\" \"return;\" \"end;\" \"return 1;\" \"return 0;\"\n    return BashOperator(\n        task_id=\"wait_for_nodes\",\n        bash_command=wait_for_nodes,\n        dag=dag,\n    )\n\n\ndef _create_kubernetes_function():\n    \"\"\"\n    Create a kubernetes function which list cluster nodes.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, namespace):\n        command = f\"echo [[ $(kubectl --project={project}\" \"'' get cluster --namespace={namespace}\" \"'' -o=json | jq '.nodes' | jq '.[]' )\" \"'' | grep '{10.9}')]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_system_function():\n    \"\"\"\n    Create a kubernetes system function which list cluster nodes.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, namespace):\n        command = f\"echo [[ $(kubectl --project={project}\" \"'' get cluster --namespace={namespace}\" \"'' -o=json | jq '.nodes' | jq '.[]')]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job():\n    \"\"\"\n    Create a kubernetes job which act as a sensor which wait for a list of\n    nodes are available.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ $((`date --date={start_time}\" \"'' +%s) - {start_time} + 600s`\" \"'' | jq '. > 0)]]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_with_backoff():\n    \"\"\"\n    Create a kubernetes job which wait for a list of nodes are available\"\n    with exponential backoff.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ $((`date --date={start_time}\" \"'' +%s) - {start_time} + 600s`\" \"'' | jq '. > 0)]]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_system():\n    \"\"\"\n    Create a kubernetes system job which wait for a list of nodes are available.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ $((`date --date={start_time}\" \"'' +%s) - {start_time} + 600s`\" \"'' | jq '. > 0)]]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_system_with_backoff():\n    \"\"\"\n    Create a kubernetes system job which wait for a list of nodes are available\"\n    with exponential backoff.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ $((`date --date={start_time}\" \"'' +%s) - {start_time} + 600s`\" \"'' | jq '. > 0)]]\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_with_inlet():\n    \"\"\"\n    Create a kubernetes job which use Inlet to get the result of the job.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Inlet.result()} == {Inlet.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_with_inlet_timeout():\n    \"\"\"\n    Create a kubernetes job which use Inlet to get the result of the job within\n    a timeout.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Inlet.result(timeout=30)} == {Inlet.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_system_inlet():\n    \"\"\"\n    Create a kubernetes system job which use Inlet to get the result of the job.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Inlet.result()} == {Inlet.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_system_inlet_timeout():\n    \"\"\"\n    Create a kubernetes system job which use Inlet to get the result of the job within\n    a timeout.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Inlet.result(timeout=30)} == {Inlet.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_with_ingo():\n    \"\"\"\n    Create a kubernetes job which use Ingo to get the result of the job.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Ingo.result()} == {Ingo.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_with_ingo_timeout():\n    \"\"\"\n    Create a kubernetes job which use Ingo to get the result of the job within\n    a timeout.\n    \"\"\"\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.sdk import task\n\n    def test_func(*, project, job_name, namespace, start_time):\n        command = f\"echo [[ {Ingo.result(timeout=30)} == {Ingo.task('nodes_are_available_sensor')}\" \"'' }}\"\n        return BashOperator(\n            task_id=\"test_func\",\n            bash_command=command,\n            dag=dag,\n        )\n\n    return test_func\n\n\ndef _create_kubernetes_job_system_"}], "metadata": {"file_name": "dag_902_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "qwen-server", "model_card": "qwen2.5-1.5b-airflow-instruct", "backend": "llama-server-client"}}
