{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_branch_day_of_week_operator.py"}, "content": "\"\"\"\nExample DAG demonstrating the usage of BranchDayOfWeekOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.weekday import BranchDayOfWeekOperator\nfrom airflow.utils.weekday import WeekDay\n\nwith DAG(\n    dag_id=\"example_weekday_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n) as dag:\n    # [START howto_operator_day_of_week_branch]\n    empty_task_1 = EmptyOperator(task_id=\"branch_true\")\n    empty_task_2 = EmptyOperator(task_id=\"branch_false\")\n    empty_task_3 = EmptyOperator(task_id=\"branch_weekend\")\n    empty_task_4 = EmptyOperator(task_id=\"branch_mid_week\")\n\n    branch = BranchDayOfWeekOperator(\n        task_id=\"make_choice\",\n        follow_task_ids_if_true=\"branch_true\",\n        follow_task_ids_if_false=\"branch_false\",\n        week_day=\"Monday\",\n    )\n    branch_weekend = BranchDayOfWeekOperator(\n        task_id=\"make_weekend_choice\",\n        follow_task_ids_if_true=\"branch_weekend\",\n        follow_task_ids_if_false=\"branch_mid_week\",\n        week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},\n    )\n\n    # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise\n    branch >> [empty_task_1, empty_task_2]\n    # Run empty_task_3 if it's a weekend, empty_task_4 otherwise\n    empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]\n    # [END howto_operator_day_of_week_branch]", "extracted_at": "2025-11-19T17:21:14.473266", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 28, "file_name": "example_branch_labels.py"}, "content": "\"\"\"\nExample DAG demonstrating the usage of labels with different branches.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\nwith DAG(\n    \"example_branch_labels\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    ingest = EmptyOperator(task_id=\"ingest\")\n    analyse = EmptyOperator(task_id=\"analyze\")\n    check = EmptyOperator(task_id=\"check_integrity\")\n    describe = EmptyOperator(task_id=\"describe_integrity\")\n    error = EmptyOperator(task_id=\"email_error\")\n    save = EmptyOperator(task_id=\"save\")\n    report = EmptyOperator(task_id=\"report\")\n\n    ingest >> analyse >> check\n    check >> Label(\"No errors\") >> save >> report\n    check >> Label(\"Errors found\") >> describe >> error >> report", "extracted_at": "2025-11-19T17:21:14.476350", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 59, "file_name": "example_bash_operator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"example_bash_operator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=[\"example\", \"example2\"],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id=\"run_this_last\",\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id=\"run_after_loop\",\n        bash_command=\"echo 1\",\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id=f\"runme_{i}\",\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last\n\n# [START howto_operator_bash_skip]\nthis_will_skip = BashOperator(\n    task_id=\"this_will_skip\",\n    bash_command='echo \"hello world\"; exit 99;',\n    dag=dag,\n)\n# [END howto_operator_bash_skip]\nthis_will_skip >> run_this_last\n\nif __name__ == \"__main__\":\n    dag.test()", "extracted_at": "2025-11-19T17:21:14.482783", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_branch_python_dop_operator_3.py"}, "content": "\"\"\"\nExample DAG demonstrating the usage of ``@task.branch`` TaskFlow API decorator with depends_on_past=True,\nwhere tasks may be run or skipped on alternating runs.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\n\n\n@task.branch()\ndef should_run(**kwargs) -> str:\n    \"\"\"\n    Determine which empty_task should be run based on if the execution date minute is even or odd.\n\n    :param dict kwargs: Context\n    :return: Id of the task to run\n    \"\"\"\n    print(\n        f\"------------- exec dttm = {kwargs['execution_date']} and minute = {kwargs['execution_date'].minute}\"\n    )\n    if kwargs[\"execution_date\"].minute % 2 == 0:\n        return \"empty_task_1\"\n    else:\n        return \"empty_task_2\"\n\n\nwith DAG(\n    dag_id=\"example_branch_dop_operator_v3\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    default_args={\"depends_on_past\": True},\n    tags=[\"example\"],\n) as dag:\n    cond = should_run()\n\n    empty_task_1 = EmptyOperator(task_id=\"empty_task_1\")\n    empty_task_2 = EmptyOperator(task_id=\"empty_task_2\")\n    cond >> [empty_task_1, empty_task_2]", "extracted_at": "2025-11-19T17:21:14.485068", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 48, "file_name": "example_branch_operator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the BranchPythonOperator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(\n        task_id=\"run_this_first\",\n    )\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    branching = BranchPythonOperator(\n        task_id=\"branching\",\n        python_callable=lambda: random.choice(options),\n    )\n    run_this_first >> branching\n\n    join = EmptyOperator(\n        task_id=\"join\",\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n    )\n\n    for option in options:\n        t = EmptyOperator(\n            task_id=option,\n        )\n\n        empty_follow = EmptyOperator(\n            task_id=\"follow_\" + option,\n        )\n\n        # Label is optional here, but it can help identify more complex branches\n        branching >> Label(option) >> t >> empty_follow >> join", "extracted_at": "2025-11-19T17:21:14.488100", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 87, "file_name": "example_branch_datetime_operator.py"}, "content": "\"\"\"\nExample DAG demonstrating the usage of DateTimeBranchOperator with datetime as well as time objects as\ntargets.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.datetime import BranchDateTimeOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndag1 = DAG(\n    dag_id=\"example_branch_datetime_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n\n# [START howto_branch_datetime_operator]\nempty_task_11 = EmptyOperator(task_id=\"date_in_range\", dag=dag1)\nempty_task_21 = EmptyOperator(task_id=\"date_outside_range\", dag=dag1)\n\ncond1 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag1,\n)\n\n# Run empty_task_11 if cond1 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond1 >> [empty_task_11, empty_task_21]\n# [END howto_branch_datetime_operator]\n\n\ndag2 = DAG(\n    dag_id=\"example_branch_datetime_operator_2\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_next_day]\nempty_task_12 = EmptyOperator(task_id=\"date_in_range\", dag=dag2)\nempty_task_22 = EmptyOperator(task_id=\"date_outside_range\", dag=dag2)\n\ncond2 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.time(0, 0, 0),\n    target_lower=pendulum.time(15, 0, 0),\n    dag=dag2,\n)\n\n# Since target_lower happens after target_upper, target_upper will be moved to the following day\n# Run empty_task_12 if cond2 executes between 15:00:00, and 00:00:00 of the following day\ncond2 >> [empty_task_12, empty_task_22]\n# [END howto_branch_datetime_operator_next_day]\n\ndag3 = DAG(\n    dag_id=\"example_branch_datetime_operator_3\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_logical_date]\nempty_task_13 = EmptyOperator(task_id=\"date_in_range\", dag=dag3)\nempty_task_23 = EmptyOperator(task_id=\"date_outside_range\", dag=dag3)\n\ncond3 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    use_task_logical_date=True,\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag3,\n)\n\n# Run empty_task_13 if cond3 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond3 >> [empty_task_13, empty_task_23]\n# [END howto_branch_datetime_operator_logical_date]", "extracted_at": "2025-11-19T17:21:14.490170", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 203, "file_name": "example_complex.py"}, "content": "\"\"\"\nExample Airflow DAG that shows the complex DAG structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\n\nwith models.DAG(\n    dag_id=\"example_complex\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"example2\", \"example3\"],\n) as dag:\n    # Create\n    create_entry_group = BashOperator(task_id=\"create_entry_group\", bash_command=\"echo create_entry_group\")\n\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\", bash_command=\"echo create_entry_group_result\"\n    )\n\n    create_entry_group_result2 = BashOperator(\n        task_id=\"create_entry_group_result2\", bash_command=\"echo create_entry_group_result2\"\n    )\n\n    create_entry_gcs = BashOperator(task_id=\"create_entry_gcs\", bash_command=\"echo create_entry_gcs\")\n\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\", bash_command=\"echo create_entry_gcs_result\"\n    )\n\n    create_entry_gcs_result2 = BashOperator(\n        task_id=\"create_entry_gcs_result2\", bash_command=\"echo create_entry_gcs_result2\"\n    )\n\n    create_tag = BashOperator(task_id=\"create_tag\", bash_command=\"echo create_tag\")\n\n    create_tag_result = BashOperator(task_id=\"create_tag_result\", bash_command=\"echo create_tag_result\")\n\n    create_tag_result2 = BashOperator(task_id=\"create_tag_result2\", bash_command=\"echo create_tag_result2\")\n\n    create_tag_template = BashOperator(task_id=\"create_tag_template\", bash_command=\"echo create_tag_template\")\n\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\", bash_command=\"echo create_tag_template_result\"\n    )\n\n    create_tag_template_result2 = BashOperator(\n        task_id=\"create_tag_template_result2\", bash_command=\"echo create_tag_template_result2\"\n    )\n\n    create_tag_template_field = BashOperator(\n        task_id=\"create_tag_template_field\", bash_command=\"echo create_tag_template_field\"\n    )\n\n    create_tag_template_field_result = BashOperator(\n        task_id=\"create_tag_template_field_result\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    create_tag_template_field_result2 = BashOperator(\n        task_id=\"create_tag_template_field_result2\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    # Delete\n    delete_entry = BashOperator(task_id=\"delete_entry\", bash_command=\"echo delete_entry\")\n    create_entry_gcs >> delete_entry\n\n    delete_entry_group = BashOperator(task_id=\"delete_entry_group\", bash_command=\"echo delete_entry_group\")\n    create_entry_group >> delete_entry_group\n\n    delete_tag = BashOperator(task_id=\"delete_tag\", bash_command=\"echo delete_tag\")\n    create_tag >> delete_tag\n\n    delete_tag_template_field = BashOperator(\n        task_id=\"delete_tag_template_field\", bash_command=\"echo delete_tag_template_field\"\n    )\n\n    delete_tag_template = BashOperator(task_id=\"delete_tag_template\", bash_command=\"echo delete_tag_template\")\n\n    # Get\n    get_entry_group = BashOperator(task_id=\"get_entry_group\", bash_command=\"echo get_entry_group\")\n\n    get_entry_group_result = BashOperator(\n        task_id=\"get_entry_group_result\", bash_command=\"echo get_entry_group_result\"\n    )\n\n    get_entry = BashOperator(task_id=\"get_entry\", bash_command=\"echo get_entry\")\n\n    get_entry_result = BashOperator(task_id=\"get_entry_result\", bash_command=\"echo get_entry_result\")\n\n    get_tag_template = BashOperator(task_id=\"get_tag_template\", bash_command=\"echo get_tag_template\")\n\n    get_tag_template_result = BashOperator(\n        task_id=\"get_tag_template_result\", bash_command=\"echo get_tag_template_result\"\n    )\n\n    # List\n    list_tags = BashOperator(task_id=\"list_tags\", bash_command=\"echo list_tags\")\n\n    list_tags_result = BashOperator(task_id=\"list_tags_result\", bash_command=\"echo list_tags_result\")\n\n    # Lookup\n    lookup_entry = BashOperator(task_id=\"lookup_entry\", bash_command=\"echo lookup_entry\")\n\n    lookup_entry_result = BashOperator(task_id=\"lookup_entry_result\", bash_command=\"echo lookup_entry_result\")\n\n    # Rename\n    rename_tag_template_field = BashOperator(\n        task_id=\"rename_tag_template_field\", bash_command=\"echo rename_tag_template_field\"\n    )\n\n    # Search\n    search_catalog = BashOperator(task_id=\"search_catalog\", bash_command=\"echo search_catalog\")\n\n    search_catalog_result = BashOperator(\n        task_id=\"search_catalog_result\", bash_command=\"echo search_catalog_result\"\n    )\n\n    # Update\n    update_entry = BashOperator(task_id=\"update_entry\", bash_command=\"echo update_entry\")\n\n    update_tag = BashOperator(task_id=\"update_tag\", bash_command=\"echo update_tag\")\n\n    update_tag_template = BashOperator(task_id=\"update_tag_template\", bash_command=\"echo update_tag_template\")\n\n    update_tag_template_field = BashOperator(\n        task_id=\"update_tag_template_field\", bash_command=\"echo update_tag_template_field\"\n    )\n\n    # Create\n    create_tasks = [\n        create_entry_group,\n        create_entry_gcs,\n        create_tag_template,\n        create_tag_template_field,\n        create_tag,\n    ]\n    chain(*create_tasks)\n\n    create_entry_group >> delete_entry_group\n    create_entry_group >> create_entry_group_result\n    create_entry_group >> create_entry_group_result2\n\n    create_entry_gcs >> delete_entry\n    create_entry_gcs >> create_entry_gcs_result\n    create_entry_gcs >> create_entry_gcs_result2\n\n    create_tag_template >> delete_tag_template_field\n    create_tag_template >> create_tag_template_result\n    create_tag_template >> create_tag_template_result2\n\n    create_tag_template_field >> delete_tag_template_field\n    create_tag_template_field >> create_tag_template_field_result\n    create_tag_template_field >> create_tag_template_field_result2\n\n    create_tag >> delete_tag\n    create_tag >> create_tag_result\n    create_tag >> create_tag_result2\n\n    # Delete\n    delete_tasks = [\n        delete_tag,\n        delete_tag_template_field,\n        delete_tag_template,\n        delete_entry_group,\n        delete_entry,\n    ]\n    chain(*delete_tasks)\n\n    # Get\n    create_tag_template >> get_tag_template >> delete_tag_template\n    get_tag_template >> get_tag_template_result\n\n    create_entry_gcs >> get_entry >> delete_entry\n    get_entry >> get_entry_result\n\n    create_entry_group >> get_entry_group >> delete_entry_group\n    get_entry_group >> get_entry_group_result\n\n    # List\n    create_tag >> list_tags >> delete_tag\n    list_tags >> list_tags_result\n\n    # Lookup\n    create_entry_gcs >> lookup_entry >> delete_entry\n    lookup_entry >> lookup_entry_result\n\n    # Rename\n    create_tag_template_field >> rename_tag_template_field >> delete_tag_template_field\n\n    # Search\n    chain(create_tasks, search_catalog, delete_tasks)\n    search_catalog >> search_catalog_result\n\n    # Update\n    create_entry_gcs >> update_entry >> delete_entry\n    create_tag >> update_tag >> delete_tag\n    create_tag_template >> update_tag_template >> delete_tag_template\n    create_tag_template_field >> update_tag_template_field >> rename_tag_template_field", "extracted_at": "2025-11-19T17:21:14.494200", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 41, "file_name": "example_branch_operator_decorator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the ``@task.branch`` TaskFlow API decorator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_python_operator_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"run_this_first\")\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    @task.branch(task_id=\"branching\")\n    def random_choice(choices: list[str]) -> str:\n        return random.choice(choices)\n\n    random_choice_instance = random_choice(choices=options)\n\n    run_this_first >> random_choice_instance\n\n    join = EmptyOperator(task_id=\"join\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n\n    for option in options:\n        t = EmptyOperator(task_id=option)\n\n        empty_follow = EmptyOperator(task_id=\"follow_\" + option)\n\n        # Label is optional here, but it can help identify more complex branches\n        random_choice_instance >> Label(option) >> t >> empty_follow >> join", "extracted_at": "2025-11-19T17:21:14.496407", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 113, "file_name": "example_datasets.py"}, "content": "\"\"\"\nExample DAG for demonstrating behavior of Datasets feature.\n\nNotes on usage:\n\nTurn on all the dags.\n\nDAG dataset_produces_1 should run because it's on a schedule.\n\nAfter dataset_produces_1 runs, dataset_consumes_1 should be triggered immediately\nbecause its only dataset dependency is managed by dataset_produces_1.\n\nNo other dags should be triggered.  Note that even though dataset_consumes_1_and_2 depends on\nthe dataset in dataset_produces_1, it will not be triggered until dataset_produces_2 runs\n(and dataset_produces_2 is left with no schedule so that we can trigger it manually).\n\nNext, trigger dataset_produces_2.  After dataset_produces_2 finishes,\ndataset_consumes_1_and_2 should run.\n\nDags dataset_consumes_1_never_scheduled and dataset_consumes_unknown_never_scheduled should not run because\nthey depend on datasets that never get updated.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, Dataset\nfrom airflow.operators.bash import BashOperator\n\n# [START dataset_def]\ndag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n# [END dataset_def]\ndag2_dataset = Dataset(\"s3://dag2/output_1.txt\", extra={\"hi\": \"bye\"})\n\nwith DAG(\n    dag_id=\"dataset_produces_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@daily\",\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag1:\n    # [START task_outlet]\n    BashOperator(outlets=[dag1_dataset], task_id=\"producing_task_1\", bash_command=\"sleep 5\")\n    # [END task_outlet]\n\nwith DAG(\n    dag_id=\"dataset_produces_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag2:\n    BashOperator(outlets=[dag2_dataset], task_id=\"producing_task_2\", bash_command=\"sleep 5\")\n\n# [START dag_dep]\nwith DAG(\n    dag_id=\"dataset_consumes_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag3:\n    # [END dag_dep]\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_1_task/dataset_other.txt\")],\n        task_id=\"consuming_1\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_and_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset, dag2_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag4:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_2\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        dag1_dataset,\n        Dataset(\"s3://this-dataset-doesnt-get-triggered\"),\n    ],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag5:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_3\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_unknown_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        Dataset(\"s3://unrelated/dataset3.txt\"),\n        Dataset(\"s3://unrelated/dataset_other_unknown.txt\"),\n    ],\n    tags=[\"dataset-scheduled\"],\n) as dag6:\n    BashOperator(\n        task_id=\"unrelated_task\",\n        outlets=[Dataset(\"s3://unrelated_task/dataset_other_unknown.txt\")],\n        bash_command=\"sleep 5\",\n    )", "extracted_at": "2025-11-19T17:21:14.538505", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 56, "file_name": "example_dag_decorator.py"}, "content": "from __future__ import annotations\n\nfrom typing import Any\n\nimport httpx\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.email import EmailOperator\nfrom airflow.utils.context import Context\n\n\nclass GetRequestOperator(BaseOperator):\n    \"\"\"Custom operator to send GET request to provided url\"\"\"\n\n    def __init__(self, *, url: str, **kwargs):\n        super().__init__(**kwargs)\n        self.url = url\n\n    def execute(self, context: Context):\n        return httpx.get(self.url).json()\n\n\n# [START dag_decorator_usage]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_decorator(email: str = \"example@example.com\"):\n    \"\"\"\n    DAG to send server IP to email.\n\n    :param email: Email to send IP to. Defaults to example@example.com.\n    \"\"\"\n    get_ip = GetRequestOperator(task_id=\"get_ip\", url=\"http://httpbin.org/get\")\n\n    @task(multiple_outputs=True)\n    def prepare_email(raw_json: dict[str, Any]) -> dict[str, str]:\n        external_ip = raw_json[\"origin\"]\n        return {\n            \"subject\": f\"Server connected from {external_ip}\",\n            \"body\": f\"Seems like today your server executing Airflow is connected from IP {external_ip}<br>\",\n        }\n\n    email_info = prepare_email(get_ip.output)\n\n    EmailOperator(\n        task_id=\"send_email\", to=email, subject=email_info[\"subject\"], html_content=email_info[\"body\"]\n    )\n\n\nexample_dag = example_dag_decorator()\n# [END dag_decorator_usage]", "extracted_at": "2025-11-19T17:21:14.540397", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 21, "file_name": "example_dynamic_task_mapping.py"}, "content": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nwith DAG(dag_id=\"example_dynamic_task_mapping\", start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def add_one(x: int):\n        return x + 1\n\n    @task\n    def sum_it(values):\n        total = sum(values)\n        print(f\"Total was {total}\")\n\n    added_values = add_one.expand(x=[1, 2, 3])\n    sum_it(added_values)", "extracted_at": "2025-11-19T17:21:14.542810", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 45, "file_name": "example_dynamic_task_mapping_with_no_taskflow_operators.py"}, "content": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import BaseOperator\n\n\nclass AddOneOperator(BaseOperator):\n    \"\"\"A custom operator that adds one to the input.\"\"\"\n\n    def __init__(self, value, **kwargs):\n        super().__init__(**kwargs)\n        self.value = value\n\n    def execute(self, context):\n        return self.value + 1\n\n\nclass SumItOperator(BaseOperator):\n    \"\"\"A custom operator that sums the input.\"\"\"\n\n    template_fields = (\"values\",)\n\n    def __init__(self, values, **kwargs):\n        super().__init__(**kwargs)\n        self.values = values\n\n    def execute(self, context):\n        total = sum(self.values)\n        print(f\"Total was {total}\")\n        return total\n\n\nwith DAG(\n    dag_id=\"example_dynamic_task_mapping_with_no_taskflow_operators\",\n    start_date=datetime(2022, 3, 4),\n    catchup=False,\n):\n    # map the task to a list of values\n    add_one_task = AddOneOperator.partial(task_id=\"add_one\").expand(value=[1, 2, 3])\n\n    # aggregate (reduce) the mapped tasks results\n    sum_it_task = SumItOperator(task_id=\"sum_it\", values=add_one_task.output)", "extracted_at": "2025-11-19T17:21:14.545039", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 198, "file_name": "example_kubernetes_executor.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n    k8s = None\n\n\nif k8s:\n    with DAG(\n        dag_id=\"example_kubernetes_executor\",\n        schedule=None,\n        start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n        catchup=False,\n        tags=[\"example3\"],\n    ) as dag:\n        # You can use annotations on your kubernetes pods!\n        start_task_executor_config = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={\"test\": \"annotation\"}))\n        }\n\n        @task(executor_config=start_task_executor_config)\n        def start_task():\n            print_stuff()\n\n        # [START task_with_volume]\n        executor_config_volume_mount = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            volume_mounts=[\n                                k8s.V1VolumeMount(mount_path=\"/foo/\", name=\"example-kubernetes-test-volume\")\n                            ],\n                        )\n                    ],\n                    volumes=[\n                        k8s.V1Volume(\n                            name=\"example-kubernetes-test-volume\",\n                            host_path=k8s.V1HostPathVolumeSource(path=\"/tmp/\"),\n                        )\n                    ],\n                )\n            ),\n        }\n\n        @task(executor_config=executor_config_volume_mount)\n        def test_volume_mount():\n            \"\"\"\n            Tests whether the volume has been mounted.\n            \"\"\"\n\n            with open(\"/foo/volume_mount_test.txt\", \"w\") as foo:\n                foo.write(\"Hello\")\n\n            return_code = os.system(\"cat /foo/volume_mount_test.txt\")\n            if return_code != 0:\n                raise ValueError(f\"Error when checking volume mount. Return code {return_code}\")\n\n        volume_task = test_volume_mount()\n        # [END task_with_volume]\n\n        # [START task_with_sidecar]\n        executor_config_sidecar = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            volume_mounts=[k8s.V1VolumeMount(mount_path=\"/shared/\", name=\"shared-empty-dir\")],\n                        ),\n                        k8s.V1Container(\n                            name=\"sidecar\",\n                            image=\"ubuntu\",\n                            args=['echo \"retrieved from mount\" > /shared/test.txt'],\n                            command=[\"bash\", \"-cx\"],\n                            volume_mounts=[k8s.V1VolumeMount(mount_path=\"/shared/\", name=\"shared-empty-dir\")],\n                        ),\n                    ],\n                    volumes=[\n                        k8s.V1Volume(name=\"shared-empty-dir\", empty_dir=k8s.V1EmptyDirVolumeSource()),\n                    ],\n                )\n            ),\n        }\n\n        @task(executor_config=executor_config_sidecar)\n        def test_sharedvolume_mount():\n            \"\"\"\n            Tests whether the volume has been mounted.\n            \"\"\"\n            for i in range(5):\n                try:\n                    return_code = os.system(\"cat /shared/test.txt\")\n                    if return_code != 0:\n                        raise ValueError(f\"Error when checking volume mount. Return code {return_code}\")\n                except ValueError as e:\n                    if i > 4:\n                        raise e\n\n        sidecar_task = test_sharedvolume_mount()\n        # [END task_with_sidecar]\n\n        # You can add labels to pods\n        executor_config_non_root = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(labels={\"release\": \"stable\"}))\n        }\n\n        @task(executor_config=executor_config_non_root)\n        def non_root_task():\n            print_stuff()\n\n        third_task = non_root_task()\n\n        executor_config_other_ns = {\n            \"pod_override\": k8s.V1Pod(\n                metadata=k8s.V1ObjectMeta(namespace=\"test-namespace\", labels={\"release\": \"stable\"})\n            )\n        }\n\n        @task(executor_config=executor_config_other_ns)\n        def other_namespace_task():\n            print_stuff()\n\n        other_ns_task = other_namespace_task()\n        worker_container_repository = conf.get(\"kubernetes_executor\", \"worker_container_repository\")\n        worker_container_tag = conf.get(\"kubernetes_executor\", \"worker_container_tag\")\n\n        # You can also change the base image, here we used the worker image for demonstration.\n        # Note that the image must have the same configuration as the\n        # worker image. Could be that you want to run this task in a special docker image that has a zip\n        # library built-in. You build the special docker image on top your worker image.\n        kube_exec_config_special = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\", image=f\"{worker_container_repository}:{worker_container_tag}\"\n                        ),\n                    ]\n                )\n            )\n        }\n\n        @task(executor_config=kube_exec_config_special)\n        def base_image_override_task():\n            print_stuff()\n\n        base_image_task = base_image_override_task()\n\n        # Use k8s_client.V1Affinity to define node affinity\n        k8s_affinity = k8s.V1Affinity(\n            pod_anti_affinity=k8s.V1PodAntiAffinity(\n                required_during_scheduling_ignored_during_execution=[\n                    k8s.V1PodAffinityTerm(\n                        label_selector=k8s.V1LabelSelector(\n                            match_expressions=[\n                                k8s.V1LabelSelectorRequirement(key=\"app\", operator=\"In\", values=[\"airflow\"])\n                            ]\n                        ),\n                        topology_key=\"kubernetes.io/hostname\",\n                    )\n                ]\n            )\n        )\n\n        # Use k8s_client.V1Toleration to define node tolerations\n        k8s_tolerations = [k8s.V1Toleration(key=\"dedicated\", operator=\"Equal\", value=\"airflow\")]\n\n        # Use k8s_client.V1ResourceRequirements to define resource limits\n        k8s_resource_requirements = k8s.V1ResourceRequirements(\n            requests={\"memory\": \"512Mi\"}, limits={\"memory\": \"512Mi\"}\n        )\n\n        kube_exec_config_resource_limits = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            resources=k8s_resource_requirements,\n                        )\n                    ],\n                    affinity=k8s_affinity,\n                    tolerations=k8s_tolerations,\n                )\n            )\n        }\n\n        @task(executor_config=kube_exec_config_resource_limits)\n        def task_with_resource_limits():\n            print_stuff()\n\n        four_task = task_with_resource_limits()\n\n        (\n            start_task()\n            >> [volume_task, other_ns_task, sidecar_task]\n            >> third_task\n            >> [base_image_task, four_task]\n        )", "extracted_at": "2025-11-19T17:21:14.547197", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 20, "file_name": "example_latest_only.py"}, "content": "\"\"\"Example of the LatestOnlyOperator\"\"\"\nfrom __future__ import annotations\n\nimport datetime as dt\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\n\nwith DAG(\n    dag_id=\"latest_only\",\n    schedule=dt.timedelta(hours=4),\n    start_date=dt.datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example2\", \"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n\n    latest_only >> task1", "extracted_at": "2025-11-19T17:21:14.552031", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 31, "file_name": "example_latest_only_with_trigger.py"}, "content": "\"\"\"\nExample LatestOnlyOperator and TriggerRule interactions\n\"\"\"\nfrom __future__ import annotations\n\n# [START example]\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=TriggerRule.ALL_DONE)\n\n    latest_only >> task1 >> [task3, task4]\n    task2 >> [task3, task4]\n# [END example]", "extracted_at": "2025-11-19T17:21:14.556907", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 80, "file_name": "example_external_task_marker_dag.py"}, "content": "\"\"\"\nExample DAG demonstrating setting up inter-DAG dependencies using ExternalTaskSensor and\nExternalTaskMarker.\n\nIn this example, child_task1 in example_external_task_marker_child depends on parent_task in\nexample_external_task_marker_parent. When parent_task is cleared with 'Recursive' selected,\nthe presence of ExternalTaskMarker tells Airflow to clear child_task1 and its downstream tasks.\n\nExternalTaskSensor will keep poking for the status of remote ExternalTaskMarker task at a regular\ninterval till one of the following will happen:\n\nExternalTaskMarker reaches the states mentioned in the allowed_states list.\nIn this case, ExternalTaskSensor will exit with a success status code\n\nExternalTaskMarker reaches the states mentioned in the failed_states list\nIn this case, ExternalTaskSensor will raise an AirflowException and user need to handle this\nwith multiple downstream tasks\n\nExternalTaskSensor times out. In this case, ExternalTaskSensor will raise AirflowSkipException\nor AirflowSensorTimeout exception\n\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.external_task import ExternalTaskMarker, ExternalTaskSensor\n\nstart_date = pendulum.datetime(2021, 1, 1, tz=\"UTC\")\n\nwith DAG(\n    dag_id=\"example_external_task_marker_parent\",\n    start_date=start_date,\n    catchup=False,\n    schedule=None,\n    tags=[\"example2\"],\n) as parent_dag:\n    # [START howto_operator_external_task_marker]\n    parent_task = ExternalTaskMarker(\n        task_id=\"parent_task\",\n        external_dag_id=\"example_external_task_marker_child\",\n        external_task_id=\"child_task1\",\n    )\n    # [END howto_operator_external_task_marker]\n\nwith DAG(\n    dag_id=\"example_external_task_marker_child\",\n    start_date=start_date,\n    schedule=None,\n    catchup=False,\n    tags=[\"example2\"],\n) as child_dag:\n    # [START howto_operator_external_task_sensor]\n    child_task1 = ExternalTaskSensor(\n        task_id=\"child_task1\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_id=parent_task.task_id,\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor]\n\n    # [START howto_operator_external_task_sensor_with_task_group]\n    child_task2 = ExternalTaskSensor(\n        task_id=\"child_task2\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_group_id=\"parent_dag_task_group_id\",\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor_with_task_group]\n\n    child_task3 = EmptyOperator(task_id=\"child_task3\")\n    child_task1 >> child_task2 >> child_task3", "extracted_at": "2025-11-19T17:21:14.558329", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 39, "file_name": "example_nested_branch_dag.py"}, "content": "\"\"\"\nExample DAG demonstrating a workflow with nested branching. The join tasks are created with\n``none_failed_min_one_success`` trigger rule such that they are skipped whenever their corresponding\nbranching tasks are skipped.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_nested_branch_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\"],\n) as dag:\n\n    @task.branch()\n    def branch(task_id_to_return: str) -> str:\n        return task_id_to_return\n\n    branch_1 = branch.override(task_id=\"branch_1\")(task_id_to_return=\"true_1\")\n    join_1 = EmptyOperator(task_id=\"join_1\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_1 = EmptyOperator(task_id=\"true_1\")\n    false_1 = EmptyOperator(task_id=\"false_1\")\n\n    branch_2 = branch.override(task_id=\"branch_2\")(task_id_to_return=\"true_2\")\n    join_2 = EmptyOperator(task_id=\"join_2\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_2 = EmptyOperator(task_id=\"true_2\")\n    false_2 = EmptyOperator(task_id=\"false_2\")\n    false_3 = EmptyOperator(task_id=\"false_3\")\n\n    branch_1 >> true_1 >> join_1\n    branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1", "extracted_at": "2025-11-19T17:21:14.594672", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 31, "file_name": "example_local_kubernetes_executor.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    k8s = None\n\nif k8s:\n    with DAG(\n        dag_id=\"example_local_kubernetes_executor\",\n        schedule=None,\n        start_date=datetime(2021, 1, 1),\n        catchup=False,\n        tags=[\"example3\"],\n    ) as dag:\n        # You can use annotations on your kubernetes pods!\n        start_task_executor_config = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={\"test\": \"annotation\"}))\n        }\n\n        @task(\n            executor_config=start_task_executor_config,\n            queue=\"kubernetes\",\n            task_id=\"task_with_kubernetes_executor\",\n        )\n        def task_with_template():\n            print_stuff()\n\n        @task(task_id=\"task_with_local_executor\")\n        def task_with_local(ds=None, **kwargs):\n            \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n            print(kwargs)\n            print(ds)\n            return \"Whatever you return gets printed in the logs\"\n\n        task_with_local() >> task_with_template()", "extracted_at": "2025-11-19T17:21:14.596794", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 141, "file_name": "example_python_operator.py"}, "content": "\"\"\"\nExample DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a\nvirtual environment.\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport tempfile\nimport time\nfrom pprint import pprint\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import ExternalPythonOperator, PythonVirtualenvOperator, is_venv_installed\n\nlog = logging.getLogger(__name__)\n\nPATH_TO_PYTHON_BINARY = sys.executable\n\nBASE_DIR = tempfile.gettempdir()\n\n\ndef x():\n    pass\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_python]\n    @task(task_id=\"print_the_context\")\n    def print_context(ds=None, **kwargs):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        pprint(kwargs)\n        print(ds)\n        return \"Whatever you return gets printed in the logs\"\n\n    run_this = print_context()\n    # [END howto_operator_python]\n\n    # [START howto_operator_python_render_sql]\n    @task(task_id=\"log_sql_query\", templates_dict={\"query\": \"sql/sample.sql\"}, templates_exts=[\".sql\"])\n    def log_sql(**kwargs):\n        logging.info(\"Python task decorator query: %s\", str(kwargs[\"templates_dict\"][\"query\"]))\n\n    log_the_sql = log_sql()\n    # [END howto_operator_python_render_sql]\n\n    # [START howto_operator_python_kwargs]\n    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively\n    for i in range(5):\n\n        @task(task_id=f\"sleep_for_{i}\")\n        def my_sleeping_function(random_base):\n            \"\"\"This is a function that will run within the DAG execution\"\"\"\n            time.sleep(random_base)\n\n        sleeping_task = my_sleeping_function(random_base=i / 10)\n\n        run_this >> log_the_sql >> sleeping_task\n    # [END howto_operator_python_kwargs]\n\n    if not is_venv_installed():\n        log.warning(\"The virtalenv_python example task requires virtualenv, please install it.\")\n    else:\n        # [START howto_operator_python_venv]\n        @task.virtualenv(\n            task_id=\"virtualenv_python\", requirements=[\"colorama==0.4.0\"], system_site_packages=False\n        )\n        def callable_virtualenv():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            from time import sleep\n\n            from colorama import Back, Fore, Style\n\n            print(Fore.RED + \"some red text\")\n            print(Back.GREEN + \"and with a green background\")\n            print(Style.DIM + \"and in dim text\")\n            print(Style.RESET_ALL)\n            for _ in range(4):\n                print(Style.DIM + \"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        virtualenv_task = callable_virtualenv()\n        # [END howto_operator_python_venv]\n\n        sleeping_task >> virtualenv_task\n\n        # [START howto_operator_external_python]\n        @task.external_python(task_id=\"external_python\", python=PATH_TO_PYTHON_BINARY)\n        def callable_external_python():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            import sys\n            from time import sleep\n\n            print(f\"Running task via {sys.executable}\")\n            print(\"Sleeping\")\n            for _ in range(4):\n                print(\"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        external_python_task = callable_external_python()\n        # [END howto_operator_external_python]\n\n        # [START howto_operator_external_python_classic]\n        external_classic = ExternalPythonOperator(\n            task_id=\"external_python_classic\",\n            python=PATH_TO_PYTHON_BINARY,\n            python_callable=x,\n        )\n        # [END howto_operator_external_python_classic]\n\n        # [START howto_operator_python_venv_classic]\n        virtual_classic = PythonVirtualenvOperator(\n            task_id=\"virtualenv_classic\",\n            requirements=\"colorama==0.4.0\",\n            python_callable=x,\n        )\n        # [END howto_operator_python_venv_classic]\n\n        run_this >> external_classic >> external_python_task >> virtual_classic", "extracted_at": "2025-11-19T17:21:14.620737", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 69, "file_name": "example_passing_params_via_test_command.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the params arguments in templated arguments.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom textwrap import dedent\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef my_py_command(params, test_mode=None, task=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command run_this <date>\n    -t '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(\n            f\" 'foo' was passed in via test={test_mode} command : kwargs[params][foo] = {task.params['foo']}\"\n        )\n    # Print out the value of \"miff\", passed in below via the Python Operator\n    print(f\" 'miff' was passed in via task params = {params['miff']}\")\n    return 1\n\n\n@task(task_id=\"env_var_test_task\")\ndef print_env_vars(test_mode=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command env_var_test_task <date>\n    --env-vars '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(f\"foo={os.environ.get('foo')}\")\n        print(f\"AIRFLOW_TEST_MODE={os.environ.get('AIRFLOW_TEST_MODE')}\")\n\n\nwith DAG(\n    \"example_passing_params_via_test_command\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=4),\n    tags=[\"example\"],\n) as dag:\n    run_this = my_py_command(params={\"miff\": \"agg\"})\n\n    my_command = dedent(\n        \"\"\"\n        echo \"'foo' was passed in via Airflow CLI Test command with value '$FOO'\"\n        echo \"'miff' was passed in via BashOperator with value '$MIFF'\"\n        \"\"\"\n    )\n\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command=my_command,\n        params={\"miff\": \"agg\"},\n        env={\"FOO\": \"{{ params.foo }}\", \"MIFF\": \"{{ params.miff }}\"},\n    )\n\n    env_var_test_task = print_env_vars()\n\n    run_this >> also_run_this", "extracted_at": "2025-11-19T17:21:14.625168", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 295, "file_name": "example_params_ui_tutorial.py"}, "content": "\"\"\"DAG demonstrating various options for a trigger form generated by DAG params.\n\nThe DAG attribute `params` is used to define a default dictionary of parameters which are usually passed\nto the DAG and which are used to render a trigger form.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport json\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        # Let's start simple: Standard dict values are detected from type and offered as entry form fields.\n        # Detected types are numbers, text, boolean, lists and dicts.\n        # Note that such auto-detected parameters are treated as optional (not required to contain a value)\n        \"x\": 3,\n        \"text\": \"Hello World!\",\n        \"flag\": False,\n        \"a_simple_list\": [\"one\", \"two\", \"three\", \"actually one value is made per line\"],\n        # But of course you might want to have it nicer! Let's add some description to parameters.\n        # Note if you can add any HTML formatting to the description, you need to use the description_html\n        # attribute.\n        \"most_loved_number\": Param(\n            42,\n            type=\"integer\",\n            title=\"Your favorite number\",\n            description_html=\"\"\"Everybody should have a favorite number. Not only math teachers.\n            If you can not think of any at the moment please think of the 42 which is very famous because\n            of the book\n            <a href='https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#\n            The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42'>\n            The Hitchhiker's Guide to the Galaxy</a>\"\"\",\n        ),\n        # If you want to have a selection list box then you can use the enum feature of JSON schema\n        \"pick_one\": Param(\n            \"value 42\",\n            type=\"string\",\n            title=\"Select one Value\",\n            description=\"You can use JSON schema enum's to generate drop down selection boxes.\",\n            enum=[f\"value {i}\" for i in range(16, 64)],\n        ),\n        # You can also label the selected values via values_display attribute\n        \"pick_with_label\": Param(\n            3,\n            type=\"number\",\n            title=\"Select one Number\",\n            description=\"With drop down selections you can also have nice display labels for the values.\",\n            enum=[*range(1, 10)],\n            values_display={\n                1: \"One\",\n                2: \"Two\",\n                3: \"Three\",\n                4: \"Four - is like you take three and get one for free!\",\n                5: \"Five\",\n                6: \"Six\",\n                7: \"Seven\",\n                8: \"Eight\",\n                9: \"Nine\",\n            },\n        ),\n        # If you want to have a list box with proposals but not enforcing a fixed list\n        # then you can use the examples feature of JSON schema\n        \"proposals\": Param(\n            \"some value\",\n            type=\"string\",\n            title=\"Field with proposals\",\n            description=\"You can use JSON schema examples's to generate drop down selection boxes \"\n            \"but allow also to enter custom values. Try typing an 'a' and see options.\",\n            examples=(\n                \"Alpha,Bravo,Charlie,Delta,Echo,Foxtrot,Golf,Hotel,India,Juliett,Kilo,Lima,Mike,November,Oscar,Papa,\"\n                \"Quebec,Romeo,Sierra,Tango,Uniform,Victor,Whiskey,X-ray,Yankee,Zulu\"\n            ).split(\",\"),\n        ),\n        # If you want to select multiple items from a fixed list JSON schema des not allow to use enum\n        # In this case the type \"array\" is being used together with \"examples\" as pick list\n        \"multi_select\": Param(\n            [\"two\", \"three\"],\n            \"Select from the list of options.\",\n            type=\"array\",\n            title=\"Multi Select\",\n            examples=[\"one\", \"two\", \"three\", \"four\", \"five\"],\n        ),\n        # A multiple options selection can also be combined with values_display\n        \"multi_select_with_label\": Param(\n            [\"2\", \"3\"],\n            \"Select from the list of options. See that options can have nicer text and still technical values\"\n            \"are propagated as values during trigger to the DAG.\",\n            type=\"array\",\n            title=\"Multi Select with Labels\",\n            examples=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n            values_display={\n                \"1\": \"One box of choccolate\",\n                \"2\": \"Two bananas\",\n                \"3\": \"Three apples\",\n                # Note: Value display mapping does not need to be complete.s\n            },\n        ),\n        # An array of numbers\n        \"array_of_numbers\": Param(\n            [1, 2, 3],\n            \"Only integers are accepted in this array\",\n            type=\"array\",\n            title=\"Array of numbers\",\n            items={\"type\": \"number\"},\n        ),\n        # Boolean as proper parameter with description\n        \"bool\": Param(\n            True,\n            type=\"boolean\",\n            title=\"Please confirm\",\n            description=\"A On/Off selection with a proper description.\",\n        ),\n        # Dates and Times are also supported\n        \"date_time\": Param(\n            f\"{datetime.date.today()}T{datetime.time(hour=12, minute=17, second=00)}+00:00\",\n            type=\"string\",\n            format=\"date-time\",\n            title=\"Date-Time Picker\",\n            description=\"Please select a date and time, use the button on the left for a pup-up calendar.\",\n        ),\n        \"date\": Param(\n            f\"{datetime.date.today()}\",\n            type=\"string\",\n            format=\"date\",\n            title=\"Date Picker\",\n            description=\"Please select a date, use the button on the left for a pup-up calendar. \"\n            \"See that here are no times!\",\n        ),\n        \"time\": Param(\n            f\"{datetime.time(hour=12, minute=13, second=14)}\",\n            type=[\"string\", \"null\"],\n            format=\"time\",\n            title=\"Time Picker\",\n            description=\"Please select a time, use the button on the left for a pup-up tool.\",\n        ),\n        # Fields can be required or not. If the defined fields are typed they are getting required by default\n        # (else they would not pass JSON schema validation) - to make typed fields optional you must\n        # permit the optional \"null\" type\n        \"required_field\": Param(\n            \"You can not trigger if no text is given here!\",\n            type=\"string\",\n            title=\"Required text field\",\n            description=\"This field is required. You can not submit without having text in here.\",\n        ),\n        \"optional_field\": Param(\n            \"optional text, you can trigger also w/o text\",\n            type=[\"null\", \"string\"],\n            title=\"Optional text field\",\n            description_html=\"This field is optional. As field content is JSON schema validated you must \"\n            \"allow the <code>null</code> type.\",\n        ),\n        # You can arrange the entry fields in sections so that you can have a better overview for the user\n        # Therefore you can add the \"section\" attribute.\n        # The benefit of the Params class definition is that the full scope of JSON schema validation\n        # can be leveraged for form fields and they will be validated before DAG submission.\n        \"checked_text\": Param(\n            \"length-checked-field\",\n            type=\"string\",\n            title=\"Text field with length check\",\n            description_html=\"\"\"This field is required. And you need to provide something between 10 and 30\n            characters. See the\n            <a href='https://json-schema.org/understanding-json-schema/reference/string.html'>\n            JSON schema description (string)</a> in for more details\"\"\",\n            minLength=10,\n            maxLength=20,\n            section=\"JSON Schema validation options\",\n        ),\n        \"checked_number\": Param(\n            100,\n            type=\"number\",\n            title=\"Number field with value check\",\n            description_html=\"\"\"This field is required. You need to provide any number between 64 and 128.\n            See the <a href='https://json-schema.org/understanding-json-schema/reference/numeric.html'>\n            JSON schema description (numbers)</a> in for more details\"\"\",\n            minimum=64,\n            maximum=128,\n            section=\"JSON Schema validation options\",\n        ),\n        # Some further cool stuff as advanced options are also possible\n        # You can have the user entering a dict object as a JSON with validation\n        \"object\": Param(\n            {\"key\": \"value\"},\n            type=[\"object\", \"null\"],\n            title=\"JSON entry field\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n        \"array_of_objects\": Param(\n            [{\"name\": \"account_name\", \"country\": \"country_name\"}],\n            \"Array with complex objects and validation rules. \"\n            \"See <a href='https://json-schema.org/understanding-json-schema\"\n            \"/reference/array.html#items'>JSON Schema validation options in specs.</a>\",\n            type=\"array\",\n            title=\"JSON array field\",\n            items={\n                \"type\": \"object\",\n                \"properties\": {\"name\": {\"type\": \"string\"}, \"country_name\": {\"type\": \"string\"}},\n                \"required\": [\"name\"],\n            },\n            section=\"Special advanced stuff with form fields\",\n        ),\n        # If you want to have static parameters which are always passed and not editable by the user\n        # then you can use the JSON schema option of passing constant values. These parameters\n        # will not be displayed but passed to the DAG\n        \"hidden_secret_field\": Param(\"constant value\", const=\"constant value\"),\n        # Finally besides the standard provided field generator you can have you own HTML form code\n        # injected - but be careful, you can also mess-up the layout!\n        \"color_picker\": Param(\n            \"#FF8800\",\n            type=\"string\",\n            title=\"Pick a color\",\n            description_html=\"\"\"This is a special HTML widget as custom implementation in the DAG code.\n            It is templated with the following parameter to render proper HTML form fields:\n            <ul>\n                <li><code>{name}</code>: Name of the HTML input field that is expected.</li>\n                <li><code>{value}</code>:\n                    (Default) value that should be displayed when showing/loading the form.</li>\n                <li>Note: If you have elements changing a value, call <code>updateJSONconf()</code> to update\n                    the form data to be posted as <code>dag_run.conf</code>.</li>\n            </ul>\n            Example: <code>&lt;input name='{name}' value='{value}' onchange='updateJSONconf()' /&gt;</code>\n            \"\"\",\n            custom_html_form=\"\"\"\n            <table width=\"100%\" cellspacing=\"5\"><tbody><tr><td>\n                <label for=\"r_{name}\">Red:</label>\n            </td><td width=\"80%\">\n                <input id=\"r_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td><td rowspan=\"3\" style=\"padding-left: 10px;\">\n                <div id=\"preview_{name}\"\n                style=\"line-height: 40px; margin-bottom: 7px; width: 100%; background-color: {value};\"\n                >&nbsp;</div>\n                <input class=\"form-control\" type=\"text\" maxlength=\"7\" id=\"{name}\" name=\"{name}\"\n                value=\"{value}\" onchange=\"v_{name}()\" />\n            </td></tr><tr><td>\n                <label for=\"g_{name}\">Green:</label>\n            </td><td>\n                <input id=\"g_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr><tr><td>\n                <label for=\"b_{name}\">Blue:</label>\n            </td><td>\n                <input id=\"b_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr></tbody></table>\n            <script lang=\"javascript\">\n                const hex_chars = \"0123456789ABCDEF\";\n                function i2hex(name) {\n                    var i = document.getElementById(name).value;\n                    return hex_chars.substr(parseInt(i / 16), 1) + hex_chars.substr(parseInt(i % 16), 1)\n                }\n                function u_{name}() {\n                    var hex_val = \"#\"+i2hex(\"r_{name}\")+i2hex(\"g_{name}\")+i2hex(\"b_{name}\");\n                    document.getElementById(\"{name}\").value = hex_val;\n                    document.getElementById(\"preview_{name}\").style.background = hex_val;\n                    updateJSONconf();\n                }\n                function hex2i(text) {\n                    return hex_chars.indexOf(text.substr(0,1)) * 16 + hex_chars.indexOf(text.substr(1,1));\n                }\n                function v_{name}() {\n                    var value = document.getElementById(\"{name}\").value.toUpperCase();\n                    document.getElementById(\"r_{name}\").value = hex2i(value.substr(1,2));\n                    document.getElementById(\"g_{name}\").value = hex2i(value.substr(3,2));\n                    document.getElementById(\"b_{name}\").value = hex2i(value.substr(5,2));\n                    document.getElementById(\"preview_{name}\").style.background = value;\n                }\n                v_{name}();\n            </script>\"\"\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n    },\n) as dag:\n\n    @task(task_id=\"show_params\")\n    def show_params(**kwargs) -> None:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if not dag_run.conf:\n            print(\"Uups, no parameters supplied as DagRun.conf, was the trigger w/o form?\")\n            raise AirflowSkipException(\"No DagRun.conf parameters supplied.\")\n        print(f\"This DAG was triggered with the following parameters:\\n{json.dumps(dag_run.conf, indent=4)}\")\n\n    show_params()", "extracted_at": "2025-11-19T17:21:14.626625", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_sensor_decorator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the sensor decorator.\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.sensors.base import PokeReturnValue\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_sensor_decorator():\n    # [END instantiate_dag]\n\n    # [START wait_function]\n    # Using a sensor operator to wait for the upstream data to be ready.\n    @task.sensor(poke_interval=60, timeout=3600, mode=\"reschedule\")\n    def wait_for_upstream() -> PokeReturnValue:\n        return PokeReturnValue(is_done=True, xcom_value=\"xcom_value\")\n\n    # [END wait_function]\n\n    # [START dummy_function]\n    @task\n    def dummy_operator() -> None:\n        pass\n\n    # [END dummy_function]\n\n    # [START main_flow]\n    wait_for_upstream() >> dummy_operator()\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_etl_dag = example_sensor_decorator()\n# [END dag_invocation]\n\n# [END tutorial]", "extracted_at": "2025-11-19T17:21:14.628922", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 87, "file_name": "example_params_trigger_ui.py"}, "content": "\"\"\"Example DAG demonstrating the usage DAG params to model a trigger UI with a user form.\n\nThis example DAG generates greetings to a list of provided names in selected languages in the logs.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        \"names\": Param(\n            [\"Linda\", \"Martha\", \"Thomas\"],\n            type=\"array\",\n            description=\"Define the list of names for which greetings should be generated in the logs.\"\n            \" Please have one name per line.\",\n            title=\"Names to greet\",\n        ),\n        \"english\": Param(True, type=\"boolean\", title=\"English\"),\n        \"german\": Param(True, type=\"boolean\", title=\"German (Formal)\"),\n        \"french\": Param(True, type=\"boolean\", title=\"French\"),\n    },\n) as dag:\n\n    @task(task_id=\"get_names\")\n    def get_names(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if \"names\" not in dag_run.conf:\n            print(\"Uuups, no names given, was no UI used to trigger?\")\n            return []\n        return dag_run.conf[\"names\"]\n\n    @task.branch(task_id=\"select_languages\")\n    def select_languages(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        selected_languages = []\n        for lang in [\"english\", \"german\", \"french\"]:\n            if lang in dag_run.conf and dag_run.conf[lang]:\n                selected_languages.append(f\"generate_{lang}_greeting\")\n        return selected_languages\n\n    @task(task_id=\"generate_english_greeting\")\n    def generate_english_greeting(name: str) -> str:\n        return f\"Hello {name}!\"\n\n    @task(task_id=\"generate_german_greeting\")\n    def generate_german_greeting(name: str) -> str:\n        return f\"Sehr geehrter Herr/Frau {name}.\"\n\n    @task(task_id=\"generate_french_greeting\")\n    def generate_french_greeting(name: str) -> str:\n        return f\"Bonjour {name}!\"\n\n    @task(task_id=\"print_greetings\", trigger_rule=TriggerRule.ALL_DONE)\n    def print_greetings(greetings1, greetings2, greetings3) -> None:\n        for g in greetings1 if greetings1 else []:\n            print(g)\n        for g in greetings2 if greetings2 else []:\n            print(g)\n        for g in greetings3 if greetings3 else []:\n            print(g)\n        if not greetings1 and not greetings2 and not greetings3:\n            print(\"sad, nobody to greet :-(\")\n\n    lang_select = select_languages()\n    names = get_names()\n    english_greetings = generate_english_greeting.expand(name=names)\n    german_greetings = generate_german_greeting.expand(name=names)\n    french_greetings = generate_french_greeting.expand(name=names)\n    lang_select >> [english_greetings, german_greetings, french_greetings]\n    results_print = print_greetings(english_greetings, german_greetings, french_greetings)", "extracted_at": "2025-11-19T17:21:14.631625", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 109, "file_name": "example_sensors.py"}, "content": "from __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.python import PythonSensor\nfrom airflow.sensors.time_delta import TimeDeltaSensor, TimeDeltaSensorAsync\nfrom airflow.sensors.time_sensor import TimeSensor, TimeSensorAsync\nfrom airflow.sensors.weekday import DayOfWeekSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n\n# [START example_callables]\ndef success_callable():\n    return True\n\n\ndef failure_callable():\n    return False\n\n\n# [END example_callables]\n\n\nwith DAG(\n    dag_id=\"example_sensors\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START example_time_delta_sensor]\n    t0 = TimeDeltaSensor(task_id=\"wait_some_seconds\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor]\n\n    # [START example_time_delta_sensor_async]\n    t0a = TimeDeltaSensorAsync(task_id=\"wait_some_seconds_async\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor_async]\n\n    # [START example_time_sensors]\n    t1 = TimeSensor(\n        task_id=\"fire_immediately\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2 = TimeSensor(\n        task_id=\"timeout_after_second_date_in_the_future\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors]\n\n    # [START example_time_sensors_async]\n    t1a = TimeSensorAsync(\n        task_id=\"fire_immediately_async\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2a = TimeSensorAsync(\n        task_id=\"timeout_after_second_date_in_the_future_async\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors_async]\n\n    # [START example_bash_sensors]\n    t3 = BashSensor(task_id=\"Sensor_succeeds\", bash_command=\"exit 0\")\n\n    t4 = BashSensor(task_id=\"Sensor_fails_after_3_seconds\", timeout=3, soft_fail=True, bash_command=\"exit 1\")\n    # [END example_bash_sensors]\n\n    t5 = BashOperator(task_id=\"remove_file\", bash_command=\"rm -rf /tmp/temporary_file_for_testing\")\n\n    # [START example_file_sensor]\n    t6 = FileSensor(task_id=\"wait_for_file\", filepath=\"/tmp/temporary_file_for_testing\")\n    # [END example_file_sensor]\n\n    t7 = BashOperator(\n        task_id=\"create_file_after_3_seconds\", bash_command=\"sleep 3; touch /tmp/temporary_file_for_testing\"\n    )\n\n    # [START example_python_sensors]\n    t8 = PythonSensor(task_id=\"success_sensor_python\", python_callable=success_callable)\n\n    t9 = PythonSensor(\n        task_id=\"failure_timeout_sensor_python\", timeout=3, soft_fail=True, python_callable=failure_callable\n    )\n    # [END example_python_sensors]\n\n    # [START example_day_of_week_sensor]\n    t10 = DayOfWeekSensor(\n        task_id=\"week_day_sensor_failing_on_timeout\", timeout=3, soft_fail=True, week_day=WeekDay.MONDAY\n    )\n    # [END example_day_of_week_sensor]\n\n    tx = BashOperator(task_id=\"print_date_in_bash\", bash_command=\"date\")\n\n    tx.trigger_rule = TriggerRule.NONE_FAILED\n    [t0, t0a, t1, t1a, t2, t2a, t3, t4] >> tx\n    t5 >> t6 >> tx\n    t7 >> tx\n    [t8, t9] >> tx\n    t10 >> tx", "extracted_at": "2025-11-19T17:21:14.634126", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 88, "file_name": "example_setup_teardown_taskflow.py"}, "content": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import setup, task, task_group, teardown\nfrom airflow.models.dag import DAG\n\nwith DAG(\n    dag_id=\"example_setup_teardown_taskflow\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def my_first_task():\n        print(\"Hello 1\")\n\n    @task\n    def my_second_task():\n        print(\"Hello 2\")\n\n    @task\n    def my_third_task():\n        print(\"Hello 3\")\n\n    # you can set setup / teardown relationships with the `as_teardown` method.\n    task_1 = my_first_task()\n    task_2 = my_second_task()\n    task_3 = my_third_task()\n    task_1 >> task_2 >> task_3.as_teardown(setups=task_1)\n\n    # The method `as_teardown` will mark task_3 as teardown, task_1 as setup, and\n    # arrow task_1 >> task_3.\n    # Now if you clear task_2, then its setup task, task_1, will be cleared in\n    # addition to its teardown task, task_3\n\n    # it's also possible to use a decorator to mark a task as setup or\n    # teardown when you define it. see below.\n\n    @setup\n    def outer_setup():\n        print(\"I am outer_setup\")\n        return \"some cluster id\"\n\n    @teardown\n    def outer_teardown(cluster_id):\n        print(\"I am outer_teardown\")\n        print(f\"Tearing down cluster: {cluster_id}\")\n\n    @task\n    def outer_work():\n        print(\"I am just a normal task\")\n\n    @task_group\n    def section_1():\n        @setup\n        def inner_setup():\n            print(\"I set up\")\n            return \"some_cluster_id\"\n\n        @task\n        def inner_work(cluster_id):\n            print(f\"doing some work with {cluster_id=}\")\n\n        @teardown\n        def inner_teardown(cluster_id):\n            print(f\"tearing down {cluster_id=}\")\n\n        # this passes the return value of `inner_setup` to both `inner_work` and `inner_teardown`\n        inner_setup_task = inner_setup()\n        inner_work(inner_setup_task) >> inner_teardown(inner_setup_task)\n\n    # by using the decorators, outer_setup and outer_teardown are already marked as setup / teardown\n    # now we just need to make sure they are linked directly.  At a low level, what we need\n    # to do so is the following::\n    #     s = outer_setup()\n    #     t = outer_teardown()\n    #     s >> t\n    #     s >> outer_work() >> t\n    # Thus, s and t are linked directly, and outer_work runs in between.  We can take advantage of\n    # the fact that we are in taskflow, along with the context manager on teardowns, as follows:\n    with outer_teardown(outer_setup()):\n        outer_work()\n\n        # and let's put section 1 inside the outer setup and teardown tasks\n        section_1()", "extracted_at": "2025-11-19T17:21:14.659442", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 31, "file_name": "example_setup_teardown.py"}, "content": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"example_setup_teardown\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    root_setup = BashOperator(task_id=\"root_setup\", bash_command=\"echo 'Hello from root_setup'\").as_setup()\n    root_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n    root_teardown = BashOperator(\n        task_id=\"root_teardown\", bash_command=\"echo 'Goodbye from root_teardown'\"\n    ).as_teardown(setups=root_setup)\n    root_setup >> root_normal >> root_teardown\n    with TaskGroup(\"section_1\") as section_1:\n        inner_setup = BashOperator(\n            task_id=\"taskgroup_setup\", bash_command=\"echo 'Hello from taskgroup_setup'\"\n        ).as_setup()\n        inner_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n        inner_teardown = BashOperator(\n            task_id=\"taskgroup_teardown\", bash_command=\"echo 'Hello from taskgroup_teardown'\"\n        ).as_teardown(setups=inner_setup)\n        inner_setup >> inner_normal >> inner_teardown\n    root_normal >> section_1", "extracted_at": "2025-11-19T17:21:14.661668", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 44, "file_name": "example_short_circuit_decorator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the `@task.short_circuit()` TaskFlow decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@dag(start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"), catchup=False, tags=[\"example\"])\ndef example_short_circuit_decorator():\n    # [START howto_operator_short_circuit]\n    @task.short_circuit()\n    def check_condition(condition):\n        return condition\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    condition_is_true = check_condition.override(task_id=\"condition_is_true\")(condition=True)\n    condition_is_false = check_condition.override(task_id=\"condition_is_false\")(condition=False)\n\n    chain(condition_is_true, *ds_true)\n    chain(condition_is_false, *ds_false)\n    # [END howto_operator_short_circuit]\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = check_condition.override(task_id=\"short_circuit\", ignore_downstream_trigger_rules=False)(\n        condition=False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)\n    # [END howto_operator_short_circuit_trigger_rules]\n\n\nexample_dag = example_short_circuit_decorator()", "extracted_at": "2025-11-19T17:21:14.682025", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_skip_dag.py"}, "content": "\"\"\"Example DAG demonstrating the EmptyOperator and a custom EmptySkipOperator which skips by default.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# Create some placeholder operators\nclass EmptySkipOperator(BaseOperator):\n    \"\"\"Empty operator which always skips the task.\"\"\"\n\n    ui_color = \"#e8b7e4\"\n\n    def execute(self, context: Context):\n        raise AirflowSkipException\n\n\ndef create_test_pipeline(suffix, trigger_rule):\n    \"\"\"\n    Instantiate a number of operators for the given DAG.\n\n    :param str suffix: Suffix to append to the operator task_ids\n    :param str trigger_rule: TriggerRule for the join task\n    :param DAG dag_: The DAG to run the operators on\n    \"\"\"\n    skip_operator = EmptySkipOperator(task_id=f\"skip_operator_{suffix}\")\n    always_true = EmptyOperator(task_id=f\"always_true_{suffix}\")\n    join = EmptyOperator(task_id=trigger_rule, trigger_rule=trigger_rule)\n    final = EmptyOperator(task_id=f\"final_{suffix}\")\n\n    skip_operator >> join\n    always_true >> join\n    join >> final\n\n\nwith DAG(\n    dag_id=\"example_skip_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_test_pipeline(\"1\", TriggerRule.ALL_SUCCESS)\n    create_test_pipeline(\"2\", TriggerRule.ONE_SUCCESS)", "extracted_at": "2025-11-19T17:21:14.685421", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 44, "file_name": "example_short_circuit_operator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the ShortCircuitOperator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import ShortCircuitOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_short_circuit_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    cond_true = ShortCircuitOperator(\n        task_id=\"condition_is_True\",\n        python_callable=lambda: True,\n    )\n\n    cond_false = ShortCircuitOperator(\n        task_id=\"condition_is_False\",\n        python_callable=lambda: False,\n    )\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    chain(cond_true, *ds_true)\n    chain(cond_false, *ds_false)\n\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = ShortCircuitOperator(\n        task_id=\"short_circuit\", ignore_downstream_trigger_rules=False, python_callable=lambda: False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)", "extracted_at": "2025-11-19T17:21:14.690497", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 47, "file_name": "example_task_group.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\nwith DAG(\n    dag_id=\"example_task_group\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start = EmptyOperator(task_id=\"start\")\n\n    # [START howto_task_group_section_1]\n    with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n        task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n        task_3 = EmptyOperator(task_id=\"task_3\")\n\n        task_1 >> [task_2, task_3]\n    # [END howto_task_group_section_1]\n\n    # [START howto_task_group_section_2]\n    with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n\n        # [START howto_task_group_inner_section_2]\n        with TaskGroup(\"inner_section_2\", tooltip=\"Tasks for inner_section2\") as inner_section_2:\n            task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            [task_2, task_3] >> task_4\n        # [END howto_task_group_inner_section_2]\n\n    # [END howto_task_group_section_2]\n\n    end = EmptyOperator(task_id=\"end\")\n\n    start >> section_1 >> section_2 >> end\n# [END howto_task_group]", "extracted_at": "2025-11-19T17:21:14.695136", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 45, "file_name": "example_subdag_operator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the SubDagOperator.\"\"\"\nfrom __future__ import annotations\n\n# [START example_subdag_operator]\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.example_dags.subdags.subdag import subdag\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.subdag import SubDagOperator\n\nDAG_NAME = \"example_subdag_operator\"\n\nwith DAG(\n    dag_id=DAG_NAME,\n    default_args={\"retries\": 2},\n    start_date=datetime.datetime(2022, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n\n    start = EmptyOperator(\n        task_id=\"start\",\n    )\n\n    section_1 = SubDagOperator(\n        task_id=\"section-1\",\n        subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),\n    )\n\n    some_other_task = EmptyOperator(\n        task_id=\"some-other-task\",\n    )\n\n    section_2 = SubDagOperator(\n        task_id=\"section-2\",\n        subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),\n    )\n\n    end = EmptyOperator(\n        task_id=\"end\",\n    )\n\n    start >> section_1 >> some_other_task >> section_2 >> end\n# [END example_subdag_operator]", "extracted_at": "2025-11-19T17:21:14.696980", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_sla_dag.py"}, "content": "\"\"\"Example DAG demonstrating SLA use in Tasks\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport time\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n\n# [START howto_task_sla]\ndef sla_callback(dag, task_list, blocking_task_list, slas, blocking_tis):\n    print(\n        \"The callback arguments are: \",\n        {\n            \"dag\": dag,\n            \"task_list\": task_list,\n            \"blocking_task_list\": blocking_task_list,\n            \"slas\": slas,\n            \"blocking_tis\": blocking_tis,\n        },\n    )\n\n\n@dag(\n    schedule=\"*/2 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    sla_miss_callback=sla_callback,\n    default_args={\"email\": \"email@example.com\"},\n)\ndef example_sla_dag():\n    @task(sla=datetime.timedelta(seconds=10))\n    def sleep_20():\n        \"\"\"Sleep for 20 seconds\"\"\"\n        time.sleep(20)\n\n    @task\n    def sleep_30():\n        \"\"\"Sleep for 30 seconds\"\"\"\n        time.sleep(30)\n\n    sleep_20() >> sleep_30()\n\n\nexample_dag = example_sla_dag()\n\n# [END howto_task_sla]", "extracted_at": "2025-11-19T17:21:14.713408", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_time_delta_sensor_async.py"}, "content": "\"\"\"\nExample DAG demonstrating ``TimeDeltaSensorAsync``, a drop in replacement for ``TimeDeltaSensor`` that\ndefers and doesn't occupy a worker slot while it waits\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.time_delta import TimeDeltaSensorAsync\n\nwith DAG(\n    dag_id=\"example_time_delta_sensor_async\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    wait = TimeDeltaSensorAsync(task_id=\"wait\", delta=datetime.timedelta(seconds=30))\n    finish = EmptyOperator(task_id=\"finish\")\n    wait >> finish", "extracted_at": "2025-11-19T17:21:14.718318", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 62, "file_name": "example_task_group_decorator.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the @taskgroup decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task, task_group\nfrom airflow.models.dag import DAG\n\n\n# [START howto_task_group_decorator]\n# Creating Tasks\n@task\ndef task_start():\n    \"\"\"Empty Task which is First Task of Dag\"\"\"\n    return \"[Task_start]\"\n\n\n@task\ndef task_1(value: int) -> str:\n    \"\"\"Empty Task1\"\"\"\n    return f\"[ Task1 {value} ]\"\n\n\n@task\ndef task_2(value: str) -> str:\n    \"\"\"Empty Task2\"\"\"\n    return f\"[ Task2 {value} ]\"\n\n\n@task\ndef task_3(value: str) -> None:\n    \"\"\"Empty Task3\"\"\"\n    print(f\"[ Task3 {value} ]\")\n\n\n@task\ndef task_end() -> None:\n    \"\"\"Empty Task which is Last Task of Dag\"\"\"\n    print(\"[ Task_End  ]\")\n\n\n# Creating TaskGroups\n@task_group\ndef task_group_function(value: int) -> None:\n    \"\"\"TaskGroup for grouping related Tasks\"\"\"\n    task_3(task_2(task_1(value)))\n\n\n# Executing Tasks and TaskGroups\nwith DAG(\n    dag_id=\"example_task_group_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start_task = task_start()\n    end_task = task_end()\n    for i in range(5):\n        current_task_group = task_group_function(i)\n        start_task >> current_task_group >> end_task\n\n# [END howto_task_group_decorator]", "extracted_at": "2025-11-19T17:21:14.718719", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_trigger_controller_dag.py"}, "content": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(\n    dag_id=\"example_trigger_controller_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n    trigger = TriggerDagRunOperator(\n        task_id=\"test_trigger_dagrun\",\n        trigger_dag_id=\"example_trigger_target_dag\",  # Ensure this equals the dag_id of the DAG to trigger\n        conf={\"message\": \"Hello World\"},\n    )", "extracted_at": "2025-11-19T17:21:14.736833", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 76, "file_name": "example_xcom.py"}, "content": "\"\"\"Example DAG demonstrating the usage of XComs.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, XComArg\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nvalue_1 = [1, 2, 3]\nvalue_2 = {\"a\": \"b\"}\n\n\n@task\ndef push(ti=None):\n    \"\"\"Pushes an XCom without a specific target\"\"\"\n    ti.xcom_push(key=\"value from pusher 1\", value=value_1)\n\n\n@task\ndef push_by_returning():\n    \"\"\"Pushes an XCom without a specific target, just by returning it\"\"\"\n    return value_2\n\n\ndef _compare_values(pulled_value, check_value):\n    if pulled_value != check_value:\n        raise ValueError(f\"The two values differ {pulled_value} and {check_value}\")\n\n\n@task\ndef puller(pulled_value_2, ti=None):\n    \"\"\"Pull all previously pushed XComs and check if the pushed values match the pulled values.\"\"\"\n    pulled_value_1 = ti.xcom_pull(task_ids=\"push\", key=\"value from pusher 1\")\n\n    _compare_values(pulled_value_1, value_1)\n    _compare_values(pulled_value_2, value_2)\n\n\n@task\ndef pull_value_from_bash_push(ti=None):\n    bash_pushed_via_return_value = ti.xcom_pull(key=\"return_value\", task_ids=\"bash_push\")\n    bash_manually_pushed_value = ti.xcom_pull(key=\"manually_pushed_value\", task_ids=\"bash_push\")\n    print(f\"The xcom value pushed by task push via return value is {bash_pushed_via_return_value}\")\n    print(f\"The xcom value pushed by task push manually is {bash_manually_pushed_value}\")\n\n\nwith DAG(\n    \"example_xcom\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    bash_push = BashOperator(\n        task_id=\"bash_push\",\n        bash_command='echo \"bash_push demo\"  && '\n        'echo \"Manually set xcom value '\n        '{{ ti.xcom_push(key=\"manually_pushed_value\", value=\"manually_pushed_value\") }}\" && '\n        'echo \"value_by_return\"',\n    )\n\n    bash_pull = BashOperator(\n        task_id=\"bash_pull\",\n        bash_command='echo \"bash pull demo\" && '\n        f'echo \"The xcom pushed manually is {XComArg(bash_push, key=\"manually_pushed_value\")}\" && '\n        f'echo \"The returned_value xcom is {XComArg(bash_push)}\" && '\n        'echo \"finished\"',\n        do_xcom_push=False,\n    )\n\n    python_pull_from_bash = pull_value_from_bash_push()\n\n    [bash_pull, python_pull_from_bash] << bash_push\n\n    puller(push_by_returning()) << push()", "extracted_at": "2025-11-19T17:21:14.742292", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 38, "file_name": "example_trigger_target_dag.py"}, "content": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef run_this_func(dag_run=None):\n    \"\"\"\n    Print the payload \"message\" passed to the DagRun conf attribute.\n\n    :param dag_run: The DagRun object\n    \"\"\"\n    print(f\"Remotely received value of {dag_run.conf.get('message')} for key=message\")\n\n\nwith DAG(\n    dag_id=\"example_trigger_target_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_this = run_this_func()\n\n    bash_task = BashOperator(\n        task_id=\"bash_task\",\n        bash_command='echo \"Here is the message: $message\"',\n        env={\"message\": '{{ dag_run.conf.get(\"message\") }}'},\n    )", "extracted_at": "2025-11-19T17:21:14.747803", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 48, "file_name": "example_xcomargs.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the XComArgs.\"\"\"\nfrom __future__ import annotations\n\nimport logging\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef generate_value():\n    \"\"\"Empty function\"\"\"\n    return \"Bring me a shrubbery!\"\n\n\n@task\ndef print_value(value, ts=None):\n    \"\"\"Empty function\"\"\"\n    log.info(\"The knights of Ni say: %s (at %s)\", value, ts)\n\n\nwith DAG(\n    dag_id=\"example_xcom_args\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    print_value(generate_value())\n\nwith DAG(\n    \"example_xcom_args_with_operators\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag2:\n    bash_op1 = BashOperator(task_id=\"c\", bash_command=\"echo c\")\n    bash_op2 = BashOperator(task_id=\"d\", bash_command=\"echo c\")\n    xcom_args_a = print_value(\"first!\")\n    xcom_args_b = print_value(\"second!\")\n\n    bash_op1 >> xcom_args_a >> xcom_args_b >> bash_op2", "extracted_at": "2025-11-19T17:21:14.759962", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 61, "file_name": "example_arangodb.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.arangodb.operators.arangodb import AQLOperator\nfrom airflow.providers.arangodb.sensors.arangodb import AQLSensor\n\ndag = DAG(\n    \"example_arangodb_operator\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n)\n\n# [START howto_aql_sensor_arangodb]\n\nsensor = AQLSensor(\n    task_id=\"aql_sensor\",\n    query=\"FOR doc IN students FILTER doc.name == 'judy' RETURN doc\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_arangodb]\n\n# [START howto_aql_sensor_template_file_arangodb]\n\nsensor2 = AQLSensor(\n    task_id=\"aql_sensor_template_file\",\n    query=\"search_judy.sql\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_template_file_arangodb]\n\n\n# [START howto_aql_operator_arangodb]\n\noperator = AQLOperator(\n    task_id=\"aql_operator\",\n    query=\"FOR doc IN students RETURN doc\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n)\n\n# [END howto_aql_operator_arangodb]\n\n# [START howto_aql_operator_template_file_arangodb]\n\noperator2 = AQLOperator(\n    task_id=\"aql_operator_template_file\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n    query=\"search_all.sql\",\n)\n\n# [END howto_aql_operator_template_file_arangodb]", "extracted_at": "2025-11-19T17:21:14.762540", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 36, "file_name": "example_cloud_task.py"}, "content": "\"\"\"\nExample Airflow DAG that sense a cloud task queue being empty.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project where the Compute Engine instance exists.\n* GCP_ZONE - Google Cloud zone where the cloud task queue exists.\n* QUEUE_NAME - Name of the cloud task queue.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.sensors.tasks import TaskQueueEmptySensor\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_ZONE = os.environ.get(\"GCE_ZONE\", \"europe-west1-b\")\nQUEUE_NAME = os.environ.get(\"GCP_QUEUE_NAME\", \"testqueue\")\n\n\nwith models.DAG(\n    \"example_gcp_cloud_tasks_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cloud_tasks_empty_sensor]\n    gcp_cloud_tasks_sensor = TaskQueueEmptySensor(\n        project_id=GCP_PROJECT_ID,\n        location=GCP_ZONE,\n        task_id=\"gcp_sense_cloud_tasks_empty\",\n        queue_name=QUEUE_NAME,\n    )\n    # [END cloud_tasks_empty_sensor]", "extracted_at": "2025-11-19T17:21:14.770272", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 271, "file_name": "example_cloud_sql_query.py"}, "content": "\"\"\"\nExample Airflow DAG that performs query in a Cloud SQL instance.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project for the Cloud SQL instance\n* GCP_REGION - Google Cloud region where the database is created\n*\n* GCSQL_POSTGRES_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_POSTGRES_USER - Name of the postgres database user\n* GCSQL_POSTGRES_PASSWORD - Password of the postgres database user\n* GCSQL_POSTGRES_PUBLIC_IP - Public IP of the Postgres database\n* GCSQL_POSTGRES_PUBLIC_PORT - Port of the postgres database\n*\n* GCSQL_MYSQL_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_MYSQL_USER - Name of the mysql database user\n* GCSQL_MYSQL_PASSWORD - Password of the mysql database user\n* GCSQL_MYSQL_PUBLIC_IP - Public IP of the mysql database\n* GCSQL_MYSQL_PUBLIC_PORT - Port of the mysql database\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom urllib.parse import quote_plus\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.cloud_sql import CloudSQLExecuteQueryOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_REGION = os.environ.get(\"GCP_REGION\", \"europe-west1\")\n\nGCSQL_POSTGRES_INSTANCE_NAME_QUERY = os.environ.get(\n    \"GCSQL_POSTGRES_INSTANCE_NAME_QUERY\", \"test-postgres-query\"\n)\nGCSQL_POSTGRES_DATABASE_NAME = os.environ.get(\"GCSQL_POSTGRES_DATABASE_NAME\", \"postgresdb\")\nGCSQL_POSTGRES_USER = os.environ.get(\"GCSQL_POSTGRES_USER\", \"postgres_user\")\nGCSQL_POSTGRES_PASSWORD = os.environ.get(\"GCSQL_POSTGRES_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_POSTGRES_PUBLIC_IP = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_POSTGRES_PUBLIC_PORT = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_PORT\", 5432)\nGCSQL_POSTGRES_CLIENT_CERT_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_CERT_FILE\", \".key/postgres-client-cert.pem\"\n)\nGCSQL_POSTGRES_CLIENT_KEY_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_KEY_FILE\", \".key/postgres-client-key.pem\"\n)\nGCSQL_POSTGRES_SERVER_CA_FILE = os.environ.get(\"GCSQL_POSTGRES_SERVER_CA_FILE\", \".key/postgres-server-ca.pem\")\n\nGCSQL_MYSQL_INSTANCE_NAME_QUERY = os.environ.get(\"GCSQL_MYSQL_INSTANCE_NAME_QUERY\", \"test-mysql-query\")\nGCSQL_MYSQL_DATABASE_NAME = os.environ.get(\"GCSQL_MYSQL_DATABASE_NAME\", \"mysqldb\")\nGCSQL_MYSQL_USER = os.environ.get(\"GCSQL_MYSQL_USER\", \"mysql_user\")\nGCSQL_MYSQL_PASSWORD = os.environ.get(\"GCSQL_MYSQL_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_MYSQL_PUBLIC_IP = os.environ.get(\"GCSQL_MYSQL_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_MYSQL_PUBLIC_PORT = os.environ.get(\"GCSQL_MYSQL_PUBLIC_PORT\", 3306)\nGCSQL_MYSQL_CLIENT_CERT_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_CERT_FILE\", \".key/mysql-client-cert.pem\")\nGCSQL_MYSQL_CLIENT_KEY_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_KEY_FILE\", \".key/mysql-client-key.pem\")\nGCSQL_MYSQL_SERVER_CA_FILE = os.environ.get(\"GCSQL_MYSQL_SERVER_CA_FILE\", \".key/mysql-server-ca.pem\")\n\nSQL = [\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",  # shows warnings logged\n    \"INSERT INTO TABLE_TEST VALUES (0)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST2 (I INTEGER)\",\n    \"DROP TABLE TABLE_TEST\",\n    \"DROP TABLE TABLE_TEST2\",\n]\n\n\n# [START howto_operator_cloudsql_query_connections]\n\nHOME_DIR = Path.home()\n\n\ndef get_absolute_path(path):\n    \"\"\"\n    Returns absolute path.\n    \"\"\"\n    return os.fspath(HOME_DIR / path)\n\n\npostgres_kwargs = dict(\n    user=quote_plus(GCSQL_POSTGRES_USER),\n    password=quote_plus(GCSQL_POSTGRES_PASSWORD),\n    public_port=GCSQL_POSTGRES_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_POSTGRES_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_POSTGRES_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_POSTGRES_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_SERVER_CA_FILE)),\n)\n\n# The connections below are created using one of the standard approaches - via environment\n# variables named AIRFLOW_CONN_* . The connections can also be created in the database\n# of AIRFLOW (using command line or UI).\n\n# Postgres: connect via proxy over TCP\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=True\".format(**postgres_kwargs)\n)\n\n# Postgres: connect via proxy over UNIX socket (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**postgres_kwargs)\n)\n\nmysql_kwargs = dict(\n    user=quote_plus(GCSQL_MYSQL_USER),\n    password=quote_plus(GCSQL_MYSQL_PASSWORD),\n    public_port=GCSQL_MYSQL_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_MYSQL_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_MYSQL_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_MYSQL_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_MYSQL_SERVER_CA_FILE)),\n)\n\n# MySQL: connect via proxy over TCP (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=True\".format(**mysql_kwargs)\n)\n\n# MySQL: connect via proxy over UNIX socket using pre-downloaded Cloud Sql Proxy binary\ntry:\n    sql_proxy_binary_path = subprocess.check_output([\"which\", \"cloud_sql_proxy\"]).decode(\"utf-8\").rstrip()\nexcept subprocess.CalledProcessError:\n    sql_proxy_binary_path = \"/tmp/anyhow_download_cloud_sql_proxy\"\n\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql Proxy binary path\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n# Special case: MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql\n# Proxy binary path AND with missing project_id\n\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL_NO_PROJECT_ID\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n\n# [END howto_operator_cloudsql_query_connections]\n\n# [START howto_operator_cloudsql_query_operators]\n\nconnection_names = [\n    \"proxy_postgres_tcp\",\n    \"proxy_postgres_socket\",\n    \"public_postgres_tcp\",\n    \"public_postgres_tcp_ssl\",\n    \"proxy_mysql_tcp\",\n    \"proxy_mysql_socket\",\n    \"public_mysql_tcp\",\n    \"public_mysql_tcp_ssl\",\n    \"public_mysql_tcp_ssl_no_project_id\",\n]\n\ntasks = []\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_sql_query\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    prev_task = None\n\n    for connection_name in connection_names:\n        task = CloudSQLExecuteQueryOperator(\n            gcp_cloudsql_conn_id=connection_name,\n            task_id=\"example_gcp_sql_task_\" + connection_name,\n            sql=SQL,\n            sql_proxy_binary_path=sql_proxy_binary_path,\n        )\n        tasks.append(task)\n        if prev_task:\n            prev_task >> task\n        prev_task = task\n\n# [END howto_operator_cloudsql_query_operators]", "extracted_at": "2025-11-19T17:21:14.781275", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 177, "file_name": "example_cloud_storage_transfer_service_aws.py"}, "content": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Cloud Transfer. This DAG relies on\nthe following OS environment variables\n\nNote that you need to provide a large enough set of data so that operations do not execute too quickly.\nOtherwise, DAG will fail.\n\n* GCP_PROJECT_ID - Google Cloud Project to use for the Google Cloud Transfer Service.\n* GCP_DESCRIPTION - Description of transfer job\n* GCP_TRANSFER_SOURCE_AWS_BUCKET - Amazon Web Services Storage bucket from which files are copied.\n* GCP_TRANSFER_SECOND_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied\n* WAIT_FOR_OPERATION_POKE_INTERVAL - interval of what to check the status of the operation\n  A smaller value than the default value accelerates the system test and ensures its correct execution with\n  smaller quantities of files in the source bucket\n  Look at documentation of :class:`~airflow.operators.sensors.BaseSensorOperator` for more information\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    ALREADY_EXISTING_IN_SINK,\n    AWS_S3_DATA_SOURCE,\n    BUCKET_NAME,\n    DESCRIPTION,\n    FILTER_JOB_NAMES,\n    FILTER_PROJECT_ID,\n    GCS_DATA_SINK,\n    JOB_NAME,\n    PROJECT_ID,\n    SCHEDULE,\n    SCHEDULE_END_DATE,\n    SCHEDULE_START_DATE,\n    START_TIME_OF_DAY,\n    STATUS,\n    TRANSFER_OPTIONS,\n    TRANSFER_SPEC,\n    GcpTransferJobsStatus,\n    GcpTransferOperationStatus,\n)\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceCancelOperationOperator,\n    CloudDataTransferServiceCreateJobOperator,\n    CloudDataTransferServiceDeleteJobOperator,\n    CloudDataTransferServiceGetOperationOperator,\n    CloudDataTransferServiceListOperationsOperator,\n    CloudDataTransferServicePauseOperationOperator,\n    CloudDataTransferServiceResumeOperationOperator,\n)\nfrom airflow.providers.google.cloud.sensors.cloud_storage_transfer_service import (\n    CloudDataTransferServiceJobStatusSensor,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_DESCRIPTION = os.environ.get(\"GCP_DESCRIPTION\", \"description\")\nGCP_TRANSFER_TARGET_BUCKET = os.environ.get(\"GCP_TRANSFER_TARGET_BUCKET\")\nWAIT_FOR_OPERATION_POKE_INTERVAL = int(os.environ.get(\"WAIT_FOR_OPERATION_POKE_INTERVAL\", 5))\n\nGCP_TRANSFER_SOURCE_AWS_BUCKET = os.environ.get(\"GCP_TRANSFER_SOURCE_AWS_BUCKET\")\nGCP_TRANSFER_FIRST_TARGET_BUCKET = os.environ.get(\n    \"GCP_TRANSFER_FIRST_TARGET_BUCKET\", \"gcp-transfer-first-target\"\n)\n\nGCP_TRANSFER_JOB_NAME = os.environ.get(\"GCP_TRANSFER_JOB_NAME\", \"transferJobs/sampleJob\")\n\n# [START howto_operator_gcp_transfer_create_job_body_aws]\naws_to_gcs_transfer_body = {\n    DESCRIPTION: GCP_DESCRIPTION,\n    STATUS: GcpTransferJobsStatus.ENABLED,\n    PROJECT_ID: GCP_PROJECT_ID,\n    JOB_NAME: GCP_TRANSFER_JOB_NAME,\n    SCHEDULE: {\n        SCHEDULE_START_DATE: datetime(2015, 1, 1).date(),\n        SCHEDULE_END_DATE: datetime(2030, 1, 1).date(),\n        START_TIME_OF_DAY: (datetime.utcnow() + timedelta(minutes=2)).time(),\n    },\n    TRANSFER_SPEC: {\n        AWS_S3_DATA_SOURCE: {BUCKET_NAME: GCP_TRANSFER_SOURCE_AWS_BUCKET},\n        GCS_DATA_SINK: {BUCKET_NAME: GCP_TRANSFER_FIRST_TARGET_BUCKET},\n        TRANSFER_OPTIONS: {ALREADY_EXISTING_IN_SINK: True},\n    },\n}\n# [END howto_operator_gcp_transfer_create_job_body_aws]\n\n\nwith models.DAG(\n    \"example_gcp_transfer_aws\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_gcp_transfer_create_job]\n    create_transfer_job_from_aws = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_transfer_job_from_aws\", body=aws_to_gcs_transfer_body\n    )\n    # [END howto_operator_gcp_transfer_create_job]\n\n    wait_for_operation_to_start = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_start\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.IN_PROGRESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n\n    # [START howto_operator_gcp_transfer_pause_operation]\n    pause_operation = CloudDataTransferServicePauseOperationOperator(\n        task_id=\"pause_operation\",\n        operation_name=\"{{task_instance.xcom_pull('wait_for_operation_to_start', \"\n        \"key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_pause_operation]\n\n    # [START howto_operator_gcp_transfer_list_operations]\n    list_operations = CloudDataTransferServiceListOperationsOperator(\n        task_id=\"list_operations\",\n        request_filter={\n            FILTER_PROJECT_ID: GCP_PROJECT_ID,\n            FILTER_JOB_NAMES: [\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\"],\n        },\n    )\n    # [END howto_operator_gcp_transfer_list_operations]\n\n    # [START howto_operator_gcp_transfer_get_operation]\n    get_operation = CloudDataTransferServiceGetOperationOperator(\n        task_id=\"get_operation\", operation_name=\"{{task_instance.xcom_pull('list_operations')[0]['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_get_operation]\n\n    # [START howto_operator_gcp_transfer_resume_operation]\n    resume_operation = CloudDataTransferServiceResumeOperationOperator(\n        task_id=\"resume_operation\", operation_name=\"{{task_instance.xcom_pull('get_operation')['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_resume_operation]\n\n    # [START howto_operator_gcp_transfer_wait_operation]\n    wait_for_operation_to_end = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_end\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.SUCCESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n    # [END howto_operator_gcp_transfer_wait_operation]\n\n    # [START howto_operator_gcp_transfer_cancel_operation]\n    cancel_operation = CloudDataTransferServiceCancelOperationOperator(\n        task_id=\"cancel_operation\",\n        operation_name=\"{{task_instance.xcom_pull(\"\n        \"'wait_for_second_operation_to_start', key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_cancel_operation]\n\n    # [START howto_operator_gcp_transfer_delete_job]\n    delete_transfer_from_aws_job = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_transfer_from_aws_job\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_gcp_transfer_delete_job]\n\n    chain(\n        create_transfer_job_from_aws,\n        wait_for_operation_to_start,\n        pause_operation,\n        list_operations,\n        get_operation,\n        resume_operation,\n        wait_for_operation_to_end,\n        cancel_operation,\n        delete_transfer_from_aws_job,\n    )", "extracted_at": "2025-11-19T17:21:14.784400", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 52, "file_name": "example_dataflow_flex_template.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nDATAFLOW_FLEX_TEMPLATE_JOB_NAME = os.environ.get(\n    \"GCP_DATAFLOW_FLEX_TEMPLATE_JOB_NAME\", \"dataflow-flex-template\"\n)\n\n# For simplicity we use the same topic name as the subscription name.\nPUBSUB_FLEX_TEMPLATE_TOPIC = os.environ.get(\n    \"GCP_DATAFLOW_PUBSUB_FLEX_TEMPLATE_TOPIC\", \"dataflow-flex-template\"\n)\nPUBSUB_FLEX_TEMPLATE_SUBSCRIPTION = PUBSUB_FLEX_TEMPLATE_TOPIC\nGCS_FLEX_TEMPLATE_TEMPLATE_PATH = os.environ.get(\n    \"GCP_DATAFLOW_GCS_FLEX_TEMPLATE_TEMPLATE_PATH\",\n    \"gs://INVALID BUCKET NAME/samples/dataflow/templates/streaming-beam-sql.json\",\n)\nBQ_FLEX_TEMPLATE_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_DATASET\", \"airflow_dataflow_samples\")\nBQ_FLEX_TEMPLATE_LOCATION = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_LOCATION>\", \"us-west1\")\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_flex_template_java\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag_flex_template:\n    # [START howto_operator_start_template_job]\n    start_flex_template = DataflowStartFlexTemplateOperator(\n        task_id=\"start_flex_template_streaming_beam_sql\",\n        project_id=GCP_PROJECT_ID,\n        body={\n            \"launchParameter\": {\n                \"containerSpecGcsPath\": GCS_FLEX_TEMPLATE_TEMPLATE_PATH,\n                \"jobName\": DATAFLOW_FLEX_TEMPLATE_JOB_NAME,\n                \"parameters\": {\n                    \"inputSubscription\": PUBSUB_FLEX_TEMPLATE_SUBSCRIPTION,\n                    \"outputTable\": f\"{GCP_PROJECT_ID}:{BQ_FLEX_TEMPLATE_DATASET}.streaming_beam_sql\",\n                },\n            }\n        },\n        do_xcom_push=True,\n        location=BQ_FLEX_TEMPLATE_LOCATION,\n    )\n    # [END howto_operator_start_template_job]", "extracted_at": "2025-11-19T17:21:14.800324", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 125, "file_name": "example_dataflow.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n        dataflow_config={\n            \"job_name\": \"start-python-job-async\",\n            \"location\": \"europe-west3\",\n            \"wait_until_finished\": False,\n        },\n    )\n    # [END howto_operator_start_python_job_async]\n\n    # [START howto_sensor_wait_for_job_status]\n    wait_for_python_job_async_done = DataflowJobStatusSensor(\n        task_id=\"wait-for-python-job-async-done\",\n        job_id=\"{{task_instance.xcom_pull('start-python-job-async')['id']}}\",\n        expected_statuses={DataflowJobStatus.JOB_STATE_DONE},\n        location=\"europe-west3\",\n    )\n    # [END howto_sensor_wait_for_job_status]\n\n    # [START howto_sensor_wait_for_job_metric]\n    def check_metric_scalar_gte(metric_name: str, value: int) -> Callable:\n        \"\"\"Check is metric greater than equals to given value.\"\"\"\n\n        def callback(metrics: list[dict]) -> bool:\n            dag_native_python_async.log.info(\"Looking for '%s' >= %d\", metric_name, value)\n            for metric in metrics:\n                context = metric.get(\"name\", {}).get(\"context\", {})\n                original_name = context.get(\"original_name\", \"\")\n                tentative = context.get(\"tentative\", \"\")\n                if original_name == \"Service-cpu_num_seconds\" and not tentative:\n                    return metric[\"scalar\"] >= value\n            raise AirflowException(f\"Metric '{metric_name}' not found in metrics\")\n\n        return callback\n\n    wait_for_python_job_async_metric = DataflowJobMetricsSensor(\n        task_id=\"wait-for-python-job-async-metric\",\n        job_id=\"{{task_instance.xcom_pull('start-python-job-async')['id']}}\",\n        location=\"europe-west3\",\n        callback=check_metric_scalar_gte(metric_name=\"Service-cpu_num_seconds\", value=100),\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_metric]\n\n    # [START howto_sensor_wait_for_job_message]\n    def check_message(messages: list[dict]) -> bool:\n        \"\"\"Check message\"\"\"\n        for message in messages:\n            if \"Adding workflow start and stop steps.\" in message.get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_python_job_async_message = DataflowJobMessagesSensor(\n        task_id=\"wait-for-python-job-async-message\",\n        job_id=\"{{task_instance.xcom_pull('start-python-job-async')['id']}}\",\n        location=\"europe-west3\",\n        callback=check_message,\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_message]\n\n    # [START howto_sensor_wait_for_job_autoscaling_event]\n    def check_autoscaling_event(autoscaling_events: list[dict]) -> bool:\n        \"\"\"Check autoscaling event\"\"\"\n        for autoscaling_event in autoscaling_events:\n            if \"Worker pool started.\" in autoscaling_event.get(\"description\", {}).get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_python_job_async_autoscaling_event = DataflowJobAutoScalingEventsSensor(\n        task_id=\"wait-for-python-job-async-autoscaling-event\",\n        job_id=\"{{task_instance.xcom_pull('start-python-job-async')['id']}}\",\n        location=\"europe-west3\",\n        callback=check_autoscaling_event,\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_autoscaling_event]\n\n    start_python_job_async >> wait_for_python_job_async_done\n    start_python_job_async >> wait_for_python_job_async_metric\n    start_python_job_async >> wait_for_python_job_async_message\n    start_python_job_async >> wait_for_python_job_async_autoscaling_event\n\n\nwith models.DAG(\n    \"example_gcp_dataflow_template\",\n    default_args=default_args,\n    start_date=START_DATE,\n    catchup=False,\n    tags=[\"example\"],\n) as dag_template:\n    # [START howto_operator_start_template_job]\n    start_template_job = DataflowTemplatedJobStartOperator(\n        task_id=\"start-template-job\",\n        project_id=PROJECT_ID,\n        template=\"gs://dataflow-templates/latest/Word_Count\",\n        parameters={\"inputFile\": \"gs://dataflow-samples/shakespeare/kinglear.txt\", \"output\": GCS_OUTPUT},\n        location=\"europe-west3\",\n    )\n    # [END howto_operator_start_template_job]\n\nwith models.DAG(\n    \"example_gcp_stop_dataflow_job\",\n    default_args=default_args,\n    start_date=START_DATE,\n    catchup=False,\n    tags=[\"example\"],\n) as dag_template:\n    # [START howto_operator_stop_dataflow_job]\n    stop_dataflow_job = DataflowStopJobOperator(\n        task_id=\"stop-dataflow-job\",\n        location=\"europe-west3\",\n        job_name_prefix=\"start-template-job\",\n    )\n    # [END howto_operator_stop_dataflow_job]\n    start_template_job = DataflowTemplatedJobStartOperator(\n        task_id=\"start-template-job\",\n        project_id=PROJECT_ID,\n        template=\"gs://dataflow-templates/latest/Word_Count\",\n        parameters={\"inputFile\": \"gs://dataflow-samples/shakespeare/kinglear.txt\", \"output\": GCS_OUTPUT},\n        location=\"europe-west3\",\n        append_job_name=False,\n    )\n\n    stop_dataflow_job >> start_template_job", "extracted_at": "2025-11-19T17:21:14.803681", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 50, "file_name": "example_dataflow_sql.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartSqlJobOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nBQ_SQL_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_DATASET\", \"airflow_dataflow_samples\")\nBQ_SQL_TABLE_INPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_INPUT\", \"beam_input\")\nBQ_SQL_TABLE_OUTPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_OUTPUT\", \"beam_output\")\nDATAFLOW_SQL_JOB_NAME = os.environ.get(\"GCP_DATAFLOW_SQL_JOB_NAME\", \"dataflow-sql\")\nDATAFLOW_SQL_LOCATION = os.environ.get(\"GCP_DATAFLOW_SQL_LOCATION\", \"us-west1\")\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_sql\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag_sql:\n    # [START howto_operator_start_sql_job]\n    start_sql = DataflowStartSqlJobOperator(\n        task_id=\"start_sql_query\",\n        job_name=DATAFLOW_SQL_JOB_NAME,\n        query=f\"\"\"\n            SELECT\n                sales_region as sales_region,\n                count(state_id) as count_state\n            FROM\n                bigquery.table.`{GCP_PROJECT_ID}`.`{BQ_SQL_DATASET}`.`{BQ_SQL_TABLE_INPUT}`\n            WHERE state_id >= @state_id_min\n            GROUP BY sales_region;\n        \"\"\",\n        options={\n            \"bigquery-project\": GCP_PROJECT_ID,\n            \"bigquery-dataset\": BQ_SQL_DATASET,\n            \"bigquery-table\": BQ_SQL_TABLE_OUTPUT,\n            \"bigquery-write-disposition\": \"write-truncate\",\n            \"parameter\": \"state_id_min:INT64:2\",\n        },\n        location=DATAFLOW_SQL_LOCATION,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_start_sql_job]", "extracted_at": "2025-11-19T17:21:14.806673", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 46, "file_name": "example_looker.py"}, "content": "\"\"\"\nExample Airflow DAG that show how to use various Looker\noperators to submit PDT materialization job and manage it.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.looker import LookerStartPdtBuildOperator\nfrom airflow.providers.google.cloud.sensors.looker import LookerCheckPdtBuildSensor\n\nwith models.DAG(\n    dag_id=\"example_gcp_looker\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START cloud_looker_async_start_pdt_sensor]\n    start_pdt_task_async = LookerStartPdtBuildOperator(\n        task_id=\"start_pdt_task_async\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n        asynchronous=True,\n    )\n\n    check_pdt_task_async_sensor = LookerCheckPdtBuildSensor(\n        task_id=\"check_pdt_task_async_sensor\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        materialization_id=start_pdt_task_async.output,\n        poke_interval=10,\n    )\n    # [END cloud_looker_async_start_pdt_sensor]\n\n    # [START how_to_cloud_looker_start_pdt_build_operator]\n    build_pdt_task = LookerStartPdtBuildOperator(\n        task_id=\"build_pdt_task\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n    )\n    # [END how_to_cloud_looker_start_pdt_build_operator]\n\n    start_pdt_task_async >> check_pdt_task_async_sensor\n\n    build_pdt_task", "extracted_at": "2025-11-19T17:21:14.815067", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 124, "file_name": "example_facebook_ads_to_gcs.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use FacebookAdsReportToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom facebook_business.adobjects.adsinsights import AdsInsights\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.facebook_ads_to_gcs import FacebookAdsReportToGcsOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\n# [START howto_GCS_env_variables]\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"free-tier-1997\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow_bucket_fb\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"Temp/this_is_my_report_csv.csv\")\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nDATASET_NAME = os.environ.get(\"DATASET_NAME\", \"airflow_test_dataset\")\nTABLE_NAME = os.environ.get(\"FB_TABLE_NAME\", \"airflow_test_datatable\")\n# [END howto_GCS_env_variables]\n\n# [START howto_FB_ADS_variables]\nFIELDS = [\n    AdsInsights.Field.campaign_name,\n    AdsInsights.Field.campaign_id,\n    AdsInsights.Field.ad_id,\n    AdsInsights.Field.clicks,\n    AdsInsights.Field.impressions,\n]\nPARAMETERS = {\"level\": \"ad\", \"date_preset\": \"yesterday\"}\n# [END howto_FB_ADS_variables]\n\nwith models.DAG(\n    \"example_facebook_ads_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"campaign_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"campaign_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"ad_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"clicks\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"impressions\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_operator_facebook_ads_to_gcs]\n    run_operator = FacebookAdsReportToGcsOperator(\n        task_id=\"run_fetch_data\",\n        owner=\"airflow\",\n        bucket_name=GCS_BUCKET,\n        parameters=PARAMETERS,\n        fields=FIELDS,\n        gcp_conn_id=GCS_CONN_ID,\n        object_name=GCS_OBJ_PATH,\n    )\n    # [END howto_operator_facebook_ads_to_gcs]\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq_example\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    chain(\n        create_bucket,\n        create_dataset,\n        create_table,\n        run_operator,\n        load_csv,\n        read_data_from_gcs_many_chunks,\n        delete_bucket,\n        delete_dataset,\n    )", "extracted_at": "2025-11-19T17:21:14.819492", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 177, "file_name": "example_presto_to_gcs.py"}, "content": "\"\"\"\nExample DAG using PrestoToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.presto_to_gcs import PrestoToGCSOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCP_PRESTO_TO_GCS_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nDATASET_NAME = os.environ.get(\"GCP_PRESTO_TO_GCS_DATASET_NAME\", \"test_presto_to_gcs_dataset\")\n\nSOURCE_MULTIPLE_TYPES = \"memory.default.test_multiple_types\"\nSOURCE_CUSTOMER_TABLE = \"tpch.sf1.customer\"\n\n\ndef safe_name(s: str) -> str:\n    \"\"\"\n    Remove invalid characters for filename\n    \"\"\"\n    return re.sub(\"[^0-9a-zA-Z_]+\", \"_\", s)\n\n\nwith models.DAG(\n    dag_id=\"example_presto_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create-dataset\", dataset_id=DATASET_NAME)\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    # [START howto_operator_presto_to_gcs_basic]\n    presto_to_gcs_basic = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_basic\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n    )\n    # [END howto_operator_presto_to_gcs_basic]\n\n    # [START howto_operator_presto_to_gcs_multiple_types]\n    presto_to_gcs_multiple_types = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_multiple_types\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_multiple_types]\n\n    # [START howto_operator_create_external_table_multiple_types]\n    create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_multiple_types\",\n        bucket=GCS_BUCKET,\n        source_objects=[f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.*.json\"],\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_MULTIPLE_TYPES)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        schema_object=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n    )\n    # [END howto_operator_create_external_table_multiple_types]\n\n    read_data_from_gcs_multiple_types = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_multiple_types\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_MULTIPLE_TYPES)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_operator_presto_to_gcs_many_chunks]\n    presto_to_gcs_many_chunks = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_many_chunks\",\n        sql=f\"select * from {SOURCE_CUSTOMER_TABLE}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n        approx_max_file_size_bytes=10_000_000,\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_many_chunks]\n\n    create_external_table_many_chunks = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_many_chunks\",\n        bucket=GCS_BUCKET,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_CUSTOMER_TABLE)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        source_objects=[f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.*.json\"],\n        schema_object=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n    )\n\n    # [START howto_operator_read_data_from_gcs_many_chunks]\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_CUSTOMER_TABLE)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_read_data_from_gcs_many_chunks]\n\n    # [START howto_operator_presto_to_gcs_csv]\n    presto_to_gcs_csv = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_csv\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.csv\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        export_format=\"csv\",\n    )\n    # [END howto_operator_presto_to_gcs_csv]\n\n    create_dataset >> presto_to_gcs_basic\n    create_dataset >> presto_to_gcs_multiple_types\n    create_dataset >> presto_to_gcs_many_chunks\n    create_dataset >> presto_to_gcs_csv\n\n    presto_to_gcs_multiple_types >> create_external_table_multiple_types >> read_data_from_gcs_multiple_types\n    presto_to_gcs_many_chunks >> create_external_table_many_chunks >> read_data_from_gcs_many_chunks\n\n    presto_to_gcs_basic >> delete_dataset\n    presto_to_gcs_csv >> delete_dataset\n    read_data_from_gcs_multiple_types >> delete_dataset\n    read_data_from_gcs_many_chunks >> delete_dataset", "extracted_at": "2025-11-19T17:21:14.831278", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 112, "file_name": "example_salesforce_to_gcs.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use SalesforceToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.salesforce_to_gcs import SalesforceToGcsOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow-salesforce-bucket\")\nDATASET_NAME = os.environ.get(\"SALESFORCE_DATASET_NAME\", \"salesforce_test_dataset\")\nTABLE_NAME = os.environ.get(\"SALESFORCE_TABLE_NAME\", \"salesforce_test_datatable\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"results.csv\")\nQUERY = \"SELECT Id, Name, Company, Phone, Email, CreatedDate, LastModifiedDate, IsDeleted FROM Lead\"\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nSALESFORCE_CONN_ID = os.environ.get(\"SALESFORCE_CONN_ID\", \"salesforce_default\")\n\n\nwith models.DAG(\n    \"example_salesforce_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n        gcp_conn_id=GCS_CONN_ID,\n    )\n\n    # [START howto_operator_salesforce_to_gcs]\n    gcs_upload_task = SalesforceToGcsOperator(\n        query=QUERY,\n        include_deleted=True,\n        bucket_name=GCS_BUCKET,\n        object_name=GCS_OBJ_PATH,\n        salesforce_conn_id=SALESFORCE_CONN_ID,\n        export_format=\"csv\",\n        coerce_to_timestamp=False,\n        record_time_added=False,\n        gcp_conn_id=GCS_CONN_ID,\n        task_id=\"upload_to_gcs\",\n        dag=dag,\n    )\n    # [END howto_operator_salesforce_to_gcs]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=DATASET_NAME, project_id=GCP_PROJECT_ID, gcp_conn_id=GCS_CONN_ID\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"company\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"phone\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"email\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"createddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"lastmodifieddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"isdeleted\", \"type\": \"BOOL\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    create_bucket >> gcs_upload_task >> load_csv\n    create_dataset >> create_table >> load_csv\n    load_csv >> read_data_from_gcs\n    read_data_from_gcs >> delete_bucket\n    read_data_from_gcs >> delete_dataset", "extracted_at": "2025-11-19T17:21:14.837879", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 737, "file_name": "example_vertex_ai.py"}, "content": "\"\"\"\nExample Airflow DAG that demonstrates operators for the Google Vertex AI service in the Google\nCloud Platform.\n\nThis DAG relies on the following OS environment variables:\n\n* GCP_VERTEX_AI_BUCKET - Google Cloud Storage bucket where the model will be saved\n  after training process was finished.\n* CUSTOM_CONTAINER_URI - path to container with model.\n* PYTHON_PACKAGE_GSC_URI - path to test model in archive.\n* LOCAL_TRAINING_SCRIPT_PATH - path to local training script.\n* DATASET_ID - ID of dataset which will be used in training process.\n* MODEL_ID - ID of model which will be used in predict process.\n* MODEL_ARTIFACT_URI - The artifact_uri should be the path to a GCS directory containing saved model\n  artifacts.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom uuid import uuid4\n\nfrom google.cloud import aiplatform\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLForecastingTrainingJobOperator,\n    CreateAutoMLImageTrainingJobOperator,\n    CreateAutoMLTabularTrainingJobOperator,\n    CreateAutoMLTextTrainingJobOperator,\n    CreateAutoMLVideoTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n    ListAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.batch_prediction_job import (\n    CreateBatchPredictionJobOperator,\n    DeleteBatchPredictionJobOperator,\n    ListBatchPredictionJobsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomContainerTrainingJobOperator,\n    CreateCustomPythonPackageTrainingJobOperator,\n    CreateCustomTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n    ListCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ExportDataOperator,\n    GetDatasetOperator,\n    ImportDataOperator,\n    ListDatasetsOperator,\n    UpdateDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.endpoint_service import (\n    CreateEndpointOperator,\n    DeleteEndpointOperator,\n    DeployModelOperator,\n    ListEndpointsOperator,\n    UndeployModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.hyperparameter_tuning_job import (\n    CreateHyperparameterTuningJobOperator,\n    DeleteHyperparameterTuningJobOperator,\n    GetHyperparameterTuningJobOperator,\n    ListHyperparameterTuningJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.model_service import (\n    DeleteModelOperator,\n    ExportModelOperator,\n    ListModelsOperator,\n    UploadModelOperator,\n)\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"an-id\")\nREGION = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\nBUCKET = os.environ.get(\"GCP_VERTEX_AI_BUCKET\", \"vertex-ai-system-tests\")\n\nSTAGING_BUCKET = f\"gs://{BUCKET}\"\nDISPLAY_NAME = str(uuid4())  # Create random display name\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nCUSTOM_CONTAINER_URI = os.environ.get(\"CUSTOM_CONTAINER_URI\", \"path_to_container_with_model\")\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nTRAINING_FRACTION_SPLIT = 0.7\nTEST_FRACTION_SPLIT = 0.15\nVALIDATION_FRACTION_SPLIT = 0.15\n\nPYTHON_PACKAGE_GCS_URI = os.environ.get(\"PYTHON_PACKAGE_GSC_URI\", \"path_to_test_model_in_arch\")\nPYTHON_MODULE_NAME = \"aiplatform_custom_trainer_script.task\"\n\nLOCAL_TRAINING_SCRIPT_PATH = os.environ.get(\"LOCAL_TRAINING_SCRIPT_PATH\", \"path_to_training_script\")\n\nTRAINING_PIPELINE_ID = \"test-training-pipeline-id\"\nCUSTOM_JOB_ID = \"test-custom-job-id\"\n\nIMAGE_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-image-dataset\"),\n}\nTABULAR_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-tabular-dataset\"),\n}\nTEXT_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-text-dataset\"),\n}\nVIDEO_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/video_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nTIME_SERIES_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/time_series_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nDATASET_ID = os.environ.get(\"DATASET_ID\", \"test-dataset-id\")\nTEST_EXPORT_CONFIG = {\"gcs_destination\": {\"output_uri_prefix\": \"gs://test-vertex-ai-bucket/exports\"}}\nTEST_IMPORT_CONFIG = [\n    {\n        \"data_item_labels\": {\n            \"test-labels-name\": \"test-labels-value\",\n        },\n        \"import_schema_uri\": (\n            \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_bounding_box_io_format_1.0.0.yaml\"\n        ),\n        \"gcs_source\": {\n            \"uris\": [\"gs://ucaip-test-us-central1/dataset/salads_oid_ml_use_public_unassigned.jsonl\"]\n        },\n    },\n]\nDATASET_TO_UPDATE = {\"display_name\": \"test-name\"}\nTEST_UPDATE_MASK = {\"paths\": [\"displayName\"]}\n\nTEST_TIME_COLUMN = \"date\"\nTEST_TIME_SERIES_IDENTIFIER_COLUMN = \"store_name\"\nTEST_TARGET_COLUMN = \"sale_dollars\"\n\nCOLUMN_SPECS = {\n    TEST_TIME_COLUMN: \"timestamp\",\n    TEST_TARGET_COLUMN: \"numeric\",\n    \"city\": \"categorical\",\n    \"zip_code\": \"categorical\",\n    \"county\": \"categorical\",\n}\n\nCOLUMN_TRANSFORMATIONS = [\n    {\"categorical\": {\"column_name\": \"Type\"}},\n    {\"numeric\": {\"column_name\": \"Age\"}},\n    {\"categorical\": {\"column_name\": \"Breed1\"}},\n    {\"categorical\": {\"column_name\": \"Color1\"}},\n    {\"categorical\": {\"column_name\": \"Color2\"}},\n    {\"categorical\": {\"column_name\": \"MaturitySize\"}},\n    {\"categorical\": {\"column_name\": \"FurLength\"}},\n    {\"categorical\": {\"column_name\": \"Vaccinated\"}},\n    {\"categorical\": {\"column_name\": \"Sterilized\"}},\n    {\"categorical\": {\"column_name\": \"Health\"}},\n    {\"numeric\": {\"column_name\": \"Fee\"}},\n    {\"numeric\": {\"column_name\": \"PhotoAmt\"}},\n]\n\nMODEL_ID = os.environ.get(\"MODEL_ID\", \"test-model-id\")\nMODEL_ARTIFACT_URI = os.environ.get(\"MODEL_ARTIFACT_URI\", \"path_to_folder_with_model_artifacts\")\nMODEL_NAME = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\"\nJOB_DISPLAY_NAME = f\"temp_create_batch_prediction_job_test_{uuid4()}\"\nBIGQUERY_SOURCE = f\"bq://{PROJECT_ID}.test_iowa_liquor_sales_forecasting_us.2021_sales_predict\"\nGCS_DESTINATION_PREFIX = \"gs://test-vertex-ai-bucket-us/output\"\nMODEL_PARAMETERS: dict | None = {}\n\nENDPOINT_CONF = {\n    \"display_name\": f\"endpoint_test_{uuid4()}\",\n}\nDEPLOYED_MODEL = {\n    # format: 'projects/{project}/locations/{location}/models/{model}'\n    \"model\": f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\",\n    \"display_name\": f\"temp_endpoint_test_{uuid4()}\",\n    \"dedicated_resources\": {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-2\",\n            \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n            \"accelerator_count\": 1,\n        },\n        \"min_replica_count\": 1,\n        \"max_replica_count\": 1,\n    },\n}\n\nMODEL_OUTPUT_CONFIG = {\n    \"artifact_destination\": {\n        \"output_uri_prefix\": STAGING_BUCKET,\n    },\n    \"export_format_id\": \"custom-trained\",\n}\nMODEL_OBJ = {\n    \"display_name\": f\"model-{str(uuid4())}\",\n    \"artifact_uri\": MODEL_ARTIFACT_URI,\n    \"container_spec\": {\n        \"image_uri\": MODEL_SERVING_CONTAINER_URI,\n        \"command\": [],\n        \"args\": [],\n        \"env\": [],\n        \"ports\": [],\n        \"predict_route\": \"\",\n        \"health_route\": \"\",\n    },\n}\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_custom_jobs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as custom_jobs_dag:\n    # [START how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n    create_custom_container_training_job = CreateCustomContainerTrainingJobOperator(\n        task_id=\"custom_container_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-container-{DISPLAY_NAME}\",\n        container_uri=CUSTOM_CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        command=[\"python3\", \"task.py\"],\n        model_display_name=f\"container-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n    create_custom_python_package_training_job = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"python_package_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-py-package-{DISPLAY_NAME}\",\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        container_uri=CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        model_display_name=f\"py-package-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_training_job_operator]\n    create_custom_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"custom_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-custom-{DISPLAY_NAME}\",\n        script_path=LOCAL_TRAINING_SCRIPT_PATH,\n        container_uri=CONTAINER_URI,\n        requirements=[\"gcsfs==0.7.1\"],\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        replica_count=1,\n        model_display_name=f\"custom-housing-model-{DISPLAY_NAME}\",\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        custom_job_id=CUSTOM_JOB_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_custom_training_job_operator]\n    list_custom_training_job = ListCustomTrainingJobOperator(\n        task_id=\"list_custom_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_custom_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_dataset\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dataset_dag:\n    # [START how_to_cloud_vertex_ai_create_dataset_operator]\n    create_image_dataset_job = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_tabular_dataset_job = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_text_dataset_job = CreateDatasetOperator(\n        task_id=\"text_dataset\",\n        dataset=TEXT_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_video_dataset_job = CreateDatasetOperator(\n        task_id=\"video_dataset\",\n        dataset=VIDEO_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_time_series_dataset_job = CreateDatasetOperator(\n        task_id=\"time_series_dataset\",\n        dataset=TIME_SERIES_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_dataset_operator]\n    delete_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=create_text_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_get_dataset_operator]\n    get_dataset = GetDatasetOperator(\n        task_id=\"get_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_tabular_dataset_job.output[\"dataset_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_export_data_operator]\n    export_data_job = ExportDataOperator(\n        task_id=\"export_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        export_config=TEST_EXPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_data_operator]\n\n    # [START how_to_cloud_vertex_ai_import_data_operator]\n    import_data_job = ImportDataOperator(\n        task_id=\"import_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=TEST_IMPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_import_data_operator]\n\n    # [START how_to_cloud_vertex_ai_list_dataset_operator]\n    list_dataset_job = ListDatasetsOperator(\n        task_id=\"list_dataset\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_update_dataset_operator]\n    update_dataset_job = UpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_video_dataset_job.output[\"dataset_id\"],\n        dataset=DATASET_TO_UPDATE,\n        update_mask=TEST_UPDATE_MASK,\n    )\n    # [END how_to_cloud_vertex_ai_update_dataset_operator]\n\n    create_time_series_dataset_job\n    create_text_dataset_job >> delete_dataset_job\n    create_tabular_dataset_job >> get_dataset\n    create_image_dataset_job >> import_data_job >> export_data_job\n    create_video_dataset_job >> update_dataset_job\n    list_dataset_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_auto_ml\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as auto_ml_dag:\n    # [START how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n    create_auto_ml_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(\n        task_id=\"auto_ml_forecasting_task\",\n        display_name=f\"auto-ml-forecasting-{DISPLAY_NAME}\",\n        optimization_objective=\"minimize-rmse\",\n        column_specs=COLUMN_SPECS,\n        # run params\n        dataset_id=DATASET_ID,\n        target_column=TEST_TARGET_COLUMN,\n        time_column=TEST_TIME_COLUMN,\n        time_series_identifier_column=TEST_TIME_SERIES_IDENTIFIER_COLUMN,\n        available_at_forecast_columns=[TEST_TIME_COLUMN],\n        unavailable_at_forecast_columns=[TEST_TARGET_COLUMN],\n        time_series_attribute_columns=[\"city\", \"zip_code\", \"county\"],\n        forecast_horizon=30,\n        context_window=30,\n        data_granularity_unit=\"day\",\n        data_granularity_count=1,\n        weight_column=None,\n        budget_milli_node_hours=1000,\n        model_display_name=f\"auto-ml-forecasting-model-{DISPLAY_NAME}\",\n        predefined_split_column_name=None,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n    create_auto_ml_image_training_job = CreateAutoMLImageTrainingJobOperator(\n        task_id=\"auto_ml_image_task\",\n        display_name=f\"auto-ml-image-{DISPLAY_NAME}\",\n        dataset_id=DATASET_ID,\n        prediction_type=\"classification\",\n        multi_label=False,\n        model_type=\"CLOUD\",\n        training_fraction_split=0.6,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.2,\n        budget_milli_node_hours=8000,\n        model_display_name=f\"auto-ml-image-model-{DISPLAY_NAME}\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n    create_auto_ml_tabular_training_job = CreateAutoMLTabularTrainingJobOperator(\n        task_id=\"auto_ml_tabular_task\",\n        display_name=f\"auto-ml-tabular-{DISPLAY_NAME}\",\n        optimization_prediction_type=\"classification\",\n        column_transformations=COLUMN_TRANSFORMATIONS,\n        dataset_id=DATASET_ID,\n        target_column=\"Adopted\",\n        training_fraction_split=0.8,\n        validation_fraction_split=0.1,\n        test_fraction_split=0.1,\n        model_display_name=\"adopted-prediction-model\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n    create_auto_ml_text_training_job = CreateAutoMLTextTrainingJobOperator(\n        task_id=\"auto_ml_text_task\",\n        display_name=f\"auto-ml-text-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        multi_label=False,\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-text-model-{DISPLAY_NAME}\",\n        training_fraction_split=0.7,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.1,\n        sync=True,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n    create_auto_ml_video_training_job = CreateAutoMLVideoTrainingJobOperator(\n        task_id=\"auto_ml_video_task\",\n        display_name=f\"auto-ml-video-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        model_type=\"CLOUD\",\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-video-model-{DISPLAY_NAME}\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n    delete_auto_ml_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n    list_auto_ml_training_job = ListAutoMLTrainingJobOperator(\n        task_id=\"list_auto_ml_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_batch_prediction_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as batch_prediction_job_dag:\n    # [START how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n    create_batch_prediction_job = CreateBatchPredictionJobOperator(\n        task_id=\"create_batch_prediction_job\",\n        job_display_name=JOB_DISPLAY_NAME,\n        model_name=MODEL_NAME,\n        predictions_format=\"csv\",\n        bigquery_source=BIGQUERY_SOURCE,\n        gcs_destination_prefix=GCS_DESTINATION_PREFIX,\n        model_parameters=MODEL_PARAMETERS,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n    list_batch_prediction_job = ListBatchPredictionJobsOperator(\n        task_id=\"list_batch_prediction_jobs\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n    delete_batch_prediction_job = DeleteBatchPredictionJobOperator(\n        task_id=\"delete_batch_prediction_job\",\n        batch_prediction_job_id=create_batch_prediction_job.output[\"batch_prediction_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n\n    create_batch_prediction_job >> delete_batch_prediction_job\n    list_batch_prediction_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_endpoint\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as endpoint_dag:\n    # [START how_to_cloud_vertex_ai_create_endpoint_operator]\n    create_endpoint = CreateEndpointOperator(\n        task_id=\"create_endpoint\",\n        endpoint=ENDPOINT_CONF,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_endpoint_operator]\n    delete_endpoint = DeleteEndpointOperator(\n        task_id=\"delete_endpoint\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_list_endpoints_operator]\n    list_endpoints = ListEndpointsOperator(\n        task_id=\"list_endpoints\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_endpoints_operator]\n\n    # [START how_to_cloud_vertex_ai_deploy_model_operator]\n    deploy_model = DeployModelOperator(\n        task_id=\"deploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model=DEPLOYED_MODEL,\n        traffic_split={\"0\": 100},\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_deploy_model_operator]\n\n    # [START how_to_cloud_vertex_ai_undeploy_model_operator]\n    undeploy_model = UndeployModelOperator(\n        task_id=\"undeploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model_id=deploy_model.output[\"deployed_model_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_undeploy_model_operator]\n\n    create_endpoint >> deploy_model >> undeploy_model >> delete_endpoint\n    list_endpoints\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_hyperparameter_tuning_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as hyperparameter_tuning_job_dag:\n    # [START how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n    create_hyperparameter_tuning_job = CreateHyperparameterTuningJobOperator(\n        task_id=\"create_hyperparameter_tuning_job\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"horses-humans-hyptertune-{DISPLAY_NAME}\",\n        worker_pool_specs=[\n            {\n                \"machine_spec\": {\n                    \"machine_type\": MACHINE_TYPE,\n                    \"accelerator_type\": ACCELERATOR_TYPE,\n                    \"accelerator_count\": ACCELERATOR_COUNT,\n                },\n                \"replica_count\": REPLICA_COUNT,\n                \"container_spec\": {\n                    \"image_uri\": f\"gcr.io/{PROJECT_ID}/horse-human:hypertune\",\n                },\n            }\n        ],\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n        parameter_spec={\n            \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n                min=0.01, max=1, scale=\"log\"\n            ),\n            \"momentum\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale=\"linear\"),\n            \"num_neurons\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n                values=[64, 128, 512], scale=\"linear\"\n            ),\n        },\n        metric_spec={\n            \"accuracy\": \"maximize\",\n        },\n        max_trial_count=15,\n        parallel_trial_count=3,\n    )\n    # [END how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n    get_hyperparameter_tuning_job = GetHyperparameterTuningJobOperator(\n        task_id=\"get_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n    delete_hyperparameter_tuning_job = DeleteHyperparameterTuningJobOperator(\n        task_id=\"delete_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n    list_hyperparameter_tuning_job = ListHyperparameterTuningJobOperator(\n        task_id=\"list_hyperparameter_tuning_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n\n    create_hyperparameter_tuning_job >> get_hyperparameter_tuning_job >> delete_hyperparameter_tuning_job\n    list_hyperparameter_tuning_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_model_service\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as model_service_dag:\n    # [START how_to_cloud_vertex_ai_upload_model_operator]\n    upload_model = UploadModelOperator(\n        task_id=\"upload_model\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        model=MODEL_OBJ,\n    )\n    # [END how_to_cloud_vertex_ai_upload_model_operator]\n\n    # [START how_to_cloud_vertex_ai_export_model_operator]\n    export_model = ExportModelOperator(\n        task_id=\"export_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n        output_config=MODEL_OUTPUT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_model_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_model_operator]\n    delete_model = DeleteModelOperator(\n        task_id=\"delete_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_model_operator]\n\n    # [START how_to_cloud_vertex_ai_list_models_operator]\n    list_models = ListModelsOperator(\n        task_id=\"list_models\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_models_operator]\n\n    upload_model >> export_model >> delete_model\n    list_models", "extracted_at": "2025-11-19T17:21:14.847445", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 36, "file_name": "example_cosmos_document_sensor.py"}, "content": "\"\"\"\nExample Airflow DAG that senses document in Azure Cosmos DB.\n\nThis DAG relies on the following OS environment variables\n\n* DATABASE_NAME - Target CosmosDB database_name.\n* COLLECTION_NAME - Target CosmosDB collection_name.\n* DOCUMENT_ID - The ID of the target document.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.cosmos import AzureCosmosDocumentSensor\n\nDATABASE_NAME = os.environ.get(\"DATABASE_NAME\", \"example-database-name\")\nCOLLECTION_NAME = os.environ.get(\"COLLECTION_NAME\", \"example-collection-name\")\nDOCUMENT_ID = os.environ.get(\"DOCUMENT_ID\", \"example-document-id\")\n\n\nwith DAG(\n    \"example_cosmos_document_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cosmos_document_sensor]\n    azure_wasb_sensor = AzureCosmosDocumentSensor(\n        database_name=DATABASE_NAME,\n        collection_name=COLLECTION_NAME,\n        document_id=DOCUMENT_ID,\n        task_id=\"cosmos_document_sensor\",\n    )\n    # [END cosmos_document_sensor]", "extracted_at": "2025-11-19T17:21:14.855610", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 195, "file_name": "example_display_video.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use DisplayVideo.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook\nfrom airflow.providers.google.marketing_platform.operators.display_video import (\n    GoogleDisplayVideo360CreateQueryOperator,\n    GoogleDisplayVideo360CreateSDFDownloadTaskOperator,\n    GoogleDisplayVideo360DeleteReportOperator,\n    GoogleDisplayVideo360DownloadLineItemsOperator,\n    GoogleDisplayVideo360DownloadReportV2Operator,\n    GoogleDisplayVideo360RunQueryOperator,\n    GoogleDisplayVideo360SDFtoGCSOperator,\n    GoogleDisplayVideo360UploadLineItemsOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.display_video import (\n    GoogleDisplayVideo360GetSDFDownloadOperationSensor,\n    GoogleDisplayVideo360RunQuerySensor,\n)\n\n# [START howto_display_video_env_variables]\nBUCKET = os.environ.get(\"GMP_DISPLAY_VIDEO_BUCKET\", \"gs://INVALID BUCKET NAME\")\nADVERTISER_ID = os.environ.get(\"GMP_ADVERTISER_ID\", 1234567)\nOBJECT_NAME = os.environ.get(\"GMP_OBJECT_NAME\", \"files/report.csv\")\nPATH_TO_UPLOAD_FILE = os.environ.get(\"GCP_GCS_PATH_TO_UPLOAD_FILE\", \"test-gcs-example.txt\")\nPATH_TO_SAVED_FILE = os.environ.get(\"GCP_GCS_PATH_TO_SAVED_FILE\", \"test-gcs-example-download.txt\")\nBUCKET_FILE_LOCATION = PATH_TO_UPLOAD_FILE.rpartition(\"/\")[-1]\nSDF_VERSION = os.environ.get(\"GMP_SDF_VERSION\", \"SDF_VERSION_5_5\")\nBQ_DATA_SET = os.environ.get(\"GMP_BQ_DATA_SET\", \"airflow_test\")\nGMP_PARTNER_ID = os.environ.get(\"GMP_PARTNER_ID\", 123)\nENTITY_TYPE = os.environ.get(\"GMP_ENTITY_TYPE\", \"LineItem\")\nERF_SOURCE_OBJECT = GoogleDisplayVideo360Hook.erf_uri(GMP_PARTNER_ID, ENTITY_TYPE)\n\nREPORT_V2 = {\n    \"metadata\": {\n        \"title\": \"Airflow Test Report\",\n        \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n        \"format\": \"CSV\",\n        \"sendNotification\": False,\n    },\n    \"params\": {\n        \"type\": \"STANDARD\",\n        \"groupBys\": [\"FILTER_DATE\", \"FILTER_PARTNER\"],\n        \"filters\": [{\"type\": \"FILTER_PARTNER\", \"value\": ADVERTISER_ID}],\n        \"metrics\": [\"METRIC_IMPRESSIONS\", \"METRIC_CLICKS\"],\n    },\n    \"schedule\": {\"frequency\": \"ONE_TIME\"},\n}\n\nPARAMETERS = {\n    \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n}\n\nCREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST: dict = {\n    \"version\": SDF_VERSION,\n    \"advertiserId\": ADVERTISER_ID,\n    \"inventorySourceFilter\": {\"inventorySourceIds\": []},\n}\n\nDOWNLOAD_LINE_ITEMS_REQUEST: dict = {\"filterType\": ADVERTISER_ID, \"format\": \"CSV\", \"fileSpec\": \"EWF\"}\n# [END howto_display_video_env_variables]\n\nSTART_DATE = datetime(2021, 1, 1)\n\nwith models.DAG(\n    \"example_display_video_misc\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag2:\n    # [START howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n    upload_erf_to_bq = GCSToBigQueryOperator(\n        task_id=\"upload_erf_to_bq\",\n        bucket=BUCKET,\n        source_objects=ERF_SOURCE_OBJECT,\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n\n    # [START howto_google_display_video_download_line_items_operator]\n    download_line_items = GoogleDisplayVideo360DownloadLineItemsOperator(\n        task_id=\"download_line_items\",\n        request_body=DOWNLOAD_LINE_ITEMS_REQUEST,\n        bucket_name=BUCKET,\n        object_name=OBJECT_NAME,\n        gzip=False,\n    )\n    # [END howto_google_display_video_download_line_items_operator]\n\n    # [START howto_google_display_video_upload_line_items_operator]\n    upload_line_items = GoogleDisplayVideo360UploadLineItemsOperator(\n        task_id=\"upload_line_items\",\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n    )\n    # [END howto_google_display_video_upload_line_items_operator]\n\nwith models.DAG(\n    \"example_display_video_sdf\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag3:\n    # [START howto_google_display_video_create_sdf_download_task_operator]\n    create_sdf_download_task = GoogleDisplayVideo360CreateSDFDownloadTaskOperator(\n        task_id=\"create_sdf_download_task\", body_request=CREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST\n    )\n    operation_name = '{{ task_instance.xcom_pull(\"create_sdf_download_task\")[\"name\"] }}'\n    # [END howto_google_display_video_create_sdf_download_task_operator]\n\n    # [START howto_google_display_video_wait_for_operation_sensor]\n    wait_for_operation = GoogleDisplayVideo360GetSDFDownloadOperationSensor(\n        task_id=\"wait_for_operation\",\n        operation_name=operation_name,\n    )\n    # [END howto_google_display_video_wait_for_operation_sensor]\n\n    # [START howto_google_display_video_save_sdf_in_gcs_operator]\n    save_sdf_in_gcs = GoogleDisplayVideo360SDFtoGCSOperator(\n        task_id=\"save_sdf_in_gcs\",\n        operation_name=operation_name,\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n        gzip=False,\n    )\n    # [END howto_google_display_video_save_sdf_in_gcs_operator]\n\n    # [START howto_google_display_video_gcs_to_big_query_operator]\n    upload_sdf_to_big_query = GCSToBigQueryOperator(\n        task_id=\"upload_sdf_to_big_query\",\n        bucket=BUCKET,\n        source_objects=[save_sdf_in_gcs.output],\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_gcs_to_big_query_operator]\n\n    create_sdf_download_task >> wait_for_operation >> save_sdf_in_gcs\n\n    # Task dependency created via `XComArgs`:\n    #   save_sdf_in_gcs >> upload_sdf_to_big_query\n\nwith models.DAG(\n    \"example_display_video_v2\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag:\n    # [START howto_google_display_video_create_query_operator]\n    create_query_v2 = GoogleDisplayVideo360CreateQueryOperator(body=REPORT_V2, task_id=\"create_query\")\n\n    query_id = cast(str, XComArg(create_query_v2, key=\"query_id\"))\n    # [END howto_google_display_video_create_query_operator]\n\n    # [START howto_google_display_video_run_query_report_operator]\n    run_query_v2 = GoogleDisplayVideo360RunQueryOperator(\n        query_id=query_id, parameters=PARAMETERS, task_id=\"run_report\"\n    )\n\n    query_id = cast(str, XComArg(run_query_v2, key=\"query_id\"))\n    report_id = cast(str, XComArg(run_query_v2, key=\"report_id\"))\n    # [END howto_google_display_video_run_query_report_operator]\n\n    # [START howto_google_display_video_wait_run_query_sensor]\n    wait_for_query = GoogleDisplayVideo360RunQuerySensor(\n        task_id=\"wait_for_query\",\n        query_id=query_id,\n        report_id=report_id,\n    )\n    # [END howto_google_display_video_wait_run_query_sensor]\n\n    # [START howto_google_display_video_get_report_operator]\n    get_report_v2 = GoogleDisplayVideo360DownloadReportV2Operator(\n        query_id=query_id,\n        report_id=report_id,\n        task_id=\"get_report\",\n        bucket_name=BUCKET,\n        report_name=\"test1.csv\",\n    )\n    # # [END howto_google_display_video_get_report_operator]\n    # # [START howto_google_display_video_delete_query_report_operator]\n    delete_report_v2 = GoogleDisplayVideo360DeleteReportOperator(report_id=report_id, task_id=\"delete_report\")\n    # # [END howto_google_display_video_delete_query_report_operator]\n\n    create_query_v2 >> run_query_v2 >> wait_for_query >> get_report_v2 >> delete_report_v2", "extracted_at": "2025-11-19T17:21:14.859305", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_wasb_sensors.py"}, "content": "\"\"\"\nExample Airflow DAG that senses blob(s) in Azure Blob Storage.\n\nThis DAG relies on the following OS environment variables\n\n* CONTAINER_NAME - The container under which to look for the blob.\n* BLOB_NAME - The name of the blob to match.\n* PREFIX - The blob with the specified prefix to match.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor, WasbPrefixSensor\n\nCONTAINER_NAME = os.environ.get(\"CONTAINER_NAME\", \"example-container-name\")\nBLOB_NAME = os.environ.get(\"BLOB_NAME\", \"example-blob-name\")\nPREFIX = os.environ.get(\"PREFIX\", \"example-prefix\")\n\n\nwith DAG(\n    \"example_wasb_sensors\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START wasb_blob_sensor]\n    azure_wasb_sensor = WasbBlobSensor(\n        container_name=CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        task_id=\"wasb_sense_blob\",\n    )\n    # [END wasb_blob_sensor]\n\n    # [START wasb_prefix_sensor]\n    azure_wasb_prefix_sensor = WasbPrefixSensor(\n        container_name=CONTAINER_NAME,\n        prefix=PREFIX,\n        task_id=\"wasb_sense_prefix\",\n    )\n    # [END wasb_prefix_sensor]", "extracted_at": "2025-11-19T17:21:14.861237", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 45, "file_name": "example_oracle.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nwith DAG(\n    max_active_runs=1,\n    max_active_tasks=3,\n    catchup=False,\n    start_date=datetime(2023, 1, 1),\n    dag_id=\"example_oracle\",\n) as dag:\n\n    # [START howto_oracle_operator]\n\n    opr_sql = SQLExecuteQueryOperator(\n        task_id=\"task_sql\", conn_id=\"oracle\", sql=\"SELECT 1 FROM DUAL\", autocommit=True\n    )\n\n    # [END howto_oracle_operator]\n\n    # [START howto_oracle_stored_procedure_operator_with_list_inout]\n\n    opr_stored_procedure_with_list_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_list_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters=[3, int],\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_list_inout]\n\n    # [START howto_oracle_stored_procedure_operator_with_dict_inout]\n\n    opr_stored_procedure_with_dict_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_dict_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters={\"val_in\": 3, \"val_out\": int},\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_dict_inout]", "extracted_at": "2025-11-19T17:21:14.879682", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 50, "file_name": "example_airbyte_trigger_job.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the AirbyteTriggerSyncOperator.\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_airbyte_operator\"\nCONN_ID = \"15bc3800-82e4-48c3-a32d-620661273f28\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_airbyte_synchronous]\n    sync_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_sync_source_dest_example\",\n        connection_id=CONN_ID,\n    )\n    # [END howto_operator_airbyte_synchronous]\n\n    # [START howto_operator_airbyte_asynchronous]\n    async_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_async_source_dest_example\",\n        connection_id=CONN_ID,\n        asynchronous=True,\n    )\n\n    airbyte_sensor = AirbyteJobSensor(\n        task_id=\"airbyte_sensor_source_dest_example\",\n        airbyte_job_id=async_source_destination.output,\n    )\n    # [END howto_operator_airbyte_asynchronous]\n\n    # Task dependency created via `XComArgs`:\n    #   async_source_destination >> airbyte_sensor\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.883246", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 15, "file_name": "example_adb_spark_batch.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n\n    spark_pi >> spark_lr\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_adb_spark_batch]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.886969", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 38, "file_name": "example_adb_spark_sql.py"}, "content": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.analyticdb_spark import AnalyticDBSparkSQLOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"adb_spark_sql_dag\"\n# [START howto_operator_adb_spark_sql]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"cluster_id\": \"your cluster\", \"rg_name\": \"your resource group\", \"region\": \"your region\"},\n    max_active_runs=1,\n    catchup=False,\n) as dag:\n\n    show_databases = AnalyticDBSparkSQLOperator(task_id=\"task1\", sql=\"SHOE DATABASES;\")\n\n    show_tables = AnalyticDBSparkSQLOperator(task_id=\"task2\", sql=\"SHOW TABLES;\")\n\n    show_databases >> show_tables\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_adb_spark_sql]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.894024", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 39, "file_name": "example_oss_bucket.py"}, "content": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import OSSCreateBucketOperator, OSSDeleteBucketOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_bucket_dag\"\n# [START howto_operator_oss_bucket]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_bucket = OSSCreateBucketOperator(task_id=\"task1\")\n\n    delete_bucket = OSSDeleteBucketOperator(task_id=\"task2\")\n\n    create_bucket >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_oss_bucket]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.907981", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 105, "file_name": "example_athena.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        WITH SERDEPROPERTIES ( \"serialization.format\" = \",\", \"field.delim\" = \",\" )\n        LOCATION \"s3://{s3_bucket}//{athena_table}\"\n        TBLPROPERTIES (\"has_encrypted_data\"=\"false\")\n        \"\"\"\n    query_read_table = f\"SELECT * from {athena_database}.{athena_table}\"\n    query_drop_table = f\"DROP TABLE IF EXISTS {athena_database}.{athena_table}\"\n    query_drop_database = f\"DROP DATABASE IF EXISTS {athena_database}\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    upload_sample_data = S3CreateObjectOperator(\n        task_id=\"upload_sample_data\",\n        s3_bucket=s3_bucket,\n        s3_key=f\"{athena_table}/{SAMPLE_FILENAME}\",\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    create_database = AthenaOperator(\n        task_id=\"create_database\",\n        query=query_create_database,\n        database=athena_database,\n        output_location=f\"s3://{s3_bucket}/\",\n        sleep_time=1,\n    )\n\n    create_table = AthenaOperator(\n        task_id=\"create_table\",\n        query=query_create_table,\n        database=athena_database,\n        output_location=f\"s3://{s3_bucket}/\",\n        sleep_time=1,\n    )\n\n    # [START howto_operator_athena]\n    read_table = AthenaOperator(\n        task_id=\"read_table\",\n        query=query_read_table,\n        database=athena_database,\n        output_location=f\"s3://{s3_bucket}/\",\n    )\n    # [END howto_operator_athena]\n    read_table.sleep_time = 1\n\n    # [START howto_sensor_athena]\n    await_query = AthenaSensor(\n        task_id=\"await_query\",\n        query_execution_id=read_table.output,\n    )\n    # [END howto_sensor_athena]\n\n    drop_table = AthenaOperator(\n        task_id=\"drop_table\",\n        query=query_drop_table,\n        database=athena_database,\n        output_location=f\"s3://{s3_bucket}/\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        sleep_time=1,\n    )\n\n    drop_database = AthenaOperator(\n        task_id=\"drop_database\",\n        query=query_drop_database,\n        database=athena_database,\n        output_location=f\"s3://{s3_bucket}/\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        sleep_time=1,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        await_bucket(s3_bucket),\n        upload_sample_data,\n        create_database,\n        # TEST BODY\n        create_table,\n        read_table,\n        await_query,\n        read_results_from_s3(s3_bucket, read_table.output),\n        # TEST TEARDOWN\n        drop_table,\n        drop_database,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.918263", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 116, "file_name": "example_appflow.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.operators.appflow import (\n    AppflowRecordsShortCircuitOperator,\n    AppflowRunAfterOperator,\n    AppflowRunBeforeOperator,\n    AppflowRunDailyOperator,\n    AppflowRunFullOperator,\n    AppflowRunOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_appflow\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    source_name = \"salesforce\"\n    flow_name = f\"{env_id}-salesforce-campaign\"\n\n    # [START howto_operator_appflow_run]\n    campaign_dump = AppflowRunOperator(\n        task_id=\"campaign_dump\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run]\n\n    # [START howto_operator_appflow_run_full]\n    campaign_dump_full = AppflowRunFullOperator(\n        task_id=\"campaign_dump_full\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run_full]\n\n    # [START howto_operator_appflow_run_daily]\n    campaign_dump_daily = AppflowRunDailyOperator(\n        task_id=\"campaign_dump_daily\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_daily]\n\n    # [START howto_operator_appflow_run_before]\n    campaign_dump_before = AppflowRunBeforeOperator(\n        task_id=\"campaign_dump_before\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_before]\n\n    # [START howto_operator_appflow_run_after]\n    campaign_dump_after = AppflowRunAfterOperator(\n        task_id=\"campaign_dump_after\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"3000-01-01\",  # Future date, so no records to dump\n    )\n    # [END howto_operator_appflow_run_after]\n\n    # [START howto_operator_appflow_shortcircuit]\n    campaign_dump_short_circuit = AppflowRecordsShortCircuitOperator(\n        task_id=\"campaign_dump_short_circuit\",\n        flow_name=flow_name,\n        appflow_run_task_id=\"campaign_dump_after\",  # Should shortcircuit, no records expected\n    )\n    # [END howto_operator_appflow_shortcircuit]\n\n    should_be_skipped = BashOperator(\n        task_id=\"should_be_skipped\",\n        bash_command=\"echo 1\",\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        campaign_dump,\n        campaign_dump_full,\n        campaign_dump_daily,\n        campaign_dump_before,\n        campaign_dump_after,\n        campaign_dump_short_circuit,\n        should_be_skipped,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.921375", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 61, "file_name": "example_oss_object.py"}, "content": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import (\n    OSSDeleteBatchObjectOperator,\n    OSSDeleteObjectOperator,\n    OSSDownloadObjectOperator,\n    OSSUploadObjectOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_object_dag\"\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_object = OSSUploadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task1\",\n    )\n\n    download_object = OSSDownloadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task2\",\n    )\n\n    delete_object = OSSDeleteObjectOperator(\n        key=\"your oss key\",\n        task_id=\"task3\",\n    )\n\n    delete_batch_object = OSSDeleteBatchObjectOperator(\n        keys=[\"obj1\", \"obj2\", \"obj3\"],\n        task_id=\"task4\",\n    )\n\n    create_object >> download_object >> delete_object >> delete_batch_object\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.922955", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 104, "file_name": "example_cloudformation.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_cloudformation\"\n\n# The CloudFormation template must have at least one resource to\n# be usable, this example uses SQS as a free and serverless option.\nCLOUDFORMATION_TEMPLATE = {\n    \"Description\": \"Stack from Airflow CloudFormation example DAG\",\n    \"Resources\": {\n        \"ExampleQueue\": {\n            \"Type\": \"AWS::SQS::Queue\",\n        }\n    },\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    cloudformation_stack_name = f\"{env_id}-stack\"\n    cloudformation_create_parameters = {\n        \"StackName\": cloudformation_stack_name,\n        \"TemplateBody\": json.dumps(CLOUDFORMATION_TEMPLATE),\n        \"TimeoutInMinutes\": 2,\n        \"OnFailure\": \"DELETE\",  # Don't leave stacks behind if creation fails.\n    }\n\n    # [START howto_operator_cloudformation_create_stack]\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=cloudformation_stack_name,\n        cloudformation_parameters=cloudformation_create_parameters,\n    )\n    # [END howto_operator_cloudformation_create_stack]\n\n    # [START howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create = CloudFormationCreateStackSensor(\n        task_id=\"wait_for_stack_create\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create.poke_interval = 10\n\n    # [START howto_operator_cloudformation_delete_stack]\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_operator_cloudformation_delete_stack]\n    delete_stack.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete = CloudFormationDeleteStackSensor(\n        task_id=\"wait_for_stack_delete\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete.poke_interval = 10\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_stack,\n        wait_for_stack_create,\n        delete_stack,\n        wait_for_stack_delete,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.945723", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 67, "file_name": "example_azure_blob_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.azure_blob_to_s3 import AzureBlobStorageToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_azure_blob_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-azure_blob-to-s3-bucket\"\n    s3_key = f\"{env_id}-azure_blob-to-s3-key\"\n    s3_key_url = f\"s3://{s3_bucket}/{s3_key}\"\n    azure_container_name = f\"{env_id}-azure_blob-to-s3-container\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_azure_blob_to_s3]\n    azure_blob_to_s3 = AzureBlobStorageToS3Operator(\n        task_id=\"azure_blob_to_s3\",\n        container_name=azure_container_name,\n        dest_s3_key=s3_key_url,\n    )\n    # [END howto_transfer_azure_blob_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        azure_blob_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.948745", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 230, "file_name": "example_datasync.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_datasync\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n\ndef get_s3_bucket_arn(bucket_name):\n    return f\"arn:aws:s3:::{bucket_name}\"\n\n\ndef create_location(bucket_name, role_arn):\n    client = boto3.client(\"datasync\")\n    response = client.create_location_s3(\n        Subdirectory=\"test\",\n        S3BucketArn=get_s3_bucket_arn(bucket_name),\n        S3Config={\n            \"BucketAccessRoleArn\": role_arn,\n        },\n    )\n    return response[\"LocationArn\"]\n\n\n@task\ndef create_source_location(bucket_source, role_arn):\n    return create_location(bucket_source, role_arn)\n\n\n@task\ndef create_destination_location(bucket_destination, role_arn):\n    return create_location(bucket_destination, role_arn)\n\n\n@task\ndef create_task(**kwargs):\n    client = boto3.client(\"datasync\")\n    response = client.create_task(\n        SourceLocationArn=kwargs[\"ti\"].xcom_pull(\"create_source_location\"),\n        DestinationLocationArn=kwargs[\"ti\"].xcom_pull(\"create_destination_location\"),\n    )\n    return response[\"TaskArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task(task_arn):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=task_arn,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_created_by_operator(**kwargs):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=kwargs[\"ti\"].xcom_pull(\"create_and_execute_task\")[\"TaskArn\"],\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef list_locations(bucket_source, bucket_destination):\n    client = boto3.client(\"datasync\")\n    return client.list_locations(\n        Filters=[\n            {\n                \"Name\": \"LocationUri\",\n                \"Values\": [\n                    f\"s3://{bucket_source}/test/\",\n                    f\"s3://{bucket_destination}/test/\",\n                    f\"s3://{bucket_source}/test_create/\",\n                    f\"s3://{bucket_destination}/test_create/\",\n                ],\n                \"Operator\": \"In\",\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_locations(locations):\n    client = boto3.client(\"datasync\")\n    for location in locations[\"Locations\"]:\n        client.delete_location(\n            LocationArn=location[\"LocationArn\"],\n        )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    s3_bucket_source: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-source\"\n    s3_bucket_destination: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-destination\"\n\n    create_s3_bucket_source = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_source\", bucket_name=s3_bucket_source\n    )\n\n    create_s3_bucket_destination = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_destination\", bucket_name=s3_bucket_destination\n    )\n\n    source_location = create_source_location(s3_bucket_source, test_context[ROLE_ARN_KEY])\n    destination_location = create_destination_location(s3_bucket_destination, test_context[ROLE_ARN_KEY])\n\n    created_task_arn = create_task()\n\n    # [START howto_operator_datasync_specific_task]\n    # Execute a specific task\n    execute_task_by_arn = DataSyncOperator(\n        task_id=\"execute_task_by_arn\",\n        task_arn=created_task_arn,\n    )\n    # [END howto_operator_datasync_specific_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_arn.wait_for_completion = False\n\n    # [START howto_operator_datasync_search_task]\n    # Search and execute a task\n    execute_task_by_locations = DataSyncOperator(\n        task_id=\"execute_task_by_locations\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test\",\n        # Only transfer files from /test/subdir folder\n        task_execution_kwargs={\n            \"Includes\": [{\"FilterType\": \"SIMPLE_PATTERN\", \"Value\": \"/test/subdir\"}],\n        },\n    )\n    # [END howto_operator_datasync_search_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_locations.wait_for_completion = False\n\n    # [START howto_operator_datasync_create_task]\n    # Create a task (the task does not exist)\n    create_and_execute_task = DataSyncOperator(\n        task_id=\"create_and_execute_task\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test_create\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test_create\",\n        create_task_kwargs={\"Name\": \"Created by Airflow\"},\n        create_source_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_source),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        create_destination_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_destination),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        delete_task_after_execution=False,\n    )\n    # [END howto_operator_datasync_create_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    create_and_execute_task.wait_for_completion = False\n\n    locations_task = list_locations(s3_bucket_source, s3_bucket_destination)\n    delete_locations_task = delete_locations(locations_task)\n\n    delete_s3_bucket_source = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_source\",\n        bucket_name=s3_bucket_source,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket_destination = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_destination\",\n        bucket_name=s3_bucket_destination,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket_source,\n        create_s3_bucket_destination,\n        source_location,\n        destination_location,\n        created_task_arn,\n        # TEST BODY\n        execute_task_by_arn,\n        execute_task_by_locations,\n        create_and_execute_task,\n        # TEST TEARDOWN\n        delete_task(created_task_arn),\n        delete_task_created_by_operator(),\n        locations_task,\n        delete_locations_task,\n        delete_s3_bucket_source,\n        delete_s3_bucket_destination,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.951423", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 270, "file_name": "example_batch.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.batch import BatchCreateComputeEnvironmentOperator, BatchOperator\nfrom airflow.providers.amazon.aws.sensors.batch import (\n    BatchComputeEnvironmentSensor,\n    BatchJobQueueSensor,\n    BatchSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import (\n    ENV_ID_KEY,\n    SystemTestContextBuilder,\n    prune_logs,\n    split_string,\n)\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_batch\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY)\n    .add_variable(SECURITY_GROUPS_KEY)\n    .build()\n)\n\nJOB_OVERRIDES: dict = {}\n\n\n@task\ndef create_job_definition(role_arn, job_definition_name):\n    boto3.client(\"batch\").register_job_definition(\n        type=\"container\",\n        containerProperties={\n            \"command\": [\n                \"sleep\",\n                \"2\",\n            ],\n            \"executionRoleArn\": role_arn,\n            \"image\": \"busybox\",\n            \"resourceRequirements\": [\n                {\"value\": \"1\", \"type\": \"VCPU\"},\n                {\"value\": \"2048\", \"type\": \"MEMORY\"},\n            ],\n            \"networkConfiguration\": {\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        jobDefinitionName=job_definition_name,\n        platformCapabilities=[\"FARGATE\"],\n    )\n\n\n@task\ndef create_job_queue(job_compute_environment_name, job_queue_name):\n    boto3.client(\"batch\").create_job_queue(\n        computeEnvironmentOrder=[\n            {\n                \"computeEnvironment\": job_compute_environment_name,\n                \"order\": 1,\n            },\n        ],\n        jobQueueName=job_queue_name,\n        priority=1,\n        state=\"ENABLED\",\n    )\n\n\n# Only describe the job if a previous task failed, to help diagnose\n@task(trigger_rule=TriggerRule.ONE_FAILED)\ndef describe_job(job_id):\n    client = boto3.client(\"batch\")\n    response = client.describe_jobs(jobs=[job_id])\n    log.info(\"Describing the job %s for debugging purposes\", job_id)\n    log.info(response[\"jobs\"])\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_definition(job_definition_name):\n    client = boto3.client(\"batch\")\n\n    response = client.describe_job_definitions(\n        jobDefinitionName=job_definition_name,\n        status=\"ACTIVE\",\n    )\n\n    for job_definition in response[\"jobDefinitions\"]:\n        client.deregister_job_definition(\n            jobDefinition=job_definition[\"jobDefinitionArn\"],\n        )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").update_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").delete_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_job_queue(job_queue_name):\n    boto3.client(\"batch\").update_job_queue(\n        jobQueue=job_queue_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_queue(job_queue_name):\n    boto3.client(\"batch\").delete_job_queue(\n        jobQueue=job_queue_name,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    batch_job_name: str = f\"{env_id}-test-job\"\n    batch_job_definition_name: str = f\"{env_id}-test-job-definition\"\n    batch_job_compute_environment_name: str = f\"{env_id}-test-job-compute-environment\"\n    batch_job_queue_name: str = f\"{env_id}-test-job-queue\"\n\n    security_groups = split_string(test_context[SECURITY_GROUPS_KEY])\n    subnets = split_string(test_context[SUBNETS_KEY])\n\n    # [START howto_operator_batch_create_compute_environment]\n    create_compute_environment = BatchCreateComputeEnvironmentOperator(\n        task_id=\"create_compute_environment\",\n        compute_environment_name=batch_job_compute_environment_name,\n        environment_type=\"MANAGED\",\n        state=\"ENABLED\",\n        compute_resources={\n            \"type\": \"FARGATE\",\n            \"maxvCpus\": 10,\n            \"securityGroupIds\": security_groups,\n            \"subnets\": subnets,\n        },\n    )\n    # [END howto_operator_batch_create_compute_environment]\n\n    # [START howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_valid\",\n        compute_environment=batch_job_compute_environment_name,\n    )\n    # [END howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid.poke_interval = 1\n\n    # [START howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_valid\",\n        job_queue=batch_job_queue_name,\n    )\n    # [END howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid.poke_interval = 1\n\n    # [START howto_operator_batch]\n    submit_batch_job = BatchOperator(\n        task_id=\"submit_batch_job\",\n        job_name=batch_job_name,\n        job_queue=batch_job_queue_name,\n        job_definition=batch_job_definition_name,\n        overrides=JOB_OVERRIDES,\n    )\n    # [END howto_operator_batch]\n\n    # BatchOperator waits by default, setting as False to test the Sensor below.\n    submit_batch_job.wait_for_completion = False\n\n    # [START howto_sensor_batch]\n    wait_for_batch_job = BatchSensor(\n        task_id=\"wait_for_batch_job\",\n        job_id=submit_batch_job.output,\n    )\n    # [END howto_sensor_batch]\n    wait_for_batch_job.poke_interval = 10\n\n    wait_for_compute_environment_disabled = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_disabled\",\n        compute_environment=batch_job_compute_environment_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_modified = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_modified\",\n        job_queue=batch_job_queue_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_deleted = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_deleted\",\n        job_queue=batch_job_queue_name,\n        treat_non_existing_as_deleted=True,\n        poke_interval=10,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/batch/job\", env_id)\n        ],\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        security_groups,\n        subnets,\n        create_job_definition(test_context[ROLE_ARN_KEY], batch_job_definition_name),\n        # TEST BODY\n        create_compute_environment,\n        wait_for_compute_environment_valid,\n        # ``create_job_queue`` is part of test setup but need the compute-environment to be created before\n        create_job_queue(batch_job_compute_environment_name, batch_job_queue_name),\n        wait_for_job_queue_valid,\n        submit_batch_job,\n        wait_for_batch_job,\n        # TEST TEARDOWN\n        describe_job(submit_batch_job.output),\n        disable_job_queue(batch_job_queue_name),\n        wait_for_job_queue_modified,\n        delete_job_queue(batch_job_queue_name),\n        wait_for_job_queue_deleted,\n        disable_compute_environment(batch_job_compute_environment_name),\n        wait_for_compute_environment_disabled,\n        delete_compute_environment(batch_job_compute_environment_name),\n        delete_job_definition(batch_job_definition_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.955440", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 376, "file_name": "example_dms.py"}, "content": "SAMPLE_DATA = [\n    (\"Airflow\", \"2015\"),\n    (\"OpenOffice\", \"2012\"),\n    (\"Subversion\", \"2000\"),\n    (\"NiFi\", \"2006\"),\n]\nSG_IP_PERMISSION = {\n    \"FromPort\": 5432,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}],\n}\n\n\ndef _get_rds_instance_endpoint(instance_name: str):\n    print(\"Retrieving RDS instance endpoint.\")\n    rds_client = boto3.client(\"rds\")\n\n    response = rds_client.describe_db_instances(DBInstanceIdentifier=instance_name)\n    rds_instance_endpoint = response[\"DBInstances\"][0][\"Endpoint\"]\n    return rds_instance_endpoint\n\n\n@task\ndef create_security_group(security_group_name: str, vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        GroupName=security_group_name,\n        Description=\"Created for DMS system test\",\n        VpcId=vpc_id,\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"],\n        IpPermissions=[SG_IP_PERMISSION],\n    )\n\n    return security_group[\"GroupId\"]\n\n\n@task\ndef create_sample_table(instance_name: str, db_name: str, table_name: str):\n    print(\"Creating sample table.\")\n\n    rds_endpoint = _get_rds_instance_endpoint(instance_name)\n    hostname = rds_endpoint[\"Address\"]\n    port = rds_endpoint[\"Port\"]\n    rds_url = f\"{RDS_PROTOCOL}://{RDS_USERNAME}:{RDS_PASSWORD}@{hostname}:{port}/{db_name}\"\n    engine = create_engine(rds_url)\n\n    table = Table(\n        table_name,\n        MetaData(engine),\n        Column(TABLE_HEADERS[0], String, primary_key=True),\n        Column(TABLE_HEADERS[1], String),\n    )\n\n    with engine.connect() as connection:\n        # Create the Table.\n        table.create()\n        load_data = table.insert().values(SAMPLE_DATA)\n        connection.execute(load_data)\n\n        # Read the data back to verify everything is working.\n        connection.execute(table.select())\n\n\n@task(multiple_outputs=True)\ndef create_dms_assets(\n    db_name: str,\n    instance_name: str,\n    replication_instance_name: str,\n    bucket_name: str,\n    role_arn,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    table_definition: dict,\n):\n    print(\"Creating DMS assets.\")\n    dms_client = boto3.client(\"dms\")\n    rds_instance_endpoint = _get_rds_instance_endpoint(instance_name)\n\n    print(\"Creating replication instance.\")\n    instance_arn = dms_client.create_replication_instance(\n        ReplicationInstanceIdentifier=replication_instance_name,\n        ReplicationInstanceClass=\"dms.t3.micro\",\n    )[\"ReplicationInstance\"][\"ReplicationInstanceArn\"]\n\n    print(\"Creating DMS source endpoint.\")\n    source_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=source_endpoint_identifier,\n        EndpointType=\"source\",\n        EngineName=RDS_ENGINE,\n        Username=RDS_USERNAME,\n        Password=RDS_PASSWORD,\n        ServerName=rds_instance_endpoint[\"Address\"],\n        Port=rds_instance_endpoint[\"Port\"],\n        DatabaseName=db_name,\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Creating DMS target endpoint.\")\n    target_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=target_endpoint_identifier,\n        EndpointType=\"target\",\n        EngineName=\"s3\",\n        S3Settings={\n            \"BucketName\": bucket_name,\n            \"BucketFolder\": \"folder\",\n            \"ServiceAccessRoleArn\": role_arn,\n            \"ExternalTableDefinition\": json.dumps(table_definition),\n        },\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Awaiting replication instance provisioning.\")\n    dms_client.get_waiter(\"replication_instance_available\").wait(\n        Filters=[{\"Name\": \"replication-instance-arn\", \"Values\": [instance_arn]}]\n    )\n\n    return {\n        \"replication_instance_arn\": instance_arn,\n        \"source_endpoint_arn\": source_endpoint_arn,\n        \"target_endpoint_arn\": target_endpoint_arn,\n    }\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dms_assets(\n    replication_instance_arn: str,\n    source_endpoint_arn: str,\n    target_endpoint_arn: str,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    replication_instance_name: str,\n):\n    dms_client = boto3.client(\"dms\")\n\n    print(\"Deleting DMS assets.\")\n    dms_client.delete_replication_instance(ReplicationInstanceArn=replication_instance_arn)\n    dms_client.delete_endpoint(EndpointArn=source_endpoint_arn)\n    dms_client.delete_endpoint(EndpointArn=target_endpoint_arn)\n\n    print(\"Awaiting DMS assets tear-down.\")\n    dms_client.get_waiter(\"replication_instance_deleted\").wait(\n        Filters=[{\"Name\": \"replication-instance-id\", \"Values\": [replication_instance_name]}]\n    )\n    dms_client.get_waiter(\"endpoint_deleted\").wait(\n        Filters=[\n            {\n                \"Name\": \"endpoint-id\",\n                \"Values\": [source_endpoint_identifier, target_endpoint_identifier],\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(security_group_id: str, security_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=security_group_id, GroupName=security_group_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    bucket_name = f\"{env_id}-dms-bucket\"\n    rds_instance_name = f\"{env_id}-instance\"\n    rds_db_name = f\"{env_id}_source_database\"  # dashes are not allowed in db name\n    rds_table_name = f\"{env_id}-table\"\n    dms_replication_instance_name = f\"{env_id}-replication-instance\"\n    dms_replication_task_id = f\"{env_id}-replication-task\"\n    source_endpoint_identifier = f\"{env_id}-source-endpoint\"\n    target_endpoint_identifier = f\"{env_id}-target-endpoint\"\n    security_group_name = f\"{env_id}-dms-security-group\"\n\n    # Sample data.\n    table_definition = {\n        \"TableCount\": \"1\",\n        \"Tables\": [\n            {\n                \"TableName\": rds_table_name,\n                \"TableColumns\": [\n                    {\n                        \"ColumnName\": TABLE_HEADERS[0],\n                        \"ColumnType\": \"STRING\",\n                        \"ColumnNullable\": \"false\",\n                        \"ColumnIsPk\": \"true\",\n                    },\n                    {\"ColumnName\": TABLE_HEADERS[1], \"ColumnType\": \"STRING\", \"ColumnLength\": \"4\"},\n                ],\n                \"TableColumnsTotal\": \"2\",\n            }\n        ],\n    }\n    table_mappings = {\n        \"rules\": [\n            {\n                \"rule-type\": \"selection\",\n                \"rule-id\": \"1\",\n                \"rule-name\": \"1\",\n                \"object-locator\": {\n                    \"schema-name\": \"public\",\n                    \"table-name\": rds_table_name,\n                },\n                \"rule-action\": \"include\",\n            }\n        ]\n    }\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    get_vpc_id = get_default_vpc_id()\n\n    create_sg = create_security_group(security_group_name, get_vpc_id)\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t3.micro\",\n        engine=RDS_ENGINE,\n        rds_kwargs={\n            \"DBName\": rds_db_name,\n            \"AllocatedStorage\": 20,\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"PubliclyAccessible\": True,\n            \"VpcSecurityGroupIds\": [\n                create_sg,\n            ],\n        },\n    )\n\n    create_assets = create_dms_assets(\n        db_name=rds_db_name,\n        instance_name=rds_instance_name,\n        replication_instance_name=dms_replication_instance_name,\n        bucket_name=bucket_name,\n        role_arn=role_arn,\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        table_definition=table_definition,\n    )\n\n    # [START howto_operator_dms_create_task]\n    create_task = DmsCreateTaskOperator(\n        task_id=\"create_task\",\n        replication_task_id=dms_replication_task_id,\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        table_mappings=table_mappings,\n    )\n    # [END howto_operator_dms_create_task]\n\n    task_arn = cast(str, create_task.output)\n\n    # [START howto_operator_dms_start_task]\n    start_task = DmsStartTaskOperator(\n        task_id=\"start_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_start_task]\n\n    # [START howto_operator_dms_describe_tasks]\n    describe_tasks = DmsDescribeTasksOperator(\n        task_id=\"describe_tasks\",\n        describe_tasks_kwargs={\n            \"Filters\": [\n                {\n                    \"Name\": \"replication-instance-arn\",\n                    \"Values\": [create_assets[\"replication_instance_arn\"]],\n                }\n            ]\n        },\n        do_xcom_push=False,\n    )\n    # [END howto_operator_dms_describe_tasks]\n\n    await_task_start = DmsTaskBaseSensor(\n        task_id=\"await_task_start\",\n        replication_task_arn=task_arn,\n        target_statuses=[\"running\"],\n        termination_statuses=[\"stopped\", \"deleting\", \"failed\"],\n        poke_interval=10,\n    )\n\n    # [START howto_operator_dms_stop_task]\n    stop_task = DmsStopTaskOperator(\n        task_id=\"stop_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_stop_task]\n\n    # TaskCompletedSensor actually waits until task reaches the \"Stopped\" state, so it will work here.\n    # [START howto_sensor_dms_task_completed]\n    await_task_stop = DmsTaskCompletedSensor(\n        task_id=\"await_task_stop\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_sensor_dms_task_completed]\n    await_task_stop.poke_interval = 10\n\n    # [START howto_operator_dms_delete_task]\n    delete_task = DmsDeleteTaskOperator(\n        task_id=\"delete_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_delete_task]\n    delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_assets = delete_dms_assets(\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        replication_instance_name=dms_replication_instance_name,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        get_vpc_id,\n        create_sg,\n        create_db_instance,\n        create_sample_table(rds_instance_name, rds_db_name, rds_table_name),\n        create_assets,\n        # TEST BODY\n        create_task,\n        start_task,\n        describe_tasks,\n        await_task_start,\n        stop_task,\n        await_task_stop,\n        # TEST TEARDOWN\n        delete_task,\n        delete_assets,\n        delete_db_instance,\n        delete_security_group(create_sg, security_group_name),\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.982309", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 96, "file_name": "example_dynamodb.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.sensors.dynamodb import DynamoDBValueSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_dynamodbvaluesensor\"\nsys_test_context_task = SystemTestContextBuilder().build()\n\nPK_ATTRIBUTE_NAME = \"PK\"\nSK_ATTRIBUTE_NAME = \"SK\"\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": PK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n    {\"AttributeName\": SK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"PK\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"SK\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10}\n\n\n@task\ndef create_table(table_name: str):\n    ddb = boto3.resource(\"dynamodb\")\n    table = ddb.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(TableName=table_name)\n    table.put_item(Item={\"PK\": \"Test\", \"SK\": \"2022-07-12T11:11:25-0400\", \"Value\": \"Testing\"})\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_table(table_name: str):\n    client = boto3.client(\"dynamodb\")\n    client.delete_table(TableName=table_name)\n    client.get_waiter(\"table_not_exists\").wait(TableName=table_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    create_table = create_table(table_name=table_name)\n    delete_table = delete_table(table_name)\n\n    # [START howto_sensor_dynamodb_value]\n    dynamodb_sensor = DynamoDBValueSensor(\n        task_id=\"waiting_for_dynamodb_item_value\",\n        table_name=table_name,\n        partition_key_name=PK_ATTRIBUTE_NAME,\n        partition_key_value=\"Test\",\n        sort_key_name=SK_ATTRIBUTE_NAME,\n        sort_key_value=\"2022-07-12T11:11:25-0400\",\n        attribute_name=\"Value\",\n        attribute_value=\"Testing\",\n    )\n    # [END howto_sensor_dynamodb_value]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        # TEST BODY\n        dynamodb_sensor,\n        # TEST TEARDOWN\n        delete_table,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.983152", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 162, "file_name": "example_ec2.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom operator import itemgetter\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    EC2CreateInstanceOperator,\n    EC2StartInstanceOperator,\n    EC2StopInstanceOperator,\n    EC2TerminateInstanceOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ec2 import EC2InstanceStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ec2\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef get_latest_ami_id():\n    \"\"\"Returns the AMI ID of the most recently-created Amazon Linux image\"\"\"\n\n    # Amazon is retiring AL2 in 2023 and replacing it with Amazon Linux 2022.\n    # This image prefix should be futureproof, but may need adjusting depending\n    # on how they name the new images.  This page should have AL2022 info when\n    # it comes available: https://aws.amazon.com/linux/amazon-linux-2022/faqs/\n    image_prefix = \"Amazon Linux*\"\n\n    images = boto3.client(\"ec2\").describe_images(\n        Filters=[\n            {\"Name\": \"description\", \"Values\": [image_prefix]},\n            {\"Name\": \"architecture\", \"Values\": [\"arm64\"]},\n        ],\n        Owners=[\"amazon\"],\n    )\n    # Sort on CreationDate\n    return max(images[\"Images\"], key=itemgetter(\"CreationDate\"))[\"ImageId\"]\n\n\n@task\ndef create_key_pair(key_name: str):\n    client = boto3.client(\"ec2\")\n\n    key_pair_id = client.create_key_pair(KeyName=key_name)[\"KeyName\"]\n    # Creating the key takes a very short but measurable time, preventing race condition:\n    client.get_waiter(\"key_pair_exists\").wait(KeyNames=[key_pair_id])\n\n    return key_pair_id\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_key_pair(key_pair_id: str):\n    boto3.client(\"ec2\").delete_key_pair(KeyName=key_pair_id)\n\n\n@task\ndef parse_response(instance_ids: list):\n    return instance_ids[0]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    instance_name = f\"{env_id}-instance\"\n    key_name = create_key_pair(key_name=f\"{env_id}_key_pair\")\n    image_id = get_latest_ami_id()\n\n    config = {\n        \"InstanceType\": \"t4g.micro\",\n        \"KeyName\": key_name,\n        \"TagSpecifications\": [\n            {\"ResourceType\": \"instance\", \"Tags\": [{\"Key\": \"Name\", \"Value\": instance_name}]}\n        ],\n        # Use IMDSv2 for greater security, see the following doc for more details:\n        # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html\n        \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n    }\n\n    # EC2CreateInstanceOperator creates and starts the EC2 instances. To test the EC2StartInstanceOperator,\n    # we will stop the instance, then start them again before terminating them.\n\n    # [START howto_operator_ec2_create_instance]\n    create_instance = EC2CreateInstanceOperator(\n        task_id=\"create_instance\",\n        image_id=image_id,\n        max_count=1,\n        min_count=1,\n        config=config,\n    )\n    # [END howto_operator_ec2_create_instance]\n    create_instance.wait_for_completion = True\n    instance_id = parse_response(create_instance.output)\n    # [START howto_operator_ec2_stop_instance]\n    stop_instance = EC2StopInstanceOperator(\n        task_id=\"stop_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_stop_instance]\n    stop_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ec2_start_instance]\n    start_instance = EC2StartInstanceOperator(\n        task_id=\"start_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_start_instance]\n\n    # [START howto_sensor_ec2_instance_state]\n    await_instance = EC2InstanceStateSensor(\n        task_id=\"await_instance\",\n        instance_id=instance_id,\n        target_state=\"running\",\n    )\n    # [END howto_sensor_ec2_instance_state]\n\n    # [START howto_operator_ec2_terminate_instance]\n    terminate_instance = EC2TerminateInstanceOperator(\n        task_id=\"terminate_instance\",\n        instance_ids=instance_id,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_ec2_terminate_instance]\n    terminate_instance.trigger_rule = TriggerRule.ALL_DONE\n    chain(\n        # TEST SETUP\n        test_context,\n        key_name,\n        image_id,\n        # TEST BODY\n        create_instance,\n        instance_id,\n        stop_instance,\n        start_instance,\n        await_instance,\n        terminate_instance,\n        # TEST TEARDOWN\n        delete_key_pair(key_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.986986", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 194, "file_name": "example_dynamodb_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom tenacity import before_log, before_sleep_log\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_dynamodb_to_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": \"ID\", \"AttributeType\": \"S\"},\n    {\"AttributeName\": \"Value\", \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"ID\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"Value\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 1, \"WriteCapacityUnits\": 1}\nS3_KEY_PREFIX = \"dynamodb-segmented-file\"\n\n\n# UpdateContinuousBackups API might need multiple attempts to succeed\n# Sometimes the API returns the error \"Backups are being enabled for the table: <...>. Please retry later\"\n# Using a retry strategy with exponential backoff to remediate that\n@tenacity.retry(\n    stop=tenacity.stop_after_attempt(20),\n    wait=tenacity.wait_exponential(min=5),\n    before=before_log(log, logging.INFO),\n    before_sleep=before_sleep_log(log, logging.WARNING),\n)\ndef enable_point_in_time_recovery(table_name: str):\n    boto3.client(\"dynamodb\").update_continuous_backups(\n        TableName=table_name,\n        PointInTimeRecoverySpecification={\n            \"PointInTimeRecoveryEnabled\": True,\n        },\n    )\n\n\n@task\ndef set_up_table(table_name: str):\n    dynamo_resource = boto3.resource(\"dynamodb\")\n    table = dynamo_resource.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n    enable_point_in_time_recovery(table_name)\n    table.put_item(Item={\"ID\": \"123\", \"Value\": \"Testing\"})\n\n\n@task\ndef get_export_time(table_name: str):\n    r = boto3.client(\"dynamodb\").describe_continuous_backups(\n        TableName=table_name,\n    )\n\n    return r[\"ContinuousBackupsDescription\"][\"PointInTimeRecoveryDescription\"][\"EarliestRestorableDateTime\"]\n\n\n@task\ndef wait_for_bucket(s3_bucket_name):\n    waiter = boto3.client(\"s3\").get_waiter(\"bucket_exists\")\n    waiter.wait(Bucket=s3_bucket_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dynamodb_table(table_name: str):\n    boto3.resource(\"dynamodb\").Table(table_name).delete()\n    boto3.client(\"dynamodb\").get_waiter(\"table_not_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    bucket_name = f\"{env_id}-dynamodb-bucket\"\n\n    create_table = set_up_table(table_name=table_name)\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    # [START howto_transfer_dynamodb_to_s3]\n    backup_db = DynamoDBToS3Operator(\n        task_id=\"backup_db\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=20,\n    )\n    # [END howto_transfer_dynamodb_to_s3]\n\n    # [START howto_transfer_dynamodb_to_s3_segmented]\n    # Segmenting allows the transfer to be parallelized into {segment} number of parallel tasks.\n    backup_db_segment_1 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_1\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-1-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 0,\n        },\n    )\n\n    backup_db_segment_2 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_2\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-2-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 1,\n        },\n    )\n    # [END howto_transfer_dynamodb_to_s3_segmented]\n\n    export_time = get_export_time(table_name)\n    # [START howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n    backup_db_to_point_in_time = DynamoDBToS3Operator(\n        task_id=\"backup_db_to_point_in_time\",\n        dynamodb_table_name=table_name,\n        file_size=1000,\n        s3_bucket_name=bucket_name,\n        export_time=export_time,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-3-\",\n    )\n    # [END howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n\n    delete_table = delete_dynamodb_table(table_name=table_name)\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        create_bucket,\n        wait_for_bucket(s3_bucket_name=bucket_name),\n        # TEST BODY\n        backup_db,\n        backup_db_segment_1,\n        backup_db_segment_2,\n        export_time,\n        backup_db_to_point_in_time,\n        # TEST TEARDOWN\n        delete_table,\n        delete_bucket,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:14.989890", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 153, "file_name": "example_ecs_fargate.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsTaskStates\nfrom airflow.providers.amazon.aws.operators.ecs import EcsRunTaskOperator\nfrom airflow.providers.amazon.aws.sensors.ecs import EcsTaskStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs_fargate\"\n\n# Externally fetched variables:\nSUBNETS_KEY = \"SUBNETS\"  # At least one public subnet is required.\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .add_variable(SECURITY_GROUPS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef create_cluster(cluster_name: str) -> None:\n    \"\"\"Creates an ECS cluster.\"\"\"\n    boto3.client(\"ecs\").create_cluster(clusterName=cluster_name)\n\n\n@task\ndef register_task_definition(task_name: str, container_name: str) -> str:\n    \"\"\"Creates a Task Definition.\"\"\"\n    response = boto3.client(\"ecs\").register_task_definition(\n        family=task_name,\n        # CPU and Memory are required for Fargate and are set to the lowest currently allowed values.\n        cpu=\"256\",\n        memory=\"512\",\n        containerDefinitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n            }\n        ],\n        requiresCompatibilities=[\"FARGATE\"],\n        networkMode=\"awsvpc\",\n    )\n\n    return response[\"taskDefinition\"][\"taskDefinitionArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_definition(task_definition_arn: str) -> None:\n    \"\"\"Deletes the Task Definition.\"\"\"\n    boto3.client(\"ecs\").deregister_task_definition(taskDefinition=task_definition_arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster(cluster_name: str) -> None:\n    \"\"\"Deletes the ECS cluster.\"\"\"\n    boto3.client(\"ecs\").delete_cluster(cluster=cluster_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-test-cluster\"\n    container_name = f\"{env_id}-test-container\"\n    task_definition_name = f\"{env_id}-test-definition\"\n\n    create_task_definition = register_task_definition(task_definition_name, container_name)\n\n    # [START howto_operator_ecs]\n    hello_world = EcsRunTaskOperator(\n        task_id=\"hello_world\",\n        cluster=cluster_name,\n        task_definition=task_definition_name,\n        launch_type=\"FARGATE\",\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo\", \"hello\", \"world\"],\n                },\n            ],\n        },\n        network_configuration={\n            \"awsvpcConfiguration\": {\n                \"subnets\": test_context[SUBNETS_KEY],\n                \"securityGroups\": test_context[SECURITY_GROUPS_KEY],\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        # You must set `reattach=True` in order to get ecs_task_arn if you plan to use a Sensor.\n        reattach=True,\n    )\n    # [END howto_operator_ecs]\n\n    # EcsRunTaskOperator waits by default, setting as False to test the Sensor below.\n    hello_world.wait_for_completion = False\n\n    # [START howto_sensor_ecs_task_state]\n    # By default, EcsTaskStateSensor waits until the task has started, but the\n    # demo task runs so fast that the sensor misses it.  This sensor instead\n    # demonstrates how to wait until the ECS Task has completed by providing\n    # the target_state and failure_states parameters.\n    await_task_finish = EcsTaskStateSensor(\n        task_id=\"await_task_finish\",\n        cluster=cluster_name,\n        task=hello_world.output[\"ecs_task_arn\"],\n        target_state=EcsTaskStates.STOPPED,\n        failure_states={EcsTaskStates.NONE},\n    )\n    # [END howto_sensor_ecs_task_state]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_cluster(cluster_name),\n        create_task_definition,\n        # TEST BODY\n        hello_world,\n        await_task_finish,\n        # TEST TEARDOWN\n        delete_task_definition(create_task_definition),\n        delete_cluster(cluster_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.010132", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 146, "file_name": "example_eks_templated.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_eks_templated\"\n\n# Example Jinja Template format, substitute your values:\n# {\n#     \"cluster_name\": \"templated-cluster\",\n#     \"cluster_role_arn\": \"arn:aws:iam::123456789012:role/role_name\",\n#     \"resources_vpc_config\": {\n#         \"subnetIds\": [\"subnet-12345ab\", \"subnet-67890cd\"],\n#         \"endpointPublicAccess\": true,\n#         \"endpointPrivateAccess\": false\n#     },\n#     \"nodegroup_name\": \"templated-nodegroup\",\n#     \"nodegroup_subnets\": \"['subnet-12345ab', 'subnet-67890cd']\",\n#     \"nodegroup_role_arn\": \"arn:aws:iam::123456789012:role/role_name\"\n# }\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"templated\"],\n    catchup=False,\n    # render_template_as_native_obj=True is what converts the Jinja to Python objects, instead of a string.\n    render_template_as_native_obj=True,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    CLUSTER_NAME = \"{{ dag_run.conf['cluster_name'] }}\"\n    NODEGROUP_NAME = \"{{ dag_run.conf['nodegroup_name'] }}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        compute=None,\n        cluster_role_arn=\"{{ dag_run.conf['cluster_role_arn'] }}\",\n        # This only works with render_template_as_native_obj flag (this dag has it set)\n        resources_vpc_config=\"{{ dag_run.conf['resources_vpc_config'] }}\",  # type: ignore[arg-type]\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        nodegroup_subnets=\"{{ dag_run.conf['nodegroup_subnets'] }}\",\n        nodegroup_role_arn=\"{{ dag_run.conf['nodegroup_role_arn'] }}\",\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_create_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=CLUSTER_NAME,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"ls\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Delete the pod when it reaches its final state, or the execution is interrupted.\n        on_finish_action=\"delete_pod\",\n    )\n\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n    )\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_delete_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.NONEXISTENT,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        delete_nodegroup,\n        await_delete_nodegroup,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.013467", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 208, "file_name": "example_ecs.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates\nfrom airflow.providers.amazon.aws.operators.ecs import (\n    EcsCreateClusterOperator,\n    EcsDeleteClusterOperator,\n    EcsDeregisterTaskDefinitionOperator,\n    EcsRegisterTaskDefinitionOperator,\n    EcsRunTaskOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ecs import (\n    EcsClusterStateSensor,\n    EcsTaskDefinitionStateSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs\"\n\n# Externally fetched variables:\nEXISTING_CLUSTER_NAME_KEY = \"CLUSTER_NAME\"\nEXISTING_CLUSTER_SUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    # NOTE:  Creating a functional ECS Cluster which uses EC2 requires manually creating\n    # and configuring a number of resources such as autoscaling groups, networking\n    # etc. which is out of scope for this demo and time-consuming for a system test\n    # To simplify this demo and make it run in a reasonable length of time as a\n    # system test, follow the steps below to create a new cluster on the AWS Console\n    # which handles all asset creation and configuration using default values:\n    # 1. https://us-east-1.console.aws.amazon.com/ecs/home?region=us-east-1#/clusters\n    # 2. Select \"EC2 Linux + Networking\" and hit \"Next\"\n    # 3. Name your cluster in the first field and click Create\n    .add_variable(EXISTING_CLUSTER_NAME_KEY)\n    .add_variable(EXISTING_CLUSTER_SUBNETS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef get_region():\n    return boto3.session.Session().region_name\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_logs(group_name: str):\n    client = boto3.client(\"logs\")\n    client.delete_log_group(logGroupName=group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    existing_cluster_name = test_context[EXISTING_CLUSTER_NAME_KEY]\n    existing_cluster_subnets = test_context[EXISTING_CLUSTER_SUBNETS_KEY]\n\n    new_cluster_name = f\"{env_id}-cluster\"\n    container_name = f\"{env_id}-container\"\n    family_name = f\"{env_id}-task-definition\"\n    asg_name = f\"{env_id}-asg\"\n\n    aws_region = get_region()\n    log_group_name = f\"/ecs_test/{env_id}\"\n\n    # [START howto_operator_ecs_create_cluster]\n    create_cluster = EcsCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_create_cluster]\n\n    # EcsCreateClusterOperator waits by default, setting as False to test the Sensor below.\n    create_cluster.wait_for_completion = False\n\n    # [START howto_sensor_ecs_cluster_state]\n    await_cluster = EcsClusterStateSensor(\n        task_id=\"await_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_sensor_ecs_cluster_state]\n\n    # [START howto_operator_ecs_register_task_definition]\n    register_task = EcsRegisterTaskDefinitionOperator(\n        task_id=\"register_task\",\n        family=family_name,\n        container_definitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": log_group_name,\n                        \"awslogs-region\": aws_region,\n                        \"awslogs-create-group\": \"true\",\n                        \"awslogs-stream-prefix\": \"ecs\",\n                    },\n                },\n            },\n        ],\n        register_task_kwargs={\n            \"cpu\": \"256\",\n            \"memory\": \"512\",\n            \"networkMode\": \"awsvpc\",\n        },\n    )\n    # [END howto_operator_ecs_register_task_definition]\n\n    # [START howto_sensor_ecs_task_definition_state]\n    await_task_definition = EcsTaskDefinitionStateSensor(\n        task_id=\"await_task_definition\",\n        task_definition=register_task.output,\n    )\n    # [END howto_sensor_ecs_task_definition_state]\n\n    # [START howto_operator_ecs_run_task]\n    run_task = EcsRunTaskOperator(\n        task_id=\"run_task\",\n        cluster=existing_cluster_name,\n        task_definition=register_task.output,\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo hello world\"],\n                },\n            ],\n        },\n        network_configuration={\"awsvpcConfiguration\": {\"subnets\": existing_cluster_subnets}},\n        # [START howto_awslogs_ecs]\n        awslogs_group=log_group_name,\n        awslogs_region=aws_region,\n        awslogs_stream_prefix=f\"ecs/{container_name}\",\n        # [END howto_awslogs_ecs]\n    )\n    # [END howto_operator_ecs_run_task]\n\n    # [START howto_operator_ecs_deregister_task_definition]\n    deregister_task = EcsDeregisterTaskDefinitionOperator(\n        task_id=\"deregister_task\",\n        task_definition=register_task.output,\n    )\n    # [END howto_operator_ecs_deregister_task_definition]\n    deregister_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ecs_delete_cluster]\n    delete_cluster = EcsDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # EcsDeleteClusterOperator waits by default, setting as False to test the Sensor below.\n    delete_cluster.wait_for_completion = False\n\n    # [START howto_operator_ecs_delete_cluster]\n    await_delete_cluster = EcsClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        cluster_name=new_cluster_name,\n        target_state=EcsClusterStates.INACTIVE,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        aws_region,\n        # TEST BODY\n        create_cluster,\n        await_cluster,\n        register_task,\n        await_task_definition,\n        run_task,\n        deregister_task,\n        delete_cluster,\n        await_delete_cluster,\n        clean_logs(log_group_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.016614", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 137, "file_name": "example_eks_with_fargate_in_one_step.py", "included_files": ["__init__.py", "k8s.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_in_one_step\"\n\n# Externally fetched variables\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # [START howto_operator_eks_create_cluster_with_fargate_profile]\n    # Create an Amazon EKS cluster control plane and an AWS Fargate compute platform in one step.\n    create_cluster_and_fargate_profile = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster_and_fargate_profile\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=\"fargate\",\n        fargate_profile_name=fargate_profile_name,\n        # Opting to use the same ARN for the cluster and the pod here,\n        # but a different ARN could be configured and passed if desired.\n        fargate_pod_execution_role_arn=fargate_pod_role_arn,\n    )\n    # [END howto_operator_eks_create_cluster_with_fargate_profile]\n\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"await_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        pod_name=\"run_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        startup_timeout_seconds=600,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_cluster_and_fargate_profile = EksDeleteClusterOperator(\n        task_id=\"delete_fargate_profile_and_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster_and_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_cluster_and_fargate_profile,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.022304", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 196, "file_name": "example_eks_with_nodegroups.py", "included_files": ["__init__.py", "k8s.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\nDAG_ID = \"example_eks_with_nodegroups\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster]\n    # Create an Amazon EKS Cluster control plane without attaching compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        compute=None,\n    )\n    # [END howto_operator_eks_create_cluster]\n\n    # [START howto_sensor_eks_cluster]\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"await_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_cluster]\n\n    # [START howto_operator_eks_create_nodegroup]\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        nodegroup_subnets=test_context[SUBNETS_KEY],\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n    )\n    # [END howto_operator_eks_create_nodegroup]\n    # The launch template enforces IMDSv2 and is required for internal compliance\n    # when running these system tests on AWS infrastructure.  It is not required\n    # for the operator to work, so I'm placing it outside the demo snippet.\n    create_nodegroup.create_nodegroup_kwargs = {\"launchTemplate\": {\"name\": launch_template_name}}\n\n    # [START howto_sensor_eks_nodegroup]\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_nodegroup]\n    await_create_nodegroup.poke_interval = 10\n\n    # [START howto_operator_eks_pod_operator]\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n    )\n    # [END howto_operator_eks_pod_operator]\n    # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n    start_pod.is_delete_operator_pod = False\n\n    # In this specific situation we want to keep the pod to be able to describe it,\n    # it is cleaned anyway with the cluster later on.\n    start_pod.is_delete_operator_pod = False\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_nodegroup]\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n    )\n    # [END howto_operator_eks_delete_nodegroup]\n    delete_nodegroup.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_delete_nodegroup\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    # [START howto_operator_eks_delete_cluster]\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=cluster_name,\n    )\n    # [END howto_operator_eks_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup,  # part of the test AND teardown\n        await_delete_nodegroup,\n        delete_cluster,  # part of the test AND teardown\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.040208", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 151, "file_name": "example_eks_with_nodegroup_in_one_step.py", "included_files": ["__init__.py", "k8s.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_nodegroup_in_one_step\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster_with_nodegroup]\n    # Create an Amazon EKS cluster control plane and an EKS nodegroup compute platform in one step.\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        # ``compute='nodegroup'`` is the default, explicitly set here for demo purposes.\n        compute=\"nodegroup\",\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n    # [END howto_operator_eks_create_cluster_with_nodegroup]\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_force_delete_cluster]\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_nodegroup_and_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_nodegroup_and_cluster\",\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n    # [END howto_operator_eks_force_delete_cluster]\n    delete_nodegroup_and_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup_and_cluster,\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.043224", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 173, "file_name": "example_eks_with_fargate_profile.py", "included_files": ["__init__.py", "k8s.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateFargateProfileOperator,\n    EksDeleteClusterOperator,\n    EksDeleteFargateProfileOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_profile\"\n\n# Externally fetched variables:\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nSELECTORS = [{\"namespace\": \"default\"}]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=None,\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    # [START howto_operator_eks_create_fargate_profile]\n    create_fargate_profile = EksCreateFargateProfileOperator(\n        task_id=\"create_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        pod_execution_role_arn=fargate_pod_role_arn,\n        fargate_profile_name=fargate_profile_name,\n        selectors=SELECTORS,\n    )\n    # [END howto_operator_eks_create_fargate_profile]\n\n    # [START howto_sensor_eks_fargate]\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_fargate]\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=cluster_name,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n        startup_timeout_seconds=200,\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile = EksDeleteFargateProfileOperator(\n        task_id=\"delete_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n    )\n    # [END howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_delete_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=cluster_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEARDOWN\n        describe_pod,\n        delete_fargate_profile,  # part of the test AND teardown\n        await_delete_fargate_profile,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.045725", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 214, "file_name": "example_emr.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyClusterOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrJobFlowSensor, EmrStepSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr\"\nCONFIG_NAME = \"EMR Runtime Role Security Configuration\"\nEXECUTION_ROLE_ARN_KEY = \"EXECUTION_ROLE_ARN\"\n\nSECURITY_CONFIGURATION = {\n    \"AuthorizationConfiguration\": {\n        \"IAMConfiguration\": {\n            \"EnableApplicationScopedIAMRole\": True,\n        },\n    },\n    # Use IMDSv2 for greater security, see the following doc for more details:\n    # https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html\n    \"InstanceMetadataServiceConfiguration\": {\n        \"MinimumInstanceMetadataServiceVersion\": 2,\n        \"HttpPutResponseHopLimit\": 2,\n    },\n}\n\n# [START howto_operator_emr_steps_config]\nSPARK_STEPS = [\n    {\n        \"Name\": \"calculate_pi\",\n        \"ActionOnFailure\": \"CONTINUE\",\n        \"HadoopJarStep\": {\n            \"Jar\": \"command-runner.jar\",\n            \"Args\": [\"/usr/lib/spark/bin/run-example\", \"SparkPi\", \"10\"],\n        },\n    }\n]\n\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"PiCalc\",\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"InstanceCount\": 1,\n            },\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": SPARK_STEPS,\n    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n    \"ServiceRole\": \"EMR_DefaultRole\",\n}\n# [END howto_operator_emr_steps_config]\n\n\n@task\ndef get_ami_id():\n    \"\"\"\n    Returns an AL2 AMI compatible with EMR\n    \"\"\"\n    return SsmHook(aws_conn_id=None).get_parameter_value(\n        \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-ebs\"\n    )\n\n\n@task\ndef configure_security_config(config_name: str):\n    boto3.client(\"emr\").create_security_configuration(\n        Name=config_name,\n        SecurityConfiguration=json.dumps(SECURITY_CONFIGURATION),\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_config(config_name: str):\n    boto3.client(\"emr\").delete_security_configuration(\n        Name=config_name,\n    )\n\n\n@task\ndef get_step_id(step_ids: list):\n    return step_ids[0]\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    config_name = f\"{CONFIG_NAME}-{env_id}\"\n    execution_role_arn = test_context[EXECUTION_ROLE_ARN_KEY]\n    s3_bucket = f\"{env_id}-emr-bucket\"\n\n    JOB_FLOW_OVERRIDES[\"LogUri\"] = f\"s3://{s3_bucket}/\"\n    JOB_FLOW_OVERRIDES[\"SecurityConfiguration\"] = config_name\n    JOB_FLOW_OVERRIDES[\"Instances\"][\"InstanceGroups\"][0][\"CustomAmiId\"] = get_ami_id()\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    create_security_configuration = configure_security_config(config_name)\n\n    # [START howto_operator_emr_create_job_flow]\n    create_job_flow = EmrCreateJobFlowOperator(\n        task_id=\"create_job_flow\",\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n    )\n    # [END howto_operator_emr_create_job_flow]\n\n    # [START howto_operator_emr_modify_cluster]\n    modify_cluster = EmrModifyClusterOperator(\n        task_id=\"modify_cluster\", cluster_id=create_job_flow.output, step_concurrency_level=1\n    )\n    # [END howto_operator_emr_modify_cluster]\n\n    # [START howto_operator_emr_add_steps]\n    add_steps = EmrAddStepsOperator(\n        task_id=\"add_steps\",\n        job_flow_id=create_job_flow.output,\n        steps=SPARK_STEPS,\n        execution_role_arn=execution_role_arn,\n    )\n    # [END howto_operator_emr_add_steps]\n    add_steps.wait_for_completion = True\n    # On rare occasion (1 in 50ish?) this system test times out.  Extending the\n    # max_attempts from the default 60 to attempt to mitigate the flaky test.\n    add_steps.waiter_max_attempts = 90\n\n    # [START howto_sensor_emr_step]\n    wait_for_step = EmrStepSensor(\n        task_id=\"wait_for_step\",\n        job_flow_id=create_job_flow.output,\n        step_id=get_step_id(add_steps.output),\n    )\n    # [END howto_sensor_emr_step]\n\n    # [START howto_operator_emr_terminate_job_flow]\n    remove_cluster = EmrTerminateJobFlowOperator(\n        task_id=\"remove_cluster\",\n        job_flow_id=create_job_flow.output,\n    )\n    # [END howto_operator_emr_terminate_job_flow]\n    remove_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_emr_job_flow]\n    check_job_flow = EmrJobFlowSensor(task_id=\"check_job_flow\", job_flow_id=create_job_flow.output)\n    # [END howto_sensor_emr_job_flow]\n    check_job_flow.poke_interval = 10\n\n    delete_security_configuration = delete_security_config(config_name)\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        create_security_configuration,\n        # TEST BODY\n        create_job_flow,\n        modify_cluster,\n        add_steps,\n        wait_for_step,\n        # TEST TEARDOWN\n        remove_cluster,\n        check_job_flow,\n        delete_security_configuration,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.050295", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 334, "file_name": "example_emr_eks.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import EksCreateClusterOperator, EksDeleteClusterOperator\nfrom airflow.providers.amazon.aws.operators.emr import EmrContainerOperator, EmrEksCreateClusterOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.providers.amazon.aws.sensors.emr import EmrContainerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_eks\"\n\n# Externally fetched variables\nROLE_ARN_KEY = \"ROLE_ARN\"\nJOB_ROLE_ARN_KEY = \"JOB_ROLE_ARN\"\nJOB_ROLE_NAME_KEY = \"JOB_ROLE_NAME\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_NAME_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nS3_FILE_NAME = \"pi.py\"\nS3_FILE_CONTENT = \"\"\"\nk = 1\ns = 0\n\nfor i in range(1000000):\n    if i % 2 == 0:\n        s += 4/k\n    else:\n        s -= 4/k\n\n    k += 2\n\nprint(s)\n\"\"\"\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\n@task\ndef enable_access_emr_on_eks(cluster, ns):\n    # Install eksctl and enable access for EMR on EKS\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-cluster-access.html\n    file = \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\"\n    commands = f\"\"\"\n        curl --silent --location \"{file}\" | tar xz -C /tmp &&\n        sudo mv /tmp/eksctl /usr/local/bin &&\n        eksctl create iamidentitymapping --cluster {cluster} --namespace {ns} --service-name \"emr-containers\"\n    \"\"\"\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef create_iam_oidc_identity_provider(cluster_name):\n    # Create an IAM OIDC identity provider\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html\n    command = f\"eksctl utils associate-iam-oidc-provider --cluster {cluster_name} --approve\"\n\n    build = subprocess.Popen(\n        command,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef delete_iam_oidc_identity_provider(cluster_name):\n    oidc_provider_issuer_url = boto3.client(\"eks\").describe_cluster(name=cluster_name,)[\"cluster\"][\n        \"identity\"\n    ][\"oidc\"][\"issuer\"]\n    oidc_provider_issuer_endpoint = oidc_provider_issuer_url.replace(\"https://\", \"\")\n\n    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n    boto3.client(\"iam\").delete_open_id_connect_provider(\n        OpenIDConnectProviderArn=f\"arn:aws:iam::{account_id}:oidc-provider/{oidc_provider_issuer_endpoint}\"\n    )\n\n\n@task\ndef update_trust_policy_execution_role(cluster_name, cluster_namespace, role_name):\n    # Remove any already existing trusted entities added with \"update-role-trust-policy\"\n    # Prevent getting an error \"Cannot exceed quota for ACLSizePerRole\"\n    client = boto3.client(\"iam\")\n    role_trust_policy = client.get_role(RoleName=role_name)[\"Role\"][\"AssumeRolePolicyDocument\"]\n    # We assume if the action is sts:AssumeRoleWithWebIdentity, the statement had been added with\n    # \"update-role-trust-policy\". Removing it to not exceed the quota\n    role_trust_policy[\"Statement\"] = list(\n        filter(\n            lambda statement: statement[\"Action\"] != \"sts:AssumeRoleWithWebIdentity\",\n            role_trust_policy[\"Statement\"],\n        )\n    )\n\n    client.update_assume_role_policy(\n        RoleName=role_name,\n        PolicyDocument=json.dumps(role_trust_policy),\n    )\n\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-trust-policy.html\n    # The action \"update-role-trust-policy\" is not available in boto3, thus we need to do it using AWS CLI\n    commands = (\n        f\"aws emr-containers update-role-trust-policy --cluster-name {cluster_name} \"\n        f\"--namespace {cluster_namespace} --role-name {role_name}\"\n    )\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_virtual_cluster(virtual_cluster_id):\n    boto3.client(\"emr-containers\").delete_virtual_cluster(\n        id=virtual_cluster_id,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n    job_role_arn = test_context[JOB_ROLE_ARN_KEY]\n    job_role_name = test_context[JOB_ROLE_NAME_KEY]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    eks_cluster_name = f\"{env_id}-cluster\"\n    virtual_cluster_name = f\"{env_id}-virtual-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    eks_namespace = \"default\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_emr_eks_config]\n    job_driver_arg = {\n        \"sparkSubmitJobDriver\": {\n            \"entryPoint\": f\"s3://{s3_bucket_name}/{S3_FILE_NAME}\",\n            \"sparkSubmitParameters\": \"--conf spark.executors.instances=2 --conf spark.executors.memory=2G \"\n            \"--conf spark.executor.cores=2 --conf spark.driver.cores=1\",\n        }\n    }\n\n    configuration_overrides_arg = {\n        \"monitoringConfiguration\": {\n            \"cloudWatchMonitoringConfiguration\": {\n                \"logGroupName\": \"/emr-eks-jobs\",\n                \"logStreamNamePrefix\": \"airflow\",\n            }\n        },\n    }\n    # [END howto_operator_emr_eks_config]\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    upload_s3_file = S3CreateObjectOperator(\n        task_id=\"upload_s3_file\",\n        s3_bucket=s3_bucket_name,\n        s3_key=S3_FILE_NAME,\n        data=S3_FILE_CONTENT,\n    )\n\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=role_arn,\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=role_arn,\n        resources_vpc_config={\"subnetIds\": subnets},\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    # [START howto_operator_emr_eks_create_cluster]\n    create_emr_eks_cluster = EmrEksCreateClusterOperator(\n        task_id=\"create_emr_eks_cluster\",\n        virtual_cluster_name=virtual_cluster_name,\n        eks_cluster_name=eks_cluster_name,\n        eks_namespace=eks_namespace,\n    )\n    # [END howto_operator_emr_eks_create_cluster]\n\n    # [START howto_operator_emr_container]\n    job_starter = EmrContainerOperator(\n        task_id=\"start_job\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        execution_role_arn=job_role_arn,\n        release_label=\"emr-6.3.0-latest\",\n        job_driver=job_driver_arg,\n        configuration_overrides=configuration_overrides_arg,\n        name=\"pi.py\",\n    )\n    # [END howto_operator_emr_container]\n    job_starter.wait_for_completion = False\n\n    # [START howto_sensor_emr_container]\n    job_waiter = EmrContainerSensor(\n        task_id=\"job_waiter\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        job_id=str(job_starter.output),\n    )\n    # [END howto_sensor_emr_container]\n\n    delete_eks_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        force_delete_compute=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_eks_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_s3_file,\n        create_launch_template(launch_template_name),\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        enable_access_emr_on_eks(eks_cluster_name, eks_namespace),\n        create_iam_oidc_identity_provider(eks_cluster_name),\n        update_trust_policy_execution_role(eks_cluster_name, eks_namespace, job_role_name),\n        # TEST BODY\n        create_emr_eks_cluster,\n        job_starter,\n        job_waiter,\n        # TEST TEARDOWN\n        delete_iam_oidc_identity_provider(eks_cluster_name),\n        delete_virtual_cluster(str(create_emr_eks_cluster.output)),\n        delete_eks_cluster,\n        await_delete_eks_cluster,\n        delete_launch_template(launch_template_name),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.074635", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 55, "file_name": "example_eventbridge.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.eventbridge import (\n    EventBridgePutEventsOperator,\n    EventBridgePutRuleOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eventbridge\"\nENTRIES = [\n    {\n        \"Detail\": '{\"event-name\": \"custom-event\"}',\n        \"EventBusName\": \"custom-bus\",\n        \"Source\": \"example.myapp\",\n        \"DetailType\": \"Sample Custom Event\",\n    }\n]\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n\n    # [START howto_operator_eventbridge_put_events]\n    put_events = EventBridgePutEventsOperator(task_id=\"put_events_task\", entries=ENTRIES)\n    # [END howto_operator_eventbridge_put_events]\n\n    # [START howto_operator_eventbridge_put_rule]\n    put_rule = EventBridgePutRuleOperator(\n        task_id=\"put_rule_task\",\n        name=\"example_rule\",\n        event_pattern='{\"source\": [\"example.myapp\"]}',\n        description=\"This rule matches events from example.myapp.\",\n    )\n    # [END howto_operator_eventbridge_put_rule]\n\n    chain(test_context, put_events, put_rule)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.076954", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 105, "file_name": "example_emr_notebook_execution.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrStartNotebookExecutionOperator,\n    EmrStopNotebookExecutionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.emr import EmrNotebookExecutionSensor\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_notebook\"\n# Externally fetched variables:\nEDITOR_ID_KEY = \"EDITOR_ID\"\nCLUSTER_ID_KEY = \"CLUSTER_ID\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(EDITOR_ID_KEY).add_variable(CLUSTER_ID_KEY).build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    editor_id = test_context[EDITOR_ID_KEY]\n    cluster_id = test_context[CLUSTER_ID_KEY]\n\n    # [START howto_operator_emr_start_notebook_execution]\n    start_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"start_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    # [END howto_operator_emr_start_notebook_execution]\n\n    notebook_execution_id_1 = start_execution.output\n\n    # [START howto_sensor_emr_notebook_execution]\n    wait_for_execution_start = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_start\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"RUNNING\"},\n        poke_interval=5,\n    )\n    # [END howto_sensor_emr_notebook_execution]\n\n    # [START howto_operator_emr_stop_notebook_execution]\n    stop_execution = EmrStopNotebookExecutionOperator(\n        task_id=\"stop_execution\",\n        notebook_execution_id=notebook_execution_id_1,\n    )\n    # [END howto_operator_emr_stop_notebook_execution]\n\n    wait_for_execution_stop = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_stop\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"STOPPED\"},\n        poke_interval=5,\n    )\n    finish_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"finish_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    notebook_execution_id_2 = finish_execution.output\n    wait_for_execution_finish = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_finish\",\n        notebook_execution_id=notebook_execution_id_2,\n        poke_interval=5,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        start_execution,\n        wait_for_execution_start,\n        stop_execution,\n        wait_for_execution_stop,\n        finish_execution,\n        # TEST TEARDOWN\n        wait_for_execution_finish,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.079001", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 149, "file_name": "example_emr_serverless.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrServerlessCreateApplicationOperator,\n    EmrServerlessDeleteApplicationOperator,\n    EmrServerlessStartJobOperator,\n    EmrServerlessStopApplicationOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrServerlessApplicationSensor, EmrServerlessJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_serverless\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"{env_id}-emr-serverless-bucket\"\n    region = boto3.session.Session().region_name\n    entryPoint = f\"s3://{region}.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py\"\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    SPARK_JOB_DRIVER = {\n        \"sparkSubmit\": {\n            \"entryPoint\": entryPoint,\n            \"entryPointArguments\": [f\"s3://{bucket_name}/output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g\\\n                --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\",\n        }\n    }\n\n    SPARK_CONFIGURATION_OVERRIDES = {\n        \"monitoringConfiguration\": {\"s3MonitoringConfiguration\": {\"logUri\": f\"s3://{bucket_name}/logs\"}}\n    }\n\n    # [START howto_operator_emr_serverless_create_application]\n    emr_serverless_app = EmrServerlessCreateApplicationOperator(\n        task_id=\"create_emr_serverless_task\",\n        release_label=\"emr-6.6.0\",\n        job_type=\"SPARK\",\n        config={\"name\": \"new_application\"},\n    )\n    # [END howto_operator_emr_serverless_create_application]\n\n    # EmrServerlessCreateApplicationOperator waits by default, setting as False to test the Sensor below.\n    emr_serverless_app.wait_for_completion = False\n\n    emr_serverless_app_id = emr_serverless_app.output\n\n    # [START howto_sensor_emr_serverless_application]\n    wait_for_app_creation = EmrServerlessApplicationSensor(\n        task_id=\"wait_for_app_creation\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_sensor_emr_serverless_application]\n    wait_for_app_creation.poke_interval = 1\n\n    # [START howto_operator_emr_serverless_start_job]\n    start_job = EmrServerlessStartJobOperator(\n        task_id=\"start_emr_serverless_job\",\n        application_id=emr_serverless_app_id,\n        execution_role_arn=role_arn,\n        job_driver=SPARK_JOB_DRIVER,\n        configuration_overrides=SPARK_CONFIGURATION_OVERRIDES,\n    )\n    # [END howto_operator_emr_serverless_start_job]\n    start_job.wait_for_completion = False\n\n    # [START howto_sensor_emr_serverless_job]\n    wait_for_job = EmrServerlessJobSensor(\n        task_id=\"wait_for_job\",\n        application_id=emr_serverless_app_id,\n        job_run_id=start_job.output,\n        # the default is to wait for job completion, here we just wait for the job to be running.\n        target_states={\"RUNNING\"},\n    )\n    # [END howto_sensor_emr_serverless_job]\n    wait_for_job.poke_interval = 10\n\n    # [START howto_operator_emr_serverless_stop_application]\n    stop_app = EmrServerlessStopApplicationOperator(\n        task_id=\"stop_application\",\n        application_id=emr_serverless_app_id,\n        force_stop=True,\n    )\n    # [END howto_operator_emr_serverless_stop_application]\n    stop_app.waiter_check_interval_seconds = 1\n\n    # [START howto_operator_emr_serverless_delete_application]\n    delete_app = EmrServerlessDeleteApplicationOperator(\n        task_id=\"delete_application\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_operator_emr_serverless_delete_application]\n    delete_app.waiter_check_interval_seconds = 1\n    delete_app.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        emr_serverless_app,\n        wait_for_app_creation,\n        start_job,\n        wait_for_job,\n        stop_app,\n        # TEST TEARDOWN\n        delete_app,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.081576", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 110, "file_name": "example_gcs_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\n# Externally fetched variables:\nGCP_PROJECT_ID = \"GCP_PROJECT_ID\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(GCP_PROJECT_ID).build()\n\nDAG_ID = \"example_gcs_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    gcp_user_project = test_context[GCP_PROJECT_ID]\n\n    s3_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    s3_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    gcs_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    gcs_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_gcs_bucket = GCSCreateBucketOperator(\n        task_id=\"create_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        resource={\"billing\": {\"requesterPays\": True}},\n        project_id=gcp_user_project,\n    )\n\n    @task\n    def upload_gcs_file(bucket_name: str, object_name: str, user_project: str):\n        hook = GCSHook()\n        with hook.provide_file_and_upload(\n            bucket_name=bucket_name,\n            object_name=object_name,\n            user_project=user_project,\n        ) as temp_file:\n            temp_file.write(b\"test\")\n\n    # [START howto_transfer_gcs_to_s3]\n    gcs_to_s3 = GCSToS3Operator(\n        task_id=\"gcs_to_s3\",\n        bucket=gcs_bucket,\n        dest_s3_key=f\"s3://{s3_bucket}/{s3_key}\",\n        replace=True,\n        gcp_user_project=gcp_user_project,\n    )\n    # [END howto_transfer_gcs_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gcs_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        trigger_rule=TriggerRule.ALL_DONE,\n        user_project=gcp_user_project,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_gcs_bucket,\n        upload_gcs_file(gcs_bucket, gcs_key, gcp_user_project),\n        create_s3_bucket,\n        # TEST BODY\n        gcs_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_gcs_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.105451", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 103, "file_name": "example_glacier_to_gcs.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import task\nfrom airflow.providers.amazon.aws.operators.glacier import (\n    GlacierCreateJobOperator,\n    GlacierUploadArchiveOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glacier import GlacierJobOperationSensor\nfrom airflow.providers.amazon.aws.transfers.glacier_to_gcs import GlacierToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_glacier_to_gcs\"\n\n\n@task\ndef create_vault(vault_name):\n    boto3.client(\"glacier\").create_vault(vaultName=vault_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_vault(vault_name):\n    boto3.client(\"glacier\").delete_vault(vaultName=vault_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    vault_name = f\"{env_id}-vault\"\n    gcs_bucket_name = f\"{env_id}-bucket\"\n    gcs_object_name = f\"{env_id}-object\"\n\n    # [START howto_operator_glacier_create_job]\n    create_glacier_job = GlacierCreateJobOperator(task_id=\"create_glacier_job\", vault_name=vault_name)\n    JOB_ID = '{{ task_instance.xcom_pull(\"create_glacier_job\")[\"jobId\"] }}'\n    # [END howto_operator_glacier_create_job]\n\n    # [START howto_sensor_glacier_job_operation]\n    wait_for_operation_complete = GlacierJobOperationSensor(\n        vault_name=vault_name,\n        job_id=JOB_ID,\n        task_id=\"wait_for_operation_complete\",\n    )\n    # [END howto_sensor_glacier_job_operation]\n\n    # [START howto_operator_glacier_upload_archive]\n    upload_archive_to_glacier = GlacierUploadArchiveOperator(\n        task_id=\"upload_data_to_glacier\", vault_name=vault_name, body=b\"Test Data\"\n    )\n    # [END howto_operator_glacier_upload_archive]\n\n    # [START howto_transfer_glacier_to_gcs]\n    transfer_archive_to_gcs = GlacierToGCSOperator(\n        task_id=\"transfer_archive_to_gcs\",\n        vault_name=vault_name,\n        bucket_name=gcs_bucket_name,\n        object_name=gcs_object_name,\n        gzip=False,\n        # Override to match your needs\n        # If chunk size is bigger than actual file size\n        # then whole file will be downloaded\n        chunk_size=1024,\n    )\n    # [END howto_transfer_glacier_to_gcs]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_vault(vault_name),\n        # TEST BODY\n        create_glacier_job,\n        wait_for_operation_complete,\n        upload_archive_to_glacier,\n        transfer_archive_to_gcs,\n        # TEST TEARDOWN\n        delete_vault(vault_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.106184", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 67, "file_name": "example_ftp_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.ftp_to_s3 import FTPToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_ftp_to_s3\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-ftp-to-s3-bucket\"\n    s3_key = f\"{env_id}-ftp-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_ftp_to_s3]\n    ftp_to_s3_task = FTPToS3Operator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        replace=True,\n    )\n    # [END howto_transfer_ftp_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        ftp_to_s3_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.108775", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 198, "file_name": "example_glue.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.client import BaseClient\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.glue import GlueJobOperator\nfrom airflow.providers.amazon.aws.operators.glue_crawler import GlueCrawlerOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glue import GlueJobSensor\nfrom airflow.providers.amazon.aws.sensors.glue_crawler import GlueCrawlerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_glue\"\n\n# Externally fetched variables:\n# Role needs S3 putobject/getobject access as well as the glue service role,\n# see docs here: https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# Example csv data used as input to the example AWS Glue Job.\nEXAMPLE_CSV = \"\"\"\napple,0.5\nmilk,2.5\nbread,4.0\n\"\"\"\n\n# Example Spark script to operate on the above sample csv data.\nEXAMPLE_SCRIPT = \"\"\"\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nglueContext = GlueContext(SparkContext.getOrCreate())\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n             database='{db_name}', table_name='input')\nprint('There are %s items in the table' % datasource.count())\n\ndatasource.toDF().write.format('csv').mode(\"append\").save('s3://{bucket_name}/output')\n\"\"\"\n\n\n@task\ndef get_role_name(arn: str) -> str:\n    return arn.split(\"/\")[-1]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef glue_cleanup(crawler_name: str, job_name: str, db_name: str) -> None:\n    client: BaseClient = boto3.client(\"glue\")\n\n    client.delete_crawler(Name=crawler_name)\n    client.delete_job(JobName=job_name)\n    client.delete_database(Name=db_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    glue_crawler_name = f\"{env_id}_crawler\"\n    glue_db_name = f\"{env_id}_glue_db\"\n    glue_job_name = f\"{env_id}_glue_job\"\n    bucket_name = f\"{env_id}-bucket\"\n    role_name = get_role_name(role_arn)\n\n    glue_crawler_config = {\n        \"Name\": glue_crawler_name,\n        \"Role\": role_arn,\n        \"DatabaseName\": glue_db_name,\n        \"Targets\": {\"S3Targets\": [{\"Path\": f\"{bucket_name}/input\"}]},\n    }\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    upload_csv = S3CreateObjectOperator(\n        task_id=\"upload_csv\",\n        s3_bucket=bucket_name,\n        s3_key=\"input/input.csv\",\n        data=EXAMPLE_CSV,\n        replace=True,\n    )\n\n    upload_script = S3CreateObjectOperator(\n        task_id=\"upload_script\",\n        s3_bucket=bucket_name,\n        s3_key=\"etl_script.py\",\n        data=EXAMPLE_SCRIPT.format(db_name=glue_db_name, bucket_name=bucket_name),\n        replace=True,\n    )\n\n    # [START howto_operator_glue_crawler]\n    crawl_s3 = GlueCrawlerOperator(\n        task_id=\"crawl_s3\",\n        config=glue_crawler_config,\n    )\n    # [END howto_operator_glue_crawler]\n\n    # GlueCrawlerOperator waits by default, setting as False to test the Sensor below.\n    crawl_s3.wait_for_completion = False\n\n    # [START howto_sensor_glue_crawler]\n    wait_for_crawl = GlueCrawlerSensor(\n        task_id=\"wait_for_crawl\",\n        crawler_name=glue_crawler_name,\n    )\n    # [END howto_sensor_glue_crawler]\n\n    # [START howto_operator_glue]\n    submit_glue_job = GlueJobOperator(\n        task_id=\"submit_glue_job\",\n        job_name=glue_job_name,\n        script_location=f\"s3://{bucket_name}/etl_script.py\",\n        s3_bucket=bucket_name,\n        iam_role_name=role_name,\n        create_job_kwargs={\"GlueVersion\": \"3.0\", \"NumberOfWorkers\": 2, \"WorkerType\": \"G.1X\"},\n    )\n    # [END howto_operator_glue]\n\n    # GlueJobOperator waits by default, setting as False to test the Sensor below.\n    submit_glue_job.wait_for_completion = False\n\n    # [START howto_sensor_glue]\n    wait_for_job = GlueJobSensor(\n        task_id=\"wait_for_job\",\n        job_name=glue_job_name,\n        # Job ID extracted from previous Glue Job Operator task\n        run_id=submit_glue_job.output,\n        verbose=True,  # prints glue job logs in airflow logs\n    )\n    # [END howto_sensor_glue]\n    wait_for_job.poke_interval = 5\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws-glue/crawlers\", glue_crawler_name),\n            (\"/aws-glue/jobs/logs-v2\", submit_glue_job.output),\n            (\"/aws-glue/jobs/error\", submit_glue_job.output),\n            (\"/aws-glue/jobs/output\", submit_glue_job.output),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_csv,\n        upload_script,\n        # TEST BODY\n        crawl_s3,\n        wait_for_crawl,\n        submit_glue_job,\n        wait_for_job,\n        # TEST TEARDOWN\n        glue_cleanup(glue_crawler_name, glue_job_name, glue_db_name),\n        delete_bucket,\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.117147", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 141, "file_name": "example_hive_to_dynamodb.py", "syntax_error": "AST parse error: unexpected indent", "included_files": ["__init__.py"]}, "content": "   https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMRforDynamoDB.Tutorial.html\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.dynamodb import DynamoDBHook\nfrom airflow.providers.amazon.aws.transfers.hive_to_dynamodb import HiveToDynamoDBOperator\nfrom airflow.utils import db\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_hive_to_dynamodb\"\n\n# Externally fetched variables:\nHIVE_CONNECTION_ID_KEY = \"HIVE_CONNECTION_ID\"\nHIVE_HOSTNAME_KEY = \"HIVE_HOSTNAME\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(HIVE_CONNECTION_ID_KEY).add_variable(HIVE_HOSTNAME_KEY).build()\n)\n\n# These values assume you set up the Hive data source following the link above.\nDYNAMODB_TABLE_HASH_KEY = \"feature_id\"\nHIVE_SQL = \"SELECT feature_id, feature_name, feature_class, state_alpha FROM hive_features\"\n\n\n@task\ndef create_dynamodb_table(table_name):\n    client = DynamoDBHook(client_type=\"dynamodb\").conn\n    client.create_table(\n        TableName=table_name,\n        KeySchema=[\n            {\"AttributeName\": DYNAMODB_TABLE_HASH_KEY, \"KeyType\": \"HASH\"},\n        ],\n        AttributeDefinitions=[\n            {\"AttributeName\": DYNAMODB_TABLE_HASH_KEY, \"AttributeType\": \"N\"},\n        ],\n        ProvisionedThroughput={\"ReadCapacityUnits\": 20, \"WriteCapacityUnits\": 20},\n    )\n\n    # DynamoDB table creation is nearly, but not quite, instantaneous.\n    # Wait for the table to be active to avoid race conditions writing to it.\n    waiter = client.get_waiter(\"table_exists\")\n    waiter.wait(TableName=table_name, WaiterConfig={\"Delay\": 1})\n\n\n@task\ndef get_dynamodb_item_count(table_name):\n    \"\"\"\n    A DynamoDB table has an ItemCount value, but it is only updated every six hours.\n    To verify this DAG worked, we will scan the table and count the items manually.\n    \"\"\"\n    table = DynamoDBHook(resource_type=\"dynamodb\").conn.Table(table_name)\n\n    response = table.scan(Select=\"COUNT\")\n    item_count = response[\"Count\"]\n\n    while \"LastEvaluatedKey\" in response:\n        response = table.scan(Select=\"COUNT\", ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n        item_count += response[\"Count\"]\n\n    print(f\"DynamoDB table contains {item_count} items.\")\n\n\n# Included for sample purposes only; in production you wouldn't delete\n# the table you just backed your data up to.  Using 'all_done' so even\n# if an intermediate step fails, the DAG will clean up after itself.\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dynamodb_table(table_name):\n    DynamoDBHook(client_type=\"dynamodb\").conn.delete_table(TableName=table_name)\n\n\n# Included for sample purposes only; in production this should\n# be configured in the environment and not be part of the DAG.\n# Note: The 'hiveserver2_default' connection will not work if Hive\n# is hosted on EMR.  You must set the host name of the connection\n# to match your EMR cluster's hostname.\n@task\ndef configure_hive_connection(connection_id, hostname):\n    db.merge_conn(\n        Connection(\n            conn_id=connection_id,\n            conn_type=\"hiveserver2\",\n            host=hostname,\n            port=10000,\n        )\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    dynamodb_table_name = f\"{env_id}-hive_to_dynamo\"\n\n    hive_connection_id = test_context[HIVE_CONNECTION_ID_KEY]\n    hive_hostname = test_context[HIVE_HOSTNAME_KEY]\n\n    # [START howto_transfer_hive_to_dynamodb]\n    backup_to_dynamodb = HiveToDynamoDBOperator(\n        task_id=\"backup_to_dynamodb\",\n        hiveserver2_conn_id=hive_connection_id,\n        sql=HIVE_SQL,\n        table_name=dynamodb_table_name,\n        table_keys=[DYNAMODB_TABLE_HASH_KEY],\n    )\n    # [END howto_transfer_hive_to_dynamodb]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        configure_hive_connection(hive_connection_id, hive_hostname),\n        create_dynamodb_table(dynamodb_table_name),\n        # TEST BODY\n        backup_to_dynamodb,\n        get_dynamodb_item_count(dynamodb_table_name),\n        # TEST TEARDOWN\n        delete_dynamodb_table(dynamodb_table_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.136088", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 84, "file_name": "example_imap_attachment_to_s3.py", "included_files": ["__init__.py"]}, "content": "\"\"\"\nThis is an example dag for using `ImapAttachmentToS3Operator` to transfer an email attachment via IMAP\nprotocol from a mail server to S3 Bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.imap_attachment_to_s3 import ImapAttachmentToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_imap_attachment_to_s3\"\n\n# Externally fetched variables:\nIMAP_ATTACHMENT_NAME_KEY = \"IMAP_ATTACHMENT_NAME\"\nIMAP_MAIL_FOLDER_KEY = \"IMAP_MAIL_FOLDER\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(IMAP_ATTACHMENT_NAME_KEY)\n    .add_variable(IMAP_MAIL_FOLDER_KEY)\n    .build()\n)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    imap_attachment_name = test_context[IMAP_ATTACHMENT_NAME_KEY]\n    imap_mail_folder = test_context[IMAP_MAIL_FOLDER_KEY]\n\n    s3_bucket = f\"{env_id}-imap-attachment-to-s3-bucket\"\n    s3_key = f\"{env_id}-imap-attachment-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_imap_attachment_to_s3]\n    task_transfer_imap_attachment_to_s3 = ImapAttachmentToS3Operator(\n        task_id=\"transfer_imap_attachment_to_s3\",\n        imap_attachment_name=imap_attachment_name,\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        imap_mail_folder=imap_mail_folder,\n        imap_mail_filter=\"All\",\n    )\n    # [END howto_transfer_imap_attachment_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_transfer_imap_attachment_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.140544", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 77, "file_name": "example_google_api_sheets_to_s3.py", "included_files": ["__init__.py"]}, "content": "\"\"\"\nThis is a basic example dag for using `GoogleApiToS3Operator` to retrieve Google Sheets data\nYou need to set all env variables to request the data.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_google_api_sheets_to_s3\"\n\nGOOGLE_SHEET_ID = os.getenv(\"GOOGLE_SHEET_ID\", \"test-google-sheet-id\")\nGOOGLE_SHEET_RANGE = os.getenv(\"GOOGLE_SHEET_RANGE\", \"test-google-sheet-range\")\nS3_DESTINATION_KEY = os.getenv(\"S3_DESTINATION_KEY\", \"s3://test-bucket/key.json\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-google-api-sheets\"\n    s3_key = f\"{env_id}-google-api-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_google_api_sheets_to_s3]\n    task_google_sheets_values_to_s3 = GoogleApiToS3Operator(\n        task_id=\"google_sheet_data_to_s3\",\n        google_api_service_name=\"sheets\",\n        google_api_service_version=\"v4\",\n        google_api_endpoint_path=\"sheets.spreadsheets.values.get\",\n        google_api_endpoint_params={\"spreadsheetId\": GOOGLE_SHEET_ID, \"range\": GOOGLE_SHEET_RANGE},\n        s3_destination_key=f\"s3://{s3_bucket}/{s3_key}\",\n    )\n    # [END howto_transfer_google_api_sheets_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_google_sheets_values_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.144577", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 163, "file_name": "example_google_api_youtube_to_s3.py", "syntax_error": "AST parse error: unterminated triple-quoted string literal", "included_files": ["__init__.py"]}, "content": "The required scope for this DAG is https://www.googleapis.com/auth/youtube.readonly.\nThis can be set via the environment variable AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT,\nor by creating a custom connection.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_google_api_youtube_to_s3\"\n\nYOUTUBE_CHANNEL_ID = \"UCSXwxpWZQ7XZ1WL3wqevChA\"\nYOUTUBE_VIDEO_PUBLISHED_AFTER = \"2019-09-25T00:00:00Z\"\nYOUTUBE_VIDEO_PUBLISHED_BEFORE = \"2019-10-18T00:00:00Z\"\nYOUTUBE_VIDEO_PARTS = \"snippet\"\nYOUTUBE_VIDEO_FIELDS = \"items(id,snippet(description,publishedAt,tags,title))\"\n\nSECRET_ARN_KEY = \"SECRET_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(SECRET_ARN_KEY).build()\n\n\n@task\ndef create_connection_gcp(conn_id_name: str, secret_arn: str):\n    json_data = boto3.client(\"secretsmanager\").get_secret_value(SecretId=secret_arn)[\"SecretString\"]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"google_cloud_platform\",\n    )\n    scopes = \"https://www.googleapis.com/auth/youtube.readonly\"\n    conn_extra = {\n        \"scope\": scopes,\n        \"project\": \"aws-oss-airflow\",\n        \"keyfile_dict\": json_data,\n    }\n    conn_extra_json = json.dumps(conn_extra)\n    conn.set_extra(conn_extra_json)\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task(task_id=\"wait_for_s3_bucket\")\ndef wait_for_bucket(s3_bucket_name):\n    waiter = boto3.client(\"s3\").get_waiter(\"bucket_exists\")\n    waiter.wait(Bucket=s3_bucket_name)\n\n\n@task(task_id=\"transform_video_ids\")\ndef transform_video_ids(**kwargs):\n    task_instance = kwargs[\"task_instance\"]\n    output = task_instance.xcom_pull(task_ids=\"video_ids_to_s3\", key=\"video_ids_response\")\n    video_ids = [item[\"id\"][\"videoId\"] for item in output[\"items\"]]\n\n    if not video_ids:\n        video_ids = []\n\n    kwargs[\"task_instance\"].xcom_push(key=\"video_ids\", value={\"id\": \",\".join(video_ids)})\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    conn_id_name = f\"{env_id}-conn-id\"\n    secret_arn = test_context[SECRET_ARN_KEY]\n\n    set_up_connection = create_connection_gcp(conn_id_name, secret_arn=secret_arn)\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create-s3-bucket\", bucket_name=s3_bucket_name)\n\n    wait_for_bucket_creation = wait_for_bucket(s3_bucket_name=s3_bucket_name)\n\n    # [START howto_transfer_google_api_youtube_search_to_s3]\n    video_ids_to_s3 = GoogleApiToS3Operator(\n        task_id=\"video_ids_to_s3\",\n        google_api_service_name=\"youtube\",\n        google_api_service_version=\"v3\",\n        google_api_endpoint_path=\"youtube.search.list\",\n        gcp_conn_id=conn_id_name,\n        google_api_endpoint_params={\n            \"part\": \"snippet\",\n            \"channelId\": YOUTUBE_CHANNEL_ID,\n            \"maxResults\": 50,\n            \"publishedAfter\": YOUTUBE_VIDEO_PUBLISHED_AFTER,\n            \"publishedBefore\": YOUTUBE_VIDEO_PUBLISHED_BEFORE,\n            \"type\": \"video\",\n            \"fields\": \"items/id/videoId\",\n        },\n        google_api_response_via_xcom=\"video_ids_response\",\n        s3_destination_key=f\"https://s3.us-west-2.amazonaws.com/{s3_bucket_name}/youtube_search\",\n        s3_overwrite=True,\n    )\n    # [END howto_transfer_google_api_youtube_search_to_s3]\n\n    transform_video_ids_task = transform_video_ids()\n\n    # [START howto_transfer_google_api_youtube_list_to_s3]\n    video_data_to_s3 = GoogleApiToS3Operator(\n        task_id=\"video_data_to_s3\",\n        google_api_service_name=\"youtube\",\n        google_api_service_version=\"v3\",\n        gcp_conn_id=conn_id_name,\n        google_api_endpoint_path=\"youtube.videos.list\",\n        google_api_endpoint_params={\n            \"part\": YOUTUBE_VIDEO_PARTS,\n            \"maxResults\": 50,\n            \"fields\": YOUTUBE_VIDEO_FIELDS,\n        },\n        google_api_endpoint_params_via_xcom=\"video_ids\",\n        s3_destination_key=f\"https://s3.us-west-2.amazonaws.com/{s3_bucket_name}/youtube_videos\",\n        s3_overwrite=True,\n    )\n    # [END howto_transfer_google_api_youtube_list_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_connection,\n        create_s3_bucket,\n        wait_for_bucket_creation,\n        # TEST BODY\n        video_ids_to_s3,\n        transform_video_ids_task,\n        video_data_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.147158", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 87, "file_name": "example_local_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nDAG_ID = \"example_local_to_s3\"\nTEMP_FILE_PATH = \"/tmp/sample-txt.txt\"\nSAMPLE_TEXT = \"This is some sample text.\"\n\n\n@task\ndef create_temp_file():\n    file = open(TEMP_FILE_PATH, \"w\")\n    file.write(SAMPLE_TEXT)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_temp_file():\n    if os.path.exists(TEMP_FILE_PATH):\n        os.remove(TEMP_FILE_PATH)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/my-temp-file.txt\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create-s3-bucket\", bucket_name=s3_bucket_name)\n    # [START howto_transfer_local_to_s3]\n    create_local_to_s3_job = LocalFilesystemToS3Operator(\n        task_id=\"create_local_to_s3_job\",\n        filename=TEMP_FILE_PATH,\n        dest_key=s3_key,\n        dest_bucket=s3_bucket_name,\n        replace=True,\n    )\n    # [END howto_transfer_local_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_temp_file(),\n        create_s3_bucket,\n        # TEST BODY\n        create_local_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_temp_file(),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.171308", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 78, "file_name": "example_mongo_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.mongo_to_s3 import MongoToS3Operator\nfrom airflow.utils.dates import datetime\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_mongo_to_s3\"\n\n# Externally fetched variables:\nMONGO_DATABASE_KEY = \"MONGO_DATABASE\"\nMONGO_COLLECTION_KEY = \"MONGO_COLLECTION\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(MONGO_DATABASE_KEY).add_variable(MONGO_COLLECTION_KEY).build()\n)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    mongo_database = test_context[MONGO_DATABASE_KEY]\n    mongo_collection = test_context[MONGO_COLLECTION_KEY]\n\n    s3_bucket = f\"{env_id}-mongo-to-s3-bucket\"\n    s3_key = f\"{env_id}-mongo-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_mongo_to_s3]\n    mongo_to_s3_job = MongoToS3Operator(\n        task_id=\"mongo_to_s3_job\",\n        mongo_collection=mongo_collection,\n        # Mongo query by matching values\n        # Here returns all documents which have \"OK\" as value for the key \"status\"\n        mongo_query={\"status\": \"OK\"},\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        mongo_db=mongo_database,\n        replace=True,\n    )\n    # [END howto_transfer_mongo_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        mongo_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.171936", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 124, "file_name": "example_lambda.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport io\nimport json\nimport zipfile\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.lambda_function import (\n    LambdaCreateFunctionOperator,\n    LambdaInvokeFunctionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.lambda_function import LambdaFunctionStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_lambda\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nCODE_CONTENT = \"\"\"\ndef test(*args):\n    print('Hello')\n\"\"\"\n\n\n# Create a zip file containing one file \"lambda_function.py\" to deploy to the lambda function\ndef create_zip(content: str):\n    zip_output = io.BytesIO()\n    with zipfile.ZipFile(zip_output, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        info = zipfile.ZipInfo(\"lambda_function.py\")\n        info.external_attr = 0o777 << 16\n        zip_file.writestr(info, content)\n    zip_output.seek(0)\n    return zip_output.read()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_lambda(function_name: str):\n    client = boto3.client(\"lambda\")\n    client.delete_function(\n        FunctionName=function_name,\n    )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    lambda_function_name: str = f\"{test_context[ENV_ID_KEY]}-function\"\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    # [START howto_operator_create_lambda_function]\n    create_lambda_function = LambdaCreateFunctionOperator(\n        task_id=\"create_lambda_function\",\n        function_name=lambda_function_name,\n        runtime=\"python3.9\",\n        role=role_arn,\n        handler=\"lambda_function.test\",\n        code={\n            \"ZipFile\": create_zip(CODE_CONTENT),\n        },\n    )\n    # [END howto_operator_create_lambda_function]\n\n    # [START howto_sensor_lambda_function_state]\n    wait_lambda_function_state = LambdaFunctionStateSensor(\n        task_id=\"wait_lambda_function_state\",\n        function_name=lambda_function_name,\n    )\n    # [END howto_sensor_lambda_function_state]\n    wait_lambda_function_state.poke_interval = 1\n\n    # [START howto_operator_invoke_lambda_function]\n    invoke_lambda_function = LambdaInvokeFunctionOperator(\n        task_id=\"invoke_lambda_function\",\n        function_name=lambda_function_name,\n        payload=json.dumps({\"SampleEvent\": {\"SampleData\": {\"Name\": \"XYZ\", \"DoB\": \"1993-01-01\"}}}),\n    )\n    # [END howto_operator_invoke_lambda_function]\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (f\"/aws/lambda/{lambda_function_name}\", None),\n        ],\n        force_delete=True,\n        retry=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_lambda_function,\n        wait_lambda_function_state,\n        invoke_lambda_function,\n        # TEST TEARDOWN\n        delete_lambda(lambda_function_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.172747", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 214, "file_name": "example_quicksight.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\n\"\"\"\nPrerequisites:\n1. The account which runs this test must manually be activated in Quicksight here:\nhttps://quicksight.aws.amazon.com/sn/console/signup?#\n2. The activation process creates an IAM Role called `aws-quicksight-service-role-v0`.\n You have to add a policy named 'AWSQuickSightS3Policy' with the S3 access permissions.\n The policy name is enforced, and the permissions json can be copied from `AmazonS3FullAccess`.\n\nNOTES:  If Create Ingestion fails for any reason, that ingestion name will remain in use and\n future runs will stall with the sensor returning a status of QUEUED \"forever\".  If you run\n into this behavior, changing the template for the ingestion name or the ENV_ID and re-running\n the test should resolve the issue.\n\"\"\"\n\nDAG_ID = \"example_quicksight\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_DATA_COLUMNS = [\"Project\", \"Year\"]\nSAMPLE_DATA = \"\"\"'Airflow','2015'\n    'OpenOffice','2012'\n    'Subversion','2000'\n    'NiFi','2006'\n\"\"\"\n\n\n@task\ndef get_aws_account_id() -> int:\n    return boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n\n\n@task\ndef create_quicksight_data_source(\n    aws_account_id: str, datasource_name: str, bucket: str, manifest_key: str\n) -> str:\n    response = boto3.client(\"quicksight\").create_data_source(\n        AwsAccountId=aws_account_id,\n        DataSourceId=datasource_name,\n        Name=datasource_name,\n        Type=\"S3\",\n        DataSourceParameters={\n            \"S3Parameters\": {\"ManifestFileLocation\": {\"Bucket\": bucket, \"Key\": manifest_key}}\n        },\n    )\n    return response[\"Arn\"]\n\n\n@task\ndef create_quicksight_dataset(aws_account_id: int, dataset_name: str, data_source_arn: str) -> None:\n    table_map = {\n        \"default\": {\n            \"S3Source\": {\n                \"DataSourceArn\": data_source_arn,\n                \"InputColumns\": [{\"Name\": name, \"Type\": \"STRING\"} for name in SAMPLE_DATA_COLUMNS],\n            }\n        }\n    }\n\n    boto3.client(\"quicksight\").create_data_set(\n        AwsAccountId=aws_account_id,\n        DataSetId=dataset_name,\n        Name=dataset_name,\n        PhysicalTableMap=table_map,\n        ImportMode=\"SPICE\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_quicksight_data_source(aws_account_id: str, datasource_name: str):\n    boto3.client(\"quicksight\").delete_data_source(AwsAccountId=aws_account_id, DataSourceId=datasource_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dataset(aws_account_id: str, dataset_name: str):\n    boto3.client(\"quicksight\").delete_data_set(AwsAccountId=aws_account_id, DataSetId=dataset_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(aws_account_id: str, dataset_name: str, ingestion_name: str) -> None:\n    client = boto3.client(\"quicksight\")\n    try:\n        client.cancel_ingestion(\n            AwsAccountId=aws_account_id,\n            DataSetId=dataset_name,\n            IngestionId=ingestion_name,\n        )\n    except client.exceptions.ResourceNotFoundException:\n        # Ingestion has already terminated on its own.\n        pass\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    account_id = get_aws_account_id()\n\n    env_id = test_context[ENV_ID_KEY]\n    bucket_name = f\"{env_id}-quicksight-bucket\"\n    data_filename = \"sample_data.csv\"\n    dataset_id = f\"{env_id}-data-set\"\n    datasource_id = f\"{env_id}-data-source\"\n    ingestion_id = f\"{env_id}-ingestion\"\n    manifest_filename = f\"{env_id}-manifest.json\"\n    manifest_contents = {\"fileLocations\": [{\"URIs\": [f\"s3://{bucket_name}/{data_filename}\"]}]}\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    upload_manifest_file = S3CreateObjectOperator(\n        task_id=\"upload_manifest_file\",\n        s3_bucket=bucket_name,\n        s3_key=manifest_filename,\n        data=json.dumps(manifest_contents),\n        replace=True,\n    )\n\n    upload_sample_data = S3CreateObjectOperator(\n        task_id=\"upload_sample_data\",\n        s3_bucket=bucket_name,\n        s3_key=data_filename,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    data_source = create_quicksight_data_source(\n        aws_account_id=account_id,\n        datasource_name=datasource_id,\n        bucket=bucket_name,\n        manifest_key=manifest_filename,\n    )\n\n    create_dataset = create_quicksight_dataset(account_id, dataset_id, data_source)\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightCreateIngestionOperator(\n        task_id=\"create_ingestion\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_operator_quicksight_create_ingestion]\n\n    # QuickSightCreateIngestionOperator waits by default, setting as False to test the Sensor below.\n    create_ingestion.wait_for_completion = False\n\n    # If this sensor appears to freeze with a \"QUEUED\" status, see note above.\n    # [START howto_sensor_quicksight]\n    await_job = QuickSightSensor(\n        task_id=\"await_job\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_sensor_quicksight]\n    await_job.poke_interval = 10\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        account_id,\n        create_s3_bucket,\n        upload_manifest_file,\n        upload_sample_data,\n        data_source,\n        create_dataset,\n        # TEST BODY\n        create_ingestion,\n        await_job,\n        # TEST TEARDOWN\n        delete_dataset(account_id, dataset_id),\n        delete_quicksight_data_source(account_id, datasource_id),\n        delete_ingestion(account_id, dataset_id, ingestion_id),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.177274", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 114, "file_name": "example_rds_event.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateEventSubscriptionOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteEventSubscriptionOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_event\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_sns_topic(env_id) -> str:\n    return boto3.client(\"sns\").create_topic(Name=f\"{env_id}-topic\")[\"TopicArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_sns_topic(topic_arn) -> None:\n    boto3.client(\"sns\").delete_topic(TopicArn=topic_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_subscription_name = f\"{test_context[ENV_ID_KEY]}-subscription\"\n\n    sns_topic = create_sns_topic(test_context[ENV_ID_KEY])\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_event_subscription]\n    create_subscription = RdsCreateEventSubscriptionOperator(\n        task_id=\"create_subscription\",\n        subscription_name=rds_subscription_name,\n        sns_topic_arn=sns_topic,\n        source_type=\"db-instance\",\n        source_ids=[rds_instance_name],\n        event_categories=[\"availability\"],\n    )\n    # [END howto_operator_rds_create_event_subscription]\n\n    # [START howto_operator_rds_delete_event_subscription]\n    delete_subscription = RdsDeleteEventSubscriptionOperator(\n        task_id=\"delete_subscription\",\n        subscription_name=rds_subscription_name,\n    )\n    # [END howto_operator_rds_delete_event_subscription]\n    delete_subscription.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        sns_topic,\n        create_db_instance,\n        # TEST BODY\n        create_subscription,\n        delete_subscription,\n        # TEST TEARDOWN\n        delete_db_instance,\n        delete_sns_topic(sns_topic),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.192281", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 173, "file_name": "example_rds_export.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCancelExportTaskOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n    RdsStartExportTaskOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RdsExportTaskExistenceSensor, RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_export\"\n\n# Externally fetched variables:\nKMS_KEY_ID_KEY = \"KMS_KEY_ID\"\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(KMS_KEY_ID_KEY).add_variable(ROLE_ARN_KEY).build()\n)\n\n\n@task\ndef get_snapshot_arn(snapshot_name: str) -> str:\n    result = RdsHook().conn.describe_db_snapshots(DBSnapshotIdentifier=snapshot_name)\n    return result[\"DBSnapshots\"][0][\"DBSnapshotArn\"]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name: str = f\"{env_id}-bucket\"\n\n    rds_db_name: str = f\"{env_id}_db\"\n    rds_instance_name: str = f\"{env_id}-instance\"\n    rds_snapshot_name: str = f\"{env_id}-snapshot\"\n    rds_export_task_id: str = f\"{env_id}-export-task\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n\n    await_snapshot = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n\n    snapshot_arn = get_snapshot_arn(rds_snapshot_name)\n\n    # [START howto_operator_rds_start_export_task]\n    start_export = RdsStartExportTaskOperator(\n        task_id=\"start_export\",\n        export_task_identifier=rds_export_task_id,\n        source_arn=snapshot_arn,\n        s3_bucket_name=bucket_name,\n        s3_prefix=\"rds-test\",\n        iam_role_arn=test_context[ROLE_ARN_KEY],\n        kms_key_id=test_context[KMS_KEY_ID_KEY],\n    )\n    # [END howto_operator_rds_start_export_task]\n\n    # RdsStartExportTaskOperator waits by default, setting as False to test the Sensor below.\n    start_export.wait_for_completion = False\n\n    # [START howto_operator_rds_cancel_export]\n    cancel_export = RdsCancelExportTaskOperator(\n        task_id=\"cancel_export\",\n        export_task_identifier=rds_export_task_id,\n    )\n    # [END howto_operator_rds_cancel_export]\n    cancel_export.check_interval = 10\n    cancel_export.max_attempts = 120\n\n    # [START howto_sensor_rds_export_task_existence]\n    export_sensor = RdsExportTaskExistenceSensor(\n        task_id=\"export_sensor\",\n        export_task_identifier=rds_export_task_id,\n        target_statuses=[\"canceled\"],\n    )\n    # [END howto_sensor_rds_export_task_existence]\n\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        create_db_instance,\n        create_snapshot,\n        await_snapshot,\n        snapshot_arn,\n        # TEST BODY\n        start_export,\n        cancel_export,\n        export_sensor,\n        # TEST TEARDOWN\n        delete_snapshot,\n        delete_bucket,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.200132", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 107, "file_name": "example_rds_instance.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsStartDbOperator,\n    RdsStopDbOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsDbSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_rds_instance\"\n\nRDS_USERNAME = \"database_username\"\n# NEVER store your production password in plaintext in a DAG like this.\n# Use Airflow Secrets or a secret manager for this in production.\nRDS_PASSWORD = \"database_password\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    rds_db_identifier = f\"{test_context[ENV_ID_KEY]}-database\"\n\n    # [START howto_operator_rds_create_db_instance]\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"AllocatedStorage\": 20,\n            \"PubliclyAccessible\": False,\n        },\n    )\n    # [END howto_operator_rds_create_db_instance]\n\n    # RdsCreateDbInstanceOperator waits by default, setting as False to test the Sensor below.\n    create_db_instance.wait_for_completion = False\n\n    # [START howto_sensor_rds_instance]\n    await_db_instance = RdsDbSensor(\n        task_id=\"await_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_sensor_rds_instance]\n\n    # [START howto_operator_rds_stop_db]\n    stop_db_instance = RdsStopDbOperator(\n        task_id=\"stop_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_stop_db]\n\n    # [START howto_operator_rds_start_db]\n    start_db_instance = RdsStartDbOperator(\n        task_id=\"start_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_start_db]\n\n    # [START howto_operator_rds_delete_db_instance]\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n    )\n    # [END howto_operator_rds_delete_db_instance]\n    delete_db_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_db_instance,\n        await_db_instance,\n        stop_db_instance,\n        start_db_instance,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.202803", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 134, "file_name": "example_rds_snapshot.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCopyDbSnapshotOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_snapshot\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_snapshot_name = f\"{test_context[ENV_ID_KEY]}-snapshot\"\n    rds_snapshot_copy_name = f\"{rds_snapshot_name}-copy\"\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_db_snapshot]\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_create_db_snapshot]\n\n    # [START howto_sensor_rds_snapshot_existence]\n    snapshot_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n    # [END howto_sensor_rds_snapshot_existence]\n\n    # [START howto_operator_rds_copy_snapshot]\n    copy_snapshot = RdsCopyDbSnapshotOperator(\n        task_id=\"copy_snapshot\",\n        db_type=\"instance\",\n        source_db_snapshot_identifier=rds_snapshot_name,\n        target_db_snapshot_identifier=rds_snapshot_copy_name,\n    )\n    # [END howto_operator_rds_copy_snapshot]\n\n    # [START howto_operator_rds_delete_snapshot]\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_delete_snapshot]\n\n    snapshot_copy_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_copy_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        target_statuses=[\"available\"],\n    )\n\n    delete_snapshot_copy = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot_copy\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_db_instance,\n        # TEST BODY\n        create_snapshot,\n        snapshot_sensor,\n        copy_snapshot,\n        delete_snapshot,\n        # TEST TEARDOWN\n        snapshot_copy_sensor,\n        delete_snapshot_copy,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.208116", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 270, "file_name": "example_redshift.py", "included_files": ["__init__.py", "ec2.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftCreateClusterSnapshotOperator,\n    RedshiftDeleteClusterOperator,\n    RedshiftDeleteClusterSnapshotOperator,\n    RedshiftPauseClusterOperator,\n    RedshiftResumeClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.redshift_data import RedshiftDataOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\nPOLL_INTERVAL = 10\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    redshift_cluster_snapshot_identifier = f\"{env_id}-snapshot\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    # [START howto_operator_redshift_cluster]\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n    # [END howto_operator_redshift_cluster]\n\n    # [START howto_sensor_redshift_cluster]\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n    # [END howto_sensor_redshift_cluster]\n\n    # [START howto_operator_redshift_create_cluster_snapshot]\n    create_cluster_snapshot = RedshiftCreateClusterSnapshotOperator(\n        task_id=\"create_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n        poll_interval=30,\n        max_attempt=100,\n        retention_period=1,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_create_cluster_snapshot]\n\n    wait_cluster_available_before_pause = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_before_pause\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_pause_cluster]\n    pause_cluster = RedshiftPauseClusterOperator(\n        task_id=\"pause_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_pause_cluster]\n\n    wait_cluster_paused = RedshiftClusterSensor(\n        task_id=\"wait_cluster_paused\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"paused\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_resume_cluster]\n    resume_cluster = RedshiftResumeClusterOperator(\n        task_id=\"resume_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_resume_cluster]\n\n    wait_cluster_available_after_resume = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_after_resume\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    # [START howto_operator_redshift_data]\n    create_table_redshift_data = RedshiftDataOperator(\n        task_id=\"create_table_redshift_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS fruit (\n            fruit_id INTEGER,\n            name VARCHAR NOT NULL,\n            color VARCHAR NOT NULL\n            );\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_data]\n\n    insert_data = RedshiftDataOperator(\n        task_id=\"insert_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            INSERT INTO fruit VALUES ( 1, 'Banana', 'Yellow');\n            INSERT INTO fruit VALUES ( 2, 'Apple', 'Red');\n            INSERT INTO fruit VALUES ( 3, 'Lemon', 'Yellow');\n            INSERT INTO fruit VALUES ( 4, 'Grape', 'Purple');\n            INSERT INTO fruit VALUES ( 5, 'Pear', 'Green');\n            INSERT INTO fruit VALUES ( 6, 'Strawberry', 'Red');\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=\"DROP TABLE IF EXISTS fruit\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_redshift_delete_cluster]\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_redshift_delete_cluster_snapshot]\n    delete_cluster_snapshot = RedshiftDeleteClusterSnapshotOperator(\n        task_id=\"delete_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster_snapshot]\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        # TEST BODY\n        create_cluster,\n        wait_cluster_available,\n        create_cluster_snapshot,\n        wait_cluster_available_before_pause,\n        pause_cluster,\n        wait_cluster_paused,\n        resume_cluster,\n        wait_cluster_available_after_resume,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        drop_table,\n        delete_cluster_snapshot,\n        delete_cluster,\n        # TEST TEARDOWN\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.237822", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 66, "file_name": "example_s3_to_ftp.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_ftp import S3ToFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_ftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-ftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-ftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_ftp]\n    s3_to_ftp_task = S3ToFTPOperator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_ftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        s3_to_ftp_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.240160", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 266, "file_name": "example_redshift_s3_transfers.py", "included_files": ["__init__.py", "ec2.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.amazon.aws.transfers.redshift_to_s3 import RedshiftToS3Operator\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift_to_s3\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nS3_KEY = \"s3_output_\"\nS3_KEY_2 = \"s3_key_2\"\nS3_KEY_PREFIX = \"s3_k\"\nREDSHIFT_TABLE = \"test_table\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\nSQL_DROP_TABLE = f\"DROP TABLE IF EXISTS {REDSHIFT_TABLE};\"\n\nDATA = \"0, 'Airflow', 'testing'\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-bucket\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"s3_create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        data=DATA,\n        replace=True,\n    )\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_redshift_to_s3]\n    transfer_redshift_to_s3 = RedshiftToS3Operator(\n        task_id=\"transfer_redshift_to_s3\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n    )\n    # [END howto_transfer_redshift_to_s3]\n\n    check_if_key_exists = S3KeySensor(\n        task_id=\"check_if_key_exists\",\n        bucket_name=bucket_name,\n        bucket_key=f\"{S3_KEY}/{REDSHIFT_TABLE}_0000_part_00\",\n    )\n\n    # [START howto_transfer_s3_to_redshift]\n    transfer_s3_to_redshift = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift]\n\n    # [START howto_transfer_s3_to_redshift_multiple_keys]\n    transfer_s3_to_redshift_multiple = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift_multiple\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_PREFIX,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift_multiple_keys]\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=SQL_DROP_TABLE,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_bucket,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_object,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        transfer_redshift_to_s3,\n        check_if_key_exists,\n        transfer_s3_to_redshift,\n        transfer_s3_to_redshift_multiple,\n        # TEST TEARDOWN\n        drop_table,\n        delete_cluster,\n        delete_sg,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.244857", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 293, "file_name": "example_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CopyObjectOperator,\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteBucketTaggingOperator,\n    S3DeleteObjectsOperator,\n    S3FileTransformOperator,\n    S3GetBucketTaggingOperator,\n    S3ListOperator,\n    S3ListPrefixesOperator,\n    S3PutBucketTaggingOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor, S3KeysUnchangedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDATA = \"\"\"\n    apple,0.5\n    milk,2.5\n    bread,4.0\n\"\"\"\n\n# Empty string prefix refers to the bucket root\n# See what prefix is here https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html\nPREFIX = \"\"\nDELIMITER = \"/\"\nTAG_KEY = \"test-s3-bucket-tagging-key\"\nTAG_VALUE = \"test-s3-bucket-tagging-value\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name = f\"{env_id}-s3-bucket\"\n    bucket_name_2 = f\"{env_id}-s3-bucket-2\"\n\n    key = f\"{env_id}-key\"\n    key_2 = f\"{env_id}-key2\"\n\n    # [START howto_sensor_s3_key_function_definition]\n    def check_fn(files: list) -> bool:\n        \"\"\"\n        Example of custom check: check if all files are bigger than ``20 bytes``\n\n        :param files: List of S3 object attributes.\n        :return: true if the criteria is met\n        \"\"\"\n        return all(f.get(\"Size\", 0) > 20 for f in files)\n\n    # [END howto_sensor_s3_key_function_definition]\n\n    # [START howto_operator_s3_create_bucket]\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_create_bucket]\n\n    create_bucket_2 = S3CreateBucketOperator(\n        task_id=\"create_bucket_2\",\n        bucket_name=bucket_name_2,\n    )\n\n    # [START howto_operator_s3_put_bucket_tagging]\n    put_tagging = S3PutBucketTaggingOperator(\n        task_id=\"put_tagging\",\n        bucket_name=bucket_name,\n        key=TAG_KEY,\n        value=TAG_VALUE,\n    )\n    # [END howto_operator_s3_put_bucket_tagging]\n\n    # [START howto_operator_s3_get_bucket_tagging]\n    get_tagging = S3GetBucketTaggingOperator(\n        task_id=\"get_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_get_bucket_tagging]\n\n    # [START howto_operator_s3_delete_bucket_tagging]\n    delete_tagging = S3DeleteBucketTaggingOperator(\n        task_id=\"delete_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_delete_bucket_tagging]\n\n    # [START howto_operator_s3_create_object]\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=key,\n        data=DATA,\n        replace=True,\n    )\n    # [END howto_operator_s3_create_object]\n\n    create_object_2 = S3CreateObjectOperator(\n        task_id=\"create_object_2\",\n        s3_bucket=bucket_name,\n        s3_key=key_2,\n        data=DATA,\n        replace=True,\n    )\n\n    # [START howto_operator_s3_list_prefixes]\n    list_prefixes = S3ListPrefixesOperator(\n        task_id=\"list_prefixes\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n        delimiter=DELIMITER,\n    )\n    # [END howto_operator_s3_list_prefixes]\n\n    # [START howto_operator_s3_list]\n    list_keys = S3ListOperator(\n        task_id=\"list_keys\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n    )\n    # [END howto_operator_s3_list]\n\n    # [START howto_sensor_s3_key_single_key]\n    # Check if a file exists\n    sensor_one_key = S3KeySensor(\n        task_id=\"sensor_one_key\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n    )\n    # [END howto_sensor_s3_key_single_key]\n\n    # [START howto_sensor_s3_key_multiple_keys]\n    # Check if both files exist\n    sensor_two_keys = S3KeySensor(\n        task_id=\"sensor_two_keys\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n    )\n    # [END howto_sensor_s3_key_multiple_keys]\n\n    # [START howto_sensor_s3_key_single_key_deferrable]\n    # Check if a file exists\n    sensor_one_key_deferrable = S3KeySensor(\n        task_id=\"sensor_one_key_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_single_key_deferrable]\n\n    # [START howto_sensor_s3_key_multiple_keys_deferrable]\n    # Check if both files exist\n    sensor_two_keys_deferrable = S3KeySensor(\n        task_id=\"sensor_two_keys_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_multiple_keys_deferrable]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function_deferrable = S3KeySensor(\n        task_id=\"sensor_key_with_function_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function = S3KeySensor(\n        task_id=\"sensor_key_with_function\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_operator_s3_copy_object]\n    copy_object = S3CopyObjectOperator(\n        task_id=\"copy_object\",\n        source_bucket_name=bucket_name,\n        dest_bucket_name=bucket_name_2,\n        source_bucket_key=key,\n        dest_bucket_key=key_2,\n    )\n    # [END howto_operator_s3_copy_object]\n\n    # [START howto_operator_s3_file_transform]\n    file_transform = S3FileTransformOperator(\n        task_id=\"file_transform\",\n        source_s3_key=f\"s3://{bucket_name}/{key}\",\n        dest_s3_key=f\"s3://{bucket_name_2}/{key_2}\",\n        # Use `cp` command as transform script as an example\n        transform_script=\"cp\",\n        replace=True,\n    )\n    # [END howto_operator_s3_file_transform]\n\n    # This task skips the `sensor_keys_unchanged` task because the S3KeysUnchangedSensor\n    # runs in poke mode only, which is not supported by the DebugExecutor, causing system tests to fail.\n    branching = BranchPythonOperator(\n        task_id=\"branch_to_delete_objects\", python_callable=lambda: \"delete_objects\"\n    )\n\n    # [START howto_sensor_s3_keys_unchanged]\n    sensor_keys_unchanged = S3KeysUnchangedSensor(\n        task_id=\"sensor_keys_unchanged\",\n        bucket_name=bucket_name_2,\n        prefix=PREFIX,\n        inactivity_period=10,  # inactivity_period in seconds\n    )\n    # [END howto_sensor_s3_keys_unchanged]\n\n    # [START howto_operator_s3_delete_objects]\n    delete_objects = S3DeleteObjectsOperator(\n        task_id=\"delete_objects\",\n        bucket=bucket_name_2,\n        keys=key_2,\n    )\n    # [END howto_operator_s3_delete_objects]\n    delete_objects.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_s3_delete_bucket]\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n    # [END howto_operator_s3_delete_bucket]\n    delete_bucket.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket_2 = S3DeleteBucketOperator(\n        task_id=\"delete_bucket_2\",\n        bucket_name=bucket_name_2,\n        force_delete=True,\n    )\n    delete_bucket_2.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_bucket,\n        create_bucket_2,\n        put_tagging,\n        get_tagging,\n        delete_tagging,\n        create_object,\n        create_object_2,\n        list_prefixes,\n        list_keys,\n        [sensor_one_key, sensor_two_keys, sensor_key_with_function],\n        [sensor_one_key_deferrable, sensor_two_keys_deferrable, sensor_key_with_function_deferrable],\n        copy_object,\n        file_transform,\n        branching,\n        sensor_keys_unchanged,\n        # TEST TEARDOWN\n        delete_objects,\n        delete_bucket,\n        delete_bucket_2,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.248522", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 66, "file_name": "example_s3_to_sftp.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sftp import S3ToSFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-sftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-sftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_sftp]\n    create_s3_to_sftp_job = S3ToSFTPOperator(\n        task_id=\"create_s3_to_sftp_job\",\n        sftp_path=\"sftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_sftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        create_s3_to_sftp_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.250441", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 269, "file_name": "example_s3_to_sql.py", "included_files": ["__init__.py", "watcher.py", "ec2.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteObjectsOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator, SQLTableCheckOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\nfrom tests.system.utils.watcher import watcher\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sql\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nSQL_TABLE_NAME = \"cocktails\"\nSQL_COLUMN_LIST = [\"cocktail_id\", \"cocktail_name\", \"base_spirit\"]\nSAMPLE_DATA = r\"\"\"1,Caipirinha,Cachaca\n2,Bramble,Gin\n3,Daiquiri,Rum\n\"\"\"\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    cluster_endpoint = RedshiftHook().conn.describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]], GroupNames=[sec_group_name]\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2023, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    conn_id_name = f\"{env_id}-conn-id\"\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    sg_name = f\"{env_id}-sg\"\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/cocktail_list.csv\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    create_table = SQLExecuteQueryOperator(\n        task_id=\"create_sample_table\",\n        conn_id=conn_id_name,\n        sql=f\"\"\"\n            CREATE TABLE IF NOT EXISTS {SQL_TABLE_NAME} (\n            cocktail_id INT NOT NULL,\n            cocktail_name VARCHAR NOT NULL,\n            base_spirit VARCHAR NOT NULL);\n          \"\"\",\n    )\n\n    # [START howto_transfer_s3_to_sql]\n    #\n    # This operator requires a parser method. The Parser should take a filename as input\n    # and return an iterable of rows.\n    # This example parser uses the builtin csv library and returns a list of rows\n    #\n    def parse_csv_to_list(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            return list(csv.reader(file))\n\n    transfer_s3_to_sql = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_list,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql]\n\n    # [START howto_transfer_s3_to_sql_generator]\n    #\n    # As the parser can return any kind of iterator, a generator is also allowed.\n    # This example parser returns a generator which prevents python from loading\n    # the whole file into memory.\n    #\n\n    def parse_csv_to_generator(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            yield from csv.reader(file)\n\n    transfer_s3_to_sql_generator = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql_paser_to_generator\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_generator,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql_generator]\n\n    check_table = SQLTableCheckOperator(\n        task_id=\"check_table\",\n        conn_id=conn_id_name,\n        table=SQL_TABLE_NAME,\n        checks={\n            \"row_count_check\": {\"check_statement\": \"COUNT(*) = 6\"},\n        },\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        conn_id=conn_id_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"drop_table\",\n        sql=f\"DROP TABLE {SQL_TABLE_NAME}\",\n    )\n\n    delete_s3_objects = S3DeleteObjectsOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_objects\",\n        bucket=s3_bucket_name,\n        keys=s3_key,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_bucket,\n        create_object,\n        create_table,\n        # TEST BODY\n        transfer_s3_to_sql,\n        transfer_s3_to_sql_generator,\n        check_table,\n        # TEST TEARDOWN\n        drop_table,\n        delete_s3_objects,\n        delete_s3_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.258474", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 689, "file_name": "example_sagemaker.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import get_current_context\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateExperimentOperator,\n    SageMakerDeleteModelOperator,\n    SageMakerModelOperator,\n    SageMakerProcessingOperator,\n    SageMakerRegisterModelVersionOperator,\n    SageMakerStartPipelineOperator,\n    SageMakerStopPipelineOperator,\n    SageMakerTrainingOperator,\n    SageMakerTransformOperator,\n    SageMakerTuningOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerAutoMLSensor,\n    SageMakerPipelineSensor,\n    SageMakerTrainingSensor,\n    SageMakerTransformSensor,\n    SageMakerTuningSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_sagemaker\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# The URI of a Docker image for handling KNN model training.\n# To find the URI of a free Amazon-provided image that can be used, substitute your\n# desired region in the following link and find the URI under \"Registry Path\".\n# https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-east-1.html#knn-us-east-1.title\n# This URI should be in the format of {12-digits}.dkr.ecr.{region}.amazonaws.com/knn\nKNN_IMAGES_BY_REGION = {\n    \"us-east-1\": \"382416733822.dkr.ecr.us-east-1.amazonaws.com/knn:1\",\n    \"us-west-2\": \"174872318107.dkr.ecr.us-west-2.amazonaws.com/knn:1\",\n}\n\nSAMPLE_SIZE = 600\n\n# This script will be the entrypoint for the docker image which will handle preprocessing the raw data\n# NOTE:  The following string must remain dedented as it is being written to a file.\nPREPROCESS_SCRIPT_TEMPLATE = \"\"\"\nimport boto3\nimport numpy as np\nimport pandas as pd\n\ndef main():\n    # Load the dataset from {input_path}/input.csv, split it into train/test\n    # subsets, and write them to {output_path}/ for the Processing Operator.\n\n    data = pd.read_csv('{input_path}/input.csv')\n\n    # Split into test and train data\n    data_train, data_test = np.split(\n        data.sample(frac=1, random_state=np.random.RandomState()), [int(0.7 * len(data))]\n    )\n\n    # Remove the \"answers\" from the test set\n    data_test.drop(['class'], axis=1, inplace=True)\n\n    # Write the splits to disk\n    data_train.to_csv('{output_path}/train.csv', index=False, header=False)\n    data_test.to_csv('{output_path}/test.csv', index=False, header=False)\n\n    print('Preprocessing Done.')\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n\ndef _create_ecr_repository(repo_name):\n    execution_role_arn = boto3.client(\"sts\").get_caller_identity()[\"Arn\"]\n    access_policy = {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"Allow access to the system test execution role\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"AWS\": execution_role_arn},\n                \"Action\": \"ecr:*\",\n            }\n        ],\n    }\n\n    client = boto3.client(\"ecr\")\n    repo = client.create_repository(repositoryName=repo_name)[\"repository\"]\n    client.set_repository_policy(repositoryName=repo[\"repositoryName\"], policyText=json.dumps(access_policy))\n\n    return repo[\"repositoryUri\"]\n\n\ndef _build_and_upload_docker_image(preprocess_script, repository_uri):\n    \"\"\"\n    We need a Docker image with the following requirements:\n      - Has numpy, pandas, requests, and boto3 installed\n      - Has our data preprocessing script mounted and set as the entry point\n    \"\"\"\n    with NamedTemporaryFile(mode=\"w+t\") as preprocessing_script, NamedTemporaryFile(mode=\"w+t\") as dockerfile:\n        preprocessing_script.write(preprocess_script)\n        preprocessing_script.flush()\n\n        dockerfile.write(\n            f\"\"\"\n            FROM public.ecr.aws/amazonlinux/amazonlinux\n            COPY {preprocessing_script.name.split('/')[2]} /preprocessing.py\n            ADD credentials /credentials\n            ENV AWS_SHARED_CREDENTIALS_FILE=/credentials\n            RUN yum install python3 pip -y\n            RUN pip3 install boto3 pandas requests\n            CMD [ \"python3\", \"/preprocessing.py\"]\n            \"\"\"\n        )\n        dockerfile.flush()\n\n        ecr_region = repository_uri.split(\".\")[3]\n        docker_build_and_push_commands = f\"\"\"\n            cp /root/.aws/credentials /tmp/credentials &&\n            # login to public ecr repo containing amazonlinux image (public login is always on us east 1)\n            aws ecr-public get-login-password --region us-east-1 |\n            docker login --username AWS --password-stdin public.ecr.aws &&\n            docker build --platform=linux/amd64 -f {dockerfile.name} -t {repository_uri} /tmp &&\n            rm /tmp/credentials &&\n\n            # login again, this time to the private repo we created to hold that specific image\n            aws ecr get-login-password --region {ecr_region} |\n            docker login --username AWS --password-stdin {repository_uri} &&\n            docker push {repository_uri}\n            \"\"\"\n        logging.info(\"building and uploading docker image for preprocessing...\")\n        docker_build = subprocess.Popen(\n            docker_build_and_push_commands,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _, stderr = docker_build.communicate()\n        if docker_build.returncode != 0:\n            raise RuntimeError(\n                \"Failed to prepare docker image for the preprocessing job.\\n\"\n                \"The following error happened while executing the sequence of bash commands:\\n\"\n                f\"{stderr.decode()}\"\n            )\n\n\ndef generate_data() -> str:\n    \"\"\"generates a very simple csv dataset with headers\"\"\"\n    content = \"class,x,y\\n\"  # headers\n    for i in range(SAMPLE_SIZE):\n        content += f\"{i%100},{i},{SAMPLE_SIZE-i}\\n\"\n    return content\n\n\n@task\ndef set_up(env_id, role_arn):\n    bucket_name = f\"{env_id}-sagemaker-example\"\n    ecr_repository_name = f\"{env_id}-repo\"\n    model_name = f\"{env_id}-KNN-model\"\n    processing_job_name = f\"{env_id}-processing\"\n    training_job_name = f\"{env_id}-train\"\n    transform_job_name = f\"{env_id}-transform\"\n    tuning_job_name = f\"{env_id}-tune\"\n    model_package_group_name = f\"{env_id}-group\"\n    pipeline_name = f\"{env_id}-pipe\"\n    auto_ml_job_name = f\"{env_id}-automl\"\n    experiment_name = f\"{env_id}-experiment\"\n\n    input_data_S3_key = f\"{env_id}/processed-input-data\"\n    prediction_output_s3_key = f\"{env_id}/transform\"\n    processing_local_input_path = \"/opt/ml/processing/input\"\n    processing_local_output_path = \"/opt/ml/processing/output\"\n    raw_data_s3_key = f\"{env_id}/preprocessing/input.csv\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    ecr_repository_uri = _create_ecr_repository(ecr_repository_name)\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    # Json definition for a dummy pipeline of 30 chained \"conditional step\" checking that 3 < 6\n    # Each step takes roughly 1 second to execute, so the pipeline runtimes is ~30 seconds, which should be\n    # enough to test stopping and awaiting without race conditions.\n    # Built using sagemaker sdk, and using json.loads(pipeline.definition())\n    pipeline_json_definition = \"\"\"{\"Version\": \"2020-12-01\", \"Metadata\": {}, \"Parameters\": [], \"PipelineExperimentConfig\": {\"ExperimentName\": {\"Get\": \"Execution.PipelineName\"}, \"TrialName\": {\"Get\": \"Execution.PipelineExecutionId\"}}, \"Steps\": [{\"Name\": \"DummyCond29\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond28\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond27\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond26\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond25\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond24\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond23\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond22\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond21\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond20\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond19\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond18\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond17\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond16\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond15\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond14\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond13\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond12\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond11\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond10\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond9\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond8\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond7\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond6\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond5\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond4\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond3\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond2\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond1\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond0\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}]}\"\"\"  # noqa: E501\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.create_pipeline(\n        PipelineName=pipeline_name, PipelineDefinition=pipeline_json_definition, RoleArn=role_arn\n    )\n\n    resource_config = {\n        \"InstanceCount\": 1,\n        \"InstanceType\": \"ml.m5.large\",\n        \"VolumeSizeInGB\": 1,\n    }\n    input_data_uri = f\"s3://{bucket_name}/{raw_data_s3_key}\"\n    processing_config = {\n        \"ProcessingJobName\": processing_job_name,\n        \"ProcessingInputs\": [\n            {\n                \"InputName\": \"input\",\n                \"AppManaged\": False,\n                \"S3Input\": {\n                    \"S3Uri\": input_data_uri,\n                    \"LocalPath\": processing_local_input_path,\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3InputMode\": \"File\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"S3CompressionType\": \"None\",\n                },\n            },\n        ],\n        \"ProcessingOutputConfig\": {\n            \"Outputs\": [\n                {\n                    \"OutputName\": \"output\",\n                    \"S3Output\": {\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}\",\n                        \"LocalPath\": processing_local_output_path,\n                        \"S3UploadMode\": \"EndOfJob\",\n                    },\n                    \"AppManaged\": False,\n                }\n            ]\n        },\n        \"ProcessingResources\": {\n            \"ClusterConfig\": resource_config,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"AppSpecification\": {\n            \"ImageUri\": ecr_repository_uri,\n        },\n        \"RoleArn\": role_arn,\n    }\n\n    training_data_source = {\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"text/csv\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/train.csv\",\n            }\n        },\n    }\n    training_config = {\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"2\",\n            \"k\": \"3\",\n            \"sample_size\": str(SAMPLE_SIZE),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                **training_data_source,\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ExperimentConfig\": {\"ExperimentName\": experiment_name},\n        \"ResourceConfig\": resource_config,\n        \"RoleArn\": role_arn,\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"TrainingJobName\": training_job_name,\n    }\n    model_trained_weights = (\n        f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\"\n    )\n    model_config = {\n        \"ExecutionRoleArn\": role_arn,\n        \"ModelName\": model_name,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": model_trained_weights,\n        },\n    }\n    tuning_config = {\n        \"HyperParameterTuningJobName\": tuning_job_name,\n        \"HyperParameterTuningJobConfig\": {\n            \"Strategy\": \"Bayesian\",\n            \"HyperParameterTuningJobObjective\": {\n                \"MetricName\": \"test:accuracy\",\n                \"Type\": \"Maximize\",\n            },\n            \"ResourceLimits\": {\n                \"MaxNumberOfTrainingJobs\": 10,\n                \"MaxParallelTrainingJobs\": 10,\n            },\n            \"ParameterRanges\": {\n                \"CategoricalParameterRanges\": [],\n                \"IntegerParameterRanges\": [\n                    # Set the min and max values of the hyperparameters you want to tune.\n                    {\n                        \"Name\": \"k\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                    {\n                        \"Name\": \"sample_size\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                ],\n            },\n        },\n        \"TrainingJobDefinition\": {\n            \"StaticHyperParameters\": {\n                \"predictor_type\": \"classifier\",\n                \"feature_dim\": \"2\",\n            },\n            \"AlgorithmSpecification\": {\"TrainingImage\": knn_image_uri, \"TrainingInputMode\": \"File\"},\n            \"InputDataConfig\": [\n                {\n                    \"ChannelName\": \"train\",\n                    **training_data_source,\n                },\n                {\n                    \"ChannelName\": \"test\",\n                    **training_data_source,\n                },\n            ],\n            \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}\"},\n            \"ResourceConfig\": resource_config,\n            \"RoleArn\": role_arn,\n            \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        },\n    }\n    transform_config = {\n        \"TransformJobName\": transform_job_name,\n        \"TransformInput\": {\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/test.csv\",\n                }\n            },\n            \"SplitType\": \"Line\",\n            \"ContentType\": \"text/csv\",\n        },\n        \"TransformOutput\": {\"S3OutputPath\": f\"s3://{bucket_name}/{prediction_output_s3_key}\"},\n        \"TransformResources\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n        },\n        \"ModelName\": model_name,\n    }\n\n    preprocess_script = PREPROCESS_SCRIPT_TEMPLATE.format(\n        input_path=processing_local_input_path, output_path=processing_local_output_path\n    )\n    _build_and_upload_docker_image(preprocess_script, ecr_repository_uri)\n\n    ti = get_current_context()[\"ti\"]\n    ti.xcom_push(key=\"docker_image\", value=ecr_repository_uri)\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"raw_data_s3_key\", value=raw_data_s3_key)\n    ti.xcom_push(key=\"ecr_repository_name\", value=ecr_repository_name)\n    ti.xcom_push(key=\"processing_config\", value=processing_config)\n    ti.xcom_push(key=\"input_data_uri\", value=input_data_uri)\n    ti.xcom_push(key=\"output_data_uri\", value=f\"s3://{bucket_name}/{training_output_s3_key}\")\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"training_job_name\", value=training_job_name)\n    ti.xcom_push(key=\"model_package_group_name\", value=model_package_group_name)\n    ti.xcom_push(key=\"pipeline_name\", value=pipeline_name)\n    ti.xcom_push(key=\"auto_ml_job_name\", value=auto_ml_job_name)\n    ti.xcom_push(key=\"experiment_name\", value=experiment_name)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"inference_code_image\", value=knn_image_uri)\n    ti.xcom_push(key=\"model_trained_weights\", value=model_trained_weights)\n    ti.xcom_push(key=\"tuning_config\", value=tuning_config)\n    ti.xcom_push(key=\"tuning_job_name\", value=tuning_job_name)\n    ti.xcom_push(key=\"transform_config\", value=transform_config)\n    ti.xcom_push(key=\"transform_job_name\", value=transform_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ecr_repository(repository_name):\n    client = boto3.client(\"ecr\")\n\n    # All images must be removed from the repo before it can be deleted.\n    image_ids = client.list_images(repositoryName=repository_name)[\"imageIds\"]\n    client.batch_delete_image(\n        repositoryName=repository_name,\n        imageIds=[{\"imageDigest\": image[\"imageDigest\"] for image in image_ids}],\n    )\n    client.delete_repository(repositoryName=repository_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_model_group(group_name, model_version_arn):\n    sgmk_client = boto3.client(\"sagemaker\")\n    # need to destroy model registered in group first\n    sgmk_client.delete_model_package(ModelPackageName=model_version_arn)\n    sgmk_client.delete_model_package_group(ModelPackageGroupName=group_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_pipeline(pipeline_name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.delete_pipeline(PipelineName=pipeline_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_experiment(name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    trials = sgmk_client.list_trials(ExperimentName=name)\n    trials_names = [s[\"TrialName\"] for s in trials[\"TrialSummaries\"]]\n    for trial in trials_names:\n        components = sgmk_client.list_trial_components(TrialName=trial)\n        components_names = [s[\"TrialComponentName\"] for s in components[\"TrialComponentSummaries\"]]\n        for component in components_names:\n            sgmk_client.disassociate_trial_component(TrialComponentName=component, TrialName=trial)\n            sgmk_client.delete_trial_component(TrialComponentName=component)\n        sgmk_client.delete_trial(TrialName=trial)\n    sgmk_client.delete_experiment(ExperimentName=name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_docker_image(image_name):\n    docker_build = subprocess.Popen(\n        f\"docker rmi {image_name}\",\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, stderr = docker_build.communicate()\n    if docker_build.returncode != 0:\n        logging.error(\n            \"Failed to delete local docker image. \"\n            \"Run 'docker images' to see if you need to clean it yourself.\\n\"\n            f\"error message: {stderr}\"\n        )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    test_setup = set_up(\n        env_id=env_id,\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_dataset = S3CreateObjectOperator(\n        task_id=\"upload_dataset\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=test_setup[\"raw_data_s3_key\"],\n        data=generate_data(),\n        replace=True,\n    )\n\n    # [START howto_operator_sagemaker_auto_ml]\n    automl = SageMakerAutoMLOperator(\n        task_id=\"auto_ML\",\n        job_name=test_setup[\"auto_ml_job_name\"],\n        s3_input=test_setup[\"input_data_uri\"],\n        target_attribute=\"class\",\n        s3_output=test_setup[\"output_data_uri\"],\n        role_arn=test_context[ROLE_ARN_KEY],\n        time_limit=30,  # will stop the job before it can do anything, but it's not the point here\n    )\n    # [END howto_operator_sagemaker_auto_ml]\n    automl.wait_for_completion = False  # just to be able to test the sensor next\n\n    # [START howto_sensor_sagemaker_auto_ml]\n    await_automl = SageMakerAutoMLSensor(job_name=test_setup[\"auto_ml_job_name\"], task_id=\"await_auto_ML\")\n    # [END howto_sensor_sagemaker_auto_ml]\n    await_automl.poke_interval = 10\n\n    # [START howto_operator_sagemaker_start_pipeline]\n    start_pipeline1 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline1\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n    # [END howto_operator_sagemaker_start_pipeline]\n\n    # [START howto_operator_sagemaker_stop_pipeline]\n    stop_pipeline1 = SageMakerStopPipelineOperator(\n        task_id=\"stop_pipeline1\",\n        pipeline_exec_arn=start_pipeline1.output,\n    )\n    # [END howto_operator_sagemaker_stop_pipeline]\n\n    start_pipeline2 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline2\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n\n    # [START howto_sensor_sagemaker_pipeline]\n    await_pipeline2 = SageMakerPipelineSensor(\n        task_id=\"await_pipeline2\",\n        pipeline_exec_arn=start_pipeline2.output,\n    )\n    # [END howto_sensor_sagemaker_pipeline]\n    await_pipeline2.poke_interval = 10\n\n    # [START howto_operator_sagemaker_experiment]\n    create_experiment = SageMakerCreateExperimentOperator(\n        task_id=\"create_experiment\", name=test_setup[\"experiment_name\"]\n    )\n    # [END howto_operator_sagemaker_experiment]\n\n    # [START howto_operator_sagemaker_processing]\n    preprocess_raw_data = SageMakerProcessingOperator(\n        task_id=\"preprocess_raw_data\",\n        config=test_setup[\"processing_config\"],\n    )\n    # [END howto_operator_sagemaker_processing]\n\n    # [START howto_operator_sagemaker_training]\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n    # [END howto_operator_sagemaker_training]\n\n    # SageMakerTrainingOperator waits by default, setting as False to test the Sensor below.\n    train_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_training]\n    await_training = SageMakerTrainingSensor(\n        task_id=\"await_training\",\n        job_name=test_setup[\"training_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_training]\n\n    # [START howto_operator_sagemaker_model]\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n    # [END howto_operator_sagemaker_model]\n\n    # [START howto_operator_sagemaker_register]\n    register_model = SageMakerRegisterModelVersionOperator(\n        task_id=\"register_model\",\n        image_uri=test_setup[\"inference_code_image\"],\n        model_url=test_setup[\"model_trained_weights\"],\n        package_group_name=test_setup[\"model_package_group_name\"],\n    )\n    # [END howto_operator_sagemaker_register]\n\n    # [START howto_operator_sagemaker_tuning]\n    tune_model = SageMakerTuningOperator(\n        task_id=\"tune_model\",\n        config=test_setup[\"tuning_config\"],\n    )\n    # [END howto_operator_sagemaker_tuning]\n\n    # SageMakerTuningOperator waits by default, setting as False to test the Sensor below.\n    tune_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_tuning]\n    await_tuning = SageMakerTuningSensor(\n        task_id=\"await_tuning\",\n        job_name=test_setup[\"tuning_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_tuning]\n\n    # [START howto_operator_sagemaker_transform]\n    test_model = SageMakerTransformOperator(\n        task_id=\"test_model\",\n        config=test_setup[\"transform_config\"],\n    )\n    # [END howto_operator_sagemaker_transform]\n\n    # SageMakerTransformOperator waits by default, setting as False to test the Sensor below.\n    test_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_transform]\n    await_transform = SageMakerTransformSensor(\n        task_id=\"await_transform\",\n        job_name=test_setup[\"transform_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_transform]\n\n    # [START howto_operator_sagemaker_delete_model]\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n    # [END howto_operator_sagemaker_delete_model]\n    delete_model.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/ProcessingJobs\", env_id),\n            (\"/aws/sagemaker/TrainingJobs\", env_id),\n            (\"/aws/sagemaker/TransformJobs\", env_id),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_dataset,\n        # TEST BODY\n        automl,\n        await_automl,\n        start_pipeline1,\n        start_pipeline2,\n        stop_pipeline1,\n        await_pipeline2,\n        create_experiment,\n        preprocess_raw_data,\n        train_model,\n        await_training,\n        create_model,\n        register_model,\n        tune_model,\n        await_tuning,\n        test_model,\n        await_transform,\n        # TEST TEARDOWN\n        delete_ecr_repository(test_setup[\"ecr_repository_name\"]),\n        delete_model_group(test_setup[\"model_package_group_name\"], register_model.output),\n        delete_model,\n        delete_bucket,\n        delete_experiment(test_setup[\"experiment_name\"]),\n        delete_pipeline(test_setup[\"pipeline_name\"]),\n        delete_docker_image(test_setup[\"docker_image\"]),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.269436", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 241, "file_name": "example_sagemaker_endpoint.py"}, "content": "TRAIN_DATA = \"0,4.9,2.5,4.5,1.7\\n1,7.0,3.2,4.7,1.4\\n0,7.3,2.9,6.3,1.8\\n2,5.1,3.5,1.4,0.2\\n\"\nSAMPLE_TEST_DATA = \"6.4,3.2,4.5,1.5\"\n\n\n@task\ndef call_endpoint(endpoint_name):\n    response = (\n        boto3.Session()\n        .client(\"sagemaker-runtime\")\n        .invoke_endpoint(\n            EndpointName=endpoint_name,\n            ContentType=\"text/csv\",\n            Body=SAMPLE_TEST_DATA,\n        )\n    )\n\n    return json.loads(response[\"Body\"].read().decode())[\"predictions\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint_config(endpoint_config_job_name):\n    boto3.client(\"sagemaker\").delete_endpoint_config(EndpointConfigName=endpoint_config_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint(endpoint_name):\n    boto3.client(\"sagemaker\").delete_endpoint(EndpointName=endpoint_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef archive_logs(log_group_name):\n    boto3.client(\"logs\").put_retention_policy(logGroupName=log_group_name, retentionInDays=1)\n\n\n@task\ndef set_up(env_id, role_arn, ti=None):\n    bucket_name = f\"{env_id}-sagemaker\"\n    input_data_s3_key = f\"{env_id}/input-data\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    endpoint_config_job_name = f\"{env_id}-endpoint-config\"\n    endpoint_name = f\"{env_id}-endpoint\"\n    model_name = f\"{env_id}-KNN-model\"\n    training_job_name = f\"{env_id}-train\"\n\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    training_config = {\n        \"TrainingJobName\": training_job_name,\n        \"RoleArn\": role_arn,\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"4\",\n            \"k\": \"3\",\n            \"sample_size\": str(TRAIN_DATA.count(\"\\n\") - 1),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                \"CompressionType\": \"None\",\n                \"ContentType\": \"text/csv\",\n                \"DataSource\": {\n                    \"S3DataSource\": {\n                        \"S3DataDistributionType\": \"FullyReplicated\",\n                        \"S3DataType\": \"S3Prefix\",\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_s3_key}/train.csv\",\n                    }\n                },\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ResourceConfig\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n            \"VolumeSizeInGB\": 1,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 6 * 60},\n    }\n\n    model_config = {\n        \"ModelName\": model_name,\n        \"ExecutionRoleArn\": role_arn,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\",  # noqa: E501\n        },\n    }\n\n    endpoint_config_config = {\n        \"EndpointConfigName\": endpoint_config_job_name,\n        \"ProductionVariants\": [\n            {\n                \"VariantName\": f\"{env_id}-demo\",\n                \"ModelName\": model_name,\n                \"InstanceType\": \"ml.t2.medium\",\n                \"InitialInstanceCount\": 1,\n            },\n        ],\n    }\n\n    deploy_endpoint_config = {\n        \"EndpointName\": endpoint_name,\n        \"EndpointConfigName\": endpoint_config_job_name,\n    }\n\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"input_data_s3_key\", value=input_data_s3_key)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"endpoint_name\", value=endpoint_name)\n    ti.xcom_push(key=\"endpoint_config_job_name\", value=endpoint_config_job_name)\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"endpoint_config_config\", value=endpoint_config_config)\n    ti.xcom_push(key=\"deploy_endpoint_config\", value=deploy_endpoint_config)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    test_setup = set_up(\n        env_id=test_context[ENV_ID_KEY],\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_data = S3CreateObjectOperator(\n        task_id=\"upload_data\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=f'{test_setup[\"input_data_s3_key\"]}/train.csv',\n        data=TRAIN_DATA,\n    )\n\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n\n    # [START howto_operator_sagemaker_endpoint_config]\n    configure_endpoint = SageMakerEndpointConfigOperator(\n        task_id=\"configure_endpoint\",\n        config=test_setup[\"endpoint_config_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint_config]\n\n    # [START howto_operator_sagemaker_endpoint]\n    deploy_endpoint = SageMakerEndpointOperator(\n        task_id=\"deploy_endpoint\",\n        config=test_setup[\"deploy_endpoint_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint]\n\n    # SageMakerEndpointOperator waits by default, setting as False to test the Sensor below.\n    deploy_endpoint.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_endpoint]\n    await_endpoint = SageMakerEndpointSensor(\n        task_id=\"await_endpoint\",\n        endpoint_name=test_setup[\"endpoint_name\"],\n    )\n    # [END howto_sensor_sagemaker_endpoint]\n\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/TrainingJobs\", test_context[ENV_ID_KEY]),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_data,\n        # TEST BODY\n        train_model,\n        create_model,\n        configure_endpoint,\n        deploy_endpoint,\n        await_endpoint,\n        call_endpoint(test_setup[\"endpoint_name\"]),\n        # TEST TEARDOWN\n        delete_endpoint_config(test_setup[\"endpoint_config_job_name\"]),\n        delete_endpoint(test_setup[\"endpoint_name\"]),\n        delete_model,\n        delete_bucket,\n        log_cleanup,\n        archive_logs(f\"/aws/sagemaker/Endpoints/{test_setup['endpoint_name']}\"),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.272122", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 72, "file_name": "example_salesforce_to_s3.py", "included_files": ["__init__.py"]}, "content": "\"\"\"\nThis is a basic example DAG for using `SalesforceToS3Operator` to retrieve Salesforce account\ndata and upload it to an Amazon S3 bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.salesforce_to_s3 import SalesforceToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_salesforce_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 7, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-salesforce-to-s3-bucket\"\n    s3_key = f\"{env_id}-salesforce-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_salesforce_to_s3]\n    upload_salesforce_data_to_s3 = SalesforceToS3Operator(\n        task_id=\"upload_salesforce_to_s3\",\n        salesforce_query=\"SELECT AccountNumber, Name FROM Account\",\n        s3_bucket_name=s3_bucket,\n        s3_key=s3_key,\n        salesforce_conn_id=\"salesforce\",\n        replace=True,\n    )\n    # [END howto_transfer_salesforce_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        upload_salesforce_data_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.312080", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 66, "file_name": "example_sftp_to_s3.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.sftp_to_s3 import SFTPToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_sftp_to_s3\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-sftp-to-s3-bucket\"\n    s3_key = f\"{env_id}-sftp-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_sftp_to_s3]\n    sftp_to_s3_job = SFTPToS3Operator(\n        task_id=\"sftp_to_s3_job\",\n        sftp_path=\"/tmp/sftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_sftp_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        sftp_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.312583", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 96, "file_name": "example_sqs.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom airflow.providers.amazon.aws.operators.sqs import SqsPublishOperator\nfrom airflow.providers.amazon.aws.sensors.sqs import SqsSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_sqs\"\n\n\n@task\ndef create_queue(queue_name) -> str:\n    return SqsHook().create_queue(queue_name=queue_name)[\"QueueUrl\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_queue(queue_url):\n    SqsHook().conn.delete_queue(QueueUrl=queue_url)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    sns_queue_name = f\"{env_id}-example-queue\"\n\n    sqs_queue = create_queue(sns_queue_name)\n\n    # [START howto_operator_sqs]\n    publish_to_queue_1 = SqsPublishOperator(\n        task_id=\"publish_to_queue_1\",\n        sqs_queue=sqs_queue,\n        message_content=\"{{ task_instance }}-{{ logical_date }}\",\n    )\n    publish_to_queue_2 = SqsPublishOperator(\n        task_id=\"publish_to_queue_2\",\n        sqs_queue=sqs_queue,\n        message_content=\"{{ task_instance }}-{{ logical_date }}\",\n    )\n    # [END howto_operator_sqs]\n\n    # [START howto_sensor_sqs]\n    read_from_queue = SqsSensor(\n        task_id=\"read_from_queue\",\n        sqs_queue=sqs_queue,\n    )\n    # Retrieve multiple batches of messages from SQS.\n    # The SQS API only returns a maximum of 10 messages per poll.\n    read_from_queue_in_batch = SqsSensor(\n        task_id=\"read_from_queue_in_batch\",\n        sqs_queue=sqs_queue,\n        # Get maximum 10 messages each poll\n        max_messages=10,\n        # Combine 3 polls before returning results\n        num_batches=3,\n    )\n    # [END howto_sensor_sqs]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        sqs_queue,\n        # TEST BODY\n        publish_to_queue_1,\n        read_from_queue,\n        publish_to_queue_2,\n        read_from_queue_in_batch,\n        # TEST TEARDOWN\n        delete_queue(sqs_queue),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.317367", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_sns.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n    # [END howto_operator_sns_publish_operator]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_sns_topic,\n        # TEST BODY\n        publish_message,\n        # TEST TEARDOWN\n        delete_topic(create_sns_topic),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.317472", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 108, "file_name": "example_step_functions.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.step_function import StepFunctionHook\nfrom airflow.providers.amazon.aws.operators.step_function import (\n    StepFunctionGetExecutionOutputOperator,\n    StepFunctionStartExecutionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.step_function import StepFunctionExecutionSensor\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_step_functions\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSTATE_MACHINE_DEFINITION = {\n    \"StartAt\": \"Wait\",\n    \"States\": {\"Wait\": {\"Type\": \"Wait\", \"Seconds\": 7, \"Next\": \"Success\"}, \"Success\": {\"Type\": \"Succeed\"}},\n}\n\n\n@task\ndef create_state_machine(env_id, role_arn):\n    # Create a Step Functions State Machine and return the ARN for use by\n    # downstream tasks.\n    return (\n        StepFunctionHook()\n        .get_conn()\n        .create_state_machine(\n            name=f\"{DAG_ID}_{env_id}\",\n            definition=json.dumps(STATE_MACHINE_DEFINITION),\n            roleArn=role_arn,\n        )[\"stateMachineArn\"]\n    )\n\n\n@task\ndef delete_state_machine(state_machine_arn):\n    StepFunctionHook().get_conn().delete_state_machine(stateMachineArn=state_machine_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # This context contains the ENV_ID and any env variables requested when the\n    # task was built above. Access the info as you would any other TaskFlow task.\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    state_machine_arn = create_state_machine(env_id, role_arn)\n\n    # [START howto_operator_step_function_start_execution]\n    start_execution = StepFunctionStartExecutionOperator(\n        task_id=\"start_execution\", state_machine_arn=state_machine_arn\n    )\n    # [END howto_operator_step_function_start_execution]\n\n    execution_arn = start_execution.output\n\n    # [START howto_sensor_step_function_execution]\n    wait_for_execution = StepFunctionExecutionSensor(\n        task_id=\"wait_for_execution\", execution_arn=execution_arn\n    )\n    # [END howto_sensor_step_function_execution]\n    wait_for_execution.poke_interval = 1\n\n    # [START howto_operator_step_function_get_execution_output]\n    get_execution_output = StepFunctionGetExecutionOutputOperator(\n        task_id=\"get_execution_output\", execution_arn=execution_arn\n    )\n    # [END howto_operator_step_function_get_execution_output]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        state_machine_arn,\n        # TEST BODY\n        start_execution,\n        wait_for_execution,\n        get_execution_output,\n        # TEST TEARDOWN\n        delete_state_machine(state_machine_arn),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.321888", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 218, "file_name": "example_sql_to_s3.py", "included_files": ["__init__.py", "ec2.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.sql_to_s3 import SqlToS3Operator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_sql_to_s3\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nREDSHIFT_TABLE = \"test_table\"\nSQL_QUERY = f\"SELECT * FROM {REDSHIFT_TABLE}\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-s3-bucket\"\n    key = f\"{env_id}-key\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_sql_to_s3]\n    sql_to_s3_task = SqlToS3Operator(\n        task_id=\"sql_to_s3_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n    )\n    # [END howto_transfer_sql_to_s3]\n\n    # [START howto_transfer_sql_to_s3_with_groupby_param]\n    sql_to_s3_task_with_groupby = SqlToS3Operator(\n        task_id=\"sql_to_s3_with_groupby_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n        groupby_kwargs={\"by\": \"color\"},\n    )\n    # [END howto_transfer_sql_to_s3_with_groupby_param]\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        sql_to_s3_task,\n        sql_to_s3_task_with_groupby,\n        # TEST TEARDOWN\n        delete_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.323456", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 10, "file_name": "example_beam.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n\n    jar_to_local_direct_runner >> start_java_pipeline_direct_runner\n    # [END howto_operator_start_java_direct_runner_pipeline]\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.332025", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 9, "file_name": "example_beam_java_flink.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n\n    jar_to_local_flink_runner >> start_java_pipeline_flink_runner\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.333864", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 25, "file_name": "example_go_dataflow.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        },\n        dataflow_config=DataflowConfiguration(\n            job_name=\"{{task.task_id}}\",\n            project_id=GCP_PROJECT_ID,\n            location=\"us-central1\",\n            wait_until_finished=False,\n        ),\n    )\n\n    wait_for_go_job_dataflow_runner_async_done = DataflowJobStatusSensor(\n        task_id=\"wait-for-go-job-async-done\",\n        job_id=\"{{task_instance.xcom_pull('start_go_job_dataflow_runner_async')['dataflow_job_id']}}\",\n        expected_statuses={DataflowJobStatus.JOB_STATE_DONE},\n        project_id=GCP_PROJECT_ID,\n        location=\"us-central1\",\n    )\n\n    start_go_job_dataflow_runner_async >> wait_for_go_job_dataflow_runner_async_done\n    # [END howto_operator_start_go_dataflow_runner_pipeline_async_gcs_file]\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.373760", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 20, "file_name": "example_go.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        runner=\"FlinkRunner\",\n        pipeline_options={\n            \"output\": \"/tmp/start_go_pipeline_local_flink_runner\",\n        },\n    )\n\n    (\n        [\n            start_go_pipeline_local_direct_runner,\n            start_go_pipeline_direct_runner,\n        ]\n        >> start_go_pipeline_local_flink_runner\n        >> start_go_pipeline_local_spark_runner\n    )\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.375596", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 9, "file_name": "example_beam_java_spark.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    )\n\n    jar_to_local_spark_runner >> start_java_pipeline_spark_runner\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.377480", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 21, "file_name": "example_python_async.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n        deferrable=True,\n    )\n    # [END howto_operator_start_python_pipeline_local_runner_flink_runner_async]\n\n    (\n        [\n            start_python_pipeline_local_direct_runner,\n            start_python_pipeline_direct_runner,\n        ]\n        >> start_python_pipeline_dataflow_runner\n        >> start_python_pipeline_local_flink_runner\n        >> start_python_pipeline_local_spark_runner\n    )\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.379002", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 19, "file_name": "example_python.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n    )\n\n    (\n        [\n            start_python_pipeline_local_direct_runner,\n            start_python_pipeline_direct_runner,\n        ]\n        >> start_python_pipeline_dataflow_runner\n        >> start_python_pipeline_local_flink_runner\n        >> start_python_pipeline_local_spark_runner\n    )\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.387462", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_cassandra_dag.py"}, "content": "# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_cassandra_operator\"\n# [START howto_operator_cassandra_sensors]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"table\": \"keyspace_name.table_name\"},\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    table_sensor = CassandraTableSensor(task_id=\"cassandra_table_sensor\")\n\n    record_sensor = CassandraRecordSensor(task_id=\"cassandra_record_sensor\", keys={\"p1\": \"v1\", \"p2\": \"v2\"})\n# [END howto_operator_cassandra_sensors]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.390869", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 26, "file_name": "example_python_dataflow.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n        dataflow_config=DataflowConfiguration(\n            job_name=\"{{task.task_id}}\",\n            project_id=GCP_PROJECT_ID,\n            location=\"us-central1\",\n            wait_until_finished=False,\n        ),\n    )\n\n    wait_for_python_job_dataflow_runner_async_done = DataflowJobStatusSensor(\n        task_id=\"wait-for-python-job-async-done\",\n        job_id=\"{{task_instance.xcom_pull('start_python_job_dataflow_runner_async')['dataflow_job_id']}}\",\n        expected_statuses={DataflowJobStatus.JOB_STATE_DONE},\n        project_id=GCP_PROJECT_ID,\n        location=\"us-central1\",\n    )\n\n    start_python_job_dataflow_runner_async >> wait_for_python_job_dataflow_runner_async_done\n    # [END howto_operator_start_python_dataflow_runner_pipeline_async_gcs_file]\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.394959", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 11, "file_name": "example_java_dataflow.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        dataflow_config={\"job_name\": \"{{task.task_id}}\", \"location\": \"us-central1\"},\n    )\n\n    jar_to_local_dataflow_runner >> start_java_pipeline_dataflow\n    # [END howto_operator_start_java_dataflow_runner_pipeline]\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.395067", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_drill_dag.py"}, "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_drill_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_drill]\n    sql_task = DrillOperator(\n        task_id=\"json_to_parquet_table\",\n        sql=\"\"\"\n        drop table if exists dfs.tmp.employee;\n        create table dfs.tmp.employee as select * from cp.`employee.json`;\n        \"\"\",\n    )\n    # [END howto_operator_drill]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.431480", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 32, "file_name": "example_druid_dag.py"}, "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_druid_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_druid_submit]\n    submit_job = DruidOperator(task_id=\"spark_submit_job\", json_index_file=\"json_index.json\")\n    # Example content of json_index.json:\n    JSON_INDEX_STR = \"\"\"\n        {\n            \"type\": \"index_hadoop\",\n            \"datasource\": \"datasource_prd\",\n            \"spec\": {\n                \"dataSchema\": {\n                    \"granularitySpec\": {\n                        \"intervals\": [\"2021-09-01/2021-09-02\"]\n                    }\n                }\n            }\n        }\n    \"\"\"\n    # [END howto_operator_druid_submit]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.437395", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 112, "file_name": "example_dag_event_listener.py"}, "content": "from airflow.utils import db\n\n\ndef load_connections():\n    db.merge_conn(\n        Connection(\n            conn_id=\"fizz_buzz_1\",\n            conn_type=\"kafka\",\n            extra=json.dumps({\"socket.timeout.ms\": 10, \"bootstrap.servers\": \"broker:29092\"}),\n        )\n    )\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"fizz_buzz_2\",\n            conn_type=\"kafka\",\n            extra=json.dumps(\n                {\n                    \"bootstrap.servers\": \"broker:29092\",\n                    \"group.id\": \"fizz_buzz\",\n                    \"enable.auto.commit\": False,\n                    \"auto.offset.reset\": \"beginning\",\n                }\n            ),\n        )\n    )\n\n\ndef _producer_function():\n    for i in range(50):\n        yield (json.dumps(i), json.dumps(i + 1))\n\n\ndef _generate_uuid():\n    return \"\".join(random.choices(string.ascii_lowercase, k=6))\n\n\nwith DAG(\n    dag_id=\"fizzbuzz-load-topic\",\n    description=\"Load Data to fizz_buzz topic\",\n    start_date=datetime(2022, 11, 1),\n    catchup=False,\n    tags=[\"fizz-buzz\"],\n) as dag:\n\n    t0 = PythonOperator(task_id=\"load_connections\", python_callable=load_connections)\n\n    t1 = ProduceToTopicOperator(\n        kafka_config_id=\"fizz_buzz_1\",\n        task_id=\"produce_to_topic\",\n        topic=\"fizz_buzz\",\n        producer_function=_producer_function,\n    )\n\nwith DAG(\n    dag_id=\"fizzbuzz-listener-dag\",\n    description=\"listen for messages with mod 3 and mod 5 are zero\",\n    start_date=datetime(2022, 11, 1),\n    catchup=False,\n    tags=[\"fizz\", \"buzz\"],\n):\n\n    def await_function(message):\n        val = json.loads(message.value())\n        print(f\"Value in message is {val}\")\n        if val % 3 == 0:\n            return val\n        if val % 5 == 0:\n            return val\n\n    def pick_downstream_dag(message, **context):\n        if message % 15 == 0:\n            print(f\"encountered {message} - executing external dag!\")\n            TriggerDagRunOperator(trigger_dag_id=\"fizz-buzz\", task_id=f\"{message}{_generate_uuid()}\").execute(\n                context\n            )\n        else:\n            if message % 3 == 0:\n                print(f\"encountered {message} FIZZ !\")\n            if message % 5 == 0:\n                print(f\"encountered {message} BUZZ !\")\n\n    # [START howto_sensor_await_message_trigger_function]\n    listen_for_message = AwaitMessageTriggerFunctionSensor(\n        kafka_config_id=\"fizz_buzz_2\",\n        task_id=\"listen_for_message\",\n        topics=[\"fizz_buzz\"],\n        apply_function=\"example_dag_event_listener.await_function\",\n        event_triggered_function=pick_downstream_dag,\n    )\n    # [END howto_sensor_await_message_trigger_function]\n\n    t0 >> t1\n\nwith DAG(\n    dag_id=\"fizz-buzz\",\n    description=\"Triggered when mod 15 is 0.\",\n    start_date=datetime(2022, 11, 1),\n    catchup=False,\n    tags=[\"fizz-buzz\"],\n):\n\n    def _fizz_buzz():\n        print(\"FIZZ BUZZ\")\n\n    fizz_buzz_task = PythonOperator(task_id=\"fizz_buzz\", python_callable=_fizz_buzz)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.440549", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 136, "file_name": "example_twitter_dag.py"}, "content": "# --------------------------------------------------------------------------------\n# Caveat: This Dag will not run because of missing scripts.\n# The purpose of this is to give you a sample of a real world example DAG!\n# --------------------------------------------------------------------------------\n\n# --------------------------------------------------------------------------------\n# Load The Dependencies\n# --------------------------------------------------------------------------------\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_twitter_dag\"\n\n\n@task\ndef fetch_tweets():\n    \"\"\"\n    This task should call Twitter API and retrieve tweets from yesterday from and to for the four twitter\n    users (Twitter_A,..,Twitter_D) There should be eight csv output files generated by this task and naming\n    convention is direction(from or to)_twitterHandle_date.csv\n    \"\"\"\n\n\n@task\ndef clean_tweets():\n    \"\"\"\n    This is a placeholder to clean the eight files. In this step you can get rid of or cherry pick columns\n    and different parts of the text.\n    \"\"\"\n\n\n@task\ndef analyze_tweets():\n    \"\"\"\n    This is a placeholder to analyze the twitter data. Could simply be a sentiment analysis through algorithms\n    like bag of words or something more complicated. You can also take a look at Web Services to do such\n    tasks.\n    \"\"\"\n\n\n@task\ndef transfer_to_db():\n    \"\"\"\n    This is a placeholder to extract summary from Hive data and store it to MySQL.\n    \"\"\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\n        \"owner\": \"Ekhtiar\",\n        \"retries\": 1,\n    },\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    fetch = fetch_tweets()\n    clean = clean_tweets()\n    analyze = analyze_tweets()\n    hive_to_mysql = transfer_to_db()\n\n    fetch >> clean >> analyze\n\n    # --------------------------------------------------------------------------------\n    # The following tasks are generated using for loop. The first task puts the eight\n    # csv files to HDFS. The second task loads these files from HDFS to respected Hive\n    # tables. These two for loops could be combined into one loop. However, in most cases,\n    # you will be running different analysis on your incoming and outgoing tweets,\n    # and hence they are kept separated in this example.\n    # --------------------------------------------------------------------------------\n\n    from_channels = [\"fromTwitter_A\", \"fromTwitter_B\", \"fromTwitter_C\", \"fromTwitter_D\"]\n    to_channels = [\"toTwitter_A\", \"toTwitter_B\", \"toTwitter_C\", \"toTwitter_D\"]\n    yesterday = date.today() - timedelta(days=1)\n    dt = yesterday.strftime(\"%Y-%m-%d\")\n    # define where you want to store the tweets csv file in your local directory\n    local_dir = \"/tmp/\"\n    # define the location where you want to store in HDFS\n    hdfs_dir = \" /tmp/\"\n\n    for channel in to_channels:\n\n        file_name = f\"to_{channel}_{dt}.csv\"\n\n        load_to_hdfs = BashOperator(\n            task_id=f\"put_{channel}_to_hdfs\",\n            bash_command=(\n                f\"HADOOP_USER_NAME=hdfs hadoop fs -put -f {local_dir}{file_name}{hdfs_dir}{channel}/\"\n            ),\n        )\n\n        # [START create_hive]\n        load_to_hive = HiveOperator(\n            task_id=f\"load_{channel}_to_hive\",\n            hql=(\n                f\"LOAD DATA INPATH '{hdfs_dir}{channel}/{file_name}'\"\n                f\"INTO TABLE {channel}\"\n                f\"PARTITION(dt='{dt}')\"\n            ),\n        )\n        # [END create_hive]\n\n        analyze >> load_to_hdfs >> load_to_hive >> hive_to_mysql\n\n    for channel in from_channels:\n        file_name = f\"from_{channel}_{dt}.csv\"\n        load_to_hdfs = BashOperator(\n            task_id=f\"put_{channel}_to_hdfs\",\n            bash_command=(\n                f\"HADOOP_USER_NAME=hdfs hadoop fs -put -f {local_dir}{file_name}{hdfs_dir}{channel}/\"\n            ),\n        )\n\n        load_to_hive = HiveOperator(\n            task_id=f\"load_{channel}_to_hive\",\n            hql=(\n                f\"LOAD DATA INPATH '{hdfs_dir}{channel}/{file_name}' \"\n                f\"INTO TABLE {channel} \"\n                f\"PARTITION(dt='{dt}')\"\n            ),\n        )\n\n        analyze >> load_to_hdfs >> load_to_hive >> hive_to_mysql\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.442713", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 91, "file_name": "example_kylin_dag.py"}, "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kylin_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    default_args={\"project\": \"learn_kylin\", \"cube\": \"kylin_sales_cube\"},\n    tags=[\"example\"],\n) as dag:\n\n    @dag.task\n    def gen_build_time():\n        \"\"\"\n        Gen build time and push to XCom (with key of \"return_value\")\n        :return: A dict with build time values.\n        \"\"\"\n        return {\"date_start\": \"1325347200000\", \"date_end\": \"1325433600000\"}\n\n    gen_build_time_task = gen_build_time()\n    gen_build_time_output_date_start = gen_build_time_task[\"date_start\"]\n    gen_build_time_output_date_end = gen_build_time_task[\"date_end\"]\n\n    build_task1 = KylinCubeOperator(\n        task_id=\"kylin_build_1\",\n        command=\"build\",\n        start_time=gen_build_time_output_date_start,\n        end_time=gen_build_time_output_date_end,\n        is_track_job=True,\n    )\n\n    build_task2 = KylinCubeOperator(\n        task_id=\"kylin_build_2\",\n        command=\"build\",\n        start_time=gen_build_time_output_date_end,\n        end_time=\"1325520000000\",\n        is_track_job=True,\n    )\n\n    refresh_task1 = KylinCubeOperator(\n        task_id=\"kylin_refresh_1\",\n        command=\"refresh\",\n        start_time=gen_build_time_output_date_start,\n        end_time=gen_build_time_output_date_end,\n        is_track_job=True,\n    )\n\n    merge_task = KylinCubeOperator(\n        task_id=\"kylin_merge\",\n        command=\"merge\",\n        start_time=gen_build_time_output_date_start,\n        end_time=\"1325520000000\",\n        is_track_job=True,\n    )\n\n    disable_task = KylinCubeOperator(\n        task_id=\"kylin_disable\",\n        command=\"disable\",\n    )\n\n    purge_task = KylinCubeOperator(\n        task_id=\"kylin_purge\",\n        command=\"purge\",\n    )\n\n    build_task3 = KylinCubeOperator(\n        task_id=\"kylin_build_3\",\n        command=\"build\",\n        start_time=gen_build_time_output_date_end,\n        end_time=\"1328730000000\",\n    )\n\n    build_task1 >> build_task2 >> refresh_task1 >> merge_task >> disable_task >> purge_task >> build_task3\n\n    # Task dependency created via `XComArgs`:\n    #   gen_build_time >> build_task1\n    #   gen_build_time >> build_task2\n    #   gen_build_time >> refresh_task1\n    #   gen_build_time >> merge_task\n    #   gen_build_time >> build_task3\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.449453", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 214, "file_name": "example_dag_hello_kafka.py"}, "content": "default_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_past\": False,\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\n\ndef load_connections():\n    # Connections needed for this example dag to finish\n    from airflow.models import Connection\n    from airflow.utils import db\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"t1-3\",\n            conn_type=\"kafka\",\n            extra=json.dumps({\"socket.timeout.ms\": 10, \"bootstrap.servers\": \"broker:29092\"}),\n        )\n    )\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"t2\",\n            conn_type=\"kafka\",\n            extra=json.dumps(\n                {\n                    \"bootstrap.servers\": \"broker:29092\",\n                    \"group.id\": \"t2\",\n                    \"enable.auto.commit\": False,\n                    \"auto.offset.reset\": \"beginning\",\n                }\n            ),\n        )\n    )\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"t4\",\n            conn_type=\"kafka\",\n            extra=json.dumps(\n                {\n                    \"bootstrap.servers\": \"broker:29092\",\n                    \"group.id\": \"t4\",\n                    \"enable.auto.commit\": False,\n                    \"auto.offset.reset\": \"beginning\",\n                }\n            ),\n        )\n    )\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"t4b\",\n            conn_type=\"kafka\",\n            extra=json.dumps(\n                {\n                    \"bootstrap.servers\": \"broker:29092\",\n                    \"group.id\": \"t4b\",\n                    \"enable.auto.commit\": False,\n                    \"auto.offset.reset\": \"beginning\",\n                }\n            ),\n        )\n    )\n\n    db.merge_conn(\n        Connection(\n            conn_id=\"t5\",\n            conn_type=\"kafka\",\n            extra=json.dumps(\n                {\n                    \"bootstrap.servers\": \"broker:29092\",\n                    \"group.id\": \"t5\",\n                    \"enable.auto.commit\": False,\n                    \"auto.offset.reset\": \"beginning\",\n                }\n            ),\n        )\n    )\n\n\ndef producer_function():\n    for i in range(20):\n        yield (json.dumps(i), json.dumps(i + 1))\n\n\nconsumer_logger = logging.getLogger(\"airflow\")\n\n\ndef consumer_function(message, prefix=None):\n    key = json.loads(message.key())\n    value = json.loads(message.value())\n    consumer_logger.info(f\"{prefix} {message.topic()} @ {message.offset()}; {key} : {value}\")\n    return\n\n\ndef consumer_function_batch(messages, prefix=None):\n    for message in messages:\n        key = json.loads(message.key())\n        value = json.loads(message.value())\n        consumer_logger.info(f\"{prefix} {message.topic()} @ {message.offset()}; {key} : {value}\")\n    return\n\n\ndef await_function(message):\n    if json.loads(message.value()) % 5 == 0:\n        return f\" Got the following message: {json.loads(message.value())}\"\n\n\ndef hello_kafka():\n    print(\"Hello Kafka !\")\n    return\n\n\nwith DAG(\n    \"kafka-example\",\n    default_args=default_args,\n    description=\"Examples of Kafka Operators\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    t0 = PythonOperator(task_id=\"load_connections\", python_callable=load_connections)\n\n    # [START howto_operator_produce_to_topic]\n    t1 = ProduceToTopicOperator(\n        kafka_config_id=\"t1-3\",\n        task_id=\"produce_to_topic\",\n        topic=\"test_1\",\n        producer_function=\"example_dag_hello_kafka.producer_function\",\n    )\n    # [END howto_operator_produce_to_topic]\n\n    t1.doc_md = \"Takes a series of messages from a generator function and publishes\"\n    \"them to the `test_1` topic of our kafka cluster.\"\n\n    # [START howto_operator_consume_from_topic]\n    t2 = ConsumeFromTopicOperator(\n        kafka_config_id=\"t2\",\n        task_id=\"consume_from_topic\",\n        topics=[\"test_1\"],\n        apply_function=\"example_dag_hello_kafka.consumer_function\",\n        apply_function_kwargs={\"prefix\": \"consumed:::\"},\n        commit_cadence=\"end_of_batch\",\n        max_messages=10,\n        max_batch_size=2,\n    )\n    # [END howto_operator_consume_from_topic]\n\n    t2.doc_md = \"Reads a series of messages from the `test_1` topic, and processes\"\n    \"them with a consumer function with a keyword argument.\"\n\n    t3 = ProduceToTopicOperator(\n        kafka_config_id=\"t1-3\",\n        task_id=\"produce_to_topic_2\",\n        topic=\"test_1\",\n        producer_function=producer_function,\n    )\n\n    t3.doc_md = \"Does the same thing as the t1 task, but passes the callable directly\"\n    \"instead of using the string notation.\"\n\n    t4 = ConsumeFromTopicOperator(\n        kafka_config_id=\"t4\",\n        task_id=\"consume_from_topic_2\",\n        topics=[\"test_1\"],\n        apply_function=functools.partial(consumer_function, prefix=\"consumed:::\"),\n        commit_cadence=\"end_of_batch\",\n        max_messages=30,\n        max_batch_size=10,\n    )\n\n    t4b = ConsumeFromTopicOperator(\n        kafka_config_id=\"t4b\",\n        task_id=\"consume_from_topic_2_b\",\n        topics=[\"test_1\"],\n        apply_function_batch=functools.partial(consumer_function_batch, prefix=\"consumed:::\"),\n        commit_cadence=\"end_of_batch\",\n        max_messages=30,\n        max_batch_size=10,\n    )\n\n    t4.doc_md = \"Does the same thing as the t2 task, but passes the callable directly\"\n    \"instead of using the string notation.\"\n\n    # [START howto_sensor_await_message]\n    t5 = AwaitMessageSensor(\n        kafka_config_id=\"t5\",\n        task_id=\"awaiting_message\",\n        topics=[\"test_1\"],\n        apply_function=\"example_dag_hello_kafka.await_function\",\n        xcom_push_key=\"retrieved_message\",\n    )\n    # [END howto_sensor_await_message]\n\n    t5.doc_md = \"A deferable task. Reads the topic `test_1` until a message with a value\"\n    \"divisible by 5 is encountered.\"\n\n    t6 = PythonOperator(task_id=\"hello_kafka\", python_callable=hello_kafka)\n\n    t6.doc_md = \"The task that is executed after the deferable task returns for execution.\"\n\n    t0 >> t1 >> t2\n    t0 >> t3 >> [t4, t4b] >> t5 >> t6\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.452559", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 20, "file_name": "example_livy.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        deferrable=True,\n    )\n\n    livy_python_task_deferrable = LivyOperator(\n        task_id=\"livy_python_task_deferrable\", file=\"/pi.py\", polling_interval=60, deferrable=True\n    )\n\n    livy_java_task_deferrable >> livy_python_task_deferrable\n    # [END create_livy_deferrable]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.454888", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_pig.py"}, "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_run_pipeline\"\n\nwith DAG(\n    dag_id=\"example_pig_operator\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START create_pig]\n    run_this = PigOperator(\n        task_id=\"run_example_pig_script\",\n        pig=\"ls /;\",\n        pig_opts=\"-x local\",\n    )\n    # [END create_pig]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.456990", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_kubernetes.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        image_pull_secrets=[k8s.V1LocalObjectReference(\"testquay\")],\n        cmds=[\"bash\", \"-cx\"],\n        arguments=[\"echo\", \"10\", \"echo pwd\"],\n        labels={\"foo\": \"bar\"},\n        name=\"airflow-private-image-pod\",\n        on_finish_action=\"delete_pod\",\n        in_cluster=True,\n        task_id=\"task-two\",\n        get_logs=True,\n    )\n    # [END howto_operator_k8s_private_image]\n\n    # [START howto_operator_k8s_write_xcom]\n    write_xcom = KubernetesPodOperator(\n        namespace=\"default\",\n        image=\"alpine\",\n        cmds=[\"sh\", \"-c\", \"mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json\"],\n        name=\"write-xcom\",\n        do_xcom_push=True,\n        on_finish_action=\"delete_pod\",\n        in_cluster=True,\n        task_id=\"write-xcom\",\n        get_logs=True,\n    )\n\n    pod_task_xcom_result = BashOperator(\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('write-xcom')[0] }}\\\"\",\n        task_id=\"pod_task_xcom_result\",\n    )\n\n    write_xcom >> pod_task_xcom_result\n    # [END howto_operator_k8s_write_xcom]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.494063", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 31, "file_name": "example_pinot_dag.py"}, "content": "with DAG(\n    dag_id=\"example_pinot_hook\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_pinot_admin_hook]\n    @task\n    def pinot_admin():\n        PinotAdminHook(conn_id=\"pinot_admin_default\", cmd_path=\"pinot-admin.sh\", pinot_admin_system_exit=True)\n\n    # [END howto_operator_pinot_admin_hook]\n    # [START howto_operator_pinot_dbapi_example]\n    @task\n    def pinot_dbi_api():\n        PinotDbApiHook(\n            task_id=\"run_example_pinot_script\",\n            pinot=\"ls /;\",\n            pinot_options=\"-x local\",\n        )\n\n    # [END howto_operator_pinot_dbapi_example]\n\n    pinot_admin()\n    pinot_dbi_api()\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.496659", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_spark_dag.py"}, "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_spark_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_spark_submit]\n    submit_job = SparkSubmitOperator(\n        application=\"${SPARK_HOME}/examples/src/main/python/pi.py\", task_id=\"submit_job\"\n    )\n    # [END howto_operator_spark_submit]\n\n    # [START howto_operator_spark_jdbc]\n    jdbc_to_spark_job = SparkJDBCOperator(\n        cmd_type=\"jdbc_to_spark\",\n        jdbc_table=\"foo\",\n        spark_jars=\"${SPARK_HOME}/jars/postgresql-42.2.12.jar\",\n        jdbc_driver=\"org.postgresql.Driver\",\n        metastore_table=\"bar\",\n        save_mode=\"overwrite\",\n        save_format=\"JSON\",\n        task_id=\"jdbc_to_spark_job\",\n    )\n\n    spark_to_jdbc_job = SparkJDBCOperator(\n        cmd_type=\"spark_to_jdbc\",\n        jdbc_table=\"foo\",\n        spark_jars=\"${SPARK_HOME}/jars/postgresql-42.2.12.jar\",\n        jdbc_driver=\"org.postgresql.Driver\",\n        metastore_table=\"bar\",\n        save_mode=\"append\",\n        task_id=\"spark_to_jdbc_job\",\n    )\n    # [END howto_operator_spark_jdbc]\n\n    # [START howto_operator_spark_sql]\n    spark_sql_job = SparkSqlOperator(\n        sql=\"SELECT COUNT(1) as cnt FROM temp_table\", master=\"local\", task_id=\"spark_sql_job\"\n    )\n    # [END howto_operator_spark_sql]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.498610", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 95, "file_name": "example_asana.py"}, "content": "\"\"\"\nExample DAG showing how to use Asana TaskOperators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.asana.operators.asana_tasks import (\n    AsanaCreateTaskOperator,\n    AsanaDeleteTaskOperator,\n    AsanaFindTaskOperator,\n    AsanaUpdateTaskOperator,\n)\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\n\nASANA_TASK_TO_UPDATE = os.environ.get(\"ASANA_TASK_TO_UPDATE\", \"update_task\")\nASANA_TASK_TO_DELETE = os.environ.get(\"ASANA_TASK_TO_DELETE\", \"delete_task\")\n# This example assumes a default project ID has been specified in the connection. If you\n# provide a different id in ASANA_PROJECT_ID_OVERRIDE, it will override this default\n# project ID in the AsanaFindTaskOperator example below\nASANA_PROJECT_ID_OVERRIDE = os.environ.get(\"ASANA_PROJECT_ID_OVERRIDE\", \"test_project\")\n# This connection should specify a personal access token and a default project ID\nCONN_ID = os.environ.get(\"ASANA_CONNECTION_ID\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_asana\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": CONN_ID},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START asana_example_dag]\n    # [START run_asana_create_task_operator]\n    # Create a task. `task_parameters` is used to specify attributes the new task should have.\n    # You must specify at least one of 'workspace', 'projects', or 'parent' in `task_parameters`\n    # unless these are specified in the connection. Any attributes you specify in\n    # `task_parameters` will override values from the connection.\n    create = AsanaCreateTaskOperator(\n        task_id=\"run_asana_create_task\",\n        task_parameters={\"notes\": \"Some notes about the task.\"},\n        name=\"New Task Name\",\n    )\n    # [END run_asana_create_task_operator]\n\n    # [START run_asana_find_task_operator]\n    # Find tasks matching search criteria. `search_parameters` is used to specify these criteria.\n    # You must specify `project`, `section`, `tag`, `user_task_list`, or both\n    # `assignee` and `workspace` in `search_parameters` or in the connection.\n    # This example shows how you can override a project specified in the connection by\n    # passing a different value for project into `search_parameters`\n    one_week_ago = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n    find = AsanaFindTaskOperator(\n        task_id=\"run_asana_find_task\",\n        search_parameters={\"project\": ASANA_PROJECT_ID_OVERRIDE, \"modified_since\": one_week_ago},\n    )\n    # [END run_asana_find_task_operator]\n\n    # [START run_asana_update_task_operator]\n    # Update a task. `task_parameters` is used to specify the new values of\n    # task attributes you want to update.\n    update = AsanaUpdateTaskOperator(\n        task_id=\"run_asana_update_task\",\n        asana_task_gid=ASANA_TASK_TO_UPDATE,\n        task_parameters={\"notes\": \"This task was updated!\", \"completed\": True},\n    )\n    # [END run_asana_update_task_operator]\n\n    # [START run_asana_delete_task_operator]\n    # Delete a task. This task will complete successfully even if `asana_task_gid` does not exist.\n    delete = AsanaDeleteTaskOperator(\n        task_id=\"run_asana_delete_task\",\n        asana_task_gid=ASANA_TASK_TO_DELETE,\n    )\n    # [END run_asana_delete_task_operator]\n\n    create >> find >> update >> delete\n    # [END asana_example_dag]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.500269", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 44, "file_name": "example_kubernetes_async.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        image_pull_secrets=[k8s.V1LocalObjectReference(\"testquay\")],\n        cmds=[\"bash\", \"-cx\"],\n        arguments=[\"echo\", \"10\", \"echo pwd\"],\n        labels={\"foo\": \"bar\"},\n        name=\"airflow-private-image-pod\",\n        on_finish_action=\"delete_pod\",\n        in_cluster=True,\n        get_logs=True,\n        deferrable=True,\n    )\n    # [END howto_operator_k8s_private_image_async]\n\n    # [START howto_operator_k8s_write_xcom_async]\n    write_xcom_async = KubernetesPodOperator(\n        task_id=\"kubernetes_write_xcom_task_async\",\n        namespace=\"default\",\n        image=\"alpine\",\n        cmds=[\"sh\", \"-c\", \"mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json\"],\n        name=\"write-xcom\",\n        do_xcom_push=True,\n        on_finish_action=\"delete_pod\",\n        in_cluster=True,\n        get_logs=True,\n        deferrable=True,\n    )\n\n    pod_task_xcom_result_async = BashOperator(\n        task_id=\"pod_task_xcom_result_async\",\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('write-xcom')[0] }}\\\"\",\n    )\n\n    write_xcom_async >> pod_task_xcom_result_async\n    # [END howto_operator_k8s_write_xcom_async]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.506117", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 53, "file_name": "example_kubernetes_decorator.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"example_kubernetes_decorator\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"cncf\", \"kubernetes\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_kubernetes]\n    @task.kubernetes(\n        image=\"python:3.8-slim-buster\",\n        name=\"k8s_test\",\n        namespace=\"default\",\n        in_cluster=False,\n        config_file=\"/path/to/.kube/config\",\n    )\n    def execute_in_k8s_pod():\n        import time\n\n        print(\"Hello from k8s pod\")\n        time.sleep(2)\n\n    @task.kubernetes(image=\"python:3.8-slim-buster\", namespace=\"default\", in_cluster=False)\n    def print_pattern():\n        n = 5\n        for i in range(0, n):\n            # inner loop to handle number of columns\n            # values changing acc. to outer loop\n            for j in range(0, i + 1):\n                # printing stars\n                print(\"* \", end=\"\")\n\n            # ending line after each row\n            print(\"\\r\")\n\n    execute_in_k8s_pod_instance = execute_in_k8s_pod()\n    print_pattern_instance = print_pattern()\n\n    execute_in_k8s_pod_instance >> print_pattern_instance\n    # [END howto_operator_kubernetes]\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.509040", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 66, "file_name": "example_kubernetes_resource.py"}, "content": "\"\"\"\nThis is an example DAG which uses KubernetesCreateResourceOperator and KubernetesDeleteResourceOperator.\nIn this example, we create two tasks which execute sequentially.\nThe first task is to create a PVC on Kubernetes cluster.\nand the second task is to delete the PVC.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.cncf.kubernetes.operators.resource import (\n    KubernetesCreateResourceOperator,\n    KubernetesDeleteResourceOperator,\n)\n\npvc_name = \"toto\"\n\npvc_conf = f\"\"\"\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: {pvc_name}\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: standard\n  resources:\n    requests:\n      storage: 5Gi\n\"\"\"\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kubernetes_resource_operator\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"max_active_runs\": 1},\n    description=\"create and delete a PVC in a kubernetes\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    t1 = KubernetesCreateResourceOperator(\n        task_id=\"create_pvc\",\n        yaml_conf=pvc_conf,\n    )\n\n    t2 = KubernetesDeleteResourceOperator(\n        task_id=\"delete_pvc\",\n        yaml_conf=pvc_conf,\n    )\n\n    t1 >> t2\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.511042", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 67, "file_name": "example_spark_kubernetes.py"}, "content": "\"\"\"\nThis is an example DAG which uses SparkKubernetesOperator and SparkKubernetesSensor.\nIn this example, we create two tasks which execute sequentially.\nThe first task is to submit sparkApplication on Kubernetes cluster(the example uses spark-pi application).\nand the second task is to check the final state of the sparkApplication that submitted in the first state.\n\nSpark-on-k8s operator is required to be already installed on Kubernetes\nhttps://github.com/GoogleCloudPlatform/spark-on-k8s-operator\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\n# [START import_module]\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n\n# Operators; we need this to operate!\nfrom airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator\nfrom airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"spark_pi\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"max_active_runs\": 1},\n    description=\"submit spark-pi as sparkApplication on kubernetes\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START SparkKubernetesOperator_DAG]\n    t1 = SparkKubernetesOperator(\n        task_id=\"spark_pi_submit\",\n        namespace=\"default\",\n        application_file=\"example_spark_kubernetes_spark_pi.yaml\",\n        do_xcom_push=True,\n        dag=dag,\n    )\n\n    t2 = SparkKubernetesSensor(\n        task_id=\"spark_pi_monitor\",\n        namespace=\"default\",\n        application_name=\"{{ task_instance.xcom_pull(task_ids='spark_pi_submit')['metadata']['name'] }}\",\n        dag=dag,\n    )\n    t1 >> t2\n\n    # [END SparkKubernetesOperator_DAG]\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.521688", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 68, "file_name": "example_sql_column_table_check.py"}, "content": "from __future__ import annotations\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLColumnCheckOperator, SQLTableCheckOperator\nfrom airflow.utils.dates import datetime\n\nAIRFLOW_DB_METADATA_TABLE = \"ab_role\"\nconnection_args = {\n    \"conn_id\": \"airflow_db\",\n    \"conn_type\": \"Postgres\",\n    \"host\": \"postgres\",\n    \"schema\": \"postgres\",\n    \"login\": \"postgres\",\n    \"password\": \"postgres\",\n    \"port\": 5432,\n}\n\nwith DAG(\n    \"example_sql_column_table_check\",\n    description=\"Example DAG for SQLColumnCheckOperator and SQLTableCheckOperator.\",\n    default_args=connection_args,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    \"\"\"\n    ### Example SQL Column and Table Check DAG\n\n    Runs the SQLColumnCheckOperator and SQLTableCheckOperator against the Airflow metadata DB.\n    \"\"\"\n\n    # [START howto_operator_sql_column_check]\n    column_check = SQLColumnCheckOperator(\n        task_id=\"column_check\",\n        table=AIRFLOW_DB_METADATA_TABLE,\n        column_mapping={\n            \"id\": {\n                \"null_check\": {\n                    \"equal_to\": 0,\n                    \"tolerance\": 0,\n                },\n                \"distinct_check\": {\n                    \"equal_to\": 1,\n                },\n            }\n        },\n    )\n    # [END howto_operator_sql_column_check]\n\n    # [START howto_operator_sql_table_check]\n    row_count_check = SQLTableCheckOperator(\n        task_id=\"row_count_check\",\n        table=AIRFLOW_DB_METADATA_TABLE,\n        checks={\n            \"row_count_check\": {\n                \"check_statement\": \"COUNT(*) = 1\",\n            }\n        },\n    )\n    # [END howto_operator_sql_table_check]\n\n    column_check >> row_count_check\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.554993", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 45, "file_name": "example_sql_execute_query.py"}, "content": "from __future__ import annotations\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.dates import datetime\n\nAIRFLOW_DB_METADATA_TABLE = \"ab_role\"\nconnection_args = {\n    \"conn_id\": \"airflow_db\",\n    \"conn_type\": \"Postgres\",\n    \"host\": \"postgres\",\n    \"schema\": \"postgres\",\n    \"login\": \"postgres\",\n    \"password\": \"postgres\",\n    \"port\": 5432,\n}\n\nwith DAG(\n    \"example_sql_execute_query\",\n    description=\"Example DAG for SQLExecuteQueryOperator.\",\n    default_args=connection_args,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    \"\"\"\n    ### Example SQL execute query DAG\n\n    Runs the SQLExecuteQueryOperator against the Airflow metadata DB.\n    \"\"\"\n\n    # [START howto_operator_sql_execute_query]\n    execute_query = SQLExecuteQueryOperator(\n        task_id=\"execute_query\",\n        sql=f\"SELECT 1; SELECT * FROM {AIRFLOW_DB_METADATA_TABLE} LIMIT 1;\",\n        split_statements=True,\n        return_last=False,\n    )\n    # [END howto_operator_sql_execute_query]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.556465", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 24, "file_name": "example_external_task_child_deferrable.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"child_dag\",\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"async\", \"core\"],\n) as dag:\n    dummy_task = BashOperator(\n        task_id=\"child_task\",\n        bash_command=\"echo 1; sleep 1; echo 2; sleep 2; echo 3; sleep 3\",\n    )\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.558707", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 54, "file_name": "example_external_task_parent_deferrable.py"}, "content": "from __future__ import annotations\n\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\nfrom airflow.utils.timezone import datetime\n\nwith DAG(\n    dag_id=\"example_external_task\",\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"async\", \"core\"],\n) as dag:\n    start = DummyOperator(task_id=\"start\")\n\n    # [START howto_external_task_async_sensor]\n    external_task_sensor = ExternalTaskSensor(\n        task_id=\"parent_task_sensor\",\n        external_task_id=\"child_task\",\n        external_dag_id=\"child_dag\",\n        deferrable=True,\n    )\n    # [END howto_external_task_async_sensor]\n\n    trigger_child_task = TriggerDagRunOperator(\n        task_id=\"trigger_child_task\",\n        trigger_dag_id=\"child_dag\",\n        allowed_states=[\n            \"success\",\n            \"failed\",\n        ],\n        execution_date=\"{{execution_date}}\",\n        poke_interval=5,\n        reset_dag_run=True,\n        wait_for_completion=True,\n    )\n\n    end = DummyOperator(task_id=\"end\")\n\n    start >> [trigger_child_task, external_task_sensor] >> end\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.563779", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 74, "file_name": "example_databricks.py"}, "content": "\"\"\"\nThis is an example DAG which uses the DatabricksSubmitRunOperator.\nIn this example, we create two tasks which execute sequentially.\nThe first task is to run a notebook at the workspace path \"/test\"\nand the second task is to run a JAR uploaded to DBFS. Both,\ntasks use new clusters.\n\nBecause we have set a downstream dependency on the notebook task,\nthe spark jar task will NOT run until the notebook task completes\nsuccessfully.\n\nThe definition of a successful run is if the run has a result_state of \"SUCCESS\".\nFor more information about the state of a run refer to\nhttps://docs.databricks.com/api/latest/jobs.html#runstate\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_databricks_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_json]\n    # Example of using the JSON parameter to initialize the operator.\n    new_cluster = {\n        \"spark_version\": \"9.1.x-scala2.12\",\n        \"node_type_id\": \"r3.xlarge\",\n        \"aws_attributes\": {\"availability\": \"ON_DEMAND\"},\n        \"num_workers\": 8,\n    }\n\n    notebook_task_params = {\n        \"new_cluster\": new_cluster,\n        \"notebook_task\": {\n            \"notebook_path\": \"/Users/airflow@example.com/PrepareData\",\n        },\n    }\n\n    notebook_task = DatabricksSubmitRunOperator(task_id=\"notebook_task\", json=notebook_task_params)\n    # [END howto_operator_databricks_json]\n\n    # [START howto_operator_databricks_named]\n    # Example of using the named parameters of DatabricksSubmitRunOperator\n    # to initialize the operator.\n    spark_jar_task = DatabricksSubmitRunOperator(\n        task_id=\"spark_jar_task\",\n        new_cluster=new_cluster,\n        spark_jar_task={\"main_class_name\": \"com.example.ProcessData\"},\n        libraries=[{\"jar\": \"dbfs:/lib/etl-0.1.jar\"}],\n    )\n    # [END howto_operator_databricks_named]\n    notebook_task >> spark_jar_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.565791", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 85, "file_name": "example_databricks_sensors.py"}, "content": "from __future__ import annotations\n\nimport os\nimport textwrap\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.sensors.databricks_partition import DatabricksPartitionSensor\nfrom airflow.providers.databricks.sensors.databricks_sql import DatabricksSqlSensor\n\n# [Env variable to be used from the OS]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n# [DAG name to be shown on Airflow UI]\nDAG_ID = \"example_databricks_sensor\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    dag.doc_md = textwrap.dedent(\n        \"\"\"\n\n        This is an example DAG which uses the DatabricksSqlSensor\n        sensor. The example task in the DAG executes the provided\n        SQL query against the Databricks SQL warehouse and if a\n        result is returned, the sensor returns True/succeeds.\n        If no results are returned, the sensor returns False/\n        fails.\n\n        \"\"\"\n    )\n    # [START howto_sensor_databricks_connection_setup]\n    # Connection string setup for Databricks workspace.\n    connection_id = \"databricks_default\"\n    sql_warehouse_name = \"Starter Warehouse\"\n    # [END howto_sensor_databricks_connection_setup]\n\n    # [START howto_sensor_databricks_sql]\n    # Example of using the Databricks SQL Sensor to check existence of data in a table.\n    sql_sensor = DatabricksSqlSensor(\n        databricks_conn_id=connection_id,\n        sql_warehouse_name=sql_warehouse_name,\n        catalog=\"hive_metastore\",\n        task_id=\"sql_sensor_task\",\n        sql=\"select * from hive_metastore.temp.sample_table_3 limit 1\",\n        timeout=60 * 2,\n    )\n    # [END howto_sensor_databricks_sql]\n\n    # [START howto_sensor_databricks_partition]\n    # Example of using the Databricks Partition Sensor to check the presence\n    # of the specified partition(s) in a table.\n    partition_sensor = DatabricksPartitionSensor(\n        databricks_conn_id=connection_id,\n        sql_warehouse_name=sql_warehouse_name,\n        catalog=\"hive_metastore\",\n        task_id=\"partition_sensor_task\",\n        table_name=\"sample_table_2\",\n        schema=\"temp\",\n        partitions={\"date\": \"2023-01-03\", \"name\": [\"abc\", \"def\"]},\n        partition_operator=\"=\",\n        timeout=60 * 2,\n    )\n    # [END howto_sensor_databricks_partition]\n\n    # Task dependency between the SQL sensor and the partition sensor.\n    # If the first task(sql_sensor) succeeds, the second task(partition_sensor)\n    # runs, else all the subsequent DAG tasks and the DAG are marked as failed.\n    (sql_sensor >> partition_sensor)\n\n    from tests.system.utils.watcher import watcher\n\n    # This example does not need a watcher in order to properly mark success/failure\n    # since it is a single task, but it is given here as an example for users to\n    # extend it to their use cases.\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.572554", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 74, "file_name": "example_databricks_repos.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.operators.databricks_repos import (\n    DatabricksReposCreateOperator,\n    DatabricksReposDeleteOperator,\n    DatabricksReposUpdateOperator,\n)\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"databricks_conn_id\": \"databricks\",\n}\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_databricks_repos_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    default_args=default_args,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_repo_create]\n    # Example of creating a Databricks Repo\n    repo_path = \"/Repos/user@domain.com/demo-repo\"\n    git_url = \"https://github.com/test/test\"\n    create_repo = DatabricksReposCreateOperator(task_id=\"create_repo\", repo_path=repo_path, git_url=git_url)\n    # [END howto_operator_databricks_repo_create]\n\n    # [START howto_operator_databricks_repo_update]\n    # Example of updating a Databricks Repo to the latest code\n    repo_path = \"/Repos/user@domain.com/demo-repo\"\n    update_repo = DatabricksReposUpdateOperator(task_id=\"update_repo\", repo_path=repo_path, branch=\"releases\")\n    # [END howto_operator_databricks_repo_update]\n\n    notebook_task_params = {\n        \"new_cluster\": {\n            \"spark_version\": \"9.1.x-scala2.12\",\n            \"node_type_id\": \"r3.xlarge\",\n            \"aws_attributes\": {\"availability\": \"ON_DEMAND\"},\n            \"num_workers\": 8,\n        },\n        \"notebook_task\": {\n            \"notebook_path\": f\"{repo_path}/PrepareData\",\n        },\n    }\n\n    notebook_task = DatabricksSubmitRunOperator(task_id=\"notebook_task\", json=notebook_task_params)\n\n    # [START howto_operator_databricks_repo_delete]\n    # Example of deleting a Databricks Repo\n    repo_path = \"/Repos/user@domain.com/demo-repo\"\n    delete_repo = DatabricksReposDeleteOperator(task_id=\"delete_repo\", repo_path=repo_path)\n    # [END howto_operator_databricks_repo_delete]\n\n    (create_repo >> update_repo >> notebook_task >> delete_repo)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.574710", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 107, "file_name": "example_databricks_sql.py"}, "content": "\"\"\"\nThis is an example DAG which uses the DatabricksSqlOperator\nand DatabricksCopyIntoOperator. The first task creates the table\nand inserts values into it. The second task uses DatabricksSqlOperator\nto select the data. The third task selects the data and stores the\noutput of selected data in file path and format specified. The fourth\ntask runs the select SQL statement written in the test.sql file. The\nfinal task using DatabricksCopyIntoOperator loads the data from the\nfile_location passed into Delta table.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks_sql import (\n    DatabricksCopyIntoOperator,\n    DatabricksSqlOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_databricks_sql_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    connection_id = \"my_connection\"\n    sql_endpoint_name = \"My Endpoint\"\n\n    # [START howto_operator_databricks_sql_multiple]\n    # Example of using the Databricks SQL Operator to perform multiple operations.\n    create = DatabricksSqlOperator(\n        databricks_conn_id=connection_id,\n        sql_endpoint_name=sql_endpoint_name,\n        task_id=\"create_and_populate_table\",\n        sql=[\n            \"drop table if exists default.my_airflow_table\",\n            \"create table default.my_airflow_table(id int, v string)\",\n            \"insert into default.my_airflow_table values (1, 'test 1'), (2, 'test 2')\",\n        ],\n    )\n    # [END howto_operator_databricks_sql_multiple]\n\n    # [START howto_operator_databricks_sql_select]\n    # Example of using the Databricks SQL Operator to select data.\n    select = DatabricksSqlOperator(\n        databricks_conn_id=connection_id,\n        sql_endpoint_name=sql_endpoint_name,\n        task_id=\"select_data\",\n        sql=\"select * from default.my_airflow_table\",\n    )\n    # [END howto_operator_databricks_sql_select]\n\n    # [START howto_operator_databricks_sql_select_file]\n    # Example of using the Databricks SQL Operator to select data into a file with JSONL format.\n    select_into_file = DatabricksSqlOperator(\n        databricks_conn_id=connection_id,\n        sql_endpoint_name=sql_endpoint_name,\n        task_id=\"select_data_into_file\",\n        sql=\"select * from default.my_airflow_table\",\n        output_path=\"/tmp/1.jsonl\",\n        output_format=\"jsonl\",\n    )\n    # [END howto_operator_databricks_sql_select_file]\n\n    # [START howto_operator_databricks_sql_multiple_file]\n    # Example of using the Databricks SQL Operator to select data.\n    # SQL statements should be in the file with name test.sql\n    create_file = DatabricksSqlOperator(\n        databricks_conn_id=connection_id,\n        sql_endpoint_name=sql_endpoint_name,\n        task_id=\"create_and_populate_from_file\",\n        sql=\"test.sql\",\n    )\n    # [END howto_operator_databricks_sql_multiple_file]\n\n    # [START howto_operator_databricks_copy_into]\n    # Example of importing data using COPY_INTO SQL command\n    import_csv = DatabricksCopyIntoOperator(\n        task_id=\"import_csv\",\n        databricks_conn_id=connection_id,\n        sql_endpoint_name=sql_endpoint_name,\n        table_name=\"my_table\",\n        file_format=\"CSV\",\n        file_location=\"abfss://container@account.dfs.core.windows.net/my-data/csv\",\n        format_options={\"header\": \"true\"},\n        force_copy=True,\n    )\n    # [END howto_operator_databricks_copy_into]\n\n    (create >> create_file >> import_csv >> select >> select_into_file)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.578431", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": true, "line_count": 97, "file_name": "example_dbt_cloud.py", "included_files": ["__init__.py"]}, "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models import DAG\n\ntry:\n    from airflow.operators.empty import EmptyOperator\nexcept ModuleNotFoundError:\n    from airflow.operators.dummy import DummyOperator as EmptyOperator  # type: ignore\n\nfrom airflow.providers.dbt.cloud.operators.dbt import (\n    DbtCloudGetJobRunArtifactOperator,\n    DbtCloudListJobsOperator,\n    DbtCloudRunJobOperator,\n)\nfrom airflow.providers.dbt.cloud.sensors.dbt import DbtCloudJobRunAsyncSensor, DbtCloudJobRunSensor\nfrom airflow.utils.edgemodifier import Label\nfrom tests.system.utils import get_test_env_id\n\nENV_ID = get_test_env_id()\nDAG_ID = \"example_dbt_cloud\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"dbt_cloud_conn_id\": \"dbt\", \"account_id\": 39151},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    begin = EmptyOperator(task_id=\"begin\")\n    end = EmptyOperator(task_id=\"end\")\n\n    # [START howto_operator_dbt_cloud_run_job]\n    trigger_job_run1 = DbtCloudRunJobOperator(\n        task_id=\"trigger_job_run1\",\n        job_id=48617,\n        check_interval=10,\n        timeout=300,\n    )\n    # [END howto_operator_dbt_cloud_run_job]\n\n    # [START howto_operator_dbt_cloud_get_artifact]\n    get_run_results_artifact = DbtCloudGetJobRunArtifactOperator(\n        task_id=\"get_run_results_artifact\", run_id=trigger_job_run1.output, path=\"run_results.json\"\n    )\n    # [END howto_operator_dbt_cloud_get_artifact]\n\n    # [START howto_operator_dbt_cloud_run_job_async]\n    trigger_job_run2 = DbtCloudRunJobOperator(\n        task_id=\"trigger_job_run2\",\n        job_id=48617,\n        wait_for_termination=False,\n        additional_run_config={\"threads_override\": 8},\n    )\n    # [END howto_operator_dbt_cloud_run_job_async]\n\n    # [START howto_operator_dbt_cloud_run_job_sensor]\n    job_run_sensor = DbtCloudJobRunSensor(\n        task_id=\"job_run_sensor\", run_id=trigger_job_run2.output, timeout=20\n    )\n    # [END howto_operator_dbt_cloud_run_job_sensor]\n\n    # [START howto_operator_dbt_cloud_run_job_sensor_defered]\n    job_run_sensor_defered = DbtCloudJobRunSensor(\n        task_id=\"job_run_sensor_defered\", run_id=trigger_job_run2.output, timeout=20, deferrable=True\n    )\n    # [END howto_operator_dbt_cloud_run_job_sensor_defered]\n\n    # [START howto_operator_dbt_cloud_run_job_async_sensor]\n    job_run_async_sensor = DbtCloudJobRunAsyncSensor(\n        task_id=\"job_run_async_sensor\", run_id=trigger_job_run2.output, timeout=20\n    )\n    # [END howto_operator_dbt_cloud_run_job_async_sensor]\n\n    # [START howto_operator_dbt_cloud_list_jobs]\n    list_dbt_jobs = DbtCloudListJobsOperator(task_id=\"list_dbt_jobs\", account_id=106277, project_id=160645)\n    # [END howto_operator_dbt_cloud_list_jobs]\n\n    begin >> Label(\"No async wait\") >> trigger_job_run1\n    begin >> Label(\"Do async wait with sensor\") >> trigger_job_run2\n    [get_run_results_artifact, job_run_sensor, list_dbt_jobs] >> end\n\n    # Task dependency created via `XComArgs`:\n    # trigger_job_run1 >> get_run_results_artifact\n    # trigger_job_run2 >> job_run_sensor\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.625300", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 38, "file_name": "example_dingding.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "                },\n            ]\n        },\n    )\n\n    msg_failure_callback = DingdingOperator(\n        task_id=\"msg_failure_callback\",\n        message_type=\"not_support_msg_type\",\n        message=\"\",\n    )\n\n    (\n        [\n            text_msg_remind_none,\n            text_msg_remind_specific,\n            text_msg_remind_include_invalid,\n            text_msg_remind_all,\n        ]\n        >> link_msg\n        >> markdown_msg\n        >> [\n            single_action_card_msg,\n            multi_action_card_msg,\n        ]\n        >> feed_card_msg\n        >> msg_failure_callback\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.634075", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 94, "file_name": "example_docker_copy_data.py"}, "content": "\"\"\"\nThis sample \"listen to directory\". move the new file and print it,\nusing docker-containers.\nThe following operators are being used: DockerOperator,\nBashOperator & ShortCircuitOperator.\nTODO: Review the workflow, change it accordingly to\nyour environment & enable the code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom docker.types import Mount\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import ShortCircuitOperator\nfrom airflow.providers.docker.operators.docker import DockerOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_sample_copy_data\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"docker\"],\n) as dag:\n\n    locate_file_cmd = \"\"\"\n        sleep 10\n        find {{params.source_location}} -type f  -printf \"%f\\n\" | head -1\n    \"\"\"\n\n    t_view = BashOperator(\n        task_id=\"view_file\",\n        bash_command=locate_file_cmd,\n        do_xcom_push=True,\n        params={\"source_location\": \"/your/input_dir/path\"},\n        dag=dag,\n    )\n\n    t_is_data_available = ShortCircuitOperator(\n        task_id=\"check_if_data_available\",\n        python_callable=lambda task_output: not task_output == \"\",\n        op_kwargs=dict(task_output=t_view.output),\n        dag=dag,\n    )\n\n    t_move = DockerOperator(\n        api_version=\"1.19\",\n        docker_url=\"tcp://localhost:2375\",  # replace it with swarm/docker endpoint\n        image=\"centos:latest\",\n        network_mode=\"bridge\",\n        mounts=[\n            Mount(source=\"/your/host/input_dir/path\", target=\"/your/input_dir/path\", type=\"bind\"),\n            Mount(source=\"/your/host/output_dir/path\", target=\"/your/output_dir/path\", type=\"bind\"),\n        ],\n        command=[\n            \"/bin/bash\",\n            \"-c\",\n            \"/bin/sleep 30; \"\n            \"/bin/mv {{ params.source_location }}/\" + str(t_view.output) + \" {{ params.target_location }};\"\n            \"/bin/echo '{{ params.target_location }}/\" + f\"{t_view.output}';\",\n        ],\n        task_id=\"move_data\",\n        do_xcom_push=True,\n        params={\"source_location\": \"/your/input_dir/path\", \"target_location\": \"/your/output_dir/path\"},\n        dag=dag,\n    )\n\n    t_print = DockerOperator(\n        api_version=\"1.19\",\n        docker_url=\"tcp://localhost:2375\",\n        image=\"centos:latest\",\n        mounts=[Mount(source=\"/your/host/output_dir/path\", target=\"/your/output_dir/path\", type=\"bind\")],\n        command=f\"cat {t_move.output}\",\n        task_id=\"print\",\n        dag=dag,\n    )\n\n    (\n        # TEST BODY\n        t_is_data_available\n        >> t_move\n        >> t_print\n    )\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.634705", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 48, "file_name": "example_docker.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.docker.operators.docker import DockerOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_test\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"docker\"],\n) as dag:\n    t1 = BashOperator(task_id=\"print_date\", bash_command=\"date\", dag=dag)\n    t2 = BashOperator(task_id=\"sleep\", bash_command=\"sleep 5\", retries=3, dag=dag)\n    # [START howto_operator_docker]\n    t3 = DockerOperator(\n        docker_url=\"unix://var/run/docker.sock\",  # Set your docker URL\n        command=\"/bin/sleep 30\",\n        image=\"centos:latest\",\n        network_mode=\"bridge\",\n        task_id=\"docker_op_tester\",\n        dag=dag,\n    )\n    # [END howto_operator_docker]\n\n    t4 = BashOperator(task_id=\"print_hello\", bash_command='echo \"hello world!!!\"', dag=dag)\n    # t1 >> t2\n    # t1 >> t3\n    # t3 >> t4\n\n    (\n        # TEST BODY\n        t1\n        >> [t2, t3]\n        >> t4\n    )\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.637835", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 37, "file_name": "example_docker_swarm.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.docker.operators.docker_swarm import DockerSwarmOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_swarm_dag\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"docker\"],\n) as dag:\n\n    t1 = DockerSwarmOperator(\n        api_version=\"auto\",\n        docker_url=\"unix://var/run/docker.sock\",  # Set your docker URL\n        command=\"/bin/sleep 10\",\n        image=\"centos:latest\",\n        auto_remove=True,\n        task_id=\"sleep_with_swarm\",\n    )\n\n    (\n        # TEST BODY\n        t1\n    )\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.639716", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 84, "file_name": "example_taskflow_api_docker_virtualenv.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    \"\"\"\n\n    # [START extract_virtualenv]\n    @task.virtualenv(\n        use_dill=True,\n        system_site_packages=False,\n        requirements=[\"funcsigs\"],\n    )\n    def extract():\n        \"\"\"\n        #### Extract task\n        A simple Extract task to get data ready for the rest of the data\n        pipeline. In this case, getting data is simulated by reading from a\n        hardcoded JSON string.\n        \"\"\"\n        import json\n\n        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n\n        order_data_dict = json.loads(data_string)\n        return order_data_dict\n\n    # [END extract_virtualenv]\n\n    # [START transform_docker]\n    @task.docker(image=\"python:3.9-slim-bullseye\", multiple_outputs=True)\n    def transform(order_data_dict: dict):\n        \"\"\"\n        #### Transform task\n        A simple Transform task which takes in the collection of order data and\n        computes the total order value.\n        \"\"\"\n        total_order_value = 0\n\n        for value in order_data_dict.values():\n            total_order_value += value\n\n        return {\"total_order_value\": total_order_value}\n\n    # [END transform_docker]\n\n    # [START load]\n    @task()\n    def load(total_order_value: float):\n        \"\"\"\n        #### Load task\n        A simple Load task which takes in the result of the Transform task and\n        instead of saving it to end user review, just prints it out.\n        \"\"\"\n\n        print(f\"Total order value is: {total_order_value:.2f}\")\n\n    # [END load]\n\n    # [START main_flow]\n    order_data = extract()\n    order_summary = transform(order_data)\n    load(order_summary[\"total_order_value\"])\n    # [END main_flow]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"docker\"],\n) as dag:\n    # The try/except here is because Airflow versions less than 2.2.0 doesn't support\n    # @task.docker decorator and we use this dag in CI test. Thus, in order not to\n    # break the CI test, we added this try/except here.\n    try:\n        # [START dag_invocation]\n        tutorial_dag = tutorial_taskflow_api_docker_virtualenv()\n        # [END dag_invocation]\n    except AttributeError:\n        pass\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)\n\n# [END tutorial]", "extracted_at": "2025-11-19T17:21:15.641615", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 71, "file_name": "example_elasticsearch_query.py"}, "content": "\"\"\"\nExample Airflow DAG for Elasticsearch Query.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.elasticsearch.hooks.elasticsearch import ElasticsearchPythonHook, ElasticsearchSQLHook\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"elasticsearch_dag\"\nCONN_ID = \"elasticsearch_default\"\n\n\n@task(task_id=\"es_print_tables\")\ndef show_tables():\n    \"\"\"\n    show_tables queries elasticsearch to list available tables\n    \"\"\"\n    # [START howto_elasticsearch_query]\n    es = ElasticsearchSQLHook(elasticsearch_conn_id=CONN_ID)\n\n    # Handle ES conn with context manager\n    with es.get_conn() as es_conn:\n        tables = es_conn.execute(\"SHOW TABLES\")\n        for table, *_ in tables:\n            print(f\"table: {table}\")\n    return True\n    # [END howto_elasticsearch_query]\n\n\n# [START howto_elasticsearch_python_hook]\ndef use_elasticsearch_hook():\n    \"\"\"\n    Use ElasticSearchPythonHook to print results from a local Elasticsearch\n    \"\"\"\n    es_hosts = [\"http://localhost:9200\"]\n    es_hook = ElasticsearchPythonHook(hosts=es_hosts)\n    query = {\"query\": {\"match_all\": {}}}\n    result = es_hook.search(query=query)\n    print(result)\n    return True\n\n\n# [END howto_elasticsearch_python_hook]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"elasticsearch\"],\n) as dag:\n    execute_query = show_tables()\n    (\n        # TEST BODY\n        execute_query\n    )\n    es_python_test = PythonOperator(\n        task_id=\"print_data_from_elasticsearch\", python_callable=use_elasticsearch_hook\n    )\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.644834", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 82, "file_name": "example_ftp.py"}, "content": "\"\"\"\nThis is an example dag for using the FTPFileTransmitOperator and FTPSFileTransmitOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.ftp.operators.ftp import (\n    FTPFileTransmitOperator,\n    FTPOperation,\n    FTPSFileTransmitOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_ftp_ftps_put_get\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"Ftp\", \"FtpFileTransmit\", \"Ftps\", \"FtpsFileTransmit\"],\n) as dag:\n    # [START howto_operator_ftp_put]\n    ftp_put = FTPFileTransmitOperator(\n        task_id=\"test_ftp_put\",\n        ftp_conn_id=\"ftp_default\",\n        local_filepath=\"/tmp/filepath\",\n        remote_filepath=\"/remote_tmp/filepath\",\n        operation=FTPOperation.PUT,\n        create_intermediate_dirs=True,\n    )\n    # [END howto_operator_ftp_put]\n\n    # [START howto_operator_ftp_get]\n    ftp_get = FTPFileTransmitOperator(\n        task_id=\"test_ftp_get\",\n        ftp_conn_id=\"ftp_default\",\n        local_filepath=\"/tmp/filepath\",\n        remote_filepath=\"/remote_tmp/filepath\",\n        operation=FTPOperation.GET,\n        create_intermediate_dirs=True,\n    )\n    # [END howto_operator_ftp_get]\n\n    # [START howto_operator_ftps_put]\n    ftps_put = FTPSFileTransmitOperator(\n        task_id=\"test_ftps_put\",\n        ftp_conn_id=\"ftps_default\",\n        local_filepath=\"/tmp/filepath\",\n        remote_filepath=\"/remote_tmp/filepath\",\n        operation=FTPOperation.PUT,\n        create_intermediate_dirs=True,\n    )\n    # [END howto_operator_ftps_put]\n\n    # [START howto_operator_ftps_get]\n    ftps_get = FTPSFileTransmitOperator(\n        task_id=\"test_ftps_get\",\n        ftp_conn_id=\"ftps_default\",\n        local_filepath=\"/tmp/filepath\",\n        remote_filepath=\"/remote_tmp/filepath\",\n        operation=FTPOperation.GET,\n        create_intermediate_dirs=True,\n    )\n    # [END howto_operator_ftps_get]\n\n    ftp_put >> ftp_get\n    ftps_put >> ftps_get\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.647868", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 10, "file_name": "example_github.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        result_processor=lambda repo: logging.info(list(repo.get_tags())),\n    )\n\n    # [END howto_operator_list_tags_github]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.682495", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 110, "file_name": "example_ads.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use GoogleAdsToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.ads.operators.ads import GoogleAdsListAccountsOperator\nfrom airflow.providers.google.ads.transfers.ads_to_gcs import GoogleAdsToGcsOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_google_ads_env_variables]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_google_ads\"\n\nBUCKET_NAME = f\"bucket_ads_{ENV_ID}\"\nCLIENT_IDS = [\"1111111111\", \"2222222222\"]\nGCS_OBJ_PATH = \"folder_name/google-ads-api-results.csv\"\nGCS_ACCOUNTS_CSV = \"folder_name/accounts.csv\"\nQUERY = \"\"\"\n    SELECT\n        segments.date,\n        customer.id,\n        campaign.id,\n        ad_group.id,\n        ad_group_ad.ad.id,\n        metrics.impressions,\n        metrics.clicks,\n        metrics.conversions,\n        metrics.all_conversions,\n        metrics.cost_micros\n    FROM\n        ad_group_ad\n    WHERE\n        segments.date >= '2020-02-01'\n        AND segments.date <= '2020-02-29'\n    \"\"\"\n\nFIELDS_TO_EXTRACT = [\n    \"segments.date.value\",\n    \"customer.id.value\",\n    \"campaign.id.value\",\n    \"ad_group.id.value\",\n    \"ad_group_ad.ad.id.value\",\n    \"metrics.impressions.value\",\n    \"metrics.clicks.value\",\n    \"metrics.conversions.value\",\n    \"metrics.all_conversions.value\",\n    \"metrics.cost_micros.value\",\n]\n# [END howto_google_ads_env_variables]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"ads\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_google_ads_to_gcs_operator]\n    run_operator = GoogleAdsToGcsOperator(\n        client_ids=CLIENT_IDS,\n        query=QUERY,\n        attributes=FIELDS_TO_EXTRACT,\n        obj=GCS_OBJ_PATH,\n        bucket=BUCKET_NAME,\n        task_id=\"run_operator\",\n    )\n    # [END howto_google_ads_to_gcs_operator]\n\n    # [START howto_ads_list_accounts_operator]\n    list_accounts = GoogleAdsListAccountsOperator(\n        task_id=\"list_accounts\", bucket=BUCKET_NAME, object_name=GCS_ACCOUNTS_CSV\n    )\n    # [END howto_ads_list_accounts_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> run_operator\n        >> list_accounts\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.695736", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 183, "file_name": "example_automl_dataset.py"}, "content": "\"\"\"\nExample Airflow DAG for Google AutoML service testing dataset operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom copy import deepcopy\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLImportDataOperator,\n    AutoMLListDatasetOperator,\n    AutoMLTablesListColumnSpecsOperator,\n    AutoMLTablesListTableSpecsOperator,\n    AutoMLTablesUpdateDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"automl_dataset\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_AUTOML_LOCATION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"system-tests-resources\"\n\nDATASET_NAME = \"test_dataset_tabular\"\nDATASET = {\n    \"display_name\": DATASET_NAME,\n    \"tables_dataset_metadata\": {\"target_column_spec_id\": \"\"},\n}\nAUTOML_DATASET_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/automl/bank-marketing.csv\"\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [AUTOML_DATASET_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\ndef get_target_column_spec(columns_specs: list[dict], column_name: str) -> str:\n    \"\"\"\n    Using column name returns spec of the column.\n    \"\"\"\n    for column in columns_specs:\n        if column[\"display_name\"] == column_name:\n            return extract_object_id(column)\n    raise Exception(f\"Unknown target column: {column_name}\")\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"automl\"],\n    user_defined_macros={\n        \"get_target_column_spec\": get_target_column_spec,\n        \"target\": \"Class\",\n        \"extract_object_id\": extract_object_id,\n    },\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    move_dataset_file = GCSSynchronizeBucketsOperator(\n        task_id=\"move_dataset_to_bucket\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=\"automl\",\n        destination_bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        destination_object=\"automl\",\n        recursive=True,\n    )\n\n    # [START howto_operator_automl_create_dataset]\n    create_dataset = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset=DATASET,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    dataset_id = create_dataset.output[\"dataset_id\"]\n    # [END howto_operator_automl_create_dataset]\n\n    # [START howto_operator_automl_import_data]\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n    # [END howto_operator_automl_import_data]\n\n    # [START howto_operator_automl_specs]\n    list_tables_spec_task = AutoMLTablesListTableSpecsOperator(\n        task_id=\"list_tables_spec_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_automl_specs]\n\n    # [START howto_operator_automl_column_specs]\n    list_columns_spec_task = AutoMLTablesListColumnSpecsOperator(\n        task_id=\"list_columns_spec_task\",\n        dataset_id=dataset_id,\n        table_spec_id=\"{{ extract_object_id(task_instance.xcom_pull('list_tables_spec_task')[0]) }}\",\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_automl_column_specs]\n\n    # [START howto_operator_automl_update_dataset]\n    update = deepcopy(DATASET)\n    update[\"name\"] = '{{ task_instance.xcom_pull(\"create_dataset\")[\"name\"] }}'\n    update[\"tables_dataset_metadata\"][  # type: ignore\n        \"target_column_spec_id\"\n    ] = \"{{ get_target_column_spec(task_instance.xcom_pull('list_columns_spec_task'), target) }}\"\n\n    update_dataset_task = AutoMLTablesUpdateDatasetOperator(\n        task_id=\"update_dataset_task\",\n        dataset=update,\n        location=GCP_AUTOML_LOCATION,\n    )\n    # [END howto_operator_automl_update_dataset]\n\n    # [START howto_operator_list_dataset]\n    list_datasets_task = AutoMLListDatasetOperator(\n        task_id=\"list_datasets_task\",\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_list_dataset]\n\n    # [START howto_operator_delete_dataset]\n    delete_dataset = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_delete_dataset]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket >> move_dataset_file, create_dataset]\n        # TEST BODY\n        >> import_dataset_task\n        >> list_tables_spec_task\n        >> list_columns_spec_task\n        >> update_dataset_task\n        >> list_datasets_task\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.696897", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 144, "file_name": "example_automl_nl_text_extraction.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_automl_text\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_AUTOML_LOCATION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"system-tests-resources\"\n\nDATASET_NAME = \"test_entity_extr\"\nDATASET = {\"display_name\": DATASET_NAME, \"text_extraction_dataset_metadata\": {}}\nAUTOML_DATASET_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/automl-text/dataset.csv\"\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [AUTOML_DATASET_BUCKET]}}\n\nMODEL_NAME = \"entity_extr_test_model\"\nMODEL = {\n    \"display_name\": MODEL_NAME,\n    \"text_extraction_model_metadata\": {},\n}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n# Example DAG for AutoML Natural Language Entities Extraction\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\", \"automl\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    move_dataset_file = GCSSynchronizeBucketsOperator(\n        task_id=\"move_data_to_bucket\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=\"automl-text\",\n        destination_bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        destination_object=\"automl-text\",\n        recursive=True,\n    )\n\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\",\n        dataset=DATASET,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n        project_id=GCP_PROJECT_ID,\n    )\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(\n        task_id=\"create_model\",\n        model=MODEL,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> move_dataset_file\n        >> create_dataset_task\n        >> import_dataset_task\n        # TEST BODY\n        >> create_model\n        # TEST TEARDOWN\n        >> delete_model_task\n        >> delete_datasets_task\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.700557", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 100, "file_name": "example_automl_nl_text_classification.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_TEXT_CLS_BUCKET = os.environ.get(\"GCP_AUTOML_TEXT_CLS_BUCKET\", \"gs://INVALID BUCKET NAME\")\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model_1\",\n    \"text_classification_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_text_cls_dataset\",\n    \"text_classification_dataset_metadata\": {\"classification_type\": \"MULTICLASS\"},\n}\n\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_TEXT_CLS_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n# Example DAG for AutoML Natural Language Text Classification\nwith models.DAG(\n    \"example_automl_text_cls\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.702573", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 269, "file_name": "example_automl_model.py"}, "content": "\"\"\"\nExample Airflow DAG for Google AutoML service testing model operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom copy import deepcopy\nfrom datetime import datetime\n\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLBatchPredictOperator,\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLDeployModelOperator,\n    AutoMLGetModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLPredictOperator,\n    AutoMLTablesListColumnSpecsOperator,\n    AutoMLTablesListTableSpecsOperator,\n    AutoMLTablesUpdateDatasetOperator,\n    AutoMLTrainModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_automl_model\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_AUTOML_LOCATION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"system-tests-resources\"\n\nDATASET_NAME = \"test_dataset_model\"\nDATASET = {\n    \"display_name\": DATASET_NAME,\n    \"tables_dataset_metadata\": {\"target_column_spec_id\": \"\"},\n}\nAUTOML_DATASET_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/automl-model/bank-marketing.csv\"\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [AUTOML_DATASET_BUCKET]}}\nIMPORT_OUTPUT_CONFIG = {\n    \"gcs_destination\": {\"output_uri_prefix\": f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/automl-model\"}\n}\n\nMODEL_NAME = \"test_model\"\nMODEL = {\n    \"display_name\": MODEL_NAME,\n    \"tables_model_metadata\": {\"train_budget_milli_node_hours\": 1000},\n}\n\nPREDICT_VALUES = [\n    Value(string_value=\"51\"),\n    Value(string_value=\"blue-collar\"),\n    Value(string_value=\"married\"),\n    Value(string_value=\"primary\"),\n    Value(string_value=\"no\"),\n    Value(string_value=\"620\"),\n    Value(string_value=\"yes\"),\n    Value(string_value=\"yes\"),\n    Value(string_value=\"cellular\"),\n    Value(string_value=\"29\"),\n    Value(string_value=\"jul\"),\n    Value(string_value=\"88\"),\n    Value(string_value=\"10\"),\n    Value(string_value=\"-1\"),\n    Value(string_value=\"0\"),\n    Value(string_value=\"unknown\"),\n]\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\ndef get_target_column_spec(columns_specs: list[dict], column_name: str) -> str:\n    \"\"\"\n    Using column name returns spec of the column.\n    \"\"\"\n    for column in columns_specs:\n        if column[\"display_name\"] == column_name:\n            return extract_object_id(column)\n    raise Exception(f\"Unknown target column: {column_name}\")\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\n        \"get_target_column_spec\": get_target_column_spec,\n        \"target\": \"Class\",\n        \"extract_object_id\": extract_object_id,\n    },\n    tags=[\"example\", \"automl\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    move_dataset_file = GCSSynchronizeBucketsOperator(\n        task_id=\"move_data_to_bucket\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=\"automl-model\",\n        destination_bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        destination_object=\"automl-model\",\n        recursive=True,\n    )\n\n    create_dataset = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset=DATASET,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    dataset_id = create_dataset.output[\"dataset_id\"]\n    MODEL[\"dataset_id\"] = dataset_id\n    import_dataset = AutoMLImportDataOperator(\n        task_id=\"import_dataset\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    list_tables_spec = AutoMLTablesListTableSpecsOperator(\n        task_id=\"list_tables_spec\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    list_columns_spec = AutoMLTablesListColumnSpecsOperator(\n        task_id=\"list_columns_spec\",\n        dataset_id=dataset_id,\n        table_spec_id=\"{{ extract_object_id(task_instance.xcom_pull('list_tables_spec')[0]) }}\",\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    update = deepcopy(DATASET)\n    update[\"name\"] = '{{ task_instance.xcom_pull(\"create_dataset\")[\"name\"] }}'\n    update[\"tables_dataset_metadata\"][  # type: ignore\n        \"target_column_spec_id\"\n    ] = \"{{ get_target_column_spec(task_instance.xcom_pull('list_columns_spec'), target) }}\"\n\n    update_dataset = AutoMLTablesUpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        dataset=update,\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    # [START howto_operator_automl_create_model]\n    create_model = AutoMLTrainModelOperator(\n        task_id=\"create_model\",\n        model=MODEL,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    model_id = create_model.output[\"model_id\"]\n    # [END howto_operator_automl_create_model]\n\n    # [START howto_operator_get_model]\n    get_model = AutoMLGetModelOperator(\n        task_id=\"get_model\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_get_model]\n\n    # [START howto_operator_deploy_model]\n    deploy_model = AutoMLDeployModelOperator(\n        task_id=\"deploy_model\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_deploy_model]\n\n    # [START howto_operator_prediction]\n    predict_task = AutoMLPredictOperator(\n        task_id=\"predict_task\",\n        model_id=model_id,\n        payload={\n            \"row\": {\n                \"values\": PREDICT_VALUES,\n            }\n        },\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_prediction]\n\n    # [START howto_operator_batch_prediction]\n    batch_predict_task = AutoMLBatchPredictOperator(\n        task_id=\"batch_predict_task\",\n        model_id=model_id,\n        input_config=IMPORT_INPUT_CONFIG,\n        output_config=IMPORT_OUTPUT_CONFIG,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_batch_prediction]\n\n    # [START howto_operator_automl_delete_model]\n    delete_model = AutoMLDeleteModelOperator(\n        task_id=\"delete_model\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_automl_delete_model]\n\n    delete_dataset = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> move_dataset_file\n        >> create_dataset\n        >> import_dataset\n        >> list_tables_spec\n        >> list_columns_spec\n        >> update_dataset\n        # TEST BODY\n        >> create_model\n        >> get_model\n        >> deploy_model\n        >> predict_task\n        >> batch_predict_task\n        # TEST TEARDOWN\n        >> delete_model\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.708133", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 103, "file_name": "example_automl_nl_text_sentiment.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_SENTIMENT_BUCKET = os.environ.get(\"GCP_AUTOML_SENTIMENT_BUCKET\", \"gs://INVALID BUCKET NAME\")\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model_1\",\n    \"text_sentiment_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_text_sentiment_dataset\",\n    \"text_sentiment_dataset_metadata\": {\"sentiment_max\": 10},\n}\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_SENTIMENT_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n# Example DAG for AutoML Natural Language Text Sentiment\nwith models.DAG(\n    \"example_automl_text_sentiment\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_model >> delete_model_task\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.708742", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 111, "file_name": "example_automl_translation.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_TRANSLATION_BUCKET = os.environ.get(\n    \"GCP_AUTOML_TRANSLATION_BUCKET\", \"gs://INVALID BUCKET NAME/file\"\n)\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model_1\",\n    \"translation_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_translation_dataset\",\n    \"translation_dataset_metadata\": {\n        \"source_language_code\": \"en\",\n        \"target_language_code\": \"es\",\n    },\n}\n\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_TRANSLATION_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\n# Example DAG for AutoML Translation\nwith models.DAG(\n    \"example_automl_translation\",\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_model >> delete_model_task\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.711383", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 106, "file_name": "example_automl_video_intelligence_classification.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_VIDEO_BUCKET = os.environ.get(\n    \"GCP_AUTOML_VIDEO_BUCKET\", \"gs://INVALID BUCKET NAME/hmdb_split1.csv\"\n)\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model_1\",\n    \"video_classification_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_video_dataset\",\n    \"video_classification_dataset_metadata\": {},\n}\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_VIDEO_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\n# Example DAG for AutoML Video Intelligence Classification\nwith models.DAG(\n    \"example_automl_video\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_model >> delete_model_task\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.749671", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 106, "file_name": "example_automl_vision_object_detection.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_DETECTION_BUCKET = os.environ.get(\n    \"GCP_AUTOML_DETECTION_BUCKET\", \"gs://INVALID BUCKET NAME/img/openimage/csv/salads_ml_use.csv\"\n)\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model\",\n    \"image_object_detection_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_detection_dataset\",\n    \"image_object_detection_dataset_metadata\": {},\n}\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_DETECTION_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\n# Example DAG for AutoML Vision Object Detection\nwith models.DAG(\n    \"example_automl_vision_detection\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_model >> delete_model_task\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.757895", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 141, "file_name": "example_automl_vision_classification.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_automl_vision\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_AUTOML_LOCATION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"system-tests-resources\"\n\nDATASET_NAME = \"test_dataset_vision\"\nDATASET = {\n    \"display_name\": DATASET_NAME,\n    \"image_classification_dataset_metadata\": {\"classification_type\": \"MULTILABEL\"},\n}\nAUTOML_DATASET_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/automl-vision/data.csv\"\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [AUTOML_DATASET_BUCKET]}}\n\nMODEL_NAME = \"test_model\"\nMODEL = {\n    \"display_name\": MODEL_NAME,\n    \"image_classification_model_metadata\": {\"train_budget\": 1},\n}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n# Example DAG for AutoML Vision Classification\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\", \"automl\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    move_dataset_file = GCSSynchronizeBucketsOperator(\n        task_id=\"move_data_to_bucket\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=\"automl-vision\",\n        destination_bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        destination_object=\"automl-vision\",\n        recursive=True,\n    )\n\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\",\n        dataset=DATASET,\n        location=GCP_AUTOML_LOCATION,\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> move_dataset_file\n        >> create_dataset_task\n        >> import_dataset_task\n        # TEST BODY\n        >> create_model\n        # TEST TEARDOWN\n        >> delete_model_task\n        >> delete_datasets_task\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.760146", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 58, "file_name": "example_azure_blob_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.azure_blob_to_gcs import AzureBlobStorageToGCSOperator\nfrom airflow.providers.microsoft.azure.sensors.wasb import (\n    WasbBlobSensor,\n)\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\n\nBLOB_NAME = os.environ.get(\"AZURE_BLOB_NAME\", \"file.txt\")\nAZURE_CONTAINER_NAME = os.environ.get(\"AZURE_CONTAINER_NAME\", \"airflow\")\nGCP_BUCKET_FILE_PATH = os.environ.get(\"GCP_BUCKET_FILE_PATH\", \"file.txt\")\nGCP_BUCKET_NAME = os.environ.get(\"GCP_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nGCP_OBJECT_NAME = os.environ.get(\"GCP_OBJECT_NAME\", \"file.txt\")\nDAG_ID = \"example_azure_blob_to_gcs\"\n\nwith DAG(\n    DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n) as dag:\n    wait_for_blob = WasbBlobSensor(\n        task_id=\"wait_for_blob\", container_name=AZURE_CONTAINER_NAME, blob_name=BLOB_NAME\n    )\n\n    # [START how_to_azure_blob_to_gcs]\n    transfer_files_to_gcs = AzureBlobStorageToGCSOperator(\n        task_id=\"transfer_files_to_gcs\",\n        # azure args\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        # GCP args\n        bucket_name=GCP_BUCKET_NAME,\n        object_name=GCP_OBJECT_NAME,\n        filename=GCP_BUCKET_FILE_PATH,\n        gzip=False,\n        impersonation_chain=None,\n    )\n    # [END how_to_azure_blob_to_gcs]\n\n    (wait_for_blob >> transfer_files_to_gcs)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.767769", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 80, "file_name": "example_bigquery_dataset.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service testing dataset operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryGetDatasetOperator,\n    BigQueryUpdateDatasetOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_dataset\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    # [START howto_operator_bigquery_create_dataset]\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n    # [END howto_operator_bigquery_create_dataset]\n\n    # [START howto_operator_bigquery_update_dataset]\n    update_dataset = BigQueryUpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        dataset_id=DATASET_NAME,\n        dataset_resource={\"description\": \"Updated dataset\"},\n    )\n    # [END howto_operator_bigquery_update_dataset]\n\n    # [START howto_operator_bigquery_get_dataset]\n    get_dataset = BigQueryGetDatasetOperator(task_id=\"get-dataset\", dataset_id=DATASET_NAME)\n    # [END howto_operator_bigquery_get_dataset]\n\n    get_dataset_result = BashOperator(\n        task_id=\"get_dataset_result\",\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('get-dataset')['id'] }}\\\"\",\n    )\n\n    # [START howto_operator_bigquery_delete_dataset]\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n    # [END howto_operator_bigquery_delete_dataset]\n    delete_dataset.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST BODY\n        create_dataset\n        >> update_dataset\n        >> get_dataset\n        >> get_dataset_result\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.769959", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 74, "file_name": "example_azure_fileshare_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.azure_fileshare_to_gcs import AzureFileShareToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"azure_fileshare_to_gcs_example\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nAZURE_SHARE_NAME = os.environ.get(\"AZURE_SHARE_NAME\", \"test-azure-share\")\nAZURE_DIRECTORY_NAME = \"test-azure-dir\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\n        \"owner\": \"airflow\",\n        \"depends_on_past\": False,\n        \"email\": [\"airflow@example.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"azure\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_azure_fileshare_to_gcs_basic]\n    sync_azure_files_with_gcs = AzureFileShareToGCSOperator(\n        task_id=\"sync_azure_files_with_gcs\",\n        share_name=AZURE_SHARE_NAME,\n        dest_gcs=BUCKET_NAME,\n        directory_name=AZURE_DIRECTORY_NAME,\n        replace=False,\n        gzip=True,\n        google_impersonation_chain=None,\n    )\n    # [END howto_operator_azure_fileshare_to_gcs_basic]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> sync_azure_files_with_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.776050", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 108, "file_name": "example_automl_video_intelligence_tracking.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google AutoML services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.automl import CloudAutoMLHook\nfrom airflow.providers.google.cloud.operators.automl import (\n    AutoMLCreateDatasetOperator,\n    AutoMLDeleteDatasetOperator,\n    AutoMLDeleteModelOperator,\n    AutoMLImportDataOperator,\n    AutoMLTrainModelOperator,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"your-project-id\")\nGCP_AUTOML_LOCATION = os.environ.get(\"GCP_AUTOML_LOCATION\", \"us-central1\")\nGCP_AUTOML_TRACKING_BUCKET = os.environ.get(\n    \"GCP_AUTOML_TRACKING_BUCKET\",\n    \"gs://INVALID BUCKET NAME/youtube_8m_videos_animal_tiny.csv\",\n)\n\n\n# Example model\nMODEL = {\n    \"display_name\": \"auto_model_1\",\n    \"video_object_tracking_model_metadata\": {},\n}\n\n# Example dataset\nDATASET = {\n    \"display_name\": \"test_video_tracking_dataset\",\n    \"video_object_tracking_dataset_metadata\": {},\n}\n\nIMPORT_INPUT_CONFIG = {\"gcs_source\": {\"input_uris\": [GCP_AUTOML_TRACKING_BUCKET]}}\n\nextract_object_id = CloudAutoMLHook.extract_object_id\n\n\n# Example DAG for AutoML Video Intelligence Object Tracking\nwith models.DAG(\n    \"example_automl_video_tracking\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    user_defined_macros={\"extract_object_id\": extract_object_id},\n    tags=[\"example\"],\n) as dag:\n    create_dataset_task = AutoMLCreateDatasetOperator(\n        task_id=\"create_dataset_task\", dataset=DATASET, location=GCP_AUTOML_LOCATION\n    )\n\n    dataset_id = cast(str, XComArg(create_dataset_task, key=\"dataset_id\"))\n    MODEL[\"dataset_id\"] = dataset_id\n\n    import_dataset_task = AutoMLImportDataOperator(\n        task_id=\"import_dataset_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        input_config=IMPORT_INPUT_CONFIG,\n    )\n\n    MODEL[\"dataset_id\"] = dataset_id\n\n    create_model = AutoMLTrainModelOperator(task_id=\"create_model\", model=MODEL, location=GCP_AUTOML_LOCATION)\n\n    model_id = cast(str, XComArg(create_model, key=\"model_id\"))\n\n    delete_model_task = AutoMLDeleteModelOperator(\n        task_id=\"delete_model_task\",\n        model_id=model_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    delete_datasets_task = AutoMLDeleteDatasetOperator(\n        task_id=\"delete_datasets_task\",\n        dataset_id=dataset_id,\n        location=GCP_AUTOML_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # TEST BODY\n    import_dataset_task >> create_model\n    # TEST TEARDOWN\n    delete_model_task >> delete_datasets_task\n\n    # Task dependencies created via `XComArgs`:\n    #   create_dataset_task >> import_dataset_task\n    #   create_dataset_task >> create_model\n    #   create_model >> delete_model_task\n    #   create_dataset_task >> delete_datasets_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.778240", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 174, "file_name": "example_bigquery_dts.py"}, "content": "\"\"\"\nExample Airflow DAG that creates and deletes Bigquery data transfer configurations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.bigquery_dts import (\n    BigQueryCreateDataTransferOperator,\n    BigQueryDataTransferServiceStartTransferRunsOperator,\n    BigQueryDeleteDataTransferConfigOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.bigquery_dts import BigQueryDataTransferServiceTransferRunSensor\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_bigquery_dts\"\n\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\n\nFILE_NAME = \"us-states.csv\"\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / FILE_NAME)\nBUCKET_URI = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDTS_BQ_TABLE = \"DTS_BQ_TABLE\"\n\n# [START howto_bigquery_dts_create_args]\n\n# In the case of Airflow, the customer needs to create a transfer\n# config with the automatic scheduling disabled and then trigger\n# a transfer run using a specialized Airflow operator\nTRANSFER_CONFIG = {\n    \"destination_dataset_id\": DATASET_NAME,\n    \"display_name\": \"test data transfer\",\n    \"data_source_id\": \"google_cloud_storage\",\n    \"schedule_options\": {\"disable_auto_scheduling\": True},\n    \"params\": {\n        \"field_delimiter\": \",\",\n        \"max_bad_records\": \"0\",\n        \"skip_leading_rows\": \"1\",\n        \"data_path_template\": BUCKET_URI,\n        \"destination_table_name_template\": DTS_BQ_TABLE,\n        \"file_format\": \"CSV\",\n    },\n}\n\n# [END howto_bigquery_dts_create_args]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=DTS_BQ_TABLE,\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_bigquery_create_data_transfer]\n    gcp_bigquery_create_transfer = BigQueryCreateDataTransferOperator(\n        transfer_config=TRANSFER_CONFIG,\n        project_id=PROJECT_ID,\n        task_id=\"gcp_bigquery_create_transfer\",\n    )\n\n    transfer_config_id = cast(str, XComArg(gcp_bigquery_create_transfer, key=\"transfer_config_id\"))\n    # [END howto_bigquery_create_data_transfer]\n\n    # [START howto_bigquery_start_transfer]\n    gcp_bigquery_start_transfer = BigQueryDataTransferServiceStartTransferRunsOperator(\n        task_id=\"gcp_bigquery_start_transfer\",\n        project_id=PROJECT_ID,\n        transfer_config_id=transfer_config_id,\n        requested_run_time={\"seconds\": int(time.time() + 60)},\n    )\n    # [END howto_bigquery_start_transfer]\n\n    # [START howto_bigquery_dts_sensor]\n    gcp_run_sensor = BigQueryDataTransferServiceTransferRunSensor(\n        task_id=\"gcp_run_sensor\",\n        transfer_config_id=transfer_config_id,\n        run_id=cast(str, XComArg(gcp_bigquery_start_transfer, key=\"run_id\")),\n        expected_statuses={\"SUCCEEDED\"},\n    )\n    # [END howto_bigquery_dts_sensor]\n\n    # [START howto_bigquery_delete_data_transfer]\n    gcp_bigquery_delete_transfer = BigQueryDeleteDataTransferConfigOperator(\n        transfer_config_id=transfer_config_id, task_id=\"gcp_bigquery_delete_transfer\"\n    )\n    # [END howto_bigquery_delete_data_transfer]\n    gcp_bigquery_delete_transfer.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   gcp_bigquery_create_transfer >> gcp_bigquery_start_transfer\n    #   gcp_bigquery_create_transfer >> gcp_run_sensor\n    #   gcp_bigquery_start_transfer >> gcp_run_sensor\n    #   gcp_bigquery_create_transfer >> gcp_bigquery_delete_transfer\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        upload_file,\n        create_dataset,\n        create_table,\n        # TEST BODY\n        gcp_bigquery_create_transfer,\n        gcp_bigquery_start_transfer,\n        gcp_run_sensor,\n        gcp_bigquery_delete_transfer,\n        # TEST TEARDOWN\n        delete_dataset,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.781623", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 92, "file_name": "example_bigquery_operations.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service local file upload and external table creation.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_operations\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_OBJECT_NAME = \"bigquery/us-states/us-states.csv\"\nCSV_FILE_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / \"us-states.csv\")\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n    )\n\n    # [START howto_operator_bigquery_create_external_table]\n    create_external_table = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table\",\n        destination_project_dataset_table=f\"{DATASET_NAME}.external_table\",\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n        source_objects=[DATA_SAMPLE_GCS_OBJECT_NAME],\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n    # [END howto_operator_bigquery_create_external_table]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        # TEST BODY\n        >> upload_file\n        >> create_external_table\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.808120", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 256, "file_name": "example_bigquery_queries_async.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\nUses Async version of the Big Query Operators\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCheckOperator,\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryGetDataOperator,\n    BigQueryInsertJobOperator,\n    BigQueryIntervalCheckOperator,\n    BigQueryValueCheckOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"bigquery_queries_async\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us\"\n\nTABLE_1 = \"table1\"\nTABLE_2 = \"table2\"\n\nSCHEMA = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"ds\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n]\n\nDATASET = DATASET_NAME\nINSERT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\nINSERT_ROWS_QUERY = (\n    f\"INSERT {DATASET}.{TABLE_1} VALUES \"\n    f\"(42, 'monthy python', '{INSERT_DATE}'), \"\n    f\"(42, 'fishy fish', '{INSERT_DATE}');\"\n)\n\n\nCONFIGURATION = {\n    \"query\": {\n        \"query\": f\"\"\"DECLARE success BOOL;\n        DECLARE size_bytes INT64;\n        DECLARE row_count INT64;\n        DECLARE DELAY_TIME DATETIME;\n        DECLARE WAIT STRING;\n        SET success = FALSE;\n\n        SELECT row_count = (SELECT row_count FROM {DATASET}.__TABLES__ WHERE table_id='NON_EXISTING_TABLE');\n        IF row_count > 0  THEN\n            SELECT 'Table Exists!' as message, retry_count as retries;\n            SET success = TRUE;\n        ELSE\n            SELECT 'Table does not exist' as message, row_count;\n            SET WAIT = 'TRUE';\n            SET DELAY_TIME = DATETIME_ADD(CURRENT_DATETIME,INTERVAL 1 MINUTE);\n            WHILE WAIT = 'TRUE' DO\n                IF (DELAY_TIME < CURRENT_DATETIME) THEN\n                    SET WAIT = 'FALSE';\n                END IF;\n            END WHILE;\n        END IF;\"\"\",\n        \"useLegacySql\": False,\n    }\n}\n\n\ndefault_args = {\n    \"execution_timeout\": timedelta(hours=6),\n    \"retries\": 2,\n    \"retry_delay\": timedelta(seconds=60),\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    default_args=default_args,\n    tags=[\"example\", \"async\", \"bigquery\"],\n    user_defined_macros={\"DATASET\": DATASET, \"TABLE\": TABLE_1},\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET,\n        location=LOCATION,\n    )\n\n    create_table_1 = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table_1\",\n        dataset_id=DATASET,\n        table_id=TABLE_1,\n        schema_fields=SCHEMA,\n        location=LOCATION,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_bigquery_insert_job_async]\n    insert_query_job = BigQueryInsertJobOperator(\n        task_id=\"insert_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": INSERT_ROWS_QUERY,\n                \"useLegacySql\": False,\n                \"priority\": \"BATCH\",\n            }\n        },\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_insert_job_async]\n\n    # [START howto_operator_bigquery_select_job_async]\n    select_query_job = BigQueryInsertJobOperator(\n        task_id=\"select_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"{% include 'resources/example_bigquery_query.sql' %}\",\n                \"useLegacySql\": False,\n            }\n        },\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_select_job_async]\n\n    # [START howto_operator_bigquery_value_check_async]\n    check_value = BigQueryValueCheckOperator(\n        task_id=\"check_value\",\n        sql=f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_1}\",\n        pass_value=2,\n        use_legacy_sql=False,\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_value_check_async]\n\n    # [START howto_operator_bigquery_interval_check_async]\n    check_interval = BigQueryIntervalCheckOperator(\n        task_id=\"check_interval\",\n        table=f\"{DATASET}.{TABLE_1}\",\n        days_back=1,\n        metrics_thresholds={\"COUNT(*)\": 1.5},\n        use_legacy_sql=False,\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_interval_check_async]\n\n    # [START howto_operator_bigquery_multi_query_async]\n    bigquery_execute_multi_query = BigQueryInsertJobOperator(\n        task_id=\"execute_multi_query\",\n        configuration={\n            \"query\": {\n                \"query\": [\n                    f\"SELECT * FROM {DATASET}.{TABLE_2}\",\n                    f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_2}\",\n                ],\n                \"useLegacySql\": False,\n            }\n        },\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_multi_query_async]\n\n    # [START howto_operator_bigquery_get_data_async]\n    get_data = BigQueryGetDataOperator(\n        task_id=\"get_data\",\n        dataset_id=DATASET,\n        table_id=TABLE_1,\n        max_results=10,\n        selected_fields=\"value,name\",\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_get_data_async]\n\n    get_data_result = BashOperator(\n        task_id=\"get_data_result\",\n        bash_command=f\"echo {get_data.output}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_bigquery_check_async]\n    check_count = BigQueryCheckOperator(\n        task_id=\"check_count\",\n        sql=f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_1}\",\n        use_legacy_sql=False,\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_check_async]\n\n    # [START howto_operator_bigquery_execute_query_save_async]\n    execute_query_save = BigQueryInsertJobOperator(\n        task_id=\"execute_query_save\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT * FROM {DATASET}.{TABLE_1}\",\n                \"useLegacySql\": False,\n                \"destinationTable\": {\n                    \"projectId\": PROJECT_ID,\n                    \"datasetId\": DATASET,\n                    \"tableId\": TABLE_2,\n                },\n            }\n        },\n        location=LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_bigquery_execute_query_save_async]\n\n    execute_long_running_query = BigQueryInsertJobOperator(\n        task_id=\"execute_long_running_query\",\n        configuration=CONFIGURATION,\n        location=LOCATION,\n        deferrable=True,\n    )\n\n    create_dataset >> create_table_1 >> insert_query_job\n    insert_query_job >> select_query_job >> check_count\n    insert_query_job >> get_data >> get_data_result\n    insert_query_job >> execute_query_save >> bigquery_execute_multi_query\n    insert_query_job >> execute_long_running_query >> check_value >> check_interval\n    [check_count, check_interval, bigquery_execute_multi_query, get_data_result] >> delete_dataset\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.822186", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 72, "file_name": "example_bigquery_operations_location.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service testing data structures with location.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_operations_location\"\n\nBQ_LOCATION = \"europe-north1\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_dataset_with_location = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset_with_location\",\n        dataset_id=DATASET_NAME,\n        location=BQ_LOCATION,\n    )\n\n    create_table_with_location = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table_with_location\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_table\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    delete_dataset_with_location = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset_with_location\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST BODY\n        create_dataset_with_location\n        >> create_table_with_location\n        # TEST TEARDOWN\n        >> delete_dataset_with_location\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.822594", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 243, "file_name": "example_bigquery_queries.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCheckOperator,\n    BigQueryColumnCheckOperator,\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryGetDataOperator,\n    BigQueryInsertJobOperator,\n    BigQueryIntervalCheckOperator,\n    BigQueryTableCheckOperator,\n    BigQueryValueCheckOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nLOCATION = \"us-east1\"\nQUERY_SQL_PATH = \"resources/example_bigquery_query.sql\"\n\nTABLE_1 = \"table1\"\nTABLE_2 = \"table2\"\n\nSCHEMA = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"ds\", \"type\": \"DATE\", \"mode\": \"NULLABLE\"},\n]\n\nDAGS_LIST = []\nlocations = [None, LOCATION]\nfor index, location in enumerate(locations, 1):\n    DAG_ID = \"bigquery_queries_location\" if location else \"bigquery_queries\"\n    DATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\n    DATASET = f\"{DATASET_NAME}{index}\"\n    INSERT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n    # [START howto_operator_bigquery_query]\n    INSERT_ROWS_QUERY = (\n        f\"INSERT {DATASET}.{TABLE_1} VALUES \"\n        f\"(42, 'monty python', '{INSERT_DATE}'), \"\n        f\"(42, 'fishy fish', '{INSERT_DATE}');\"\n    )\n    # [END howto_operator_bigquery_query]\n\n    with models.DAG(\n        DAG_ID,\n        schedule=\"@once\",\n        start_date=datetime(2021, 1, 1),\n        catchup=False,\n        tags=[\"example\", \"bigquery\"],\n        user_defined_macros={\"DATASET\": DATASET, \"TABLE\": TABLE_1, \"QUERY_SQL_PATH\": QUERY_SQL_PATH},\n    ) as dag:\n        create_dataset = BigQueryCreateEmptyDatasetOperator(\n            task_id=\"create_dataset\",\n            dataset_id=DATASET,\n            location=location,\n        )\n\n        create_table_1 = BigQueryCreateEmptyTableOperator(\n            task_id=\"create_table_1\",\n            dataset_id=DATASET,\n            table_id=TABLE_1,\n            schema_fields=SCHEMA,\n            location=location,\n        )\n\n        create_table_2 = BigQueryCreateEmptyTableOperator(\n            task_id=\"create_table_2\",\n            dataset_id=DATASET,\n            table_id=TABLE_2,\n            schema_fields=SCHEMA,\n            location=location,\n        )\n\n        # [START howto_operator_bigquery_insert_job]\n        insert_query_job = BigQueryInsertJobOperator(\n            task_id=\"insert_query_job\",\n            configuration={\n                \"query\": {\n                    \"query\": INSERT_ROWS_QUERY,\n                    \"useLegacySql\": False,\n                    \"priority\": \"BATCH\",\n                }\n            },\n            location=location,\n        )\n        # [END howto_operator_bigquery_insert_job]\n\n        # [START howto_operator_bigquery_select_job]\n        select_query_job = BigQueryInsertJobOperator(\n            task_id=\"select_query_job\",\n            configuration={\n                \"query\": {\n                    \"query\": \"{% include QUERY_SQL_PATH %}\",\n                    \"useLegacySql\": False,\n                }\n            },\n            location=location,\n        )\n        # [END howto_operator_bigquery_select_job]\n\n        execute_insert_query = BigQueryInsertJobOperator(\n            task_id=\"execute_insert_query\",\n            configuration={\n                \"query\": {\n                    \"query\": INSERT_ROWS_QUERY,\n                    \"useLegacySql\": False,\n                }\n            },\n            location=location,\n        )\n\n        execute_query_save = BigQueryInsertJobOperator(\n            task_id=\"execute_query_save\",\n            configuration={\n                \"query\": {\n                    \"query\": f\"SELECT * FROM {DATASET}.{TABLE_1}\",\n                    \"useLegacySql\": False,\n                    \"destinationTable\": {\n                        \"projectId\": PROJECT_ID,\n                        \"datasetId\": DATASET,\n                        \"tableId\": TABLE_2,\n                    },\n                }\n            },\n            location=location,\n        )\n\n        bigquery_execute_multi_query = BigQueryInsertJobOperator(\n            task_id=\"execute_multi_query\",\n            configuration={\n                \"query\": {\n                    \"query\": [\n                        f\"SELECT * FROM {DATASET}.{TABLE_2}\",\n                        f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_2}\",\n                    ],\n                    \"useLegacySql\": False,\n                }\n            },\n            location=location,\n        )\n\n        # [START howto_operator_bigquery_get_data]\n        get_data = BigQueryGetDataOperator(\n            task_id=\"get_data\",\n            dataset_id=DATASET,\n            table_id=TABLE_1,\n            max_results=10,\n            selected_fields=\"value,name\",\n            location=location,\n        )\n        # [END howto_operator_bigquery_get_data]\n\n        get_data_result = BashOperator(\n            task_id=\"get_data_result\",\n            bash_command=f\"echo {get_data.output}\",\n        )\n\n        # [START howto_operator_bigquery_check]\n        check_count = BigQueryCheckOperator(\n            task_id=\"check_count\",\n            sql=f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_1}\",\n            use_legacy_sql=False,\n            location=location,\n        )\n        # [END howto_operator_bigquery_check]\n\n        # [START howto_operator_bigquery_value_check]\n        check_value = BigQueryValueCheckOperator(\n            task_id=\"check_value\",\n            sql=f\"SELECT COUNT(*) FROM {DATASET}.{TABLE_1}\",\n            pass_value=4,\n            use_legacy_sql=False,\n            location=location,\n        )\n        # [END howto_operator_bigquery_value_check]\n\n        # [START howto_operator_bigquery_interval_check]\n        check_interval = BigQueryIntervalCheckOperator(\n            task_id=\"check_interval\",\n            table=f\"{DATASET}.{TABLE_1}\",\n            days_back=1,\n            metrics_thresholds={\"COUNT(*)\": 1.5},\n            use_legacy_sql=False,\n            location=location,\n        )\n        # [END howto_operator_bigquery_interval_check]\n\n        # [START howto_operator_bigquery_column_check]\n        column_check = BigQueryColumnCheckOperator(\n            task_id=\"column_check\",\n            table=f\"{DATASET}.{TABLE_1}\",\n            column_mapping={\"value\": {\"null_check\": {\"equal_to\": 0}}},\n        )\n        # [END howto_operator_bigquery_column_check]\n\n        # [START howto_operator_bigquery_table_check]\n        table_check = BigQueryTableCheckOperator(\n            task_id=\"table_check\",\n            table=f\"{DATASET}.{TABLE_1}\",\n            checks={\"row_count_check\": {\"check_statement\": \"COUNT(*) = 4\"}},\n        )\n        # [END howto_operator_bigquery_table_check]\n\n        delete_dataset = BigQueryDeleteDatasetOperator(\n            task_id=\"delete_dataset\",\n            dataset_id=DATASET,\n            delete_contents=True,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n        # TEST SETUP\n        create_dataset >> [create_table_1, create_table_2]\n        # TEST BODY\n        [create_table_1, create_table_2] >> insert_query_job >> [select_query_job, execute_insert_query]\n        execute_insert_query >> get_data >> get_data_result >> delete_dataset\n        execute_insert_query >> execute_query_save >> bigquery_execute_multi_query >> delete_dataset\n        execute_insert_query >> [check_count, check_value, check_interval] >> delete_dataset\n        execute_insert_query >> [column_check, table_check] >> delete_dataset\n\n        from tests.system.utils.watcher import watcher\n\n        # This test needs watcher in order to properly mark success/failure\n        # when \"tearDown\" task with trigger rule is part of the DAG\n        list(dag.tasks) >> watcher()\n\n    DAGS_LIST.append(dag)\n    globals()[DAG_ID] = dag\n\nfor dag in DAGS_LIST:\n    from tests.system.utils import get_test_run\n\n    # Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\n    test_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.828347", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 157, "file_name": "example_bigquery_sensors.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery Sensors.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.sensors.bigquery import (\n    BigQueryTableExistenceAsyncSensor,\n    BigQueryTableExistencePartitionAsyncSensor,\n    BigQueryTableExistenceSensor,\n    BigQueryTablePartitionExistenceSensor,\n)\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_sensors\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"\")\n\nTABLE_NAME = \"partitioned_table\"\nINSERT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n\nPARTITION_NAME = \"{{ ds_nodash }}\"\n\nINSERT_ROWS_QUERY = f\"INSERT {DATASET_NAME}.{TABLE_NAME} VALUES (42, '{{{{ ds }}}}')\"\n\nSCHEMA = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"ds\", \"type\": \"DATE\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n    user_defined_macros={\"DATASET\": DATASET_NAME, \"TABLE\": TABLE_NAME},\n    default_args={\"project_id\": PROJECT_ID},\n) as dag:\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=DATASET_NAME, project_id=PROJECT_ID\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=SCHEMA,\n        time_partitioning={\n            \"type\": \"DAY\",\n            \"field\": \"ds\",\n        },\n    )\n    # [START howto_sensor_bigquery_table]\n    check_table_exists: BaseOperator = BigQueryTableExistenceSensor(\n        task_id=\"check_table_exists\", project_id=PROJECT_ID, dataset_id=DATASET_NAME, table_id=TABLE_NAME\n    )\n    # [END howto_sensor_bigquery_table]\n\n    # [START howto_sensor_bigquery_table_defered]\n    check_table_exists: BaseOperator = BigQueryTableExistenceSensor(\n        task_id=\"check_table_exists_defered\",\n        project_id=PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        deferrable=True,\n    )\n    # [END howto_sensor_bigquery_table_defered]\n\n    # [START howto_sensor_async_bigquery_table]\n    check_table_exists_async = BigQueryTableExistenceAsyncSensor(\n        task_id=\"check_table_exists_async\",\n        project_id=PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n    )\n    # [END howto_sensor_async_bigquery_table]\n\n    execute_insert_query: BaseOperator = BigQueryInsertJobOperator(\n        task_id=\"execute_insert_query\",\n        configuration={\n            \"query\": {\n                \"query\": INSERT_ROWS_QUERY,\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_sensor_bigquery_table_partition]\n    check_table_partition_exists: BaseSensorOperator = BigQueryTablePartitionExistenceSensor(\n        task_id=\"check_table_partition_exists\",\n        project_id=PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        partition_id=PARTITION_NAME,\n    )\n    # [END howto_sensor_bigquery_table_partition]\n\n    # [START howto_sensor_bigquery_table_partition_defered]\n    check_table_partition_exists: BaseSensorOperator = BigQueryTablePartitionExistenceSensor(\n        task_id=\"check_table_partition_exists_defered\",\n        project_id=PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        partition_id=PARTITION_NAME,\n        deferrable=True,\n    )\n    # [END howto_sensor_bigquery_table_partition_defered]\n\n    # [START howto_sensor_bigquery_table_partition_async]\n    check_table_partition_exists_async: BaseSensorOperator = BigQueryTableExistencePartitionAsyncSensor(\n        task_id=\"check_table_partition_exists_async\",\n        partition_id=PARTITION_NAME,\n        project_id=PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n    )\n    # [END howto_sensor_bigquery_table_partition_async]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_dataset >> create_table\n    create_table >> [check_table_exists, execute_insert_query]\n    execute_insert_query >> check_table_partition_exists\n    [check_table_exists, check_table_exists_async, check_table_partition_exists] >> delete_dataset\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.829180", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 217, "file_name": "example_bigquery_tables.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service testing tables.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryDeleteTableOperator,\n    BigQueryGetDatasetTablesOperator,\n    BigQueryUpdateDatasetOperator,\n    BigQueryUpdateTableOperator,\n    BigQueryUpdateTableSchemaOperator,\n    BigQueryUpsertTableOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_tables\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nSCHEMA_JSON_LOCAL_SRC = str(Path(__file__).parent / \"resources\" / \"update_table_schema.json\")\nSCHEMA_JSON_DESTINATION = \"update_table_schema.json\"\nGCS_PATH_TO_SCHEMA_JSON = f\"gs://{BUCKET_NAME}/{SCHEMA_JSON_DESTINATION}\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_schema_json = LocalFilesystemToGCSOperator(\n        task_id=\"upload_schema_json\",\n        src=SCHEMA_JSON_LOCAL_SRC,\n        dst=SCHEMA_JSON_DESTINATION,\n        bucket=BUCKET_NAME,\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    # [START howto_operator_bigquery_create_table]\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_table\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n    # [END howto_operator_bigquery_create_table]\n\n    # [START howto_operator_bigquery_create_view]\n    create_view = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_view\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_view\",\n        view={\n            \"query\": f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.test_table`\",\n            \"useLegacySql\": False,\n        },\n    )\n    # [END howto_operator_bigquery_create_view]\n\n    # [START howto_operator_bigquery_create_materialized_view]\n    create_materialized_view = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_materialized_view\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_materialized_view\",\n        materialized_view={\n            \"query\": f\"SELECT SUM(salary) AS sum_salary FROM `{PROJECT_ID}.{DATASET_NAME}.test_table`\",\n            \"enableRefresh\": True,\n            \"refreshIntervalMs\": 2000000,\n        },\n    )\n    # [END howto_operator_bigquery_create_materialized_view]\n\n    # [START howto_operator_bigquery_delete_view]\n    delete_view = BigQueryDeleteTableOperator(\n        task_id=\"delete_view\",\n        deletion_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.test_view\",\n    )\n    # [END howto_operator_bigquery_delete_view]\n\n    # [START howto_operator_bigquery_update_table]\n    update_table = BigQueryUpdateTableOperator(\n        task_id=\"update_table\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_table\",\n        fields=[\"friendlyName\", \"description\"],\n        table_resource={\n            \"friendlyName\": \"Updated Table\",\n            \"description\": \"Updated Table\",\n        },\n    )\n    # [END howto_operator_bigquery_update_table]\n\n    # [START howto_operator_bigquery_upsert_table]\n    upsert_table = BigQueryUpsertTableOperator(\n        task_id=\"upsert_table\",\n        dataset_id=DATASET_NAME,\n        table_resource={\n            \"tableReference\": {\"tableId\": \"test_table_id\"},\n            \"expirationTime\": (int(time.time()) + 300) * 1000,\n        },\n    )\n    # [END howto_operator_bigquery_upsert_table]\n\n    # [START howto_operator_bigquery_update_table_schema]\n    update_table_schema = BigQueryUpdateTableSchemaOperator(\n        task_id=\"update_table_schema\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_table\",\n        schema_fields_updates=[\n            {\"name\": \"emp_name\", \"description\": \"Name of employee\"},\n            {\"name\": \"salary\", \"description\": \"Monthly salary in USD\"},\n        ],\n    )\n    # [END howto_operator_bigquery_update_table_schema]\n\n    # [START howto_operator_bigquery_create_table_schema_json]\n    update_table_schema_json = BigQueryCreateEmptyTableOperator(\n        task_id=\"update_table_schema_json\",\n        dataset_id=DATASET_NAME,\n        table_id=\"test_table\",\n        gcs_schema_object=GCS_PATH_TO_SCHEMA_JSON,\n    )\n    # [END howto_operator_bigquery_create_table_schema_json]\n\n    # [START howto_operator_bigquery_delete_materialized_view]\n    delete_materialized_view = BigQueryDeleteTableOperator(\n        task_id=\"delete_materialized_view\",\n        deletion_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.test_materialized_view\",\n    )\n    # [END howto_operator_bigquery_delete_materialized_view]\n\n    # [START howto_operator_bigquery_get_dataset_tables]\n    get_dataset_tables = BigQueryGetDatasetTablesOperator(\n        task_id=\"get_dataset_tables\", dataset_id=DATASET_NAME\n    )\n    # [END howto_operator_bigquery_get_dataset_tables]\n\n    update_dataset = BigQueryUpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        dataset_id=DATASET_NAME,\n        dataset_resource={\"description\": \"Updated dataset\"},\n    )\n\n    # [START howto_operator_bigquery_delete_table]\n    delete_table = BigQueryDeleteTableOperator(\n        task_id=\"delete_table\",\n        deletion_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.test_table\",\n    )\n    # [END howto_operator_bigquery_delete_table]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n    delete_dataset.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_dataset\n        >> upload_schema_json\n        # TEST BODY\n        >> update_dataset\n        >> create_table\n        >> create_view\n        >> create_materialized_view\n        >> [\n            get_dataset_tables,\n            delete_view,\n        ]\n        >> update_table\n        >> upsert_table\n        >> update_table_schema\n        >> update_table_schema_json\n        >> delete_materialized_view\n        >> delete_table\n        # TEST TEARDOWN\n        >> delete_bucket\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.838252", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 95, "file_name": "example_bigquery_to_bigquery.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies BigQueryToBigQueryOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_to_bigquery\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nORIGIN = \"origin\"\nTARGET = \"target\"\nLOCATION = \"US\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n        location=LOCATION,\n    )\n\n    create_origin_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_origin_table\",\n        dataset_id=DATASET_NAME,\n        table_id=\"origin\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    create_target_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_target_table\",\n        dataset_id=DATASET_NAME,\n        table_id=\"target\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_operator_bigquery_to_bigquery]\n    copy_selected_data = BigQueryToBigQueryOperator(\n        task_id=\"copy_selected_data\",\n        source_project_dataset_tables=f\"{DATASET_NAME}.{ORIGIN}\",\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TARGET}\",\n    )\n    # [END howto_operator_bigquery_to_bigquery]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> [create_origin_table, create_target_table]\n        # TEST BODY\n        >> copy_selected_data\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.840778", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 91, "file_name": "example_bigquery_to_gcs.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies BigQueryToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_to_gcs\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nBUCKET_FILE = \"test.csv\"\nTABLE = \"test\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_operator_bigquery_to_gcs]\n    bigquery_to_gcs = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs\",\n        source_project_dataset_table=f\"{DATASET_NAME}.{TABLE}\",\n        destination_cloud_storage_uris=[f\"gs://{BUCKET_NAME}/{BUCKET_FILE}\"],\n    )\n    # [END howto_operator_bigquery_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_gcs\n        # TEST TEARDOWN\n        >> [delete_bucket, delete_dataset]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.843301", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 90, "file_name": "example_bigquery_to_gcs_async.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies BigQueryToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"bigquery_to_gcs_async\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nBUCKET_FILE = \"test.csv\"\nTABLE = \"test\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    bigquery_to_gcs_async = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs\",\n        source_project_dataset_table=f\"{DATASET_NAME}.{TABLE}\",\n        destination_cloud_storage_uris=[f\"gs://{BUCKET_NAME}/{BUCKET_FILE}\"],\n        deferrable=True,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_gcs_async\n        # TEST TEARDOWN\n        >> [delete_bucket, delete_dataset]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.866071", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 83, "file_name": "example_bigquery_to_mssql.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\n\ntry:\n    from airflow.providers.google.cloud.transfers.bigquery_to_mssql import BigQueryToMsSqlOperator\nexcept ImportError:\n    pytest.skip(\"MsSQL not available\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nDAG_ID = \"example_bigquery_to_mssql\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDATA_EXPORT_BUCKET_NAME = os.environ.get(\"GCP_BIGQUERY_EXPORT_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nTABLE = \"table_42\"\ndestination_table = \"mssql_table_test\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    # [START howto_operator_bigquery_to_mssql]\n    bigquery_to_mssql = BigQueryToMsSqlOperator(\n        task_id=\"bigquery_to_mssql\",\n        source_project_dataset_table=f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE}\",\n        target_table_name=destination_table,\n        replace=False,\n    )\n    # [END howto_operator_bigquery_to_mssql]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_mssql\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.886376", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 109, "file_name": "example_bigquery_transfer.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_bigquery_transfer\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nORIGIN = \"origin\"\nTARGET = \"target\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_origin_table = BigQueryCreateEmptyTableOperator(\n        task_id=f\"create_{ORIGIN}_table\",\n        dataset_id=DATASET_NAME,\n        table_id=ORIGIN,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    create_target_table = BigQueryCreateEmptyTableOperator(\n        task_id=f\"create_{TARGET}_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TARGET,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    copy_selected_data = BigQueryToBigQueryOperator(\n        task_id=\"copy_selected_data\",\n        source_project_dataset_tables=f\"{DATASET_NAME}.{ORIGIN}\",\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TARGET}\",\n    )\n\n    bigquery_to_gcs = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs\",\n        source_project_dataset_table=f\"{DATASET_NAME}.{ORIGIN}\",\n        destination_cloud_storage_uris=[f\"gs://{BUCKET_NAME}/export-bigquery.csv\"],\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_dataset\n        >> create_origin_table\n        >> create_target_table\n        # TEST BODY\n        >> copy_selected_data\n        >> bigquery_to_gcs\n        # TEST TEARDOWN\n        >> delete_dataset\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.888443", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 82, "file_name": "example_bigquery_to_mysql.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\n\ntry:\n    from airflow.providers.google.cloud.transfers.bigquery_to_mysql import BigQueryToMySqlOperator\nexcept ImportError:\n    pytest.skip(\"MySQL not available\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_bigquery_to_mysql\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDATA_EXPORT_BUCKET_NAME = os.environ.get(\"GCP_BIGQUERY_EXPORT_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nTABLE = \"table_42\"\ndestination_table = \"mysql_table_test\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    # [START howto_operator_bigquery_to_mysql]\n    bigquery_to_mysql = BigQueryToMySqlOperator(\n        task_id=\"bigquery_to_mysql\",\n        dataset_table=f\"{DATASET_NAME}.{TABLE}\",\n        target_table_name=destination_table,\n        replace=False,\n    )\n    # [END howto_operator_bigquery_to_mysql]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_mysql\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.891149", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 207, "file_name": "example_bigtable.py"}, "content": "\"\"\"\nExample Airflow DAG that creates and performs following operations on Cloud Bigtable:\n- creates an Instance\n- creates a Table\n- updates Cluster\n- waits for Table replication completeness\n- deletes the Table\n- deletes the Instance\n\nThis DAG relies on the following environment variables:\n\n* GCP_PROJECT_ID - Google Cloud project\n* CBT_INSTANCE_ID - desired ID of a Cloud Bigtable instance\n* CBT_INSTANCE_DISPLAY_NAME - desired human-readable display name of the Instance\n* CBT_INSTANCE_TYPE - type of the Instance, e.g. 1 for DEVELOPMENT\n    See https://googleapis.github.io/google-cloud-python/latest/bigtable/instance.html#google.cloud.bigtable.instance.Instance\n* CBT_INSTANCE_LABELS - labels to add for the Instance\n* CBT_CLUSTER_ID - desired ID of the main Cluster created for the Instance\n* CBT_CLUSTER_ZONE - zone in which main Cluster will be created. e.g. europe-west1-b\n    See available zones: https://cloud.google.com/bigtable/docs/locations\n* CBT_CLUSTER_NODES - initial amount of nodes of the Cluster\n* CBT_CLUSTER_NODES_UPDATED - amount of nodes for BigtableClusterUpdateOperator\n* CBT_CLUSTER_STORAGE_TYPE - storage for the Cluster, e.g. 1 for SSD\n    See https://googleapis.github.io/google-cloud-python/latest/bigtable/instance.html#google.cloud.bigtable.instance.Instance.cluster\n* CBT_TABLE_ID - desired ID of the Table\n* CBT_POKE_INTERVAL - number of seconds between every attempt of Sensor check\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.decorators import task_group\nfrom airflow.providers.google.cloud.operators.bigtable import (\n    BigtableCreateInstanceOperator,\n    BigtableCreateTableOperator,\n    BigtableDeleteInstanceOperator,\n    BigtableDeleteTableOperator,\n    BigtableUpdateClusterOperator,\n    BigtableUpdateInstanceOperator,\n)\nfrom airflow.providers.google.cloud.sensors.bigtable import BigtableTableReplicationCompletedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"bigtable\"\n\n\nCBT_INSTANCE_ID = f\"bigtable-instance-id-{ENV_ID}\"\nCBT_INSTANCE_DISPLAY_NAME = \"Instance-name\"\nCBT_INSTANCE_DISPLAY_NAME_UPDATED = f\"{CBT_INSTANCE_DISPLAY_NAME} - updated\"\nCBT_INSTANCE_TYPE = 2\nCBT_INSTANCE_TYPE_PROD = 1\nCBT_INSTANCE_LABELS = {}\nCBT_INSTANCE_LABELS_UPDATED = {\"env\": \"prod\"}\nCBT_CLUSTER_ID = f\"bigtable-cluster-id-{ENV_ID}\"\nCBT_CLUSTER_ZONE = \"europe-west1-b\"\nCBT_CLUSTER_NODES = 3\nCBT_CLUSTER_NODES_UPDATED = 5\nCBT_CLUSTER_STORAGE_TYPE = 2\nCBT_TABLE_ID = f\"bigtable-table-id{ENV_ID}\"\nCBT_POKE_INTERVAL = 60\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"bigtable\", \"example\"],\n) as dag:\n    # [START howto_operator_gcp_bigtable_instance_create]\n    create_instance_task = BigtableCreateInstanceOperator(\n        project_id=PROJECT_ID,\n        instance_id=CBT_INSTANCE_ID,\n        main_cluster_id=CBT_CLUSTER_ID,\n        main_cluster_zone=CBT_CLUSTER_ZONE,\n        instance_display_name=CBT_INSTANCE_DISPLAY_NAME,\n        instance_type=CBT_INSTANCE_TYPE,\n        instance_labels=CBT_INSTANCE_LABELS,\n        cluster_nodes=None,\n        cluster_storage_type=CBT_CLUSTER_STORAGE_TYPE,\n        task_id=\"create_instance_task\",\n    )\n    create_instance_task2 = BigtableCreateInstanceOperator(\n        instance_id=CBT_INSTANCE_ID,\n        main_cluster_id=CBT_CLUSTER_ID,\n        main_cluster_zone=CBT_CLUSTER_ZONE,\n        instance_display_name=CBT_INSTANCE_DISPLAY_NAME,\n        instance_type=CBT_INSTANCE_TYPE,\n        instance_labels=CBT_INSTANCE_LABELS,\n        cluster_nodes=CBT_CLUSTER_NODES,\n        cluster_storage_type=CBT_CLUSTER_STORAGE_TYPE,\n        task_id=\"create_instance_task2\",\n    )\n    # [END howto_operator_gcp_bigtable_instance_create]\n\n    @task_group()\n    def create_tables():\n        # [START howto_operator_gcp_bigtable_table_create]\n        create_table_task = BigtableCreateTableOperator(\n            project_id=PROJECT_ID,\n            instance_id=CBT_INSTANCE_ID,\n            table_id=CBT_TABLE_ID,\n            task_id=\"create_table\",\n        )\n        create_table_task2 = BigtableCreateTableOperator(\n            instance_id=CBT_INSTANCE_ID,\n            table_id=CBT_TABLE_ID,\n            task_id=\"create_table_task2\",\n        )\n        # [END howto_operator_gcp_bigtable_table_create]\n        create_table_task >> create_table_task2\n\n    @task_group()\n    def update_clusters_and_instance():\n        # [START howto_operator_gcp_bigtable_cluster_update]\n        cluster_update_task = BigtableUpdateClusterOperator(\n            project_id=PROJECT_ID,\n            instance_id=CBT_INSTANCE_ID,\n            cluster_id=CBT_CLUSTER_ID,\n            nodes=CBT_CLUSTER_NODES_UPDATED,\n            task_id=\"update_cluster_task\",\n        )\n        cluster_update_task2 = BigtableUpdateClusterOperator(\n            instance_id=CBT_INSTANCE_ID,\n            cluster_id=CBT_CLUSTER_ID,\n            nodes=CBT_CLUSTER_NODES_UPDATED,\n            task_id=\"update_cluster_task2\",\n        )\n        # [END howto_operator_gcp_bigtable_cluster_update]\n\n        # [START howto_operator_gcp_bigtable_instance_update]\n        update_instance_task = BigtableUpdateInstanceOperator(\n            instance_id=CBT_INSTANCE_ID,\n            instance_display_name=CBT_INSTANCE_DISPLAY_NAME_UPDATED,\n            instance_type=CBT_INSTANCE_TYPE_PROD,\n            instance_labels=CBT_INSTANCE_LABELS_UPDATED,\n            task_id=\"update_instance_task\",\n        )\n        # [END howto_operator_gcp_bigtable_instance_update]\n\n        [cluster_update_task, cluster_update_task2] >> update_instance_task\n\n    # [START howto_operator_gcp_bigtable_table_wait_for_replication]\n    wait_for_table_replication_task = BigtableTableReplicationCompletedSensor(\n        instance_id=CBT_INSTANCE_ID,\n        table_id=CBT_TABLE_ID,\n        poke_interval=CBT_POKE_INTERVAL,\n        timeout=180,\n        task_id=\"wait_for_table_replication_task2\",\n    )\n    # [END howto_operator_gcp_bigtable_table_wait_for_replication]\n\n    # [START howto_operator_gcp_bigtable_table_delete]\n    delete_table_task = BigtableDeleteTableOperator(\n        project_id=PROJECT_ID,\n        instance_id=CBT_INSTANCE_ID,\n        table_id=CBT_TABLE_ID,\n        task_id=\"delete_table_task\",\n    )\n    delete_table_task2 = BigtableDeleteTableOperator(\n        instance_id=CBT_INSTANCE_ID,\n        table_id=CBT_TABLE_ID,\n        task_id=\"delete_table_task2\",\n    )\n    # [END howto_operator_gcp_bigtable_table_delete]\n    delete_table_task.trigger_rule = TriggerRule.ALL_DONE\n    delete_table_task2.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_bigtable_instance_delete]\n    delete_instance_task = BigtableDeleteInstanceOperator(\n        project_id=PROJECT_ID,\n        instance_id=CBT_INSTANCE_ID,\n        task_id=\"delete_instance_task\",\n    )\n    delete_instance_task2 = BigtableDeleteInstanceOperator(\n        instance_id=CBT_INSTANCE_ID,\n        task_id=\"delete_instance_task2\",\n    )\n    # [END howto_operator_gcp_bigtable_instance_delete]\n    delete_instance_task.trigger_rule = TriggerRule.ALL_DONE\n    delete_instance_task2.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        [create_instance_task, create_instance_task2]\n        >> create_tables()\n        >> wait_for_table_replication_task\n        >> update_clusters_and_instance()\n        >> delete_table_task\n        >> delete_table_task2\n        >> [delete_instance_task, delete_instance_task2]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.894428", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 82, "file_name": "example_bigquery_to_postgres.py"}, "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\n\ntry:\n    from airflow.providers.google.cloud.transfers.bigquery_to_postgres import BigQueryToPostgresOperator\nexcept ImportError:\n    pytest.skip(\"PostgreSQL not available\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_bigquery_to_postgres\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nDATA_EXPORT_BUCKET_NAME = os.environ.get(\"GCP_BIGQUERY_EXPORT_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nTABLE = \"table_42\"\ndestination_table = \"postgres_table_test\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n    # [START howto_operator_bigquery_to_postgres]\n    bigquery_to_postgres = BigQueryToPostgresOperator(\n        task_id=\"bigquery_to_postgres\",\n        dataset_table=f\"{DATASET_NAME}.{TABLE}\",\n        target_table_name=destination_table,\n        replace=False,\n    )\n    # [END howto_operator_bigquery_to_postgres]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE,\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_postgres\n        # TEST TEARDOWN\n        >> delete_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.896536", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 203, "file_name": "example_cloud_build_async.py"}, "content": "\"\"\"\nExample Airflow DAG that displays interactions with Google Cloud Build.\n\nThis DAG relies on the following OS environment variables:\n\n* PROJECT_ID - Google Cloud Project to use for the Cloud Function.\n* GCP_CLOUD_BUILD_ARCHIVE_URL - Path to the zipped source in Google Cloud Storage.\n    This object must be a gzipped archive file (.tar.gz) containing source to build.\n* GCP_CLOUD_BUILD_REPOSITORY_NAME - Name of the Cloud Source Repository.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, cast\n\nimport yaml\nfrom future.backports.urllib.parse import urlparse\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.cloud_build import (\n    CloudBuildCancelBuildOperator,\n    CloudBuildCreateBuildOperator,\n    CloudBuildGetBuildOperator,\n    CloudBuildListBuildsOperator,\n    CloudBuildRetryBuildOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_cloud_build_async\"\n\nBUCKET_NAME_SRC = f\"bucket-src-{DAG_ID}-{ENV_ID}\"\n\nGCP_SOURCE_ARCHIVE_URL = f\"gs://{BUCKET_NAME_SRC}/file.tar.gz\"\nGCP_SOURCE_REPOSITORY_NAME = \"test-cloud-build-repo\"\n\nGCP_SOURCE_ARCHIVE_URL_PARTS = urlparse(GCP_SOURCE_ARCHIVE_URL)\nGCP_SOURCE_BUCKET_NAME = GCP_SOURCE_ARCHIVE_URL_PARTS.netloc\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / \"file.tar.gz\")\n\n# [START howto_operator_gcp_create_build_from_storage_body]\ncreate_build_from_storage_body = {\n    \"source\": {\"storage_source\": GCP_SOURCE_ARCHIVE_URL},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello world\"]}],\n}\n# [END howto_operator_gcp_create_build_from_storage_body]\n\n# [START howto_operator_create_build_from_repo_body]\ncreate_build_from_repo_body: dict[str, Any] = {\n    \"source\": {\"repo_source\": {\"repo_name\": GCP_SOURCE_REPOSITORY_NAME, \"branch_name\": \"master\"}},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello world\"]}],\n}\n# [END howto_operator_create_build_from_repo_body]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket_src = GCSCreateBucketOperator(task_id=\"create_bucket_src\", bucket_name=BUCKET_NAME_SRC)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=\"file.tar.gz\",\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    # [START howto_operator_create_build_from_storage_async]\n    create_build_from_storage = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_storage\",\n        project_id=PROJECT_ID,\n        build=create_build_from_storage_body,\n        deferrable=True,\n    )\n    # [END howto_operator_create_build_from_storage_async]\n\n    # [START howto_operator_create_build_from_storage_result]\n    create_build_from_storage_result = BashOperator(\n        bash_command=f\"echo {cast(str, XComArg(create_build_from_storage, key='results'))}\",\n        task_id=\"create_build_from_storage_result\",\n    )\n    # [END howto_operator_create_build_from_storage_result]\n\n    # [START howto_operator_create_build_from_repo_async]\n    create_build_from_repo = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_repo\",\n        project_id=PROJECT_ID,\n        build=create_build_from_repo_body,\n        deferrable=True,\n    )\n    # [END howto_operator_create_build_from_repo_async]\n\n    # [START howto_operator_create_build_from_repo_result]\n    create_build_from_repo_result = BashOperator(\n        bash_command=f\"echo {cast(str, XComArg(create_build_from_repo, key='results'))}\",\n        task_id=\"create_build_from_repo_result\",\n    )\n    # [END howto_operator_create_build_from_repo_result]\n\n    # [START howto_operator_list_builds]\n    list_builds = CloudBuildListBuildsOperator(\n        task_id=\"list_builds\",\n        project_id=PROJECT_ID,\n        location=\"global\",\n    )\n    # [END howto_operator_list_builds]\n\n    # [START howto_operator_create_build_without_wait_async]\n    create_build_without_wait = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_without_wait\",\n        project_id=PROJECT_ID,\n        build=create_build_from_repo_body,\n        wait=False,\n        deferrable=True,\n    )\n    # [END howto_operator_create_build_without_wait_async]\n\n    # [START howto_operator_cancel_build]\n    cancel_build = CloudBuildCancelBuildOperator(\n        task_id=\"cancel_build\",\n        id_=cast(str, XComArg(create_build_without_wait, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_cancel_build]\n\n    # [START howto_operator_retry_build]\n    retry_build = CloudBuildRetryBuildOperator(\n        task_id=\"retry_build\",\n        id_=cast(str, XComArg(cancel_build, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_retry_build]\n\n    # [START howto_operator_get_build]\n    get_build = CloudBuildGetBuildOperator(\n        task_id=\"get_build\",\n        id_=cast(str, XComArg(retry_build, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_build]\n\n    # [START howto_operator_gcp_create_build_from_yaml_body_async]\n    create_build_from_file = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_file\",\n        project_id=PROJECT_ID,\n        build=yaml.safe_load((Path(CURRENT_FOLDER) / \"resources\" / \"example_cloud_build.yaml\").read_text()),\n        params={\"name\": \"Airflow\"},\n        deferrable=True,\n    )\n    # [END howto_operator_gcp_create_build_from_yaml_body_async]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket_src,\n        upload_file,\n        # TEST BODY\n        create_build_from_storage,\n        create_build_from_storage_result,\n        create_build_from_repo,\n        create_build_from_repo_result,\n        list_builds,\n        create_build_without_wait,\n        cancel_build,\n        retry_build,\n        get_build,\n        create_build_from_file,\n        # TEST TEARDOWN\n        delete_bucket_src,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.900367", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 199, "file_name": "example_cloud_build.py"}, "content": "\"\"\"\nExample Airflow DAG that displays interactions with Google Cloud Build.\n\nThis DAG relies on the following OS environment variables:\n\n* PROJECT_ID - Google Cloud Project to use for the Cloud Function.\n* GCP_CLOUD_BUILD_ARCHIVE_URL - Path to the zipped source in Google Cloud Storage.\n    This object must be a gzipped archive file (.tar.gz) containing source to build.\n* GCP_CLOUD_BUILD_REPOSITORY_NAME - Name of the Cloud Source Repository.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, cast\n\nimport yaml\nfrom future.backports.urllib.parse import urlsplit\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.cloud_build import (\n    CloudBuildCancelBuildOperator,\n    CloudBuildCreateBuildOperator,\n    CloudBuildGetBuildOperator,\n    CloudBuildListBuildsOperator,\n    CloudBuildRetryBuildOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_cloud_build\"\n\nBUCKET_NAME_SRC = f\"bucket-src-{DAG_ID}-{ENV_ID}\"\n\nGCP_SOURCE_ARCHIVE_URL = f\"gs://{BUCKET_NAME_SRC}/file.tar.gz\"\nGCP_SOURCE_REPOSITORY_NAME = \"test-cloud-build-repo\"\n\nGCP_SOURCE_ARCHIVE_URL_PARTS = urlsplit(GCP_SOURCE_ARCHIVE_URL)\nGCP_SOURCE_BUCKET_NAME = GCP_SOURCE_ARCHIVE_URL_PARTS.netloc\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / \"file.tar.gz\")\n\n# [START howto_operator_gcp_create_build_from_storage_body]\ncreate_build_from_storage_body = {\n    \"source\": {\"storage_source\": GCP_SOURCE_ARCHIVE_URL},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello world\"]}],\n}\n# [END howto_operator_gcp_create_build_from_storage_body]\n\n# [START howto_operator_create_build_from_repo_body]\ncreate_build_from_repo_body: dict[str, Any] = {\n    \"source\": {\"repo_source\": {\"repo_name\": GCP_SOURCE_REPOSITORY_NAME, \"branch_name\": \"master\"}},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello world\"]}],\n}\n# [END howto_operator_create_build_from_repo_body]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket_src = GCSCreateBucketOperator(task_id=\"create_bucket_src\", bucket_name=BUCKET_NAME_SRC)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=\"file.tar.gz\",\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    # [START howto_operator_create_build_from_storage]\n    create_build_from_storage = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_storage\",\n        project_id=PROJECT_ID,\n        build=create_build_from_storage_body,\n    )\n    # [END howto_operator_create_build_from_storage]\n\n    # [START howto_operator_create_build_from_storage_result]\n    create_build_from_storage_result = BashOperator(\n        bash_command=f\"echo {cast(str, XComArg(create_build_from_storage, key='results'))}\",\n        task_id=\"create_build_from_storage_result\",\n    )\n    # [END howto_operator_create_build_from_storage_result]\n\n    # [START howto_operator_create_build_from_repo]\n    create_build_from_repo = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_repo\",\n        project_id=PROJECT_ID,\n        build=create_build_from_repo_body,\n    )\n    # [END howto_operator_create_build_from_repo]\n\n    # [START howto_operator_create_build_from_repo_result]\n    create_build_from_repo_result = BashOperator(\n        bash_command=f\"echo {cast(str, XComArg(create_build_from_repo, key='results'))}\",\n        task_id=\"create_build_from_repo_result\",\n    )\n    # [END howto_operator_create_build_from_repo_result]\n\n    # [START howto_operator_list_builds]\n    list_builds = CloudBuildListBuildsOperator(\n        task_id=\"list_builds\",\n        project_id=PROJECT_ID,\n        location=\"global\",\n    )\n    # [END howto_operator_list_builds]\n\n    # [START howto_operator_create_build_without_wait]\n    create_build_without_wait = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_without_wait\",\n        project_id=PROJECT_ID,\n        build=create_build_from_repo_body,\n        wait=False,\n    )\n    # [END howto_operator_create_build_without_wait]\n\n    # [START howto_operator_cancel_build]\n    cancel_build = CloudBuildCancelBuildOperator(\n        task_id=\"cancel_build\",\n        id_=cast(str, XComArg(create_build_without_wait, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_cancel_build]\n\n    # [START howto_operator_retry_build]\n    retry_build = CloudBuildRetryBuildOperator(\n        task_id=\"retry_build\",\n        id_=cast(str, XComArg(cancel_build, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_retry_build]\n\n    # [START howto_operator_get_build]\n    get_build = CloudBuildGetBuildOperator(\n        task_id=\"get_build\",\n        id_=cast(str, XComArg(retry_build, key=\"id\")),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_build]\n\n    # [START howto_operator_gcp_create_build_from_yaml_body]\n    create_build_from_file = CloudBuildCreateBuildOperator(\n        task_id=\"create_build_from_file\",\n        project_id=PROJECT_ID,\n        build=yaml.safe_load((Path(CURRENT_FOLDER) / \"resources\" / \"example_cloud_build.yaml\").read_text()),\n        params={\"name\": \"Airflow\"},\n    )\n    # [END howto_operator_gcp_create_build_from_yaml_body]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket_src,\n        upload_file,\n        # TEST BODY\n        create_build_from_storage,\n        create_build_from_storage_result,\n        create_build_from_repo,\n        create_build_from_repo_result,\n        list_builds,\n        create_build_without_wait,\n        cancel_build,\n        retry_build,\n        get_build,\n        create_build_from_file,\n        # TEST TEARDOWN\n        delete_bucket_src,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.902896", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 145, "file_name": "example_cloud_build_trigger.py"}, "content": "\"\"\"\nExample Airflow DAG that displays interactions with Google Cloud Build.\nThis DAG relies on the following OS environment variables:\n* PROJECT_ID - Google Cloud Project to use for the Cloud Function.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import Any, cast\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.cloud_build import (\n    CloudBuildCreateBuildTriggerOperator,\n    CloudBuildDeleteBuildTriggerOperator,\n    CloudBuildGetBuildTriggerOperator,\n    CloudBuildListBuildTriggersOperator,\n    CloudBuildRunBuildTriggerOperator,\n    CloudBuildUpdateBuildTriggerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_cloud_build_trigger\"\n\nGCP_SOURCE_REPOSITORY_NAME = \"test-cloud-build-repo\"\n\nTRIGGER_NAME = f\"cloud-build-trigger-{ENV_ID}\"\n\n# [START howto_operator_gcp_create_build_trigger_body]\ncreate_build_trigger_body = {\n    \"name\": TRIGGER_NAME,\n    \"trigger_template\": {\n        \"project_id\": PROJECT_ID,\n        \"repo_name\": GCP_SOURCE_REPOSITORY_NAME,\n        \"branch_name\": \"main\",\n    },\n    \"filename\": \"example_cloud_build.yaml\",\n}\n# [END howto_operator_gcp_create_build_trigger_body]\n\n# [START howto_operator_gcp_update_build_trigger_body]\nupdate_build_trigger_body = {\n    \"name\": TRIGGER_NAME,\n    \"trigger_template\": {\n        \"project_id\": PROJECT_ID,\n        \"repo_name\": GCP_SOURCE_REPOSITORY_NAME,\n        \"branch_name\": \"master\",\n    },\n    \"filename\": \"example_cloud_build.yaml\",\n}\n# [END START howto_operator_gcp_update_build_trigger_body]\n\n# [START howto_operator_create_build_from_repo_body]\ncreate_build_from_repo_body: dict[str, Any] = {\n    \"source\": {\"repo_source\": {\"repo_name\": GCP_SOURCE_REPOSITORY_NAME, \"branch_name\": \"master\"}},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello world\"]}],\n}\n# [END howto_operator_create_build_from_repo_body]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_create_build_trigger]\n    create_build_trigger = CloudBuildCreateBuildTriggerOperator(\n        task_id=\"create_build_trigger\", project_id=PROJECT_ID, trigger=create_build_trigger_body\n    )\n    # [END howto_operator_create_build_trigger]\n\n    build_trigger_id = cast(str, XComArg(create_build_trigger, key=\"id\"))\n\n    # [START howto_operator_run_build_trigger]\n    run_build_trigger = CloudBuildRunBuildTriggerOperator(\n        task_id=\"run_build_trigger\",\n        project_id=PROJECT_ID,\n        trigger_id=build_trigger_id,\n        source=create_build_from_repo_body[\"source\"][\"repo_source\"],\n    )\n    # [END howto_operator_run_build_trigger]\n\n    # [START howto_operator_update_build_trigger]\n    update_build_trigger = CloudBuildUpdateBuildTriggerOperator(\n        task_id=\"update_build_trigger\",\n        project_id=PROJECT_ID,\n        trigger_id=build_trigger_id,\n        trigger=update_build_trigger_body,\n    )\n    # [END howto_operator_update_build_trigger]\n\n    # [START howto_operator_get_build_trigger]\n    get_build_trigger = CloudBuildGetBuildTriggerOperator(\n        task_id=\"get_build_trigger\",\n        project_id=PROJECT_ID,\n        trigger_id=build_trigger_id,\n    )\n    # [END howto_operator_get_build_trigger]\n\n    # [START howto_operator_delete_build_trigger]\n    delete_build_trigger = CloudBuildDeleteBuildTriggerOperator(\n        task_id=\"delete_build_trigger\",\n        project_id=PROJECT_ID,\n        trigger_id=build_trigger_id,\n    )\n    # [END howto_operator_delete_build_trigger]\n    delete_build_trigger.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_list_build_triggers]\n    list_build_triggers = CloudBuildListBuildTriggersOperator(\n        task_id=\"list_build_triggers\",\n        project_id=PROJECT_ID,\n        location=\"global\",\n        page_size=5,\n    )\n    # [END howto_operator_list_build_triggers]\n\n    chain(\n        create_build_trigger,\n        run_build_trigger,\n        update_build_trigger,\n        get_build_trigger,\n        delete_build_trigger,\n        list_build_triggers,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.930455", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 136, "file_name": "example_cloud_memorystore_memcached.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Memorystore Memcached service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.cloud_memorystore import (\n    CloudMemorystoreMemcachedApplyParametersOperator,\n    CloudMemorystoreMemcachedCreateInstanceOperator,\n    CloudMemorystoreMemcachedDeleteInstanceOperator,\n    CloudMemorystoreMemcachedGetInstanceOperator,\n    CloudMemorystoreMemcachedListInstancesOperator,\n    CloudMemorystoreMemcachedUpdateInstanceOperator,\n    CloudMemorystoreMemcachedUpdateParametersOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"google_project_id\")\n\nDAG_ID = \"cloud_memorystore_memcached\"\n\nMEMORYSTORE_MEMCACHED_INSTANCE_NAME = f\"memcached-{ENV_ID}-1\"\nLOCATION = \"europe-north1\"\n\n# [START howto_operator_memcached_instance]\nMEMCACHED_INSTANCE = {\n    \"name\": \"\",\n    \"node_count\": 1,\n    \"node_config\": {\"cpu_count\": 1, \"memory_size_mb\": 1024},\n    \"zones\": [LOCATION + \"-a\"],\n}\n# [END howto_operator_memcached_instance]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_create_instance_memcached]\n    create_memcached_instance = CloudMemorystoreMemcachedCreateInstanceOperator(\n        task_id=\"create-instance\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        instance=MEMCACHED_INSTANCE,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_create_instance_memcached]\n\n    # [START howto_operator_delete_instance_memcached]\n    delete_memcached_instance = CloudMemorystoreMemcachedDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_instance_memcached]\n    delete_memcached_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_get_instance_memcached]\n    get_memcached_instance = CloudMemorystoreMemcachedGetInstanceOperator(\n        task_id=\"get-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_get_instance_memcached]\n\n    # [START howto_operator_list_instances_memcached]\n    list_memcached_instances = CloudMemorystoreMemcachedListInstancesOperator(\n        task_id=\"list-instances\", location=\"-\", project_id=PROJECT_ID\n    )\n    # [END howto_operator_list_instances_memcached]\n\n    # [START howto_operator_update_instance_memcached]\n    update_memcached_instance = CloudMemorystoreMemcachedUpdateInstanceOperator(\n        task_id=\"update-instance\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n        update_mask=FieldMask(paths=[\"node_count\"]),\n        instance={\"node_count\": 2},  # 2\n    )\n    # [END howto_operator_update_instance_memcached]\n\n    # [START howto_operator_update_and_apply_parameters_memcached]\n    update_memcached_parameters = CloudMemorystoreMemcachedUpdateParametersOperator(\n        task_id=\"update-parameters\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n        update_mask={\"paths\": [\"params\"]},\n        parameters={\"params\": {\"protocol\": \"ascii\", \"hash_algorithm\": \"jenkins\"}},\n    )\n\n    apply_memcached_parameters = CloudMemorystoreMemcachedApplyParametersOperator(\n        task_id=\"apply-parameters\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_MEMCACHED_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n        node_ids=[\"node-a-1\"],\n        apply_all=False,\n    )\n    # [END howto_operator_update_and_apply_parameters_memcached]\n\n    (\n        create_memcached_instance\n        >> get_memcached_instance\n        >> list_memcached_instances\n        >> update_memcached_instance\n        >> update_memcached_parameters\n        >> apply_memcached_parameters\n        >> delete_memcached_instance\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.954430", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 133, "file_name": "example_functions.py", "syntax_error": "AST parse error: unterminated triple-quoted string literal"}, "content": "* PROJECT_ID - Google Cloud Project to use for the Cloud Function.\n* LOCATION - Google Cloud Functions region where the function should be\n  created.\n* ENTRYPOINT - Name of the executable function in the source code.\n* and one of the below:\n\n    * SOURCE_ARCHIVE_URL - Path to the zipped source in Google Cloud Storage\n\n    * SOURCE_UPLOAD_URL - Generated upload URL for the zipped source and ZIP_PATH - Local path to\n      the zipped source archive\n\n    * SOURCE_REPOSITORY - The URL pointing to the hosted repository where the function\n      is defined in a supported Cloud Source Repository URL format\n      https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions#SourceRepository\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.functions import (\n    CloudFunctionDeleteFunctionOperator,\n    CloudFunctionDeployFunctionOperator,\n    CloudFunctionInvokeFunctionOperator,\n)\n\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_function\"\n\n# make sure there are no dashes in function name (!)\nSHORT_FUNCTION_NAME = \"hello\"\n\nLOCATION = \"europe-west1\"\n\nFUNCTION_NAME = f\"projects/{PROJECT_ID}/locations/{LOCATION}/functions/{SHORT_FUNCTION_NAME}\"\nSOURCE_ARCHIVE_URL = \"\"\nSOURCE_UPLOAD_URL = \"\"\n\nrepo = \"test-repo\"\nSOURCE_REPOSITORY = (\n    f\"https://source.developers.google.com/projects/{PROJECT_ID}/repos/{repo}/moveable-aliases/master\"\n)\n\nZIP_PATH = \"\"\nENTRYPOINT = \"helloWorld\"\nRUNTIME = \"nodejs14\"\nVALIDATE_BODY = True\n\n# [START howto_operator_gcf_deploy_body]\nbody = {\"name\": FUNCTION_NAME, \"entryPoint\": ENTRYPOINT, \"runtime\": RUNTIME, \"httpsTrigger\": {}}\n# [END howto_operator_gcf_deploy_body]\n\n# [START howto_operator_gcf_default_args]\ndefault_args: dict[str, Any] = {\"retries\": 3}\n# [END howto_operator_gcf_default_args]\n\n# [START howto_operator_gcf_deploy_variants]\nif SOURCE_ARCHIVE_URL:\n    body[\"sourceArchiveUrl\"] = SOURCE_ARCHIVE_URL\nelif SOURCE_REPOSITORY:\n    body[\"sourceRepository\"] = {\"url\": SOURCE_REPOSITORY}\nelif ZIP_PATH:\n    body[\"sourceUploadUrl\"] = \"\"\n    default_args[\"zip_path\"] = ZIP_PATH\nelif SOURCE_UPLOAD_URL:\n    body[\"sourceUploadUrl\"] = SOURCE_UPLOAD_URL\nelse:\n    raise Exception(\"Please provide one of the source_code parameters\")\n# [END howto_operator_gcf_deploy_variants]\n\n\nwith models.DAG(\n    DAG_ID,\n    default_args=default_args,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_gcf_deploy]\n    deploy_task = CloudFunctionDeployFunctionOperator(\n        task_id=\"gcf_deploy_task\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        body=body,\n        validate_body=VALIDATE_BODY,\n    )\n    # [END howto_operator_gcf_deploy]\n\n    # [START howto_operator_gcf_deploy_no_project_id]\n    deploy2_task = CloudFunctionDeployFunctionOperator(\n        task_id=\"gcf_deploy2_task\", location=LOCATION, body=body, validate_body=VALIDATE_BODY\n    )\n    # [END howto_operator_gcf_deploy_no_project_id]\n\n    # [START howto_operator_gcf_invoke_function]\n    invoke_task = CloudFunctionInvokeFunctionOperator(\n        task_id=\"invoke_task\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        input_data={},\n        function_id=SHORT_FUNCTION_NAME,\n    )\n    # [END howto_operator_gcf_invoke_function]\n\n    # [START howto_operator_gcf_delete]\n    delete_task = CloudFunctionDeleteFunctionOperator(task_id=\"gcf_delete_task\", name=FUNCTION_NAME)\n    # [END howto_operator_gcf_delete]\n\n    chain(\n        deploy_task,\n        deploy2_task,\n        invoke_task,\n        delete_task,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.956098", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 257, "file_name": "example_cloud_memorystore_redis.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Memorystore service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.redis_v1 import FailoverInstanceRequest, Instance\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.cloud_memorystore import (\n    CloudMemorystoreCreateInstanceAndImportOperator,\n    CloudMemorystoreCreateInstanceOperator,\n    CloudMemorystoreDeleteInstanceOperator,\n    CloudMemorystoreExportAndDeleteInstanceOperator,\n    CloudMemorystoreExportInstanceOperator,\n    CloudMemorystoreFailoverInstanceOperator,\n    CloudMemorystoreGetInstanceOperator,\n    CloudMemorystoreImportOperator,\n    CloudMemorystoreListInstancesOperator,\n    CloudMemorystoreScaleInstanceOperator,\n    CloudMemorystoreUpdateInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSBucketCreateAclEntryOperator,\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nENV_ID_LOWER = ENV_ID.lower() if ENV_ID else \"\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_memorystore_redis\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-north1\"\nMEMORYSTORE_REDIS_INSTANCE_NAME = f\"redis-{ENV_ID_LOWER}-1\"\nMEMORYSTORE_REDIS_INSTANCE_NAME_2 = f\"redis-{ENV_ID_LOWER}-2\"\nMEMORYSTORE_REDIS_INSTANCE_NAME_3 = f\"redis-{ENV_ID_LOWER}-3\"\n\nEXPORT_GCS_URL = f\"gs://{BUCKET_NAME}/my-export.rdb\"\n\n# [START howto_operator_instance]\nFIRST_INSTANCE = {\"tier\": Instance.Tier.BASIC, \"memory_size_gb\": 1}\n# [END howto_operator_instance]\n\nSECOND_INSTANCE = {\"tier\": Instance.Tier.STANDARD_HA, \"memory_size_gb\": 3}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_create_instance]\n    create_instance = CloudMemorystoreCreateInstanceOperator(\n        task_id=\"create-instance\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_REDIS_INSTANCE_NAME,\n        instance=FIRST_INSTANCE,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_create_instance]\n\n    # [START howto_operator_create_instance_result]\n    create_instance_result = BashOperator(\n        task_id=\"create-instance-result\",\n        bash_command=f\"echo {create_instance.output}\",\n    )\n    # [END howto_operator_create_instance_result]\n\n    create_instance_2 = CloudMemorystoreCreateInstanceOperator(\n        task_id=\"create-instance-2\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_REDIS_INSTANCE_NAME_2,\n        instance=SECOND_INSTANCE,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_get_instance]\n    get_instance = CloudMemorystoreGetInstanceOperator(\n        task_id=\"get-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_get_instance]\n\n    # [START howto_operator_get_instance_result]\n    get_instance_result = BashOperator(\n        task_id=\"get-instance-result\", bash_command=f\"echo {get_instance.output}\"\n    )\n    # [END howto_operator_get_instance_result]\n\n    # [START howto_operator_failover_instance]\n    failover_instance = CloudMemorystoreFailoverInstanceOperator(\n        task_id=\"failover-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME_2,\n        data_protection_mode=FailoverInstanceRequest.DataProtectionMode(\n            FailoverInstanceRequest.DataProtectionMode.LIMITED_DATA_LOSS\n        ),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_failover_instance]\n\n    # [START howto_operator_list_instances]\n    list_instances = CloudMemorystoreListInstancesOperator(\n        task_id=\"list-instances\", location=\"-\", page_size=100, project_id=PROJECT_ID\n    )\n    # [END howto_operator_list_instances]\n\n    # [START howto_operator_list_instances_result]\n    list_instances_result = BashOperator(\n        task_id=\"list-instances-result\", bash_command=f\"echo {get_instance.output}\"\n    )\n    # [END howto_operator_list_instances_result]\n\n    # [START howto_operator_update_instance]\n    update_instance = CloudMemorystoreUpdateInstanceOperator(\n        task_id=\"update-instance\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_REDIS_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n        update_mask={\"paths\": [\"memory_size_gb\"]},\n        instance={\"memory_size_gb\": 2},\n    )\n    # [END howto_operator_update_instance]\n\n    # [START howto_operator_set_acl_permission]\n    set_acl_permission = GCSBucketCreateAclEntryOperator(\n        task_id=\"gcs-set-acl-permission\",\n        bucket=BUCKET_NAME,\n        entity=\"user-{{ task_instance.xcom_pull('get-instance')['persistence_iam_identity']\"\n        \".split(':', 2)[1] }}\",\n        role=\"OWNER\",\n    )\n    # [END howto_operator_set_acl_permission]\n\n    # [START howto_operator_export_instance]\n    export_instance = CloudMemorystoreExportInstanceOperator(\n        task_id=\"export-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME,\n        output_config={\"gcs_destination\": {\"uri\": EXPORT_GCS_URL}},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_export_instance]\n\n    # [START howto_operator_import_instance]\n    import_instance = CloudMemorystoreImportOperator(\n        task_id=\"import-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME_2,\n        input_config={\"gcs_source\": {\"uri\": EXPORT_GCS_URL}},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_import_instance]\n\n    # [START howto_operator_delete_instance]\n    delete_instance = CloudMemorystoreDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_delete_instance]\n    delete_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_instance_2 = CloudMemorystoreDeleteInstanceOperator(\n        task_id=\"delete-instance-2\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME_2,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [END howto_operator_create_instance_and_import]\n    create_instance_and_import = CloudMemorystoreCreateInstanceAndImportOperator(\n        task_id=\"create-instance-and-import\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_REDIS_INSTANCE_NAME_3,\n        instance=FIRST_INSTANCE,\n        input_config={\"gcs_source\": {\"uri\": EXPORT_GCS_URL}},\n        project_id=PROJECT_ID,\n    )\n    # [START howto_operator_create_instance_and_import]\n\n    # [START howto_operator_scale_instance]\n    scale_instance = CloudMemorystoreScaleInstanceOperator(\n        task_id=\"scale-instance\",\n        location=LOCATION,\n        instance_id=MEMORYSTORE_REDIS_INSTANCE_NAME_3,\n        project_id=PROJECT_ID,\n        memory_size_gb=3,\n    )\n    # [END howto_operator_scale_instance]\n\n    # [END howto_operator_export_and_delete_instance]\n    export_and_delete_instance = CloudMemorystoreExportAndDeleteInstanceOperator(\n        task_id=\"export-and-delete-instance\",\n        location=LOCATION,\n        instance=MEMORYSTORE_REDIS_INSTANCE_NAME_3,\n        output_config={\"gcs_destination\": {\"uri\": EXPORT_GCS_URL}},\n        project_id=PROJECT_ID,\n    )\n    # [START howto_operator_export_and_delete_instance]\n    export_and_delete_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> create_instance\n        >> create_instance_result\n        >> get_instance\n        >> get_instance_result\n        >> set_acl_permission\n        >> export_instance\n        >> update_instance\n        >> list_instances\n        >> list_instances_result\n        >> create_instance_2\n        >> failover_instance\n        >> import_instance\n        >> delete_instance\n        >> delete_instance_2\n        >> create_instance_and_import\n        >> scale_instance\n        >> export_and_delete_instance\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.959628", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 306, "file_name": "example_cloud_sql.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, patches and deletes a Cloud SQL instance, and also\ncreates, patches and deletes a database inside the instance, in Google Cloud.\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom urllib.parse import urlsplit\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.cloud_sql import (\n    CloudSQLCloneInstanceOperator,\n    CloudSQLCreateInstanceDatabaseOperator,\n    CloudSQLCreateInstanceOperator,\n    CloudSQLDeleteInstanceDatabaseOperator,\n    CloudSQLDeleteInstanceOperator,\n    CloudSQLExportInstanceOperator,\n    CloudSQLImportInstanceOperator,\n    CloudSQLInstancePatchOperator,\n    CloudSQLPatchInstanceDatabaseOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSBucketCreateAclEntryOperator,\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSObjectCreateAclEntryOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"cloudsql\"\n\nINSTANCE_NAME = f\"{DAG_ID}-{ENV_ID}-instance\"\nDB_NAME = f\"{DAG_ID}-{ENV_ID}-db\"\n\nBUCKET_NAME = f\"{DAG_ID}_{ENV_ID}_bucket\"\nFILE_NAME = f\"{DAG_ID}_{ENV_ID}_exportImportTestFile\"\nFILE_URI = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\nFAILOVER_REPLICA_NAME = f\"{INSTANCE_NAME}-failover-replica\"\nREAD_REPLICA_NAME = f\"{INSTANCE_NAME}-read-replica\"\nCLONED_INSTANCE_NAME = f\"{INSTANCE_NAME}-clone\"\n\n# Bodies below represent Cloud SQL instance resources:\n# https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances\n\n# [START howto_operator_cloudsql_create_body]\nbody = {\n    \"name\": INSTANCE_NAME,\n    \"settings\": {\n        \"tier\": \"db-n1-standard-1\",\n        \"backupConfiguration\": {\"binaryLogEnabled\": True, \"enabled\": True, \"startTime\": \"05:00\"},\n        \"activationPolicy\": \"ALWAYS\",\n        \"dataDiskSizeGb\": 30,\n        \"dataDiskType\": \"PD_SSD\",\n        \"databaseFlags\": [],\n        \"ipConfiguration\": {\n            \"ipv4Enabled\": True,\n            \"requireSsl\": True,\n        },\n        \"locationPreference\": {\"zone\": \"europe-west4-a\"},\n        \"maintenanceWindow\": {\"hour\": 5, \"day\": 7, \"updateTrack\": \"canary\"},\n        \"pricingPlan\": \"PER_USE\",\n        \"replicationType\": \"ASYNCHRONOUS\",\n        \"storageAutoResize\": True,\n        \"storageAutoResizeLimit\": 0,\n        \"userLabels\": {\"my-key\": \"my-value\"},\n    },\n    \"failoverReplica\": {\"name\": FAILOVER_REPLICA_NAME},\n    \"databaseVersion\": \"MYSQL_5_7\",\n    \"region\": \"europe-west4\",\n}\n# [END howto_operator_cloudsql_create_body]\n\n# [START howto_operator_cloudsql_create_replica]\nread_replica_body = {\n    \"name\": READ_REPLICA_NAME,\n    \"settings\": {\n        \"tier\": \"db-n1-standard-1\",\n    },\n    \"databaseVersion\": \"MYSQL_5_7\",\n    \"region\": \"europe-west4\",\n    \"masterInstanceName\": INSTANCE_NAME,\n}\n# [END howto_operator_cloudsql_create_replica]\n\n\n# [START howto_operator_cloudsql_patch_body]\npatch_body = {\n    \"name\": INSTANCE_NAME,\n    \"settings\": {\n        \"dataDiskSizeGb\": 35,\n        \"maintenanceWindow\": {\"hour\": 3, \"day\": 6, \"updateTrack\": \"canary\"},\n        \"userLabels\": {\"my-key-patch\": \"my-value-patch\"},\n    },\n}\n# [END howto_operator_cloudsql_patch_body]\n# [START howto_operator_cloudsql_export_body]\nexport_body = {\n    \"exportContext\": {\n        \"fileType\": \"sql\",\n        \"uri\": FILE_URI,\n        \"sqlExportOptions\": {\"schemaOnly\": False},\n        \"offload\": True,\n    }\n}\n# [END howto_operator_cloudsql_export_body]\n# [START howto_operator_cloudsql_import_body]\nimport_body = {\"importContext\": {\"fileType\": \"sql\", \"uri\": FILE_URI}}\n# [END howto_operator_cloudsql_import_body]\n# [START howto_operator_cloudsql_db_create_body]\ndb_create_body = {\"instance\": INSTANCE_NAME, \"name\": DB_NAME, \"project\": PROJECT_ID}\n# [END howto_operator_cloudsql_db_create_body]\n# [START howto_operator_cloudsql_db_patch_body]\ndb_patch_body = {\"charset\": \"utf16\", \"collation\": \"utf16_general_ci\"}\n# [END howto_operator_cloudsql_db_patch_body]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cloud_sql\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # ############################################## #\n    # ### INSTANCES SET UP ######################### #\n    # ############################################## #\n\n    # [START howto_operator_cloudsql_create]\n    sql_instance_create_task = CloudSQLCreateInstanceOperator(\n        body=body, instance=INSTANCE_NAME, task_id=\"sql_instance_create_task\"\n    )\n    # [END howto_operator_cloudsql_create]\n\n    sql_instance_read_replica_create = CloudSQLCreateInstanceOperator(\n        body=read_replica_body,\n        instance=READ_REPLICA_NAME,\n        task_id=\"sql_instance_read_replica_create\",\n    )\n\n    # ############################################## #\n    # ### MODIFYING INSTANCE AND ITS DATABASE ###### #\n    # ############################################## #\n\n    # [START howto_operator_cloudsql_patch]\n    sql_instance_patch_task = CloudSQLInstancePatchOperator(\n        body=patch_body, instance=INSTANCE_NAME, task_id=\"sql_instance_patch_task\"\n    )\n    # [END howto_operator_cloudsql_patch]\n\n    # [START howto_operator_cloudsql_db_create]\n    sql_db_create_task = CloudSQLCreateInstanceDatabaseOperator(\n        body=db_create_body, instance=INSTANCE_NAME, task_id=\"sql_db_create_task\"\n    )\n    # [END howto_operator_cloudsql_db_create]\n\n    # [START howto_operator_cloudsql_db_patch]\n    sql_db_patch_task = CloudSQLPatchInstanceDatabaseOperator(\n        body=db_patch_body,\n        instance=INSTANCE_NAME,\n        database=DB_NAME,\n        task_id=\"sql_db_patch_task\",\n    )\n    # [END howto_operator_cloudsql_db_patch]\n\n    # ############################################## #\n    # ### EXPORTING & IMPORTING SQL ################ #\n    # ############################################## #\n    file_url_split = urlsplit(FILE_URI)\n\n    # For export & import to work we need to add the Cloud SQL instance's Service Account\n    # write access to the destination GCS bucket.\n    service_account_email = XComArg(sql_instance_create_task, key=\"service_account_email\")\n\n    # [START howto_operator_cloudsql_export_gcs_permissions]\n    sql_gcp_add_bucket_permission_task = GCSBucketCreateAclEntryOperator(\n        entity=f\"user-{service_account_email}\",\n        role=\"WRITER\",\n        bucket=file_url_split[1],  # netloc (bucket)\n        task_id=\"sql_gcp_add_bucket_permission_task\",\n    )\n    # [END howto_operator_cloudsql_export_gcs_permissions]\n\n    # [START howto_operator_cloudsql_export]\n    sql_export_task = CloudSQLExportInstanceOperator(\n        body=export_body, instance=INSTANCE_NAME, task_id=\"sql_export_task\"\n    )\n    # [END howto_operator_cloudsql_export]\n\n    # For import to work we need to add the Cloud SQL instance's Service Account\n    # read access to the target GCS object.\n    # [START howto_operator_cloudsql_import_gcs_permissions]\n    sql_gcp_add_object_permission_task = GCSObjectCreateAclEntryOperator(\n        entity=f\"user-{service_account_email}\",\n        role=\"READER\",\n        bucket=file_url_split[1],  # netloc (bucket)\n        object_name=file_url_split[2][1:],  # path (strip first '/')\n        task_id=\"sql_gcp_add_object_permission_task\",\n    )\n    # [END howto_operator_cloudsql_import_gcs_permissions]\n\n    # [START howto_operator_cloudsql_import]\n    sql_import_task = CloudSQLImportInstanceOperator(\n        body=import_body, instance=INSTANCE_NAME, task_id=\"sql_import_task\"\n    )\n    # [END howto_operator_cloudsql_import]\n\n    # ############################################## #\n    # ### CLONE AN INSTANCE ######################## #\n    # ############################################## #\n    # [START howto_operator_cloudsql_clone]\n    sql_instance_clone = CloudSQLCloneInstanceOperator(\n        instance=INSTANCE_NAME, destination_instance_name=CLONED_INSTANCE_NAME, task_id=\"sql_instance_clone\"\n    )\n    # [END howto_operator_cloudsql_clone]\n\n    # ############################################## #\n    # ### DELETING A DATABASE FROM AN INSTANCE ##### #\n    # ############################################## #\n\n    # [START howto_operator_cloudsql_db_delete]\n    sql_db_delete_task = CloudSQLDeleteInstanceDatabaseOperator(\n        instance=INSTANCE_NAME, database=DB_NAME, task_id=\"sql_db_delete_task\"\n    )\n    # [END howto_operator_cloudsql_db_delete]\n    sql_db_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # ############################################## #\n    # ### INSTANCES TEAR DOWN ###################### #\n    # ############################################## #\n\n    # [START howto_operator_cloudsql_replicas_delete]\n    sql_instance_failover_replica_delete_task = CloudSQLDeleteInstanceOperator(\n        instance=FAILOVER_REPLICA_NAME,\n        task_id=\"sql_instance_failover_replica_delete_task\",\n    )\n\n    sql_instance_read_replica_delete_task = CloudSQLDeleteInstanceOperator(\n        instance=READ_REPLICA_NAME, task_id=\"sql_instance_read_replica_delete_task\"\n    )\n    # [END howto_operator_cloudsql_replicas_delete]\n    sql_instance_failover_replica_delete_task.trigger_rule = TriggerRule.ALL_DONE\n    sql_instance_read_replica_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    sql_instance_clone_delete_task = CloudSQLDeleteInstanceOperator(\n        instance=CLONED_INSTANCE_NAME,\n        task_id=\"sql_instance_clone_delete_task\",\n    )\n\n    # [START howto_operator_cloudsql_delete]\n    sql_instance_delete_task = CloudSQLDeleteInstanceOperator(\n        instance=INSTANCE_NAME, task_id=\"sql_instance_delete_task\"\n    )\n    # [END howto_operator_cloudsql_delete]\n    sql_instance_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> sql_instance_create_task\n        >> sql_instance_read_replica_create\n        >> sql_instance_patch_task\n        >> sql_db_create_task\n        >> sql_db_patch_task\n        >> sql_gcp_add_bucket_permission_task\n        >> sql_export_task\n        >> sql_gcp_add_object_permission_task\n        >> sql_import_task\n        >> sql_instance_clone\n        >> sql_db_delete_task\n        >> sql_instance_failover_replica_delete_task\n        >> sql_instance_read_replica_delete_task\n        >> sql_instance_clone_delete_task\n        >> sql_instance_delete_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   sql_instance_create_task >> sql_gcp_add_bucket_permission_task\n    #   sql_instance_create_task >> sql_gcp_add_object_permission_task\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.966543", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 109, "file_name": "example_cloud_composer_deferrable.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.cloud_composer import (\n    CloudComposerCreateEnvironmentOperator,\n    CloudComposerDeleteEnvironmentOperator,\n    CloudComposerUpdateEnvironmentOperator,\n)\nfrom airflow.providers.google.cloud.sensors.cloud_composer import CloudComposerEnvironmentSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_composer_deferrable\"\n\nREGION = \"us-central1\"\n\nENVIRONMENT_ID = f\"test-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\n# [START howto_operator_composer_simple_environment]\nENVIRONMENT = {\n    \"config\": {\n        \"software_config\": {\"image_version\": \"composer-2.0.28-airflow-2.2.5\"},\n    }\n}\n# [END howto_operator_composer_simple_environment]\n\n# [START howto_operator_composer_update_environment]\nUPDATED_ENVIRONMENT = {\n    \"labels\": {\n        \"label2\": \"testing\",\n    }\n}\nUPDATE_MASK = {\"paths\": [\"labels.label2\"]}\n# [END howto_operator_composer_update_environment]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"composer\"],\n) as dag:\n    # [START howto_operator_create_composer_environment_deferrable_mode]\n    defer_create_env = CloudComposerCreateEnvironmentOperator(\n        task_id=\"defer_create_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n        environment=ENVIRONMENT,\n        deferrable=True,\n    )\n    # [END howto_operator_create_composer_environment_deferrable_mode]\n\n    operation_name = defer_create_env.output[\"operation_id\"]\n\n    wait_for_execution = CloudComposerEnvironmentSensor(\n        task_id=\"wait_for_execution\",\n        operation_name=operation_name,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_update_composer_environment_deferrable_mode]\n    defer_update_env = CloudComposerUpdateEnvironmentOperator(\n        task_id=\"defer_update_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n        update_mask=UPDATE_MASK,\n        environment=UPDATED_ENVIRONMENT,\n        deferrable=True,\n    )\n    # [END howto_operator_update_composer_environment_deferrable_mode]\n\n    # [START howto_operator_delete_composer_environment_deferrable_mode]\n    defer_delete_env = CloudComposerDeleteEnvironmentOperator(\n        task_id=\"defer_delete_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n        deferrable=True,\n    )\n    # [END howto_operator_delete_composer_environment_deferrable_mode]\n    defer_delete_env.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        defer_create_env,\n        wait_for_execution,\n        defer_update_env,\n        defer_delete_env,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.969371", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 161, "file_name": "example_cloud_sql_deferrable.py", "syntax_error": "AST parse error: unterminated triple-quoted string literal"}, "content": "* GCP_PROJECT_ID - Google Cloud project for the Cloud SQL instance.\n* INSTANCE_NAME - Name of the Cloud SQL instance.\n* DB_NAME - Name of the database inside a Cloud SQL instance.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom urllib.parse import urlsplit\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.cloud_sql import (\n    CloudSQLCreateInstanceDatabaseOperator,\n    CloudSQLCreateInstanceOperator,\n    CloudSQLDeleteInstanceDatabaseOperator,\n    CloudSQLDeleteInstanceOperator,\n    CloudSQLExportInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSBucketCreateAclEntryOperator,\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"cloudsql-def\"\n\nINSTANCE_NAME = f\"{DAG_ID}-{ENV_ID}-instance\"\nDB_NAME = f\"{DAG_ID}-{ENV_ID}-db\"\n\nBUCKET_NAME = f\"{DAG_ID}_{ENV_ID}_bucket\"\nFILE_NAME = f\"{DAG_ID}_{ENV_ID}_exportImportTestFile\"\nFILE_URI = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# Bodies below represent Cloud SQL instance resources:\n# https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances\n\nbody = {\n    \"name\": INSTANCE_NAME,\n    \"settings\": {\n        \"tier\": \"db-n1-standard-1\",\n        \"backupConfiguration\": {\"binaryLogEnabled\": True, \"enabled\": True, \"startTime\": \"05:00\"},\n        \"activationPolicy\": \"ALWAYS\",\n        \"dataDiskSizeGb\": 30,\n        \"dataDiskType\": \"PD_SSD\",\n        \"databaseFlags\": [],\n        \"ipConfiguration\": {\n            \"ipv4Enabled\": True,\n            \"requireSsl\": True,\n        },\n        \"locationPreference\": {\"zone\": \"europe-west4-a\"},\n        \"maintenanceWindow\": {\"hour\": 5, \"day\": 7, \"updateTrack\": \"canary\"},\n        \"pricingPlan\": \"PER_USE\",\n        \"replicationType\": \"ASYNCHRONOUS\",\n        \"storageAutoResize\": True,\n        \"storageAutoResizeLimit\": 0,\n        \"userLabels\": {\"my-key\": \"my-value\"},\n    },\n    \"databaseVersion\": \"MYSQL_5_7\",\n    \"region\": \"europe-west4\",\n}\n\nexport_body = {\n    \"exportContext\": {\n        \"fileType\": \"sql\",\n        \"uri\": FILE_URI,\n        \"sqlExportOptions\": {\"schemaOnly\": False},\n        \"offload\": True,\n    }\n}\n\ndb_create_body = {\"instance\": INSTANCE_NAME, \"name\": DB_NAME, \"project\": PROJECT_ID}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cloud_sql\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    sql_instance_create_task = CloudSQLCreateInstanceOperator(\n        body=body, instance=INSTANCE_NAME, task_id=\"sql_instance_create_task\"\n    )\n\n    sql_db_create_task = CloudSQLCreateInstanceDatabaseOperator(\n        body=db_create_body, instance=INSTANCE_NAME, task_id=\"sql_db_create_task\"\n    )\n\n    file_url_split = urlsplit(FILE_URI)\n\n    # For export & import to work we need to add the Cloud SQL instance's Service Account\n    # write access to the destination GCS bucket.\n    service_account_email = XComArg(sql_instance_create_task, key=\"service_account_email\")\n\n    sql_gcp_add_bucket_permission_task = GCSBucketCreateAclEntryOperator(\n        entity=f\"user-{service_account_email}\",\n        role=\"WRITER\",\n        bucket=file_url_split[1],  # netloc (bucket)\n        task_id=\"sql_gcp_add_bucket_permission_task\",\n    )\n\n    # [START howto_operator_cloudsql_export_async]\n    sql_export_task = CloudSQLExportInstanceOperator(\n        body=export_body,\n        instance=INSTANCE_NAME,\n        task_id=\"sql_export_task\",\n        deferrable=True,\n    )\n    # [END howto_operator_cloudsql_export_async]\n\n    sql_db_delete_task = CloudSQLDeleteInstanceDatabaseOperator(\n        instance=INSTANCE_NAME, database=DB_NAME, task_id=\"sql_db_delete_task\"\n    )\n    sql_db_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    sql_instance_delete_task = CloudSQLDeleteInstanceOperator(\n        instance=INSTANCE_NAME, task_id=\"sql_instance_delete_task\"\n    )\n    sql_instance_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> sql_instance_create_task\n        >> sql_db_create_task\n        >> sql_gcp_add_bucket_permission_task\n        >> sql_export_task\n        >> sql_db_delete_task\n        >> sql_instance_delete_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   sql_instance_create_task >> sql_gcp_add_bucket_permission_task\n    #   sql_instance_create_task >> sql_gcp_add_object_permission_task\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.972789", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 119, "file_name": "example_cloud_composer.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.cloud_composer import (\n    CloudComposerCreateEnvironmentOperator,\n    CloudComposerDeleteEnvironmentOperator,\n    CloudComposerGetEnvironmentOperator,\n    CloudComposerListEnvironmentsOperator,\n    CloudComposerListImageVersionsOperator,\n    CloudComposerUpdateEnvironmentOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_composer\"\n\nREGION = \"us-central1\"\n\n# [START howto_operator_composer_simple_environment]\n\nENVIRONMENT_ID = f\"test-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\n\nENVIRONMENT = {\n    \"config\": {\n        \"software_config\": {\"image_version\": \"composer-2.0.28-airflow-2.2.5\"},\n    }\n}\n# [END howto_operator_composer_simple_environment]\n\n# [START howto_operator_composer_update_environment]\nUPDATED_ENVIRONMENT = {\n    \"labels\": {\n        \"label1\": \"testing\",\n    }\n}\nUPDATE_MASK = {\"paths\": [\"labels.label1\"]}\n# [END howto_operator_composer_update_environment]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"composer\"],\n) as dag:\n    # [START howto_operator_composer_image_list]\n    image_versions = CloudComposerListImageVersionsOperator(\n        task_id=\"image_versions\",\n        project_id=PROJECT_ID,\n        region=REGION,\n    )\n    # [END howto_operator_composer_image_list]\n\n    # [START howto_operator_create_composer_environment]\n    create_env = CloudComposerCreateEnvironmentOperator(\n        task_id=\"create_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n        environment=ENVIRONMENT,\n    )\n    # [END howto_operator_create_composer_environment]\n\n    # [START howto_operator_list_composer_environments]\n    list_envs = CloudComposerListEnvironmentsOperator(\n        task_id=\"list_envs\", project_id=PROJECT_ID, region=REGION\n    )\n    # [END howto_operator_list_composer_environments]\n\n    # [START howto_operator_get_composer_environment]\n    get_env = CloudComposerGetEnvironmentOperator(\n        task_id=\"get_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n    )\n    # [END howto_operator_get_composer_environment]\n\n    # [START howto_operator_update_composer_environment]\n    update_env = CloudComposerUpdateEnvironmentOperator(\n        task_id=\"update_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n        update_mask=UPDATE_MASK,\n        environment=UPDATED_ENVIRONMENT,\n    )\n    # [END howto_operator_update_composer_environment]\n\n    # [START howto_operator_delete_composer_environment]\n    delete_env = CloudComposerDeleteEnvironmentOperator(\n        task_id=\"delete_env\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        environment_id=ENVIRONMENT_ID,\n    )\n    # [END howto_operator_delete_composer_environment]\n    delete_env.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(image_versions, create_env, list_envs, get_env, update_env, delete_env)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.973404", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 258, "file_name": "example_compute.py"}, "content": "\"\"\"\nExample Airflow DAG that starts, stops and sets the machine type of a Google Compute\nEngine instance.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineDeleteInstanceTemplateOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineInsertInstanceTemplateOperator,\n    ComputeEngineSetMachineTypeOperator,\n    ComputeEngineStartInstanceOperator,\n    ComputeEngineStopInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gce_args_common]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"cloud_compute\"\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nGCE_INSTANCE_NAME = \"instance-compute-test\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nTEMPLATE_NAME = \"instance-template\"\n\nINSTANCE_TEMPLATE_BODY = {\n    \"name\": TEMPLATE_NAME,\n    \"properties\": {\n        \"machine_type\": SHORT_MACHINE_TYPE_NAME,\n        \"disks\": [\n            {\n                \"auto_delete\": True,\n                \"boot\": True,\n                \"device_name\": TEMPLATE_NAME,\n                \"initialize_params\": {\n                    \"disk_size_gb\": \"10\",\n                    \"disk_type\": \"pd-balanced\",\n                    \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n                },\n            }\n        ],\n        \"network_interfaces\": [{\"network\": \"global/networks/default\"}],\n    },\n}\nGCE_INSTANCE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": GCE_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"stack_type\": \"IPV4_ONLY\",\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\nGCE_INSTANCE_FROM_TEMPLATE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n}\n# [END howto_operator_gce_args_common]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gce_insert]\n    gce_instance_insert = ComputeEngineInsertInstanceOperator(\n        task_id=\"gcp_compute_create_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=GCE_INSTANCE_BODY,\n    )\n    # [END howto_operator_gce_insert]\n\n    # Duplicate start for idempotence testing\n    # [START howto_operator_gce_insert_no_project_id]\n    gce_instance_insert2 = ComputeEngineInsertInstanceOperator(\n        task_id=\"gcp_compute_create_instance_task_2\",\n        zone=LOCATION,\n        body=GCE_INSTANCE_BODY,\n    )\n    # [END howto_operator_gce_insert_no_project_id]\n\n    # [START howto_operator_gce_igm_insert_template]\n    gce_instance_template_insert = ComputeEngineInsertInstanceTemplateOperator(\n        task_id=\"gcp_compute_create_template_task\",\n        project_id=PROJECT_ID,\n        body=INSTANCE_TEMPLATE_BODY,\n    )\n    # [END howto_operator_gce_igm_insert_template]\n\n    # Added to check for idempotence\n    # [START howto_operator_gce_igm_insert_template_no_project_id]\n    gce_instance_template_insert2 = ComputeEngineInsertInstanceTemplateOperator(\n        task_id=\"gcp_compute_create_template_task_2\",\n        body=INSTANCE_TEMPLATE_BODY,\n    )\n    # [END howto_operator_gce_igm_insert_template_no_project_id]\n\n    # [START howto_operator_gce_insert_from_template]\n    gce_instance_insert_from_template = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"gcp_compute_create_instance_from_template_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=GCE_INSTANCE_FROM_TEMPLATE_BODY,\n        source_instance_template=f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n    )\n    # [END howto_operator_gce_insert_from_template]\n\n    # Duplicate start for idempotence testing\n    # [START howto_operator_gce_insert_from_template_no_project_id]\n    gce_instance_insert_from_template2 = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"gcp_compute_create_instance_from_template_task_2\",\n        zone=LOCATION,\n        body=GCE_INSTANCE_FROM_TEMPLATE_BODY,\n        source_instance_template=f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n    )\n    # [END howto_operator_gce_insert_from_template_no_project_id]\n\n    # [START howto_operator_gce_start]\n    gce_instance_start = ComputeEngineStartInstanceOperator(\n        task_id=\"gcp_compute_start_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_start]\n\n    # Duplicate start for idempotence testing\n    # [START howto_operator_gce_start_no_project_id]\n    gce_instance_start2 = ComputeEngineStartInstanceOperator(\n        task_id=\"gcp_compute_start_task_2\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_start_no_project_id]\n\n    # [START howto_operator_gce_stop]\n    gce_instance_stop = ComputeEngineStopInstanceOperator(\n        task_id=\"gcp_compute_stop_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_stop]\n    gce_instance_stop.trigger_rule = TriggerRule.ALL_DONE\n\n    # Duplicate stop for idempotence testing\n    # [START howto_operator_gce_stop_no_project_id]\n    gce_instance_stop2 = ComputeEngineStopInstanceOperator(\n        task_id=\"gcp_compute_stop_task_2\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_stop_no_project_id]\n    gce_instance_stop2.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gce_set_machine_type]\n    gce_set_machine_type = ComputeEngineSetMachineTypeOperator(\n        task_id=\"gcp_compute_set_machine_type\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n        body={\"machineType\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\"},\n    )\n    # [END howto_operator_gce_set_machine_type]\n\n    # Duplicate set machine type for idempotence testing\n    # [START howto_operator_gce_set_machine_type_no_project_id]\n    gce_set_machine_type2 = ComputeEngineSetMachineTypeOperator(\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n        body={\"machineType\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\"},\n        task_id=\"gcp_compute_set_machine_type_2\",\n    )\n    # [END howto_operator_gce_set_machine_type_no_project_id]\n\n    # [START howto_operator_gce_delete_no_project_id]\n    gce_instance_delete = ComputeEngineDeleteInstanceOperator(\n        task_id=\"gcp_compute_delete_instance_task\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_no_project_id]\n    gce_instance_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gce_delete_no_project_id]\n    gce_instance_delete2 = ComputeEngineDeleteInstanceOperator(\n        task_id=\"gcp_compute_delete_instance_task_2\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_no_project_id]\n    gce_instance_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gce_delete_new_template_no_project_id]\n    gce_instance_template_delete = ComputeEngineDeleteInstanceTemplateOperator(\n        task_id=\"gcp_compute_delete_template_task\",\n        resource_id=TEMPLATE_NAME,\n    )\n    # [END howto_operator_gce_delete_new_template_no_project_id]\n    gce_instance_template_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        gce_instance_insert,\n        gce_instance_insert2,\n        gce_instance_delete,\n        gce_instance_template_insert,\n        gce_instance_template_insert2,\n        gce_instance_insert_from_template,\n        gce_instance_insert_from_template2,\n        gce_instance_start,\n        gce_instance_start2,\n        gce_instance_stop,\n        gce_instance_stop2,\n        gce_set_machine_type,\n        gce_set_machine_type2,\n        gce_instance_delete2,\n        gce_instance_template_delete,\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:15.987326", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 226, "file_name": "example_compute_igm.py"}, "content": "\"\"\"\nExample Airflow DAG that:\n* creates a copy of existing Instance Template\n* updates existing template in Instance Group Manager\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineCopyInstanceTemplateOperator,\n    ComputeEngineDeleteInstanceGroupManagerOperator,\n    ComputeEngineDeleteInstanceTemplateOperator,\n    ComputeEngineInsertInstanceGroupManagerOperator,\n    ComputeEngineInsertInstanceTemplateOperator,\n    ComputeEngineInstanceGroupUpdateManagerTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nDAG_ID = \"cloud_compute_igm\"\n\n# [START howto_operator_compute_template_copy_args]\nTEMPLATE_NAME = \"instance-template-igm-test\"\nNEW_TEMPLATE_NAME = \"instance-template-test-new\"\n\nINSTANCE_TEMPLATE_BODY = {\n    \"name\": TEMPLATE_NAME,\n    \"properties\": {\n        \"machine_type\": SHORT_MACHINE_TYPE_NAME,\n        \"disks\": [\n            {\n                \"auto_delete\": True,\n                \"boot\": True,\n                \"device_name\": TEMPLATE_NAME,\n                \"initialize_params\": {\n                    \"disk_size_gb\": \"10\",\n                    \"disk_type\": \"pd-balanced\",\n                    \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n                },\n            }\n        ],\n        \"network_interfaces\": [{\"network\": \"global/networks/default\"}],\n    },\n}\n\nNEW_DESCRIPTION = \"Test new description\"\nINSTANCE_TEMPLATE_BODY_UPDATE = {\n    \"name\": NEW_TEMPLATE_NAME,\n    \"description\": NEW_DESCRIPTION,\n    \"properties\": {\"machine_type\": \"n1-standard-2\"},\n}\n# [END howto_operator_compute_template_copy_args]\n\n# [START howto_operator_compute_igm_update_template_args]\nINSTANCE_GROUP_MANAGER_NAME = \"instance-group-test\"\nINSTANCE_GROUP_MANAGER_BODY = {\n    \"name\": INSTANCE_GROUP_MANAGER_NAME,\n    \"base_instance_name\": INSTANCE_GROUP_MANAGER_NAME,\n    \"instance_template\": f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n    \"target_size\": 1,\n}\n\nSOURCE_TEMPLATE_URL = (\n    f\"https://www.googleapis.com/compute/beta/projects/{PROJECT_ID}/\"\n    f\"global/instanceTemplates/{TEMPLATE_NAME}\"\n)\n\n\nDESTINATION_TEMPLATE_URL = (\n    f\"https://www.googleapis.com/compute/beta/projects/{PROJECT_ID}/\"\n    f\"global/instanceTemplates/{NEW_TEMPLATE_NAME}\"\n)\n\nUPDATE_POLICY = {\n    \"type\": \"OPPORTUNISTIC\",\n    \"minimalAction\": \"RESTART\",\n    \"maxSurge\": {\"fixed\": 1},\n    \"minReadySec\": 1800,\n}\n# [END howto_operator_compute_igm_update_template_args]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gce_igm_insert_template]\n    gce_instance_template_insert = ComputeEngineInsertInstanceTemplateOperator(\n        task_id=\"gcp_compute_create_template_task\",\n        project_id=PROJECT_ID,\n        body=INSTANCE_TEMPLATE_BODY,\n    )\n    # [END howto_operator_gce_igm_insert_template]\n\n    # Added to check for idempotence\n    # [START howto_operator_gce_igm_insert_template_no_project_id]\n    gce_instance_template_insert2 = ComputeEngineInsertInstanceTemplateOperator(\n        task_id=\"gcp_compute_create_template_task_2\",\n        body=INSTANCE_TEMPLATE_BODY,\n    )\n    # [END howto_operator_gce_igm_insert_template_no_project_id]\n\n    # [START howto_operator_gce_igm_copy_template]\n    gce_instance_template_copy = ComputeEngineCopyInstanceTemplateOperator(\n        task_id=\"gcp_compute_igm_copy_template_task\",\n        project_id=PROJECT_ID,\n        resource_id=TEMPLATE_NAME,\n        body_patch=INSTANCE_TEMPLATE_BODY_UPDATE,\n    )\n    # [END howto_operator_gce_igm_copy_template]\n\n    # Added to check for idempotence\n    # [START howto_operator_gce_igm_copy_template_no_project_id]\n    gce_instance_template_copy2 = ComputeEngineCopyInstanceTemplateOperator(\n        task_id=\"gcp_compute_igm_copy_template_task_2\",\n        resource_id=TEMPLATE_NAME,\n        body_patch=INSTANCE_TEMPLATE_BODY_UPDATE,\n    )\n    # [END howto_operator_gce_igm_copy_template_no_project_id]\n\n    # [START howto_operator_gce_insert_igm]\n    gce_igm_insert = ComputeEngineInsertInstanceGroupManagerOperator(\n        task_id=\"gcp_compute_create_group_task\",\n        zone=LOCATION,\n        body=INSTANCE_GROUP_MANAGER_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gce_insert_igm]\n\n    # Added to check for idempotence\n    # [START howto_operator_gce_insert_igm_no_project_id]\n    gce_igm_insert2 = ComputeEngineInsertInstanceGroupManagerOperator(\n        task_id=\"gcp_compute_create_group_task_2\",\n        zone=LOCATION,\n        body=INSTANCE_GROUP_MANAGER_BODY,\n    )\n    # [END howto_operator_gce_insert_igm_no_project_id]\n\n    # [START howto_operator_gce_igm_update_template]\n    gce_instance_group_manager_update_template = ComputeEngineInstanceGroupUpdateManagerTemplateOperator(\n        task_id=\"gcp_compute_igm_group_manager_update_template\",\n        project_id=PROJECT_ID,\n        resource_id=INSTANCE_GROUP_MANAGER_NAME,\n        zone=LOCATION,\n        source_template=SOURCE_TEMPLATE_URL,\n        destination_template=DESTINATION_TEMPLATE_URL,\n        update_policy=UPDATE_POLICY,\n    )\n    # [END howto_operator_gce_igm_update_template]\n\n    # Added to check for idempotence (and without UPDATE_POLICY)\n    # [START howto_operator_gce_igm_update_template_no_project_id]\n    gce_instance_group_manager_update_template2 = ComputeEngineInstanceGroupUpdateManagerTemplateOperator(\n        task_id=\"gcp_compute_igm_group_manager_update_template_2\",\n        resource_id=INSTANCE_GROUP_MANAGER_NAME,\n        zone=LOCATION,\n        source_template=SOURCE_TEMPLATE_URL,\n        destination_template=DESTINATION_TEMPLATE_URL,\n    )\n    # [END howto_operator_gce_igm_update_template_no_project_id]\n\n    # [START howto_operator_gce_delete_old_template_no_project_id]\n    gce_instance_template_old_delete = ComputeEngineDeleteInstanceTemplateOperator(\n        task_id=\"gcp_compute_delete_old_template_task\",\n        resource_id=TEMPLATE_NAME,\n    )\n    # [END howto_operator_gce_delete_old_template_no_project_id]\n    gce_instance_template_old_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gce_delete_new_template_no_project_id]\n    gce_instance_template_new_delete = ComputeEngineDeleteInstanceTemplateOperator(\n        task_id=\"gcp_compute_delete_new_template_task\",\n        resource_id=NEW_TEMPLATE_NAME,\n    )\n    # [END howto_operator_gce_delete_new_template_no_project_id]\n    gce_instance_template_new_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gce_delete_igm_no_project_id]\n    gce_igm_delete = ComputeEngineDeleteInstanceGroupManagerOperator(\n        task_id=\"gcp_compute_delete_group_task\",\n        resource_id=INSTANCE_GROUP_MANAGER_NAME,\n        zone=LOCATION,\n    )\n    # [END howto_operator_gce_delete_igm_no_project_id]\n    gce_igm_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        gce_instance_template_insert,\n        gce_instance_template_insert2,\n        gce_instance_template_copy,\n        gce_instance_template_copy2,\n        gce_igm_insert,\n        gce_igm_insert2,\n        gce_instance_group_manager_update_template,\n        gce_instance_group_manager_update_template2,\n        gce_igm_delete,\n        gce_instance_template_old_delete,\n        gce_instance_template_new_delete,\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.016904", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 130, "file_name": "example_compute_ssh.py"}, "content": "\"\"\"\nExample Airflow DAG that starts, stops and sets the machine type of a Google Compute\nEngine instance.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.compute_ssh import ComputeEngineSSHHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gce_args_common]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_ssh\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nGCE_INSTANCE_NAME = \"instance-ssh-test\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nGCE_INSTANCE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": GCE_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"stack_type\": \"IPV4_ONLY\",\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\n# [END howto_operator_gce_args_common]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute-ssh\"],\n) as dag:\n    # [START howto_operator_gce_insert]\n    gce_instance_insert = ComputeEngineInsertInstanceOperator(\n        task_id=\"gcp_compute_create_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=GCE_INSTANCE_BODY,\n    )\n    # [END howto_operator_gce_insert]\n\n    # [START howto_execute_command_on_remote_1]\n    metadata_without_iap_tunnel1 = SSHOperator(\n        task_id=\"metadata_without_iap_tunnel1\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            project_id=PROJECT_ID,\n            use_oslogin=False,\n            use_iap_tunnel=False,\n            cmd_timeout=1,\n        ),\n        command=\"echo metadata_without_iap_tunnel1\",\n    )\n    # [END howto_execute_command_on_remote_1]\n\n    # [START howto_execute_command_on_remote_2]\n    metadata_without_iap_tunnel2 = SSHOperator(\n        task_id=\"metadata_without_iap_tunnel2\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            use_oslogin=False,\n            use_iap_tunnel=False,\n            cmd_timeout=100,\n        ),\n        command=\"echo metadata_without_iap_tunnel2\",\n    )\n    # [END howto_execute_command_on_remote_2]\n\n    # [START howto_operator_gce_delete_no_project_id]\n    gce_instance_delete = ComputeEngineDeleteInstanceOperator(\n        task_id=\"gcp_compute_delete_instance_task\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_no_project_id]\n    gce_instance_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        gce_instance_insert,\n        metadata_without_iap_tunnel1,\n        metadata_without_iap_tunnel2,\n        gce_instance_delete,\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.024445", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 144, "file_name": "example_dlp_info_types.py"}, "content": "\"\"\"\nExample Airflow DAG that creates and manage Data Loss Prevention info types.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.dlp_v2.types import ContentItem, InspectConfig, InspectTemplate\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateStoredInfoTypeOperator,\n    CloudDLPDeleteStoredInfoTypeOperator,\n    CloudDLPGetStoredInfoTypeOperator,\n    CloudDLPListInfoTypesOperator,\n    CloudDLPListStoredInfoTypesOperator,\n    CloudDLPUpdateStoredInfoTypeOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dlp_info_types\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nTEMPLATE_ID = f\"dlp-inspect-info-{ENV_ID}\"\nITEM = ContentItem(\n    table={\n        \"headers\": [{\"name\": \"column1\"}],\n        \"rows\": [{\"values\": [{\"string_value\": \"My phone number is (206) 555-0123\"}]}],\n    }\n)\nINSPECT_CONFIG = InspectConfig(info_types=[{\"name\": \"PHONE_NUMBER\"}, {\"name\": \"US_TOLLFREE_PHONE_NUMBER\"}])\nINSPECT_TEMPLATE = InspectTemplate(inspect_config=INSPECT_CONFIG)\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nFILE_NAME = \"dictionary.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\nFILE_SET = \"tmp/\"\nDICTIONARY_PATH = FILE_SET + FILE_NAME\nOBJECT_GCS_URI = f\"gs://{BUCKET_NAME}/{FILE_SET}\"\nOBJECT_GCS_OUTPUT_URI = OBJECT_GCS_URI + FILE_NAME\n\nCUSTOM_INFO_TYPE_ID = \"custom_info_type\"\nCUSTOM_INFO_TYPES = {\n    \"large_custom_dictionary\": {\n        \"output_path\": {\"path\": OBJECT_GCS_OUTPUT_URI},\n        \"cloud_storage_file_set\": {\"url\": OBJECT_GCS_URI + \"*\"},\n    }\n}\nUPDATE_CUSTOM_INFO_TYPE = {\n    \"large_custom_dictionary\": {\n        \"output_path\": {\"path\": OBJECT_GCS_OUTPUT_URI},\n        \"cloud_storage_file_set\": {\"url\": OBJECT_GCS_URI + \"*\"},\n    }\n}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=DICTIONARY_PATH,\n        bucket=BUCKET_NAME,\n    )\n\n    list_possible_info_types = CloudDLPListInfoTypesOperator(task_id=\"list_info_types\")\n\n    # [START howto_operator_dlp_create_info_type]\n    create_info_type = CloudDLPCreateStoredInfoTypeOperator(\n        project_id=PROJECT_ID,\n        config=CUSTOM_INFO_TYPES,\n        stored_info_type_id=CUSTOM_INFO_TYPE_ID,\n        task_id=\"create_info_type\",\n    )\n    # [END howto_operator_dlp_create_info_type]\n\n    list_stored_info_types = CloudDLPListStoredInfoTypesOperator(\n        task_id=\"list_stored_info_types\", project_id=PROJECT_ID\n    )\n\n    get_stored_info_type = CloudDLPGetStoredInfoTypeOperator(\n        task_id=\"get_stored_info_type\", project_id=PROJECT_ID, stored_info_type_id=CUSTOM_INFO_TYPE_ID\n    )\n\n    # [START howto_operator_dlp_update_info_type]\n    update_info_type = CloudDLPUpdateStoredInfoTypeOperator(\n        project_id=PROJECT_ID,\n        stored_info_type_id=CUSTOM_INFO_TYPE_ID,\n        config=UPDATE_CUSTOM_INFO_TYPE,\n        task_id=\"update_info_type\",\n    )\n    # [END howto_operator_dlp_update_info_type]\n\n    # [START howto_operator_dlp_delete_info_type]\n    delete_info_type = CloudDLPDeleteStoredInfoTypeOperator(\n        project_id=PROJECT_ID,\n        stored_info_type_id=CUSTOM_INFO_TYPE_ID,\n        task_id=\"delete_info_type\",\n    )\n    # [END howto_operator_dlp_delete_info_type]\n    delete_info_type.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> upload_file\n        >> list_possible_info_types\n        >> create_info_type\n        >> list_stored_info_types\n        >> get_stored_info_type\n        >> update_info_type\n        >> delete_info_type\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.043035", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 131, "file_name": "example_compute_ssh_parallel.py"}, "content": "\"\"\"\nExample Airflow DAG that starts, stops and sets the machine type of a Google Compute\nEngine instance.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.compute_ssh import ComputeEngineSSHHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gce_args_common]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_ssh_parallel\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nGCE_INSTANCE_NAME = \"inst-ssh-test-parallel\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nGCE_INSTANCE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": GCE_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"stack_type\": \"IPV4_ONLY\",\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\n# [END howto_operator_gce_args_common]\n\nwith models.DAG(\n    DAG_ID,\n    schedule_interval=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute-ssh-parallel\"],\n) as dag:\n    # [START howto_operator_gce_insert]\n    gce_instance_insert = ComputeEngineInsertInstanceOperator(\n        task_id=\"gcp_compute_create_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=GCE_INSTANCE_BODY,\n    )\n    # [END howto_operator_gce_insert]\n\n    # [START howto_execute_command_on_remote_1]\n    metadata_without_iap_tunnel = SSHOperator(\n        task_id=\"metadata_without_iap_tunnel\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username1\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            project_id=PROJECT_ID,\n            use_oslogin=False,\n            use_iap_tunnel=False,\n            max_retries=5,\n            cmd_timeout=1,\n        ),\n        command=\"echo metadata_without_iap_tunnel\",\n    )\n    # [END howto_execute_command_on_remote_1]\n\n    # [START howto_execute_command_on_remote_2]\n    metadata_with_iap_tunnel = SSHOperator(\n        task_id=\"metadata_with_iap_tunnel\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username2\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            use_oslogin=False,\n            use_iap_tunnel=True,\n            max_retries=5,\n            cmd_timeout=1,\n        ),\n        command=\"echo metadata_with_iap_tunnel\",\n    )\n    # [END howto_execute_command_on_remote_2]\n\n    # [START howto_operator_gce_delete_no_project_id]\n    gce_instance_delete = ComputeEngineDeleteInstanceOperator(\n        task_id=\"gcp_compute_delete_instance_task\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_no_project_id]\n    gce_instance_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        gce_instance_insert,\n        [metadata_without_iap_tunnel, metadata_with_iap_tunnel],\n        gce_instance_delete,\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.046435", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 83, "file_name": "example_dlp_job.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, update, get and delete trigger for Data Loss Prevention actions.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dlp_v2.types import InspectConfig, InspectJobConfig\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCancelDLPJobOperator,\n    CloudDLPCreateDLPJobOperator,\n    CloudDLPDeleteDLPJobOperator,\n    CloudDLPGetDLPJobOperator,\n    CloudDLPListDLPJobsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dlp_job\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nJOB_ID = f\"dlp_job_{ENV_ID}\"\n\nINSPECT_CONFIG = InspectConfig(info_types=[{\"name\": \"PHONE_NUMBER\"}, {\"name\": \"US_TOLLFREE_PHONE_NUMBER\"}])\nINSPECT_JOB = InspectJobConfig(\n    inspect_config=INSPECT_CONFIG,\n    storage_config={\n        \"datastore_options\": {\"partition_id\": {\"project_id\": PROJECT_ID}, \"kind\": {\"name\": \"test\"}}\n    },\n)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    create_job = CloudDLPCreateDLPJobOperator(\n        task_id=\"create_job\", project_id=PROJECT_ID, inspect_job=INSPECT_JOB, job_id=JOB_ID\n    )\n\n    list_jobs = CloudDLPListDLPJobsOperator(\n        task_id=\"list_jobs\", project_id=PROJECT_ID, results_filter=\"state=DONE\"\n    )\n\n    get_job = CloudDLPGetDLPJobOperator(\n        task_id=\"get_job\",\n        project_id=PROJECT_ID,\n        dlp_job_id=\"{{ task_instance.xcom_pull('create_job')['name'].split('/')[-1] }}\",\n    )\n\n    cancel_job = CloudDLPCancelDLPJobOperator(\n        task_id=\"cancel_job\",\n        project_id=PROJECT_ID,\n        dlp_job_id=\"{{ task_instance.xcom_pull('create_job')['name'].split('/')[-1] }}\",\n    )\n\n    delete_job = CloudDLPDeleteDLPJobOperator(\n        task_id=\"delete_job\",\n        project_id=PROJECT_ID,\n        dlp_job_id=\"{{ task_instance.xcom_pull('create_job')['name'].split('/')[-1] }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (create_job >> list_jobs >> get_job >> cancel_job >> delete_job)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.050200", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 138, "file_name": "example_compute_ssh_os_login.py"}, "content": "\"\"\"\nExample Airflow DAG that starts, stops and sets the machine type of a Google Compute\nEngine instance.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.compute_ssh import ComputeEngineSSHHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gce_args_common]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_ssh_os_login\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nGCE_INSTANCE_NAME = \"instance-ssh-test-oslogin\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nGCE_INSTANCE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n    \"metadata\": {\n        \"items\": [\n            {\n                \"key\": \"enable-oslogin\",\n                \"value\": \"TRUE\",\n            }\n        ]\n    },\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": GCE_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"stack_type\": \"IPV4_ONLY\",\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\n# [END howto_operator_gce_args_common]\n\nwith models.DAG(\n    DAG_ID,\n    schedule_interval=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute-ssh\", \"os-login\"],\n) as dag:\n    # [START howto_operator_gce_insert]\n    gce_instance_insert = ComputeEngineInsertInstanceOperator(\n        task_id=\"gcp_compute_create_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=GCE_INSTANCE_BODY,\n    )\n    # [END howto_operator_gce_insert]\n\n    # [START howto_execute_command_on_remote_1]\n    os_login_task1 = SSHOperator(\n        task_id=\"os_login_task1\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            project_id=PROJECT_ID,\n            use_oslogin=True,\n            use_iap_tunnel=False,\n            cmd_timeout=1,\n        ),\n        command=\"echo os_login1\",\n    )\n    # [END howto_execute_command_on_remote_1]\n\n    # [START howto_execute_command_on_remote_2]\n    os_login_task2 = SSHOperator(\n        task_id=\"os_login_task2\",\n        ssh_hook=ComputeEngineSSHHook(\n            user=\"username\",\n            instance_name=GCE_INSTANCE_NAME,\n            zone=LOCATION,\n            use_oslogin=True,\n            use_iap_tunnel=False,\n            cmd_timeout=1,\n        ),\n        command=\"echo os_login2\",\n    )\n    # [END howto_execute_command_on_remote_2]\n\n    # [START howto_operator_gce_delete_no_project_id]\n    gce_instance_delete = ComputeEngineDeleteInstanceOperator(\n        task_id=\"gcp_compute_delete_instance_task\",\n        zone=LOCATION,\n        resource_id=GCE_INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_no_project_id]\n    gce_instance_delete.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        gce_instance_insert,\n        os_login_task1,\n        os_login_task2,\n        gce_instance_delete,\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.050935", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 150, "file_name": "example_dlp_deidentify_content.py"}, "content": "\"\"\"\nExample Airflow DAG that de-identifies potentially sensitive info using Data Loss Prevention operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dlp_v2.types import ContentItem, DeidentifyTemplate, InspectConfig\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateDeidentifyTemplateOperator,\n    CloudDLPDeidentifyContentOperator,\n    CloudDLPDeleteDeidentifyTemplateOperator,\n    CloudDLPGetDeidentifyTemplateOperator,\n    CloudDLPListDeidentifyTemplatesOperator,\n    CloudDLPReidentifyContentOperator,\n    CloudDLPUpdateDeidentifyTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dlp_deidentify_content\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nCRYPTO_KEY_NAME = f\"{DAG_ID}_{ENV_ID}\"\n\nITEM = ContentItem(\n    table={\n        \"headers\": [{\"name\": \"column1\"}],\n        \"rows\": [{\"values\": [{\"string_value\": \"My phone number is (206) 555-0123\"}]}],\n    }\n)\nINSPECT_CONFIG = InspectConfig(info_types=[{\"name\": \"PHONE_NUMBER\"}, {\"name\": \"US_TOLLFREE_PHONE_NUMBER\"}])\n\n# [START dlp_deidentify_config_example]\nDEIDENTIFY_CONFIG = {\n    \"info_type_transformations\": {\n        \"transformations\": [\n            {\n                \"primitive_transformation\": {\n                    \"replace_config\": {\"new_value\": {\"string_value\": \"[deidentified_number]\"}}\n                }\n            }\n        ]\n    }\n}\n# [END dlp_deidentify_config_example]\n\nREVERSIBLE_DEIDENTIFY_CONFIG = {\n    \"info_type_transformations\": {\n        \"transformations\": [\n            {\n                \"primitive_transformation\": {\n                    \"crypto_deterministic_config\": {\n                        \"crypto_key\": {\"transient\": {\"name\": CRYPTO_KEY_NAME}},\n                        \"surrogate_info_type\": {\"name\": \"PHONE_NUMBER\"},\n                    }\n                }\n            }\n        ]\n    }\n}\n\nTEMPLATE_ID = f\"template_{DAG_ID}_{ENV_ID}\"\nDEIDENTIFY_TEMPLATE = DeidentifyTemplate(deidentify_config=DEIDENTIFY_CONFIG)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    # [START _howto_operator_dlp_deidentify_content]\n    deidentify_content = CloudDLPDeidentifyContentOperator(\n        project_id=PROJECT_ID,\n        item=ITEM,\n        deidentify_config=DEIDENTIFY_CONFIG,\n        inspect_config=INSPECT_CONFIG,\n        task_id=\"deidentify_content\",\n    )\n    # [END _howto_operator_dlp_deidentify_content]\n\n    reidentify_content = CloudDLPReidentifyContentOperator(\n        task_id=\"reidentify_content\",\n        project_id=PROJECT_ID,\n        item=ContentItem(value=\"{{ task_instance.xcom_pull('deidentify_content')['item'] }}\"),\n        reidentify_config=REVERSIBLE_DEIDENTIFY_CONFIG,\n        inspect_config=INSPECT_CONFIG,\n    )\n\n    create_template = CloudDLPCreateDeidentifyTemplateOperator(\n        task_id=\"create_template\",\n        project_id=PROJECT_ID,\n        template_id=TEMPLATE_ID,\n        deidentify_template=DEIDENTIFY_TEMPLATE,\n    )\n\n    list_templates = CloudDLPListDeidentifyTemplatesOperator(task_id=\"list_templates\", project_id=PROJECT_ID)\n\n    get_template = CloudDLPGetDeidentifyTemplateOperator(\n        task_id=\"get_template\", project_id=PROJECT_ID, template_id=TEMPLATE_ID\n    )\n\n    update_template = CloudDLPUpdateDeidentifyTemplateOperator(\n        task_id=\"update_template\",\n        project_id=PROJECT_ID,\n        template_id=TEMPLATE_ID,\n        deidentify_template=DEIDENTIFY_TEMPLATE,\n    )\n\n    deidentify_content_with_template = CloudDLPDeidentifyContentOperator(\n        project_id=PROJECT_ID,\n        item=ITEM,\n        deidentify_template_name=\"{{ task_instance.xcom_pull('create_template')['name'] }}\",\n        inspect_config=INSPECT_CONFIG,\n        task_id=\"deidentify_content_with_template\",\n    )\n\n    delete_template = CloudDLPDeleteDeidentifyTemplateOperator(\n        task_id=\"delete_template\",\n        project_id=PROJECT_ID,\n        template_id=TEMPLATE_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        deidentify_content\n        >> reidentify_content\n        >> create_template\n        >> list_templates\n        >> get_template\n        >> update_template\n        >> deidentify_content_with_template\n        >> delete_template\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.054614", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 108, "file_name": "example_dlp_inspect_template.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, updates, list and deletes Data Loss Prevention inspect templates.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dlp_v2.types import ContentItem, InspectConfig, InspectTemplate\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateInspectTemplateOperator,\n    CloudDLPDeleteInspectTemplateOperator,\n    CloudDLPGetInspectTemplateOperator,\n    CloudDLPInspectContentOperator,\n    CloudDLPListInspectTemplatesOperator,\n    CloudDLPUpdateInspectTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dlp_inspect_template\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nTEMPLATE_ID = f\"dlp-inspect-{ENV_ID}\"\nITEM = ContentItem(\n    table={\n        \"headers\": [{\"name\": \"column1\"}],\n        \"rows\": [{\"values\": [{\"string_value\": \"My phone number is (206) 555-0123\"}]}],\n    }\n)\nINSPECT_CONFIG = InspectConfig(info_types=[{\"name\": \"PHONE_NUMBER\"}, {\"name\": \"US_TOLLFREE_PHONE_NUMBER\"}])\nINSPECT_TEMPLATE = InspectTemplate(inspect_config=INSPECT_CONFIG)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    # [START howto_operator_dlp_create_inspect_template]\n    create_template = CloudDLPCreateInspectTemplateOperator(\n        task_id=\"create_template\",\n        project_id=PROJECT_ID,\n        inspect_template=INSPECT_TEMPLATE,\n        template_id=TEMPLATE_ID,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_dlp_create_inspect_template]\n\n    list_templates = CloudDLPListInspectTemplatesOperator(\n        task_id=\"list_templates\",\n        project_id=PROJECT_ID,\n    )\n\n    get_template = CloudDLPGetInspectTemplateOperator(\n        task_id=\"get_template\", project_id=PROJECT_ID, template_id=TEMPLATE_ID\n    )\n\n    update_template = CloudDLPUpdateInspectTemplateOperator(\n        task_id=\"update_template\",\n        project_id=PROJECT_ID,\n        template_id=TEMPLATE_ID,\n        inspect_template=INSPECT_TEMPLATE,\n    )\n\n    # [START howto_operator_dlp_use_inspect_template]\n    inspect_content = CloudDLPInspectContentOperator(\n        task_id=\"inspect_content\",\n        project_id=PROJECT_ID,\n        item=ITEM,\n        inspect_template_name=\"{{ task_instance.xcom_pull('create_template', key='return_value')['name'] }}\",\n    )\n    # [END howto_operator_dlp_use_inspect_template]\n\n    # [START howto_operator_dlp_delete_inspect_template]\n    delete_template = CloudDLPDeleteInspectTemplateOperator(\n        task_id=\"delete_template\",\n        template_id=TEMPLATE_ID,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_dlp_delete_inspect_template]\n    delete_template.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        create_template\n        >> list_templates\n        >> get_template\n        >> update_template\n        >> inspect_content\n        >> delete_template\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.055214", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 88, "file_name": "example_dlp_job_trigger.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, update, get and delete trigger for Data Loss Prevention actions.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobTriggerOperator,\n    CloudDLPDeleteJobTriggerOperator,\n    CloudDLPGetDLPJobTriggerOperator,\n    CloudDLPListJobTriggersOperator,\n    CloudDLPUpdateJobTriggerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dlp_job_trigger\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nJOB_TRIGGER = {\n    \"inspect_job\": {\n        \"storage_config\": {\n            \"datastore_options\": {\"partition_id\": {\"project_id\": PROJECT_ID}, \"kind\": {\"name\": \"test\"}}\n        }\n    },\n    \"triggers\": [{\"schedule\": {\"recurrence_period_duration\": {\"seconds\": 60 * 60 * 24}}}],\n    \"status\": \"HEALTHY\",\n}\n\nTRIGGER_ID = f\"trigger_{ENV_ID}\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"dlp\", \"example\"],\n) as dag:\n    # [START howto_operator_dlp_create_job_trigger]\n    create_trigger = CloudDLPCreateJobTriggerOperator(\n        project_id=PROJECT_ID,\n        job_trigger=JOB_TRIGGER,\n        trigger_id=TRIGGER_ID,\n        task_id=\"create_trigger\",\n    )\n    # [END howto_operator_dlp_create_job_trigger]\n\n    list_triggers = CloudDLPListJobTriggersOperator(task_id=\"list_triggers\", project_id=PROJECT_ID)\n\n    get_trigger = CloudDLPGetDLPJobTriggerOperator(\n        task_id=\"get_trigger\", project_id=PROJECT_ID, job_trigger_id=TRIGGER_ID\n    )\n\n    JOB_TRIGGER[\"triggers\"] = [{\"schedule\": {\"recurrence_period_duration\": {\"seconds\": 2 * 60 * 60 * 24}}}]\n\n    # [START howto_operator_dlp_update_job_trigger]\n    update_trigger = CloudDLPUpdateJobTriggerOperator(\n        project_id=PROJECT_ID,\n        job_trigger_id=TRIGGER_ID,\n        job_trigger=JOB_TRIGGER,\n        task_id=\"update_info_type\",\n    )\n    # [END howto_operator_dlp_update_job_trigger]\n\n    # [START howto_operator_dlp_delete_job_trigger]\n    delete_trigger = CloudDLPDeleteJobTriggerOperator(\n        project_id=PROJECT_ID, job_trigger_id=TRIGGER_ID, task_id=\"delete_info_type\"\n    )\n    # [END howto_operator_dlp_delete_job_trigger]\n    delete_trigger.trigger_rule = TriggerRule.ALL_DONE\n\n    (create_trigger >> list_triggers >> get_trigger >> update_trigger >> delete_trigger)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.083549", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 65, "file_name": "example_dataflow_go.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        },\n        dataflow_config=DataflowConfiguration(job_name=\"start_go_job\", location=LOCATION),\n    )\n\n    wait_for_go_job_async_done = DataflowJobStatusSensor(\n        task_id=\"wait_for_go_job_async_done\",\n        job_id=\"{{task_instance.xcom_pull('start_go_pipeline_dataflow_runner')['dataflow_job_id']}}\",\n        expected_statuses={DataflowJobStatus.JOB_STATE_DONE},\n        location=LOCATION,\n    )\n\n    def check_message(messages: list[dict]) -> bool:\n        \"\"\"Check message\"\"\"\n        for message in messages:\n            if \"Adding workflow start and stop steps.\" in message.get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_go_job_async_message = DataflowJobMessagesSensor(\n        task_id=\"wait_for_go_job_async_message\",\n        job_id=\"{{task_instance.xcom_pull('start_go_pipeline_dataflow_runner')['dataflow_job_id']}}\",\n        location=LOCATION,\n        callback=check_message,\n        fail_on_terminal_state=False,\n    )\n\n    def check_autoscaling_event(autoscaling_events: list[dict]) -> bool:\n        \"\"\"Check autoscaling event\"\"\"\n        for autoscaling_event in autoscaling_events:\n            if \"Worker pool started.\" in autoscaling_event.get(\"description\", {}).get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_go_job_async_autoscaling_event = DataflowJobAutoScalingEventsSensor(\n        task_id=\"wait_for_go_job_async_autoscaling_event\",\n        job_id=\"{{task_instance.xcom_pull('start_go_pipeline_dataflow_runner')['dataflow_job_id']}}\",\n        location=LOCATION,\n        callback=check_autoscaling_event,\n        fail_on_terminal_state=False,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> start_go_pipeline_dataflow_runner\n        >> [\n            wait_for_go_job_async_done,\n            wait_for_go_job_async_message,\n            wait_for_go_job_async_autoscaling_event,\n        ]\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.088584", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 25, "file_name": "example_dataflow_native_java.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        dataflow_config={\n            \"check_if_running\": CheckJobRunning.IgnoreJob,\n            \"location\": LOCATION,\n            \"poll_sleep\": 10,\n        },\n    )\n    # [END howto_operator_start_java_job_jar_on_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    create_bucket >> download_file >> [start_java_job_local, start_java_job] >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.104815", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 30, "file_name": "example_dataflow_native_python.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> start_python_job\n        >> start_python_job_local\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.106977", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 124, "file_name": "example_dataflow_template.py"}, "content": "\"\"\"\nExample Airflow DAG for testing Google Dataflow\n:class:`~airflow.providers.google.cloud.operators.dataflow.DataflowTemplatedJobStartOperator` operator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowStartFlexTemplateOperator,\n    DataflowTemplatedJobStartOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"dataflow_template\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\n\nCSV_FILE_NAME = \"input.csv\"\nAVRO_FILE_NAME = \"output.avro\"\nAVRO_SCHEMA = \"schema.json\"\nGCS_TMP = f\"gs://{BUCKET_NAME}/temp/\"\nGCS_STAGING = f\"gs://{BUCKET_NAME}/staging/\"\nGCS_OUTPUT = f\"gs://{BUCKET_NAME}/output\"\nPYTHON_FILE_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / CSV_FILE_NAME)\nSCHEMA_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / AVRO_SCHEMA)\nLOCATION = \"europe-west3\"\n\ndefault_args = {\n    \"dataflow_default_options\": {\n        \"tempLocation\": GCS_TMP,\n        \"stagingLocation\": GCS_STAGING,\n    }\n}\nBODY = {\n    \"launchParameter\": {\n        \"jobName\": \"test-flex-template\",\n        \"parameters\": {\n            \"inputFileSpec\": f\"gs://{BUCKET_NAME}/{CSV_FILE_NAME}\",\n            \"outputBucket\": f\"gs://{BUCKET_NAME}/output/{AVRO_FILE_NAME}\",\n            \"outputFileFormat\": \"avro\",\n            \"inputFileFormat\": \"csv\",\n            \"schema\": f\"gs://{BUCKET_NAME}/{AVRO_SCHEMA}\",\n        },\n        \"environment\": {},\n        \"containerSpecGcsPath\": \"gs://dataflow-templates/latest/flex/File_Format_Conversion\",\n    },\n}\n\nwith models.DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataflow\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=PYTHON_FILE_LOCAL_PATH,\n        dst=CSV_FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    upload_schema = LocalFilesystemToGCSOperator(\n        task_id=\"upload_schema_to_bucket\",\n        src=SCHEMA_LOCAL_PATH,\n        dst=AVRO_SCHEMA,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_start_template_job]\n    start_template_job = DataflowTemplatedJobStartOperator(\n        task_id=\"start_template_job\",\n        project_id=PROJECT_ID,\n        template=\"gs://dataflow-templates/latest/Word_Count\",\n        parameters={\"inputFile\": f\"gs://{BUCKET_NAME}/{CSV_FILE_NAME}\", \"output\": GCS_OUTPUT},\n        location=LOCATION,\n    )\n    # [END howto_operator_start_template_job]\n\n    # [START howto_operator_start_flex_template_job]\n    start_flex_template_job = DataflowStartFlexTemplateOperator(\n        task_id=\"start_flex_template_job\",\n        project_id=PROJECT_ID,\n        body=BODY,\n        location=LOCATION,\n        append_job_name=False,\n    )\n    # [END howto_operator_start_flex_template_job]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> upload_file\n        >> upload_schema\n        >> start_template_job\n        >> start_flex_template_job\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.110668", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 293, "file_name": "example_datafusion.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use DataFusion.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datafusion import (\n    CloudDataFusionCreateInstanceOperator,\n    CloudDataFusionCreatePipelineOperator,\n    CloudDataFusionDeleteInstanceOperator,\n    CloudDataFusionDeletePipelineOperator,\n    CloudDataFusionGetInstanceOperator,\n    CloudDataFusionListPipelinesOperator,\n    CloudDataFusionRestartInstanceOperator,\n    CloudDataFusionStartPipelineOperator,\n    CloudDataFusionStopPipelineOperator,\n    CloudDataFusionUpdateInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.datafusion import CloudDataFusionPipelineStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_data_fusion_env_variables]\nSERVICE_ACCOUNT = os.environ.get(\"GCP_DATAFUSION_SERVICE_ACCOUNT\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nLOCATION = \"europe-north1\"\nDAG_ID = \"example_data_fusion\"\nINSTANCE_NAME = \"test-instance\"\nINSTANCE = {\n    \"type\": \"BASIC\",\n    \"displayName\": INSTANCE_NAME,\n    \"dataprocServiceAccount\": SERVICE_ACCOUNT,\n}\n\nBUCKET_NAME_1 = \"test-datafusion-1\"\nBUCKET_NAME_2 = \"test-datafusion-2\"\nBUCKET_NAME_1_URI = f\"gs://{BUCKET_NAME_1}\"\nBUCKET_NAME_2_URI = f\"gs://{BUCKET_NAME_2}\"\n\nPIPELINE_NAME = \"test-pipe\"\nPIPELINE = {\n    \"artifact\": {\n        \"name\": \"cdap-data-pipeline\",\n        \"version\": \"6.8.3\",\n        \"scope\": \"SYSTEM\",\n    },\n    \"description\": \"Data Pipeline Application\",\n    \"name\": PIPELINE_NAME,\n    \"config\": {\n        \"resources\": {\"memoryMB\": 2048, \"virtualCores\": 1},\n        \"driverResources\": {\"memoryMB\": 2048, \"virtualCores\": 1},\n        \"connections\": [{\"from\": \"GCS\", \"to\": \"GCS2\"}],\n        \"comments\": [],\n        \"postActions\": [],\n        \"properties\": {},\n        \"processTimingEnabled\": \"true\",\n        \"stageLoggingEnabled\": \"false\",\n        \"stages\": [\n            {\n                \"name\": \"GCS\",\n                \"plugin\": {\n                    \"name\": \"GCSFile\",\n                    \"type\": \"batchsource\",\n                    \"label\": \"GCS\",\n                    \"artifact\": {\"name\": \"google-cloud\", \"version\": \"0.21.2\", \"scope\": \"SYSTEM\"},\n                    \"properties\": {\n                        \"project\": \"auto-detect\",\n                        \"format\": \"text\",\n                        \"skipHeader\": \"false\",\n                        \"serviceFilePath\": \"auto-detect\",\n                        \"filenameOnly\": \"false\",\n                        \"recursive\": \"false\",\n                        \"encrypted\": \"false\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                        \"path\": BUCKET_NAME_1_URI,\n                        \"referenceName\": \"foo_bucket\",\n                        \"useConnection\": \"false\",\n                        \"serviceAccountType\": \"filePath\",\n                        \"sampleSize\": \"1000\",\n                        \"fileEncoding\": \"UTF-8\",\n                    },\n                },\n                \"outputSchema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\"\\\n                    :[{\"name\":\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                \"id\": \"GCS\",\n            },\n            {\n                \"name\": \"GCS2\",\n                \"plugin\": {\n                    \"name\": \"GCS\",\n                    \"type\": \"batchsink\",\n                    \"label\": \"GCS2\",\n                    \"artifact\": {\"name\": \"google-cloud\", \"version\": \"0.21.2\", \"scope\": \"SYSTEM\"},\n                    \"properties\": {\n                        \"project\": \"auto-detect\",\n                        \"suffix\": \"yyyy-MM-dd-HH-mm\",\n                        \"format\": \"json\",\n                        \"serviceFilePath\": \"auto-detect\",\n                        \"location\": \"us\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                        \"referenceName\": \"bar\",\n                        \"path\": BUCKET_NAME_2_URI,\n                        \"serviceAccountType\": \"filePath\",\n                        \"contentType\": \"application/octet-stream\",\n                    },\n                },\n                \"outputSchema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\"\\\n                    :[{\"name\":\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                \"inputSchema\": [\n                    {\n                        \"name\": \"GCS\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                    }\n                ],\n                \"id\": \"GCS2\",\n            },\n        ],\n        \"schedule\": \"0 * * * *\",\n        \"engine\": \"spark\",\n        \"numOfRecordsPreview\": 100,\n        \"description\": \"Data Pipeline Application\",\n        \"maxConcurrentRuns\": 1,\n    },\n}\n# [END howto_data_fusion_env_variables]\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datafusion\"],\n) as dag:\n    create_bucket1 = GCSCreateBucketOperator(\n        task_id=\"create_bucket1\",\n        bucket_name=BUCKET_NAME_1,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket2 = GCSCreateBucketOperator(\n        task_id=\"create_bucket2\",\n        bucket_name=BUCKET_NAME_2,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_cloud_data_fusion_create_instance_operator]\n    create_instance = CloudDataFusionCreateInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        instance=INSTANCE,\n        task_id=\"create_instance\",\n    )\n    # [END howto_cloud_data_fusion_create_instance_operator]\n\n    # [START howto_cloud_data_fusion_get_instance_operator]\n    get_instance = CloudDataFusionGetInstanceOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"get_instance\"\n    )\n    # [END howto_cloud_data_fusion_get_instance_operator]\n\n    # [START howto_cloud_data_fusion_restart_instance_operator]\n    restart_instance = CloudDataFusionRestartInstanceOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"restart_instance\"\n    )\n    # [END howto_cloud_data_fusion_restart_instance_operator]\n\n    # [START howto_cloud_data_fusion_update_instance_operator]\n    update_instance = CloudDataFusionUpdateInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        instance=INSTANCE,\n        update_mask=\"\",\n        task_id=\"update_instance\",\n    )\n    # [END howto_cloud_data_fusion_update_instance_operator]\n\n    # [START howto_cloud_data_fusion_create_pipeline]\n    create_pipeline = CloudDataFusionCreatePipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        pipeline=PIPELINE,\n        instance_name=INSTANCE_NAME,\n        task_id=\"create_pipeline\",\n    )\n    # [END howto_cloud_data_fusion_create_pipeline]\n\n    # [START howto_cloud_data_fusion_list_pipelines]\n    list_pipelines = CloudDataFusionListPipelinesOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"list_pipelines\"\n    )\n    # [END howto_cloud_data_fusion_list_pipelines]\n\n    # [START howto_cloud_data_fusion_start_pipeline]\n    start_pipeline = CloudDataFusionStartPipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        task_id=\"start_pipeline\",\n    )\n    # [END howto_cloud_data_fusion_start_pipeline]\n\n    # [START howto_cloud_data_fusion_start_pipeline_async]\n    start_pipeline_async = CloudDataFusionStartPipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        asynchronous=True,\n        task_id=\"start_pipeline_async\",\n    )\n    # [END howto_cloud_data_fusion_start_pipeline_async]\n\n    # [START howto_cloud_data_fusion_start_pipeline_sensor]\n    start_pipeline_sensor = CloudDataFusionPipelineStateSensor(\n        task_id=\"pipeline_state_sensor\",\n        pipeline_name=PIPELINE_NAME,\n        pipeline_id=start_pipeline_async.output,\n        expected_statuses=[\"COMPLETED\"],\n        failure_statuses=[\"FAILED\"],\n        instance_name=INSTANCE_NAME,\n        location=LOCATION,\n    )\n    # [END howto_cloud_data_fusion_start_pipeline_sensor]\n\n    # [START howto_cloud_data_fusion_stop_pipeline]\n    stop_pipeline = CloudDataFusionStopPipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        task_id=\"stop_pipeline\",\n    )\n    # [END howto_cloud_data_fusion_stop_pipeline]\n\n    # [START howto_cloud_data_fusion_delete_pipeline]\n    delete_pipeline = CloudDataFusionDeletePipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        task_id=\"delete_pipeline\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_cloud_data_fusion_delete_pipeline]\n\n    # [START howto_cloud_data_fusion_delete_instance_operator]\n    delete_instance = CloudDataFusionDeleteInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        task_id=\"delete_instance\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_cloud_data_fusion_delete_instance_operator]\n\n    delete_bucket1 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket1\", bucket_name=BUCKET_NAME_1, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket2 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket2\", bucket_name=BUCKET_NAME_1, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket1, create_bucket2]\n        # TEST BODY\n        >> create_instance\n        >> get_instance\n        >> restart_instance\n        >> update_instance\n        >> create_pipeline\n        >> list_pipelines\n        >> start_pipeline_async\n        >> start_pipeline_sensor\n        >> start_pipeline\n        >> stop_pipeline\n        >> delete_pipeline\n        >> delete_instance\n        # TEST TEARDOWN\n        >> [delete_bucket1, delete_bucket2]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule is part of the DAG\nlist(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.117008", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 109, "file_name": "example_dataflow_native_python_async.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        py_interpreter=\"python3\",\n        py_system_site_packages=False,\n        dataflow_config={\n            \"job_name\": \"start_python_job_async\",\n            \"location\": LOCATION,\n            \"wait_until_finished\": False,\n        },\n    )\n    # [END howto_operator_start_python_job_async]\n\n    # [START howto_sensor_wait_for_job_status]\n    wait_for_python_job_async_done = DataflowJobStatusSensor(\n        task_id=\"wait_for_python_job_async_done\",\n        job_id=\"{{task_instance.xcom_pull('start_python_job_async')['dataflow_job_id']}}\",\n        expected_statuses={DataflowJobStatus.JOB_STATE_DONE},\n        location=LOCATION,\n    )\n    # [END howto_sensor_wait_for_job_status]\n\n    # [START howto_sensor_wait_for_job_metric]\n    def check_metric_scalar_gte(metric_name: str, value: int) -> Callable:\n        \"\"\"Check is metric greater than equals to given value.\"\"\"\n\n        def callback(metrics: list[dict]) -> bool:\n            dag.log.info(\"Looking for '%s' >= %d\", metric_name, value)\n            for metric in metrics:\n                context = metric.get(\"name\", {}).get(\"context\", {})\n                original_name = context.get(\"original_name\", \"\")\n                tentative = context.get(\"tentative\", \"\")\n                if original_name == \"Service-cpu_num_seconds\" and not tentative:\n                    return metric[\"scalar\"] >= value\n            raise AirflowException(f\"Metric '{metric_name}' not found in metrics\")\n\n        return callback\n\n    wait_for_python_job_async_metric = DataflowJobMetricsSensor(\n        task_id=\"wait_for_python_job_async_metric\",\n        job_id=\"{{task_instance.xcom_pull('start_python_job_async')['dataflow_job_id']}}\",\n        location=LOCATION,\n        callback=check_metric_scalar_gte(metric_name=\"Service-cpu_num_seconds\", value=100),\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_metric]\n\n    # [START howto_sensor_wait_for_job_message]\n    def check_message(messages: list[dict]) -> bool:\n        \"\"\"Check message\"\"\"\n        for message in messages:\n            if \"Adding workflow start and stop steps.\" in message.get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_python_job_async_message = DataflowJobMessagesSensor(\n        task_id=\"wait_for_python_job_async_message\",\n        job_id=\"{{task_instance.xcom_pull('start_python_job_async')['dataflow_job_id']}}\",\n        location=LOCATION,\n        callback=check_message,\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_message]\n\n    # [START howto_sensor_wait_for_job_autoscaling_event]\n    def check_autoscaling_event(autoscaling_events: list[dict]) -> bool:\n        \"\"\"Check autoscaling event\"\"\"\n        for autoscaling_event in autoscaling_events:\n            if \"Worker pool started.\" in autoscaling_event.get(\"description\", {}).get(\"messageText\", \"\"):\n                return True\n        return False\n\n    wait_for_python_job_async_autoscaling_event = DataflowJobAutoScalingEventsSensor(\n        task_id=\"wait_for_python_job_async_autoscaling_event\",\n        job_id=\"{{task_instance.xcom_pull('start_python_job_async')['dataflow_job_id']}}\",\n        location=LOCATION,\n        callback=check_autoscaling_event,\n        fail_on_terminal_state=False,\n    )\n    # [END howto_sensor_wait_for_job_autoscaling_event]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> start_python_job_async\n        >> [\n            wait_for_python_job_async_done,\n            wait_for_python_job_async_metric,\n            wait_for_python_job_async_message,\n            wait_for_python_job_async_autoscaling_event,\n        ]\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.120762", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 307, "file_name": "example_dataform.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataform service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dataform_v1beta1 import WorkflowInvocation\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryDeleteDatasetOperator\nfrom airflow.providers.google.cloud.operators.dataform import (\n    DataformCancelWorkflowInvocationOperator,\n    DataformCreateCompilationResultOperator,\n    DataformCreateRepositoryOperator,\n    DataformCreateWorkflowInvocationOperator,\n    DataformCreateWorkspaceOperator,\n    DataformDeleteRepositoryOperator,\n    DataformDeleteWorkspaceOperator,\n    DataformGetCompilationResultOperator,\n    DataformGetWorkflowInvocationOperator,\n    DataformInstallNpmPackagesOperator,\n    DataformMakeDirectoryOperator,\n    DataformRemoveDirectoryOperator,\n    DataformRemoveFileOperator,\n    DataformWriteFileOperator,\n)\nfrom airflow.providers.google.cloud.sensors.dataform import DataformWorkflowInvocationStateSensor\nfrom airflow.providers.google.cloud.utils.dataform import make_initialization_workspace_flow\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_dataform\"\n\nREPOSITORY_ID = f\"example_dataform_repository_{ENV_ID}\"\nREGION = \"us-central1\"\nWORKSPACE_ID = f\"example_dataform_workspace_{ENV_ID}\"\nDEFAULT_DATASET = \"dataform\"\n\n# This DAG is not self-run we need to do some extra configuration to execute it in automation process\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataform\"],\n) as dag:\n    # [START howto_operator_create_repository]\n    make_repository = DataformCreateRepositoryOperator(\n        task_id=\"make-repository\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n    )\n    # [END howto_operator_create_repository]\n\n    # [START howto_operator_create_workspace]\n    make_workspace = DataformCreateWorkspaceOperator(\n        task_id=\"make-workspace\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n    )\n    # [END howto_operator_create_workspace]\n\n    # Delete the default dataset if it exists in the bigquery\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DEFAULT_DATASET,\n        delete_contents=True,\n    )\n\n    # [START howto_initialize_workspace]\n    first_initialization_step, last_initialization_step = make_initialization_workspace_flow(\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n        package_name=f\"dataform_package_{ENV_ID}\",\n        without_installation=True,\n    )\n    # [END howto_initialize_workspace]\n\n    # [START howto_operator_install_npm_packages]\n    install_npm_packages = DataformInstallNpmPackagesOperator(\n        task_id=\"install-npm-packages\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n    )\n    # [END howto_operator_install_npm_packages]\n\n    # [START howto_operator_create_compilation_result]\n    create_compilation_result = DataformCreateCompilationResultOperator(\n        task_id=\"create-compilation-result\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        compilation_result={\n            \"git_commitish\": \"main\",\n            \"workspace\": (\n                f\"projects/{PROJECT_ID}/locations/{REGION}/repositories/{REPOSITORY_ID}/\"\n                f\"workspaces/{WORKSPACE_ID}\"\n            ),\n        },\n    )\n    # [END howto_operator_create_compilation_result]\n\n    # [START howto_operator_get_compilation_result]\n    get_compilation_result = DataformGetCompilationResultOperator(\n        task_id=\"get-compilation-result\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        compilation_result_id=(\n            \"{{ task_instance.xcom_pull('create-compilation-result')['name'].split('/')[-1] }}\"\n        ),\n    )\n    # [END howto_operator_get_compilation_result]]\n\n    # [START howto_operator_create_workflow_invocation]\n    create_workflow_invocation = DataformCreateWorkflowInvocationOperator(\n        task_id=\"create-workflow-invocation\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workflow_invocation={\n            \"compilation_result\": \"{{ task_instance.xcom_pull('create-compilation-result')['name'] }}\"\n        },\n    )\n    # [END howto_operator_create_workflow_invocation]\n\n    # [START howto_operator_create_workflow_invocation_async]\n    create_workflow_invocation_async = DataformCreateWorkflowInvocationOperator(\n        task_id=\"create-workflow-invocation-async\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        asynchronous=True,\n        workflow_invocation={\n            \"compilation_result\": \"{{ task_instance.xcom_pull('create-compilation-result')['name'] }}\"\n        },\n    )\n\n    is_workflow_invocation_done = DataformWorkflowInvocationStateSensor(\n        task_id=\"is-workflow-invocation-done\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workflow_invocation_id=(\n            \"{{ task_instance.xcom_pull('create-workflow-invocation')['name'].split('/')[-1] }}\"\n        ),\n        expected_statuses={WorkflowInvocation.State.SUCCEEDED},\n    )\n    # [END howto_operator_create_workflow_invocation_async]\n\n    # [START howto_operator_get_workflow_invocation]\n    get_workflow_invocation = DataformGetWorkflowInvocationOperator(\n        task_id=\"get-workflow-invocation\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workflow_invocation_id=(\n            \"{{ task_instance.xcom_pull('create-workflow-invocation')['name'].split('/')[-1] }}\"\n        ),\n    )\n    # [END howto_operator_get_workflow_invocation]\n\n    create_workflow_invocation_for_cancel = DataformCreateWorkflowInvocationOperator(\n        task_id=\"create-workflow-invocation-for-cancel\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workflow_invocation={\n            \"compilation_result\": \"{{ task_instance.xcom_pull('create-compilation-result')['name'] }}\"\n        },\n        asynchronous=True,\n    )\n\n    # [START howto_operator_cancel_workflow_invocation]\n    cancel_workflow_invocation = DataformCancelWorkflowInvocationOperator(\n        task_id=\"cancel-workflow-invocation\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workflow_invocation_id=(\n            \"{{ task_instance.xcom_pull('create-workflow-invocation-for-cancel')['name'].split('/')[-1] }}\"\n        ),\n    )\n    # [END howto_operator_cancel_workflow_invocation]\n\n    # [START howto_operator_make_directory]\n    make_test_directory = DataformMakeDirectoryOperator(\n        task_id=\"make-test-directory\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n        directory_path=\"test\",\n    )\n    # [END howto_operator_make_directory]\n\n    # [START howto_operator_write_file]\n    test_file_content = b\"\"\"\n    test test for test file\n    \"\"\"\n    write_test_file = DataformWriteFileOperator(\n        task_id=\"make-test-file\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n        filepath=\"test/test.txt\",\n        contents=test_file_content,\n    )\n    # [END howto_operator_write_file]\n\n    # [START howto_operator_remove_file]\n    remove_test_file = DataformRemoveFileOperator(\n        task_id=\"remove-test-file\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n        filepath=\"test/test.txt\",\n    )\n    # [END howto_operator_remove_file]\n\n    # [START howto_operator_remove_directory]\n    remove_test_directory = DataformRemoveDirectoryOperator(\n        task_id=\"remove-test-directory\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n        directory_path=\"test\",\n    )\n    # [END howto_operator_remove_directory]\n\n    delete_dataset_2 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset_2\",\n        dataset_id=DEFAULT_DATASET,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_delete_workspace]\n    delete_workspace = DataformDeleteWorkspaceOperator(\n        task_id=\"delete-workspace\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n        workspace_id=WORKSPACE_ID,\n    )\n    # [END howto_operator_delete_workspace]\n\n    delete_workspace.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_delete_repository]\n    delete_repository = DataformDeleteRepositoryOperator(\n        task_id=\"delete-repository\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        repository_id=REPOSITORY_ID,\n    )\n    # [END howto_operator_delete_repository]\n\n    delete_repository.trigger_rule = TriggerRule.ALL_DONE\n\n    (make_repository >> make_workspace >> delete_dataset >> first_initialization_step)\n    (\n        last_initialization_step\n        >> install_npm_packages\n        >> create_compilation_result\n        >> get_compilation_result\n        >> create_workflow_invocation\n        >> get_workflow_invocation\n        >> create_workflow_invocation_async\n        >> is_workflow_invocation_done\n        >> create_workflow_invocation_for_cancel\n        >> cancel_workflow_invocation\n        >> make_test_directory\n        >> write_test_file\n        >> remove_test_file\n        >> remove_test_directory\n        >> delete_dataset_2\n        >> delete_workspace\n        >> delete_repository\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.122204", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 269, "file_name": "example_datafusion_async.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use DataFusion.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datafusion import (\n    CloudDataFusionCreateInstanceOperator,\n    CloudDataFusionCreatePipelineOperator,\n    CloudDataFusionDeleteInstanceOperator,\n    CloudDataFusionDeletePipelineOperator,\n    CloudDataFusionGetInstanceOperator,\n    CloudDataFusionListPipelinesOperator,\n    CloudDataFusionRestartInstanceOperator,\n    CloudDataFusionStartPipelineOperator,\n    CloudDataFusionStopPipelineOperator,\n    CloudDataFusionUpdateInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_data_fusion_env_variables]\nSERVICE_ACCOUNT = os.environ.get(\"GCP_DATAFUSION_SERVICE_ACCOUNT\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_data_fusion_async\"\nLOCATION = \"europe-north1\"\nINSTANCE_NAME = \"test-instance-async\"\nINSTANCE = {\n    \"type\": \"BASIC\",\n    \"displayName\": INSTANCE_NAME,\n    \"dataprocServiceAccount\": SERVICE_ACCOUNT,\n}\n\nBUCKET_NAME_1 = \"test-datafusion-async-1\"\nBUCKET_NAME_2 = \"test-datafusion-async-2\"\nBUCKET_NAME_1_URI = f\"gs://{BUCKET_NAME_1}\"\nBUCKET_NAME_2_URI = f\"gs://{BUCKET_NAME_2}\"\n\nPIPELINE_NAME = \"test-pipe\"\nPIPELINE = {\n    \"artifact\": {\n        \"name\": \"cdap-data-pipeline\",\n        \"version\": \"6.8.3\",\n        \"scope\": \"SYSTEM\",\n    },\n    \"description\": \"Data Pipeline Application\",\n    \"name\": PIPELINE_NAME,\n    \"config\": {\n        \"resources\": {\"memoryMB\": 2048, \"virtualCores\": 1},\n        \"driverResources\": {\"memoryMB\": 2048, \"virtualCores\": 1},\n        \"connections\": [{\"from\": \"GCS\", \"to\": \"GCS2\"}],\n        \"comments\": [],\n        \"postActions\": [],\n        \"properties\": {},\n        \"processTimingEnabled\": \"true\",\n        \"stageLoggingEnabled\": \"false\",\n        \"stages\": [\n            {\n                \"name\": \"GCS\",\n                \"plugin\": {\n                    \"name\": \"GCSFile\",\n                    \"type\": \"batchsource\",\n                    \"label\": \"GCS\",\n                    \"artifact\": {\"name\": \"google-cloud\", \"version\": \"0.21.2\", \"scope\": \"SYSTEM\"},\n                    \"properties\": {\n                        \"project\": \"auto-detect\",\n                        \"format\": \"text\",\n                        \"skipHeader\": \"false\",\n                        \"serviceFilePath\": \"auto-detect\",\n                        \"filenameOnly\": \"false\",\n                        \"recursive\": \"false\",\n                        \"encrypted\": \"false\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                        \"path\": BUCKET_NAME_1_URI,\n                        \"referenceName\": \"foo_bucket\",\n                        \"useConnection\": \"false\",\n                        \"serviceAccountType\": \"filePath\",\n                        \"sampleSize\": \"1000\",\n                        \"fileEncoding\": \"UTF-8\",\n                    },\n                },\n                \"outputSchema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\"\\\n                    :[{\"name\":\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                \"id\": \"GCS\",\n            },\n            {\n                \"name\": \"GCS2\",\n                \"plugin\": {\n                    \"name\": \"GCS\",\n                    \"type\": \"batchsink\",\n                    \"label\": \"GCS2\",\n                    \"artifact\": {\"name\": \"google-cloud\", \"version\": \"0.21.2\", \"scope\": \"SYSTEM\"},\n                    \"properties\": {\n                        \"project\": \"auto-detect\",\n                        \"suffix\": \"yyyy-MM-dd-HH-mm\",\n                        \"format\": \"json\",\n                        \"serviceFilePath\": \"auto-detect\",\n                        \"location\": \"us\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                        \"referenceName\": \"bar\",\n                        \"path\": BUCKET_NAME_2_URI,\n                        \"serviceAccountType\": \"filePath\",\n                        \"contentType\": \"application/octet-stream\",\n                    },\n                },\n                \"outputSchema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\"\\\n                    :[{\"name\":\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                \"inputSchema\": [\n                    {\n                        \"name\": \"GCS\",\n                        \"schema\": '{\"type\":\"record\",\"name\":\"textfile\",\"fields\":[{\"name\"\\\n                            :\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}',\n                    }\n                ],\n                \"id\": \"GCS2\",\n            },\n        ],\n        \"schedule\": \"0 * * * *\",\n        \"engine\": \"spark\",\n        \"numOfRecordsPreview\": 100,\n        \"description\": \"Data Pipeline Application\",\n        \"maxConcurrentRuns\": 1,\n    },\n}\n# [END howto_data_fusion_env_variables]\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datafusion\", \"deferrable\"],\n) as dag:\n    create_bucket1 = GCSCreateBucketOperator(\n        task_id=\"create_bucket1\",\n        bucket_name=BUCKET_NAME_1,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket2 = GCSCreateBucketOperator(\n        task_id=\"create_bucket2\",\n        bucket_name=BUCKET_NAME_2,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_cloud_data_fusion_create_instance_operator]\n    create_instance = CloudDataFusionCreateInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        instance=INSTANCE,\n        task_id=\"create_instance\",\n    )\n    # [END howto_cloud_data_fusion_create_instance_operator]\n\n    # [START howto_cloud_data_fusion_get_instance_operator]\n    get_instance = CloudDataFusionGetInstanceOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"get_instance\"\n    )\n    # [END howto_cloud_data_fusion_get_instance_operator]\n\n    # [START howto_cloud_data_fusion_restart_instance_operator]\n    restart_instance = CloudDataFusionRestartInstanceOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"restart_instance\"\n    )\n    # [END howto_cloud_data_fusion_restart_instance_operator]\n\n    # [START howto_cloud_data_fusion_update_instance_operator]\n    update_instance = CloudDataFusionUpdateInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        instance=INSTANCE,\n        update_mask=\"\",\n        task_id=\"update_instance\",\n    )\n    # [END howto_cloud_data_fusion_update_instance_operator]\n\n    # [START howto_cloud_data_fusion_create_pipeline]\n    create_pipeline = CloudDataFusionCreatePipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        pipeline=PIPELINE,\n        instance_name=INSTANCE_NAME,\n        task_id=\"create_pipeline\",\n    )\n    # [END howto_cloud_data_fusion_create_pipeline]\n\n    # [START howto_cloud_data_fusion_list_pipelines]\n    list_pipelines = CloudDataFusionListPipelinesOperator(\n        location=LOCATION, instance_name=INSTANCE_NAME, task_id=\"list_pipelines\"\n    )\n    # [END howto_cloud_data_fusion_list_pipelines]\n\n    # [START howto_cloud_data_fusion_start_pipeline_def]\n    start_pipeline_def = CloudDataFusionStartPipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        deferrable=True,\n        task_id=\"start_pipeline_def\",\n    )\n    # [END howto_cloud_data_fusion_start_pipeline_def]\n\n    # [START howto_cloud_data_fusion_stop_pipeline]\n    stop_pipeline = CloudDataFusionStopPipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        task_id=\"stop_pipeline\",\n    )\n    # [END howto_cloud_data_fusion_stop_pipeline]\n\n    # [START howto_cloud_data_fusion_delete_pipeline]\n    delete_pipeline = CloudDataFusionDeletePipelineOperator(\n        location=LOCATION,\n        pipeline_name=PIPELINE_NAME,\n        instance_name=INSTANCE_NAME,\n        task_id=\"delete_pipeline\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_cloud_data_fusion_delete_pipeline]\n\n    # [START howto_cloud_data_fusion_delete_instance_operator]\n    delete_instance = CloudDataFusionDeleteInstanceOperator(\n        location=LOCATION,\n        instance_name=INSTANCE_NAME,\n        task_id=\"delete_instance\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_cloud_data_fusion_delete_instance_operator]\n\n    delete_bucket1 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket1\", bucket_name=BUCKET_NAME_1, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket2 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket2\", bucket_name=BUCKET_NAME_1, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket1, create_bucket2]\n        # TEST BODY\n        >> create_instance\n        >> get_instance\n        >> restart_instance\n        >> update_instance\n        >> create_pipeline\n        >> list_pipelines\n        >> start_pipeline_def\n        >> stop_pipeline\n        >> delete_pipeline\n        >> delete_instance\n        # TEST TEARDOWN\n        >> [delete_bucket1, delete_bucket2]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule is part of the DAG\nlist(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.144995", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 198, "file_name": "example_dataplex.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use Dataplex.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateLakeOperator,\n    DataplexCreateTaskOperator,\n    DataplexDeleteLakeOperator,\n    DataplexDeleteTaskOperator,\n    DataplexGetTaskOperator,\n    DataplexListTasksOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataplex import DataplexTaskStateSensor\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_dataplex\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nSPARK_FILE_NAME = \"spark_example_pi.py\"\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / SPARK_FILE_NAME)\n\nLAKE_ID = f\"test-lake-{ENV_ID}\"\nREGION = \"us-central1\"\n\nSERVICE_ACC = os.environ.get(\"GCP_DATAPLEX_SERVICE_ACC\")\n\nSPARK_FILE_FULL_PATH = f\"gs://{BUCKET_NAME}/{SPARK_FILE_NAME}\"\nDATAPLEX_TASK_ID = f\"test-task-{ENV_ID}\"\nTRIGGER_SPEC_TYPE = \"ON_DEMAND\"\n\n# [START howto_dataplex_configuration]\nEXAMPLE_TASK_BODY = {\n    \"trigger_spec\": {\"type_\": TRIGGER_SPEC_TYPE},\n    \"execution_spec\": {\"service_account\": SERVICE_ACC},\n    \"spark\": {\"python_script_file\": SPARK_FILE_FULL_PATH},\n}\n# [END howto_dataplex_configuration]\n\n# [START howto_dataplex_lake_configuration]\nEXAMPLE_LAKE_BODY = {\n    \"display_name\": \"test_display_name\",\n    \"labels\": [],\n    \"description\": \"test_description\",\n    \"metastore\": {\"service\": \"\"},\n}\n# [END howto_dataplex_lake_configuration]\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\", \"dataplex\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=SPARK_FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n    # [START howto_dataplex_create_lake_operator]\n    create_lake = DataplexCreateLakeOperator(\n        project_id=PROJECT_ID, region=REGION, body=EXAMPLE_LAKE_BODY, lake_id=LAKE_ID, task_id=\"create_lake\"\n    )\n    # [END howto_dataplex_create_lake_operator]\n\n    # [START howto_dataplex_create_task_operator]\n    create_dataplex_task = DataplexCreateTaskOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        body=EXAMPLE_TASK_BODY,\n        dataplex_task_id=DATAPLEX_TASK_ID,\n        task_id=\"create_dataplex_task\",\n    )\n    # [END howto_dataplex_create_task_operator]\n\n    # [START howto_dataplex_async_create_task_operator]\n    create_dataplex_task_async = DataplexCreateTaskOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        body=EXAMPLE_TASK_BODY,\n        dataplex_task_id=f\"{DATAPLEX_TASK_ID}-1\",\n        asynchronous=True,\n        task_id=\"create_dataplex_task_async\",\n    )\n    # [END howto_dataplex_async_create_task_operator]\n\n    # [START howto_dataplex_delete_task_operator]\n    delete_dataplex_task_async = DataplexDeleteTaskOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        dataplex_task_id=f\"{DATAPLEX_TASK_ID}-1\",\n        task_id=\"delete_dataplex_task_async\",\n    )\n    # [END howto_dataplex_delete_task_operator]\n\n    # [START howto_dataplex_list_tasks_operator]\n    list_dataplex_task = DataplexListTasksOperator(\n        project_id=PROJECT_ID, region=REGION, lake_id=LAKE_ID, task_id=\"list_dataplex_task\"\n    )\n    # [END howto_dataplex_list_tasks_operator]\n\n    # [START howto_dataplex_get_task_operator]\n    get_dataplex_task = DataplexGetTaskOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        dataplex_task_id=DATAPLEX_TASK_ID,\n        task_id=\"get_dataplex_task\",\n    )\n    # [END howto_dataplex_get_task_operator]\n\n    # [START howto_dataplex_task_state_sensor]\n    dataplex_task_state = DataplexTaskStateSensor(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        dataplex_task_id=DATAPLEX_TASK_ID,\n        task_id=\"dataplex_task_state\",\n    )\n    # [END howto_dataplex_task_state_sensor]\n\n    # [START howto_dataplex_delete_task_operator]\n    delete_dataplex_task = DataplexDeleteTaskOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        dataplex_task_id=DATAPLEX_TASK_ID,\n        task_id=\"delete_dataplex_task\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_dataplex_delete_task_operator]\n\n    # [START howto_dataplex_delete_lake_operator]\n    delete_lake = DataplexDeleteLakeOperator(\n        project_id=PROJECT_ID,\n        region=REGION,\n        lake_id=LAKE_ID,\n        task_id=\"delete_lake\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_dataplex_delete_lake_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        upload_file,\n        # TEST BODY\n        create_lake,\n        create_dataplex_task,\n        get_dataplex_task,\n        list_dataplex_task,\n        create_dataplex_task_async,\n        delete_dataplex_task_async,\n        dataplex_task_state,\n        # TEST TEARDOWN\n        delete_dataplex_task,\n        delete_lake,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.148185", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 127, "file_name": "example_dataproc_batch.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\"],\n) as dag:\n    # [START how_to_cloud_dataproc_create_batch_operator]\n    create_batch = DataprocCreateBatchOperator(\n        task_id=\"create_batch\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG,\n        batch_id=BATCH_ID,\n    )\n\n    create_batch_2 = DataprocCreateBatchOperator(\n        task_id=\"create_batch_2\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG,\n        batch_id=BATCH_ID_2,\n        result_retry=Retry(maximum=10.0, initial=10.0, multiplier=1.0),\n    )\n\n    create_batch_3 = DataprocCreateBatchOperator(\n        task_id=\"create_batch_3\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG,\n        batch_id=BATCH_ID_3,\n        asynchronous=True,\n    )\n    # [END how_to_cloud_dataproc_create_batch_operator]\n\n    # [START how_to_cloud_dataproc_batch_async_sensor]\n    batch_async_sensor = DataprocBatchSensor(\n        task_id=\"batch_async_sensor\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        batch_id=BATCH_ID_3,\n        poke_interval=10,\n    )\n    # [END how_to_cloud_dataproc_batch_async_sensor]\n\n    # [START how_to_cloud_dataproc_get_batch_operator]\n    get_batch = DataprocGetBatchOperator(\n        task_id=\"get_batch\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID\n    )\n\n    get_batch_2 = DataprocGetBatchOperator(\n        task_id=\"get_batch_2\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID_2\n    )\n    # [END how_to_cloud_dataproc_get_batch_operator]\n\n    # [START how_to_cloud_dataproc_list_batches_operator]\n    list_batches = DataprocListBatchesOperator(\n        task_id=\"list_batches\",\n        project_id=PROJECT_ID,\n        region=REGION,\n    )\n    # [END how_to_cloud_dataproc_list_batches_operator]\n\n    create_batch_4 = DataprocCreateBatchOperator(\n        task_id=\"create_batch_4\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG,\n        batch_id=BATCH_ID_4,\n        asynchronous=True,\n    )\n\n    # [START how_to_cloud_dataproc_cancel_operation_operator]\n    cancel_operation = DataprocCancelOperationOperator(\n        task_id=\"cancel_operation\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        operation_name=\"{{ task_instance.xcom_pull('create_batch_4') }}\",\n    )\n    # [END how_to_cloud_dataproc_cancel_operation_operator]\n\n    # [START how_to_cloud_dataproc_delete_batch_operator]\n    delete_batch = DataprocDeleteBatchOperator(\n        task_id=\"delete_batch\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID\n    )\n    delete_batch_2 = DataprocDeleteBatchOperator(\n        task_id=\"delete_batch_2\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID_2\n    )\n    delete_batch_3 = DataprocDeleteBatchOperator(\n        task_id=\"delete_batch_3\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID_3\n    )\n    delete_batch_4 = DataprocDeleteBatchOperator(\n        task_id=\"delete_batch_4\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID_4\n    )\n    # [END how_to_cloud_dataproc_delete_batch_operator]\n    delete_batch.trigger_rule = TriggerRule.ALL_DONE\n    delete_batch_2.trigger_rule = TriggerRule.ALL_DONE\n    delete_batch_3.trigger_rule = TriggerRule.ALL_DONE\n    delete_batch_4.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST SETUP\n        [create_batch, create_batch_2, create_batch_3]\n        # TEST BODY\n        >> batch_async_sensor\n        >> [get_batch, get_batch_2, list_batches]\n        >> create_batch_4\n        >> cancel_operation\n        # TEST TEARDOWN\n        >> [delete_batch, delete_batch_2, delete_batch_3, delete_batch_4]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.162942", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 159, "file_name": "example_dataprep.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use Google Dataprep.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataprep import (\n    DataprepCopyFlowOperator,\n    DataprepDeleteFlowOperator,\n    DataprepGetJobGroupOperator,\n    DataprepGetJobsForJobGroupOperator,\n    DataprepRunFlowOperator,\n    DataprepRunJobGroupOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataprep import DataprepJobGroupIsFinishedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_dataprep\"\n\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nGCS_BUCKET_NAME = f\"dataprep-bucket-{DAG_ID}-{ENV_ID}\"\nGCS_BUCKET_PATH = f\"gs://{GCS_BUCKET_NAME}/task_results/\"\n\nFLOW_ID = os.environ.get(\"FLOW_ID\")\nRECIPE_ID = os.environ.get(\"RECIPE_ID\")\nRECIPE_NAME = os.environ.get(\"RECIPE_NAME\")\nWRITE_SETTINGS = (\n    {\n        \"writesettings\": [\n            {\n                \"path\": GCS_BUCKET_PATH,\n                \"action\": \"create\",\n                \"format\": \"csv\",\n            }\n        ],\n    },\n)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    catchup=False,\n    tags=[\"example\", \"dataprep\"],\n    render_template_as_native_obj=True,\n) as dag:\n    create_bucket_task = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET_NAME,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # [START how_to_dataprep_run_job_group_operator]\n    run_job_group_task = DataprepRunJobGroupOperator(\n        task_id=\"run_job_group\",\n        project_id=GCP_PROJECT_ID,\n        body_request={\n            \"wrangledDataset\": {\"id\": RECIPE_ID},\n            \"overrides\": WRITE_SETTINGS,\n        },\n    )\n    # [END how_to_dataprep_run_job_group_operator]\n\n    # [START how_to_dataprep_copy_flow_operator]\n    copy_task = DataprepCopyFlowOperator(\n        task_id=\"copy_flow\",\n        project_id=GCP_PROJECT_ID,\n        flow_id=FLOW_ID,\n        name=f\"dataprep_example_flow_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END how_to_dataprep_copy_flow_operator]\n\n    # [START how_to_dataprep_dataprep_run_flow_operator]\n    run_flow_task = DataprepRunFlowOperator(\n        task_id=\"run_flow\",\n        project_id=GCP_PROJECT_ID,\n        flow_id=\"{{ task_instance.xcom_pull('copy_flow')['id'] }}\",\n        body_request={\n            \"overrides\": {\n                RECIPE_NAME: WRITE_SETTINGS,\n            },\n        },\n    )\n    # [END how_to_dataprep_dataprep_run_flow_operator]\n\n    # [START how_to_dataprep_get_job_group_operator]\n    get_job_group_task = DataprepGetJobGroupOperator(\n        task_id=\"get_job_group\",\n        project_id=GCP_PROJECT_ID,\n        job_group_id=\"{{ task_instance.xcom_pull('run_flow')['data'][0]['id'] }}\",\n        embed=\"\",\n        include_deleted=False,\n    )\n    # [END how_to_dataprep_get_job_group_operator]\n\n    # [START how_to_dataprep_get_jobs_for_job_group_operator]\n    get_jobs_for_job_group_task = DataprepGetJobsForJobGroupOperator(\n        task_id=\"get_jobs_for_job_group\",\n        job_group_id=\"{{ task_instance.xcom_pull('run_flow')['data'][0]['id'] }}\",\n    )\n    # [END how_to_dataprep_get_jobs_for_job_group_operator]\n\n    # [START how_to_dataprep_job_group_finished_sensor]\n    check_flow_status_sensor = DataprepJobGroupIsFinishedSensor(\n        task_id=\"check_flow_status\",\n        job_group_id=\"{{ task_instance.xcom_pull('run_flow')['data'][0]['id'] }}\",\n    )\n    # [END how_to_dataprep_job_group_finished_sensor]\n\n    # [START how_to_dataprep_job_group_finished_sensor]\n    check_job_group_status_sensor = DataprepJobGroupIsFinishedSensor(\n        task_id=\"check_job_group_status\",\n        job_group_id=\"{{ task_instance.xcom_pull('run_job_group')['id'] }}\",\n    )\n    # [END how_to_dataprep_job_group_finished_sensor]\n\n    # [START how_to_dataprep_delete_flow_operator]\n    delete_flow_task = DataprepDeleteFlowOperator(\n        task_id=\"delete_flow\",\n        flow_id=\"{{ task_instance.xcom_pull('copy_flow')['id'] }}\",\n    )\n    # [END how_to_dataprep_delete_flow_operator]\n    delete_flow_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket_task = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_task\n        >> copy_task\n        # TEST BODY\n        >> [run_job_group_task, run_flow_task]\n        >> get_job_group_task\n        >> get_jobs_for_job_group_task\n        # TEST TEARDOWN\n        >> check_flow_status_sensor\n        >> [delete_flow_task, check_job_group_status_sensor]\n        >> delete_bucket_task\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.171844", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 54, "file_name": "example_dataproc_batch_deferrable.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"batch\", \"deferrable\"],\n) as dag:\n    # [START how_to_cloud_dataproc_create_batch_operator_async]\n    create_batch = DataprocCreateBatchOperator(\n        task_id=\"create_batch\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG,\n        batch_id=BATCH_ID,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_create_batch_operator_async]\n\n    get_batch = DataprocGetBatchOperator(\n        task_id=\"get_batch\", project_id=PROJECT_ID, region=REGION, batch_id=BATCH_ID\n    )\n\n    delete_batch = DataprocDeleteBatchOperator(\n        task_id=\"delete_batch\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch_id=BATCH_ID,\n    )\n    delete_batch.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST SETUP\n        create_batch\n        # TEST BODY\n        >> get_batch\n        # TEST TEARDOWN\n        >> delete_batch\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.171974", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 76, "file_name": "example_dataproc_batch_persistent.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n    \"environment_config\": {\n        \"peripherals_config\": {\n            \"spark_history_server_config\": {\n                \"dataproc_cluster\": f\"projects/{PROJECT_ID}/regions/{REGION}/clusters/{CLUSTER_NAME}\"\n            }\n        }\n    },\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"batch\", \"persistent\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START how_to_cloud_dataproc_create_cluster_for_persistent_history_server]\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_for_phs\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_GENERATOR_CONFIG_FOR_PHS,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n    # [END how_to_cloud_dataproc_create_cluster_for_persistent_history_server]\n\n    # [START how_to_cloud_dataproc_create_batch_operator_with_persistent_history_server]\n    create_batch = DataprocCreateBatchOperator(\n        task_id=\"create_batch_with_phs\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        batch=BATCH_CONFIG_WITH_PHS,\n        batch_id=BATCH_ID,\n    )\n    # [END how_to_cloud_dataproc_create_batch_operator_with_persistent_history_server]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_cluster\n        # TEST BODY\n        >> create_batch\n        # TEST TEARDOWN\n        >> delete_cluster\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.177737", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 115, "file_name": "example_dataproc_cluster_generator.py"}, "content": "\"\"\"\nExample Airflow DAG testing Dataproc\noperators for managing a cluster and submitting jobs.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    ClusterGenerator,\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_cluster_generation\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\nZONE = \"europe-west1-b\"\nINIT_FILE_SRC = str(Path(__file__).parent / \"resources\" / \"pip-install.sh\")\n\n# Cluster definition: Generating Cluster Config for DataprocCreateClusterOperator\n# [START how_to_cloud_dataproc_create_cluster_generate_cluster_config]\n\nINIT_FILE = \"pip-install.sh\"\n\nCLUSTER_GENERATOR_CONFIG = ClusterGenerator(\n    project_id=PROJECT_ID,\n    zone=ZONE,\n    master_machine_type=\"n1-standard-4\",\n    worker_machine_type=\"n1-standard-4\",\n    num_workers=2,\n    storage_bucket=BUCKET_NAME,\n    init_actions_uris=[f\"gs://{BUCKET_NAME}/{INIT_FILE}\"],\n    metadata={\"PIP_PACKAGES\": \"pyyaml requests pandas openpyxl\"},\n    num_preemptible_workers=1,\n    preemptibility=\"PREEMPTIBLE\",\n).make()\n\n# [END how_to_cloud_dataproc_create_cluster_generate_cluster_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=INIT_FILE_SRC,\n        dst=INIT_FILE,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START how_to_cloud_dataproc_create_cluster_generate_cluster_config_operator]\n\n    create_dataproc_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_dataproc_cluster\",\n        cluster_name=CLUSTER_NAME,\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_config=CLUSTER_GENERATOR_CONFIG,\n    )\n\n    # [END how_to_cloud_dataproc_create_cluster_generate_cluster_config_operator]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >>\n        # TEST BODY\n        create_dataproc_cluster\n        # TEST TEARDOWN\n        >> [delete_cluster, delete_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.181454", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 123, "file_name": "example_dataproc_cluster_deferrable.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_cluster_def\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 1024,\n        },\n        \"is_preemptible\": True,\n        \"preemptibility\": \"PREEMPTIBLE\",\n    },\n}\n\n# Update options\n# [START how_to_cloud_dataproc_updatemask_cluster_operator]\nCLUSTER_UPDATE = {\n    \"config\": {\"worker_config\": {\"num_instances\": 3}, \"secondary_worker_config\": {\"num_instances\": 3}}\n}\nUPDATE_MASK = {\n    \"paths\": [\"config.worker_config.num_instances\", \"config.secondary_worker_config.num_instances\"]\n}\n# [END how_to_cloud_dataproc_updatemask_cluster_operator]\n\nTIMEOUT = {\"seconds\": 1 * 24 * 60 * 60}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"deferrable\"],\n) as dag:\n    # [START how_to_cloud_dataproc_create_cluster_operator_async]\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_create_cluster_operator_async]\n\n    # [START how_to_cloud_dataproc_update_cluster_operator_async]\n    update_cluster = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        cluster_name=CLUSTER_NAME,\n        cluster=CLUSTER_UPDATE,\n        update_mask=UPDATE_MASK,\n        graceful_decommission_timeout=TIMEOUT,\n        project_id=PROJECT_ID,\n        region=REGION,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_update_cluster_operator_async]\n\n    # [START how_to_cloud_dataproc_delete_cluster_operator_async]\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_delete_cluster_operator_async]\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> update_cluster\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.184052", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 136, "file_name": "example_dataproc_gke.py"}, "content": "\"\"\"\nExample Airflow DAG that show how to create a Dataproc cluster in Google Kubernetes Engine.\n\nRequired environment variables:\nGKE_NAMESPACE = os.environ.get(\"GKE_NAMESPACE\", f\"{CLUSTER_NAME}\")\nA GKE cluster can support multiple DP clusters running in different namespaces.\nDefine a namespace or assign a default one.\nNotice: optional kubernetes_namespace parameter in VIRTUAL_CLUSTER_CONFIG should be the same as GKE_NAMESPACE\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc-gke\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nREGION = \"us-central1\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nGKE_CLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}-gke\".replace(\"_\", \"-\")\nWORKLOAD_POOL = f\"{PROJECT_ID}.svc.id.goog\"\nGKE_CLUSTER_CONFIG = {\n    \"name\": GKE_CLUSTER_NAME,\n    \"workload_identity_config\": {\n        \"workload_pool\": WORKLOAD_POOL,\n    },\n    \"initial_node_count\": 1,\n}\nGKE_NAMESPACE = os.environ.get(\"GKE_NAMESPACE\", f\"{CLUSTER_NAME}\")\n# [START how_to_cloud_dataproc_create_cluster_in_gke_config]\n\nVIRTUAL_CLUSTER_CONFIG = {\n    \"kubernetes_cluster_config\": {\n        \"gke_cluster_config\": {\n            \"gke_cluster_target\": f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{GKE_CLUSTER_NAME}\",\n            \"node_pool_target\": [\n                {\n                    \"node_pool\": f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{GKE_CLUSTER_NAME}/nodePools/dp\",  # noqa\n                    \"roles\": [\"DEFAULT\"],\n                    \"node_pool_config\": {\n                        \"config\": {\n                            \"preemptible\": True,\n                        }\n                    },\n                }\n            ],\n        },\n        \"kubernetes_software_config\": {\"component_version\": {\"SPARK\": b\"3\"}},\n    },\n    \"staging_bucket\": \"test-staging-bucket\",\n}\n\n# [END how_to_cloud_dataproc_create_cluster_in_gke_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"gke\"],\n) as dag:\n    create_gke_cluster = GKECreateClusterOperator(\n        task_id=\"create_gke_cluster\",\n        project_id=PROJECT_ID,\n        location=REGION,\n        body=GKE_CLUSTER_CONFIG,\n    )\n\n    add_iam_policy_binding = BashOperator(\n        task_id=\"add_iam_policy_binding\",\n        bash_command=f\"gcloud projects add-iam-policy-binding {PROJECT_ID} \"\n        f\"--member=serviceAccount:{WORKLOAD_POOL}[{GKE_NAMESPACE}/agent] \"\n        \"--role=roles/iam.workloadIdentityUser\",\n    )\n\n    # [START how_to_cloud_dataproc_create_cluster_operator_in_gke]\n    create_cluster_in_gke = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_in_gke\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        virtual_cluster_config=VIRTUAL_CLUSTER_CONFIG,\n    )\n    # [END how_to_cloud_dataproc_create_cluster_operator_in_gke]\n\n    delete_dataproc_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_dataproc_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gke_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete_gke_cluster\",\n        name=GKE_CLUSTER_NAME,\n        project_id=PROJECT_ID,\n        location=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_gke_cluster\n        >> add_iam_policy_binding\n        # TEST BODY\n        >> create_cluster_in_gke\n        # TEST TEARDOWN\n        >> [delete_gke_cluster, delete_dataproc_cluster]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.204749", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 106, "file_name": "example_dataproc_cluster_update.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_update\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Update options\n# [START how_to_cloud_dataproc_updatemask_cluster_operator]\nCLUSTER_UPDATE = {\n    \"config\": {\"worker_config\": {\"num_instances\": 3}, \"secondary_worker_config\": {\"num_instances\": 3}}\n}\nUPDATE_MASK = {\n    \"paths\": [\"config.worker_config.num_instances\", \"config.secondary_worker_config.num_instances\"]\n}\n# [END how_to_cloud_dataproc_updatemask_cluster_operator]\n\nTIMEOUT = {\"seconds\": 1 * 24 * 60 * 60}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START how_to_cloud_dataproc_update_cluster_operator]\n    scale_cluster = DataprocUpdateClusterOperator(\n        task_id=\"scale_cluster\",\n        cluster_name=CLUSTER_NAME,\n        cluster=CLUSTER_UPDATE,\n        update_mask=UPDATE_MASK,\n        graceful_decommission_timeout=TIMEOUT,\n        project_id=PROJECT_ID,\n        region=REGION,\n    )\n    # [END how_to_cloud_dataproc_update_cluster_operator]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> scale_cluster\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.207159", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 110, "file_name": "example_dataproc_hadoop.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with hadoop job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_hadoop\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\nOUTPUT_FOLDER = \"wordcount\"\nOUTPUT_PATH = f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}/\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 3,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_hadoop_config]\nHADOOP_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"hadoop_job\": {\n        \"main_jar_file_uri\": \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n        \"args\": [\"wordcount\", \"gs://pub/shakespeare/rose.txt\", OUTPUT_PATH],\n    },\n}\n# [END how_to_cloud_dataproc_hadoop_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"hadoop\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    hadoop_task = DataprocSubmitJobOperator(\n        task_id=\"hadoop_task\", job=HADOOP_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_cluster]\n        # TEST BODY\n        >> hadoop_task\n        # TEST TEARDOWN\n        >> [delete_cluster, delete_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.222605", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 95, "file_name": "example_dataproc_pig.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with pig job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dataproc_pig\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_pig_config]\nPIG_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"pig_job\": {\"query_list\": {\"queries\": [\"define sin HiveUDF('sin');\"]}},\n}\n# [END how_to_cloud_dataproc_pig_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"pig\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    pig_task = DataprocSubmitJobOperator(\n        task_id=\"pig_task\", job=PIG_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> pig_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.228996", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 102, "file_name": "example_dataproc_presto.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with presto job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_presto\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\n# [START how_to_cloud_dataproc_create_cluster]\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"software_config\": {\n        \"optional_components\": [\n            \"PRESTO\",\n        ],\n        \"image_version\": \"2.0\",\n    },\n}\n# [END how_to_cloud_dataproc_create_cluster]\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_presto_config]\nPRESTO_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"presto_job\": {\"query_list\": {\"queries\": [\"SHOW CATALOGS\"]}},\n}\n# [END how_to_cloud_dataproc_presto_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"presto\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    presto_task = DataprocSubmitJobOperator(\n        task_id=\"presto_task\", job=PRESTO_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> presto_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.231234", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 111, "file_name": "example_dataproc_hive.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with hive job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_hive\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\n# [START how_to_cloud_dataproc_create_cluster]\n\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 1024,\n        },\n        \"is_preemptible\": True,\n        \"preemptibility\": \"PREEMPTIBLE\",\n    },\n}\n\n# [END how_to_cloud_dataproc_create_cluster]\n\n# [START how_to_cloud_dataproc_hive_config]\nHIVE_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"hive_job\": {\"query_list\": {\"queries\": [\"SHOW DATABASES;\"]}},\n}\n# [END how_to_cloud_dataproc_hive_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"hive\"],\n) as dag:\n    # [START how_to_cloud_dataproc_create_cluster_operator]\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n    # [END how_to_cloud_dataproc_create_cluster_operator]\n\n    hive_task = DataprocSubmitJobOperator(\n        task_id=\"hive_task\", job=HIVE_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    # [START how_to_cloud_dataproc_delete_cluster_operator]\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n    )\n    # [END how_to_cloud_dataproc_delete_cluster_operator]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> hive_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.234212", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 98, "file_name": "example_dataproc_pyspark.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with pyspark job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_pyspark\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nJOB_FILE_URI = \"gs://airflow-system-tests-resources/dataproc/pyspark/dataproc-pyspark-job-pi.py\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_pyspark_config]\nPYSPARK_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"pyspark_job\": {\"main_python_file_uri\": JOB_FILE_URI},\n}\n# [END how_to_cloud_dataproc_pyspark_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"pyspark\"],\n) as dag:\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START how_to_cloud_dataproc_submit_job_to_cluster_operator]\n    pyspark_task = DataprocSubmitJobOperator(\n        task_id=\"pyspark_task\", job=PYSPARK_JOB, region=REGION, project_id=PROJECT_ID\n    )\n    # [END how_to_cloud_dataproc_submit_job_to_cluster_operator]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> pyspark_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.236634", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 52, "file_name": "example_dataproc_spark.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n}\n# [END how_to_cloud_dataproc_spark_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"spark\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    spark_task = DataprocSubmitJobOperator(\n        task_id=\"spark_task\", job=SPARK_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> spark_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.243172", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 52, "file_name": "example_dataproc_spark_deferrable.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n}\n# [END how_to_cloud_dataproc_spark_deferrable_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"spark\", \"deferrable\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    spark_task = DataprocSubmitJobOperator(\n        task_id=\"spark_task\", job=SPARK_JOB, region=REGION, project_id=PROJECT_ID, deferrable=True\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> spark_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.264408", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 62, "file_name": "example_dataproc_spark_async.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    },\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"spark\", \"async\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    # [START cloud_dataproc_async_submit_sensor]\n    spark_task_async = DataprocSubmitJobOperator(\n        task_id=\"spark_task_async\", job=SPARK_JOB, region=REGION, project_id=PROJECT_ID, asynchronous=True\n    )\n\n    spark_task_async_sensor = DataprocJobSensor(\n        task_id=\"spark_task_async_sensor_task\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        dataproc_job_id=spark_task_async.output,\n        poke_interval=10,\n    )\n    # [END cloud_dataproc_async_submit_sensor]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> spark_task_async\n        >> spark_task_async_sensor\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.265944", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 94, "file_name": "example_dataproc_spark_sql.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with spark sql job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_spark_sql\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"dataproc-spark-sql-{ENV_ID}\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_sparksql_config]\nSPARK_SQL_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_sql_job\": {\"query_list\": {\"queries\": [\"SHOW DATABASES;\"]}},\n}\n# [END how_to_cloud_dataproc_sparksql_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    spark_sql_task = DataprocSubmitJobOperator(\n        task_id=\"spark_sql_task\", job=SPARK_SQL_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> spark_sql_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.273899", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 104, "file_name": "example_dataproc_trino.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with trino job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_trino\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\n# [START how_to_cloud_dataproc_create_cluster]\n\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"software_config\": {\n        \"optional_components\": [\n            \"TRINO\",\n        ],\n        \"image_version\": \"2.1\",\n    },\n}\n\n# [END how_to_cloud_dataproc_create_cluster]\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_trino_config]\nTRINO_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"trino_job\": {\"query_list\": {\"queries\": [\"SHOW CATALOGS\"]}},\n}\n# [END how_to_cloud_dataproc_trino_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"trino\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    trino_task = DataprocSubmitJobOperator(\n        task_id=\"trino_task\", job=TRINO_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> trino_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.287994", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 96, "file_name": "example_dataproc_sparkr.py"}, "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with sparkr job.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_sparkr\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nJOB_FILE_URI = \"gs://airflow-system-tests-resources/dataproc/sparkr/dataproc-sparkr-job.r\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_sparkr_config]\nSPARKR_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_r_job\": {\"main_r_file_uri\": JOB_FILE_URI},\n}\n# [END how_to_cloud_dataproc_sparkr_config]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"sparkr\"],\n) as dag:\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    sparkr_task = DataprocSubmitJobOperator(\n        task_id=\"sparkr_task\", job=SPARKR_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> sparkr_task\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.294846", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 100, "file_name": "example_dataproc_workflow_deferrable.py"}, "content": "\"\"\"\nExample Airflow DAG for Dataproc workflow operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateWorkflowTemplateOperator,\n    DataprocInstantiateInlineWorkflowTemplateOperator,\n    DataprocInstantiateWorkflowTemplateOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_workflow_def\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nREGION = \"europe-west1\"\nCLUSTER_NAME = f\"{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\nPIG_JOB = {\"query_list\": {\"queries\": [\"define sin HiveUDF('sin');\"]}}\nWORKFLOW_NAME = \"airflow-dataproc-test\"\nWORKFLOW_TEMPLATE = {\n    \"id\": WORKFLOW_NAME,\n    \"placement\": {\n        \"managed_cluster\": {\n            \"cluster_name\": CLUSTER_NAME,\n            \"config\": CLUSTER_CONFIG,\n        }\n    },\n    \"jobs\": [{\"step_id\": \"pig_job_1\", \"pig_job\": PIG_JOB}],\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"workflow\", \"deferrable\"],\n) as dag:\n    create_workflow_template = DataprocCreateWorkflowTemplateOperator(\n        task_id=\"create_workflow_template\",\n        template=WORKFLOW_TEMPLATE,\n        project_id=PROJECT_ID,\n        region=REGION,\n    )\n\n    # [START how_to_cloud_dataproc_trigger_workflow_template_async]\n    trigger_workflow_async = DataprocInstantiateWorkflowTemplateOperator(\n        task_id=\"trigger_workflow_async\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        template_id=WORKFLOW_NAME,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_trigger_workflow_template_async]\n\n    # [START how_to_cloud_dataproc_instantiate_inline_workflow_template_async]\n    instantiate_inline_workflow_template_async = DataprocInstantiateInlineWorkflowTemplateOperator(\n        task_id=\"instantiate_inline_workflow_template_async\",\n        template=WORKFLOW_TEMPLATE,\n        region=REGION,\n        deferrable=True,\n    )\n    # [END how_to_cloud_dataproc_instantiate_inline_workflow_template_async]\n\n    (\n        # TEST SETUP\n        create_workflow_template\n        # TEST BODY\n        >> trigger_workflow_async\n        # TEST TEARDOWN\n        >> instantiate_inline_workflow_template_async\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.296965", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 182, "file_name": "example_dataproc_metastore.py"}, "content": "\"\"\"\nExample Airflow DAG that show how to use various Dataproc Metastore\noperators to manage a service.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom pathlib import Path\n\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc_metastore import (\n    DataprocMetastoreCreateMetadataImportOperator,\n    DataprocMetastoreCreateServiceOperator,\n    DataprocMetastoreDeleteServiceOperator,\n    DataprocMetastoreExportMetadataOperator,\n    DataprocMetastoreGetServiceOperator,\n    DataprocMetastoreUpdateServiceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dataproc_metastore\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\nSERVICE_ID = f\"{DAG_ID}-service-{ENV_ID}\".replace(\"_\", \"-\")\nMETADATA_IMPORT_ID = f\"{DAG_ID}-metadata-{ENV_ID}\".replace(\"_\", \"-\")\n\nREGION = \"europe-west1\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nTIMEOUT = 2400\nDB_TYPE = \"MYSQL\"\nDESTINATION_GCS_FOLDER = f\"gs://{BUCKET_NAME}/>\"\n\nHIVE_FILE_SRC = str(Path(__file__).parent.parent / \"dataproc\" / \"resources\" / \"hive.sql\")\nHIVE_FILE = \"data/hive.sql\"\nGCS_URI = f\"gs://{BUCKET_NAME}/data/hive.sql\"\n\n# Service definition\n# Docs: https://cloud.google.com/dataproc-metastore/docs/reference/rest/v1/projects.locations.services#Service\n# [START how_to_cloud_dataproc_metastore_create_service]\nSERVICE = {\n    \"name\": \"test-service\",\n}\n# [END how_to_cloud_dataproc_metastore_create_service]\n\n# [START how_to_cloud_dataproc_metastore_create_metadata_import]\nMETADATA_IMPORT = {\n    \"name\": \"test-metadata-import\",\n    \"database_dump\": {\n        \"gcs_uri\": GCS_URI,\n        \"database_type\": DB_TYPE,\n    },\n}\n# [END how_to_cloud_dataproc_metastore_create_metadata_import]\n\n# Update service\n# [START how_to_cloud_dataproc_metastore_update_service]\nSERVICE_TO_UPDATE = {\n    \"labels\": {\n        \"mylocalmachine\": \"mylocalmachine\",\n        \"systemtest\": \"systemtest\",\n    }\n}\nUPDATE_MASK = FieldMask(paths=[\"labels\"])\n# [END how_to_cloud_dataproc_metastore_update_service]\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"metastore\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=HIVE_FILE_SRC,\n        dst=HIVE_FILE,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START how_to_cloud_dataproc_metastore_create_service_operator]\n    create_service = DataprocMetastoreCreateServiceOperator(\n        task_id=\"create_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service=SERVICE,\n        service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_create_service_operator]\n\n    # [START how_to_cloud_dataproc_metastore_get_service_operator]\n    get_service = DataprocMetastoreGetServiceOperator(\n        task_id=\"get_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service_id=SERVICE_ID,\n    )\n    # [END how_to_cloud_dataproc_metastore_get_service_operator]\n\n    # [START how_to_cloud_dataproc_metastore_update_service_operator]\n    update_service = DataprocMetastoreUpdateServiceOperator(\n        task_id=\"update_service\",\n        project_id=PROJECT_ID,\n        service_id=SERVICE_ID,\n        region=REGION,\n        service=SERVICE_TO_UPDATE,\n        update_mask=UPDATE_MASK,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_update_service_operator]\n\n    # [START how_to_cloud_dataproc_metastore_create_metadata_import_operator]\n    import_metadata = DataprocMetastoreCreateMetadataImportOperator(\n        task_id=\"import_metadata\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        service_id=SERVICE_ID,\n        metadata_import=METADATA_IMPORT,\n        metadata_import_id=METADATA_IMPORT_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_create_metadata_import_operator]\n\n    # [START how_to_cloud_dataproc_metastore_export_metadata_operator]\n    export_metadata = DataprocMetastoreExportMetadataOperator(\n        task_id=\"export_metadata\",\n        destination_gcs_folder=DESTINATION_GCS_FOLDER,\n        project_id=PROJECT_ID,\n        region=REGION,\n        service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_export_metadata_operator]\n\n    # [START how_to_cloud_dataproc_metastore_delete_service_operator]\n    delete_service = DataprocMetastoreDeleteServiceOperator(\n        task_id=\"delete_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_delete_service_operator]\n\n    delete_service.trigger_rule = TriggerRule.ALL_DONE\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_bucket\n        >> upload_file\n        >> create_service\n        >> get_service\n        >> update_service\n        >> import_metadata\n        >> export_metadata\n        >> delete_service\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.301499", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 95, "file_name": "example_dataproc_workflow.py"}, "content": "\"\"\"\nExample Airflow DAG for Dataproc workflow operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateWorkflowTemplateOperator,\n    DataprocInstantiateInlineWorkflowTemplateOperator,\n    DataprocInstantiateWorkflowTemplateOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_workflow\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nREGION = \"europe-west1\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n}\nPIG_JOB = {\"query_list\": {\"queries\": [\"define sin HiveUDF('sin');\"]}}\nWORKFLOW_NAME = \"airflow-dataproc-test\"\nWORKFLOW_TEMPLATE = {\n    \"id\": WORKFLOW_NAME,\n    \"placement\": {\n        \"managed_cluster\": {\n            \"cluster_name\": CLUSTER_NAME,\n            \"config\": CLUSTER_CONFIG,\n        }\n    },\n    \"jobs\": [{\"step_id\": \"pig_job_1\", \"pig_job\": PIG_JOB}],\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"workflow\"],\n) as dag:\n    # [START how_to_cloud_dataproc_create_workflow_template]\n    create_workflow_template = DataprocCreateWorkflowTemplateOperator(\n        task_id=\"create_workflow_template\",\n        template=WORKFLOW_TEMPLATE,\n        project_id=PROJECT_ID,\n        region=REGION,\n    )\n    # [END how_to_cloud_dataproc_create_workflow_template]\n\n    # [START how_to_cloud_dataproc_trigger_workflow_template]\n    trigger_workflow = DataprocInstantiateWorkflowTemplateOperator(\n        task_id=\"trigger_workflow\", region=REGION, project_id=PROJECT_ID, template_id=WORKFLOW_NAME\n    )\n    # [END how_to_cloud_dataproc_trigger_workflow_template]\n\n    # [START how_to_cloud_dataproc_instantiate_inline_workflow_template]\n    instantiate_inline_workflow_template = DataprocInstantiateInlineWorkflowTemplateOperator(\n        task_id=\"instantiate_inline_workflow_template\", template=WORKFLOW_TEMPLATE, region=REGION\n    )\n    # [END how_to_cloud_dataproc_instantiate_inline_workflow_template]\n\n    (\n        # TEST SETUP\n        create_workflow_template\n        # TEST BODY\n        >> trigger_workflow\n        # TEST TEARDOWN\n        >> instantiate_inline_workflow_template\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.303210", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 119, "file_name": "example_dataproc_metastore_backup.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies Dataproc Metastore\noperators for managing backups.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataproc_metastore import (\n    DataprocMetastoreCreateBackupOperator,\n    DataprocMetastoreCreateServiceOperator,\n    DataprocMetastoreDeleteBackupOperator,\n    DataprocMetastoreDeleteServiceOperator,\n    DataprocMetastoreListBackupsOperator,\n    DataprocMetastoreRestoreServiceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dataproc_metastore_backup\"\n\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\nSERVICE_ID = f\"{DAG_ID}-service-{ENV_ID}\".replace(\"_\", \"-\")\nBACKUP_ID = f\"{DAG_ID}-backup-{ENV_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\nTIMEOUT = 1200\n# Service definition\nSERVICE = {\n    \"name\": \"test-service\",\n}\n# Backup definition\n# [START how_to_cloud_dataproc_metastore_create_backup]\nBACKUP = {\n    \"name\": \"test-backup\",\n}\n# [END how_to_cloud_dataproc_metastore_create_backup]\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"metastore\"],\n) as dag:\n    create_service = DataprocMetastoreCreateServiceOperator(\n        task_id=\"create_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service=SERVICE,\n        service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n    )\n    # [START how_to_cloud_dataproc_metastore_create_backup_operator]\n    backup_service = DataprocMetastoreCreateBackupOperator(\n        task_id=\"create_backup\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        service_id=SERVICE_ID,\n        backup=BACKUP,\n        backup_id=BACKUP_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_create_backup_operator]\n    # [START how_to_cloud_dataproc_metastore_list_backups_operator]\n    list_backups = DataprocMetastoreListBackupsOperator(\n        task_id=\"list_backups\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        service_id=SERVICE_ID,\n    )\n    # [END how_to_cloud_dataproc_metastore_list_backups_operator]\n    # [START how_to_cloud_dataproc_metastore_delete_backup_operator]\n    delete_backup = DataprocMetastoreDeleteBackupOperator(\n        task_id=\"delete_backup\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        service_id=SERVICE_ID,\n        backup_id=BACKUP_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_delete_backup_operator]\n    delete_backup.trigger_rule = TriggerRule.ALL_DONE\n    # [START how_to_cloud_dataproc_metastore_restore_service_operator]\n    restore_service = DataprocMetastoreRestoreServiceOperator(\n        task_id=\"restore_metastore\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service_id=SERVICE_ID,\n        backup_id=BACKUP_ID,\n        backup_region=REGION,\n        backup_project_id=PROJECT_ID,\n        backup_service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n    )\n    # [END how_to_cloud_dataproc_metastore_restore_service_operator]\n    delete_service = DataprocMetastoreDeleteServiceOperator(\n        task_id=\"delete_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service_id=SERVICE_ID,\n        timeout=TIMEOUT,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    (create_service >> backup_service >> list_backups >> restore_service >> delete_backup >> delete_service)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.328520", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 230, "file_name": "example_dataproc_metastore_hive_partition_sensor.py"}, "content": "\"\"\"\nExample Airflow DAG that show how to check Hive partitions existence\nusing Dataproc Metastore Sensor.\n\nNote that Metastore service must be configured to use gRPC endpoints.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.providers.google.cloud.hooks.gcs import _parse_gcs_url\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.dataproc_metastore import (\n    DataprocMetastoreCreateServiceOperator,\n    DataprocMetastoreDeleteServiceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataproc_metastore import MetastoreHivePartitionSensor\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"hive_partition_sensor\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"demo-project\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"demo-env\")\nREGION = \"us-central1\"\nNETWORK = \"default\"\n\nMETASTORE_SERVICE_ID = f\"metastore-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nMETASTORE_TIMEOUT = 2400\nMETASTORE_SERVICE = {\n    \"name\": METASTORE_SERVICE_ID,\n    \"hive_metastore_config\": {\n        \"endpoint_protocol\": \"GRPC\",\n    },\n    \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n}\nMETASTORE_SERVICE_QFN = f\"projects/{PROJECT_ID}/locations/{REGION}/services/{METASTORE_SERVICE_ID}\"\nDATAPROC_CLUSTER_NAME = f\"cluster-{DAG_ID}\".replace(\"_\", \"-\")\nDATAPROC_CLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-2\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-2\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"metastore_config\": {\n        \"dataproc_metastore_service\": METASTORE_SERVICE_QFN,\n    },\n    \"gce_cluster_config\": {\n        \"service_account_scopes\": [\n            \"https://www.googleapis.com/auth/cloud-platform\",\n        ],\n    },\n}\n\nTABLE_NAME = \"transactions_partitioned\"\nCOLUMN = \"TransactionType\"\nPARTITION_1 = f\"{COLUMN}=credit\".lower()\nPARTITION_2 = f\"{COLUMN}=debit\".lower()\nSOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nSOURCE_DATA_PATH = \"dataproc/hive\"\nSOURCE_DATA_FILE_NAME = \"part-00000.parquet\"\nEXTERNAL_TABLE_BUCKET = \"{{task_instance.xcom_pull(task_ids='get_hive_warehouse_bucket_task', key='bucket')}}\"\nQUERY_CREATE_EXTERNAL_TABLE = f\"\"\"\nCREATE EXTERNAL TABLE IF NOT EXISTS transactions\n(SubmissionDate DATE, TransactionAmount DOUBLE, TransactionType STRING)\nSTORED AS PARQUET\nLOCATION 'gs://{EXTERNAL_TABLE_BUCKET}/{SOURCE_DATA_PATH}';\n\"\"\"\nQUERY_CREATE_PARTITIONED_TABLE = f\"\"\"\nCREATE EXTERNAL TABLE IF NOT EXISTS {TABLE_NAME}\n(SubmissionDate DATE, TransactionAmount DOUBLE)\nPARTITIONED BY ({COLUMN} STRING);\n\"\"\"\nQUERY_COPY_DATA_WITH_PARTITIONS = f\"\"\"\nSET hive.exec.dynamic.partition.mode=nonstrict;\nINSERT INTO TABLE {TABLE_NAME} PARTITION ({COLUMN})\nSELECT SubmissionDate,TransactionAmount,TransactionType FROM transactions;\n\"\"\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"metastore\", \"partition\", \"hive\", \"sensor\"],\n) as dag:\n\n    create_metastore_service = DataprocMetastoreCreateServiceOperator(\n        task_id=\"create_metastore_service\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        service=METASTORE_SERVICE,\n        service_id=METASTORE_SERVICE_ID,\n        timeout=METASTORE_TIMEOUT,\n    )\n\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=DATAPROC_CLUSTER_NAME,\n        project_id=PROJECT_ID,\n        cluster_config=DATAPROC_CLUSTER_CONFIG,\n        region=REGION,\n    )\n\n    @task(task_id=\"get_hive_warehouse_bucket_task\")\n    def get_hive_warehouse_bucket(**kwargs):\n        \"\"\"Returns Hive Metastore Warehouse GCS bucket name.\"\"\"\n        ti = kwargs[\"ti\"]\n        metastore_service: dict = ti.xcom_pull(task_ids=\"create_metastore_service\")\n        config_overrides: dict = metastore_service[\"hive_metastore_config\"][\"config_overrides\"]\n        destination_dir: str = config_overrides[\"hive.metastore.warehouse.dir\"]\n        bucket, _ = _parse_gcs_url(destination_dir)\n        ti.xcom_push(key=\"bucket\", value=bucket)\n\n    get_hive_warehouse_bucket_task = get_hive_warehouse_bucket()\n\n    copy_source_data = GCSToGCSOperator(\n        task_id=\"copy_source_data\",\n        source_bucket=SOURCE_DATA_BUCKET,\n        source_object=f\"{SOURCE_DATA_PATH}/{SOURCE_DATA_FILE_NAME}\",\n        destination_bucket=EXTERNAL_TABLE_BUCKET,\n        destination_object=f\"{SOURCE_DATA_PATH}/{SOURCE_DATA_FILE_NAME}\",\n    )\n\n    create_external_table = DataprocSubmitJobOperator(\n        task_id=\"create_external_table\",\n        job={\n            \"reference\": {\"project_id\": PROJECT_ID},\n            \"placement\": {\"cluster_name\": DATAPROC_CLUSTER_NAME},\n            \"hive_job\": {\"query_list\": {\"queries\": [QUERY_CREATE_EXTERNAL_TABLE]}},\n        },\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    create_partitioned_table = DataprocSubmitJobOperator(\n        task_id=\"create_partitioned_table\",\n        job={\n            \"reference\": {\"project_id\": PROJECT_ID},\n            \"placement\": {\"cluster_name\": DATAPROC_CLUSTER_NAME},\n            \"hive_job\": {\"query_list\": {\"queries\": [QUERY_CREATE_PARTITIONED_TABLE]}},\n        },\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    partition_data = DataprocSubmitJobOperator(\n        task_id=\"partition_data\",\n        job={\n            \"reference\": {\"project_id\": PROJECT_ID},\n            \"placement\": {\"cluster_name\": DATAPROC_CLUSTER_NAME},\n            \"hive_job\": {\"query_list\": {\"queries\": [QUERY_COPY_DATA_WITH_PARTITIONS]}},\n        },\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_cloud_dataproc_metastore_hive_partition_sensor]\n    hive_partition_sensor = MetastoreHivePartitionSensor(\n        task_id=\"hive_partition_sensor\",\n        service_id=METASTORE_SERVICE_ID,\n        region=REGION,\n        table=TABLE_NAME,\n        partitions=[PARTITION_1, PARTITION_2],\n    )\n    # [END how_to_cloud_dataproc_metastore_hive_partition_sensor]\n\n    delete_dataproc_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_dataproc_cluster\",\n        cluster_name=DATAPROC_CLUSTER_NAME,\n        project_id=PROJECT_ID,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_metastore_service = DataprocMetastoreDeleteServiceOperator(\n        task_id=\"delete_metastore_service\",\n        service_id=METASTORE_SERVICE_ID,\n        project_id=PROJECT_ID,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_warehouse_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_warehouse_bucket\",\n        bucket_name=EXTERNAL_TABLE_BUCKET,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # TEST SETUP\n    (\n        create_metastore_service\n        >> create_cluster\n        >> get_hive_warehouse_bucket_task\n        >> copy_source_data\n        >> create_external_table\n        >> create_partitioned_table\n        >> partition_data\n    )\n    (\n        create_metastore_service\n        # TEST BODY\n        >> hive_partition_sensor\n        # TEST TEARDOWN\n        >> [delete_dataproc_cluster, delete_metastore_service, delete_warehouse_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.331710", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 82, "file_name": "example_datastore_commit.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies Datastore commit operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreAllocateIdsOperator,\n    CloudDatastoreBeginTransactionOperator,\n    CloudDatastoreCommitOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_commit\"\n\n# [START how_to_keys_def]\nKEYS = [\n    {\n        \"partitionId\": {\"projectId\": PROJECT_ID, \"namespaceId\": \"\"},\n        \"path\": {\"kind\": \"airflow\"},\n    }\n]\n# [END how_to_keys_def]\n\n# [START how_to_transaction_def]\nTRANSACTION_OPTIONS: dict[str, Any] = {\"readWrite\": {}}\n# [END how_to_transaction_def]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"datastore\", \"example\"],\n) as dag:\n    # [START how_to_allocate_ids]\n    allocate_ids = CloudDatastoreAllocateIdsOperator(\n        task_id=\"allocate_ids\", partial_keys=KEYS, project_id=PROJECT_ID\n    )\n    # [END how_to_allocate_ids]\n\n    # [START how_to_begin_transaction]\n    begin_transaction_commit = CloudDatastoreBeginTransactionOperator(\n        task_id=\"begin_transaction_commit\",\n        transaction_options=TRANSACTION_OPTIONS,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_begin_transaction]\n\n    # [START how_to_commit_def]\n    COMMIT_BODY = {\n        \"mode\": \"TRANSACTIONAL\",\n        \"mutations\": [\n            {\n                \"insert\": {\n                    \"key\": KEYS[0],\n                    \"properties\": {\"string\": {\"stringValue\": \"airflow is awesome!\"}},\n                }\n            }\n        ],\n        \"transaction\": begin_transaction_commit.output,\n    }\n    # [END how_to_commit_def]\n\n    # [START how_to_commit_task]\n    commit_task = CloudDatastoreCommitOperator(task_id=\"commit_task\", body=COMMIT_BODY, project_id=PROJECT_ID)\n    # [END how_to_commit_task]\n\n    allocate_ids >> begin_transaction_commit >> commit_task\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.336100", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 97, "file_name": "example_datastore_export_import.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies Datastore export and import operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreDeleteOperationOperator,\n    CloudDatastoreExportEntitiesOperator,\n    CloudDatastoreGetOperationOperator,\n    CloudDatastoreImportEntitiesOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_export_import\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"datastore\", \"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID, location=\"EU\"\n    )\n\n    # [START how_to_export_task]\n    export_task = CloudDatastoreExportEntitiesOperator(\n        task_id=\"export_task\",\n        bucket=BUCKET_NAME,\n        project_id=PROJECT_ID,\n        overwrite_existing=True,\n    )\n    # [END how_to_export_task]\n\n    # [START how_to_import_task]\n    import_task = CloudDatastoreImportEntitiesOperator(\n        task_id=\"import_task\",\n        bucket=\"{{ task_instance.xcom_pull('export_task')['response']['outputUrl'].split('/')[2] }}\",\n        file=\"{{ '/'.join(task_instance.xcom_pull('export_task')['response']['outputUrl'].split('/')[3:]) }}\",\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_import_task]\n\n    # [START get_operation_state]\n    get_operation = CloudDatastoreGetOperationOperator(\n        task_id=\"get_operation\", name=\"{{ task_instance.xcom_pull('export_task')['name'] }}\"\n    )\n    # [END get_operation_state]\n\n    # [START delete_operation]\n    delete_export_operation = CloudDatastoreDeleteOperationOperator(\n        task_id=\"delete_export_operation\",\n        name=\"{{ task_instance.xcom_pull('export_task')['name'] }}\",\n    )\n    # [END delete_operation]\n    delete_export_operation.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_import_operation = CloudDatastoreDeleteOperationOperator(\n        task_id=\"delete_import_operation\",\n        name=\"{{ task_instance.xcom_pull('export_task')['name'] }}\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        export_task,\n        import_task,\n        get_operation,\n        [delete_bucket, delete_export_operation, delete_import_operation],\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.343343", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 50, "file_name": "example_datastore_rollback.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies Datastore rollback operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreBeginTransactionOperator,\n    CloudDatastoreRollbackOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_rollback\"\n\nTRANSACTION_OPTIONS: dict[str, Any] = {\"readWrite\": {}}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"datastore\", \"example\"],\n) as dag:\n    begin_transaction_to_rollback = CloudDatastoreBeginTransactionOperator(\n        task_id=\"begin_transaction_to_rollback\",\n        transaction_options=TRANSACTION_OPTIONS,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_rollback_transaction]\n    rollback_transaction = CloudDatastoreRollbackOperator(\n        task_id=\"rollback_transaction\",\n        transaction=begin_transaction_to_rollback.output,\n    )\n    # [END how_to_rollback_transaction]\n\n    begin_transaction_to_rollback >> rollback_transaction\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.349756", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 149, "file_name": "example_firestore.py"}, "content": "\"\"\"\nExample Airflow DAG that shows interactions with Google Cloud Firestore.\n\nPrerequisites\n=============\n\nThis example uses two Google Cloud projects:\n\n* ``GCP_PROJECT_ID`` - It contains a bucket and a firestore database.\n* ``G_FIRESTORE_PROJECT_ID`` - it contains the Data Warehouse based on the BigQuery service.\n\nSaving in a bucket should be possible from the ``G_FIRESTORE_PROJECT_ID`` project.\nReading from a bucket should be possible from the ``GCP_PROJECT_ID`` project.\n\nThe bucket and dataset should be located in the same region.\n\nIf you want to run this example, you must do the following:\n\n1. Create Google Cloud project and enable the BigQuery API\n2. Create the Firebase project\n3. Create a bucket in the same location as the Firebase project\n4. Grant Firebase admin account permissions to manage BigQuery. This is required to create a dataset.\n5. Create a bucket in Firebase project and\n6. Give read/write access for Firebase admin to bucket to step no. 5.\n7. Create collection in the Firestore database.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.firebase.operators.firestore import CloudFirestoreExportDatabaseOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcp_firestore\"\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-gcp-project\")\nFIRESTORE_PROJECT_ID = os.environ.get(\"G_FIRESTORE_PROJECT_ID\", \"example-firebase-project\")\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nEXPORT_DESTINATION_URL = os.environ.get(\"GCP_FIRESTORE_ARCHIVE_URL\", f\"gs://{BUCKET_NAME}/namespace/\")\nEXPORT_COLLECTION_ID = os.environ.get(\"GCP_FIRESTORE_COLLECTION_ID\", \"firestore_collection_id\")\nEXTERNAL_TABLE_SOURCE_URI = (\n    f\"{EXPORT_DESTINATION_URL}/all_namespaces/kind_{EXPORT_COLLECTION_ID}\"\n    f\"/all_namespaces_kind_{EXPORT_COLLECTION_ID}.export_metadata\"\n)\nDATASET_LOCATION = os.environ.get(\"GCP_FIRESTORE_DATASET_LOCATION\", \"EU\")\n\nif BUCKET_NAME is None:\n    raise ValueError(\"Bucket name is required. Please set GCP_FIRESTORE_ARCHIVE_URL env variable.\")\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"firestore\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, location=DATASET_LOCATION\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n        location=DATASET_LOCATION,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # [START howto_operator_export_database_to_gcs]\n    export_database_to_gcs = CloudFirestoreExportDatabaseOperator(\n        task_id=\"export_database_to_gcs\",\n        project_id=FIRESTORE_PROJECT_ID,\n        body={\"outputUriPrefix\": EXPORT_DESTINATION_URL, \"collectionIds\": [EXPORT_COLLECTION_ID]},\n    )\n    # [END howto_operator_export_database_to_gcs]\n\n    # [START howto_operator_create_external_table_multiple_types]\n    create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table\",\n        bucket=BUCKET_NAME,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": \"firestore_data\",\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"DATASTORE_BACKUP\",\n                \"compression\": \"NONE\",\n                \"sourceUris\": [EXTERNAL_TABLE_SOURCE_URI],\n            },\n        },\n    )\n    # [END howto_operator_create_external_table_multiple_types]\n\n    read_data_from_gcs_multiple_types = BigQueryInsertJobOperator(\n        task_id=\"execute_query\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.firestore_data`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        project_id=GCP_PROJECT_ID,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        # TEST BODY\n        >> export_database_to_gcs\n        >> create_external_table_multiple_types\n        >> read_data_from_gcs_multiple_types\n        # TEST TEARDOWN\n        >> [delete_dataset, delete_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.356457", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 67, "file_name": "example_datastore_query.py"}, "content": "\"\"\"\nAirflow System Test DAG that verifies Datastore query operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreAllocateIdsOperator,\n    CloudDatastoreBeginTransactionOperator,\n    CloudDatastoreRunQueryOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_query\"\n\nKEYS = [\n    {\n        \"partitionId\": {\"projectId\": PROJECT_ID, \"namespaceId\": \"\"},\n        \"path\": {\"kind\": \"airflow\"},\n    }\n]\n\nTRANSACTION_OPTIONS: dict[str, Any] = {\"readWrite\": {}}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"datastore\", \"example\"],\n) as dag:\n    allocate_ids = CloudDatastoreAllocateIdsOperator(\n        task_id=\"allocate_ids\", partial_keys=KEYS, project_id=PROJECT_ID\n    )\n\n    begin_transaction_query = CloudDatastoreBeginTransactionOperator(\n        task_id=\"begin_transaction_query\",\n        transaction_options=TRANSACTION_OPTIONS,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_query_def]\n    QUERY = {\n        \"partitionId\": {\"projectId\": PROJECT_ID, \"namespaceId\": \"query\"},\n        \"readOptions\": {\"transaction\": begin_transaction_query.output},\n        \"query\": {},\n    }\n    # [END how_to_query_def]\n\n    # [START how_to_run_query]\n    run_query = CloudDatastoreRunQueryOperator(task_id=\"run_query\", body=QUERY, project_id=PROJECT_ID)\n    # [END how_to_run_query]\n\n    allocate_ids >> begin_transaction_query >> run_query\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.358610", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 61, "file_name": "example_calendar_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.calendar_to_gcs import GoogleCalendarToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_calendar_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nCALENDAR_ID = os.environ.get(\"CALENDAR_ID\", \"1234567890qwerty\")\nAPI_VERSION = \"v3\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"calendar\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START upload_calendar_to_gcs]\n    upload_calendar_to_gcs = GoogleCalendarToGCSOperator(\n        task_id=\"upload_calendar_to_gcs\",\n        destination_bucket=BUCKET_NAME,\n        calendar_id=CALENDAR_ID,\n        api_version=API_VERSION,\n    )\n    # [END upload_calendar_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_calendar_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.362693", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 105, "file_name": "example_gcs_acl.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage ACL (Access Control List) operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSBucketCreateAclEntryOperator,\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSObjectCreateAclEntryOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_acl\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\nGCS_ACL_ENTITY = \"allUsers\"\nGCS_ACL_BUCKET_ROLE = \"OWNER\"\nGCS_ACL_OBJECT_ROLE = \"OWNER\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"acl\", \"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n        resource={\n            \"iamConfiguration\": {\n                \"uniformBucketLevelAccess\": {\n                    \"enabled\": False,\n                },\n            },\n        },\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcs_bucket_create_acl_entry_task]\n    gcs_bucket_create_acl_entry_task = GCSBucketCreateAclEntryOperator(\n        bucket=BUCKET_NAME,\n        entity=GCS_ACL_ENTITY,\n        role=GCS_ACL_BUCKET_ROLE,\n        task_id=\"gcs_bucket_create_acl_entry_task\",\n    )\n    # [END howto_operator_gcs_bucket_create_acl_entry_task]\n\n    # [START howto_operator_gcs_object_create_acl_entry_task]\n    gcs_object_create_acl_entry_task = GCSObjectCreateAclEntryOperator(\n        bucket=BUCKET_NAME,\n        object_name=FILE_NAME,\n        entity=GCS_ACL_ENTITY,\n        role=GCS_ACL_OBJECT_ROLE,\n        task_id=\"gcs_object_create_acl_entry_task\",\n    )\n    # [END howto_operator_gcs_object_create_acl_entry_task]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> gcs_bucket_create_acl_entry_task\n        >> gcs_object_create_acl_entry_task\n        # TEST TEARDOWN,\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.383880", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 114, "file_name": "example_gcs_copy_delete.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage operators for listing, copying and deleting\nbucket content.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSDeleteObjectsOperator,\n    GCSListObjectsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_copy_delete\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    # [START howto_operator_gcs_list_bucket]\n    list_buckets = GCSListObjectsOperator(task_id=\"list_buckets\", bucket=BUCKET_NAME_SRC)\n    # [END howto_operator_gcs_list_bucket]\n\n    list_buckets_result = BashOperator(\n        task_id=\"list_buckets_result\",\n        bash_command=f\"echo {list_buckets.output}\",\n    )\n\n    copy_file = GCSToGCSOperator(\n        task_id=\"copy_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_gcs_delete_object]\n    delete_files = GCSDeleteObjectsOperator(\n        task_id=\"delete_files\", bucket_name=BUCKET_NAME_SRC, objects=[FILE_NAME]\n    )\n    # [END howto_operator_gcs_delete_object]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        [create_bucket_src, create_bucket_dst],\n        upload_file,\n        # TEST BODY\n        list_buckets,\n        list_buckets_result,\n        copy_file,\n        delete_files,\n        # TEST TEARDOWN\n        [delete_bucket_src, delete_bucket_dst],\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.387978", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 187, "file_name": "example_gcs_sensor.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage sensors.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.gcs import (\n    GCSObjectExistenceAsyncSensor,\n    GCSObjectExistenceSensor,\n    GCSObjectsWithPrefixExistenceSensor,\n    GCSObjectUpdateSensor,\n    GCSUploadSessionCompleteSensor,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_sensor\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\n\ndef workaround_in_debug_executor(cls):\n    \"\"\"\n    DebugExecutor change sensor mode from poke to reschedule. Some sensors don't work correctly\n    in reschedule mode. They are decorated with `poke_mode_only` decorator to fail when mode is changed.\n    This method creates dummy property to overwrite it and force poke method to always return True.\n    \"\"\"\n    cls.mode = dummy_mode_property()\n    cls.poke = lambda self, ctx: True\n\n\ndef dummy_mode_property():\n    def mode_getter(self):\n        return self._mode\n\n    def mode_setter(self, value):\n        self._mode = value\n\n    return property(mode_getter, mode_setter)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    workaround_in_debug_executor(GCSUploadSessionCompleteSensor)\n\n    # [START howto_sensor_gcs_upload_session_complete_task]\n    gcs_upload_session_complete = GCSUploadSessionCompleteSensor(\n        bucket=BUCKET_NAME,\n        prefix=FILE_NAME,\n        inactivity_period=15,\n        min_objects=1,\n        allow_delete=True,\n        previous_objects=set(),\n        task_id=\"gcs_upload_session_complete_task\",\n    )\n    # [END howto_sensor_gcs_upload_session_complete_task]\n\n    # [START howto_sensor_gcs_upload_session_async_task]\n    gcs_upload_session_async_complete = GCSUploadSessionCompleteSensor(\n        bucket=BUCKET_NAME,\n        prefix=FILE_NAME,\n        inactivity_period=15,\n        min_objects=1,\n        allow_delete=True,\n        previous_objects=set(),\n        task_id=\"gcs_upload_session_async_complete\",\n        deferrable=True,\n    )\n    # [END howto_sensor_gcs_upload_session_async_task]\n\n    # [START howto_sensor_object_update_exists_task]\n    gcs_update_object_exists = GCSObjectUpdateSensor(\n        bucket=BUCKET_NAME,\n        object=FILE_NAME,\n        task_id=\"gcs_object_update_sensor_task\",\n    )\n    # [END howto_sensor_object_update_exists_task]\n\n    # [START howto_sensor_object_update_exists_task_async]\n    gcs_update_object_exists_async = GCSObjectUpdateSensor(\n        bucket=BUCKET_NAME, object=FILE_NAME, task_id=\"gcs_object_update_sensor_task_async\", deferrable=True\n    )\n    # [END howto_sensor_object_update_exists_task_async]\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_sensor_object_exists_task]\n    gcs_object_exists = GCSObjectExistenceSensor(\n        bucket=BUCKET_NAME,\n        object=FILE_NAME,\n        task_id=\"gcs_object_exists_task\",\n    )\n    # [END howto_sensor_object_exists_task]\n\n    # [START howto_sensor_object_exists_task_async]\n    gcs_object_exists_async = GCSObjectExistenceAsyncSensor(\n        bucket=BUCKET_NAME,\n        object=FILE_NAME,\n        task_id=\"gcs_object_exists_task_async\",\n    )\n    # [END howto_sensor_object_exists_task_async]\n\n    # [START howto_sensor_object_exists_task_defered]\n    gcs_object_exists_defered = GCSObjectExistenceSensor(\n        bucket=BUCKET_NAME, object=FILE_NAME, task_id=\"gcs_object_exists_defered\", deferrable=True\n    )\n    # [END howto_sensor_object_exists_task_defered]\n\n    # [START howto_sensor_object_with_prefix_exists_task]\n    gcs_object_with_prefix_exists = GCSObjectsWithPrefixExistenceSensor(\n        bucket=BUCKET_NAME,\n        prefix=FILE_NAME[:5],\n        task_id=\"gcs_object_with_prefix_exists_task\",\n    )\n    # [END howto_sensor_object_with_prefix_exists_task]\n\n    # [START howto_sensor_object_with_prefix_exists_task_async]\n    gcs_object_with_prefix_exists_async = GCSObjectsWithPrefixExistenceSensor(\n        bucket=BUCKET_NAME,\n        prefix=FILE_NAME[:5],\n        task_id=\"gcs_object_with_prefix_exists_task_async\",\n        deferrable=True,\n    )\n    # [END howto_sensor_object_with_prefix_exists_task_async]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        upload_file,\n        # TEST BODY\n        [\n            gcs_object_exists,\n            gcs_object_exists_defered,\n            gcs_object_exists_async,\n            gcs_object_with_prefix_exists,\n        ],\n        # TEST TEARDOWN\n        delete_bucket,\n    )\n    chain(\n        create_bucket,\n        # TEST BODY\n        gcs_upload_session_complete,\n        gcs_update_object_exists,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.391947", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 74, "file_name": "example_gcs_to_bigquery.py"}, "content": "\"\"\"\nExample DAG using GCSToBigQueryOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcs_to_bigquery_operator\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"test\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_test_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_airflow_test_dataset\", dataset_id=DATASET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_gcs_to_bigquery]\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"bigquery/us-states/us-states.csv\"],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    delete_test_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_airflow_test_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_test_dataset\n        # TEST BODY\n        >> load_csv\n        # TEST TEARDOWN\n        >> delete_test_dataset\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.399704", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 166, "file_name": "example_gcs_to_bigquery_async.py"}, "content": "\"\"\"\nExample DAG using GCSToBigQueryOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"gcs_to_bigquery_operator_async\"\n\nDATASET_NAME_STR = f\"dataset_{DAG_ID}_{ENV_ID}_STR\"\nDATASET_NAME_DATE = f\"dataset_{DAG_ID}_{ENV_ID}_DATE\"\nDATASET_NAME_JSON = f\"dataset_{DAG_ID}_{ENV_ID}_JSON\"\nDATASET_NAME_DELIMITER = f\"dataset_{DAG_ID}_{ENV_ID}_DELIMITER\"\nTABLE_NAME_STR = \"test_str\"\nTABLE_NAME_DATE = \"test_date\"\nTABLE_NAME_JSON = \"test_json\"\nTABLE_NAME_DELIMITER = \"test_delimiter\"\nMAX_ID_STR = \"name\"\nMAX_ID_DATE = \"date\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_test_dataset_for_string_fields = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_airflow_test_dataset_str\", dataset_id=DATASET_NAME_STR, project_id=PROJECT_ID\n    )\n\n    create_test_dataset_for_date_fields = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_airflow_test_dataset_date\", dataset_id=DATASET_NAME_DATE, project_id=PROJECT_ID\n    )\n\n    create_test_dataset_for_json_fields = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_airflow_test_dataset_json\", dataset_id=DATASET_NAME_JSON, project_id=PROJECT_ID\n    )\n\n    create_test_dataset_for_delimiter_fields = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_airflow_test_dataset_delimiter\",\n        dataset_id=DATASET_NAME_DELIMITER,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_gcs_to_bigquery_async]\n    load_string_based_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example_str_csv_async\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"bigquery/us-states/us-states.csv\"],\n        destination_project_dataset_table=f\"{DATASET_NAME_STR}.{TABLE_NAME_STR}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        external_table=False,\n        autodetect=True,\n        max_id_key=\"string_field_0\",\n        deferrable=True,\n    )\n\n    load_date_based_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example_date_csv_async\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"bigquery/us-states/us-states-by-date.csv\"],\n        destination_project_dataset_table=f\"{DATASET_NAME_DATE}.{TABLE_NAME_DATE}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        external_table=False,\n        autodetect=True,\n        max_id_key=MAX_ID_DATE,\n        deferrable=True,\n    )\n\n    load_json = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example_date_json_async\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"bigquery/us-states/us-states.json\"],\n        source_format=\"NEWLINE_DELIMITED_JSON\",\n        destination_project_dataset_table=f\"{DATASET_NAME_JSON}.{TABLE_NAME_JSON}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        external_table=False,\n        autodetect=True,\n        max_id_key=MAX_ID_STR,\n        deferrable=True,\n    )\n\n    load_csv_delimiter = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example_delimiter_async\",\n        bucket=\"big-query-samples\",\n        source_objects=[\"employees-tabular.csv\"],\n        source_format=\"csv\",\n        destination_project_dataset_table=f\"{DATASET_NAME_DELIMITER}.{TABLE_NAME_DELIMITER}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        external_table=False,\n        autodetect=True,\n        field_delimiter=\"\\t\",\n        quote_character=\"\",\n        max_id_key=MAX_ID_STR,\n        deferrable=True,\n    )\n    # [END howto_operator_gcs_to_bigquery_async]\n\n    delete_test_dataset_str = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_airflow_test_str_dataset\",\n        dataset_id=DATASET_NAME_STR,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_test_dataset_date = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_airflow_test_date_dataset\",\n        dataset_id=DATASET_NAME_DATE,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_test_dataset_json = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_airflow_test_json_dataset\",\n        dataset_id=DATASET_NAME_JSON,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_test_dataset_delimiter = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_airflow_test_delimiter\",\n        dataset_id=DATASET_NAME_JSON,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_test_dataset_for_string_fields\n        >> create_test_dataset_for_date_fields\n        >> create_test_dataset_for_json_fields\n        >> create_test_dataset_for_delimiter_fields\n        # TEST BODY\n        >> load_string_based_csv\n        >> load_date_based_csv\n        >> load_json\n        >> load_csv_delimiter\n        # TEST TEARDOWN\n        >> delete_test_dataset_str\n        >> delete_test_dataset_date\n        >> delete_test_dataset_json\n        >> delete_test_dataset_delimiter\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.411245", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 123, "file_name": "example_gcs_to_gdrive.py"}, "content": "\"\"\"\nExample DAG using GoogleCloudStorageToGoogleDriveOperator.\n\nUsing this operator requires the following additional scopes:\nhttps://www.googleapis.com/auth/drive\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.suite.transfers.gcs_to_gdrive import GCSToGoogleDriveOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nFOLDER_ID = os.environ.get(\"GCP_GDRIVE_FOLDER_ID\", \"abcd1234\")\n\nDAG_ID = \"example_gcs_to_gdrive\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nTMP_PATH = \"tmp\"\n\nCURRENT_FOLDER = Path(__file__).parent\nLOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\")\n\nFILE_LOCAL_PATH = str(Path(LOCAL_PATH))\nFILE_NAME = \"example_upload.txt\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=f\"{FILE_LOCAL_PATH}/{FILE_NAME}\",\n        dst=f\"{TMP_PATH}/{FILE_NAME}\",\n        bucket=BUCKET_NAME,\n    )\n\n    upload_file_2 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_fil_2\",\n        src=f\"{FILE_LOCAL_PATH}/{FILE_NAME}\",\n        dst=f\"{TMP_PATH}/2_{FILE_NAME}\",\n        bucket=BUCKET_NAME,\n    )\n    # [START howto_operator_gcs_to_gdrive_copy_single_file]\n    copy_single_file = GCSToGoogleDriveOperator(\n        task_id=\"copy_single_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=f\"{TMP_PATH}/{FILE_NAME}\",\n        destination_object=f\"copied_tmp/copied_{FILE_NAME}\",\n    )\n    # [END howto_operator_gcs_to_gdrive_copy_single_file]\n\n    # [START howto_operator_gcs_to_gdrive_copy_single_file_into_folder]\n    copy_single_file_into_folder = GCSToGoogleDriveOperator(\n        task_id=\"copy_single_file_into_folder\",\n        source_bucket=BUCKET_NAME,\n        source_object=f\"{TMP_PATH}/{FILE_NAME}\",\n        destination_object=f\"copied_tmp/copied_{FILE_NAME}\",\n        destination_folder_id=FOLDER_ID,\n    )\n    # [END howto_operator_gcs_to_gdrive_copy_single_file_into_folder]\n\n    # [START howto_operator_gcs_to_gdrive_copy_files]\n    copy_files = GCSToGoogleDriveOperator(\n        task_id=\"copy_files\",\n        source_bucket=BUCKET_NAME,\n        source_object=f\"{TMP_PATH}/*\",\n        destination_object=\"copied_tmp/\",\n    )\n    # [END howto_operator_gcs_to_gdrive_copy_files]\n\n    # [START howto_operator_gcs_to_gdrive_move_files]\n    move_files = GCSToGoogleDriveOperator(\n        task_id=\"move_files\",\n        source_bucket=BUCKET_NAME,\n        source_object=f\"{TMP_PATH}/*.txt\",\n        move_object=True,\n    )\n    # [END howto_operator_gcs_to_gdrive_move_files]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >> upload_file_2\n        # TEST BODY\n        >> copy_single_file\n        >> copy_files\n        >> move_files\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.414106", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 243, "file_name": "example_gcs_to_gcs.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSSynchronizeBucketsOperator and\nGCSToGCSOperator operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_to_gcs\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\nRANDOM_FILE_NAME = OBJECT_1 = OBJECT_2 = \"/tmp/random.bin\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    generate_random_file = BashOperator(\n        task_id=\"generate_random_file\",\n        bash_command=f\"cat /dev/urandom | head -c $((1 * 1024 * 1024)) > {RANDOM_FILE_NAME}\",\n    )\n\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n\n    upload_file_src = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_src\",\n        src=RANDOM_FILE_NAME,\n        dst=RANDOM_FILE_NAME,\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    upload_file_src_sub = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_src_sub\",\n        src=RANDOM_FILE_NAME,\n        dst=f\"subdir/{RANDOM_FILE_NAME}\",\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    upload_file_dst = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_dst\",\n        src=RANDOM_FILE_NAME,\n        dst=RANDOM_FILE_NAME,\n        bucket=BUCKET_NAME_DST,\n    )\n\n    upload_file_dst_sub = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_dst_sub\",\n        src=RANDOM_FILE_NAME,\n        dst=f\"subdir/{RANDOM_FILE_NAME}\",\n        bucket=BUCKET_NAME_DST,\n    )\n\n    # [START howto_synch_bucket]\n    sync_bucket = GCSSynchronizeBucketsOperator(\n        task_id=\"sync_bucket\", source_bucket=BUCKET_NAME_SRC, destination_bucket=BUCKET_NAME_DST\n    )\n    # [END howto_synch_bucket]\n\n    # [START howto_synch_full_bucket]\n    sync_full_bucket = GCSSynchronizeBucketsOperator(\n        task_id=\"sync_full_bucket\",\n        source_bucket=BUCKET_NAME_SRC,\n        destination_bucket=BUCKET_NAME_DST,\n        delete_extra_files=True,\n        allow_overwrite=True,\n    )\n    # [END howto_synch_full_bucket]\n\n    # [START howto_synch_to_subdir]\n    sync_to_subdirectory = GCSSynchronizeBucketsOperator(\n        task_id=\"sync_to_subdirectory\",\n        source_bucket=BUCKET_NAME_SRC,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"subdir/\",\n    )\n    # [END howto_synch_to_subdir]\n\n    # [START howto_sync_from_subdir]\n    sync_from_subdirectory = GCSSynchronizeBucketsOperator(\n        task_id=\"sync_from_subdirectory\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"subdir/\",\n        destination_bucket=BUCKET_NAME_DST,\n    )\n    # [END howto_sync_from_subdir]\n\n    # [START howto_operator_gcs_to_gcs_single_file]\n    copy_single_file = GCSToGCSOperator(\n        task_id=\"copy_single_gcs_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=OBJECT_1,\n        destination_bucket=BUCKET_NAME_DST,  # If not supplied the source_bucket value will be used\n        destination_object=\"backup_\" + OBJECT_1,  # If not supplied the source_object value will be used\n        exact_match=True,\n    )\n    # [END howto_operator_gcs_to_gcs_single_file]\n\n    # [START howto_operator_gcs_to_gcs_wildcard]\n    copy_files_with_wildcard = GCSToGCSOperator(\n        task_id=\"copy_files_with_wildcard\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"data/*.txt\",\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n    )\n    # [END howto_operator_gcs_to_gcs_wildcard]\n\n    # [START howto_operator_gcs_to_gcs_without_wildcard]\n    copy_files_without_wildcard = GCSToGCSOperator(\n        task_id=\"copy_files_without_wildcard\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"subdir/\",\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n    )\n    # [END howto_operator_gcs_to_gcs_without_wildcard]\n\n    # [START howto_operator_gcs_to_gcs_delimiter]\n    copy_files_with_delimiter = GCSToGCSOperator(\n        task_id=\"copy_files_with_delimiter\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"data/\",\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n        delimiter=\".txt\",\n    )\n    # [END howto_operator_gcs_to_gcs_delimiter]\n\n    # [START howto_operator_gcs_to_gcs_match_glob]\n    copy_files_with_match_glob = GCSToGCSOperator(\n        task_id=\"copy_files_with_match_glob\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"data/\",\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n        match_glob=\"**/*.txt\",\n    )\n    # [END howto_operator_gcs_to_gcs_match_glob]\n\n    # [START howto_operator_gcs_to_gcs_list]\n    copy_files_with_list = GCSToGCSOperator(\n        task_id=\"copy_files_with_list\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_objects=[OBJECT_1, OBJECT_2],  # Instead of files each element could be a wildcard expression\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n    )\n    # [END howto_operator_gcs_to_gcs_list]\n\n    # [START howto_operator_gcs_to_gcs_single_file_move]\n    move_single_file = GCSToGCSOperator(\n        task_id=\"move_single_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=OBJECT_1,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup_\" + OBJECT_1,\n        exact_match=True,\n        move_object=True,\n    )\n    # [END howto_operator_gcs_to_gcs_single_file_move]\n\n    # [START howto_operator_gcs_to_gcs_list_move]\n    move_files_with_list = GCSToGCSOperator(\n        task_id=\"move_files_with_list\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_objects=[OBJECT_1, OBJECT_2],\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"backup/\",\n    )\n    # [END howto_operator_gcs_to_gcs_list_move]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        generate_random_file,\n        [create_bucket_src, create_bucket_dst],\n        [upload_file_src, upload_file_src_sub],\n        [upload_file_dst, upload_file_dst_sub],\n        # TEST BODY\n        sync_bucket,\n        sync_full_bucket,\n        sync_to_subdirectory,\n        sync_from_subdirectory,\n        copy_single_file,\n        copy_files_with_wildcard,\n        copy_files_without_wildcard,\n        copy_files_with_delimiter,\n        copy_files_with_match_glob,\n        copy_files_with_list,\n        move_single_file,\n        move_files_with_list,\n        # TEST TEARDOWN\n        [delete_bucket_src, delete_bucket_dst],\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.416733", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 69, "file_name": "example_gcs_to_sheets.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sheets_to_gcs import GoogleSheetsToGCSOperator\nfrom airflow.providers.google.suite.transfers.gcs_to_sheets import GCSToGoogleSheetsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_gcs_to_sheets\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"example-spreadsheetID\")\nNEW_SPREADSHEET_ID = os.environ.get(\"NEW_SPREADSHEET_ID\", \"1234567890qwerty\")\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",  # Override to match your needs\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_sheet_to_gcs = GoogleSheetsToGCSOperator(\n        task_id=\"upload_sheet_to_gcs\",\n        destination_bucket=BUCKET_NAME,\n        spreadsheet_id=SPREADSHEET_ID,\n    )\n\n    # [START upload_gcs_to_sheets]\n    upload_gcs_to_sheet = GCSToGoogleSheetsOperator(\n        task_id=\"upload_gcs_to_sheet\",\n        bucket_name=BUCKET_NAME,\n        object_name=\"{{ task_instance.xcom_pull('upload_sheet_to_gcs')[0] }}\",\n        spreadsheet_id=NEW_SPREADSHEET_ID,\n    )\n    # [END upload_gcs_to_sheets]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_sheet_to_gcs\n        # TEST BODY\n        >> upload_gcs_to_sheet\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.420093", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 85, "file_name": "example_gcs_transform.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSFileTransformOperator operator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSFileTransformOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_transform\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\nTRANSFORM_SCRIPT_PATH = str(Path(__file__).parent / \"resources\" / \"transform_script.py\")\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcs_transform]\n    transform_file = GCSFileTransformOperator(\n        task_id=\"transform_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=FILE_NAME,\n        transform_script=[\"python\", TRANSFORM_SCRIPT_PATH],\n    )\n    # [END howto_operator_gcs_transform]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> transform_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.443050", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 104, "file_name": "example_gcs_transform_timespan.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSTimeSpanFileTransformOperator operator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSTimeSpanFileTransformOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_transform_timespan\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\n\nSOURCE_GCP_CONN_ID = DESTINATION_GCP_CONN_ID = \"google_cloud_default\"\n\nFILE_NAME = \"example_upload.txt\"\nSOURCE_PREFIX = \"timespan_source\"\nDESTINATION_PREFIX = \"timespan_destination\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\nTRANSFORM_SCRIPT_PATH = str(Path(__file__).parent / \"resources\" / \"transform_timespan.py\")\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=f\"{SOURCE_PREFIX}/{FILE_NAME}\",\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    # [START howto_operator_gcs_timespan_file_transform_operator_Task]\n    gcs_timespan_transform_files_task = GCSTimeSpanFileTransformOperator(\n        task_id=\"gcs_timespan_transform_files\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_prefix=SOURCE_PREFIX,\n        source_gcp_conn_id=SOURCE_GCP_CONN_ID,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_prefix=DESTINATION_PREFIX,\n        destination_gcp_conn_id=DESTINATION_GCP_CONN_ID,\n        transform_script=[\"python\", TRANSFORM_SCRIPT_PATH],\n    )\n    # [END howto_operator_gcs_timespan_file_transform_operator_Task]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        [create_bucket_src, create_bucket_dst],\n        upload_file,\n        # TEST BODY\n        gcs_timespan_transform_files_task,\n        # TEST TEARDOWN\n        [delete_bucket_src, delete_bucket_dst],\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.445369", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 85, "file_name": "example_gcs_upload_download.py"}, "content": "\"\"\"\nExample Airflow DAG for testing interaction between Google Cloud Storage and local file system.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalFilesystemOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_upload_download\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_upload.txt\"\nPATH_TO_SAVED_FILE = \"example_upload_download.txt\"\nUPLOAD_FILE_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    # [START howto_operator_gcs_create_bucket]\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcs_create_bucket]\n\n    # [START howto_operator_local_filesystem_to_gcs]\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=UPLOAD_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n    # [END howto_operator_local_filesystem_to_gcs]\n\n    # [START howto_operator_gcs_download_file_task]\n    download_file = GCSToLocalFilesystemOperator(\n        task_id=\"download_file\",\n        object_name=FILE_NAME,\n        bucket=BUCKET_NAME,\n        filename=PATH_TO_SAVED_FILE,\n    )\n    # [END howto_operator_gcs_download_file_task]\n\n    # [START howto_operator_gcs_delete_bucket]\n    delete_bucket = GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n    # [END howto_operator_gcs_delete_bucket]\n    delete_bucket.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> download_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.450608", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 94, "file_name": "example_gdrive_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gdrive_to_gcs import GoogleDriveToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.suite.sensors.drive import GoogleDriveFileExistenceSensor\nfrom airflow.providers.google.suite.transfers.gcs_to_gdrive import GCSToGoogleDriveOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gdrive_to_gcs_with_gdrive_sensor\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nOBJECT = \"abc123xyz\"\nFOLDER_ID = \"\"\nFILE_NAME = \"example_upload.txt\"\nLOCAL_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    copy_single_file = GCSToGoogleDriveOperator(\n        task_id=\"copy_single_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START detect_file]\n    detect_file = GoogleDriveFileExistenceSensor(\n        task_id=\"detect_file\", folder_id=FOLDER_ID, file_name=FILE_NAME\n    )\n    # [END detect_file]\n\n    # [START upload_gdrive_to_gcs]\n    upload_gdrive_to_gcs = GoogleDriveToGCSOperator(\n        task_id=\"upload_gdrive_object_to_gcs\",\n        folder_id=FOLDER_ID,\n        file_name=FILE_NAME,\n        bucket_name=BUCKET_NAME,\n        object_name=OBJECT,\n    )\n    # [END upload_gdrive_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >> copy_single_file\n        # TEST BODY\n        >> detect_file\n        >> upload_gdrive_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.460458", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 73, "file_name": "example_mssql_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\n\ntry:\n    from airflow.providers.google.cloud.transfers.mssql_to_gcs import MSSQLToGCSOperator\nexcept ImportError:\n    pytest.skip(\"MSSQL not available\", allow_module_level=True)\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_mssql_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nFILENAME = \"test_file\"\n\nSQL_QUERY = \"USE airflow SELECT * FROM Country;\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mssql\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_mssql_to_gcs]\n    upload_mssql_to_gcs = MSSQLToGCSOperator(\n        task_id=\"mssql_to_gcs\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=SQL_QUERY,\n        bucket=BUCKET_NAME,\n        filename=FILENAME,\n        export_format=\"csv\",\n    )\n    # [END howto_operator_mssql_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_mssql_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.473159", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 58, "file_name": "example_oracle_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.oracle_to_gcs import OracleToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_oracle_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILENAME = \"test_file\"\nSQL_QUERY = \"SELECT * from test_table\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"oracle\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_oracle_to_gcs]\n    upload_oracle_to_gcs = OracleToGCSOperator(\n        task_id=\"oracle_to_gcs\", sql=SQL_QUERY, bucket=BUCKET_NAME, filename=FILENAME, export_format=\"csv\"\n    )\n    # [END howto_operator_oracle_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_oracle_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.482785", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 92, "file_name": "example_s3_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_s3_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\nGCS_BUCKET_URL = f\"gs://{BUCKET_NAME}/\"\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE = str(Path(__file__).parent / \"resources\" / FILE_NAME)\nPREFIX = \"TESTS\"\n\n\n@task(task_id=\"upload_file_to_s3\")\ndef upload_file():\n    \"\"\"A callable to upload file to AWS bucket\"\"\"\n    s3_hook = S3Hook()\n    s3_hook.load_file(filename=UPLOAD_FILE, key=PREFIX, bucket_name=BUCKET_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"s3\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\", bucket_name=BUCKET_NAME, region_name=\"us-east-1\"\n    )\n\n    create_gcs_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=GCP_PROJECT_ID,\n    )\n    # [START howto_transfer_s3togcs_operator]\n    transfer_to_gcs = S3ToGCSOperator(\n        task_id=\"s3_to_gcs_task\",\n        bucket=BUCKET_NAME,\n        prefix=PREFIX,\n        dest_gcs=GCS_BUCKET_URL,\n        apply_gcs_prefix=True,\n    )\n    # [END howto_transfer_s3togcs_operator]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=BUCKET_NAME,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gcs_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_gcs_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_gcs_bucket\n        >> create_s3_bucket\n        >> upload_file()\n        # TEST BODY\n        >> transfer_to_gcs\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n        >> delete_gcs_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.488187", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 66, "file_name": "example_mysql_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\n\ntry:\n    from airflow.providers.google.cloud.transfers.mysql_to_gcs import MySQLToGCSOperator\nexcept ImportError:\n    pytest.skip(\"MySQL not available\", allow_module_level=True)\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_mysql_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILENAME = \"test_file\"\n\nSQL_QUERY = \"SELECT * from test_table\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"mysql\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_mysql_to_gcs]\n    upload_mysql_to_gcs = MySQLToGCSOperator(\n        task_id=\"mysql_to_gcs\", sql=SQL_QUERY, bucket=BUCKET_NAME, filename=FILENAME, export_format=\"csv\"\n    )\n    # [END howto_operator_mysql_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_mysql_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.490287", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 113, "file_name": "example_sftp_to_gcs.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_sftp_to_gcs\"\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\n\nTMP_PATH = \"tmp\"\nDIR = \"tests_sftp_hook_dir\"\nSUBDIR = \"subdir\"\n\nOBJECT_SRC_1 = \"parent-1.bin\"\nOBJECT_SRC_2 = \"parent-2.bin\"\n\nCURRENT_FOLDER = Path(__file__).parent\nLOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\")\n\nFILE_LOCAL_PATH = str(Path(LOCAL_PATH) / TMP_PATH / DIR)\nFILE_NAME = \"tmp.tar.gz\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_data_file\", bash_command=f\"tar xvf {LOCAL_PATH}/{FILE_NAME} -C {LOCAL_PATH}\"\n    )\n\n    # [START howto_operator_sftp_to_gcs_copy_single_file]\n    copy_file_from_sftp_to_gcs = SFTPToGCSOperator(\n        task_id=\"file-copy-sftp-to-gcs\",\n        source_path=f\"{FILE_LOCAL_PATH}/{OBJECT_SRC_1}\",\n        destination_bucket=BUCKET_NAME,\n    )\n    # [END howto_operator_sftp_to_gcs_copy_single_file]\n\n    # [START howto_operator_sftp_to_gcs_move_single_file_destination]\n    move_file_from_sftp_to_gcs_destination = SFTPToGCSOperator(\n        task_id=\"file-move-sftp-to-gcs-destination\",\n        source_path=f\"{FILE_LOCAL_PATH}/{OBJECT_SRC_2}\",\n        destination_bucket=BUCKET_NAME,\n        destination_path=\"destination_dir/destination_filename.bin\",\n        move_object=True,\n    )\n    # [END howto_operator_sftp_to_gcs_move_single_file_destination]\n\n    # [START howto_operator_sftp_to_gcs_copy_directory]\n    copy_directory_from_sftp_to_gcs = SFTPToGCSOperator(\n        task_id=\"dir-copy-sftp-to-gcs\",\n        source_path=f\"{FILE_LOCAL_PATH}/{SUBDIR}/*\",\n        destination_bucket=BUCKET_NAME,\n    )\n    # [END howto_operator_sftp_to_gcs_copy_directory]\n\n    # [START howto_operator_sftp_to_gcs_move_specific_files]\n    move_specific_files_from_sftp_to_gcs = SFTPToGCSOperator(\n        task_id=\"dir-move-specific-files-sftp-to-gcs\",\n        source_path=f\"{FILE_LOCAL_PATH}/{SUBDIR}/*.bin\",\n        destination_bucket=BUCKET_NAME,\n        destination_path=\"specific_files/\",\n        move_object=True,\n    )\n    # [END howto_operator_sftp_to_gcs_move_specific_files]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        unzip_file,\n        # TEST BODY\n        copy_file_from_sftp_to_gcs,\n        move_file_from_sftp_to_gcs_destination,\n        copy_directory_from_sftp_to_gcs,\n        move_specific_files_from_sftp_to_gcs,\n        # TEST TEARDOWN\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.503978", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 94, "file_name": "example_sheets.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sheets_to_gcs import GoogleSheetsToGCSOperator\nfrom airflow.providers.google.suite.operators.sheets import GoogleSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.suite.transfers.gcs_to_sheets import GCSToGoogleSheetsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_sheets_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"1234567890qwerty\")\nNEW_SPREADSHEET_ID = os.environ.get(\"NEW_SPREADSHEET_ID\", \"1234567890qwerty\")\n\nSPREADSHEET = {\n    \"properties\": {\"title\": \"Test1\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Sheet1\"}}],\n}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sheets\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START upload_sheet_to_gcs]\n    upload_sheet_to_gcs = GoogleSheetsToGCSOperator(\n        task_id=\"upload_sheet_to_gcs\",\n        destination_bucket=BUCKET_NAME,\n        spreadsheet_id=SPREADSHEET_ID,\n    )\n    # [END upload_sheet_to_gcs]\n\n    # [START create_spreadsheet]\n    create_spreadsheet = GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"create_spreadsheet\", spreadsheet=SPREADSHEET\n    )\n    # [END create_spreadsheet]\n\n    # [START print_spreadsheet_url]\n    print_spreadsheet_url = BashOperator(\n        task_id=\"print_spreadsheet_url\",\n        bash_command=f\"echo {XComArg(create_spreadsheet, key='spreadsheet_url')}\",\n    )\n    # [END print_spreadsheet_url]\n\n    # [START upload_gcs_to_sheet]\n    upload_gcs_to_sheet = GCSToGoogleSheetsOperator(\n        task_id=\"upload_gcs_to_sheet\",\n        bucket_name=BUCKET_NAME,\n        object_name=\"{{ task_instance.xcom_pull('upload_sheet_to_gcs')[0] }}\",\n        spreadsheet_id=NEW_SPREADSHEET_ID,\n    )\n    # [END upload_gcs_to_sheet]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> create_spreadsheet\n        >> print_spreadsheet_url\n        >> upload_sheet_to_gcs\n        >> upload_gcs_to_sheet\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.508039", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 87, "file_name": "example_s3_to_gcs_async.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_s3_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nGCS_BUCKET_URL = f\"gs://{BUCKET_NAME}/\"\nUPLOAD_FILE = \"/tmp/example-file.txt\"\nPREFIX = \"TESTS\"\n\n\n@task(task_id=\"upload_file_to_s3\")\ndef upload_file():\n    \"\"\"A callable to upload file to AWS bucket\"\"\"\n    s3_hook = S3Hook()\n    s3_hook.load_file(filename=UPLOAD_FILE, key=PREFIX, bucket_name=BUCKET_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"s3\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket\", bucket_name=BUCKET_NAME, region_name=\"us-east-1\"\n    )\n\n    create_gcs_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    # [START howto_transfer_s3togcs_operator_async]\n    transfer_to_gcs = S3ToGCSOperator(\n        task_id=\"s3_to_gcs_task\", bucket=BUCKET_NAME, prefix=PREFIX, dest_gcs=GCS_BUCKET_URL, deferrable=True\n    )\n    # [END howto_transfer_s3togcs_operator_async]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=BUCKET_NAME,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gcs_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_gcs_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_gcs_bucket\n        >> create_s3_bucket\n        >> upload_file()\n        # TEST BODY\n        >> transfer_to_gcs\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n        >> delete_gcs_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.510755", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 59, "file_name": "example_sheets_to_gcs.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sheets_to_gcs import GoogleSheetsToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_sheets_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"1234567890qwerty\")\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sheets\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START upload_sheet_to_gcs]\n    upload_sheet_to_gcs = GoogleSheetsToGCSOperator(\n        task_id=\"upload_sheet_to_gcs\",\n        destination_bucket=BUCKET_NAME,\n        spreadsheet_id=SPREADSHEET_ID,\n    )\n    # [END upload_sheet_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_sheet_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.514374", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 215, "file_name": "example_trino_to_gcs.py"}, "content": "\"\"\"\nExample DAG using TrinoToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.trino_to_gcs import TrinoToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_to_gcs\"\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\n\nSOURCE_SCHEMA_COLUMNS = \"memory.information_schema.columns\"\nSOURCE_CUSTOMER_TABLE = \"tpch.sf1.customer\"\n\n\ndef safe_name(s: str) -> str:\n    \"\"\"\n    Remove invalid characters for filename\n    \"\"\"\n    return re.sub(\"[^0-9a-zA-Z_]+\", \"_\", s)\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create-dataset\", dataset_id=DATASET_NAME)\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=GCS_BUCKET)\n\n    delete_bucket = GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=GCS_BUCKET)\n\n    # [START howto_operator_trino_to_gcs_basic]\n    trino_to_gcs_basic = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_basic\",\n        sql=f\"select * from {SOURCE_SCHEMA_COLUMNS}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}.{{}}.json\",\n    )\n    # [END howto_operator_trino_to_gcs_basic]\n\n    # [START howto_operator_trino_to_gcs_multiple_types]\n    trino_to_gcs_multiple_types = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_multiple_types\",\n        sql=f\"select * from {SOURCE_SCHEMA_COLUMNS}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}-schema.json\",\n        gzip=False,\n    )\n    # [END howto_operator_trino_to_gcs_multiple_types]\n\n    # [START howto_operator_create_external_table_multiple_types]\n    create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_multiple_types\",\n        bucket=GCS_BUCKET,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"table_catalog\", \"type\": \"STRING\"},\n                    {\"name\": \"table_schema\", \"type\": \"STRING\"},\n                    {\"name\": \"table_name\", \"type\": \"STRING\"},\n                    {\"name\": \"column_name\", \"type\": \"STRING\"},\n                    {\"name\": \"ordinal_position\", \"type\": \"INT64\"},\n                    {\"name\": \"column_default\", \"type\": \"STRING\"},\n                    {\"name\": \"is_nullable\", \"type\": \"STRING\"},\n                    {\"name\": \"data_type\", \"type\": \"STRING\"},\n                ],\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"sourceUris\": [f\"gs://{GCS_BUCKET}/{safe_name(SOURCE_SCHEMA_COLUMNS)}.*.json\"],\n            },\n        },\n        source_objects=[f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}.*.json\"],\n        schema_object=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}-schema.json\",\n    )\n    # [END howto_operator_create_external_table_multiple_types]\n\n    read_data_from_gcs_multiple_types = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_multiple_types\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_operator_trino_to_gcs_many_chunks]\n    trino_to_gcs_many_chunks = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_many_chunks\",\n        sql=f\"select * from {SOURCE_CUSTOMER_TABLE}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n        approx_max_file_size_bytes=10_000_000,\n        gzip=False,\n    )\n    # [END howto_operator_trino_to_gcs_many_chunks]\n\n    create_external_table_many_chunks = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_many_chunks\",\n        bucket=GCS_BUCKET,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_CUSTOMER_TABLE)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"custkey\", \"type\": \"INT64\"},\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"address\", \"type\": \"STRING\"},\n                    {\"name\": \"nationkey\", \"type\": \"INT64\"},\n                    {\"name\": \"phone\", \"type\": \"STRING\"},\n                    {\"name\": \"acctbal\", \"type\": \"FLOAT64\"},\n                    {\"name\": \"mktsegment\", \"type\": \"STRING\"},\n                    {\"name\": \"comment\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"sourceUris\": [f\"gs://{GCS_BUCKET}/{safe_name(SOURCE_CUSTOMER_TABLE)}.*.json\"],\n            },\n        },\n        source_objects=[f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.*.json\"],\n        schema_object=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n    )\n\n    # [START howto_operator_read_data_from_gcs_many_chunks]\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_CUSTOMER_TABLE)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_read_data_from_gcs_many_chunks]\n\n    # [START howto_operator_trino_to_gcs_csv]\n    trino_to_gcs_csv = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_csv\",\n        sql=f\"select * from {SOURCE_SCHEMA_COLUMNS}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}.{{}}.csv\",\n        schema_filename=f\"{safe_name(SOURCE_SCHEMA_COLUMNS)}-schema.json\",\n        export_format=\"csv\",\n    )\n    # [END howto_operator_trino_to_gcs_csv]\n\n    (\n        # TEST SETUP\n        [create_dataset, create_bucket]\n        # TEST BODY\n        >> trino_to_gcs_basic\n        >> trino_to_gcs_multiple_types\n        >> trino_to_gcs_many_chunks\n        >> trino_to_gcs_csv\n        >> create_external_table_multiple_types\n        >> create_external_table_many_chunks\n        >> read_data_from_gcs_multiple_types\n        >> read_data_from_gcs_many_chunks\n        # TEST TEARDOWN\n        >> [delete_dataset, delete_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.532861", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 102, "file_name": "example_kubernetes_engine.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Kubernetes Engine.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n    GKEStartPodOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"kubernetes_engine\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_LOCATION = \"europe-north1-a\"\nCLUSTER_NAME = f\"cluster-name-test-build-{ENV_ID}\"\n\n# [START howto_operator_gcp_gke_create_cluster_definition]\nCLUSTER = {\"name\": CLUSTER_NAME, \"initial_node_count\": 1}\n# [END howto_operator_gcp_gke_create_cluster_definition]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gke_create_cluster]\n    create_cluster = GKECreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        body=CLUSTER,\n    )\n    # [END howto_operator_gke_create_cluster]\n\n    pod_task = GKEStartPodOperator(\n        task_id=\"pod_task\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        namespace=\"default\",\n        image=\"perl\",\n        name=\"test-pod\",\n        in_cluster=False,\n        on_finish_action=\"delete_pod\",\n    )\n\n    # [START howto_operator_gke_start_pod_xcom]\n    pod_task_xcom = GKEStartPodOperator(\n        task_id=\"pod_task_xcom\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        do_xcom_push=True,\n        namespace=\"default\",\n        image=\"alpine\",\n        cmds=[\"sh\", \"-c\", \"mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json\"],\n        name=\"test-pod-xcom\",\n        in_cluster=False,\n        on_finish_action=\"delete_pod\",\n    )\n    # [END howto_operator_gke_start_pod_xcom]\n\n    # [START howto_operator_gke_xcom_result]\n    pod_task_xcom_result = BashOperator(\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('pod_task_xcom')[0] }}\\\"\",\n        task_id=\"pod_task_xcom_result\",\n    )\n    # [END howto_operator_gke_xcom_result]\n\n    # [START howto_operator_gke_delete_cluster]\n    delete_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        name=CLUSTER_NAME,\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n    )\n    # [END howto_operator_gke_delete_cluster]\n\n    create_cluster >> pod_task >> delete_cluster\n    create_cluster >> pod_task_xcom >> delete_cluster\n    pod_task_xcom >> pod_task_xcom_result\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.540237", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 106, "file_name": "example_kubernetes_engine_async.py"}, "content": "\"\"\"\nExample Airflow DAG for asynchronous mode of Google Kubernetes Engine.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n    GKEStartPodOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"kubernetes_engine_async\"\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nGCP_LOCATION = \"europe-north1-a\"\nCLUSTER_NAME = f\"example-cluster-defer-{ENV_ID}\".replace(\"_\", \"-\")\n\nCLUSTER = {\"name\": CLUSTER_NAME, \"initial_node_count\": 1}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gke_create_cluster_async]\n    create_cluster = GKECreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        body=CLUSTER,\n        deferrable=True,\n    )\n    # [END howto_operator_gke_create_cluster_async]\n\n    pod_task = GKEStartPodOperator(\n        task_id=\"pod_task\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        namespace=\"default\",\n        image=\"perl\",\n        name=\"test-pod-async\",\n        in_cluster=False,\n        on_finish_action=\"delete_pod\",\n        get_logs=True,\n        deferrable=True,\n    )\n\n    # [START howto_operator_gke_start_pod_xcom_async]\n    pod_task_xcom_async = GKEStartPodOperator(\n        task_id=\"pod_task_xcom_async\",\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        cluster_name=CLUSTER_NAME,\n        namespace=\"default\",\n        image=\"alpine\",\n        cmds=[\"sh\", \"-c\", \"mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json\"],\n        name=\"test-pod-xcom-async\",\n        in_cluster=False,\n        on_finish_action=\"delete_pod\",\n        do_xcom_push=True,\n        deferrable=True,\n        get_logs=True,\n    )\n    # [END howto_operator_gke_start_pod_xcom_async]\n\n    # [START howto_operator_gke_xcom_result_async]\n    pod_task_xcom_result = BashOperator(\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('pod_task_xcom')[0] }}\\\"\",\n        task_id=\"pod_task_xcom_result\",\n    )\n    # [END howto_operator_gke_xcom_result_async]\n\n    # [START howto_operator_gke_delete_cluster_async]\n    delete_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        name=CLUSTER_NAME,\n        project_id=GCP_PROJECT_ID,\n        location=GCP_LOCATION,\n        deferrable=True,\n    )\n    # [END howto_operator_gke_delete_cluster_async]\n\n    create_cluster >> pod_task >> delete_cluster\n    create_cluster >> pod_task_xcom_async >> delete_cluster\n    pod_task_xcom_async >> pod_task_xcom_result\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.543152", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 125, "file_name": "example_life_sciences.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.life_sciences import LifeSciencesRunPipelineOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_life_sciences\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}-{ENV_ID}\"\n\nFILE_NAME = \"file\"\nLOCATION = \"us-central1\"\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / FILE_NAME)\n\n# [START howto_configure_simple_action_pipeline]\nSIMPLE_ACTION_PIPELINE = {\n    \"pipeline\": {\n        \"actions\": [\n            {\"imageUri\": \"bash\", \"commands\": [\"-c\", \"echo Hello, world\"]},\n        ],\n        \"resources\": {\n            \"regions\": [f\"{LOCATION}\"],\n            \"virtualMachine\": {\n                \"machineType\": \"n1-standard-1\",\n            },\n        },\n    },\n}\n# [END howto_configure_simple_action_pipeline]\n\n# [START howto_configure_multiple_action_pipeline]\nMULTI_ACTION_PIPELINE = {\n    \"pipeline\": {\n        \"actions\": [\n            {\n                \"imageUri\": \"google/cloud-sdk\",\n                \"commands\": [\"gsutil\", \"cp\", f\"gs://{BUCKET_NAME}/{FILE_NAME}\", \"/tmp\"],\n            },\n            {\"imageUri\": \"bash\", \"commands\": [\"-c\", \"echo Hello, world\"]},\n            {\n                \"imageUri\": \"google/cloud-sdk\",\n                \"commands\": [\n                    \"gsutil\",\n                    \"cp\",\n                    f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n                    f\"gs://{BUCKET_NAME}/output.in\",\n                ],\n            },\n        ],\n        \"resources\": {\n            \"regions\": [f\"{LOCATION}\"],\n            \"virtualMachine\": {\n                \"machineType\": \"n1-standard-1\",\n            },\n        },\n    }\n}\n# [END howto_configure_multiple_action_pipeline]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_run_pipeline]\n    simple_life_science_action_pipeline = LifeSciencesRunPipelineOperator(\n        task_id=\"simple-action-pipeline\",\n        body=SIMPLE_ACTION_PIPELINE,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n    )\n    # [END howto_run_pipeline]\n\n    multiple_life_science_action_pipeline = LifeSciencesRunPipelineOperator(\n        task_id=\"multi-action-pipeline\", body=MULTI_ACTION_PIPELINE, project_id=PROJECT_ID, location=LOCATION\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        upload_file,\n        # TEST BODY\n        simple_life_science_action_pipeline,\n        multiple_life_science_action_pipeline,\n        # TEST TEARDOWN\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.546758", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 107, "file_name": "example_natural_language.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Natural Language service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.language_v1 import Document\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.natural_language import (\n    CloudNaturalLanguageAnalyzeEntitiesOperator,\n    CloudNaturalLanguageAnalyzeEntitySentimentOperator,\n    CloudNaturalLanguageAnalyzeSentimentOperator,\n    CloudNaturalLanguageClassifyTextOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcp_natural_language\"\n\n# [START howto_operator_gcp_natural_language_document_text]\nTEXT = \"\"\"Airflow is a platform to programmatically author, schedule and monitor workflows.\n\nUse Airflow to author workflows as Directed Acyclic Graphs (DAGs) of tasks. The Airflow scheduler executes\n your tasks on an array of workers while following the specified dependencies. Rich command line utilities\n make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize\n pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\"\"\"\ndocument = Document(content=TEXT, type=\"PLAIN_TEXT\")\n# [END howto_operator_gcp_natural_language_document_text]\n\n# [START howto_operator_gcp_natural_language_document_gcs]\nGCS_CONTENT_URI = \"gs://INVALID BUCKET NAME/sentiment-me.txt\"\ndocument_gcs = Document(gcs_content_uri=GCS_CONTENT_URI, type=\"PLAIN_TEXT\")\n# [END howto_operator_gcp_natural_language_document_gcs]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcp_natural_language_analyze_entities]\n    analyze_entities = CloudNaturalLanguageAnalyzeEntitiesOperator(\n        document=document, task_id=\"analyze_entities\"\n    )\n    # [END howto_operator_gcp_natural_language_analyze_entities]\n\n    # [START howto_operator_gcp_natural_language_analyze_entities_result]\n    analyze_entities_result = BashOperator(\n        bash_command=f\"echo {analyze_entities.output}\",\n        task_id=\"analyze_entities_result\",\n    )\n    # [END howto_operator_gcp_natural_language_analyze_entities_result]\n\n    # [START howto_operator_gcp_natural_language_analyze_entity_sentiment]\n    analyze_entity_sentiment = CloudNaturalLanguageAnalyzeEntitySentimentOperator(\n        document=document, task_id=\"analyze_entity_sentiment\"\n    )\n    # [END howto_operator_gcp_natural_language_analyze_entity_sentiment]\n\n    # [START howto_operator_gcp_natural_language_analyze_entity_sentiment_result]\n    analyze_entity_sentiment_result = BashOperator(\n        bash_command=f\"echo {analyze_entity_sentiment.output}\",\n        task_id=\"analyze_entity_sentiment_result\",\n    )\n    # [END howto_operator_gcp_natural_language_analyze_entity_sentiment_result]\n\n    # [START howto_operator_gcp_natural_language_analyze_sentiment]\n    analyze_sentiment = CloudNaturalLanguageAnalyzeSentimentOperator(\n        document=document, task_id=\"analyze_sentiment\"\n    )\n    # [END howto_operator_gcp_natural_language_analyze_sentiment]\n\n    # [START howto_operator_gcp_natural_language_analyze_sentiment_result]\n    analyze_sentiment_result = BashOperator(\n        bash_command=f\"echo {analyze_sentiment.output}\",\n        task_id=\"analyze_sentiment_result\",\n    )\n    # [END howto_operator_gcp_natural_language_analyze_sentiment_result]\n\n    # [START howto_operator_gcp_natural_language_analyze_classify_text]\n    analyze_classify_text = CloudNaturalLanguageClassifyTextOperator(\n        document=document, task_id=\"analyze_classify_text\"\n    )\n    # [END howto_operator_gcp_natural_language_analyze_classify_text]\n\n    # [START howto_operator_gcp_natural_language_analyze_classify_text_result]\n    analyze_classify_text_result = BashOperator(\n        bash_command=f\"echo {analyze_classify_text.output}\",\n        task_id=\"analyze_classify_text_result\",\n    )\n    # [END howto_operator_gcp_natural_language_analyze_classify_text_result]\n\n    analyze_entities >> analyze_entities_result\n    analyze_entity_sentiment >> analyze_entity_sentiment_result\n    analyze_sentiment >> analyze_sentiment_result\n    analyze_classify_text >> analyze_classify_text_result\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.566404", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 311, "file_name": "example_mlengine.py"}, "content": "\"\"\"\nExample Airflow DAG for Google ML Engine service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport pathlib\nfrom datetime import datetime\nfrom math import ceil\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.mlengine import (\n    MLEngineCreateModelOperator,\n    MLEngineCreateVersionOperator,\n    MLEngineDeleteModelOperator,\n    MLEngineDeleteVersionOperator,\n    MLEngineGetModelOperator,\n    MLEngineListVersionsOperator,\n    MLEngineSetDefaultVersionOperator,\n    MLEngineStartBatchPredictionJobOperator,\n    MLEngineStartTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.cloud.utils import mlengine_operator_utils\nfrom airflow.utils.trigger_rule import TriggerRule\n\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\nDAG_ID = \"example_gcp_mlengine\"\nPREDICT_FILE_NAME = \"predict.json\"\nMODEL_NAME = f\"model_{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\nBUCKET_PATH = f\"gs://{BUCKET_NAME}\"\nJOB_DIR = f\"{BUCKET_PATH}/job-dir\"\nSAVED_MODEL_PATH = f\"{JOB_DIR}/\"\nPREDICTION_INPUT = f\"{BUCKET_PATH}/{PREDICT_FILE_NAME}\"\nPREDICTION_OUTPUT = f\"{BUCKET_PATH}/prediction_output/\"\nTRAINER_URI = \"gs://airflow-system-tests-resources/ml-engine/trainer-0.2.tar.gz\"\nTRAINER_PY_MODULE = \"trainer.task\"\nSUMMARY_TMP = f\"{BUCKET_PATH}/tmp/\"\nSUMMARY_STAGING = f\"{BUCKET_PATH}/staging/\"\n\nBASE_DIR = pathlib.Path(__file__).parent.resolve()\nPATH_TO_PREDICT_FILE = BASE_DIR / PREDICT_FILE_NAME\n\n\ndef generate_model_predict_input_data() -> list[int]:\n    return [1, 4, 9, 16, 25, 36]\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"ml_engine\"],\n    params={\"model_name\": MODEL_NAME},\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create-bucket\",\n        bucket_name=BUCKET_NAME,\n    )\n\n    @task(task_id=\"write-predict-data-file\")\n    def write_predict_file(path_to_file: str):\n        predict_data = generate_model_predict_input_data()\n        with open(path_to_file, \"w\") as file:\n            for predict_value in predict_data:\n                file.write(f'{{\"input_layer\": [{predict_value}]}}\\n')\n\n    write_data = write_predict_file(path_to_file=PATH_TO_PREDICT_FILE)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload-predict-file\",\n        src=[PATH_TO_PREDICT_FILE],\n        dst=PREDICT_FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcp_mlengine_training]\n    training = MLEngineStartTrainingJobOperator(\n        task_id=\"training\",\n        project_id=PROJECT_ID,\n        region=\"us-central1\",\n        job_id=\"training-job-{{ ts_nodash }}-{{ params.model_name }}\",\n        runtime_version=\"1.15\",\n        python_version=\"3.7\",\n        job_dir=JOB_DIR,\n        package_uris=[TRAINER_URI],\n        training_python_module=TRAINER_PY_MODULE,\n        training_args=[],\n        labels={\"job_type\": \"training\"},\n    )\n    # [END howto_operator_gcp_mlengine_training]\n\n    # [START howto_operator_gcp_mlengine_create_model]\n    create_model = MLEngineCreateModelOperator(\n        task_id=\"create-model\",\n        project_id=PROJECT_ID,\n        model={\n            \"name\": MODEL_NAME,\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_model]\n\n    # [START howto_operator_gcp_mlengine_get_model]\n    get_model = MLEngineGetModelOperator(\n        task_id=\"get-model\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n    )\n    # [END howto_operator_gcp_mlengine_get_model]\n\n    # [START howto_operator_gcp_mlengine_print_model]\n    get_model_result = BashOperator(\n        bash_command=f\"echo {get_model.output}\",\n        task_id=\"get-model-result\",\n    )\n    # [END howto_operator_gcp_mlengine_print_model]\n\n    # [START howto_operator_gcp_mlengine_create_version1]\n    create_version_v1 = MLEngineCreateVersionOperator(\n        task_id=\"create-version-v1\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version={\n            \"name\": \"v1\",\n            \"description\": \"First-version\",\n            \"deployment_uri\": JOB_DIR,\n            \"runtime_version\": \"1.15\",\n            \"machineType\": \"mls1-c1-m2\",\n            \"framework\": \"TENSORFLOW\",\n            \"pythonVersion\": \"3.7\",\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_version1]\n\n    # [START howto_operator_gcp_mlengine_create_version2]\n    create_version_v2 = MLEngineCreateVersionOperator(\n        task_id=\"create-version-v2\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version={\n            \"name\": \"v2\",\n            \"description\": \"Second version\",\n            \"deployment_uri\": JOB_DIR,\n            \"runtime_version\": \"1.15\",\n            \"machineType\": \"mls1-c1-m2\",\n            \"framework\": \"TENSORFLOW\",\n            \"pythonVersion\": \"3.7\",\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_version2]\n\n    # [START howto_operator_gcp_mlengine_default_version]\n    set_defaults_version = MLEngineSetDefaultVersionOperator(\n        task_id=\"set-default-version\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v2\",\n    )\n    # [END howto_operator_gcp_mlengine_default_version]\n\n    # [START howto_operator_gcp_mlengine_list_versions]\n    list_version = MLEngineListVersionsOperator(\n        task_id=\"list-version\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n    )\n    # [END howto_operator_gcp_mlengine_list_versions]\n\n    # [START howto_operator_gcp_mlengine_print_versions]\n    list_version_result = BashOperator(\n        bash_command=f\"echo {list_version.output}\",\n        task_id=\"list-version-result\",\n    )\n    # [END howto_operator_gcp_mlengine_print_versions]\n\n    # [START howto_operator_gcp_mlengine_get_prediction]\n    prediction = MLEngineStartBatchPredictionJobOperator(\n        task_id=\"prediction\",\n        project_id=PROJECT_ID,\n        job_id=\"prediction-{{ ts_nodash }}-{{ params.model_name }}\",\n        region=\"us-central1\",\n        model_name=MODEL_NAME,\n        data_format=\"TEXT\",\n        input_paths=[PREDICTION_INPUT],\n        output_path=PREDICTION_OUTPUT,\n        labels={\"job_type\": \"prediction\"},\n    )\n    # [END howto_operator_gcp_mlengine_get_prediction]\n\n    # [START howto_operator_gcp_mlengine_delete_version]\n    delete_version_v1 = MLEngineDeleteVersionOperator(\n        task_id=\"delete-version-v1\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v1\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_mlengine_delete_version]\n\n    delete_version_v2 = MLEngineDeleteVersionOperator(\n        task_id=\"delete-version-v2\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_gcp_mlengine_delete_model]\n    delete_model = MLEngineDeleteModelOperator(\n        task_id=\"delete-model\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_mlengine_delete_model]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete-bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_gcp_mlengine_get_metric]\n    def get_metric_fn_and_keys():\n        \"\"\"\n        Gets metric function and keys used to generate summary\n        \"\"\"\n\n        def normalize_value(inst: dict):\n            val = int(inst[\"output_layer\"][0])\n            return tuple([val])  # returns a tuple.\n\n        return normalize_value, [\"val\"]  # key order must match.\n\n    # [END howto_operator_gcp_mlengine_get_metric]\n\n    # [START howto_operator_gcp_mlengine_validate_error]\n    def validate_err_and_count(summary: dict) -> dict:\n        \"\"\"\n        Validate summary result\n        \"\"\"\n        summary = summary.get(\"val\", 0)\n        initial_values = generate_model_predict_input_data()\n        initial_summary = sum(initial_values) / len(initial_values)\n\n        multiplier = ceil(summary / initial_summary)\n        if multiplier != 2:\n            raise ValueError(f\"Multiplier is not equal 2; multiplier: {multiplier}\")\n        return summary\n\n    # [END howto_operator_gcp_mlengine_validate_error]\n\n    # [START howto_operator_gcp_mlengine_evaluate]\n    evaluate_prediction, evaluate_summary, evaluate_validation = mlengine_operator_utils.create_evaluate_ops(\n        task_prefix=\"evaluate-ops\",\n        data_format=\"TEXT\",\n        input_paths=[PREDICTION_INPUT],\n        prediction_path=PREDICTION_OUTPUT,\n        metric_fn_and_keys=get_metric_fn_and_keys(),\n        validate_fn=validate_err_and_count,\n        batch_prediction_job_id=\"evaluate-ops-{{ ts_nodash }}-{{ params.model_name }}\",\n        project_id=PROJECT_ID,\n        region=\"us-central1\",\n        dataflow_options={\n            \"project\": PROJECT_ID,\n            \"tempLocation\": SUMMARY_TMP,\n            \"stagingLocation\": SUMMARY_STAGING,\n        },\n        model_name=MODEL_NAME,\n        version_name=\"v1\",\n        py_interpreter=\"python3\",\n    )\n    # [END howto_operator_gcp_mlengine_evaluate]\n\n    # TEST SETUP\n    create_bucket >> write_data >> upload_file\n    upload_file >> [prediction, evaluate_prediction]\n    create_bucket >> training >> create_version_v1\n\n    # TEST BODY\n    create_model >> get_model >> [get_model_result, delete_model]\n    create_model >> create_version_v1 >> create_version_v2 >> set_defaults_version >> list_version\n\n    create_version_v1 >> prediction\n    create_version_v1 >> evaluate_prediction\n    create_version_v2 >> prediction\n\n    list_version >> [list_version_result, delete_version_v1]\n    prediction >> delete_version_v1\n\n    # TEST TEARDOWN\n    evaluate_validation >> delete_version_v1 >> delete_version_v2 >> delete_model >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.568289", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 141, "file_name": "example_pubsub.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google PubSub services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.pubsub import (\n    PubSubCreateSubscriptionOperator,\n    PubSubCreateTopicOperator,\n    PubSubDeleteSubscriptionOperator,\n    PubSubDeleteTopicOperator,\n    PubSubPublishMessageOperator,\n    PubSubPullOperator,\n)\nfrom airflow.providers.google.cloud.sensors.pubsub import PubSubPullSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"pubsub\"\n\nTOPIC_ID = f\"topic-{DAG_ID}-{ENV_ID}\"\nMESSAGE = {\"data\": b\"Tool\", \"attributes\": {\"name\": \"wrench\", \"mass\": \"1.3kg\", \"count\": \"3\"}}\nMESSAGE_TWO = {\"data\": b\"Tool\", \"attributes\": {\"name\": \"wrench\", \"mass\": \"1.2kg\", \"count\": \"2\"}}\n\n# [START howto_operator_gcp_pubsub_pull_messages_result_cmd]\necho_cmd = \"\"\"\n{% for m in task_instance.xcom_pull('pull_messages') %}\n    echo \"AckID: {{ m.get('ackId') }}, Base64-Encoded: {{ m.get('message') }}\"\n{% endfor %}\n\"\"\"\n# [END howto_operator_gcp_pubsub_pull_messages_result_cmd]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_gcp_pubsub_create_topic]\n    create_topic = PubSubCreateTopicOperator(\n        task_id=\"create_topic\", topic=TOPIC_ID, project_id=PROJECT_ID, fail_if_exists=False\n    )\n    # [END howto_operator_gcp_pubsub_create_topic]\n\n    # [START howto_operator_gcp_pubsub_create_subscription]\n    subscribe_task = PubSubCreateSubscriptionOperator(\n        task_id=\"subscribe_task\", project_id=PROJECT_ID, topic=TOPIC_ID\n    )\n    # [END howto_operator_gcp_pubsub_create_subscription]\n\n    # [START howto_operator_gcp_pubsub_pull_message_with_sensor]\n    subscription = subscribe_task.output\n\n    pull_messages = PubSubPullSensor(\n        task_id=\"pull_messages\",\n        ack_messages=True,\n        project_id=PROJECT_ID,\n        subscription=subscription,\n    )\n    # [END howto_operator_gcp_pubsub_pull_message_with_sensor]\n\n    # [START howto_operator_gcp_pubsub_pull_messages_result]\n    pull_messages_result = BashOperator(task_id=\"pull_messages_result\", bash_command=echo_cmd)\n    # [END howto_operator_gcp_pubsub_pull_messages_result]\n\n    # [START howto_operator_gcp_pubsub_pull_message_with_operator]\n\n    pull_messages_operator = PubSubPullOperator(\n        task_id=\"pull_messages_operator\",\n        ack_messages=True,\n        project_id=PROJECT_ID,\n        subscription=subscription,\n    )\n    # [END howto_operator_gcp_pubsub_pull_message_with_operator]\n\n    # [START howto_operator_gcp_pubsub_publish]\n    publish_task = PubSubPublishMessageOperator(\n        task_id=\"publish_task\",\n        project_id=PROJECT_ID,\n        topic=TOPIC_ID,\n        messages=[MESSAGE, MESSAGE],\n    )\n    # [END howto_operator_gcp_pubsub_publish]\n\n    publish_task2 = PubSubPublishMessageOperator(\n        task_id=\"publish_task2\",\n        project_id=PROJECT_ID,\n        topic=TOPIC_ID,\n        messages=[MESSAGE_TWO, MESSAGE_TWO],\n    )\n\n    # [START howto_operator_gcp_pubsub_unsubscribe]\n    unsubscribe_task = PubSubDeleteSubscriptionOperator(\n        task_id=\"unsubscribe_task\",\n        project_id=PROJECT_ID,\n        subscription=subscription,\n    )\n    # [END howto_operator_gcp_pubsub_unsubscribe]\n    unsubscribe_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_pubsub_delete_topic]\n    delete_topic = PubSubDeleteTopicOperator(task_id=\"delete_topic\", topic=TOPIC_ID, project_id=PROJECT_ID)\n    # [END howto_operator_gcp_pubsub_delete_topic]\n    delete_topic.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        create_topic\n        >> subscribe_task\n        >> publish_task\n        >> pull_messages\n        >> pull_messages_result\n        >> publish_task2\n        >> pull_messages_operator\n        >> unsubscribe_task\n        >> delete_topic\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   subscribe_task >> pull_messages\n    #   subscribe_task >> pull_messages_operator\n    #   subscribe_task >> unsubscribe_task\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.571667", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 312, "file_name": "example_mlengine_async.py"}, "content": "\"\"\"\nExample Airflow DAG for Google ML Engine service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport pathlib\nfrom datetime import datetime\nfrom math import ceil\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.mlengine import (\n    MLEngineCreateModelOperator,\n    MLEngineCreateVersionOperator,\n    MLEngineDeleteModelOperator,\n    MLEngineDeleteVersionOperator,\n    MLEngineGetModelOperator,\n    MLEngineListVersionsOperator,\n    MLEngineSetDefaultVersionOperator,\n    MLEngineStartBatchPredictionJobOperator,\n    MLEngineStartTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.cloud.utils import mlengine_operator_utils\nfrom airflow.utils.trigger_rule import TriggerRule\n\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\nDAG_ID = \"async_example_gcp_mlengine\"\nPREDICT_FILE_NAME = \"async_predict.json\"\nMODEL_NAME = f\"model_{DAG_ID}_{ENV_ID}\".replace(\"-\", \"_\")\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\nBUCKET_PATH = f\"gs://{BUCKET_NAME}\"\nJOB_DIR = f\"{BUCKET_PATH}/job-dir\"\nSAVED_MODEL_PATH = f\"{JOB_DIR}/\"\nPREDICTION_INPUT = f\"{BUCKET_PATH}/{PREDICT_FILE_NAME}\"\nPREDICTION_OUTPUT = f\"{BUCKET_PATH}/prediction_output/\"\nTRAINER_URI = \"gs://airflow-system-tests-resources/ml-engine/async-trainer-0.2.tar.gz\"\nTRAINER_PY_MODULE = \"trainer.task\"\nSUMMARY_TMP = f\"{BUCKET_PATH}/tmp/\"\nSUMMARY_STAGING = f\"{BUCKET_PATH}/staging/\"\n\nBASE_DIR = pathlib.Path(__file__).parent.resolve()\nPATH_TO_PREDICT_FILE = BASE_DIR / PREDICT_FILE_NAME\n\n\ndef generate_model_predict_input_data() -> list[int]:\n    return [1, 4, 9, 16, 25, 36]\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"ml_engine\", \"deferrable\"],\n    params={\"model_name\": MODEL_NAME},\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create-bucket\",\n        bucket_name=BUCKET_NAME,\n    )\n\n    @task(task_id=\"write-predict-data-file\")\n    def write_predict_file(path_to_file: str):\n        predict_data = generate_model_predict_input_data()\n        with open(path_to_file, \"w\") as file:\n            for predict_value in predict_data:\n                file.write(f'{{\"input_layer\": [{predict_value}]}}\\n')\n\n    write_data = write_predict_file(path_to_file=PATH_TO_PREDICT_FILE)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload-predict-file\",\n        src=[PATH_TO_PREDICT_FILE],\n        dst=PREDICT_FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcp_mlengine_training_async]\n    training = MLEngineStartTrainingJobOperator(\n        task_id=\"training\",\n        project_id=PROJECT_ID,\n        region=\"us-central1\",\n        job_id=\"async_training-job-{{ ts_nodash }}-{{ params.model_name }}\",\n        runtime_version=\"1.15\",\n        python_version=\"3.7\",\n        job_dir=JOB_DIR,\n        package_uris=[TRAINER_URI],\n        training_python_module=TRAINER_PY_MODULE,\n        training_args=[],\n        labels={\"job_type\": \"training\"},\n        deferrable=True,\n    )\n    # [END howto_operator_gcp_mlengine_training_async]\n\n    # [START howto_operator_gcp_mlengine_create_model]\n    create_model = MLEngineCreateModelOperator(\n        task_id=\"create-model\",\n        project_id=PROJECT_ID,\n        model={\n            \"name\": MODEL_NAME,\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_model]\n\n    # [START howto_operator_gcp_mlengine_get_model]\n    get_model = MLEngineGetModelOperator(\n        task_id=\"get-model\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n    )\n    # [END howto_operator_gcp_mlengine_get_model]\n\n    # [START howto_operator_gcp_mlengine_print_model]\n    get_model_result = BashOperator(\n        bash_command=f\"echo {get_model.output}\",\n        task_id=\"get-model-result\",\n    )\n    # [END howto_operator_gcp_mlengine_print_model]\n\n    # [START howto_operator_gcp_mlengine_create_version1]\n    create_version_v1 = MLEngineCreateVersionOperator(\n        task_id=\"create-version-v1\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version={\n            \"name\": \"v1\",\n            \"description\": \"First-version\",\n            \"deployment_uri\": JOB_DIR,\n            \"runtime_version\": \"1.15\",\n            \"machineType\": \"mls1-c1-m2\",\n            \"framework\": \"TENSORFLOW\",\n            \"pythonVersion\": \"3.7\",\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_version1]\n\n    # [START howto_operator_gcp_mlengine_create_version2]\n    create_version_v2 = MLEngineCreateVersionOperator(\n        task_id=\"create-version-v2\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version={\n            \"name\": \"v2\",\n            \"description\": \"Second version\",\n            \"deployment_uri\": JOB_DIR,\n            \"runtime_version\": \"1.15\",\n            \"machineType\": \"mls1-c1-m2\",\n            \"framework\": \"TENSORFLOW\",\n            \"pythonVersion\": \"3.7\",\n        },\n    )\n    # [END howto_operator_gcp_mlengine_create_version2]\n\n    # [START howto_operator_gcp_mlengine_default_version]\n    set_defaults_version = MLEngineSetDefaultVersionOperator(\n        task_id=\"set-default-version\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v2\",\n    )\n    # [END howto_operator_gcp_mlengine_default_version]\n\n    # [START howto_operator_gcp_mlengine_list_versions]\n    list_version = MLEngineListVersionsOperator(\n        task_id=\"list-version\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n    )\n    # [END howto_operator_gcp_mlengine_list_versions]\n\n    # [START howto_operator_gcp_mlengine_print_versions]\n    list_version_result = BashOperator(\n        bash_command=f\"echo {list_version.output}\",\n        task_id=\"list-version-result\",\n    )\n    # [END howto_operator_gcp_mlengine_print_versions]\n\n    # [START howto_operator_gcp_mlengine_get_prediction]\n    prediction = MLEngineStartBatchPredictionJobOperator(\n        task_id=\"prediction\",\n        project_id=PROJECT_ID,\n        job_id=\"async-prediction-{{ ts_nodash }}-{{ params.model_name }}\",\n        region=\"us-central1\",\n        model_name=MODEL_NAME,\n        data_format=\"TEXT\",\n        input_paths=[PREDICTION_INPUT],\n        output_path=PREDICTION_OUTPUT,\n        labels={\"job_type\": \"prediction\"},\n    )\n    # [END howto_operator_gcp_mlengine_get_prediction]\n\n    # [START howto_operator_gcp_mlengine_delete_version]\n    delete_version_v1 = MLEngineDeleteVersionOperator(\n        task_id=\"delete-version-v1\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v1\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_mlengine_delete_version]\n\n    delete_version_v2 = MLEngineDeleteVersionOperator(\n        task_id=\"delete-version-v2\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        version_name=\"v2\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_gcp_mlengine_delete_model]\n    delete_model = MLEngineDeleteModelOperator(\n        task_id=\"delete-model\",\n        project_id=PROJECT_ID,\n        model_name=MODEL_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_mlengine_delete_model]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete-bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_gcp_mlengine_get_metric]\n    def get_metric_fn_and_keys():\n        \"\"\"\n        Gets metric function and keys used to generate summary\n        \"\"\"\n\n        def normalize_value(inst: dict):\n            val = int(inst[\"output_layer\"][0])\n            return tuple([val])  # returns a tuple.\n\n        return normalize_value, [\"val\"]  # key order must match.\n\n    # [END howto_operator_gcp_mlengine_get_metric]\n\n    # [START howto_operator_gcp_mlengine_validate_error]\n    def validate_err_and_count(summary: dict) -> dict:\n        \"\"\"\n        Validate summary result\n        \"\"\"\n        summary = summary.get(\"val\", 0)\n        initial_values = generate_model_predict_input_data()\n        initial_summary = sum(initial_values) / len(initial_values)\n\n        multiplier = ceil(summary / initial_summary)\n        if multiplier != 2:\n            raise ValueError(f\"Multiplier is not equal 2; multiplier: {multiplier}\")\n        return summary\n\n    # [END howto_operator_gcp_mlengine_validate_error]\n\n    # [START howto_operator_gcp_mlengine_evaluate]\n    evaluate_prediction, evaluate_summary, evaluate_validation = mlengine_operator_utils.create_evaluate_ops(\n        task_prefix=\"evaluate-ops\",\n        data_format=\"TEXT\",\n        input_paths=[PREDICTION_INPUT],\n        prediction_path=PREDICTION_OUTPUT,\n        metric_fn_and_keys=get_metric_fn_and_keys(),\n        validate_fn=validate_err_and_count,\n        batch_prediction_job_id=\"async-evaluate-ops-{{ ts_nodash }}-{{ params.model_name }}\",\n        project_id=PROJECT_ID,\n        region=\"us-central1\",\n        dataflow_options={\n            \"project\": PROJECT_ID,\n            \"tempLocation\": SUMMARY_TMP,\n            \"stagingLocation\": SUMMARY_STAGING,\n        },\n        model_name=MODEL_NAME,\n        version_name=\"v1\",\n        py_interpreter=\"python3\",\n    )\n    # [END howto_operator_gcp_mlengine_evaluate]\n\n    # TEST SETUP\n    create_bucket >> write_data >> upload_file\n    upload_file >> [prediction, evaluate_prediction]\n    create_bucket >> training >> create_version_v1\n\n    # TEST BODY\n    create_model >> get_model >> [get_model_result, delete_model]\n    create_model >> create_version_v1 >> create_version_v2 >> set_defaults_version >> list_version\n\n    create_version_v1 >> prediction\n    create_version_v1 >> evaluate_prediction\n    create_version_v2 >> prediction\n\n    list_version >> [list_version_result, delete_version_v1]\n    prediction >> delete_version_v1\n\n    # TEST TEARDOWN\n    evaluate_validation >> delete_version_v1 >> delete_version_v2 >> delete_model >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.575898", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 96, "file_name": "example_pubsub_deferrable.py"}, "content": "\"\"\"\nExample Airflow DAG that uses Google PubSub services.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.pubsub import (\n    PubSubCreateSubscriptionOperator,\n    PubSubCreateTopicOperator,\n    PubSubDeleteSubscriptionOperator,\n    PubSubDeleteTopicOperator,\n    PubSubPublishMessageOperator,\n)\nfrom airflow.providers.google.cloud.sensors.pubsub import PubSubPullSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"pubsub_async\"\n\nTOPIC_ID = f\"topic-{DAG_ID}-{ENV_ID}\"\nMESSAGE = {\"data\": b\"Tool\", \"attributes\": {\"name\": \"wrench\", \"mass\": \"1.3kg\", \"count\": \"3\"}}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_topic = PubSubCreateTopicOperator(\n        task_id=\"create_topic\", topic=TOPIC_ID, project_id=PROJECT_ID, fail_if_exists=False\n    )\n\n    subscribe_task = PubSubCreateSubscriptionOperator(\n        task_id=\"subscribe_task\", project_id=PROJECT_ID, topic=TOPIC_ID\n    )\n\n    publish_task = PubSubPublishMessageOperator(\n        task_id=\"publish_task\",\n        project_id=PROJECT_ID,\n        topic=TOPIC_ID,\n        messages=[MESSAGE, MESSAGE],\n    )\n    subscription = subscribe_task.output\n\n    # [START howto_operator_gcp_pubsub_pull_message_with_async_sensor]\n    pull_messages_async = PubSubPullSensor(\n        task_id=\"pull_messages_async\",\n        ack_messages=True,\n        project_id=PROJECT_ID,\n        subscription=subscription,\n        deferrable=True,\n    )\n    # [END howto_operator_gcp_pubsub_pull_message_with_async_sensor]\n\n    unsubscribe_task = PubSubDeleteSubscriptionOperator(\n        task_id=\"unsubscribe_task\",\n        project_id=PROJECT_ID,\n        subscription=subscription,\n    )\n    unsubscribe_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_topic = PubSubDeleteTopicOperator(task_id=\"delete_topic\", topic=TOPIC_ID, project_id=PROJECT_ID)\n    delete_topic.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        create_topic\n        >> subscribe_task\n        >> publish_task\n        >> pull_messages_async\n        >> unsubscribe_task\n        >> delete_topic\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   subscribe_task >> pull_messages_async\n    #   subscribe_task >> unsubscribe_task\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.597766", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 154, "file_name": "example_spanner.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, updates, queries and deletes a Cloud Spanner instance.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.spanner import (\n    SpannerDeleteDatabaseInstanceOperator,\n    SpannerDeleteInstanceOperator,\n    SpannerDeployDatabaseInstanceOperator,\n    SpannerDeployInstanceOperator,\n    SpannerQueryDatabaseInstanceOperator,\n    SpannerUpdateDatabaseInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"spanner\"\n\nGCP_SPANNER_INSTANCE_ID = f\"instance-{DAG_ID}-{ENV_ID}\"\nGCP_SPANNER_DATABASE_ID = f\"database-{DAG_ID}-{ENV_ID}\"\nGCP_SPANNER_CONFIG_NAME = f\"projects/{PROJECT_ID}/instanceConfigs/regional-europe-west3\"\nGCP_SPANNER_NODE_COUNT = 1\nGCP_SPANNER_DISPLAY_NAME = \"InstanceSpanner\"\n# OPERATION_ID should be unique per operation\nOPERATION_ID = \"unique_operation_id\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"spanner\"],\n) as dag:\n    # Create\n    # [START howto_operator_spanner_deploy]\n    spanner_instance_create_task = SpannerDeployInstanceOperator(\n        project_id=PROJECT_ID,\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        configuration_name=GCP_SPANNER_CONFIG_NAME,\n        node_count=GCP_SPANNER_NODE_COUNT,\n        display_name=GCP_SPANNER_DISPLAY_NAME,\n        task_id=\"spanner_instance_create_task\",\n    )\n    spanner_instance_update_task = SpannerDeployInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        configuration_name=GCP_SPANNER_CONFIG_NAME,\n        node_count=GCP_SPANNER_NODE_COUNT + 1,\n        display_name=GCP_SPANNER_DISPLAY_NAME + \"_updated\",\n        task_id=\"spanner_instance_update_task\",\n    )\n    # [END howto_operator_spanner_deploy]\n\n    # [START howto_operator_spanner_database_deploy]\n    spanner_database_deploy_task = SpannerDeployDatabaseInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        ddl_statements=[\n            \"CREATE TABLE my_table1 (id INT64, name STRING(MAX)) PRIMARY KEY (id)\",\n            \"CREATE TABLE my_table2 (id INT64, name STRING(MAX)) PRIMARY KEY (id)\",\n        ],\n        task_id=\"spanner_database_deploy_task\",\n    )\n    # [END howto_operator_spanner_database_deploy]\n\n    # [START howto_operator_spanner_database_update]\n    spanner_database_update_task = SpannerUpdateDatabaseInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        ddl_statements=[\n            \"CREATE TABLE my_table3 (id INT64, name STRING(MAX)) PRIMARY KEY (id)\",\n        ],\n        task_id=\"spanner_database_update_task\",\n    )\n    # [END howto_operator_spanner_database_update]\n\n    # [START howto_operator_spanner_database_update_idempotent]\n    spanner_database_update_idempotent1_task = SpannerUpdateDatabaseInstanceOperator(\n        project_id=PROJECT_ID,\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        operation_id=OPERATION_ID,\n        ddl_statements=[\n            \"CREATE TABLE my_table_unique (id INT64, name STRING(MAX)) PRIMARY KEY (id)\",\n        ],\n        task_id=\"spanner_database_update_idempotent1_task\",\n    )\n    spanner_database_update_idempotent2_task = SpannerUpdateDatabaseInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        operation_id=OPERATION_ID,\n        ddl_statements=[\n            \"CREATE TABLE my_table_unique (id INT64, name STRING(MAX)) PRIMARY KEY (id)\",\n        ],\n        task_id=\"spanner_database_update_idempotent2_task\",\n    )\n    # [END howto_operator_spanner_database_update_idempotent]\n\n    # [START howto_operator_spanner_query]\n    spanner_instance_query_task = SpannerQueryDatabaseInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        query=[\"DELETE FROM my_table2 WHERE true\"],\n        task_id=\"spanner_instance_query_task\",\n    )\n    # [END howto_operator_spanner_query]\n\n    # [START howto_operator_spanner_database_delete]\n    spanner_database_delete_task = SpannerDeleteDatabaseInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID,\n        database_id=GCP_SPANNER_DATABASE_ID,\n        task_id=\"spanner_database_delete_task\",\n    )\n    # [END howto_operator_spanner_database_delete]\n    spanner_database_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_spanner_delete]\n    spanner_instance_delete_task = SpannerDeleteInstanceOperator(\n        instance_id=GCP_SPANNER_INSTANCE_ID, task_id=\"spanner_instance_delete_task\"\n    )\n    # [END howto_operator_spanner_delete]\n    spanner_instance_delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        spanner_instance_create_task\n        >> spanner_instance_update_task\n        >> spanner_database_deploy_task\n        >> spanner_database_update_task\n        >> spanner_database_update_idempotent1_task\n        >> spanner_database_update_idempotent2_task\n        >> spanner_instance_query_task\n        >> spanner_database_delete_task\n        >> spanner_instance_delete_task\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.604503", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 81, "file_name": "example_speech_to_text.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.speech_to_text import CloudSpeechToTextRecognizeSpeechOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import CloudTextToSpeechSynthesizeOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"speech_to_text\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_speech_to_text_gcp_filename]\nFILE_NAME = f\"test-audio-file-{DAG_ID}-{ENV_ID}\"\n# [END howto_operator_speech_to_text_gcp_filename]\n\n# [START howto_operator_text_to_speech_api_arguments]\nINPUT = {\"text\": \"Sample text for demo purposes\"}\nVOICE = {\"language_code\": \"en-US\", \"ssml_gender\": \"FEMALE\"}\nAUDIO_CONFIG = {\"audio_encoding\": \"LINEAR16\"}\n# [END howto_operator_text_to_speech_api_arguments]\n\n# [START howto_operator_speech_to_text_api_arguments]\nCONFIG = {\"encoding\": \"LINEAR16\", \"language_code\": \"en_US\"}\nAUDIO = {\"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\"}\n# [END howto_operator_speech_to_text_api_arguments]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"speech_to_text\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    text_to_speech_synthesize_task = CloudTextToSpeechSynthesizeOperator(\n        project_id=PROJECT_ID,\n        input_data=INPUT,\n        voice=VOICE,\n        audio_config=AUDIO_CONFIG,\n        target_bucket_name=BUCKET_NAME,\n        target_filename=FILE_NAME,\n        task_id=\"text_to_speech_synthesize_task\",\n    )\n    # [START howto_operator_speech_to_text_recognize]\n    speech_to_text_recognize_task = CloudSpeechToTextRecognizeSpeechOperator(\n        config=CONFIG, audio=AUDIO, task_id=\"speech_to_text_recognize_task\"\n    )\n    # [END howto_operator_speech_to_text_recognize]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> text_to_speech_synthesize_task\n        >> speech_to_text_recognize_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.606658", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 85, "file_name": "example_sql_to_sheets.py"}, "content": "\"\"\"\nRequired environment variables:\n```\nDB_CONNECTION = os.environ.get(\"DB_CONNECTION\")\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"test-id\")\n```\n\nFirst, you need a db instance that is accessible from the Airflow environment.\nYou can, for example, create a Cloud SQL instance and connect to it from\nwithin breeze with Cloud SQL proxy:\nhttps://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy\n\n# DB setup\nCreate db:\n```\nCREATE DATABASE test_db;\n```\n\nSwitch to db:\n```\n\\c test_db\n```\n\nCreate table and insert some rows\n```\nCREATE TABLE test_table (col1 INT, col2 INT);\nINSERT INTO test_table VALUES (1,2), (3,4), (5,6), (7,8);\n```\n\n# Setup connections\ndb connection:\nIn airflow UI, set one db connection, for example \"postgres_default\"\nand make sure the \"Test\" at the bottom succeeds\n\ngoogle cloud connection:\nWe need additional scopes for this test\nscopes: https://www.googleapis.com/auth/spreadsheets, https://www.googleapis.com/auth/cloud-platform\n\n# Sheet\nFinally, you need a Google Sheet you have access to, for testing you can\ncreate a public sheet and get its ID.\n\n# Tear Down\nYou can delete the db with\n```\nDROP DATABASE test_db;\n```\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.suite.transfers.sql_to_sheets import SQLToGoogleSheetsOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDB_CONNECTION = os.environ.get(\"DB_CONNECTION\")\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"test-id\")\n\nDAG_ID = \"example_sql_to_sheets\"\nSQL = \"select col2 from test_table\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",  # Override to match your needs\n    catchup=False,\n    tags=[\"example\", \"sql\"],\n) as dag:\n    # [START upload_sql_to_sheets]\n    upload_gcs_to_sheet = SQLToGoogleSheetsOperator(\n        task_id=\"upload_sql_to_sheet\",\n        sql=SQL,\n        sql_conn_id=DB_CONNECTION,\n        database=\"test_db\",\n        spreadsheet_id=SPREADSHEET_ID,\n    )\n    # [END upload_sql_to_sheets]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.613241", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 109, "file_name": "example_cloud_storage_transfer_service_gcp_to_gcs.py"}, "content": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Cloud Transfer.\n\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud Project to use for the Google Cloud Transfer Service.\n* GCP_TRANSFER_FIRST_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied from AWS.\n  It is also a source bucket in next step\n* GCP_TRANSFER_SECOND_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceGCSToGCSOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_transfer_gcs_to_gcs\"\n\nBUCKET_NAME_SRC = f\"src-bucket-{DAG_ID}-{ENV_ID}\"\nBUCKET_NAME_DST = f\"dst-bucket-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"file\"\nFILE_URI = f\"gs://{BUCKET_NAME_SRC}/{FILE_NAME}\"\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / FILE_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"transfer\", \"gcs\"],\n) as dag:\n\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_transfer_gcs_to_gcs]\n    transfer_gcs_to_gcs = CloudDataTransferServiceGCSToGCSOperator(\n        task_id=\"transfer_gcs_to_gcs\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_path=FILE_URI,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_path=FILE_URI,\n        wait=True,\n    )\n    # [END howto_operator_transfer_gcs_to_gcs]\n\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # TEST SETUP\n    create_bucket_src >> upload_file\n    [create_bucket_dst, upload_file] >> transfer_gcs_to_gcs\n    (\n        [create_bucket_dst, create_bucket_src >> upload_file]\n        # TEST BODY\n        >> transfer_gcs_to_gcs\n        # TEST TEARDOWN\n        >> [delete_bucket_src, delete_bucket_dst]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.625164", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 220, "file_name": "example_stackdriver.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Stackdriver service.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.stackdriver import (\n    StackdriverDeleteAlertOperator,\n    StackdriverDeleteNotificationChannelOperator,\n    StackdriverDisableAlertPoliciesOperator,\n    StackdriverDisableNotificationChannelsOperator,\n    StackdriverEnableAlertPoliciesOperator,\n    StackdriverEnableNotificationChannelsOperator,\n    StackdriverListAlertPoliciesOperator,\n    StackdriverListNotificationChannelsOperator,\n    StackdriverUpsertAlertOperator,\n    StackdriverUpsertNotificationChannelOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"stackdriver\"\n\nALERT_1_NAME = f\"alert_{ENV_ID}_1\"\nALERT_2_NAME = f\"alert_{ENV_ID}_2\"\nCHANNEL_1_NAME = f\"channel_{ENV_ID}_1\"\nCHANNEL_2_NAME = f\"channel_{ENV_ID}_2\"\n\nTEST_ALERT_POLICY_1 = {\n    \"combiner\": \"OR\",\n    \"enabled\": True,\n    \"display_name\": ALERT_1_NAME,\n    \"conditions\": [\n        {\n            \"condition_threshold\": {\n                \"filter\": (\n                    'metric.label.state=\"blocked\" AND '\n                    'metric.type=\"agent.googleapis.com/processes/count_by_state\" '\n                    'AND resource.type=\"gce_instance\"'\n                ),\n                \"comparison\": \"COMPARISON_GT\",\n                \"threshold_value\": 100,\n                \"duration\": {\"seconds\": 900},\n                \"trigger\": {\"percent\": 0},\n                \"aggregations\": [\n                    {\n                        \"alignment_period\": {\"seconds\": 60},\n                        \"per_series_aligner\": \"ALIGN_MEAN\",\n                        \"cross_series_reducer\": \"REDUCE_MEAN\",\n                        \"group_by_fields\": [\"project\", \"resource.label.instance_id\", \"resource.label.zone\"],\n                    }\n                ],\n            },\n            \"display_name\": f\"{ALERT_1_NAME}_policy_1\",\n        }\n    ],\n}\n\nTEST_ALERT_POLICY_2 = {\n    \"combiner\": \"OR\",\n    \"enabled\": False,\n    \"display_name\": ALERT_2_NAME,\n    \"conditions\": [\n        {\n            \"condition_threshold\": {\n                \"filter\": (\n                    'metric.label.state=\"blocked\" AND '\n                    'metric.type=\"agent.googleapis.com/processes/count_by_state\" AND '\n                    'resource.type=\"gce_instance\"'\n                ),\n                \"comparison\": \"COMPARISON_GT\",\n                \"threshold_value\": 100,\n                \"duration\": {\"seconds\": 900},\n                \"trigger\": {\"percent\": 0},\n                \"aggregations\": [\n                    {\n                        \"alignment_period\": {\"seconds\": 60},\n                        \"per_series_aligner\": \"ALIGN_MEAN\",\n                        \"cross_series_reducer\": \"REDUCE_MEAN\",\n                        \"group_by_fields\": [\"project\", \"resource.label.instance_id\", \"resource.label.zone\"],\n                    }\n                ],\n            },\n            \"display_name\": f\"{ALERT_2_NAME}_policy_2\",\n        }\n    ],\n}\n\nTEST_NOTIFICATION_CHANNEL_1 = {\n    \"display_name\": CHANNEL_1_NAME,\n    \"enabled\": True,\n    \"labels\": {\"topic\": f\"projects/{PROJECT_ID}/topics/notificationTopic\"},\n    \"type\": \"pubsub\",\n}\n\nTEST_NOTIFICATION_CHANNEL_2 = {\n    \"display_name\": CHANNEL_2_NAME,\n    \"enabled\": False,\n    \"labels\": {\"topic\": f\"projects/{PROJECT_ID}/topics/notificationTopic2\"},\n    \"type\": \"pubsub\",\n}\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"stackdriver\"],\n) as dag:\n    # [START howto_operator_gcp_stackdriver_upsert_notification_channel]\n    create_notification_channel = StackdriverUpsertNotificationChannelOperator(\n        task_id=\"create-notification-channel\",\n        channels=json.dumps({\"channels\": [TEST_NOTIFICATION_CHANNEL_1, TEST_NOTIFICATION_CHANNEL_2]}),\n    )\n    # [END howto_operator_gcp_stackdriver_upsert_notification_channel]\n\n    # [START howto_operator_gcp_stackdriver_enable_notification_channel]\n    enable_notification_channel = StackdriverEnableNotificationChannelsOperator(\n        task_id=\"enable-notification-channel\", filter_='type=\"pubsub\"'\n    )\n    # [END howto_operator_gcp_stackdriver_enable_notification_channel]\n\n    # [START howto_operator_gcp_stackdriver_disable_notification_channel]\n    disable_notification_channel = StackdriverDisableNotificationChannelsOperator(\n        task_id=\"disable-notification-channel\", filter_=f'displayName=\"{CHANNEL_1_NAME}\"'\n    )\n    # [END howto_operator_gcp_stackdriver_disable_notification_channel]\n\n    # [START howto_operator_gcp_stackdriver_list_notification_channel]\n    list_notification_channel = StackdriverListNotificationChannelsOperator(\n        task_id=\"list-notification-channel\", filter_='type=\"pubsub\"'\n    )\n    # [END howto_operator_gcp_stackdriver_list_notification_channel]\n\n    # [START howto_operator_gcp_stackdriver_upsert_alert_policy]\n    create_alert_policy = StackdriverUpsertAlertOperator(\n        task_id=\"create-alert-policies\",\n        alerts=json.dumps({\"policies\": [TEST_ALERT_POLICY_1, TEST_ALERT_POLICY_2]}),\n    )\n    # [END howto_operator_gcp_stackdriver_upsert_alert_policy]\n\n    # [START howto_operator_gcp_stackdriver_enable_alert_policy]\n    enable_alert_policy = StackdriverEnableAlertPoliciesOperator(\n        task_id=\"enable-alert-policies\",\n        filter_=f'(displayName=\"{ALERT_1_NAME}\" OR displayName=\"{ALERT_2_NAME}\")',\n    )\n    # [END howto_operator_gcp_stackdriver_enable_alert_policy]\n\n    # [START howto_operator_gcp_stackdriver_disable_alert_policy]\n    disable_alert_policy = StackdriverDisableAlertPoliciesOperator(\n        task_id=\"disable-alert-policies\",\n        filter_=f'displayName=\"{ALERT_1_NAME}\"',\n    )\n    # [END howto_operator_gcp_stackdriver_disable_alert_policy]\n\n    # [START howto_operator_gcp_stackdriver_list_alert_policy]\n    list_alert_policies = StackdriverListAlertPoliciesOperator(\n        task_id=\"list-alert-policies\",\n    )\n    # [END howto_operator_gcp_stackdriver_list_alert_policy]\n\n    # [START howto_operator_gcp_stackdriver_delete_notification_channel]\n    delete_notification_channel = StackdriverDeleteNotificationChannelOperator(\n        task_id=\"delete-notification-channel\",\n        name=\"{{ task_instance.xcom_pull('list-notification-channel')[0]['name'] }}\",\n    )\n    # [END howto_operator_gcp_stackdriver_delete_notification_channel]\n    delete_notification_channel.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_notification_channel_2 = StackdriverDeleteNotificationChannelOperator(\n        task_id=\"delete-notification-channel-2\",\n        name=\"{{ task_instance.xcom_pull('list-notification-channel')[1]['name'] }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_gcp_stackdriver_delete_alert_policy]\n    delete_alert_policy = StackdriverDeleteAlertOperator(\n        task_id=\"delete-alert-policy\",\n        name=\"{{ task_instance.xcom_pull('list-alert-policies')[0]['name'] }}\",\n    )\n    # [END howto_operator_gcp_stackdriver_delete_alert_policy]\n    delete_alert_policy.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_alert_policy_2 = StackdriverDeleteAlertOperator(\n        task_id=\"delete-alert-policy-2\",\n        name=\"{{ task_instance.xcom_pull('list-alert-policies')[1]['name'] }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        create_notification_channel,\n        enable_notification_channel,\n        disable_notification_channel,\n        list_notification_channel,\n        create_alert_policy,\n        enable_alert_policy,\n        disable_alert_policy,\n        list_alert_policies,\n        delete_notification_channel,\n        delete_notification_channel_2,\n        delete_alert_policy,\n        delete_alert_policy_2,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.629894", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 192, "file_name": "example_cloud_storage_transfer_service_gcp.py"}, "content": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Cloud Transfer.\n\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud Project to use for the Google Cloud Transfer Service.\n* GCP_TRANSFER_FIRST_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied from AWS.\n  It is also a source bucket in next step\n* GCP_TRANSFER_SECOND_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    ALREADY_EXISTING_IN_SINK,\n    BUCKET_NAME,\n    DESCRIPTION,\n    FILTER_JOB_NAMES,\n    FILTER_PROJECT_ID,\n    GCS_DATA_SINK,\n    GCS_DATA_SOURCE,\n    PROJECT_ID,\n    SCHEDULE,\n    SCHEDULE_END_DATE,\n    SCHEDULE_START_DATE,\n    START_TIME_OF_DAY,\n    STATUS,\n    TRANSFER_JOB,\n    TRANSFER_JOB_FIELD_MASK,\n    TRANSFER_OPTIONS,\n    TRANSFER_SPEC,\n    GcpTransferJobsStatus,\n    GcpTransferOperationStatus,\n)\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceCreateJobOperator,\n    CloudDataTransferServiceDeleteJobOperator,\n    CloudDataTransferServiceGetOperationOperator,\n    CloudDataTransferServiceListOperationsOperator,\n    CloudDataTransferServiceUpdateJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.cloud_storage_transfer_service import (\n    CloudDataTransferServiceJobStatusSensor,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID_TRANSFER = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_transfer\"\n\nBUCKET_NAME_SRC = f\"src-bucket-{DAG_ID}-{ENV_ID}\"\nBUCKET_NAME_DST = f\"dst-bucket-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"file\"\nFILE_URI = f\"gs://{BUCKET_NAME_SRC}/{FILE_NAME}\"\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / FILE_NAME)\n\n# [START howto_operator_gcp_transfer_create_job_body_gcp]\ngcs_to_gcs_transfer_body = {\n    DESCRIPTION: \"description\",\n    STATUS: GcpTransferJobsStatus.ENABLED,\n    PROJECT_ID: PROJECT_ID_TRANSFER,\n    SCHEDULE: {\n        SCHEDULE_START_DATE: datetime(2015, 1, 1).date(),\n        SCHEDULE_END_DATE: datetime(2030, 1, 1).date(),\n        START_TIME_OF_DAY: (datetime.utcnow() + timedelta(seconds=120)).time(),\n    },\n    TRANSFER_SPEC: {\n        GCS_DATA_SOURCE: {BUCKET_NAME: BUCKET_NAME_SRC},\n        GCS_DATA_SINK: {BUCKET_NAME: BUCKET_NAME_DST},\n        TRANSFER_OPTIONS: {ALREADY_EXISTING_IN_SINK: True},\n    },\n}\n# [END howto_operator_gcp_transfer_create_job_body_gcp]\n\n# [START howto_operator_gcp_transfer_update_job_body]\nupdate_body = {\n    PROJECT_ID: PROJECT_ID_TRANSFER,\n    TRANSFER_JOB: {DESCRIPTION: \"description_updated\"},\n    TRANSFER_JOB_FIELD_MASK: \"description\",\n}\n# [END howto_operator_gcp_transfer_update_job_body]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID_TRANSFER,\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME_SRC,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID_TRANSFER,\n    )\n\n    create_transfer = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_transfer\",\n        body=gcs_to_gcs_transfer_body,\n    )\n\n    # [START howto_operator_gcp_transfer_update_job]\n    update_transfer = CloudDataTransferServiceUpdateJobOperator(\n        task_id=\"update_transfer\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer')['name']}}\",\n        body=update_body,\n    )\n    # [END howto_operator_gcp_transfer_update_job]\n\n    wait_for_transfer = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_transfer\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer')['name']}}\",\n        project_id=PROJECT_ID_TRANSFER,\n        expected_statuses={GcpTransferOperationStatus.SUCCESS},\n    )\n\n    list_operations = CloudDataTransferServiceListOperationsOperator(\n        task_id=\"list_operations\",\n        request_filter={\n            FILTER_PROJECT_ID: PROJECT_ID_TRANSFER,\n            FILTER_JOB_NAMES: [\"{{task_instance.xcom_pull('create_transfer')['name']}}\"],\n        },\n    )\n\n    get_operation = CloudDataTransferServiceGetOperationOperator(\n        task_id=\"get_operation\",\n        operation_name=\"{{task_instance.xcom_pull('list_operations')[0]['name']}}\",\n    )\n\n    delete_transfer = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_transfer_from_gcp_job\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer')['name']}}\",\n        project_id=PROJECT_ID_TRANSFER,\n    )\n\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket_src,\n        upload_file,\n        create_bucket_dst,\n        create_transfer,\n        wait_for_transfer,\n        update_transfer,\n        list_operations,\n        get_operation,\n        delete_transfer,\n        delete_bucket_src,\n        delete_bucket_dst,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.634143", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 156, "file_name": "example_queue.py"}, "content": "\"\"\"\nExample Airflow DAG that creates, gets, lists, updates, purges, pauses, resumes\nand deletes Queues in the Google Cloud Tasks service in the Google Cloud.\n\nRequired setup:\n- GCP_APP_ENGINE_LOCATION: GCP Project's App Engine location `gcloud app describe | grep locationId`.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.api_core.retry import Retry\nfrom google.cloud.tasks_v2.types import Queue\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.tasks import (\n    CloudTasksQueueCreateOperator,\n    CloudTasksQueueDeleteOperator,\n    CloudTasksQueueGetOperator,\n    CloudTasksQueuePauseOperator,\n    CloudTasksQueuePurgeOperator,\n    CloudTasksQueueResumeOperator,\n    CloudTasksQueuesListOperator,\n    CloudTasksQueueUpdateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"cloud_tasks_queue\"\n\nLOCATION = os.environ.get(\"GCP_APP_ENGINE_LOCATION\", \"europe-west2\")\nQUEUE_ID = f\"queue-{ENV_ID}-{DAG_ID.replace('_', '-')}\"\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"tasks\"],\n) as dag:\n\n    @task(task_id=\"random_string\")\n    def generate_random_string():\n        \"\"\"\n        Generate random string for queue and task names.\n        Queue name cannot be repeated in preceding 7 days and\n        task name in the last 1 hour.\n        \"\"\"\n        import random\n        import string\n\n        return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=8))\n\n    random_string = generate_random_string()\n\n    # [START create_queue]\n    create_queue = CloudTasksQueueCreateOperator(\n        location=LOCATION,\n        task_queue=Queue(stackdriver_logging_config=dict(sampling_ratio=0.5)),\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"create_queue\",\n    )\n    # [END create_queue]\n\n    # [START delete_queue]\n    delete_queue = CloudTasksQueueDeleteOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"delete_queue\",\n    )\n    # [END delete_queue]\n    delete_queue.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START resume_queue]\n    resume_queue = CloudTasksQueueResumeOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"resume_queue\",\n    )\n    # [END resume_queue]\n\n    # [START pause_queue]\n    pause_queue = CloudTasksQueuePauseOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"pause_queue\",\n    )\n    # [END pause_queue]\n\n    # [START purge_queue]\n    purge_queue = CloudTasksQueuePurgeOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"purge_queue\",\n    )\n    # [END purge_queue]\n\n    # [START get_queue]\n    get_queue = CloudTasksQueueGetOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"get_queue\",\n    )\n\n    get_queue_result = BashOperator(\n        task_id=\"get_queue_result\",\n        bash_command=f\"echo {get_queue.output}\",\n    )\n    # [END get_queue]\n\n    # [START update_queue]\n    update_queue = CloudTasksQueueUpdateOperator(\n        task_queue=Queue(stackdriver_logging_config=dict(sampling_ratio=1)),\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        update_mask=FieldMask(paths=[\"stackdriver_logging_config.sampling_ratio\"]),\n        task_id=\"update_queue\",\n    )\n    # [END update_queue]\n\n    # [START list_queue]\n    list_queue = CloudTasksQueuesListOperator(location=LOCATION, task_id=\"list_queue\")\n    # [END list_queue]\n\n    chain(\n        random_string,\n        create_queue,\n        update_queue,\n        pause_queue,\n        resume_queue,\n        purge_queue,\n        get_queue,\n        get_queue_result,\n        list_queue,\n        delete_queue,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.638644", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 152, "file_name": "example_tasks.py"}, "content": "\"\"\"\nExample Airflow DAG that creates and deletes Queues and creates, gets, lists,\nruns and deletes Tasks in the Google Cloud Tasks service in the Google Cloud.\n\nRequired setup:\n- GCP_APP_ENGINE_LOCATION: GCP Project's App Engine location `gcloud app describe | grep locationId`.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom google.api_core.retry import Retry\nfrom google.cloud.tasks_v2.types import Queue\nfrom google.protobuf import timestamp_pb2\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.tasks import (\n    CloudTasksQueueCreateOperator,\n    CloudTasksQueueDeleteOperator,\n    CloudTasksTaskCreateOperator,\n    CloudTasksTaskDeleteOperator,\n    CloudTasksTaskGetOperator,\n    CloudTasksTaskRunOperator,\n    CloudTasksTasksListOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"cloud_tasks_tasks\"\n\ntimestamp = timestamp_pb2.Timestamp()\ntimestamp.FromDatetime(datetime.now() + timedelta(hours=12))\n\nLOCATION = os.environ.get(\"GCP_APP_ENGINE_LOCATION\", \"europe-west2\")\n# queue cannot use recent names even if queue was removed\nQUEUE_ID = f\"queue-{ENV_ID}-{DAG_ID.replace('_', '-')}\"\nTASK_NAME = \"task-to-run\"\n\n\nTASK = {\n    \"app_engine_http_request\": {  # Specify the type of request.\n        \"http_method\": \"POST\",\n        \"relative_uri\": \"/example_task_handler\",\n        \"body\": b\"Hello\",\n    },\n    \"schedule_time\": timestamp,\n}\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"tasks\"],\n) as dag:\n\n    @task(task_id=\"random_string\")\n    def generate_random_string():\n        \"\"\"\n        Generate random string for queue and task names.\n        Queue name cannot be repeated in preceding 7 days and\n        task name in the last 1 hour.\n        \"\"\"\n        import random\n        import string\n\n        return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=8))\n\n    random_string = generate_random_string()\n\n    create_queue = CloudTasksQueueCreateOperator(\n        location=LOCATION,\n        task_queue=Queue(stackdriver_logging_config=dict(sampling_ratio=0.5)),\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"create_queue\",\n    )\n\n    delete_queue = CloudTasksQueueDeleteOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"delete_queue\",\n    )\n    delete_queue.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START create_task]\n    create_task = CloudTasksTaskCreateOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task=TASK,\n        task_name=TASK_NAME + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"create_task_to_run\",\n    )\n    # [END create_task]\n\n    # [START tasks_get]\n    tasks_get = CloudTasksTaskGetOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_name=TASK_NAME + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"tasks_get\",\n    )\n    # [END tasks_get]\n\n    # [START run_task]\n    run_task = CloudTasksTaskRunOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_name=TASK_NAME + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        retry=Retry(maximum=10.0),\n        task_id=\"run_task\",\n    )\n    # [END run_task]\n\n    # [START list_tasks]\n    list_tasks = CloudTasksTasksListOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"list_tasks\",\n    )\n    # [END list_tasks]\n\n    # [START delete_task]\n    delete_task = CloudTasksTaskDeleteOperator(\n        location=LOCATION,\n        queue_name=QUEUE_ID + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_name=TASK_NAME + \"{{ task_instance.xcom_pull(task_ids='random_string') }}\",\n        task_id=\"delete_task\",\n    )\n    # [END delete_task]\n\n    chain(\n        random_string, create_queue, create_task, tasks_get, list_tasks, run_task, delete_task, delete_queue\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.655866", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 72, "file_name": "example_text_to_speech.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import CloudTextToSpeechSynthesizeOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"text_to_speech\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_text_to_speech_gcp_filename]\nFILENAME = \"gcp-speech-test-file\"\n# [END howto_operator_text_to_speech_gcp_filename]\n\n# [START howto_operator_text_to_speech_api_arguments]\nINPUT = {\"text\": \"Sample text for demo purposes\"}\nVOICE = {\"language_code\": \"en-US\", \"ssml_gender\": \"FEMALE\"}\nAUDIO_CONFIG = {\"audio_encoding\": \"LINEAR16\"}\n# [END howto_operator_text_to_speech_api_arguments]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"text_to_speech\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_operator_text_to_speech_synthesize]\n    text_to_speech_synthesize_task = CloudTextToSpeechSynthesizeOperator(\n        input_data=INPUT,\n        voice=VOICE,\n        audio_config=AUDIO_CONFIG,\n        target_bucket_name=BUCKET_NAME,\n        target_filename=FILENAME,\n        task_id=\"text_to_speech_synthesize_task\",\n    )\n    # [END howto_operator_text_to_speech_synthesize]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> text_to_speech_synthesize_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.661433", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 164, "file_name": "example_gcs_to_sftp.py"}, "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"gcs_to_sftp\"\n\nSFTP_CONN_ID = \"ssh_default\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDESTINATION_PATH_1 = \"/tmp/single-file/\"\nDESTINATION_PATH_2 = \"/tmp/dest-dir-1/\"\nDESTINATION_PATH_3 = \"/tmp/dest-dir-2/\"\nFILE_NAME = GCS_SRC_FILE = \"empty.txt\"\nUPLOAD_SRC = str(Path(__file__).parent / \"resources\" / FILE_NAME)\nGCS_SRC_FILE_IN_DIR = f\"dir-1/{FILE_NAME}\"\nGCS_SRC_DIR = \"dir-2/*\"\nUPLOAD_IN_DIR_DST = f\"dir-2/{FILE_NAME}\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file_1 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_1\",\n        src=UPLOAD_SRC,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n    upload_file_2 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_2\",\n        src=UPLOAD_SRC,\n        dst=GCS_SRC_FILE_IN_DIR,\n        bucket=BUCKET_NAME,\n    )\n    upload_file_3 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_3\",\n        src=UPLOAD_SRC,\n        dst=UPLOAD_IN_DIR_DST,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcs_to_sftp_copy_single_file]\n    copy_file_from_gcs_to_sftp = GCSToSFTPOperator(\n        task_id=\"file-copy-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        source_bucket=BUCKET_NAME,\n        source_object=GCS_SRC_FILE,\n        destination_path=DESTINATION_PATH_1,\n    )\n    # [END howto_operator_gcs_to_sftp_copy_single_file]\n\n    check_copy_file_from_gcs_to_sftp = SFTPSensor(\n        task_id=\"check-file-copy-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        timeout=60,\n        path=os.path.join(DESTINATION_PATH_1, GCS_SRC_FILE),\n    )\n\n    # [START howto_operator_gcs_to_sftp_move_single_file_destination]\n    move_file_from_gcs_to_sftp = GCSToSFTPOperator(\n        task_id=\"file-move-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        source_bucket=BUCKET_NAME,\n        source_object=GCS_SRC_FILE_IN_DIR,\n        destination_path=DESTINATION_PATH_1,\n        move_object=True,\n    )\n    # [END howto_operator_gcs_to_sftp_move_single_file_destination]\n\n    check_move_file_from_gcs_to_sftp = SFTPSensor(\n        task_id=\"check-file-move-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        timeout=60,\n        path=os.path.join(DESTINATION_PATH_1, GCS_SRC_FILE_IN_DIR),\n    )\n\n    # [START howto_operator_gcs_to_sftp_copy_directory]\n    copy_dir_from_gcs_to_sftp = GCSToSFTPOperator(\n        task_id=\"dir-copy-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        source_bucket=BUCKET_NAME,\n        source_object=GCS_SRC_DIR,\n        destination_path=DESTINATION_PATH_2,\n    )\n    # [END howto_operator_gcs_to_sftp_copy_directory]\n\n    check_copy_dir_from_gcs_to_sftp = SFTPSensor(\n        task_id=\"check-dir-copy-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        timeout=60,\n        path=os.path.join(DESTINATION_PATH_2, \"dir-2\", GCS_SRC_FILE),\n    )\n\n    # [START howto_operator_gcs_to_sftp_move_specific_files]\n    move_dir_from_gcs_to_sftp = GCSToSFTPOperator(\n        task_id=\"dir-move-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        source_bucket=BUCKET_NAME,\n        source_object=GCS_SRC_DIR,\n        destination_path=DESTINATION_PATH_3,\n        keep_directory_structure=False,\n    )\n    # [END howto_operator_gcs_to_sftp_move_specific_files]\n\n    check_move_dir_from_gcs_to_sftp = SFTPSensor(\n        task_id=\"check-dir-move-gsc-to-sftp\",\n        sftp_conn_id=SFTP_CONN_ID,\n        timeout=60,\n        path=os.path.join(DESTINATION_PATH_3, GCS_SRC_FILE),\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> (upload_file_1, upload_file_2, upload_file_3)\n        # TEST BODY\n        >> copy_file_from_gcs_to_sftp\n        >> check_copy_file_from_gcs_to_sftp\n        >> move_file_from_gcs_to_sftp\n        >> check_move_file_from_gcs_to_sftp\n        >> copy_dir_from_gcs_to_sftp\n        >> check_copy_dir_from_gcs_to_sftp\n        >> move_dir_from_gcs_to_sftp\n        >> check_move_dir_from_gcs_to_sftp\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.667329", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 101, "file_name": "example_gdrive_to_local.py"}, "content": "\"\"\"\nExample DAG using GoogleDriveToLocalOperator.\n\nUsing this operator requires the following additional scopes:\nhttps://www.googleapis.com/auth/drive\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gdrive_to_local import GoogleDriveToLocalOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.suite.sensors.drive import GoogleDriveFileExistenceSensor\nfrom airflow.providers.google.suite.transfers.gcs_to_gdrive import GCSToGoogleDriveOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nFILE_NAME = \"empty.txt\"\nOUTPUT_FILE = \"out_file.txt\"\nDAG_ID = \"example_gdrive_to_local\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nLOCAL_PATH = str(Path(__file__).parent / \"resources\" / FILE_NAME)\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=LOCAL_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n\n    copy_single_file = GCSToGoogleDriveOperator(\n        task_id=\"copy_single_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START detect_file]\n    detect_file = GoogleDriveFileExistenceSensor(\n        task_id=\"detect_file\",\n        folder_id=\"\",\n        file_name=FILE_NAME,\n    )\n    # [END detect_file]\n\n    # [START download_from_gdrive_to_local]\n    download_from_gdrive_to_local = GoogleDriveToLocalOperator(\n        task_id=\"download_from_gdrive_to_local\",\n        folder_id=\"\",\n        file_name=FILE_NAME,\n        output_file=OUTPUT_FILE,\n    )\n    # [END download_from_gdrive_to_local]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        upload_file,\n        copy_single_file,\n        # TEST BODY\n        detect_file,\n        download_from_gdrive_to_local,\n        # TEST TEARDOWN\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.669074", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 210, "file_name": "example_postgres_to_gcs.py"}, "content": "\"\"\"\nExample DAG using PostgresToGoogleCloudStorageOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom googleapiclient import discovery\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.google.cloud.operators.cloud_sql import (\n    CloudSQLCreateInstanceDatabaseOperator,\n    CloudSQLCreateInstanceOperator,\n    CloudSQLDeleteInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.providers.google.cloud.transfers.postgres_to_gcs import PostgresToGCSOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_postgres_to_gcs\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"example-project\")\n\nCLOUD_SQL_INSTANCE = f\"cloud-sql-{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nCLOUD_SQL_INSTANCE_CREATION_BODY = {\n    \"name\": CLOUD_SQL_INSTANCE,\n    \"settings\": {\n        \"tier\": \"db-custom-1-3840\",\n        \"dataDiskSizeGb\": 30,\n        \"ipConfiguration\": {\n            \"ipv4Enabled\": True,\n            \"requireSsl\": False,\n            # Consider specifying your network mask\n            # for allowing requests only from the trusted sources, not from anywhere\n            \"authorizedNetworks\": [\n                {\"value\": \"0.0.0.0/0\"},\n            ],\n        },\n        \"pricingPlan\": \"PER_USE\",\n    },\n    \"databaseVersion\": \"POSTGRES_15\",\n    \"region\": \"us-central1\",\n}\nDB_NAME = f\"{DAG_ID}-{ENV_ID}-db\".replace(\"-\", \"_\")\nDB_PORT = 5432\nDB_CREATE_BODY = {\"instance\": CLOUD_SQL_INSTANCE, \"name\": DB_NAME, \"project\": PROJECT_ID}\nDB_USER_NAME = \"demo_user\"\nDB_USER_PASSWORD = \"demo_password\"\nCONNECTION_ID = f\"postgres_{DAG_ID}_{ENV_ID}\".replace(\"-\", \"_\")\n\nBUCKET_NAME = f\"{DAG_ID}_{ENV_ID}_bucket\"\nFILE_NAME = \"result.json\"\n\nSQL_TABLE = \"test_table\"\nSQL_CREATE = f\"CREATE TABLE IF NOT EXISTS {SQL_TABLE} (col_1 INT, col_2 VARCHAR(8))\"\nSQL_INSERT = f\"INSERT INTO {SQL_TABLE} (col_1, col_2) VALUES (1, 'one'), (2, 'two')\"\nSQL_SELECT = f\"SELECT * FROM {SQL_TABLE}\"\n\nlog = logging.getLogger(__name__)\n\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"postgres\", \"gcs\"],\n) as dag:\n    create_cloud_sql_instance = CloudSQLCreateInstanceOperator(\n        task_id=\"create_cloud_sql_instance\",\n        project_id=PROJECT_ID,\n        instance=CLOUD_SQL_INSTANCE,\n        body=CLOUD_SQL_INSTANCE_CREATION_BODY,\n    )\n\n    create_database = CloudSQLCreateInstanceDatabaseOperator(\n        task_id=\"create_database\", body=DB_CREATE_BODY, instance=CLOUD_SQL_INSTANCE\n    )\n\n    @task\n    def create_user() -> None:\n        with discovery.build(\"sqladmin\", \"v1beta4\") as service:\n            request = service.users().insert(\n                project=PROJECT_ID,\n                instance=CLOUD_SQL_INSTANCE,\n                body={\n                    \"name\": DB_USER_NAME,\n                    \"password\": DB_USER_PASSWORD,\n                },\n            )\n            request.execute()\n\n    create_user_task = create_user()\n\n    @task\n    def get_public_ip() -> str | None:\n        with discovery.build(\"sqladmin\", \"v1beta4\") as service:\n            request = service.connect().get(\n                project=PROJECT_ID, instance=CLOUD_SQL_INSTANCE, fields=\"ipAddresses\"\n            )\n            response = request.execute()\n            for ip_item in response.get(\"ipAddresses\", []):\n                if ip_item[\"type\"] == \"PRIMARY\":\n                    return ip_item[\"ipAddress\"]\n\n    get_public_ip_task = get_public_ip()\n\n    @task\n    def setup_postgres_connection(**kwargs) -> None:\n        public_ip = kwargs[\"ti\"].xcom_pull(task_ids=\"get_public_ip\")\n        connection = Connection(\n            conn_id=CONNECTION_ID,\n            description=\"Example PostgreSQL connection\",\n            conn_type=\"postgres\",\n            host=public_ip,\n            login=DB_USER_NAME,\n            password=DB_USER_PASSWORD,\n            schema=DB_NAME,\n            port=DB_PORT,\n        )\n        session: Session = Session()\n        if session.query(Connection).filter(Connection.conn_id == CONNECTION_ID).first():\n            log.warning(\"Connection %s already exists\", CONNECTION_ID)\n            return None\n\n        session.add(connection)\n        session.commit()\n\n    setup_postgres_connection_task = setup_postgres_connection()\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n    )\n\n    create_sql_table = SQLExecuteQueryOperator(\n        task_id=\"create_sql_table\",\n        conn_id=CONNECTION_ID,\n        sql=SQL_CREATE,\n    )\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=CONNECTION_ID,\n        sql=SQL_INSERT,\n    )\n\n    # [START howto_operator_postgres_to_gcs]\n    get_data = PostgresToGCSOperator(\n        task_id=\"get_data\",\n        postgres_conn_id=CONNECTION_ID,\n        sql=SQL_SELECT,\n        bucket=BUCKET_NAME,\n        filename=FILE_NAME,\n        gzip=False,\n    )\n    # [END howto_operator_postgres_to_gcs]\n\n    delete_cloud_sql_instance = CloudSQLDeleteInstanceOperator(\n        task_id=\"delete_cloud_sql_instance\",\n        project_id=PROJECT_ID,\n        instance=CLOUD_SQL_INSTANCE,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_postgres_connection = BashOperator(\n        task_id=\"delete_postgres_connection\",\n        bash_command=f\"airflow connections delete {CONNECTION_ID}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # TEST SETUP\n    create_cloud_sql_instance >> [create_database, create_user_task, get_public_ip_task]\n    [create_user_task, get_public_ip_task] >> setup_postgres_connection_task\n    create_database >> setup_postgres_connection_task >> create_sql_table >> insert_data\n    (\n        [insert_data, create_bucket]\n        # TEST BODY\n        >> get_data\n        # TEST TEARDOWN\n        >> [delete_cloud_sql_instance, delete_postgres_connection, delete_bucket]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.678830", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 177, "file_name": "example_vertex_ai_auto_ml_forecasting_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLForecastingTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nFORECASTING_DISPLAY_NAME = f\"auto-ml-forecasting-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-forecasting-model-{ENV_ID}\"\n\nFORECAST_GCS_BUCKET_NAME = f\"bucket_forecast_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nFORECAST_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"forecast-dataset.csv.zip\")\nFORECAST_GCS_OBJECT_NAME = \"vertex-ai/forecast-dataset.csv\"\nFORECAST_CSV_FILE_LOCAL_PATH = \"/forecast/forecast-dataset.csv\"\n\nFORECAST_DATASET = {\n    \"display_name\": f\"forecast-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.time_series,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{FORECAST_GCS_BUCKET_NAME}/vertex-ai/forecast-dataset.csv\"]}\n            }\n        },\n        Value(),\n    ),\n}\n\nTEST_TIME_COLUMN = \"date\"\nTEST_TIME_SERIES_IDENTIFIER_COLUMN = \"store_name\"\nTEST_TARGET_COLUMN = \"sale_dollars\"\n\nCOLUMN_SPECS = {\n    TEST_TIME_COLUMN: \"timestamp\",\n    TEST_TARGET_COLUMN: \"numeric\",\n    \"city\": \"categorical\",\n    \"zip_code\": \"categorical\",\n    \"county\": \"categorical\",\n}\n\n\nwith models.DAG(\n    f\"{DAG_ID}_forecasting_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=FORECAST_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {FORECAST_ZIP_CSV_FILE_LOCAL_PATH} -d /forecast/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=FORECAST_CSV_FILE_LOCAL_PATH,\n        dst=FORECAST_GCS_OBJECT_NAME,\n        bucket=FORECAST_GCS_BUCKET_NAME,\n    )\n\n    create_forecast_dataset = CreateDatasetOperator(\n        task_id=\"forecast_dataset\",\n        dataset=FORECAST_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    forecast_dataset_id = create_forecast_dataset.output[\"dataset_id\"]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n    create_auto_ml_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(\n        task_id=\"auto_ml_forecasting_task\",\n        display_name=FORECASTING_DISPLAY_NAME,\n        optimization_objective=\"minimize-rmse\",\n        column_specs=COLUMN_SPECS,\n        # run params\n        dataset_id=forecast_dataset_id,\n        target_column=TEST_TARGET_COLUMN,\n        time_column=TEST_TIME_COLUMN,\n        time_series_identifier_column=TEST_TIME_SERIES_IDENTIFIER_COLUMN,\n        available_at_forecast_columns=[TEST_TIME_COLUMN],\n        unavailable_at_forecast_columns=[TEST_TARGET_COLUMN],\n        time_series_attribute_columns=[\"city\", \"zip_code\", \"county\"],\n        forecast_horizon=30,\n        context_window=30,\n        data_granularity_unit=\"day\",\n        data_granularity_count=1,\n        weight_column=None,\n        budget_milli_node_hours=1000,\n        model_display_name=MODEL_DISPLAY_NAME,\n        predefined_split_column_name=None,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n    delete_auto_ml_forecasting_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_forecasting_training_job\",\n        training_pipeline_id=create_auto_ml_forecasting_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n\n    delete_forecast_dataset = DeleteDatasetOperator(\n        task_id=\"delete_forecast_dataset\",\n        dataset_id=forecast_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=FORECAST_GCS_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /forecast/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_forecast_dataset\n        # TEST BODY\n        >> create_auto_ml_forecasting_training_job\n        # TEST TEARDOWN\n        >> delete_auto_ml_forecasting_training_job\n        >> delete_forecast_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.688923", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 51, "file_name": "example_translate.py"}, "content": "\"\"\"\nExample Airflow DAG that translates text in Google Cloud Translate\nservice in the Google Cloud.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.translate import CloudTranslateTextOperator\n\nDAG_ID = \"example_gcp_translate\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_translate_text]\n    product_set_create = CloudTranslateTextOperator(\n        task_id=\"translate\",\n        values=[\"za gl ja\"],\n        target_language=\"en\",\n        format_=\"text\",\n        source_language=None,\n        model=\"base\",\n    )\n    # [END howto_operator_translate_text]\n    # [START howto_operator_translate_access]\n    translation_access = BashOperator(\n        task_id=\"access\", bash_command=\"echo '{{ task_instance.xcom_pull(\\\"translate\\\")[0] }}'\"\n    )\n    # [END howto_operator_translate_access]\n    product_set_create >> translation_access\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.691029", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 104, "file_name": "example_translate_speech.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import CloudTextToSpeechSynthesizeOperator\nfrom airflow.providers.google.cloud.operators.translate_speech import CloudTranslateSpeechOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_translate_speech\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_translate_speech_gcp_filename]\nFILE_NAME = f\"test-translate-speech-file-{DAG_ID}-{ENV_ID}\"\n# [END howto_operator_translate_speech_gcp_filename]\n\n# [START howto_operator_text_to_speech_api_arguments]\nINPUT = {\"text\": \"Sample text for demo purposes\"}\nVOICE = {\"language_code\": \"en-US\", \"ssml_gender\": \"FEMALE\"}\nAUDIO_CONFIG = {\"audio_encoding\": \"LINEAR16\"}\n# [END howto_operator_text_to_speech_api_arguments]\n\n# [START howto_operator_translate_speech_arguments]\nCONFIG = {\"encoding\": \"LINEAR16\", \"language_code\": \"en_US\"}\nAUDIO = {\"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\"}\nTARGET_LANGUAGE = \"pl\"\nFORMAT = \"text\"\nMODEL = \"base\"\nSOURCE_LANGUAGE = None\n# [END howto_operator_translate_speech_arguments]\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    text_to_speech_synthesize_task = CloudTextToSpeechSynthesizeOperator(\n        project_id=PROJECT_ID,\n        input_data=INPUT,\n        voice=VOICE,\n        audio_config=AUDIO_CONFIG,\n        target_bucket_name=BUCKET_NAME,\n        target_filename=FILE_NAME,\n        task_id=\"text_to_speech_synthesize_task\",\n    )\n    # [START howto_operator_translate_speech]\n    translate_speech_task = CloudTranslateSpeechOperator(\n        project_id=PROJECT_ID,\n        audio=AUDIO,\n        config=CONFIG,\n        target_language=TARGET_LANGUAGE,\n        format_=FORMAT,\n        source_language=SOURCE_LANGUAGE,\n        model=MODEL,\n        task_id=\"translate_speech_task\",\n    )\n    translate_speech_task2 = CloudTranslateSpeechOperator(\n        audio=AUDIO,\n        config=CONFIG,\n        target_language=TARGET_LANGUAGE,\n        format_=FORMAT,\n        source_language=SOURCE_LANGUAGE,\n        model=MODEL,\n        task_id=\"translate_speech_task2\",\n    )\n    # [END howto_operator_translate_speech]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> text_to_speech_synthesize_task\n        >> translate_speech_task\n        >> translate_speech_task2\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.698514", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 167, "file_name": "example_vertex_ai_auto_ml_image_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLImageTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ImportDataOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nIMAGE_DISPLAY_NAME = f\"auto-ml-image-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-image-model-{ENV_ID}\"\n\nIMAGE_GCS_BUCKET_NAME = f\"bucket_image_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nIMAGE_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"image-dataset.csv.zip\")\nIMAGE_GCS_OBJECT_NAME = \"vertex-ai/image-dataset.csv\"\nIMAGE_CSV_FILE_LOCAL_PATH = \"/image/image-dataset.csv\"\n\nIMAGE_DATASET = {\n    \"display_name\": f\"image-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.image,\n    \"metadata\": Value(string_value=\"image-dataset\"),\n}\nIMAGE_DATA_CONFIG = [\n    {\n        \"import_schema_uri\": schema.dataset.ioformat.image.single_label_classification,\n        \"gcs_source\": {\"uris\": [f\"gs://{IMAGE_GCS_BUCKET_NAME}/vertex-ai/image-dataset.csv\"]},\n    },\n]\n\n\nwith models.DAG(\n    f\"{DAG_ID}_image_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=IMAGE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {IMAGE_ZIP_CSV_FILE_LOCAL_PATH} -d /image/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=IMAGE_CSV_FILE_LOCAL_PATH,\n        dst=IMAGE_GCS_OBJECT_NAME,\n        bucket=IMAGE_GCS_BUCKET_NAME,\n    )\n\n    create_image_dataset = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    image_dataset_id = create_image_dataset.output[\"dataset_id\"]\n\n    import_image_dataset = ImportDataOperator(\n        task_id=\"import_image_data\",\n        dataset_id=image_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=IMAGE_DATA_CONFIG,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n    create_auto_ml_image_training_job = CreateAutoMLImageTrainingJobOperator(\n        task_id=\"auto_ml_image_task\",\n        display_name=IMAGE_DISPLAY_NAME,\n        dataset_id=image_dataset_id,\n        prediction_type=\"classification\",\n        multi_label=False,\n        model_type=\"CLOUD\",\n        training_fraction_split=0.6,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.2,\n        budget_milli_node_hours=8000,\n        model_display_name=MODEL_DISPLAY_NAME,\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n\n    delete_auto_ml_image_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=create_auto_ml_image_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_image_dataset = DeleteDatasetOperator(\n        task_id=\"delete_image_dataset\",\n        dataset_id=image_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=IMAGE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /image/*\",\n    )\n\n    (\n        # TEST SETUP\n        [\n            create_bucket,\n            create_image_dataset,\n        ]\n        >> unzip_file\n        >> upload_files\n        >> import_image_dataset\n        # TEST BODY\n        >> create_auto_ml_image_training_job\n        # TEST TEARDOWN\n        >> delete_auto_ml_image_training_job\n        >> delete_image_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.713810", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 41, "file_name": "example_vertex_ai_auto_ml_list_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import ListAutoMLTrainingJobOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\n\n\nwith models.DAG(\n    f\"{DAG_ID}_list_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\", \"list_operation\"],\n) as dag:\n\n    # [START how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n    list_auto_ml_training_job = ListAutoMLTrainingJobOperator(\n        task_id=\"list_auto_ml_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.715182", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 165, "file_name": "example_vertex_ai_auto_ml_text_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLTextTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ImportDataOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nTEXT_DISPLAY_NAME = f\"auto-ml-text-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-text-model-{ENV_ID}\"\n\nTEXT_GCS_BUCKET_NAME = f\"bucket_text_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nTEXT_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"text-dataset.csv.zip\")\nTEXT_GCS_OBJECT_NAME = \"vertex-ai/text-dataset.csv\"\nTEXT_CSV_FILE_LOCAL_PATH = \"/text/text-dataset.csv\"\n\nTEXT_DATASET = {\n    \"display_name\": f\"text-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.text,\n    \"metadata\": Value(string_value=\"text-dataset\"),\n}\nTEXT_DATA_CONFIG = [\n    {\n        \"import_schema_uri\": schema.dataset.ioformat.text.single_label_classification,\n        \"gcs_source\": {\"uris\": [f\"gs://{TEXT_GCS_BUCKET_NAME}/vertex-ai/text-dataset.csv\"]},\n    },\n]\n\nwith models.DAG(\n    f\"{DAG_ID}_text_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=TEXT_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {TEXT_ZIP_CSV_FILE_LOCAL_PATH} -d /text/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=TEXT_CSV_FILE_LOCAL_PATH,\n        dst=TEXT_GCS_OBJECT_NAME,\n        bucket=TEXT_GCS_BUCKET_NAME,\n    )\n\n    create_text_dataset = CreateDatasetOperator(\n        task_id=\"text_dataset\",\n        dataset=TEXT_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    text_dataset_id = create_text_dataset.output[\"dataset_id\"]\n\n    import_text_dataset = ImportDataOperator(\n        task_id=\"import_text_data\",\n        dataset_id=text_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=TEXT_DATA_CONFIG,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n    create_auto_ml_text_training_job = CreateAutoMLTextTrainingJobOperator(\n        task_id=\"auto_ml_text_task\",\n        display_name=TEXT_DISPLAY_NAME,\n        prediction_type=\"classification\",\n        multi_label=False,\n        dataset_id=text_dataset_id,\n        model_display_name=MODEL_DISPLAY_NAME,\n        training_fraction_split=0.7,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.1,\n        sync=True,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n\n    delete_auto_ml_text_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_text_training_job\",\n        training_pipeline_id=create_auto_ml_text_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_text_dataset = DeleteDatasetOperator(\n        task_id=\"delete_text_dataset\",\n        dataset_id=text_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=TEXT_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /text/*\",\n    )\n\n    (\n        # TEST SETUP\n        [\n            create_bucket,\n            create_text_dataset,\n        ]\n        >> unzip_file\n        >> upload_files\n        >> import_text_dataset\n        # TEST BODY\n        >> create_auto_ml_text_training_job\n        # TEST TEARDOWN\n        >> delete_auto_ml_text_training_job\n        >> delete_text_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.723311", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 171, "file_name": "example_vertex_ai_auto_ml_tabular_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLTabularTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nTABULAR_DISPLAY_NAME = f\"auto-ml-tabular-{ENV_ID}\"\nMODEL_DISPLAY_NAME = \"adopted-prediction-model\"\n\nTABULAR_GCS_BUCKET_NAME = f\"bucket_tabular_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nTABULAR_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"tabular-dataset.csv.zip\")\nTABULAR_GCS_OBJECT_NAME = \"vertex-ai/tabular-dataset.csv\"\nTABULAR_CSV_FILE_LOCAL_PATH = \"/tabular/tabular-dataset.csv\"\n\nTABULAR_DATASET = {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{TABULAR_GCS_BUCKET_NAME}/vertex-ai/tabular-dataset.csv\"]}\n            }\n        },\n        Value(),\n    ),\n}\n\nCOLUMN_TRANSFORMATIONS = [\n    {\"categorical\": {\"column_name\": \"Type\"}},\n    {\"numeric\": {\"column_name\": \"Age\"}},\n    {\"categorical\": {\"column_name\": \"Breed1\"}},\n    {\"categorical\": {\"column_name\": \"Color1\"}},\n    {\"categorical\": {\"column_name\": \"Color2\"}},\n    {\"categorical\": {\"column_name\": \"MaturitySize\"}},\n    {\"categorical\": {\"column_name\": \"FurLength\"}},\n    {\"categorical\": {\"column_name\": \"Vaccinated\"}},\n    {\"categorical\": {\"column_name\": \"Sterilized\"}},\n    {\"categorical\": {\"column_name\": \"Health\"}},\n    {\"numeric\": {\"column_name\": \"Fee\"}},\n    {\"numeric\": {\"column_name\": \"PhotoAmt\"}},\n]\n\nwith models.DAG(\n    f\"{DAG_ID}_tabular_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=TABULAR_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {TABULAR_ZIP_CSV_FILE_LOCAL_PATH} -d /tabular/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=TABULAR_CSV_FILE_LOCAL_PATH,\n        dst=TABULAR_GCS_OBJECT_NAME,\n        bucket=TABULAR_GCS_BUCKET_NAME,\n    )\n\n    create_tabular_dataset = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    tabular_dataset_id = create_tabular_dataset.output[\"dataset_id\"]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n    create_auto_ml_tabular_training_job = CreateAutoMLTabularTrainingJobOperator(\n        task_id=\"auto_ml_tabular_task\",\n        display_name=TABULAR_DISPLAY_NAME,\n        optimization_prediction_type=\"classification\",\n        column_transformations=COLUMN_TRANSFORMATIONS,\n        dataset_id=tabular_dataset_id,\n        target_column=\"Adopted\",\n        training_fraction_split=0.8,\n        validation_fraction_split=0.1,\n        test_fraction_split=0.1,\n        model_display_name=MODEL_DISPLAY_NAME,\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n\n    delete_auto_ml_tabular_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=create_auto_ml_tabular_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_tabular_dataset = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=tabular_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=TABULAR_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /tabular/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_tabular_dataset\n        # TEST BODY\n        >> create_auto_ml_tabular_training_job\n        # TEST TEARDOWN\n        >> delete_auto_ml_tabular_training_job\n        >> delete_tabular_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.726235", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 161, "file_name": "example_vertex_ai_auto_ml_video_training.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLVideoTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ImportDataOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nVIDEO_DISPLAY_NAME = f\"auto-ml-video-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-video-model-{ENV_ID}\"\n\nVIDEO_GCS_BUCKET_NAME = f\"bucket_custom_python_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nVIDEO_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"video-dataset.csv.zip\")\nVIDEO_GCS_OBJECT_NAME = \"vertex-ai/video-dataset.csv\"\nVIDEO_CSV_FILE_LOCAL_PATH = \"/video/video-dataset.csv\"\n\nVIDEO_DATASET = {\n    \"display_name\": f\"video-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.video,\n    \"metadata\": Value(string_value=\"video-dataset\"),\n}\nVIDEO_DATA_CONFIG = [\n    {\n        \"import_schema_uri\": schema.dataset.ioformat.video.classification,\n        \"gcs_source\": {\"uris\": [f\"gs://{VIDEO_GCS_BUCKET_NAME}/vertex-ai/video-dataset.csv\"]},\n    },\n]\n\nwith models.DAG(\n    f\"{DAG_ID}_video_training_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=VIDEO_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {VIDEO_ZIP_CSV_FILE_LOCAL_PATH} -d /video/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=VIDEO_CSV_FILE_LOCAL_PATH,\n        dst=VIDEO_GCS_OBJECT_NAME,\n        bucket=VIDEO_GCS_BUCKET_NAME,\n    )\n\n    create_video_dataset = CreateDatasetOperator(\n        task_id=\"video_dataset\",\n        dataset=VIDEO_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    video_dataset_id = create_video_dataset.output[\"dataset_id\"]\n\n    import_video_dataset = ImportDataOperator(\n        task_id=\"import_video_data\",\n        dataset_id=video_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=VIDEO_DATA_CONFIG,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n    create_auto_ml_video_training_job = CreateAutoMLVideoTrainingJobOperator(\n        task_id=\"auto_ml_video_task\",\n        display_name=VIDEO_DISPLAY_NAME,\n        prediction_type=\"classification\",\n        model_type=\"CLOUD\",\n        dataset_id=video_dataset_id,\n        model_display_name=MODEL_DISPLAY_NAME,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n\n    delete_auto_ml_video_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_video_training_job\",\n        training_pipeline_id=create_auto_ml_video_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_video_dataset = DeleteDatasetOperator(\n        task_id=\"delete_video_dataset\",\n        dataset_id=video_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=VIDEO_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /video/*\",\n    )\n\n    (\n        # TEST SETUP\n        [\n            create_bucket,\n            create_video_dataset,\n        ]\n        >> unzip_file\n        >> upload_files\n        >> import_video_dataset\n        # TEST BODY\n        >> create_auto_ml_video_training_job\n        # TEST TEARDOWN\n        >> delete_auto_ml_video_training_job\n        >> delete_video_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.745047", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 216, "file_name": "example_vertex_ai_batch_prediction_job.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Batch Prediction operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLForecastingTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.batch_prediction_job import (\n    CreateBatchPredictionJobOperator,\n    DeleteBatchPredictionJobOperator,\n    ListBatchPredictionJobsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_batch_prediction_job_operations\"\nREGION = \"us-central1\"\n\nFORECAST_DISPLAY_NAME = f\"auto-ml-forecasting-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-forecasting-model-{ENV_ID}\"\n\nJOB_DISPLAY_NAME = f\"batch_prediction_job_test_{ENV_ID}\"\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/forecast-dataset.csv\"\nFORECAST_ZIP_CSV_FILE_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / \"forecast-dataset.csv.zip\")\nFORECAST_CSV_FILE_LOCAL_PATH = \"/batch-prediction/forecast-dataset.csv\"\n\nFORECAST_DATASET = {\n    \"display_name\": f\"forecast-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.time_series,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}\n            }\n        },\n        Value(),\n    ),\n}\n\nTEST_TIME_COLUMN = \"date\"\nTEST_TIME_SERIES_IDENTIFIER_COLUMN = \"store_name\"\nTEST_TARGET_COLUMN = \"sale_dollars\"\nCOLUMN_SPECS = {\n    TEST_TIME_COLUMN: \"timestamp\",\n    TEST_TARGET_COLUMN: \"numeric\",\n    \"city\": \"categorical\",\n    \"zip_code\": \"categorical\",\n    \"county\": \"categorical\",\n}\n\nBIGQUERY_SOURCE = f\"bq://{PROJECT_ID}.test_iowa_liquor_sales_forecasting_us.2021_sales_predict\"\nGCS_DESTINATION_PREFIX = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/output\"\nMODEL_PARAMETERS = ParseDict({}, Value())\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    render_template_as_native_obj=True,\n    tags=[\"example\", \"vertex_ai\", \"batch_prediction_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"mkdir -p /batch-prediction && \"\n        f\"unzip {FORECAST_ZIP_CSV_FILE_LOCAL_PATH} -d /batch-prediction/\",\n    )\n\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=FORECAST_CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n    )\n    create_forecast_dataset = CreateDatasetOperator(\n        task_id=\"forecast_dataset\",\n        dataset=FORECAST_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_auto_ml_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(\n        task_id=\"auto_ml_forecasting_task\",\n        display_name=FORECAST_DISPLAY_NAME,\n        optimization_objective=\"minimize-rmse\",\n        column_specs=COLUMN_SPECS,\n        # run params\n        dataset_id=create_forecast_dataset.output[\"dataset_id\"],\n        target_column=TEST_TARGET_COLUMN,\n        time_column=TEST_TIME_COLUMN,\n        time_series_identifier_column=TEST_TIME_SERIES_IDENTIFIER_COLUMN,\n        available_at_forecast_columns=[TEST_TIME_COLUMN],\n        unavailable_at_forecast_columns=[TEST_TARGET_COLUMN],\n        time_series_attribute_columns=[\"city\", \"zip_code\", \"county\"],\n        forecast_horizon=30,\n        context_window=30,\n        data_granularity_unit=\"day\",\n        data_granularity_count=1,\n        weight_column=None,\n        budget_milli_node_hours=1000,\n        model_display_name=MODEL_DISPLAY_NAME,\n        predefined_split_column_name=None,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n    create_batch_prediction_job = CreateBatchPredictionJobOperator(\n        task_id=\"create_batch_prediction_job\",\n        job_display_name=JOB_DISPLAY_NAME,\n        model_name=\"{{ti.xcom_pull('auto_ml_forecasting_task')['name']}}\",\n        predictions_format=\"csv\",\n        bigquery_source=BIGQUERY_SOURCE,\n        gcs_destination_prefix=GCS_DESTINATION_PREFIX,\n        model_parameters=MODEL_PARAMETERS,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n    list_batch_prediction_job = ListBatchPredictionJobsOperator(\n        task_id=\"list_batch_prediction_jobs\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n    delete_batch_prediction_job = DeleteBatchPredictionJobOperator(\n        task_id=\"delete_batch_prediction_job\",\n        batch_prediction_job_id=create_batch_prediction_job.output[\"batch_prediction_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n\n    delete_auto_ml_forecasting_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_forecasting_training_job\",\n        training_pipeline_id=create_auto_ml_forecasting_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_forecast_dataset = DeleteDatasetOperator(\n        task_id=\"delete_forecast_dataset\",\n        dataset_id=create_forecast_dataset.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /batch-prediction/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_forecast_dataset\n        >> create_auto_ml_forecasting_training_job\n        # TEST BODY\n        >> create_batch_prediction_job\n        >> list_batch_prediction_job\n        # TEST TEARDOWN\n        >> delete_batch_prediction_job\n        >> delete_auto_ml_forecasting_training_job\n        >> delete_forecast_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.747836", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 166, "file_name": "example_vertex_ai_custom_container.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Custom Jobs operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomContainerTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_custom_job_operations\"\nREGION = \"us-central1\"\nCONTAINER_DISPLAY_NAME = f\"train-housing-container-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"container-housing-model-{ENV_ID}\"\n\nCUSTOM_CONTAINER_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/california_housing_train.csv\"\nCSV_FILE_LOCAL_PATH = \"/custom-job-container/california_housing_train.csv\"\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nCSV_ZIP_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"California-housing-custom-container.zip\")\n\nTABULAR_DATASET = lambda bucket_name: {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\"input_config\": {\"gcs_source\": {\"uri\": [f\"gs://{bucket_name}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}}},\n        Value(),\n    ),\n}\n\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nCUSTOM_CONTAINER_URI = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/system-tests/housing:latest\"\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nTRAINING_FRACTION_SPLIT = 0.7\nTEST_FRACTION_SPLIT = 0.15\nVALIDATION_FRACTION_SPLIT = 0.15\n\n\nwith models.DAG(\n    f\"{DAG_ID}_custom_container\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"custom_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=CUSTOM_CONTAINER_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"mkdir -p /custom-job-container/ && \"\n        f\"unzip {CSV_ZIP_FILE_LOCAL_PATH} -d /custom-job-container/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=CUSTOM_CONTAINER_GCS_BUCKET_NAME,\n    )\n    create_tabular_dataset = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET(CUSTOM_CONTAINER_GCS_BUCKET_NAME),\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    tabular_dataset_id = create_tabular_dataset.output[\"dataset_id\"]\n\n    # [START how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n    create_custom_container_training_job = CreateCustomContainerTrainingJobOperator(\n        task_id=\"custom_container_task\",\n        staging_bucket=f\"gs://{CUSTOM_CONTAINER_GCS_BUCKET_NAME}\",\n        display_name=CONTAINER_DISPLAY_NAME,\n        container_uri=CUSTOM_CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=tabular_dataset_id,\n        command=[\"python3\", \"task.py\"],\n        model_display_name=MODEL_DISPLAY_NAME,\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=create_custom_container_training_job.output[\"training_id\"],\n        custom_job_id=create_custom_container_training_job.output[\"custom_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_tabular_dataset = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=tabular_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=CUSTOM_CONTAINER_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /custom-job-container/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_tabular_dataset\n        # TEST BODY\n        >> create_custom_container_training_job\n        # TEST TEARDOWN\n        >> delete_custom_training_job\n        >> delete_tabular_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.750759", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 158, "file_name": "example_vertex_ai_custom_job.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Custom Jobs operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"vertex_ai_custom_job_operations\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nREGION = \"us-central1\"\nCUSTOM_DISPLAY_NAME = f\"train-housing-custom-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"custom-housing-model-{ENV_ID}\"\n\nCUSTOM_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/california_housing_train.csv\"\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nCSV_ZIP_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"California-housing-custom-job.zip\")\nCSV_FILE_LOCAL_PATH = \"/custom-job/california_housing_train.csv\"\n\nTABULAR_DATASET = lambda bucket_name: {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\"input_config\": {\"gcs_source\": {\"uri\": [f\"gs://{bucket_name}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}}},\n        Value(),\n    ),\n}\n\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\n\nLOCAL_TRAINING_SCRIPT_PATH = \"/custom-job/california_housing_training_script.py\"\n\n\nwith models.DAG(\n    f\"{DAG_ID}_custom\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"custom_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=CUSTOM_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"mkdir -p /custom-job && unzip {CSV_ZIP_FILE_LOCAL_PATH} -d /custom-job/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=CUSTOM_GCS_BUCKET_NAME,\n    )\n    create_tabular_dataset = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET(CUSTOM_GCS_BUCKET_NAME),\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    tabular_dataset_id = create_tabular_dataset.output[\"dataset_id\"]\n\n    # [START how_to_cloud_vertex_ai_create_custom_training_job_operator]\n    create_custom_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"custom_task\",\n        staging_bucket=f\"gs://{CUSTOM_GCS_BUCKET_NAME}\",\n        display_name=CUSTOM_DISPLAY_NAME,\n        script_path=LOCAL_TRAINING_SCRIPT_PATH,\n        container_uri=CONTAINER_URI,\n        requirements=[\"gcsfs==0.7.1\"],\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=tabular_dataset_id,\n        replica_count=REPLICA_COUNT,\n        model_display_name=MODEL_DISPLAY_NAME,\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=create_custom_training_job.output[\"training_id\"],\n        custom_job_id=create_custom_training_job.output[\"custom_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n\n    delete_tabular_dataset = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=tabular_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=CUSTOM_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /custom-job/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_tabular_dataset\n        # TEST BODY\n        >> create_custom_training_job\n        # TEST TEARDOWN\n        >> delete_custom_training_job\n        >> delete_tabular_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.753514", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 173, "file_name": "example_vertex_ai_custom_job_python_package.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Custom Jobs operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomPythonPackageTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_custom_job_operations\"\nREGION = \"us-central1\"\nPACKAGE_DISPLAY_NAME = f\"train-housing-py-package-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"py-package-housing-model-{ENV_ID}\"\n\nCUSTOM_PYTHON_GCS_BUCKET_NAME = f\"bucket_python_{DAG_ID}_{ENV_ID}\"\n\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/california_housing_train.csv\"\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nCSV_ZIP_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"California-housing-python-package.zip\")\nCSV_FILE_LOCAL_PATH = \"/custom-job-python/california_housing_train.csv\"\nTAR_FILE_LOCAL_PATH = \"/custom-job-python/custom_trainer_script-0.1.tar\"\nFILES_TO_UPLOAD = [\n    CSV_FILE_LOCAL_PATH,\n    TAR_FILE_LOCAL_PATH,\n]\n\nTABULAR_DATASET = lambda bucket_name: {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\"input_config\": {\"gcs_source\": {\"uri\": [f\"gs://{bucket_name}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}}},\n        Value(),\n    ),\n}\n\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nTRAINING_FRACTION_SPLIT = 0.7\nTEST_FRACTION_SPLIT = 0.15\nVALIDATION_FRACTION_SPLIT = 0.15\n\nPYTHON_PACKAGE_GCS_URI = f\"gs://{CUSTOM_PYTHON_GCS_BUCKET_NAME}/vertex-ai/custom_trainer_script-0.1.tar\"\nPYTHON_MODULE_NAME = \"aiplatform_custom_trainer_script.task\"\n\n\nwith models.DAG(\n    f\"{DAG_ID}_python_package\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"custom_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=CUSTOM_PYTHON_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"mkdir -p /custom-job-python && unzip {CSV_ZIP_FILE_LOCAL_PATH} -d /custom-job-python/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=FILES_TO_UPLOAD,\n        dst=\"vertex-ai/\",\n        bucket=CUSTOM_PYTHON_GCS_BUCKET_NAME,\n    )\n    create_tabular_dataset = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET(CUSTOM_PYTHON_GCS_BUCKET_NAME),\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    tabular_dataset_id = create_tabular_dataset.output[\"dataset_id\"]\n\n    # [START how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n    create_custom_python_package_training_job = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"python_package_task\",\n        staging_bucket=f\"gs://{CUSTOM_PYTHON_GCS_BUCKET_NAME}\",\n        display_name=PACKAGE_DISPLAY_NAME,\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        container_uri=CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=tabular_dataset_id,\n        model_display_name=MODEL_DISPLAY_NAME,\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=create_custom_python_package_training_job.output[\"training_id\"],\n        custom_job_id=create_custom_python_package_training_job.output[\"custom_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_tabular_dataset = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=tabular_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=CUSTOM_PYTHON_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /custom-job-python/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_tabular_dataset\n        # TEST BODY\n        >> create_custom_python_package_training_job\n        # TEST TEARDOWN\n        >> delete_custom_training_job\n        >> delete_tabular_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.775101", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 289, "file_name": "example_vertex_ai_dataset.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Dataset operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ExportDataOperator,\n    GetDatasetOperator,\n    ImportDataOperator,\n    ListDatasetsOperator,\n    UpdateDatasetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_dataset_operations\"\nREGION = \"us-central1\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nALL_DATASETS_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"all-datasets.zip\")\n\nCSV_FILES_LOCAL_PATH = [\n    \"/all-datasets/forecast-dataset.csv\",\n    \"/all-datasets/image-dataset.csv\",\n    \"/all-datasets/tabular-dataset.csv\",\n    \"/all-datasets/text-dataset.csv\",\n    \"/all-datasets/video-dataset.csv\",\n]\n\nTIME_SERIES_DATASET = {\n    \"display_name\": f\"time-series-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.time_series,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/vertex-ai/forecast-dataset.csv\"]}\n            }\n        },\n        Value(),\n    ),\n}\nIMAGE_DATASET = {\n    \"display_name\": f\"image-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.image,\n    \"metadata\": Value(string_value=\"image-dataset\"),\n}\nTABULAR_DATASET = {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/vertex-ai/tabular-dataset.csv\"]}\n            }\n        },\n        Value(),\n    ),\n}\nTEXT_DATASET = {\n    \"display_name\": f\"text-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.text,\n    \"metadata\": Value(string_value=\"text-dataset\"),\n}\nVIDEO_DATASET = {\n    \"display_name\": f\"video-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.video,\n    \"metadata\": Value(string_value=\"video-dataset\"),\n}\nTEST_EXPORT_CONFIG = {\"gcs_destination\": {\"output_uri_prefix\": f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/exports\"}}\nTEST_IMPORT_CONFIG = [\n    {\n        \"data_item_labels\": {\n            \"test-labels-name\": \"test-labels-value\",\n        },\n        \"import_schema_uri\": (\n            \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_bounding_box_io_format_1.0.0.yaml\"\n        ),\n        \"gcs_source\": {\n            \"uris\": [\n                \"gs://system-tests-resources/vertex-ai/dataset/salads_oid_ml_use_public_unassigned.jsonl\"\n            ]\n        },\n    },\n]\nDATASET_TO_UPDATE = {\"display_name\": \"test-name\"}\nTEST_UPDATE_MASK = {\"paths\": [\"displayName\"]}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"dataset\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {ALL_DATASETS_ZIP_CSV_FILE_LOCAL_PATH} -d /all-datasets/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=CSV_FILES_LOCAL_PATH,\n        dst=\"vertex-ai/\",\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_dataset_operator]\n    create_image_dataset_job = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_tabular_dataset_job = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_text_dataset_job = CreateDatasetOperator(\n        task_id=\"text_dataset\",\n        dataset=TEXT_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_video_dataset_job = CreateDatasetOperator(\n        task_id=\"video_dataset\",\n        dataset=VIDEO_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_time_series_dataset_job = CreateDatasetOperator(\n        task_id=\"time_series_dataset\",\n        dataset=TIME_SERIES_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_dataset_operator]\n    delete_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=create_text_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_get_dataset_operator]\n    get_dataset = GetDatasetOperator(\n        task_id=\"get_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_tabular_dataset_job.output[\"dataset_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_export_data_operator]\n    export_data_job = ExportDataOperator(\n        task_id=\"export_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        export_config=TEST_EXPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_data_operator]\n\n    # [START how_to_cloud_vertex_ai_import_data_operator]\n    import_data_job = ImportDataOperator(\n        task_id=\"import_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=TEST_IMPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_import_data_operator]\n\n    # [START how_to_cloud_vertex_ai_list_dataset_operator]\n    list_dataset_job = ListDatasetsOperator(\n        task_id=\"list_dataset\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_update_dataset_operator]\n    update_dataset_job = UpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_video_dataset_job.output[\"dataset_id\"],\n        dataset=DATASET_TO_UPDATE,\n        update_mask=TEST_UPDATE_MASK,\n    )\n    # [END how_to_cloud_vertex_ai_update_dataset_operator]\n\n    delete_time_series_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_time_series_dataset\",\n        dataset_id=create_time_series_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_tabular_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=create_tabular_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_image_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_image_dataset\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_video_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_video_dataset\",\n        dataset_id=create_video_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /all-datasets/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        # TEST BODY\n        >> [\n            create_time_series_dataset_job >> delete_time_series_dataset_job,\n            create_text_dataset_job >> delete_dataset_job,\n            create_tabular_dataset_job >> get_dataset >> delete_tabular_dataset_job,\n            create_image_dataset_job >> import_data_job >> export_data_job >> delete_image_dataset_job,\n            create_video_dataset_job >> update_dataset_job >> delete_video_dataset_job,\n            list_dataset_job,\n        ]\n        # TEST TEARDOWN\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.780249", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 140, "file_name": "example_vertex_ai_hyperparameter_tuning_job.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Hyperparameter Tuning Job operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud import aiplatform\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.hyperparameter_tuning_job import (\n    CreateHyperparameterTuningJobOperator,\n    DeleteHyperparameterTuningJobOperator,\n    GetHyperparameterTuningJobOperator,\n    ListHyperparameterTuningJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_hyperparameter_tuning_job_operations\"\nREGION = \"us-central1\"\nDISPLAY_NAME = f\"hyperparameter-tuning-job-{ENV_ID}\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_hyperparameter_tuning_job_{ENV_ID}\"\nSTAGING_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nWORKER_POOL_SPECS = [\n    {\n        \"machine_spec\": {\n            \"machine_type\": MACHINE_TYPE,\n            \"accelerator_type\": ACCELERATOR_TYPE,\n            \"accelerator_count\": ACCELERATOR_COUNT,\n        },\n        \"replica_count\": REPLICA_COUNT,\n        \"container_spec\": {\n            \"image_uri\": f\"gcr.io/{PROJECT_ID}/horse-human:hypertune\",\n        },\n    }\n]\nPARAM_SPECS = {\n    \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.01, max=1, scale=\"log\"),\n    \"momentum\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale=\"linear\"),\n    \"num_neurons\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n        values=[64, 128, 512], scale=\"linear\"\n    ),\n}\nMETRIC_SPEC = {\n    \"accuracy\": \"maximize\",\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"hyperparameter_tuning_job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n\n    # [START how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n    create_hyperparameter_tuning_job = CreateHyperparameterTuningJobOperator(\n        task_id=\"create_hyperparameter_tuning_job\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=DISPLAY_NAME,\n        worker_pool_specs=WORKER_POOL_SPECS,\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n        parameter_spec=PARAM_SPECS,\n        metric_spec=METRIC_SPEC,\n        max_trial_count=15,\n        parallel_trial_count=3,\n    )\n    # [END how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n    get_hyperparameter_tuning_job = GetHyperparameterTuningJobOperator(\n        task_id=\"get_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n    delete_hyperparameter_tuning_job = DeleteHyperparameterTuningJobOperator(\n        task_id=\"delete_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n    list_hyperparameter_tuning_job = ListHyperparameterTuningJobOperator(\n        task_id=\"list_hyperparameter_tuning_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> create_hyperparameter_tuning_job\n        >> get_hyperparameter_tuning_job\n        >> delete_hyperparameter_tuning_job\n        >> list_hyperparameter_tuning_job\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.782826", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 233, "file_name": "example_vertex_ai_endpoint.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Endpoint Service operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLImageTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ImportDataOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.endpoint_service import (\n    CreateEndpointOperator,\n    DeleteEndpointOperator,\n    DeployModelOperator,\n    ListEndpointsOperator,\n    UndeployModelOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_endpoint_service_operations\"\nREGION = \"us-central1\"\nIMAGE_DISPLAY_NAME = f\"auto-ml-image-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"auto-ml-image-model-{ENV_ID}\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/image-dataset.csv\"\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nIMAGE_ZIP_CSV_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"image-dataset.csv.zip\")\nIMAGE_CSV_FILE_LOCAL_PATH = \"/endpoint/image-dataset.csv\"\n\nIMAGE_DATASET = {\n    \"display_name\": f\"image-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.image,\n    \"metadata\": Value(string_value=\"image-dataset\"),\n}\nIMAGE_DATA_CONFIG = [\n    {\n        \"import_schema_uri\": schema.dataset.ioformat.image.single_label_classification,\n        \"gcs_source\": {\"uris\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/vertex-ai/image-dataset.csv\"]},\n    },\n]\n\nENDPOINT_CONF = {\n    \"display_name\": f\"endpoint_test_{ENV_ID}\",\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    render_template_as_native_obj=True,\n    tags=[\"example\", \"vertex_ai\", \"endpoint_service\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"unzip {IMAGE_ZIP_CSV_FILE_LOCAL_PATH} -d /endpoint/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=IMAGE_CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n    )\n    create_image_dataset = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    image_dataset_id = create_image_dataset.output[\"dataset_id\"]\n\n    import_image_dataset = ImportDataOperator(\n        task_id=\"import_image_data\",\n        dataset_id=image_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=IMAGE_DATA_CONFIG,\n    )\n\n    create_auto_ml_image_training_job = CreateAutoMLImageTrainingJobOperator(\n        task_id=\"auto_ml_image_task\",\n        display_name=IMAGE_DISPLAY_NAME,\n        dataset_id=image_dataset_id,\n        prediction_type=\"classification\",\n        multi_label=False,\n        model_type=\"CLOUD\",\n        training_fraction_split=0.6,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.2,\n        budget_milli_node_hours=8000,\n        model_display_name=MODEL_DISPLAY_NAME,\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    DEPLOYED_MODEL = {\n        # format: 'projects/{project}/locations/{location}/models/{model}'\n        \"model\": \"{{ti.xcom_pull('auto_ml_image_task')['name']}}\",\n        \"display_name\": f\"temp_endpoint_test_{ENV_ID}\",\n        \"automatic_resources\": {\n            \"min_replica_count\": 1,\n            \"max_replica_count\": 1,\n        },\n    }\n\n    # [START how_to_cloud_vertex_ai_create_endpoint_operator]\n    create_endpoint = CreateEndpointOperator(\n        task_id=\"create_endpoint\",\n        endpoint=ENDPOINT_CONF,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_endpoint_operator]\n    delete_endpoint = DeleteEndpointOperator(\n        task_id=\"delete_endpoint\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_list_endpoints_operator]\n    list_endpoints = ListEndpointsOperator(\n        task_id=\"list_endpoints\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_endpoints_operator]\n\n    # [START how_to_cloud_vertex_ai_deploy_model_operator]\n    deploy_model = DeployModelOperator(\n        task_id=\"deploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model=DEPLOYED_MODEL,\n        traffic_split={\"0\": 100},\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_deploy_model_operator]\n\n    # [START how_to_cloud_vertex_ai_undeploy_model_operator]\n    undeploy_model = UndeployModelOperator(\n        task_id=\"undeploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model_id=deploy_model.output[\"deployed_model_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_undeploy_model_operator]\n\n    delete_auto_ml_image_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=create_auto_ml_image_training_job.output[\"training_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_image_dataset = DeleteDatasetOperator(\n        task_id=\"delete_image_dataset\",\n        dataset_id=image_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /endpoint/*\",\n    )\n\n    (\n        # TEST SETUP\n        [\n            create_bucket,\n            create_image_dataset,\n        ]\n        >> unzip_file\n        >> upload_files\n        >> import_image_dataset\n        >> create_auto_ml_image_training_job\n        # TEST BODY\n        >> create_endpoint\n        >> deploy_model\n        >> undeploy_model\n        >> delete_endpoint\n        >> list_endpoints\n        # TEST TEARDOWN\n        >> delete_auto_ml_image_training_job\n        >> delete_image_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.788916", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 228, "file_name": "example_vertex_ai_model_service.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Model Service operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.aiplatform import schema\nfrom google.protobuf.json_format import ParseDict\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.model_service import (\n    DeleteModelOperator,\n    ExportModelOperator,\n    ListModelsOperator,\n    UploadModelOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_model_service_operations\"\nREGION = \"us-central1\"\nTRAIN_DISPLAY_NAME = f\"train-housing-custom-{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"custom-housing-model-{ENV_ID}\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSTAGING_BUCKET = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}\"\n\nDATA_SAMPLE_GCS_OBJECT_NAME = \"vertex-ai/california_housing_train.csv\"\nCSV_FILE_LOCAL_PATH = \"/model_service/california_housing_train.csv\"\nRESOURCES_PATH = Path(__file__).parent / \"resources\"\nCSV_ZIP_FILE_LOCAL_PATH = str(RESOURCES_PATH / \"California-housing-ai-model.zip\")\n\nTABULAR_DATASET = {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": ParseDict(\n        {\n            \"input_config\": {\n                \"gcs_source\": {\"uri\": [f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/{DATA_SAMPLE_GCS_OBJECT_NAME}\"]}\n            }\n        },\n        Value(),\n    ),\n}\n\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\n\nLOCAL_TRAINING_SCRIPT_PATH = \"/model_service/california_housing_training_script.py\"\n\nMODEL_OUTPUT_CONFIG = {\n    \"artifact_destination\": {\n        \"output_uri_prefix\": STAGING_BUCKET,\n    },\n    \"export_format_id\": \"custom-trained\",\n}\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nMODEL_OBJ = {\n    \"display_name\": f\"model-{ENV_ID}\",\n    \"artifact_uri\": \"{{ti.xcom_pull('custom_task')['artifactUri']}}\",\n    \"container_spec\": {\n        \"image_uri\": MODEL_SERVING_CONTAINER_URI,\n        \"command\": [],\n        \"args\": [],\n        \"env\": [],\n        \"ports\": [],\n        \"predict_route\": \"\",\n        \"health_route\": \"\",\n    },\n}\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    render_template_as_native_obj=True,\n    tags=[\"example\", \"vertex_ai\", \"model_service\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        storage_class=\"REGIONAL\",\n        location=REGION,\n    )\n    unzip_file = BashOperator(\n        task_id=\"unzip_csv_data_file\",\n        bash_command=f\"mkdir -p /model_service && unzip {CSV_ZIP_FILE_LOCAL_PATH} -d /model_service/\",\n    )\n    upload_files = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_to_bucket\",\n        src=CSV_FILE_LOCAL_PATH,\n        dst=DATA_SAMPLE_GCS_OBJECT_NAME,\n        bucket=DATA_SAMPLE_GCS_BUCKET_NAME,\n    )\n    create_tabular_dataset = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    tabular_dataset_id = create_tabular_dataset.output[\"dataset_id\"]\n\n    create_custom_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"custom_task\",\n        staging_bucket=f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}\",\n        display_name=TRAIN_DISPLAY_NAME,\n        script_path=LOCAL_TRAINING_SCRIPT_PATH,\n        container_uri=CONTAINER_URI,\n        requirements=[\"gcsfs==0.7.1\"],\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=tabular_dataset_id,\n        replica_count=1,\n        model_display_name=MODEL_DISPLAY_NAME,\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n\n    # [START how_to_cloud_vertex_ai_upload_model_operator]\n    upload_model = UploadModelOperator(\n        task_id=\"upload_model\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        model=MODEL_OBJ,\n    )\n    # [END how_to_cloud_vertex_ai_upload_model_operator]\n\n    # [START how_to_cloud_vertex_ai_export_model_operator]\n    export_model = ExportModelOperator(\n        task_id=\"export_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n        output_config=MODEL_OUTPUT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_model_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_model_operator]\n    delete_model = DeleteModelOperator(\n        task_id=\"delete_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_cloud_vertex_ai_delete_model_operator]\n\n    # [START how_to_cloud_vertex_ai_list_models_operator]\n    list_models = ListModelsOperator(\n        task_id=\"list_models\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_models_operator]\n\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=create_custom_training_job.output[\"training_id\"],\n        custom_job_id=create_custom_training_job.output[\"custom_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_tabular_dataset = DeleteDatasetOperator(\n        task_id=\"delete_tabular_dataset\",\n        dataset_id=tabular_dataset_id,\n        region=REGION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    clear_folder = BashOperator(\n        task_id=\"clear_folder\",\n        bash_command=\"rm -r /model_service/*\",\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> unzip_file\n        >> upload_files\n        >> create_tabular_dataset\n        >> create_custom_training_job\n        # TEST BODY\n        >> upload_model\n        >> export_model\n        >> delete_model\n        >> list_models\n        # TEST TEARDOWN\n        >> delete_custom_training_job\n        >> delete_tabular_dataset\n        >> delete_bucket\n        >> clear_folder\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.806976", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 39, "file_name": "example_vertex_ai_list_custom_jobs.py"}, "content": "# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Custom Jobs operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import ListCustomTrainingJobOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"vertex_ai_custom_job_operations\"\nREGION = \"us-central1\"\n\nwith models.DAG(\n    f\"{DAG_ID}_list_custom_job\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"custom_job\"],\n) as dag:\n    # [START how_to_cloud_vertex_ai_list_custom_training_job_operator]\n    list_custom_training_job = ListCustomTrainingJobOperator(\n        task_id=\"list_custom_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_custom_training_job_operator]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.814871", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 151, "file_name": "example_video_intelligence.py"}, "content": "\"\"\"\nExample Airflow DAG that demonstrates operators for the Google Cloud Video Intelligence service in the Google\nCloud Platform.\n\nThis DAG relies on the following OS environment variables:\n\n* BUCKET_NAME - Google Cloud Storage bucket where the file exists.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.api_core.retry import Retry\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.video_intelligence import (\n    CloudVideoIntelligenceDetectVideoExplicitContentOperator,\n    CloudVideoIntelligenceDetectVideoLabelsOperator,\n    CloudVideoIntelligenceDetectVideoShotsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\nDAG_ID = \"example_gcp_video_intelligence\"\n\n# Public bucket holding the sample data\nBUCKET_NAME_SRC = \"cloud-samples-data\"\n# Path to the data inside the public bucket\nPATH_SRC = \"video/cat.mp4\"\n\n# [START howto_operator_video_intelligence_os_args]\nBUCKET_NAME_DST = f\"bucket-src-{DAG_ID}-{ENV_ID}\"\n# [END howto_operator_video_intelligence_os_args]\n\nFILE_NAME = \"video.mp4\"\n\n# [START howto_operator_video_intelligence_other_args]\nINPUT_URI = f\"gs://{BUCKET_NAME_DST}/{FILE_NAME}\"\n# [END howto_operator_video_intelligence_other_args]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME_DST)\n\n    copy_single_file = GCSToGCSOperator(\n        task_id=\"copy_single_gcs_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=PATH_SRC,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_video_intelligence_detect_labels]\n    detect_video_label = CloudVideoIntelligenceDetectVideoLabelsOperator(\n        input_uri=INPUT_URI,\n        output_uri=None,\n        video_context=None,\n        timeout=5,\n        task_id=\"detect_video_label\",\n    )\n    # [END howto_operator_video_intelligence_detect_labels]\n\n    # [START howto_operator_video_intelligence_detect_labels_result]\n    detect_video_label_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('detect_video_label')\"\n        \"['annotationResults'][0]['shotLabelAnnotations'][0]['entity']}}\",\n        task_id=\"detect_video_label_result\",\n    )\n    # [END howto_operator_video_intelligence_detect_labels_result]\n\n    # [START howto_operator_video_intelligence_detect_explicit_content]\n    detect_video_explicit_content = CloudVideoIntelligenceDetectVideoExplicitContentOperator(\n        input_uri=INPUT_URI,\n        output_uri=None,\n        video_context=None,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"detect_video_explicit_content\",\n    )\n    # [END howto_operator_video_intelligence_detect_explicit_content]\n\n    # [START howto_operator_video_intelligence_detect_explicit_content_result]\n    detect_video_explicit_content_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('detect_video_explicit_content')\"\n        \"['annotationResults'][0]['explicitAnnotation']['frames'][0]}}\",\n        task_id=\"detect_video_explicit_content_result\",\n    )\n    # [END howto_operator_video_intelligence_detect_explicit_content_result]\n\n    # [START howto_operator_video_intelligence_detect_video_shots]\n    detect_video_shots = CloudVideoIntelligenceDetectVideoShotsOperator(\n        input_uri=INPUT_URI,\n        output_uri=None,\n        video_context=None,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"detect_video_shots\",\n    )\n    # [END howto_operator_video_intelligence_detect_video_shots]\n\n    # [START howto_operator_video_intelligence_detect_video_shots_result]\n    detect_video_shots_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('detect_video_shots')\"\n        \"['annotationResults'][0]['shotAnnotations'][0]}}\",\n        task_id=\"detect_video_shots_result\",\n    )\n    # [END howto_operator_video_intelligence_detect_video_shots_result]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        # TEST SETUP\n        create_bucket,\n        copy_single_file,\n        # TEST BODY\n        detect_video_label,\n        detect_video_label_result,\n        detect_video_explicit_content,\n        detect_video_explicit_content_result,\n        detect_video_shots,\n        detect_video_shots_result,\n        # TEST TEARDOWN\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.817711", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 184, "file_name": "example_vision_annotate_image.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImageSafeSearchOperator,\n    CloudVisionDetectTextOperator,\n    CloudVisionImageAnnotateOperator,\n    CloudVisionTextDetectOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_retry_import]\nfrom google.api_core.retry import Retry  # isort:skip\n\n# [END howto_operator_vision_retry_import]\n\n# [START howto_operator_vision_enums_import]\nfrom google.cloud.vision_v1 import Feature  # isort:skip\n\n# [END howto_operator_vision_enums_import]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_vision_annotate_image\"\n\nLOCATION = \"europe-west1\"\n\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\n\nGCP_VISION_ANNOTATE_IMAGE_URL = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# [START howto_operator_vision_annotate_image_request]\nannotate_image_request = {\n    \"image\": {\"source\": {\"image_uri\": GCP_VISION_ANNOTATE_IMAGE_URL}},\n    \"features\": [{\"type_\": Feature.Type.LOGO_DETECTION}],\n}\n# [END howto_operator_vision_annotate_image_request]\n\n# [START howto_operator_vision_detect_image_param]\nDETECT_IMAGE = {\"source\": {\"image_uri\": GCP_VISION_ANNOTATE_IMAGE_URL}}\n# [END howto_operator_vision_detect_image_param]\n\n\n# Public bucket holding the sample data\nBUCKET_NAME_SRC = \"cloud-samples-data\"\n# Path to the data inside the public bucket\nPATH_SRC = \"vision/ocr/sign.jpg\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", project_id=PROJECT_ID, bucket_name=BUCKET_NAME\n    )\n\n    copy_single_file = GCSToGCSOperator(\n        task_id=\"copy_single_gcs_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=[PATH_SRC],\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_vision_annotate_image]\n    annotate_image = CloudVisionImageAnnotateOperator(\n        request=annotate_image_request,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"annotate_image\",\n    )\n    # [END howto_operator_vision_annotate_image]\n\n    # [START howto_operator_vision_annotate_image_result]\n    annotate_image_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('annotate_image')\"\n        \"['logoAnnotations'][0]['description'] }}\",\n        task_id=\"annotate_image_result\",\n    )\n    # [END howto_operator_vision_annotate_image_result]\n\n    # [START howto_operator_vision_detect_text]\n    detect_text = CloudVisionDetectTextOperator(\n        image=DETECT_IMAGE,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"detect_text\",\n        language_hints=\"en\",\n        web_detection_params={\"include_geo_results\": True},\n    )\n    # [END howto_operator_vision_detect_text]\n\n    # [START howto_operator_vision_detect_text_result]\n    detect_text_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('detect_text')['textAnnotations'][0] }}\",\n        task_id=\"detect_text_result\",\n    )\n    # [END howto_operator_vision_detect_text_result]\n\n    # [START howto_operator_vision_document_detect_text]\n    document_detect_text = CloudVisionTextDetectOperator(\n        image=DETECT_IMAGE, retry=Retry(maximum=10.0), timeout=5, task_id=\"document_detect_text\"\n    )\n    # [END howto_operator_vision_document_detect_text]\n\n    # [START howto_operator_vision_document_detect_text_result]\n    document_detect_text_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('document_detect_text')['textAnnotations'][0] }}\",\n        task_id=\"document_detect_text_result\",\n    )\n    # [END howto_operator_vision_document_detect_text_result]\n\n    # [START howto_operator_vision_detect_labels]\n    detect_labels = CloudVisionDetectImageLabelsOperator(\n        image=DETECT_IMAGE, retry=Retry(maximum=10.0), timeout=5, task_id=\"detect_labels\"\n    )\n    # [END howto_operator_vision_detect_labels]\n\n    # [START howto_operator_vision_detect_labels_result]\n    detect_labels_result = BashOperator(\n        bash_command=\"echo {{ task_instance.xcom_pull('detect_labels')['labelAnnotations'][0] }}\",\n        task_id=\"detect_labels_result\",\n    )\n    # [END howto_operator_vision_detect_labels_result]\n\n    # [START howto_operator_vision_detect_safe_search]\n    detect_safe_search = CloudVisionDetectImageSafeSearchOperator(\n        image=DETECT_IMAGE, retry=Retry(maximum=10.0), timeout=5, task_id=\"detect_safe_search\"\n    )\n    # [END howto_operator_vision_detect_safe_search]\n\n    # [START howto_operator_vision_detect_safe_search_result]\n    detect_safe_search_result = BashOperator(\n        bash_command=f\"echo {detect_safe_search.output}\",\n        task_id=\"detect_safe_search_result\",\n    )\n    # [END howto_operator_vision_detect_safe_search_result]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        copy_single_file,\n        annotate_image,\n        annotate_image_result,\n        detect_text,\n        detect_text_result,\n        document_detect_text,\n        document_detect_text_result,\n        detect_labels,\n        detect_labels_result,\n        detect_safe_search,\n        detect_safe_search_result,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.820663", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 260, "file_name": "example_vision_autogenerated.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductOperator,\n    CloudVisionUpdateProductSetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_retry_import]\nfrom google.api_core.retry import Retry  # isort:skip\n\n# [END howto_operator_vision_retry_import]\n# [START howto_operator_vision_product_set_import]\nfrom google.cloud.vision_v1.types import ProductSet  # isort:skip\n\n# [END howto_operator_vision_product_set_import]\n# [START howto_operator_vision_product_import]\nfrom google.cloud.vision_v1.types import Product  # isort:skip\n\n# [END howto_operator_vision_product_import]\n# [START howto_operator_vision_reference_image_import]\nfrom google.cloud.vision_v1.types import ReferenceImage  # isort:skip\n\n# [END howto_operator_vision_reference_image_import]\n# [START howto_operator_vision_enums_import]\nfrom google.cloud.vision_v1 import Feature  # isort:skip\n\n# [END howto_operator_vision_enums_import]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_vision_autogenerated_id\"\n\nLOCATION = \"europe-west1\"\n\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\n\nGCP_VISION_PRODUCT_SET_ID = \"product_set_explicit_id\"\nGCP_VISION_PRODUCT_ID = \"product_explicit_id\"\nGCP_VISION_REFERENCE_IMAGE_ID = f\"reference_image_explicit_id-{ENV_ID}\"\n\nVISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# [START howto_operator_vision_product_set]\nproduct_set = ProductSet(display_name=\"My Product Set\")\n# [END howto_operator_vision_product_set]\n\n# [START howto_operator_vision_product]\nproduct = Product(display_name=\"My Product 1\", product_category=\"toys\")\n# [END howto_operator_vision_product]\n\n# [START howto_operator_vision_reference_image]\nreference_image = ReferenceImage(uri=VISION_IMAGE_URL)\n# [END howto_operator_vision_reference_image]\n\n# [START howto_operator_vision_annotate_image_request]\nannotate_image_request = {\n    \"image\": {\"source\": {\"image_uri\": VISION_IMAGE_URL}},\n    \"features\": [{\"type_\": Feature.Type.LOGO_DETECTION}],\n}\n# [END howto_operator_vision_annotate_image_request]\n\n# [START howto_operator_vision_detect_image_param]\nDETECT_IMAGE = {\"source\": {\"image_uri\": VISION_IMAGE_URL}}\n# [END howto_operator_vision_detect_image_param]\n\n# Public bucket holding the sample data\nBUCKET_NAME_SRC = \"cloud-samples-data\"\n# Path to the data inside the public bucket\nPATH_SRC = \"vision/ocr/sign.jpg\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", project_id=PROJECT_ID, bucket_name=BUCKET_NAME\n    )\n\n    copy_single_file = GCSToGCSOperator(\n        task_id=\"copy_single_gcs_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=[PATH_SRC],\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_vision_product_set_create]\n    product_set_create = CloudVisionCreateProductSetOperator(\n        location=LOCATION,\n        product_set=product_set,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_set_create\",\n    )\n    # [END howto_operator_vision_product_set_create]\n\n    product_set_create_output = \"{{ task_instance.xcom_pull('product_set_create') }}\"\n\n    # [START howto_operator_vision_product_set_get]\n    product_set_get = CloudVisionGetProductSetOperator(\n        location=LOCATION,\n        product_set_id=product_set_create_output,\n        task_id=\"product_set_get\",\n    )\n    # [END howto_operator_vision_product_set_get]\n\n    # [START howto_operator_vision_product_set_update]\n    product_set_update = CloudVisionUpdateProductSetOperator(\n        location=LOCATION,\n        product_set_id=product_set_create_output,\n        product_set=ProductSet(display_name=\"My Product Set 2\"),\n        task_id=\"product_set_update\",\n    )\n    # [END howto_operator_vision_product_set_update]\n\n    # [START howto_operator_vision_product_set_delete]\n    product_set_delete = CloudVisionDeleteProductSetOperator(\n        location=LOCATION,\n        product_set_id=product_set_create_output,\n        task_id=\"product_set_delete\",\n    )\n    # [END howto_operator_vision_product_set_delete]\n\n    # [START howto_operator_vision_product_create]\n    product_create = CloudVisionCreateProductOperator(\n        location=LOCATION,\n        product=product,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_create\",\n    )\n    # [END howto_operator_vision_product_create]\n\n    # [START howto_operator_vision_product_get]\n    product_get = CloudVisionGetProductOperator(\n        location=LOCATION,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        task_id=\"product_get\",\n    )\n    # [END howto_operator_vision_product_get]\n\n    # [START howto_operator_vision_product_update]\n    product_update = CloudVisionUpdateProductOperator(\n        location=LOCATION,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        product=Product(display_name=\"My Product 2\", description=\"My updated description\"),\n        task_id=\"product_update\",\n    )\n    # [END howto_operator_vision_product_update]\n\n    # [START howto_operator_vision_product_delete]\n    product_delete = CloudVisionDeleteProductOperator(\n        location=LOCATION,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        task_id=\"product_delete\",\n    )\n    # [END howto_operator_vision_product_delete]\n\n    # [START howto_operator_vision_reference_image_create]\n    reference_image_create = CloudVisionCreateReferenceImageOperator(\n        location=LOCATION,\n        reference_image=reference_image,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"reference_image_create\",\n    )\n    # [END howto_operator_vision_reference_image_create]\n\n    # [START howto_operator_vision_reference_image_delete]\n    reference_image_delete = CloudVisionDeleteReferenceImageOperator(\n        location=LOCATION,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"reference_image_delete\",\n    )\n    # [END howto_operator_vision_reference_image_delete]\n\n    # [START howto_operator_vision_add_product_to_product_set]\n    add_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n        location=LOCATION,\n        product_set_id=product_set_create_output,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"add_product_to_product_set\",\n    )\n    # [END howto_operator_vision_add_product_to_product_set]\n\n    # [START howto_operator_vision_remove_product_from_product_set]\n    remove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator(\n        location=LOCATION,\n        product_set_id=product_set_create_output,\n        product_id=\"{{ task_instance.xcom_pull('product_create') }}\",\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"remove_product_from_product_set\",\n    )\n    # [END howto_operator_vision_remove_product_from_product_set]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        copy_single_file,\n        product_create,\n        product_get,\n        product_update,\n        product_set_create,\n        product_set_get,\n        product_set_update,\n        reference_image_create,\n        add_product_to_product_set,\n        reference_image_delete,\n        remove_product_from_product_set,\n        product_set_delete,\n        product_delete,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.836057", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 194, "file_name": "example_datacatalog_entries.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogGetEntryGroupOperator,\n    CloudDataCatalogGetEntryOperator,\n    CloudDataCatalogLookupEntryOperator,\n    CloudDataCatalogUpdateEntryOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_entries\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"id_{DAG_ID}_{ENV_ID}\"\nENTRY_GROUP_NAME = f\"name {DAG_ID} {ENV_ID}\"\nENTRY_ID = \"python_files\"\nENTRY_NAME = \"Wizard\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # Create\n    # [START howto_operator_gcp_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"display_name\": ENTRY_GROUP_NAME},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_group_result]\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\",\n        bash_command=f\"echo {XComArg(create_entry_group, key='entry_group_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group_result]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs]\n    create_entry_gcs = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry_gcs\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        entry={\n            \"display_name\": ENTRY_NAME,\n            \"type_\": \"FILESET\",\n            \"gcs_fileset_spec\": {\"file_patterns\": [f\"gs://{BUCKET_NAME}/**\"]},\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs_result]\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\",\n        bash_command=f\"echo {XComArg(create_entry_gcs, key='entry_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs_result]\n\n    # Get\n    # [START howto_operator_gcp_datacatalog_get_entry_group]\n    get_entry_group = CloudDataCatalogGetEntryGroupOperator(\n        task_id=\"get_entry_group\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        read_mask=FieldMask(paths=[\"name\", \"display_name\"]),\n    )\n    # [END howto_operator_gcp_datacatalog_get_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_get_entry_group_result]\n    get_entry_group_result = BashOperator(\n        task_id=\"get_entry_group_result\",\n        bash_command=f\"echo {get_entry_group.output}\",\n    )\n    # [END howto_operator_gcp_datacatalog_get_entry_group_result]\n\n    # [START howto_operator_gcp_datacatalog_get_entry]\n    get_entry = CloudDataCatalogGetEntryOperator(\n        task_id=\"get_entry\", location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n    )\n    # [END howto_operator_gcp_datacatalog_get_entry]\n\n    # [START howto_operator_gcp_datacatalog_get_entry_result]\n    get_entry_result = BashOperator(task_id=\"get_entry_result\", bash_command=f\"echo {get_entry.output}\")\n    # [END howto_operator_gcp_datacatalog_get_entry_result]\n\n    # Lookup\n    # [START howto_operator_gcp_datacatalog_lookup_entry_linked_resource]\n    current_entry_template = (\n        \"//datacatalog.googleapis.com/projects/{project_id}/locations/{location}/\"\n        \"entryGroups/{entry_group}/entries/{entry}\"\n    )\n    lookup_entry_linked_resource = CloudDataCatalogLookupEntryOperator(\n        task_id=\"lookup_entry\",\n        linked_resource=current_entry_template.format(\n            project_id=PROJECT_ID, location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n        ),\n    )\n    # [END howto_operator_gcp_datacatalog_lookup_entry_linked_resource]\n\n    # [START howto_operator_gcp_datacatalog_lookup_entry_result]\n    lookup_entry_result = BashOperator(\n        task_id=\"lookup_entry_result\",\n        bash_command=\"echo \\\"{{ task_instance.xcom_pull('lookup_entry')['display_name'] }}\\\"\",\n    )\n    # [END howto_operator_gcp_datacatalog_lookup_entry_result]\n\n    # Update\n    # [START howto_operator_gcp_datacatalog_update_entry]\n    update_entry = CloudDataCatalogUpdateEntryOperator(\n        task_id=\"update_entry\",\n        entry={\"display_name\": f\"{ENTRY_NAME} UPDATED\"},\n        update_mask={\"paths\": [\"display_name\"]},\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n    )\n    # [END howto_operator_gcp_datacatalog_update_entry]\n\n    # Delete\n    # [START howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\", location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\", location=LOCATION, entry_group=ENTRY_GROUP_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> create_entry_group\n        >> create_entry_group_result\n        >> get_entry_group\n        >> get_entry_group_result\n        >> create_entry_gcs\n        >> create_entry_gcs_result\n        >> get_entry\n        >> get_entry_result\n        >> lookup_entry_linked_resource\n        >> lookup_entry_result\n        >> update_entry\n        >> delete_entry\n        >> delete_entry_group\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.844372", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 272, "file_name": "example_vision_explicit.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductOperator,\n    CloudVisionUpdateProductSetOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_retry_import]\nfrom google.api_core.retry import Retry  # isort:skip\n\n# [END howto_operator_vision_retry_import]\n# [START howto_operator_vision_product_set_import_2]\nfrom google.cloud.vision_v1.types import ProductSet  # isort:skip\n\n# [END howto_operator_vision_product_set_import_2]\n# [START howto_operator_vision_product_import_2]\nfrom google.cloud.vision_v1.types import Product  # isort:skip\n\n# [END howto_operator_vision_product_import_2]\n# [START howto_operator_vision_reference_image_import_2]\nfrom google.cloud.vision_v1.types import ReferenceImage  # isort:skip\n\n# [END howto_operator_vision_reference_image_import_2]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_vision_explicit_id\"\n\nLOCATION = \"europe-west1\"\n\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\n\nGCP_VISION_PRODUCT_SET_ID = \"product_set_explicit_id\"\nGCP_VISION_PRODUCT_ID = \"product_explicit_id\"\nGCP_VISION_REFERENCE_IMAGE_ID = \"reference_image_explicit_id\"\n\nVISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# [START howto_operator_vision_product_set]\nproduct_set = ProductSet(display_name=\"My Product Set\")\n# [END howto_operator_vision_product_set]\n\n# [START howto_operator_vision_product]\nproduct = Product(display_name=\"My Product 1\", product_category=\"toys\")\n# [END howto_operator_vision_product]\n\n# [START howto_operator_vision_reference_image]\nreference_image = ReferenceImage(uri=VISION_IMAGE_URL)\n# [END howto_operator_vision_reference_image]\n\n# Public bucket holding the sample data\nBUCKET_NAME_SRC = \"cloud-samples-data\"\n# Path to the data inside the public bucket\nPATH_SRC = \"vision/ocr/sign.jpg\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", project_id=PROJECT_ID, bucket_name=BUCKET_NAME\n    )\n\n    copy_single_file = GCSToGCSOperator(\n        task_id=\"copy_single_gcs_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=[PATH_SRC],\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_vision_product_set_create_2]\n    product_set_create_2 = CloudVisionCreateProductSetOperator(\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        location=LOCATION,\n        product_set=product_set,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_set_create_2\",\n    )\n    # [END howto_operator_vision_product_set_create_2]\n\n    # Second 'create' task with the same product_set_id to demonstrate idempotence\n    product_set_create_2_idempotence = CloudVisionCreateProductSetOperator(\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        location=LOCATION,\n        product_set=product_set,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_set_create_2_idempotence\",\n    )\n\n    # [START howto_operator_vision_product_set_get_2]\n    product_set_get_2 = CloudVisionGetProductSetOperator(\n        location=LOCATION, product_set_id=GCP_VISION_PRODUCT_SET_ID, task_id=\"product_set_get_2\"\n    )\n    # [END howto_operator_vision_product_set_get_2]\n\n    # [START howto_operator_vision_product_set_update_2]\n    product_set_update_2 = CloudVisionUpdateProductSetOperator(\n        location=LOCATION,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        product_set=ProductSet(display_name=\"My Product Set 2\"),\n        task_id=\"product_set_update_2\",\n    )\n    # [END howto_operator_vision_product_set_update_2]\n\n    # [START howto_operator_vision_product_set_delete_2]\n    product_set_delete_2 = CloudVisionDeleteProductSetOperator(\n        location=LOCATION, product_set_id=GCP_VISION_PRODUCT_SET_ID, task_id=\"product_set_delete_2\"\n    )\n    # [END howto_operator_vision_product_set_delete_2]\n\n    # [START howto_operator_vision_product_create_2]\n    product_create_2 = CloudVisionCreateProductOperator(\n        product_id=GCP_VISION_PRODUCT_ID,\n        location=LOCATION,\n        product=product,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_create_2\",\n    )\n    # [END howto_operator_vision_product_create_2]\n\n    # Second 'create' task with the same product_id to demonstrate idempotence\n    product_create_2_idempotence = CloudVisionCreateProductOperator(\n        product_id=GCP_VISION_PRODUCT_ID,\n        location=LOCATION,\n        product=product,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"product_create_2_idempotence\",\n    )\n\n    # [START howto_operator_vision_product_get_2]\n    product_get_2 = CloudVisionGetProductOperator(\n        location=LOCATION, product_id=GCP_VISION_PRODUCT_ID, task_id=\"product_get_2\"\n    )\n    # [END howto_operator_vision_product_get_2]\n\n    # [START howto_operator_vision_product_update_2]\n    product_update_2 = CloudVisionUpdateProductOperator(\n        location=LOCATION,\n        product_id=GCP_VISION_PRODUCT_ID,\n        product=Product(display_name=\"My Product 2\", description=\"My updated description\"),\n        task_id=\"product_update_2\",\n    )\n    # [END howto_operator_vision_product_update_2]\n\n    # [START howto_operator_vision_product_delete_2]\n    product_delete_2 = CloudVisionDeleteProductOperator(\n        location=LOCATION, product_id=GCP_VISION_PRODUCT_ID, task_id=\"product_delete_2\"\n    )\n    # [END howto_operator_vision_product_delete_2]\n\n    # [START howto_operator_vision_reference_image_create_2]\n    reference_image_create_2 = CloudVisionCreateReferenceImageOperator(\n        location=LOCATION,\n        reference_image=reference_image,\n        product_id=GCP_VISION_PRODUCT_ID,\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"reference_image_create_2\",\n    )\n    # [END howto_operator_vision_reference_image_create_2]\n\n    # [START howto_operator_vision_reference_image_delete_2]\n    reference_image_delete_2 = CloudVisionDeleteReferenceImageOperator(\n        location=LOCATION,\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        product_id=GCP_VISION_PRODUCT_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"reference_image_delete_2\",\n    )\n    # [END howto_operator_vision_reference_image_delete_2]\n\n    # Second 'create' task with the same product_id to demonstrate idempotence\n    reference_image_create_2_idempotence = CloudVisionCreateReferenceImageOperator(\n        location=LOCATION,\n        reference_image=reference_image,\n        product_id=GCP_VISION_PRODUCT_ID,\n        reference_image_id=GCP_VISION_REFERENCE_IMAGE_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"reference_image_create_2_idempotence\",\n    )\n\n    # [START howto_operator_vision_add_product_to_product_set_2]\n    add_product_to_product_set_2 = CloudVisionAddProductToProductSetOperator(\n        location=LOCATION,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        product_id=GCP_VISION_PRODUCT_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"add_product_to_product_set_2\",\n    )\n    # [END howto_operator_vision_add_product_to_product_set_2]\n\n    # [START howto_operator_vision_remove_product_from_product_set_2]\n    remove_product_from_product_set_2 = CloudVisionRemoveProductFromProductSetOperator(\n        location=LOCATION,\n        product_set_id=GCP_VISION_PRODUCT_SET_ID,\n        product_id=GCP_VISION_PRODUCT_ID,\n        retry=Retry(maximum=10.0),\n        timeout=5,\n        task_id=\"remove_product_from_product_set_2\",\n    )\n    # [END howto_operator_vision_remove_product_from_product_set_2]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        copy_single_file,\n        product_set_create_2,\n        product_set_get_2,\n        product_set_update_2,\n        product_create_2,\n        product_create_2_idempotence,\n        product_get_2,\n        product_update_2,\n        reference_image_create_2,\n        reference_image_create_2_idempotence,\n        add_product_to_product_set_2,\n        remove_product_from_product_set_2,\n        reference_image_delete_2,\n        product_delete_2,\n        product_set_delete_2,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.848546", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 227, "file_name": "example_workflows.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.workflows import (\n    WorkflowsCancelExecutionOperator,\n    WorkflowsCreateExecutionOperator,\n    WorkflowsCreateWorkflowOperator,\n    WorkflowsDeleteWorkflowOperator,\n    WorkflowsGetExecutionOperator,\n    WorkflowsGetWorkflowOperator,\n    WorkflowsListExecutionsOperator,\n    WorkflowsListWorkflowsOperator,\n    WorkflowsUpdateWorkflowOperator,\n)\nfrom airflow.providers.google.cloud.sensors.workflows import WorkflowExecutionSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_workflows\"\n\nLOCATION = \"us-central1\"\nWORKFLOW_ID = f\"workflow-{DAG_ID}-{ENV_ID}\"\n\n# [START how_to_define_workflow]\nWORKFLOW_CONTENT = \"\"\"\n- getCurrentTime:\n    call: http.get\n    args:\n        url: https://us-central1-workflowsample.cloudfunctions.net/datetime\n    result: currentTime\n- readWikipedia:\n    call: http.get\n    args:\n        url: https://en.wikipedia.org/w/api.php\n        query:\n            action: opensearch\n            search: ${currentTime.body.dayOfTheWeek}\n    result: wikiResult\n- returnResult:\n    return: ${wikiResult.body[1]}\n\"\"\"\n\nWORKFLOW = {\n    \"description\": \"Test workflow\",\n    \"labels\": {\"airflow-version\": \"dev\"},\n    \"source_contents\": WORKFLOW_CONTENT,\n}\n# [END how_to_define_workflow]\n\nEXECUTION = {\"argument\": \"\"}\n\nSLEEP_WORKFLOW_ID = f\"sleep-workflow-{DAG_ID}-{ENV_ID}\"\nSLEEP_WORKFLOW_CONTENT = \"\"\"\n- someSleep:\n    call: sys.sleep\n    args:\n        seconds: 120\n\"\"\"\n\nSLEEP_WORKFLOW = {\n    \"description\": \"Test workflow\",\n    \"labels\": {\"airflow-version\": \"dev\"},\n    \"source_contents\": SLEEP_WORKFLOW_CONTENT,\n}\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START how_to_create_workflow]\n    create_workflow = WorkflowsCreateWorkflowOperator(\n        task_id=\"create_workflow\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow=WORKFLOW,\n        workflow_id=WORKFLOW_ID,\n    )\n    # [END how_to_create_workflow]\n\n    # [START how_to_update_workflow]\n    update_workflow = WorkflowsUpdateWorkflowOperator(\n        task_id=\"update_workflow\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_ID,\n        update_mask=FieldMask(paths=[\"name\", \"description\"]),\n    )\n    # [END how_to_update_workflow]\n\n    # [START how_to_get_workflow]\n    get_workflow = WorkflowsGetWorkflowOperator(\n        task_id=\"get_workflow\", location=LOCATION, project_id=PROJECT_ID, workflow_id=WORKFLOW_ID\n    )\n    # [END how_to_get_workflow]\n\n    # [START how_to_list_workflows]\n    list_workflows = WorkflowsListWorkflowsOperator(\n        task_id=\"list_workflows\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_list_workflows]\n\n    # [START how_to_delete_workflow]\n    delete_workflow = WorkflowsDeleteWorkflowOperator(\n        task_id=\"delete_workflow\", location=LOCATION, project_id=PROJECT_ID, workflow_id=WORKFLOW_ID\n    )\n    # [END how_to_delete_workflow]\n    delete_workflow.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START how_to_create_execution]\n    create_execution = WorkflowsCreateExecutionOperator(\n        task_id=\"create_execution\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        execution=EXECUTION,\n        workflow_id=WORKFLOW_ID,\n    )\n    # [END how_to_create_execution]\n\n    create_execution_id = cast(str, XComArg(create_execution, key=\"execution_id\"))\n\n    # [START how_to_wait_for_execution]\n    wait_for_execution = WorkflowExecutionSensor(\n        task_id=\"wait_for_execution\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_ID,\n        execution_id=create_execution_id,\n    )\n    # [END how_to_wait_for_execution]\n\n    # [START how_to_get_execution]\n    get_execution = WorkflowsGetExecutionOperator(\n        task_id=\"get_execution\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_ID,\n        execution_id=create_execution_id,\n    )\n    # [END how_to_get_execution]\n\n    # [START how_to_list_executions]\n    list_executions = WorkflowsListExecutionsOperator(\n        task_id=\"list_executions\", location=LOCATION, project_id=PROJECT_ID, workflow_id=WORKFLOW_ID\n    )\n    # [END how_to_list_executions]\n\n    create_workflow_for_cancel = WorkflowsCreateWorkflowOperator(\n        task_id=\"create_workflow_for_cancel\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow=SLEEP_WORKFLOW,\n        workflow_id=SLEEP_WORKFLOW_ID,\n    )\n\n    create_execution_for_cancel = WorkflowsCreateExecutionOperator(\n        task_id=\"create_execution_for_cancel\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        execution=EXECUTION,\n        workflow_id=SLEEP_WORKFLOW_ID,\n    )\n\n    cancel_execution_id = cast(str, XComArg(create_execution_for_cancel, key=\"execution_id\"))\n\n    # [START how_to_cancel_execution]\n    cancel_execution = WorkflowsCancelExecutionOperator(\n        task_id=\"cancel_execution\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow_id=SLEEP_WORKFLOW_ID,\n        execution_id=cancel_execution_id,\n    )\n    # [END how_to_cancel_execution]\n\n    delete_workflow_for_cancel = WorkflowsDeleteWorkflowOperator(\n        task_id=\"delete_workflow_for_cancel\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        workflow_id=SLEEP_WORKFLOW_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_workflow >> update_workflow >> [get_workflow, list_workflows]\n    update_workflow >> [create_execution, create_execution_for_cancel]\n\n    wait_for_execution >> [get_execution, list_executions]\n    (\n        create_workflow_for_cancel\n        >> create_execution_for_cancel\n        >> cancel_execution\n        >> delete_workflow_for_cancel\n    )\n\n    [cancel_execution, list_executions] >> delete_workflow\n\n    # Task dependencies created via `XComArgs`:\n    #   create_execution >> wait_for_execution\n    #   create_execution >> get_execution\n    #   create_execution >> cancel_execution\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.850737", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 215, "file_name": "example_datacatalog_search_catalog.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom google.cloud.datacatalog import TagField, TagTemplateField\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateTagOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteTagOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogSearchCatalogOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_search_catalog\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"id_{DAG_ID}_{ENV_ID}\"\nENTRY_GROUP_NAME = f\"name {DAG_ID} {ENV_ID}\"\nENTRY_ID = \"python_files\"\nENTRY_NAME = \"Wizard\"\nTEMPLATE_ID = f\"template_id_search_{ENV_ID}\"\nTAG_TEMPLATE_DISPLAY_NAME = f\"Data Catalog {DAG_ID} {ENV_ID}\"\nFIELD_NAME_1 = \"first\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # Create\n    # [START howto_operator_gcp_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"display_name\": ENTRY_GROUP_NAME},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_group_result]\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\",\n        bash_command=f\"echo {XComArg(create_entry_group, key='entry_group_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group_result]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs]\n    create_entry_gcs = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry_gcs\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        entry={\n            \"display_name\": ENTRY_NAME,\n            \"type_\": \"FILESET\",\n            \"gcs_fileset_spec\": {\"file_patterns\": [f\"gs://{BUCKET_NAME}/**\"]},\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs_result]\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\",\n        bash_command=f\"echo {XComArg(create_entry_gcs, key='entry_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs_result]\n\n    # [START howto_operator_gcp_datacatalog_create_tag]\n    create_tag = CloudDataCatalogCreateTagOperator(\n        task_id=\"create_tag\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry=ENTRY_ID,\n        template_id=TEMPLATE_ID,\n        tag={\"fields\": {FIELD_NAME_1: TagField(string_value=\"example-value-string\")}},\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag]\n\n    tag_id = cast(str, XComArg(create_tag, key=\"tag_id\"))\n\n    # [START howto_operator_gcp_datacatalog_create_tag_result]\n    create_tag_result = BashOperator(\n        task_id=\"create_tag_result\",\n        bash_command=f\"echo {tag_id}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_result]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template]\n    create_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create_tag_template\",\n        location=LOCATION,\n        tag_template_id=TEMPLATE_ID,\n        tag_template={\n            \"display_name\": TAG_TEMPLATE_DISPLAY_NAME,\n            \"fields\": {\n                FIELD_NAME_1: TagTemplateField(\n                    display_name=\"first-field\", type_=dict(primitive_type=\"STRING\")\n                )\n            },\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_result]\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\",\n        bash_command=f\"echo {XComArg(create_tag_template, key='tag_template_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_result]\n\n    # Search\n    # [START howto_operator_gcp_datacatalog_search_catalog]\n    search_catalog = CloudDataCatalogSearchCatalogOperator(\n        task_id=\"search_catalog\", scope={\"include_project_ids\": [PROJECT_ID]}, query=f\"projectid:{PROJECT_ID}\"\n    )\n    # [END howto_operator_gcp_datacatalog_search_catalog]\n\n    # [START howto_operator_gcp_datacatalog_search_catalog_result]\n    search_catalog_result = BashOperator(\n        task_id=\"search_catalog_result\",\n        bash_command=f\"echo {search_catalog.output}\",\n    )\n    # [END howto_operator_gcp_datacatalog_search_catalog_result]\n\n    # Delete\n    # [START howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\", location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\", location=LOCATION, entry_group=ENTRY_GROUP_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_tag]\n    delete_tag = CloudDataCatalogDeleteTagOperator(\n        task_id=\"delete_tag\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry=ENTRY_ID,\n        tag=tag_id,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag]\n    delete_tag.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n        task_id=\"delete_tag_template\", location=LOCATION, tag_template=TEMPLATE_ID, force=True\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> create_entry_group\n        >> create_entry_group_result\n        >> create_entry_gcs\n        >> create_entry_gcs_result\n        >> create_tag_template\n        >> create_tag_template_result\n        >> create_tag\n        >> create_tag_result\n        >> search_catalog\n        >> search_catalog_result\n        >> delete_tag\n        >> delete_tag_template\n        >> delete_entry\n        >> delete_entry_group\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.865470", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 55, "file_name": "example_leveldb.py"}, "content": "\"\"\"\nExample use of LevelDB operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.exceptions import AirflowOptionalProviderFeatureException\n\ntry:\n    from airflow.providers.google.leveldb.operators.leveldb import LevelDBOperator\nexcept AirflowOptionalProviderFeatureException:\n    pytest.skip(\"LevelDB not available\", allow_module_level=True)\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_leveldb\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_leveldb_get_key]\n    get_key_leveldb_task = LevelDBOperator(task_id=\"get_key_leveldb\", command=\"get\", key=b\"key\")\n    # [END howto_operator_leveldb_get_key]\n    # [START howto_operator_leveldb_put_key]\n    put_key_leveldb_task = LevelDBOperator(\n        task_id=\"put_key_leveldb\",\n        command=\"put\",\n        key=b\"another_key\",\n        value=b\"another_value\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_leveldb_put_key]\n    get_key_leveldb_task >> put_key_leveldb_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.877214", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 227, "file_name": "example_datacatalog_tags.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom google.cloud.datacatalog import TagField, TagTemplateField\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateTagOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteTagOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogListTagsOperator,\n    CloudDataCatalogUpdateTagOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_tags\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"id_{DAG_ID}_{ENV_ID}\"\nENTRY_GROUP_NAME = f\"name {DAG_ID} {ENV_ID}\"\nENTRY_ID = \"python_files\"\nENTRY_NAME = \"Wizard\"\nTEMPLATE_ID = f\"template_id_tags_{ENV_ID}\"\nTAG_TEMPLATE_DISPLAY_NAME = f\"Data Catalog {DAG_ID} {ENV_ID}\"\nFIELD_NAME_1 = \"first\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # Create\n    # [START howto_operator_gcp_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"display_name\": ENTRY_GROUP_NAME},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_group_result]\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\",\n        bash_command=f\"echo {XComArg(create_entry_group, key='entry_group_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group_result]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs]\n    create_entry_gcs = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry_gcs\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        entry={\n            \"display_name\": ENTRY_NAME,\n            \"type_\": \"FILESET\",\n            \"gcs_fileset_spec\": {\"file_patterns\": [f\"gs://{BUCKET_NAME}/**\"]},\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs_result]\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\",\n        bash_command=f\"echo {XComArg(create_entry_gcs, key='entry_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs_result]\n\n    # [START howto_operator_gcp_datacatalog_create_tag]\n    create_tag = CloudDataCatalogCreateTagOperator(\n        task_id=\"create_tag\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry=ENTRY_ID,\n        template_id=TEMPLATE_ID,\n        tag={\"fields\": {FIELD_NAME_1: TagField(string_value=\"example-value-string\")}},\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag]\n\n    tag_id = cast(str, XComArg(create_tag, key=\"tag_id\"))\n\n    # [START howto_operator_gcp_datacatalog_create_tag_result]\n    create_tag_result = BashOperator(\n        task_id=\"create_tag_result\",\n        bash_command=f\"echo {tag_id}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_result]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template]\n    create_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create_tag_template\",\n        location=LOCATION,\n        tag_template_id=TEMPLATE_ID,\n        tag_template={\n            \"display_name\": TAG_TEMPLATE_DISPLAY_NAME,\n            \"fields\": {\n                FIELD_NAME_1: TagTemplateField(\n                    display_name=\"first-field\", type_=dict(primitive_type=\"STRING\")\n                )\n            },\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_result]\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\",\n        bash_command=f\"echo {XComArg(create_tag_template, key='tag_template_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_result]\n\n    # List\n    # [START howto_operator_gcp_datacatalog_list_tags]\n    list_tags = CloudDataCatalogListTagsOperator(\n        task_id=\"list_tags\", location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n    )\n    # [END howto_operator_gcp_datacatalog_list_tags]\n\n    # [START howto_operator_gcp_datacatalog_list_tags_result]\n    list_tags_result = BashOperator(task_id=\"list_tags_result\", bash_command=f\"echo {list_tags.output}\")\n    # [END howto_operator_gcp_datacatalog_list_tags_result]\n\n    # Update\n    # [START howto_operator_gcp_datacatalog_update_tag]\n    update_tag = CloudDataCatalogUpdateTagOperator(\n        task_id=\"update_tag\",\n        tag={\"fields\": {FIELD_NAME_1: TagField(string_value=\"new-value-string\")}},\n        update_mask={\"paths\": [\"fields\"]},\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry=ENTRY_ID,\n        tag_id=tag_id,\n    )\n    # [END howto_operator_gcp_datacatalog_update_tag]\n\n    # # Delete\n    # [START howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\", location=LOCATION, entry_group=ENTRY_GROUP_ID, entry=ENTRY_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\", location=LOCATION, entry_group=ENTRY_GROUP_ID\n    )\n    # [END howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_tag]\n    delete_tag = CloudDataCatalogDeleteTagOperator(\n        task_id=\"delete_tag\",\n        location=LOCATION,\n        entry_group=ENTRY_GROUP_ID,\n        entry=ENTRY_ID,\n        tag=tag_id,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag]\n    delete_tag.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n        task_id=\"delete_tag_template\", location=LOCATION, tag_template=TEMPLATE_ID, force=True\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> create_entry_group\n        >> create_entry_group_result\n        >> create_entry_gcs\n        >> create_entry_gcs_result\n        >> create_tag_template\n        >> create_tag_template_result\n        >> create_tag\n        >> create_tag_result\n        >> list_tags\n        >> list_tags_result\n        >> update_tag\n        >> delete_tag\n        >> delete_tag_template\n        >> delete_entry\n        >> delete_entry_group\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.884618", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 177, "file_name": "example_datacatalog_tag_templates.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.datacatalog import FieldType, TagTemplateField\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateTagTemplateFieldOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteTagTemplateFieldOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogGetTagTemplateOperator,\n    CloudDataCatalogRenameTagTemplateFieldOperator,\n    CloudDataCatalogUpdateTagTemplateFieldOperator,\n    CloudDataCatalogUpdateTagTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_tag_templates\"\n\nLOCATION = \"us-central1\"\nTEMPLATE_ID = f\"template_id_templ_{ENV_ID}\"\nTAG_TEMPLATE_DISPLAY_NAME = f\"Data Catalog {DAG_ID} {ENV_ID}\"\nFIELD_NAME_1 = \"first\"\nFIELD_NAME_2 = \"second\"\nFIELD_NAME_3 = \"first-rename\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # Create\n    # [START howto_operator_gcp_datacatalog_create_tag_template]\n    create_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create_tag_template\",\n        location=LOCATION,\n        tag_template_id=TEMPLATE_ID,\n        tag_template={\n            \"display_name\": TAG_TEMPLATE_DISPLAY_NAME,\n            \"fields\": {\n                FIELD_NAME_1: TagTemplateField(\n                    display_name=\"first-field\", type_=dict(primitive_type=\"STRING\")\n                )\n            },\n        },\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_result]\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\",\n        bash_command=f\"echo {XComArg(create_tag_template, key='tag_template_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_result]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_field]\n    create_tag_template_field = CloudDataCatalogCreateTagTemplateFieldOperator(\n        task_id=\"create_tag_template_field\",\n        location=LOCATION,\n        tag_template=TEMPLATE_ID,\n        tag_template_field_id=FIELD_NAME_2,\n        tag_template_field=TagTemplateField(\n            display_name=\"second-field\", type_=FieldType(primitive_type=\"STRING\")\n        ),\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_field]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_field_result]\n    create_tag_template_field_result = BashOperator(\n        task_id=\"create_tag_template_field_result\",\n        bash_command=f\"echo {XComArg(create_tag_template_field, key='tag_template_field_id')}\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_field_result]\n\n    # Get\n    # [START howto_operator_gcp_datacatalog_get_tag_template]\n    get_tag_template = CloudDataCatalogGetTagTemplateOperator(\n        task_id=\"get_tag_template\", location=LOCATION, tag_template=TEMPLATE_ID\n    )\n    # [END howto_operator_gcp_datacatalog_get_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_get_tag_template_result]\n    get_tag_template_result = BashOperator(\n        task_id=\"get_tag_template_result\",\n        bash_command=f\"echo {get_tag_template.output}\",\n    )\n    # [END howto_operator_gcp_datacatalog_get_tag_template_result]\n\n    # Rename\n    # [START howto_operator_gcp_datacatalog_rename_tag_template_field]\n    rename_tag_template_field = CloudDataCatalogRenameTagTemplateFieldOperator(\n        task_id=\"rename_tag_template_field\",\n        location=LOCATION,\n        tag_template=TEMPLATE_ID,\n        field=FIELD_NAME_1,\n        new_tag_template_field_id=FIELD_NAME_3,\n    )\n    # [END howto_operator_gcp_datacatalog_rename_tag_template_field]\n\n    # Update\n    # [START howto_operator_gcp_datacatalog_update_tag_template]\n    update_tag_template = CloudDataCatalogUpdateTagTemplateOperator(\n        task_id=\"update_tag_template\",\n        tag_template={\"display_name\": f\"{TAG_TEMPLATE_DISPLAY_NAME} UPDATED\"},\n        update_mask={\"paths\": [\"display_name\"]},\n        location=LOCATION,\n        tag_template_id=TEMPLATE_ID,\n    )\n    # [END howto_operator_gcp_datacatalog_update_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_update_tag_template_field]\n    update_tag_template_field = CloudDataCatalogUpdateTagTemplateFieldOperator(\n        task_id=\"update_tag_template_field\",\n        tag_template_field={\"display_name\": \"Updated template field\"},\n        update_mask={\"paths\": [\"display_name\"]},\n        location=LOCATION,\n        tag_template=TEMPLATE_ID,\n        tag_template_field_id=FIELD_NAME_1,\n    )\n    # [END howto_operator_gcp_datacatalog_update_tag_template_field]\n\n    # Delete\n    # [START howto_operator_gcp_datacatalog_delete_tag_template_field]\n    delete_tag_template_field = CloudDataCatalogDeleteTagTemplateFieldOperator(\n        task_id=\"delete_tag_template_field\",\n        location=LOCATION,\n        tag_template=TEMPLATE_ID,\n        field=FIELD_NAME_2,\n        force=True,\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag_template_field]\n    delete_tag_template_field.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n        task_id=\"delete_tag_template\", location=LOCATION, tag_template=TEMPLATE_ID, force=True\n    )\n    # [END howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_tag_template.trigger_rule = TriggerRule.ALL_DONE\n\n    (\n        # TEST BODY\n        create_tag_template\n        >> create_tag_template_result\n        >> create_tag_template_field\n        >> create_tag_template_field_result\n        >> get_tag_template\n        >> get_tag_template_result\n        >> update_tag_template\n        >> update_tag_template_field\n        >> rename_tag_template_field\n        >> delete_tag_template_field\n        >> delete_tag_template\n    )\n\n    # ### Everything below this line is not part of example ###\n    # ### Just for system tests purpose ###\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.887832", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 84, "file_name": "example_analytics.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use Google Analytics 360.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.marketing_platform.operators.analytics import (\n    GoogleAnalyticsDataImportUploadOperator,\n    GoogleAnalyticsDeletePreviousDataUploadsOperator,\n    GoogleAnalyticsGetAdsLinkOperator,\n    GoogleAnalyticsListAccountsOperator,\n    GoogleAnalyticsModifyFileHeadersDataImportOperator,\n    GoogleAnalyticsRetrieveAdsLinksListOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_google_analytics\"\n\nACCOUNT_ID = os.environ.get(\"GA_ACCOUNT_ID\", \"123456789\")\n\nBUCKET = os.environ.get(\"GMP_ANALYTICS_BUCKET\", \"test-airflow-analytics-bucket\")\nBUCKET_FILENAME = \"data.csv\"\nWEB_PROPERTY_ID = os.environ.get(\"GA_WEB_PROPERTY\", \"UA-12345678-1\")\nWEB_PROPERTY_AD_WORDS_LINK_ID = os.environ.get(\"GA_WEB_PROPERTY_AD_WORDS_LINK_ID\", \"rQafFTPOQdmkx4U-fxUfhj\")\nDATA_ID = \"kjdDu3_tQa6n8Q1kXFtSmg\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"analytics\"],\n) as dag:\n    # [START howto_marketing_platform_list_accounts_operator]\n    list_account = GoogleAnalyticsListAccountsOperator(task_id=\"list_account\")\n    # [END howto_marketing_platform_list_accounts_operator]\n\n    # [START howto_marketing_platform_get_ads_link_operator]\n    get_ad_words_link = GoogleAnalyticsGetAdsLinkOperator(\n        web_property_ad_words_link_id=WEB_PROPERTY_AD_WORDS_LINK_ID,\n        web_property_id=WEB_PROPERTY_ID,\n        account_id=ACCOUNT_ID,\n        task_id=\"get_ad_words_link\",\n    )\n    # [END howto_marketing_platform_get_ads_link_operator]\n\n    # [START howto_marketing_platform_retrieve_ads_links_list_operator]\n    list_ad_words_link = GoogleAnalyticsRetrieveAdsLinksListOperator(\n        task_id=\"list_ad_link\", account_id=ACCOUNT_ID, web_property_id=WEB_PROPERTY_ID\n    )\n    # [END howto_marketing_platform_retrieve_ads_links_list_operator]\n\n    upload = GoogleAnalyticsDataImportUploadOperator(\n        task_id=\"upload\",\n        storage_bucket=BUCKET,\n        storage_name_object=BUCKET_FILENAME,\n        account_id=ACCOUNT_ID,\n        web_property_id=WEB_PROPERTY_ID,\n        custom_data_source_id=DATA_ID,\n    )\n\n    delete = GoogleAnalyticsDeletePreviousDataUploadsOperator(\n        task_id=\"delete\",\n        account_id=ACCOUNT_ID,\n        web_property_id=WEB_PROPERTY_ID,\n        custom_data_source_id=DATA_ID,\n    )\n\n    transform = GoogleAnalyticsModifyFileHeadersDataImportOperator(\n        task_id=\"transform\",\n        storage_bucket=BUCKET,\n        storage_name_object=BUCKET_FILENAME,\n    )\n\n    upload >> [delete, transform]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.895429", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 199, "file_name": "example_campaign_manager.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use CampaignManager.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport time\nimport uuid\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.marketing_platform.operators.campaign_manager import (\n    GoogleCampaignManagerBatchInsertConversionsOperator,\n    GoogleCampaignManagerBatchUpdateConversionsOperator,\n    GoogleCampaignManagerDeleteReportOperator,\n    GoogleCampaignManagerDownloadReportOperator,\n    GoogleCampaignManagerInsertReportOperator,\n    GoogleCampaignManagerRunReportOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.campaign_manager import (\n    GoogleCampaignManagerReportSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_campaign_manager\"\n\nPROFILE_ID = os.environ.get(\"MARKETING_PROFILE_ID\", \"123456789\")\nFLOODLIGHT_ACTIVITY_ID = int(os.environ.get(\"FLOODLIGHT_ACTIVITY_ID\", 12345))\nFLOODLIGHT_CONFIGURATION_ID = int(os.environ.get(\"FLOODLIGHT_CONFIGURATION_ID\", 12345))\nENCRYPTION_ENTITY_ID = int(os.environ.get(\"ENCRYPTION_ENTITY_ID\", 12345))\nDEVICE_ID = os.environ.get(\"DEVICE_ID\", \"12345\")\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nREPORT_NAME = f\"report_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = f\"file_{DAG_ID}_{ENV_ID}\"\nACCOUNT_ID = f\"account_{DAG_ID}_{ENV_ID}\"\nFORMAT = \"CSV\"\n\n# For more information, please check\n# https://developers.google.com/doubleclick-advertisers/rest/v4/reports#type\nREPORT = {\n    \"kind\": \"dfareporting#report\",\n    \"type\": \"STANDARD\",\n    \"name\": REPORT_NAME,\n    \"fileName\": FILE_NAME,\n    \"accountId\": ACCOUNT_ID,\n    \"format\": FORMAT,\n    \"criteria\": {\n        \"dateRange\": {\n            \"kind\": \"dfareporting#dateRange\",\n            \"relativeDateRange\": \"LAST_365_DAYS\",\n        },\n        \"dimensions\": [{\"kind\": \"dfareporting#sortedDimension\", \"name\": \"campaign\"}],\n        \"metricNames\": [\"activeViewImpressionDistributionViewable\"],\n    },\n}\n\n# For more information, please check\n# https://developers.google.com/doubleclick-advertisers/rest/v4/Conversion\nCONVERSION = {\n    \"kind\": \"dfareporting#conversion\",\n    \"floodlightActivityId\": FLOODLIGHT_ACTIVITY_ID,\n    \"floodlightConfigurationId\": FLOODLIGHT_CONFIGURATION_ID,\n    \"mobileDeviceId\": DEVICE_ID,\n    \"ordinal\": \"0\",\n    \"quantity\": 42,\n    \"value\": 123.4,\n    \"timestampMicros\": int(time.time()) * 1000000,\n    \"customVariables\": [\n        {\n            \"kind\": \"dfareporting#customFloodlightVariable\",\n            \"type\": \"U4\",\n            \"value\": \"value\",\n        }\n    ],\n}\n\nCONVERSION_UPDATE = {\n    \"kind\": \"dfareporting#conversion\",\n    \"floodlightActivityId\": FLOODLIGHT_ACTIVITY_ID,\n    \"floodlightConfigurationId\": FLOODLIGHT_CONFIGURATION_ID,\n    \"mobileDeviceId\": DEVICE_ID,\n    \"ordinal\": \"0\",\n    \"quantity\": 42,\n    \"value\": 123.4,\n}\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"campaign\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    # [START howto_campaign_manager_insert_report_operator]\n    create_report = GoogleCampaignManagerInsertReportOperator(\n        profile_id=PROFILE_ID, report=REPORT, task_id=\"create_report\"\n    )\n    report_id = cast(str, XComArg(create_report, key=\"report_id\"))\n    # [END howto_campaign_manager_insert_report_operator]\n\n    # [START howto_campaign_manager_run_report_operator]\n    run_report = GoogleCampaignManagerRunReportOperator(\n        profile_id=PROFILE_ID, report_id=report_id, task_id=\"run_report\"\n    )\n    file_id = cast(str, XComArg(run_report, key=\"file_id\"))\n    # [END howto_campaign_manager_run_report_operator]\n\n    # [START howto_campaign_manager_wait_for_operation]\n    wait_for_report = GoogleCampaignManagerReportSensor(\n        task_id=\"wait_for_report\",\n        profile_id=PROFILE_ID,\n        report_id=report_id,\n        file_id=file_id,\n    )\n    # [END howto_campaign_manager_wait_for_operation]\n\n    # [START howto_campaign_manager_get_report_operator]\n    report_name = f\"reports/report_{str(uuid.uuid1())}\"\n    get_report = GoogleCampaignManagerDownloadReportOperator(\n        task_id=\"get_report\",\n        profile_id=PROFILE_ID,\n        report_id=report_id,\n        file_id=file_id,\n        report_name=report_name,\n        bucket_name=BUCKET_NAME,\n    )\n    # [END howto_campaign_manager_get_report_operator]\n\n    # [START howto_campaign_manager_delete_report_operator]\n    delete_report = GoogleCampaignManagerDeleteReportOperator(\n        profile_id=PROFILE_ID,\n        report_name=REPORT_NAME,\n        task_id=\"delete_report\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_campaign_manager_delete_report_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # [START howto_campaign_manager_insert_conversions]\n    insert_conversion = GoogleCampaignManagerBatchInsertConversionsOperator(\n        task_id=\"insert_conversion\",\n        profile_id=PROFILE_ID,\n        conversions=[CONVERSION],\n        encryption_source=\"AD_SERVING\",\n        encryption_entity_type=\"DCM_ADVERTISER\",\n        encryption_entity_id=ENCRYPTION_ENTITY_ID,\n    )\n    # [END howto_campaign_manager_insert_conversions]\n\n    # [START howto_campaign_manager_update_conversions]\n    update_conversion = GoogleCampaignManagerBatchUpdateConversionsOperator(\n        task_id=\"update_conversion\",\n        profile_id=PROFILE_ID,\n        conversions=[CONVERSION_UPDATE],\n        encryption_source=\"AD_SERVING\",\n        encryption_entity_type=\"DCM_ADVERTISER\",\n        encryption_entity_id=ENCRYPTION_ENTITY_ID,\n        max_failed_updates=1,\n    )\n    # [END howto_campaign_manager_update_conversions]\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_report\n        # TEST BODY\n        >> run_report\n        >> wait_for_report\n        >> get_report\n        >> insert_conversion\n        >> update_conversion\n        # TEST TEARDOWN\n        >> delete_report\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.908746", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 68, "file_name": "example_local_to_drive.py"}, "content": "\"\"\"\nExample DAG using LocalFilesystemToGoogleDriveOperator.\n\nUsing this operator requires the following additional scopes:\nhttps://www.googleapis.com/auth/drive\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import models\nfrom airflow.providers.google.suite.transfers.local_to_drive import LocalFilesystemToGoogleDriveOperator\n\nDAG_ID = \"example_local_to_drive\"\n\nFILE_NAME_1 = \"test1\"\nFILE_NAME_2 = \"test2\"\n\nLOCAL_PATH = str(Path(__file__).parent / \"resources\")\n\nSINGLE_FILE_LOCAL_PATHS = [str(Path(LOCAL_PATH) / FILE_NAME_1)]\nMULTIPLE_FILES_LOCAL_PATHS = [str(Path(LOCAL_PATH) / FILE_NAME_1), str(Path(LOCAL_PATH) / FILE_NAME_2)]\n\nDRIVE_FOLDER = \"test-folder\"\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_local_to_drive_upload_single_file]\n    upload_single_file = LocalFilesystemToGoogleDriveOperator(\n        task_id=\"upload_single_file\",\n        local_paths=SINGLE_FILE_LOCAL_PATHS,\n        drive_folder=DRIVE_FOLDER,\n    )\n    # [END howto_operator_local_to_drive_upload_single_file]\n\n    # [START howto_operator_local_to_drive_upload_multiple_files]\n    upload_multiple_files = LocalFilesystemToGoogleDriveOperator(\n        task_id=\"upload_multiple_files\",\n        local_paths=MULTIPLE_FILES_LOCAL_PATHS,\n        drive_folder=DRIVE_FOLDER,\n        ignore_if_missing=True,\n    )\n    # [END howto_operator_local_to_drive_upload_multiple_files]\n\n    (\n        # TEST BODY\n        upload_single_file\n        >> upload_multiple_files\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.910858", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 72, "file_name": "example_search_ads.py"}, "content": "\"\"\"\nExample Airflow DAG that shows how to use SearchAds.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.marketing_platform.operators.search_ads import (\n    GoogleSearchAdsDownloadReportOperator,\n    GoogleSearchAdsInsertReportOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.search_ads import GoogleSearchAdsReportSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_search_ads\"\n\n# [START howto_search_ads_env_variables]\nAGENCY_ID = os.environ.get(\"GMP_AGENCY_ID\")\nADVERTISER_ID = os.environ.get(\"GMP_ADVERTISER_ID\")\nGCS_BUCKET = os.environ.get(\"GMP_GCS_BUCKET\", \"test-cm-bucket\")\n\nREPORT = {\n    \"reportScope\": {\"agencyId\": AGENCY_ID, \"advertiserId\": ADVERTISER_ID},\n    \"reportType\": \"account\",\n    \"columns\": [{\"columnName\": \"agency\"}, {\"columnName\": \"lastModifiedTimestamp\"}],\n    \"includeRemovedEntities\": False,\n    \"statisticsCurrency\": \"usd\",\n    \"maxRowsPerFile\": 1000000,\n    \"downloadFormat\": \"csv\",\n}\n# [END howto_search_ads_env_variables]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_search_ads_generate_report_operator]\n    generate_report = GoogleSearchAdsInsertReportOperator(report=REPORT, task_id=\"generate_report\")\n    # [END howto_search_ads_generate_report_operator]\n\n    # [START howto_search_ads_get_report_id]\n    report_id = cast(str, XComArg(generate_report, key=\"report_id\"))\n    # [END howto_search_ads_get_report_id]\n\n    # [START howto_search_ads_get_report_operator]\n    wait_for_report = GoogleSearchAdsReportSensor(report_id=report_id, task_id=\"wait_for_report\")\n    # [END howto_search_ads_get_report_operator]\n\n    # [START howto_search_ads_getfile_report_operator]\n    download_report = GoogleSearchAdsDownloadReportOperator(\n        report_id=report_id, bucket_name=GCS_BUCKET, task_id=\"download_report\"\n    )\n    # [END howto_search_ads_getfile_report_operator]\n\n    wait_for_report >> download_report\n\n    # Task dependencies created via `XComArgs`:\n    #   generate_report >> wait_for_report\n    #   generate_report >> download_report\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.912671", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 102, "file_name": "example_http.py"}, "content": "\"\"\"Example HTTP operator and sensor\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nfrom airflow.providers.http.sensors.http import HttpSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_http_operator\"\n\n\ndag = DAG(\n    DAG_ID,\n    default_args={\"retries\": 1},\n    tags=[\"example\"],\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n)\n\ndag.doc_md = __doc__\n\n# task_post_op, task_get_op and task_put_op are examples of tasks created by instantiating operators\n# [START howto_operator_http_task_post_op]\ntask_post_op = SimpleHttpOperator(\n    task_id=\"post_op\",\n    endpoint=\"post\",\n    data=json.dumps({\"priority\": 5}),\n    headers={\"Content-Type\": \"application/json\"},\n    response_check=lambda response: response.json()[\"json\"][\"priority\"] == 5,\n    dag=dag,\n)\n# [END howto_operator_http_task_post_op]\n# [START howto_operator_http_task_post_op_formenc]\ntask_post_op_formenc = SimpleHttpOperator(\n    task_id=\"post_op_formenc\",\n    endpoint=\"post\",\n    data=\"name=Joe\",\n    headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n    dag=dag,\n)\n# [END howto_operator_http_task_post_op_formenc]\n# [START howto_operator_http_task_get_op]\ntask_get_op = SimpleHttpOperator(\n    task_id=\"get_op\",\n    method=\"GET\",\n    endpoint=\"get\",\n    data={\"param1\": \"value1\", \"param2\": \"value2\"},\n    headers={},\n    dag=dag,\n)\n# [END howto_operator_http_task_get_op]\n# [START howto_operator_http_task_get_op_response_filter]\ntask_get_op_response_filter = SimpleHttpOperator(\n    task_id=\"get_op_response_filter\",\n    method=\"GET\",\n    endpoint=\"get\",\n    response_filter=lambda response: response.json()[\"nested\"][\"property\"],\n    dag=dag,\n)\n# [END howto_operator_http_task_get_op_response_filter]\n# [START howto_operator_http_task_put_op]\ntask_put_op = SimpleHttpOperator(\n    task_id=\"put_op\",\n    method=\"PUT\",\n    endpoint=\"put\",\n    data=json.dumps({\"priority\": 5}),\n    headers={\"Content-Type\": \"application/json\"},\n    dag=dag,\n)\n# [END howto_operator_http_task_put_op]\n# [START howto_operator_http_task_del_op]\ntask_del_op = SimpleHttpOperator(\n    task_id=\"del_op\",\n    method=\"DELETE\",\n    endpoint=\"delete\",\n    data=\"some=data\",\n    headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n    dag=dag,\n)\n# [END howto_operator_http_task_del_op]\n# [START howto_operator_http_http_sensor_check]\ntask_http_sensor_check = HttpSensor(\n    task_id=\"http_sensor_check\",\n    http_conn_id=\"http_default\",\n    endpoint=\"\",\n    request_params={},\n    response_check=lambda response: \"httpbin\" in response.text,\n    poke_interval=5,\n    dag=dag,\n)\n# [END howto_operator_http_http_sensor_check]\ntask_http_sensor_check >> task_post_op >> task_get_op >> task_get_op_response_filter\ntask_get_op_response_filter >> task_put_op >> task_del_op >> task_post_op_formenc\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.934215", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 57, "file_name": "example_influxdb.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\n\n\n@task(task_id=\"influxdb_task\")\ndef test_influxdb_hook():\n    bucket_name = \"test-influx\"\n    influxdb_hook = InfluxDBHook()\n    client = influxdb_hook.get_conn()\n    print(client)\n    print(f\"Organization name {influxdb_hook.org_name}\")\n\n    # Make sure enough permissions to create bucket.\n    influxdb_hook.create_bucket(bucket_name, \"Bucket to test influxdb connection\", influxdb_hook.org_name)\n    influxdb_hook.write(bucket_name, \"test_point\", \"location\", \"Prague\", \"temperature\", 25.3, True)\n\n    tables = influxdb_hook.query('from(bucket:\"test-influx\") |> range(start: -10m)')\n\n    for table in tables:\n        print(table)\n        for record in table.records:\n            print(record.values)\n\n    bucket_id = influxdb_hook.find_bucket_id_by_name(bucket_name)\n    print(bucket_id)\n    # Delete bucket takes bucket id.\n    influxdb_hook.delete_bucket(bucket_name)\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"influxdb_example_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    max_active_runs=1,\n    tags=[\"example\"],\n) as dag:\n    test_influxdb_hook()\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.937229", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 59, "file_name": "example_jdbc_queries.py"}, "content": "\"\"\"Example DAG demonstrating the usage of the JdbcOperator.\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\n\ntry:\n    from airflow.operators.empty import EmptyOperator\nexcept ModuleNotFoundError:\n    from airflow.operators.dummy import DummyOperator as EmptyOperator  # type: ignore\n\nfrom airflow.providers.jdbc.operators.jdbc import JdbcOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_jdbc_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    run_this_last = EmptyOperator(task_id=\"run_this_last\")\n\n    # [START howto_operator_jdbc_template]\n    delete_data = JdbcOperator(\n        task_id=\"delete_data\",\n        sql=\"delete from my_schema.my_table where dt = {{ ds }}\",\n        jdbc_conn_id=\"my_jdbc_connection\",\n        autocommit=True,\n    )\n    # [END howto_operator_jdbc_template]\n\n    # [START howto_operator_jdbc]\n    insert_data = JdbcOperator(\n        task_id=\"insert_data\",\n        sql=\"insert into my_schema.my_table select dt, value from my_schema.source_data\",\n        jdbc_conn_id=\"my_jdbc_connection\",\n        autocommit=True,\n    )\n    # [END howto_operator_jdbc]\n\n    delete_data >> insert_data >> run_this_last\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.946550", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 33, "file_name": "example_influxdb_query.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.influxdb.operators.influxdb import InfluxDBOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_influxdb_operator\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_influxdb]\n\n    query_influxdb_task = InfluxDBOperator(\n        influxdb_conn_id=\"influxdb_conn_id\",\n        task_id=\"query_influxdb\",\n        sql='from(bucket:\"test-influx\") |> range(start: -10m, stop: {{ds}})',\n        dag=dag,\n    )\n\n    # [END howto_operator_influxdb]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.955499", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 61, "file_name": "example_jenkins_job_trigger.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom requests import Request\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.providers.jenkins.hooks.jenkins import JenkinsHook\nfrom airflow.providers.jenkins.operators.jenkins_job_trigger import JenkinsJobTriggerOperator\n\nJENKINS_CONNECTION_ID = \"your_jenkins_connection\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"test_jenkins\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"concurrency\": 8,\n        \"max_active_runs\": 8,\n    },\n    start_date=datetime(2017, 6, 1),\n    schedule=None,\n) as dag:\n    job_trigger = JenkinsJobTriggerOperator(\n        task_id=\"trigger_job\",\n        job_name=\"generate-merlin-config\",\n        parameters={\"first_parameter\": \"a_value\", \"second_parameter\": \"18\"},\n        # parameters=\"resources/parameter.json\", You can also pass a path to a json file containing your param\n        jenkins_connection_id=JENKINS_CONNECTION_ID,  # The connection must be configured first\n    )\n\n    @task\n    def grab_artifact_from_jenkins(url):\n        \"\"\"\n        Grab an artifact from the previous job\n        The python-jenkins library doesn't expose a method for that\n        But it's totally possible to build manually the request for that\n        \"\"\"\n        hook = JenkinsHook(JENKINS_CONNECTION_ID)\n        jenkins_server = hook.get_jenkins_server()\n        # The JenkinsJobTriggerOperator store the job url in the xcom variable corresponding to the task\n        # You can then use it to access things or to get the job number\n        # This url looks like : http://jenkins_url/job/job_name/job_number/\n        url += \"artifact/myartifact.xml\"  # Or any other artifact name\n        request = Request(method=\"GET\", url=url)\n        response = jenkins_server.jenkins_open(request)\n        return response  # We store the artifact content in a xcom variable for later use\n\n    grab_artifact_from_jenkins(job_trigger.output)\n\n    # Task dependency created via `XComArgs`:\n    #   job_trigger >> grab_artifact_from_jenkins()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.959219", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 101, "file_name": "example_adf_run_pipeline.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\nfrom typing import cast\n\nfrom airflow.models import DAG\nfrom airflow.models.xcom_arg import XComArg\n\ntry:\n    from airflow.operators.empty import EmptyOperator\nexcept ModuleNotFoundError:\n    from airflow.operators.dummy import DummyOperator as EmptyOperator  # type: ignore\n\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\nfrom airflow.providers.microsoft.azure.sensors.data_factory import (\n    AzureDataFactoryPipelineRunStatusAsyncSensor,\n    AzureDataFactoryPipelineRunStatusSensor,\n)\nfrom airflow.utils.edgemodifier import Label\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_run_pipeline\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 8, 13),\n    schedule=\"@daily\",\n    catchup=False,\n    default_args={\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=3),\n        \"azure_data_factory_conn_id\": \"azure_data_factory\",\n        \"factory_name\": \"my-data-factory\",  # This can also be specified in the ADF connection.\n        \"resource_group_name\": \"my-resource-group\",  # This can also be specified in the ADF connection.\n    },\n    default_view=\"graph\",\n) as dag:\n    begin = EmptyOperator(task_id=\"begin\")\n    end = EmptyOperator(task_id=\"end\")\n\n    # [START howto_operator_adf_run_pipeline]\n    run_pipeline1 = AzureDataFactoryRunPipelineOperator(\n        task_id=\"run_pipeline1\",\n        pipeline_name=\"pipeline1\",\n        parameters={\"myParam\": \"value\"},\n    )\n    # [END howto_operator_adf_run_pipeline]\n\n    # [START howto_operator_adf_run_pipeline_async]\n    run_pipeline2 = AzureDataFactoryRunPipelineOperator(\n        task_id=\"run_pipeline2\",\n        pipeline_name=\"pipeline2\",\n        wait_for_termination=False,\n    )\n\n    pipeline_run_sensor = AzureDataFactoryPipelineRunStatusSensor(\n        task_id=\"pipeline_run_sensor\",\n        run_id=cast(str, XComArg(run_pipeline2, key=\"run_id\")),\n    )\n\n    # Performs polling on the Airflow Triggerer thus freeing up resources on Airflow Worker\n    pipeline_run_sensor = AzureDataFactoryPipelineRunStatusSensor(\n        task_id=\"pipeline_run_sensor_defered\",\n        run_id=cast(str, XComArg(run_pipeline2, key=\"run_id\")),\n        deferrable=True,\n    )\n\n    # The following sensor is deprecated.\n    # Please use the AzureDataFactoryPipelineRunStatusSensor and set deferrable to True\n    pipeline_run_async_sensor = AzureDataFactoryPipelineRunStatusAsyncSensor(\n        task_id=\"pipeline_run_async_sensor\",\n        run_id=cast(str, XComArg(run_pipeline2, key=\"run_id\")),\n    )\n    # [END howto_operator_adf_run_pipeline_async]\n\n    # [START howto_operator_adf_run_pipeline_with_deferrable_flag]\n    run_pipeline3 = AzureDataFactoryRunPipelineOperator(\n        task_id=\"run_pipeline3\", pipeline_name=\"pipeline1\", parameters={\"myParam\": \"value\"}, deferrable=True\n    )\n    # [END howto_operator_adf_run_pipeline_with_deferrable_flag]\n\n    begin >> Label(\"No async wait\") >> run_pipeline1\n    begin >> Label(\"Do async wait with sensor\") >> run_pipeline2\n    begin >> Label(\"Do async wait with deferrable operator\") >> run_pipeline3\n    [run_pipeline1, pipeline_run_sensor, pipeline_run_async_sensor, run_pipeline3] >> end\n    [run_pipeline1, pipeline_run_sensor, pipeline_run_async_sensor] >> end\n\n    # Task dependency created via `XComArgs`:\n    #   run_pipeline2 >> pipeline_run_sensor\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.972079", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_adls_delete.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\n\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nREMOTE_FILE_PATH = os.environ.get(\"REMOTE_LOCAL_PATH\", \"remote.txt\")\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls_delete\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n\n    upload_file = LocalFilesystemToADLSOperator(\n        task_id=\"upload_task\",\n        local_path=LOCAL_FILE_PATH,\n        remote_path=REMOTE_FILE_PATH,\n    )\n    # [START howto_operator_adls_delete]\n    remove_file = ADLSDeleteOperator(task_id=\"delete_task\", path=REMOTE_FILE_PATH, recursive=True)\n    # [END howto_operator_adls_delete]\n\n    upload_file >> remove_file\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.974811", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 42, "file_name": "example_azure_container_instances.py"}, "content": "\"\"\"\nThis is an example dag for using the AzureContainerInstancesOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import AzureContainerInstancesOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"aci_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"retries\": 1},\n    schedule=timedelta(days=1),\n    start_date=datetime(2018, 11, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    t1 = AzureContainerInstancesOperator(\n        ci_conn_id=\"azure_default\",\n        registry_conn_id=None,\n        resource_group=\"resource-group\",\n        name=\"aci-test-{{ ds }}\",\n        image=\"hello-world\",\n        region=\"WestUS2\",\n        environment_variables={},\n        volumes=[],\n        memory_in_gb=4.0,\n        cpu=1.0,\n        task_id=\"start_container\",\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.977663", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 58, "file_name": "example_azure_cosmosdb.py"}, "content": "\"\"\"\nThis is only an example DAG to highlight usage of AzureCosmosDocumentSensor to detect\nif a document now exists.\n\nYou can trigger this manually with `airflow dags trigger example_cosmosdb_sensor`.\n\n*Note: Make sure that connection `azure_cosmos_default` is properly set before running\nthis example.*\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.cosmos import AzureCosmosInsertDocumentOperator\nfrom airflow.providers.microsoft.azure.sensors.cosmos import AzureCosmosDocumentSensor\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_azure_cosmosdb_sensor\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"database_name\": \"airflow_example_db\"},\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    doc_md=__doc__,\n    tags=[\"example\"],\n) as dag:\n\n    t1 = AzureCosmosDocumentSensor(\n        task_id=\"check_cosmos_file\",\n        collection_name=\"airflow_example_coll\",\n        document_id=\"airflow_checkid\",\n    )\n\n    t2 = AzureCosmosInsertDocumentOperator(\n        task_id=\"insert_cosmos_file\",\n        collection_name=\"new-collection\",\n        document={\"id\": \"someuniqueid\", \"param1\": \"value1\", \"param2\": \"value2\"},\n    )\n\n    t1 >> t2\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:16.998352", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 168, "file_name": "example_azure_service_bus.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nimport pytest\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\n\ntry:\n    from airflow.providers.microsoft.azure.operators.asb import (\n        ASBReceiveSubscriptionMessageOperator,\n        AzureServiceBusCreateQueueOperator,\n        AzureServiceBusDeleteQueueOperator,\n        AzureServiceBusReceiveMessageOperator,\n        AzureServiceBusSendMessageOperator,\n        AzureServiceBusSubscriptionCreateOperator,\n        AzureServiceBusSubscriptionDeleteOperator,\n        AzureServiceBusTopicCreateOperator,\n        AzureServiceBusTopicDeleteOperator,\n        AzureServiceBusUpdateSubscriptionOperator,\n    )\nexcept ImportError:\n    pytest.skip(\"Azure Service Bus not available\", allow_module_level=True)\n\nEXECUTION_TIMEOUT = int(os.getenv(\"EXECUTION_TIMEOUT\", 6))\n\nCLIENT_ID = os.getenv(\"CLIENT_ID\", \"\")\nQUEUE_NAME = \"sb_mgmt_queue_test\"\nMESSAGE = \"Test Message\"\nMESSAGE_LIST = [f\"{MESSAGE} {n}\" for n in range(0, 10)]\nTOPIC_NAME = \"sb_mgmt_topic_test\"\nSUBSCRIPTION_NAME = \"sb_mgmt_subscription\"\n\nwith DAG(\n    dag_id=\"example_azure_service_bus\",\n    start_date=datetime(2021, 8, 13),\n    schedule=None,\n    catchup=False,\n    default_args={\n        \"execution_timeout\": timedelta(hours=EXECUTION_TIMEOUT),\n        \"azure_service_bus_conn_id\": \"azure_service_bus_default\",\n    },\n    tags=[\"example\", \"Azure service bus\"],\n) as dag:\n    # [START howto_operator_create_service_bus_queue]\n    create_service_bus_queue = AzureServiceBusCreateQueueOperator(\n        task_id=\"create_service_bus_queue\",\n        queue_name=QUEUE_NAME,\n    )\n    # [END howto_operator_create_service_bus_queue]\n\n    # [START howto_operator_send_message_to_service_bus_queue]\n    send_message_to_service_bus_queue = AzureServiceBusSendMessageOperator(\n        task_id=\"send_message_to_service_bus_queue\",\n        message=MESSAGE,\n        queue_name=QUEUE_NAME,\n        batch=False,\n    )\n    # [END howto_operator_send_message_to_service_bus_queue]\n\n    # [START howto_operator_send_list_message_to_service_bus_queue]\n    send_list_message_to_service_bus_queue = AzureServiceBusSendMessageOperator(\n        task_id=\"send_list_message_to_service_bus_queue\",\n        message=MESSAGE_LIST,\n        queue_name=QUEUE_NAME,\n        batch=False,\n    )\n    # [END howto_operator_send_list_message_to_service_bus_queue]\n\n    # [START howto_operator_send_batch_message_to_service_bus_queue]\n    send_batch_message_to_service_bus_queue = AzureServiceBusSendMessageOperator(\n        task_id=\"send_batch_message_to_service_bus_queue\",\n        message=MESSAGE_LIST,\n        queue_name=QUEUE_NAME,\n        batch=True,\n    )\n    # [END howto_operator_send_batch_message_to_service_bus_queue]\n\n    # [START howto_operator_receive_message_service_bus_queue]\n    receive_message_service_bus_queue = AzureServiceBusReceiveMessageOperator(\n        task_id=\"receive_message_service_bus_queue\",\n        queue_name=QUEUE_NAME,\n        max_message_count=20,\n        max_wait_time=5,\n    )\n    # [END howto_operator_receive_message_service_bus_queue]\n\n    # [START howto_operator_create_service_bus_topic]\n    create_service_bus_topic = AzureServiceBusTopicCreateOperator(\n        task_id=\"create_service_bus_topic\", topic_name=TOPIC_NAME\n    )\n    # [END howto_operator_create_service_bus_topic]\n\n    # [START howto_operator_create_service_bus_subscription]\n    create_service_bus_subscription = AzureServiceBusSubscriptionCreateOperator(\n        task_id=\"create_service_bus_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n    )\n    # [END howto_operator_create_service_bus_subscription]\n\n    # [START howto_operator_update_service_bus_subscription]\n    update_service_bus_subscription = AzureServiceBusUpdateSubscriptionOperator(\n        task_id=\"update_service_bus_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        max_delivery_count=5,\n    )\n    # [END howto_operator_update_service_bus_subscription]\n\n    # [START howto_operator_receive_message_service_bus_subscription]\n    receive_message_service_bus_subscription = ASBReceiveSubscriptionMessageOperator(\n        task_id=\"receive_message_service_bus_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        max_message_count=10,\n    )\n    # [END howto_operator_receive_message_service_bus_subscription]\n\n    # [START howto_operator_delete_service_bus_subscription]\n    delete_service_bus_subscription = AzureServiceBusSubscriptionDeleteOperator(\n        task_id=\"delete_service_bus_subscription\",\n        topic_name=TOPIC_NAME,\n        subscription_name=SUBSCRIPTION_NAME,\n        trigger_rule=\"all_done\",\n    )\n    # [END howto_operator_delete_service_bus_subscription]\n\n    # [START howto_operator_delete_service_bus_topic]\n    delete_asb_topic = AzureServiceBusTopicDeleteOperator(\n        task_id=\"delete_asb_topic\",\n        topic_name=TOPIC_NAME,\n    )\n    # [END howto_operator_delete_service_bus_topic]\n\n    # [START howto_operator_delete_service_bus_queue]\n    delete_service_bus_queue = AzureServiceBusDeleteQueueOperator(\n        task_id=\"delete_service_bus_queue\", queue_name=QUEUE_NAME, trigger_rule=\"all_done\"\n    )\n    # [END howto_operator_delete_service_bus_queue]\n\n    chain(\n        create_service_bus_queue,\n        create_service_bus_topic,\n        create_service_bus_subscription,\n        send_message_to_service_bus_queue,\n        send_list_message_to_service_bus_queue,\n        send_batch_message_to_service_bus_queue,\n        receive_message_service_bus_queue,\n        update_service_bus_subscription,\n        receive_message_service_bus_subscription,\n        delete_service_bus_subscription,\n        delete_asb_topic,\n        delete_service_bus_queue,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.002876", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 57, "file_name": "example_azure_synapse.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunSparkBatchOperator\n\nAIRFLOW_HOME = os.getenv(\"AIRFLOW_HOME\", \"/usr/local/airflow\")\nEXECUTION_TIMEOUT = int(os.getenv(\"EXECUTION_TIMEOUT\", 6))\n\ndefault_args = {\n    \"execution_timeout\": timedelta(hours=EXECUTION_TIMEOUT),\n    \"retries\": int(os.getenv(\"DEFAULT_TASK_RETRIES\", 2)),\n    \"retry_delay\": timedelta(seconds=int(os.getenv(\"DEFAULT_RETRY_DELAY_SECONDS\", 60))),\n}\n\nSPARK_JOB_PAYLOAD = {\n    \"name\": \"SparkJob\",\n    \"file\": \"abfss://spark@providersstorageaccgen2.dfs.core.windows.net/wordcount.py\",\n    \"args\": [\n        \"abfss://spark@providersstorageaccgen2.dfs.core.windows.net/shakespeare.txt\",\n        \"abfss://spark@providersstorageaccgen2.dfs.core.windows.net/results/\",\n    ],\n    \"jars\": [],\n    \"pyFiles\": [],\n    \"files\": [],\n    \"conf\": {\n        \"spark.dynamicAllocation.enabled\": \"false\",\n        \"spark.dynamicAllocation.minExecutors\": \"1\",\n        \"spark.dynamicAllocation.maxExecutors\": \"2\",\n    },\n    \"numExecutors\": 2,\n    \"executorCores\": 4,\n    \"executorMemory\": \"28g\",\n    \"driverCores\": 4,\n    \"driverMemory\": \"28g\",\n}\n\nwith DAG(\n    dag_id=\"example_synapse_spark_job\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    default_args=default_args,\n    tags=[\"example\", \"synapse\"],\n) as dag:\n    # [START howto_operator_azure_synapse]\n    run_spark_job = AzureSynapseRunSparkBatchOperator(\n        task_id=\"run_spark_job\", spark_pool=\"provsparkpool\", payload=SPARK_JOB_PAYLOAD  # type: ignore\n    )\n    # [END howto_operator_azure_synapse]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.003235", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 51, "file_name": "example_fileshare.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.hooks.fileshare import AzureFileShareHook\n\nNAME = \"myfileshare\"\nDIRECTORY = \"mydirectory\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_fileshare\"\n\n\n@task\ndef create_fileshare():\n    \"\"\"Create a fileshare with directory\"\"\"\n    hook = AzureFileShareHook()\n    hook.create_share(NAME)\n    hook.create_directory(share_name=NAME, directory_name=DIRECTORY)\n    exists = hook.check_for_directory(share_name=NAME, directory_name=DIRECTORY)\n    if not exists:\n        raise Exception\n\n\n@task\ndef delete_fileshare():\n    \"\"\"Delete a fileshare\"\"\"\n    hook = AzureFileShareHook()\n    hook.delete_share(NAME)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_fileshare() >> delete_fileshare()\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.009128", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 43, "file_name": "example_local_to_adls.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\n\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nREMOTE_FILE_PATH = os.environ.get(\"REMOTE_LOCAL_PATH\", \"remote.txt\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_adls\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_local_to_adls]\n    upload_file = LocalFilesystemToADLSOperator(\n        task_id=\"upload_task\",\n        local_path=LOCAL_FILE_PATH,\n        remote_path=REMOTE_FILE_PATH,\n    )\n    # [END howto_operator_local_to_adls]\n\n    delete_file = ADLSDeleteOperator(task_id=\"remove_task\", path=REMOTE_FILE_PATH, recursive=True)\n\n    upload_file >> delete_file\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.012308", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_local_to_wasb.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.operators.wasb_delete_blob import WasbDeleteBlobOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_wasb import LocalFilesystemToWasbOperator\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\n\nPATH_TO_UPLOAD_FILE = os.environ.get(\"AZURE_PATH_TO_UPLOAD_FILE\", \"example-text.txt\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_wasb\"\nAZURE_CONTAINER_NAME = os.environ.get(\"AZURE_CONTAINER_NAME\", \"mycontainer\")\nAZURE_BLOB_NAME = os.environ.get(\"AZURE_BLOB_NAME\", \"myblob\")\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_local_to_wasb]\n    upload = LocalFilesystemToWasbOperator(\n        task_id=\"upload_file\",\n        file_path=PATH_TO_UPLOAD_FILE,\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=AZURE_BLOB_NAME,\n    )\n    # [END howto_operator_local_to_wasb]\n    delete = WasbDeleteBlobOperator(\n        task_id=\"delete_file\", blob_name=AZURE_BLOB_NAME, container_name=AZURE_CONTAINER_NAME\n    )\n\n    upload >> delete\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.034435", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 70, "file_name": "example_sftp_to_wasb.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.providers.microsoft.azure.operators.wasb_delete_blob import WasbDeleteBlobOperator\nfrom airflow.providers.microsoft.azure.transfers.sftp_to_wasb import SFTPToWasbOperator\nfrom airflow.providers.sftp.hooks.sftp import SFTPHook\nfrom airflow.providers.sftp.operators.sftp import SFTPOperator\n\nAZURE_CONTAINER_NAME = os.environ.get(\"AZURE_CONTAINER_NAME\", \"airflow\")\nBLOB_PREFIX = os.environ.get(\"AZURE_BLOB_PREFIX\", \"airflow\")\nSFTP_SRC_PATH = os.environ.get(\"SFTP_SRC_PATH\", \"/sftp\")\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_SRC_PATH\", \"/tmp\")\nSAMPLE_FILENAME = os.environ.get(\"SFTP_SAMPLE_FILENAME\", \"sftp_to_wasb_test.txt\")\nFILE_COMPLETE_PATH = os.path.join(LOCAL_FILE_PATH, SAMPLE_FILENAME)\nSFTP_FILE_COMPLETE_PATH = os.path.join(SFTP_SRC_PATH, SAMPLE_FILENAME)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_to_wasb\"\n\n\n@task\ndef delete_sftp_file():\n    \"\"\"Delete a file at SFTP SERVER\"\"\"\n    SFTPHook().delete_file(SFTP_FILE_COMPLETE_PATH)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=None,\n    catchup=False,\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n) as dag:\n    transfer_files_to_sftp_step = SFTPOperator(\n        task_id=\"transfer_files_from_local_to_sftp\",\n        local_filepath=FILE_COMPLETE_PATH,\n        remote_filepath=SFTP_FILE_COMPLETE_PATH,\n    )\n\n    # [START how_to_sftp_to_wasb]\n    transfer_files_to_azure = SFTPToWasbOperator(\n        task_id=\"transfer_files_from_sftp_to_wasb\",\n        # SFTP args\n        sftp_source_path=SFTP_SRC_PATH,\n        # AZURE args\n        container_name=AZURE_CONTAINER_NAME,\n        blob_prefix=BLOB_PREFIX,\n    )\n    # [END how_to_sftp_to_wasb]\n\n    delete_blob_file_step = WasbDeleteBlobOperator(\n        task_id=\"delete_blob_files\",\n        container_name=AZURE_CONTAINER_NAME,\n        blob_name=BLOB_PREFIX + SAMPLE_FILENAME,\n    )\n\n    transfer_files_to_sftp_step >> transfer_files_to_azure >> delete_blob_file_step >> delete_sftp_file()\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.037264", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 140, "file_name": "example_mssql.py"}, "content": "\"\"\"\nExample use of MsSql related operators.\n\"\"\"\nfrom __future__ import annotations\n\n# [START mssql_operator_howto_guide]\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow import DAG\n\ntry:\n    from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\n    from airflow.providers.microsoft.mssql.operators.mssql import MsSqlOperator\nexcept ImportError:\n    pytest.skip(\"MSSQL provider not available\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mssql\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 10, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_mssql]\n\n    # Example of creating a task to create a table in MsSql\n\n    create_table_mssql_task = MsSqlOperator(\n        task_id=\"create_country_table\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=r\"\"\"\n        CREATE TABLE Country (\n            country_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n            name TEXT,\n            continent TEXT\n        );\n        \"\"\",\n        dag=dag,\n    )\n\n    # [END howto_operator_mssql]\n\n    # [START mssql_hook_howto_guide_insert_mssql_hook]\n\n    @dag.task(task_id=\"insert_mssql_task\")\n    def insert_mssql_hook():\n        mssql_hook = MsSqlHook(mssql_conn_id=\"airflow_mssql\", schema=\"airflow\")\n\n        rows = [\n            (\"India\", \"Asia\"),\n            (\"Germany\", \"Europe\"),\n            (\"Argentina\", \"South America\"),\n            (\"Ghana\", \"Africa\"),\n            (\"Japan\", \"Asia\"),\n            (\"Namibia\", \"Africa\"),\n        ]\n        target_fields = [\"name\", \"continent\"]\n        mssql_hook.insert_rows(table=\"Country\", rows=rows, target_fields=target_fields)\n\n    # [END mssql_hook_howto_guide_insert_mssql_hook]\n\n    # [START mssql_operator_howto_guide_create_table_mssql_from_external_file]\n    # Example of creating a task that calls an sql command from an external file.\n    create_table_mssql_from_external_file = MsSqlOperator(\n        task_id=\"create_table_from_external_file\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=\"create_table.sql\",\n        dag=dag,\n    )\n    # [END mssql_operator_howto_guide_create_table_mssql_from_external_file]\n\n    # [START mssql_operator_howto_guide_populate_user_table]\n    populate_user_table = MsSqlOperator(\n        task_id=\"populate_user_table\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=r\"\"\"\n                INSERT INTO Users (username, description)\n                VALUES ( 'Danny', 'Musician');\n                INSERT INTO Users (username, description)\n                VALUES ( 'Simone', 'Chef');\n                INSERT INTO Users (username, description)\n                VALUES ( 'Lily', 'Florist');\n                INSERT INTO Users (username, description)\n                VALUES ( 'Tim', 'Pet shop owner');\n                \"\"\",\n    )\n    # [END mssql_operator_howto_guide_populate_user_table]\n\n    # [START mssql_operator_howto_guide_get_all_countries]\n    get_all_countries = MsSqlOperator(\n        task_id=\"get_all_countries\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=r\"\"\"SELECT * FROM Country;\"\"\",\n    )\n    # [END mssql_operator_howto_guide_get_all_countries]\n\n    # [START mssql_operator_howto_guide_get_all_description]\n    get_all_description = MsSqlOperator(\n        task_id=\"get_all_description\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=r\"\"\"SELECT description FROM Users;\"\"\",\n    )\n    # [END mssql_operator_howto_guide_get_all_description]\n\n    # [START mssql_operator_howto_guide_params_passing_get_query]\n    get_countries_from_continent = MsSqlOperator(\n        task_id=\"get_countries_from_continent\",\n        mssql_conn_id=\"airflow_mssql\",\n        sql=r\"\"\"SELECT * FROM Country where {{ params.column }}='{{ params.value }}';\"\"\",\n        params={\"column\": \"CONVERT(VARCHAR, continent)\", \"value\": \"Asia\"},\n    )\n    # [END mssql_operator_howto_guide_params_passing_get_query]\n    (\n        create_table_mssql_task\n        >> insert_mssql_hook()\n        >> create_table_mssql_from_external_file\n        >> populate_user_table\n        >> get_all_countries\n        >> get_all_description\n        >> get_countries_from_continent\n    )\n    # [END mssql_operator_howto_guide]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.039779", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 47, "file_name": "example_mysql.py"}, "content": "\"\"\"\nExample use of MySql related operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.mysql.operators.mysql import MySqlOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mysql\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"mysql_conn_id\": \"mysql_conn_id\"},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_mysql]\n\n    drop_table_mysql_task = MySqlOperator(\n        task_id=\"drop_table_mysql\", sql=r\"\"\"DROP TABLE table_name;\"\"\", dag=dag\n    )\n\n    # [END howto_operator_mysql]\n\n    # [START howto_operator_mysql_external_file]\n\n    mysql_task = MySqlOperator(\n        task_id=\"drop_table_mysql_external_file\",\n        sql=\"/scripts/drop_table.sql\",\n        dag=dag,\n    )\n\n    # [END howto_operator_mysql_external_file]\n\n    drop_table_mysql_task >> mysql_task\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.055119", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 36, "file_name": "example_neo4j.py"}, "content": "\"\"\"\nExample use of Neo4j related operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.neo4j.operators.neo4j import Neo4jOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_neo4j\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START run_query_neo4j_operator]\n\n    neo4j_task = Neo4jOperator(\n        task_id=\"run_neo4j_query\",\n        neo4j_conn_id=\"neo4j_conn_id\",\n        sql='MATCH (tom {name: \"Tom Hanks\", date: \"{{ds}}\"}) RETURN tom',\n        dag=dag,\n    )\n\n    # [END run_query_neo4j_operator]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.058916", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 65, "file_name": "example_winrm.py"}, "content": "\"\"\"\nThis is an example dag for using the WinRMOperator.\n\"\"\"\nfrom __future__ import annotations\n\n# --------------------------------------------------------------------------------\n# Load The Dependencies\n# --------------------------------------------------------------------------------\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\n\n# --------------------------------------------------------------------------------\n# Caveat: This Dag will not run because of missing scripts.\n# The purpose of this is to give you a sample of a real world example DAG!\n# --------------------------------------------------------------------------------\n\n\ntry:\n    from airflow.operators.empty import EmptyOperator\nexcept ModuleNotFoundError:\n    from airflow.operators.dummy import DummyOperator as EmptyOperator  # type: ignore\n\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"POC_winrm_parallel\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    run_this_last = EmptyOperator(task_id=\"run_this_last\")\n\n    # [START create_hook]\n    winRMHook = WinRMHook(ssh_conn_id=\"ssh_POC1\")\n    # [END create_hook]\n\n    # [START run_operator]\n    t1 = WinRMOperator(task_id=\"wintask1\", command=\"ls -altr\", winrm_hook=winRMHook)\n\n    t2 = WinRMOperator(task_id=\"wintask2\", command=\"sleep 60\", winrm_hook=winRMHook)\n\n    t3 = WinRMOperator(task_id=\"wintask3\", command=\"echo 'luke test' \", winrm_hook=winRMHook)\n    # [END run_operator]\n\n    [t1, t2, t3] >> run_this_last\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.063197", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 42, "file_name": "example_opsgenie_alert.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.opsgenie.operators.opsgenie import (\n    OpsgenieCloseAlertOperator,\n    OpsgenieCreateAlertOperator,\n    OpsgenieDeleteAlertOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"opsgenie_alert_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    # [START howto_opsgenie_create_alert_operator]\n    opsgenie_alert_operator = OpsgenieCreateAlertOperator(task_id=\"opsgenie_task\", message=\"Hello World!\")\n    # [END howto_opsgenie_create_alert_operator]\n\n    # [START howto_opsgenie_close_alert_operator]\n    opsgenie_close_alert_operator = OpsgenieCloseAlertOperator(\n        task_id=\"opsgenie_close_task\", identifier=\"identifier_example\"\n    )\n    # [END howto_opsgenie_close_alert_operator]\n\n    # [START howto_opsgenie_delete_alert_operator]\n    opsgenie_delete_alert_operator = OpsgenieDeleteAlertOperator(\n        task_id=\"opsgenie_delete_task\", identifier=\"identifier_example\"\n    )\n    # [END howto_opsgenie_delete_alert_operator]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.070392", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 40, "file_name": "example_papermill.py"}, "content": "\"\"\"\nThis DAG will use Papermill to run the notebook \"hello_world\", based on the execution date\nit will create an output notebook \"out-<date>\". All fields, including the keys in the parameters, are\ntemplated.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.papermill.operators.papermill import PapermillOperator\n\nSTART_DATE = datetime(2021, 1, 1)\nSCHEDULE_INTERVAL = \"0 0 * * *\"\nDAGRUN_TIMEOUT = timedelta(minutes=60)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_papermill_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    dagrun_timeout=DAGRUN_TIMEOUT,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_papermill]\n    run_this = PapermillOperator(\n        task_id=\"run_example_notebook\",\n        input_nb=\"/tmp/hello_world.ipynb\",\n        output_nb=\"/tmp/out-{{ execution_date }}.ipynb\",\n        parameters={\"msgs\": \"Ran from Airflow at {{ execution_date }}!\"},\n    )\n    # [END howto_operator_papermill]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.070640", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 40, "file_name": "example_plexus.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.plexus.operators.job import PlexusJobOperator\n\nHOME = \"/home/acc\"\nT3_PRERUN_SCRIPT = \"cp {home}/imdb/run_scripts/mlflow.sh {home}/ && chmod +x mlflow.sh\".format(home=HOME)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"test\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"owner\": \"core scientific\", \"retries\": 1},\n    description=\"testing plexus operator\",\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START plexus_job_op]\n    t1 = PlexusJobOperator(\n        task_id=\"test\",\n        job_params={\n            \"name\": \"test\",\n            \"app\": \"MLFlow Pipeline 01\",\n            \"queue\": \"DGX-2 (gpu:Tesla V100-SXM3-32GB)\",\n            \"num_nodes\": 1,\n            \"num_cores\": 1,\n            \"prerun_script\": T3_PRERUN_SCRIPT,\n        },\n    )\n    # [END plexus_job_op]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.102222", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 76, "file_name": "example_postgres.py"}, "content": "from __future__ import annotations\n\nimport datetime\nimport os\n\nfrom airflow import DAG\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\n# [START postgres_operator_howto_guide]\n\n\n# create_pet_table, populate_pet_table, get_all_pets, and get_birth_date are examples of tasks created by\n# instantiating the Postgres Operator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"postgres_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime.datetime(2020, 2, 2),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START postgres_operator_howto_guide_create_pet_table]\n    create_pet_table = PostgresOperator(\n        task_id=\"create_pet_table\",\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS pet (\n            pet_id SERIAL PRIMARY KEY,\n            name VARCHAR NOT NULL,\n            pet_type VARCHAR NOT NULL,\n            birth_date DATE NOT NULL,\n            OWNER VARCHAR NOT NULL);\n          \"\"\",\n    )\n    # [END postgres_operator_howto_guide_create_pet_table]\n    # [START postgres_operator_howto_guide_populate_pet_table]\n    populate_pet_table = PostgresOperator(\n        task_id=\"populate_pet_table\",\n        sql=\"\"\"\n            INSERT INTO pet (name, pet_type, birth_date, OWNER)\n            VALUES ( 'Max', 'Dog', '2018-07-05', 'Jane');\n            INSERT INTO pet (name, pet_type, birth_date, OWNER)\n            VALUES ( 'Susie', 'Cat', '2019-05-01', 'Phil');\n            INSERT INTO pet (name, pet_type, birth_date, OWNER)\n            VALUES ( 'Lester', 'Hamster', '2020-06-23', 'Lily');\n            INSERT INTO pet (name, pet_type, birth_date, OWNER)\n            VALUES ( 'Quincy', 'Parrot', '2013-08-11', 'Anne');\n            \"\"\",\n    )\n    # [END postgres_operator_howto_guide_populate_pet_table]\n    # [START postgres_operator_howto_guide_get_all_pets]\n    get_all_pets = PostgresOperator(task_id=\"get_all_pets\", sql=\"SELECT * FROM pet;\")\n    # [END postgres_operator_howto_guide_get_all_pets]\n    # [START postgres_operator_howto_guide_get_birth_date]\n    get_birth_date = PostgresOperator(\n        task_id=\"get_birth_date\",\n        sql=\"SELECT * FROM pet WHERE birth_date BETWEEN SYMMETRIC %(begin_date)s AND %(end_date)s\",\n        parameters={\"begin_date\": \"2020-01-01\", \"end_date\": \"2020-12-31\"},\n        hook_params={\"options\": \"-c statement_timeout=3000ms\"},\n    )\n    # [END postgres_operator_howto_guide_get_birth_date]\n\n    create_pet_table >> populate_pet_table >> get_all_pets >> get_birth_date\n    # [END postgres_operator_howto_guide]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.102602", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 62, "file_name": "example_papermill_verify.py"}, "content": "\"\"\"\nThis DAG will use Papermill to run the notebook \"hello_world\", based on the execution date\nit will create an output notebook \"out-<date>\". All fields, including the keys in the parameters, are\ntemplated.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nimport scrapbook as sb\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.lineage import AUTO\nfrom airflow.providers.papermill.operators.papermill import PapermillOperator\n\nSTART_DATE = datetime(2021, 1, 1)\nSCHEDULE_INTERVAL = \"0 0 * * *\"\nDAGRUN_TIMEOUT = timedelta(minutes=60)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_papermill_operator_verify\"\n\n\n# [START howto_verify_operator_papermill]\n@task\ndef check_notebook(inlets, execution_date):\n    \"\"\"\n    Verify the message in the notebook\n    \"\"\"\n    notebook = sb.read_notebook(inlets[0].url)\n    message = notebook.scraps[\"message\"]\n    print(f\"Message in notebook {message} for {execution_date}\")\n\n    if message.data != f\"Ran from Airflow at {execution_date}!\":\n        return False\n\n    return True\n\n\nwith DAG(\n    dag_id=\"example_papermill_operator_verify\",\n    schedule=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    dagrun_timeout=DAGRUN_TIMEOUT,\n    catchup=False,\n) as dag:\n\n    run_this = PapermillOperator(\n        task_id=\"run_example_notebook\",\n        input_nb=os.path.join(os.path.dirname(os.path.realpath(__file__)), \"input_notebook.ipynb\"),\n        output_nb=\"/tmp/out-{{ execution_date }}.ipynb\",\n        parameters={\"msgs\": \"Ran from Airflow at {{ execution_date }}!\"},\n    )\n\n    run_this >> check_notebook(inlets=AUTO, execution_date=\"{{ execution_date }}\")\n# [END howto_verify_operator_papermill]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.104871", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 38, "file_name": "example_gcs_to_presto.py"}, "content": "\"\"\"\nExample DAG using GCSToPrestoOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.presto.transfers.gcs_to_presto import GCSToPrestoOperator\n\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"test28397yeo\")\nPATH_TO_FILE = os.environ.get(\"GCP_PATH\", \"path/to/file\")\nPRESTO_TABLE = os.environ.get(\"PRESTO_TABLE\", \"test_table\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_presto\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START gcs_csv_to_presto_table]\n    gcs_csv_to_presto_table = GCSToPrestoOperator(\n        task_id=\"gcs_csv_to_presto_table\",\n        source_bucket=BUCKET,\n        source_object=PATH_TO_FILE,\n        presto_table=PRESTO_TABLE,\n    )\n    # [END gcs_csv_to_presto_table]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.115524", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 77, "file_name": "example_qubole_sensors.py"}, "content": "from __future__ import annotations\n\nimport os\nimport textwrap\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.qubole.sensors.qubole import QuboleFileSensor, QubolePartitionSensor\n\nSTART_DATE = datetime(2021, 1, 1)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_qubole_sensor\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=START_DATE,\n    tags=[\"example\"],\n) as dag:\n    dag.doc_md = textwrap.dedent(\n        \"\"\"\n        This is only an example DAG to highlight usage of QuboleSensor in various scenarios,\n        some of these tasks may or may not work based on your QDS account setup.\n\n        Run a shell command from Qubole Analyze against your Airflow cluster with following to\n        trigger it manually `airflow dags trigger example_qubole_sensor`.\n\n        *Note: Make sure that connection `qubole_default` is properly set before running\n        this example.*\n        \"\"\"\n    )\n\n    # [START howto_sensor_qubole_run_file_sensor]\n    check_s3_file = QuboleFileSensor(\n        task_id=\"check_s3_file\",\n        poke_interval=60,\n        timeout=600,\n        data={\n            \"files\": [\n                \"s3://paid-qubole/HadoopAPIExamples/jars/hadoop-0.20.1-dev-streaming.jar\",\n                \"s3://paid-qubole/HadoopAPITests/data/{{ ds.split('-')[2] }}.tsv\",\n            ]  # will check for availability of all the files in array\n        },\n    )\n    # [END howto_sensor_qubole_run_file_sensor]\n\n    # [START howto_sensor_qubole_run_partition_sensor]\n    check_hive_partition = QubolePartitionSensor(\n        task_id=\"check_hive_partition\",\n        poke_interval=10,\n        timeout=60,\n        data={\n            \"schema\": \"default\",\n            \"table\": \"my_partitioned_table\",\n            \"columns\": [\n                {\"column\": \"month\", \"values\": [\"{{ ds.split('-')[1] }}\"]},\n                {\"column\": \"day\", \"values\": [\"{{ ds.split('-')[2] }}\", \"{{ yesterday_ds.split('-')[2] }}\"]},\n            ],  # will check for partitions like [month=12/day=12,month=12/day=13]\n        },\n    )\n    # [END howto_sensor_qubole_run_partition_sensor]\n\n    check_s3_file >> check_hive_partition\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.122043", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 41, "file_name": "example_qubole.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "    /** Computes an approximation to pi */\n    object SparkPi {\n      def main(args: Array[String]) {\n        val conf = new SparkConf().setAppName(\"Spark Pi\")\n        val spark = new SparkContext(conf)\n        val slices = if (args.length > 0) args(0).toInt else 2\n        val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow\n        val count = spark.parallelize(1 until n, slices).map { i =>\n          val x = random * 2 - 1\n          val y = random * 2 - 1\n          if (x*x + y*y < 1) 1 else 0\n        }.reduce(_ + _)\n        println(\"Pi is roughly \" + 4.0 * count / n)\n        spark.stop()\n      }\n    }\n    \"\"\"\n\n    spark_cmd = QuboleOperator(\n        task_id=\"spark_cmd\",\n        command_type=\"sparkcmd\",\n        program=prog,\n        language=\"scala\",\n        arguments=\"--class SparkPi\",\n        tags=\"airflow_example_run\",\n    )\n    # [END howto_operator_qubole_run_spark_scala]\n\n    branching >> db_import >> spark_cmd >> join\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.127216", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 81, "file_name": "example_bulk.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.salesforce.operators.bulk import SalesforceBulkOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_salesforce_bulk\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    # [START howto_salesforce_bulk_insert_operation]\n    bulk_insert = SalesforceBulkOperator(\n        task_id=\"bulk_insert\",\n        operation=\"insert\",\n        object_name=\"Account\",\n        payload=[\n            {\"Id\": \"000000000000000AAA\", \"Name\": \"account1\"},\n            {\"Name\": \"account2\"},\n        ],\n        external_id_field=\"Id\",\n        batch_size=10000,\n        use_serial=False,\n    )\n    # [END howto_salesforce_bulk_insert_operation]\n\n    # [START howto_salesforce_bulk_update_operation]\n    bulk_update = SalesforceBulkOperator(\n        task_id=\"bulk_update\",\n        operation=\"update\",\n        object_name=\"Account\",\n        payload=[\n            {\"Id\": \"000000000000000AAA\", \"Name\": \"account1\"},\n            {\"Id\": \"000000000000000BBB\", \"Name\": \"account2\"},\n        ],\n        batch_size=10000,\n        use_serial=False,\n    )\n    # [END howto_salesforce_bulk_update_operation]\n\n    # [START howto_salesforce_bulk_upsert_operation]\n    bulk_upsert = SalesforceBulkOperator(\n        task_id=\"bulk_upsert\",\n        operation=\"upsert\",\n        object_name=\"Account\",\n        payload=[\n            {\"Id\": \"000000000000000AAA\", \"Name\": \"account1\"},\n            {\"Name\": \"account2\"},\n        ],\n        external_id_field=\"Id\",\n        batch_size=10000,\n        use_serial=False,\n    )\n    # [END howto_salesforce_bulk_upsert_operation]\n\n    # [START howto_salesforce_bulk_delete_operation]\n    bulk_delete = SalesforceBulkOperator(\n        task_id=\"bulk_delete\",\n        operation=\"delete\",\n        object_name=\"Account\",\n        payload=[\n            {\"Id\": \"000000000000000AAA\"},\n            {\"Id\": \"000000000000000BBB\"},\n        ],\n        batch_size=10000,\n        use_serial=False,\n    )\n    # [END howto_salesforce_bulk_delete_operation]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.131148", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 66, "file_name": "example_redis_publish.py"}, "content": "\"\"\"\nThis is an example DAG which uses RedisPublishOperator, RedisPubSubSensor and RedisKeySensor.\nIn this example, we create 3 tasks which execute sequentially.\nThe first task is to publish a particular message to redis using the RedisPublishOperator.\nThe second task is to wait for a particular message at a particular channel to arrive in redis\nusing the RedisPubSubSensor, and the third task is to wait for a particular key to arrive in\nredis using the RedisKeySensor.\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\n# [START import_module]\nfrom airflow import DAG\nfrom airflow.providers.redis.operators.redis_publish import RedisPublishOperator\nfrom airflow.providers.redis.sensors.redis_key import RedisKeySensor\n\n# [END import_module]\n# [START instantiate_dag]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n\ndefault_args = {\n    \"start_date\": datetime(2023, 5, 15),\n    \"max_active_runs\": 1,\n}\n\nwith DAG(\n    dag_id=\"redis_example\",\n    default_args=default_args,\n) as dag:\n    # [START RedisPublishOperator_DAG]\n    publish_task = RedisPublishOperator(\n        task_id=\"publish_task\",\n        redis_conn_id=\"redis_default\",\n        channel=\"your_channel\",\n        message=\"Start processing\",\n        dag=dag,\n    )\n\n    # [END RedisPublishOperator_DAG]\n\n    # [START RedisKeySensor_DAG]\n    key_sensor_task = RedisKeySensor(\n        task_id=\"key_sensor_task\",\n        redis_conn_id=\"redis_default\",\n        key=\"your_key\",\n        dag=dag,\n        timeout=600,\n        poke_interval=30,\n    )\n    # [END RedisKeySensor_DAG]\n\n    publish_task >> key_sensor_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.131473", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 32, "file_name": "example_salesforce_apex_rest.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.salesforce.operators.salesforce_apex_rest import SalesforceApexRestOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_trino\"\n\n\nwith DAG(\n    dag_id=\"salesforce_apex_rest_operator_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    # [START howto_salesforce_apex_rest_operator]\n    payload = {\"activity\": [{\"user\": \"12345\", \"action\": \"update page\", \"time\": \"2014-04-21T13:00:15Z\"}]}\n\n    apex_operator = SalesforceApexRestOperator(\n        task_id=\"apex_task\", method=\"POST\", endpoint=\"User/Activity\", payload=payload\n    )\n    # [END howto_salesforce_apex_rest_operator]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.168683", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 40, "file_name": "example_singularity.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_sample\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"retries\": 1},\n    schedule=timedelta(minutes=10),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    t1 = BashOperator(task_id=\"print_date\", bash_command=\"date\")\n\n    t2 = BashOperator(task_id=\"sleep\", bash_command=\"sleep 5\", retries=3)\n\n    t3 = SingularityOperator(\n        command=\"/bin/sleep 30\",\n        image=\"docker://busybox:1.30.1\",\n        task_id=\"singularity_op_tester\",\n    )\n\n    t4 = BashOperator(task_id=\"print_hello\", bash_command='echo \"hello world!!!\"')\n\n    t1 >> [t2, t3]\n    t3 >> t4\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.170788", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 71, "file_name": "example_sftp_sensor.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\n\nSFTP_DIRECTORY = os.environ.get(\"SFTP_DIRECTORY\", \"example-empty-directory\").rstrip(\"/\") + \"/\"\nFULL_FILE_PATH = f\"{SFTP_DIRECTORY}example_test_sftp_sensor_decory_file.txt\"\nSFTP_DEFAULT_CONNECTION = \"sftp_default\"\n\n\n@task.python\ndef sleep_function():\n    import time\n\n    time.sleep(60)\n\n\nwith DAG(\n    \"example_sftp_sensor\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sftp\"],\n) as dag:\n\n    # [START howto_operator_sftp_sensor_decorator]\n    @task.sftp_sensor(task_id=\"sftp_sensor\", path=FULL_FILE_PATH, poke_interval=10)\n    def sftp_sensor_decorator():\n        print(\"Files were successfully found!\")\n        # add your logic\n        return \"done\"\n\n    # [END howto_operator_sftp_sensor_decorator]\n\n    remove_file_task_start = SSHOperator(\n        task_id=\"remove_file_start\",\n        command=f\"rm {FULL_FILE_PATH} || true\",\n        ssh_conn_id=SFTP_DEFAULT_CONNECTION,\n    )\n    remove_file_task_end = SSHOperator(\n        task_id=\"remove_file_end\", command=f\"rm {FULL_FILE_PATH} || true\", ssh_conn_id=SFTP_DEFAULT_CONNECTION\n    )\n    create_decoy_file_task = SSHOperator(\n        task_id=\"create_file\", command=f\"touch {FULL_FILE_PATH}\", ssh_conn_id=SFTP_DEFAULT_CONNECTION\n    )\n    sleep_task = sleep_function()\n    sftp_with_sensor = sftp_sensor_decorator()\n\n    # [START howto_operator_sftp_sensor]\n    sftp_with_operator = SFTPSensor(task_id=\"sftp_operator\", path=FULL_FILE_PATH, poke_interval=10)\n    # [END howto_operator_sftp_sensor]\n\n    remove_file_task_start >> sleep_task >> create_decoy_file_task\n    remove_file_task_start >> [sftp_with_operator, sftp_with_sensor] >> remove_file_task_end\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.173060", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 42, "file_name": "example_slack.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIFileOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"slack_example_dag\"\n\n# [START slack_operator_howto_guide]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"slack_conn_id\": \"slack\", \"channel\": \"#general\", \"initial_comment\": \"Hello World!\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n) as dag:\n\n    # Send file with filename and filetype\n    slack_operator_file = SlackAPIFileOperator(\n        task_id=\"slack_file_upload_1\",\n        filename=\"/files/dags/test.txt\",\n        filetype=\"txt\",\n    )\n\n    # Send file content\n    slack_operator_file_content = SlackAPIFileOperator(\n        task_id=\"slack_file_upload_2\",\n        content=\"file content in txt\",\n    )\n    # [END slack_operator_howto_guide]\n\n    slack_operator_file >> slack_operator_file_content\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.184347", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 39, "file_name": "example_sql_to_slack.py"}, "content": "\"\"\"\nExample DAG using SqlToSlackOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.slack.transfers.sql_to_slack import SqlToSlackOperator\n\nSQL_TABLE = os.environ.get(\"SQL_TABLE\", \"test_table\")\nSQL_CONN_ID = \"presto_default\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_to_slack\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_sql_to_slack]\n    SqlToSlackOperator(\n        task_id=\"presto_to_slack\",\n        sql_conn_id=SQL_CONN_ID,\n        sql=f\"SELECT col FROM {SQL_TABLE}\",\n        slack_channel=\"my_channel\",\n        slack_conn_id=\"slack_default\",\n        slack_message=\"message: {{ ds }}, {{ results_df }}\",\n    )\n    # [END howto_operator_sql_to_slack]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.184609", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 81, "file_name": "example_snowflake.py"}, "content": "\"\"\"\nExample use of Snowflake related operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator, SnowflakeSqlApiOperator\n\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\nSNOWFLAKE_SAMPLE_TABLE = \"sample_table\"\n\n# SQL commands\nCREATE_TABLE_SQL_STRING = (\n    f\"CREATE OR REPLACE TRANSIENT TABLE {SNOWFLAKE_SAMPLE_TABLE} (name VARCHAR(250), id INT);\"\n)\nSQL_INSERT_STATEMENT = f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES ('name', %(id)s)\"\nSQL_LIST = [SQL_INSERT_STATEMENT % {\"id\": n} for n in range(0, 10)]\nSQL_MULTIPLE_STMTS = \"; \".join(SQL_LIST)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_snowflake\"\n\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"snowflake_conn_id\": SNOWFLAKE_CONN_ID},\n    tags=[\"example\"],\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START howto_operator_snowflake]\n    snowflake_op_sql_str = SnowflakeOperator(task_id=\"snowflake_op_sql_str\", sql=CREATE_TABLE_SQL_STRING)\n\n    snowflake_op_with_params = SnowflakeOperator(\n        task_id=\"snowflake_op_with_params\",\n        sql=SQL_INSERT_STATEMENT,\n        parameters={\"id\": 56},\n    )\n\n    snowflake_op_sql_list = SnowflakeOperator(task_id=\"snowflake_op_sql_list\", sql=SQL_LIST)\n\n    snowflake_op_sql_multiple_stmts = SnowflakeOperator(\n        task_id=\"snowflake_op_sql_multiple_stmts\",\n        sql=SQL_MULTIPLE_STMTS,\n        split_statements=True,\n    )\n\n    snowflake_op_template_file = SnowflakeOperator(\n        task_id=\"snowflake_op_template_file\",\n        sql=\"example_snowflake_snowflake_op_template_file.sql\",\n    )\n\n    # [END howto_operator_snowflake]\n\n    # [START howto_snowflake_sql_api_operator]\n    snowflake_sql_api_op_sql_multiple_stmt = SnowflakeSqlApiOperator(\n        task_id=\"snowflake_op_sql_multiple_stmt\",\n        sql=SQL_MULTIPLE_STMTS,\n        statement_count=len(SQL_LIST),\n    )\n    # [END howto_snowflake_sql_api_operator]\n\n    (\n        snowflake_op_sql_str\n        >> [\n            snowflake_op_with_params,\n            snowflake_op_sql_list,\n            snowflake_op_template_file,\n            snowflake_op_sql_multiple_stmts,\n            snowflake_sql_api_op_sql_multiple_stmt,\n        ]\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.195160", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_s3_to_snowflake.py"}, "content": "\"\"\"\nExample use of Snowflake related operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator\n\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\n# TODO: should be able to rely on connection's schema, but currently param required by S3ToSnowflakeTransfer\nSNOWFLAKE_STAGE = \"stage_name\"\nSNOWFLAKE_SAMPLE_TABLE = \"sample_table\"\nS3_FILE_PATH = \"</path/to/file/sample_file.csv\"\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_snowflake\"\n\n# [START howto_operator_snowflake]\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"snowflake_conn_id\": SNOWFLAKE_CONN_ID},\n    tags=[\"example\"],\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START howto_operator_s3_to_snowflake]\n\n    copy_into_table = S3ToSnowflakeOperator(\n        task_id=\"copy_into_table\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n        s3_keys=[S3_FILE_PATH],\n        table=SNOWFLAKE_SAMPLE_TABLE,\n        stage=SNOWFLAKE_STAGE,\n        file_format=\"(type = 'CSV',field_delimiter = ';')\",\n        pattern=\".*[.]csv\",\n    )\n\n    # [END howto_operator_s3_to_snowflake]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.199398", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 49, "file_name": "example_snowflake_to_slack.py"}, "content": "\"\"\"\nExample use of Snowflake related operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.transfers.snowflake_to_slack import SnowflakeToSlackOperator\n\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\nSLACK_CONN_ID = \"my_slack_conn\"\nSNOWFLAKE_SAMPLE_TABLE = \"sample_table\"\n\n# SQL commands\nSNOWFLAKE_SLACK_SQL = f\"SELECT name, id FROM {SNOWFLAKE_SAMPLE_TABLE} LIMIT 10;\"\nSNOWFLAKE_SLACK_MESSAGE = (\n    \"Results in an ASCII table:\\n```{{ results_df | tabulate(tablefmt='pretty', headers='keys') }}```\"\n)\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_snowflake_to_slack\"\n\n# [START howto_operator_snowflake]\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"snowflake_conn_id\": SNOWFLAKE_CONN_ID},\n    tags=[\"example\"],\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START howto_operator_snowflake_to_slack]\n\n    slack_report = SnowflakeToSlackOperator(\n        task_id=\"slack_report\",\n        sql=SNOWFLAKE_SLACK_SQL,\n        slack_message=SNOWFLAKE_SLACK_MESSAGE,\n        slack_conn_id=SLACK_CONN_ID,\n    )\n\n    # [END howto_operator_snowflake_to_slack]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.201852", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 86, "file_name": "example_sqlite.py"}, "content": "\"\"\"\nThis is an example DAG for the use of the SqliteOperator.\nIn this example, we create two tasks that execute in sequence.\nThe first task calls an sql command, defined in the SQLite operator,\nwhich when triggered, is performed on the connected sqlite database.\nThe second task is similar but instead calls the SQL command from an external file.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.sqlite.hooks.sqlite import SqliteHook\nfrom airflow.providers.sqlite.operators.sqlite import SqliteOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sqlite\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@daily\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_sqlite]\n\n    # Example of creating a task that calls a common CREATE TABLE sql command.\n    create_table_sqlite_task = SqliteOperator(\n        task_id=\"create_table_sqlite\",\n        sql=r\"\"\"\n        CREATE TABLE Customers (\n            customer_id INT PRIMARY KEY,\n            first_name TEXT,\n            last_name TEXT\n        );\n        \"\"\",\n    )\n\n    # [END howto_operator_sqlite]\n\n    @dag.task(task_id=\"insert_sqlite_task\")\n    def insert_sqlite_hook():\n        sqlite_hook = SqliteHook()\n\n        rows = [(\"James\", \"11\"), (\"James\", \"22\"), (\"James\", \"33\")]\n        target_fields = [\"first_name\", \"last_name\"]\n        sqlite_hook.insert_rows(table=\"Customers\", rows=rows, target_fields=target_fields)\n\n    @dag.task(task_id=\"replace_sqlite_task\")\n    def replace_sqlite_hook():\n        sqlite_hook = SqliteHook()\n\n        rows = [(\"James\", \"11\"), (\"James\", \"22\"), (\"James\", \"33\")]\n        target_fields = [\"first_name\", \"last_name\"]\n        sqlite_hook.insert_rows(table=\"Customers\", rows=rows, target_fields=target_fields, replace=True)\n\n    # [START howto_operator_sqlite_external_file]\n\n    # Example of creating a task that calls an sql command from an external file.\n    external_create_table_sqlite_task = SqliteOperator(\n        task_id=\"create_table_sqlite_external_file\",\n        sql=\"create_table.sql\",\n    )\n\n    # [END howto_operator_sqlite_external_file]\n\n    (\n        create_table_sqlite_task\n        >> external_create_table_sqlite_task\n        >> insert_sqlite_hook()\n        >> replace_sqlite_hook()\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.229954", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 59, "file_name": "example_tableau.py"}, "content": "\"\"\"\nThis is an example dag that performs two refresh operations on a Tableau Workbook aka Extract. The first one\nwaits until it succeeds. The second does not wait since this is an asynchronous operation and we don't know\nwhen the operation actually finishes. That's why we have another task that checks only that.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.tableau.operators.tableau import TableauOperator\nfrom airflow.providers.tableau.sensors.tableau import TableauJobStatusSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_tableau_refresh_workbook\"\n\nwith DAG(\n    dag_id=\"example_tableau\",\n    default_args={\"site_id\": \"my_site\"},\n    dagrun_timeout=timedelta(hours=2),\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # Refreshes a workbook and waits until it succeeds.\n    # [START howto_operator_tableau]\n    task_refresh_workbook_blocking = TableauOperator(\n        resource=\"workbooks\",\n        method=\"refresh\",\n        find=\"MyWorkbook\",\n        match_with=\"name\",\n        blocking_refresh=True,\n        task_id=\"refresh_tableau_workbook_blocking\",\n    )\n    # [END howto_operator_tableau]\n    # Refreshes a workbook and does not wait until it succeeds.\n    task_refresh_workbook_non_blocking = TableauOperator(\n        resource=\"workbooks\",\n        method=\"refresh\",\n        find=\"MyWorkbook\",\n        match_with=\"name\",\n        blocking_refresh=False,\n        task_id=\"refresh_tableau_workbook_non_blocking\",\n    )\n    # The following task queries the status of the workbook refresh job until it succeeds.\n    task_check_job_status = TableauJobStatusSensor(\n        job_id=task_refresh_workbook_non_blocking.output,\n        task_id=\"check_tableau_job_status\",\n    )\n\n    # Task dependency created via XComArgs:\n    #   task_refresh_workbook_non_blocking >> task_check_job_status\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.231848", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 39, "file_name": "example_tabular.py"}, "content": "from __future__ import annotations\n\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.tabular.hooks.tabular import TabularHook\n\nbash_command = f\"\"\"\necho \"Our token: {TabularHook().get_token_macro()}\"\necho \"Also as an environment variable:\"\nenv | grep TOKEN\n\"\"\"\n\nwith DAG(\n    \"tabular_example\",\n    default_args={\n        \"owner\": \"airflow\",\n        \"depends_on_past\": False,\n        \"email\": [\"airflow@airflow.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=timedelta(1),\n    catchup=False,\n) as dag:\n    # This also works for the SparkSubmit operator\n    BashOperator(\n        task_id=\"with_tabular_environment_variable\",\n        bash_command=bash_command,\n        env={\"TOKEN\": TabularHook().get_token_macro()},\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.233909", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 34, "file_name": "example_telegram.py"}, "content": "\"\"\"\nExample use of Telegram operator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.telegram.operators.telegram import TelegramOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_telegram\"\nCONN_ID = \"telegram_conn_id\"\nCHAT_ID = \"-3222103937\"\n\nwith DAG(DAG_ID, start_date=datetime(2021, 1, 1), tags=[\"example\"]) as dag:\n\n    # [START howto_operator_telegram]\n\n    send_message_telegram_task = TelegramOperator(\n        task_id=\"send_message_telegram\",\n        telegram_conn_id=CONN_ID,\n        chat_id=CHAT_ID,\n        text=\"Hello from Airflow!\",\n        dag=dag,\n    )\n\n    # [END howto_operator_telegram]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.242992", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 38, "file_name": "example_gcs_to_trino.py"}, "content": "\"\"\"\nExample DAG using GCSToTrinoOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\n\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"test28397yeo\")\nPATH_TO_FILE = os.environ.get(\"GCP_PATH\", \"path/to/file\")\nTRINO_TABLE = os.environ.get(\"TRINO_TABLE\", \"test_table\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_trino\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START gcs_csv_to_trino_table]\n    gcs_csv_to_trino_table = GCSToTrinoOperator(\n        task_id=\"gcs_csv_to_trino_table\",\n        source_bucket=BUCKET,\n        source_object=PATH_TO_FILE,\n        trino_table=TRINO_TABLE,\n    )\n    # [END gcs_csv_to_trino_table]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.244861", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 54, "file_name": "example_yandexcloud.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "            args=[\"1000\"],\n        ),\n    )\n    operation = hook.sdk.client(job_service_grpc_pb.JobServiceStub).Create(request)\n    operation_result = hook.sdk.wait_operation_and_get_result(\n        operation, response_type=job_pb.Job, meta_type=job_service_pb.CreateJobMetadata\n    )\n    return MessageToDict(operation_result.response)\n\n\n@task(trigger_rule=\"all_done\")\ndef delete_cluster(\n    cluster_id: str,\n    yandex_conn_id: str | None = None,\n):\n    hook = YandexCloudBaseHook(yandex_conn_id=yandex_conn_id)\n\n    operation = hook.sdk.client(cluster_service_grpc_pb.ClusterServiceStub).Delete(\n        cluster_service_pb.DeleteClusterRequest(cluster_id=cluster_id)\n    )\n    hook.sdk.wait_operation_and_get_result(\n        operation,\n        meta_type=cluster_service_pb.DeleteClusterMetadata,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    cluster_id = create_cluster(\n        folder_id=YC_FOLDER_ID,\n        subnet_id=YC_SUBNET_ID,\n        zone=YC_ZONE_NAME,\n        service_account_id=YC_SERVICE_ACCOUNT_ID,\n    )\n    spark_job = run_spark_job(cluster_id=cluster_id)\n    delete_task = delete_cluster(cluster_id=cluster_id)\n\n    spark_job >> delete_task\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.257903", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 82, "file_name": "example_trino.py"}, "content": "\"\"\"\nExample DAG using TrinoOperator.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.trino.operators.trino import TrinoOperator\n\nSCHEMA = \"hive.cities\"\nTABLE = \"city\"\nTABLE1 = \"city1\"\nTABLE2 = \"city2\"\n\n# [START howto_operator_trino]\n\nwith models.DAG(\n    dag_id=\"example_trino\",\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    trino_create_schema = TrinoOperator(\n        task_id=\"trino_create_schema\",\n        sql=f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA} WITH (location = 's3://irisbkt/cities/');\",\n        handler=list,\n    )\n    trino_create_table = TrinoOperator(\n        task_id=\"trino_create_table\",\n        sql=f\"\"\"CREATE TABLE IF NOT EXISTS {SCHEMA}.{TABLE}(\n        cityid bigint,\n        cityname varchar\n        )\"\"\",\n        handler=list,\n    )\n\n    trino_insert = TrinoOperator(\n        task_id=\"trino_insert\",\n        sql=f\"\"\"INSERT INTO {SCHEMA}.{TABLE} VALUES (1, 'San Francisco');\"\"\",\n        handler=list,\n    )\n\n    trino_multiple_queries = TrinoOperator(\n        task_id=\"trino_multiple_queries\",\n        sql=f\"\"\"CREATE TABLE IF NOT EXISTS {SCHEMA}.{TABLE1}(cityid bigint,cityname varchar);\n        INSERT INTO {SCHEMA}.{TABLE1} VALUES (2, 'San Jose');\n        CREATE TABLE IF NOT EXISTS {SCHEMA}.{TABLE2}(cityid bigint,cityname varchar);\n        INSERT INTO {SCHEMA}.{TABLE2} VALUES (3, 'San Diego');\"\"\",\n        handler=list,\n    )\n\n    trino_templated_query = TrinoOperator(\n        task_id=\"trino_templated_query\",\n        sql=\"SELECT * FROM {{ params.SCHEMA }}.{{ params.TABLE }}\",\n        handler=list,\n        params={\"SCHEMA\": SCHEMA, \"TABLE\": TABLE1},\n    )\n    trino_parameterized_query = TrinoOperator(\n        task_id=\"trino_parameterized_query\",\n        sql=f\"select * from {SCHEMA}.{TABLE2} where cityname = ?\",\n        parameters=(\"San Diego\",),\n        handler=list,\n    )\n\n    (\n        trino_create_schema\n        >> trino_create_table\n        >> trino_insert\n        >> trino_multiple_queries\n        >> trino_templated_query\n        >> trino_parameterized_query\n    )\n\n    # [END howto_operator_trino]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.263873", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 97, "file_name": "example_yandexcloud_dataproc.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        file_uris=[\n            \"s3a://data-proc-public/jobs/sources/mapreduce-001/mapper.py\",\n            \"s3a://data-proc-public/jobs/sources/mapreduce-001/reducer.py\",\n        ],\n        args=[\n            \"-mapper\",\n            \"mapper.py\",\n            \"-reducer\",\n            \"reducer.py\",\n            \"-numReduceTasks\",\n            \"1\",\n            \"-input\",\n            \"s3a://data-proc-public/jobs/sources/data/cities500.txt.bz2\",\n            \"-output\",\n            f\"s3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results/{uuid.uuid4()}\",\n        ],\n        properties={\n            \"yarn.app.mapreduce.am.resource.mb\": \"2048\",\n            \"yarn.app.mapreduce.am.command-opts\": \"-Xmx2048m\",\n            \"mapreduce.job.maps\": \"6\",\n        },\n    )\n\n    create_spark_job = DataprocCreateSparkJobOperator(\n        task_id=\"create_spark_job\",\n        main_jar_file_uri=\"s3a://data-proc-public/jobs/sources/java/dataproc-examples-1.0.jar\",\n        main_class=\"ru.yandex.cloud.dataproc.examples.PopulationSparkJob\",\n        file_uris=[\n            \"s3a://data-proc-public/jobs/sources/data/config.json\",\n        ],\n        archive_uris=[\n            \"s3a://data-proc-public/jobs/sources/data/country-codes.csv.zip\",\n        ],\n        jar_file_uris=[\n            \"s3a://data-proc-public/jobs/sources/java/icu4j-61.1.jar\",\n            \"s3a://data-proc-public/jobs/sources/java/commons-lang-2.6.jar\",\n            \"s3a://data-proc-public/jobs/sources/java/opencsv-4.1.jar\",\n            \"s3a://data-proc-public/jobs/sources/java/json-20190722.jar\",\n        ],\n        args=[\n            \"s3a://data-proc-public/jobs/sources/data/cities500.txt.bz2\",\n            f\"s3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results/${{JOB_ID}}\",\n        ],\n        properties={\n            \"spark.submit.deployMode\": \"cluster\",\n        },\n        packages=[\"org.slf4j:slf4j-simple:1.7.30\"],\n        repositories=[\"https://repo1.maven.org/maven2\"],\n        exclude_packages=[\"com.amazonaws:amazon-kinesis-client\"],\n    )\n\n    create_pyspark_job = DataprocCreatePysparkJobOperator(\n        task_id=\"create_pyspark_job\",\n        main_python_file_uri=\"s3a://data-proc-public/jobs/sources/pyspark-001/main.py\",\n        python_file_uris=[\n            \"s3a://data-proc-public/jobs/sources/pyspark-001/geonames.py\",\n        ],\n        file_uris=[\n            \"s3a://data-proc-public/jobs/sources/data/config.json\",\n        ],\n        archive_uris=[\n            \"s3a://data-proc-public/jobs/sources/data/country-codes.csv.zip\",\n        ],\n        args=[\n            \"s3a://data-proc-public/jobs/sources/data/cities500.txt.bz2\",\n            f\"s3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results/${{JOB_ID}}\",\n        ],\n        jar_file_uris=[\n            \"s3a://data-proc-public/jobs/sources/java/dataproc-examples-1.0.jar\",\n            \"s3a://data-proc-public/jobs/sources/java/icu4j-61.1.jar\",\n            \"s3a://data-proc-public/jobs/sources/java/commons-lang-2.6.jar\",\n        ],\n        properties={\n            \"spark.submit.deployMode\": \"cluster\",\n        },\n        packages=[\"org.slf4j:slf4j-simple:1.7.30\"],\n        repositories=[\"https://repo1.maven.org/maven2\"],\n        exclude_packages=[\"com.amazonaws:amazon-kinesis-client\"],\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    create_cluster >> create_mapreduce_job >> create_hive_query >> create_hive_query_from_file\n    create_hive_query_from_file >> create_spark_job >> create_pyspark_job >> delete_cluster\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.264977", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": false, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 20, "file_name": "example_yandexcloud_dataproc_lightweight.py", "syntax_error": "AST parse error: unexpected indent"}, "content": "        args=[\"1000\"],\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        cluster_id=create_cluster.cluster_id,\n        task_id=\"delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    create_spark_job >> delete_cluster\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.289726", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
{"metadata": {"syntax_valid": true, "airflow_version": "2.7.2", "is_multifile": false, "line_count": 35, "file_name": "example_zendesk_custom_get.py"}, "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.providers.zendesk.hooks.zendesk import ZendeskHook\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"zendesk_custom_get_dag\"\n\n\n@task\ndef fetch_organizations() -> list[dict]:\n    hook = ZendeskHook()\n    response = hook.get(\n        url=\"https://yourdomain.zendesk.com/api/v2/organizations.json\",\n    )\n    return [org.to_dict() for org in response]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    fetch_organizations()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "extracted_at": "2025-11-19T17:21:17.291635", "mining_metadata": {"mining_timestamp": "2025-11-19T17:21:17.293787", "is_test_run": true, "output_file": "datasets/raw/test_dags.jsonl"}}
