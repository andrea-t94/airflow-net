{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4544843f",
   "metadata": {},
   "source": [
    "Better to use the notebook on high RAM CPU.  \n",
    "That's because push_to_hub_gguf method is CPU RAM intensive since, while applying GGUF conversion and quantisation, keeps both the og merged model and the new in RAM.  \n",
    "Another option is to split the work manually into 3 separate steps: download and merge, convert and quantise, push to hub. Each step will save locally the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf677b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fa8a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "Can PyTorch see GPU? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Can PyTorch see GPU? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6451770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  5 13:46:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3735df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Could not find Config class in trl.trainer.dpo_trainer. Found: []\n",
      "Unsloth: Could not find Config class in trl.trainer.iterative_sft_trainer. Found: []\n",
      "Unsloth: Could not find Config class in trl.trainer.sft_trainer. Found: []\n",
      "==((====))==  Unsloth 2025.11.6: Fast Qwen2 patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa53bddba44481a8bbcd5289cc876df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aad1a2fec04669bba0b78b4a0659a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe597875acc64bec8234b564d70dccb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5382727bf81449f8c44c2b1345bd2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a56f003a2534f04adc73b73c0ac7edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755af53f9dc4f70b99d1a9abe80d0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0398f524c7214f98ac69b9a1123e9422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8292b64bd1d24c3b916d46c298ec57cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "max_seq_length = 4096 # covers all DAG files lenght\n",
    "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. (QLoRa)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16*2,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eafabd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589229df1d024c45938047834b33b706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/764 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6c535adea44b0788b3f865f6d67412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d60a7ccf15406c8663f9e1c37e6920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/eval-00000-of-00001.parquet:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a111a777849f88a950224bf33057f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/559k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0e3ba690e4442fabcd13cefd1be9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426b925796d149d9896c1e8118e28f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023fba5b3cb0482cbde4bd7fba1045e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset loaded from Hugging Face Hub\n",
      "\n",
      "Dataset Split Sizes:\n",
      "  Train: 7414 samples\n",
      "  Eval:  412 samples\n",
      "  Test:  412 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc53e74206842959021d04cfb1a34a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd61646f1e462c98190b2644ec94f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe171af14b84dfcac5027f4b3c8df40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chat template applied to all splits\n",
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face Hub (already split into train/eval/test)\n",
    "HF_DATASET = \"andrea-t94/airflow-dag-dataset\"\n",
    "split_dataset = load_dataset(HF_DATASET)\n",
    "\n",
    "print(f\"‚úì Dataset loaded from Hugging Face Hub\")\n",
    "print(f\"\\nDataset Split Sizes:\")\n",
    "print(f\"  Train: {len(split_dataset['train'])} samples\")\n",
    "print(f\"  Eval:  {len(split_dataset['eval'])} samples\")\n",
    "print(f\"  Test:  {len(split_dataset['test'])} samples\")\n",
    "\n",
    "# Dataset is already in ChatML format (messages field)\n",
    "# Apply the chat template for Qwen fine-tuning\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Apply the chat template - tokenizer will handle ChatML formatting\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting to all splits\n",
    "split_dataset = split_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"\\n‚úì Chat template applied to all splits\")\n",
    "print(\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cfe4710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446749ead8b54ad4b89233cb6b5f1f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dd20629ba748029fae9fdc8b29d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,568 | Num Epochs = 3 | Total steps = 336\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 18,464,768 of 1,562,179,072 (1.18% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
      "6.582 GB of memory reserved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 29:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.424876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.293044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.242225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = split_dataset[\"train\"],\n",
    "    eval_dataset = split_dataset[\"eval\"], # Pass the validation set here\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can set to True for speed boost, but be careful with short seqs\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4, # Increased from 2 -> 16 (T4 handles this easily for 1.5B)\n",
    "        gradient_accumulation_steps = 8,  # 4 * 8 = Effective Batch Size of 32. \n",
    "        warmup_steps = 10,\n",
    "        max_steps = -1,\n",
    "        num_train_epochs = 3, # Start with 1 epoch to test time! typically 3 is good.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\", # Key for memory saving\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_strategy = \"steps\", # Save checkpoint every epoch\n",
    "        eval_strategy = \"steps\", # Check eval loss during training\n",
    "        eval_steps = 100, # Evaluate every 100 steps\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "\n",
    "# START TRAINING\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447f839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3507175a91a4a9591c026eb592301fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3158324fe34c47c2879117504e2cdc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3165b873f434d9592e5275b0921b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04e6246c3524682851d5f7d902b2ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acdfc3ef2e44df3a524e655eb7a7b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:   0%|          | 45.7kB / 73.9MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/andrea-t94/qwen2.5-1.5b-airflow-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e7fbd181c04111adaea1975651b490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf4c1dd05c040e1bfac9da3db0b4605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264827d5b88549679f10acbd09e5bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mphzn090vv/tokenizer.json: 100%|##########| 11.4MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved LoRA Adapters (Source)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432c88850ce0489596bff9263d8f547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6c33ec2ebe42e0a9ad4eb6d6ab398b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82d4f1e76c749c8aa2dc5754f0307d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01946511412e4a4a9b1d8a9f81530885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...uct-merged/tokenizer.json: 100%|##########| 11.4MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f82377b3dfe45f9894d28f946e26926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2d02ae93cb44b3b82a6d4e634ff8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b340ea726d4737855e256611de0fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393611702a4d4d73a236eb86089bd0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-merged/model.safetensors:   2%|1         | 58.7MB / 3.09GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:47<00:00, 47.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/andrea-t94/qwen2.5-1.5b-airflow-instruct-merged`\n",
      "‚úÖ Saved Merged FP16 Model (Standard)\n",
      "üßπ Cleaning RAM before GGUF conversion...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-624490491.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. You must be logged in to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "repo_name = \"andrea-t94/qwen2.5-1.5b-airflow-instruct\"\n",
    "base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# --- A. Push LoRA Adapters (Most Portable & Lightweight) ---\n",
    "# This allows anyone to load your fine-tune on top of Qwen-Base\n",
    "model.push_to_hub(repo_name,\n",
    "                   token = 'hf_...')\n",
    "tokenizer.push_to_hub(repo_name,\n",
    "                      token = 'hf_...')\n",
    "print(\"‚úÖ Saved LoRA Adapters (Source)\")\n",
    "\n",
    "# --- B. Push Merged FP16 Model (The \"Standard\" Standalone) ---\n",
    "# This merges the adapters into the base model and saves as full precision (safetensors).\n",
    "# Use this if you want to deploy to vLLM later or re-quantize to AWQ/GPTQ.\n",
    "model.push_to_hub_merged(\n",
    "    repo_name + \"-merged\", \n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    "    token = 'hf_...'\n",
    ")\n",
    "print(\"‚úÖ Saved Merged FP16 Model (Standard)\")\n",
    "\n",
    "print(\"‚è≥ (3/3) Converting and Pushing GGUF...\")\n",
    "model.push_to_hub_gguf(\n",
    "    repo_name + \"-GGUF\",\n",
    "    tokenizer,\n",
    "    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "    token = 'hf_...'\n",
    ")\n",
    "print(\"‚úÖ All steps complete! GGUF Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b83a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "üìä EVALUATION: Base vs. Fine-Tuned\n",
      "=========================================\n",
      "\n",
      "üîπ TEST SAMPLE 1\n",
      "üìù PROMPT: <|im_start|>system\n",
      "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs... (truncated)\n",
      "----------------------------------------\n",
      "üß† BASE MODEL:\n",
      "```python\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "from airflow import DAG\n",
      "from airflow.operators.dummy_operator import DummyOperator\n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "import pendulum\n",
      "\n",
      "# Define DAG settings\n",
      "DAG_ID = \"snowflake_product_data_pipeline\"\n",
      "SCHEDULE_INTERVAL = No...\n",
      "--------------------\n",
      "üöÄ FINE-TUNED:\n",
      "To create a data pipeline in Apache Airflow to load sample product data into a Snowflake table and validate the data load, follow these steps:\n",
      "\n",
      "1. Install the necessary Python libraries.\n",
      "2. Set up your Airflow environment.\n",
      "3. Create the DAG structure.\n",
      "4. Define tasks for loading data and validation....\n",
      "----------------------------------------\n",
      "‚úÖ GROUND TRUTH:\n",
      "\"\"\"\n",
      "Example use of Snowflake Snowpark Python related operators.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import TYPE_CHECKING\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from snowflake.snowpark import Session\n",
      "\n",
      "from airflow import DAG\n",
      "from airflow.providers.snowflake.operators.sn...\n",
      "=========================================\n",
      "\n",
      "üîπ TEST SAMPLE 2\n",
      "üìù PROMPT: <|im_start|>system\n",
      "You are a helpful coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "I have a pandas Da... (truncated)\n",
      "----------------------------------------\n",
      "üß† BASE MODEL:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create your DataFrame\n",
      "data = ['Data', 'DataFrame', 'API', 1234, {'key': 'value'}]\n",
      "df = pd.DataFrame(data=data, columns=['text'])\n",
      "\n",
      "def add_length_column(df):\n",
      "    \"\"\"\n",
      "    Adds a new column 'text_len' to the dataframe df.\n",
      "    This column contains the lengths of strings ...\n",
      "--------------------\n",
      "üöÄ FINE-TUNED:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Creating a sample dataframe\n",
      "data = ['Data Science', 'Machine Learning', 'Deep Learning']\n",
      "df = pd.DataFrame(data=data, columns=['text'])\n",
      "\n",
      "def add_length_column(df):\n",
      "    \"\"\"\n",
      "    Adds a new column 'text_len' to the dataframe which represents \n",
      "    the length of each stri...\n",
      "----------------------------------------\n",
      "‚úÖ GROUND TRUTH:\n",
      "You can easily add a new column `text_len` to your DataFrame `df` that contains the length of each string in the `text` column by using the `str.len()` method provided by pandas. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Sample DataFrame\n",
      "data = {'text': ['hello', 'world', 'pandas',...\n",
      "=========================================\n",
      "\n",
      "üîπ TEST SAMPLE 3\n",
      "üìù PROMPT: <|im_start|>system\n",
      "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs... (truncated)\n",
      "----------------------------------------\n",
      "üß† BASE MODEL:\n",
      "```python\n",
      "from datetime import timedelta, date\n",
      "from airflow import DAG\n",
      "from airflow.operators.empty import EmptyOperator\n",
      "from airflow.utils.dates import days_ago\n",
      "\n",
      "# Define default arguments for the DAG\n",
      "default_args = {\n",
      "    'owner': 'airflow',\n",
      "    'start_date': date(2024, 1, 1),\n",
      "}\n",
      "\n",
      "# Create a DAG obj...\n",
      "--------------------\n",
      "üöÄ FINE-TUNED:\n",
      "```python\n",
      "from datetime import timedelta\n",
      "from airflow.models.dag import DAG\n",
      "from airflow.utils.state import State\n",
      "from airflow.contrib.sensors.after_workday_timetable import AfterWorkdayTimetableSensor\n",
      "\n",
      "# Define DAG properties\n",
      "DAG_ID = \"custom_after_workday_dag\"\n",
      "SCHEDULE_INTERVAL = \"*/5 * * * *\"  # ...\n",
      "----------------------------------------\n",
      "‚úÖ GROUND TRUTH:\n",
      "from __future__ import annotations\n",
      "\n",
      "import pendulum\n",
      "\n",
      "from airflow.example_dags.plugins.workday import AfterWorkdayTimetable\n",
      "from airflow.providers.standard.operators.empty import EmptyOperator\n",
      "from airflow.sdk import DAG\n",
      "\n",
      "with DAG(\n",
      "    dag_id=\"example_workday_timetable\",\n",
      "    start_date=pendulum.date...\n",
      "=========================================\n",
      "\n",
      "üîπ TEST SAMPLE 4\n",
      "üìù PROMPT: <|im_start|>system\n",
      "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs... (truncated)\n",
      "----------------------------------------\n",
      "üß† BASE MODEL:\n",
      "To create the specified workflow in Airflow, we'll need to use several Airflow operators such as `GetMetricsOperator`, `ExecuteReportOperator`, `GetCampaignConversionDataOperator`, and others. Below is a sample DAG that outlines this process:\n",
      "\n",
      "```python\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "from...\n",
      "--------------------\n",
      "üöÄ FINE-TUNED:\n",
      "To create the specified workflow in Apache Airflow, we'll follow these steps:\n",
      "\n",
      "1. **Define Airflow DAG**: Create a new DAG named `campaign_metrics_dag` with specific settings for scheduling.\n",
      "2. **Setup Python Operators**: Use PythonOperator to execute scripts or tasks that interact with Campaign Man...\n",
      "----------------------------------------\n",
      "‚úÖ GROUND TRUTH:\n",
      "\"\"\"\n",
      "Example Airflow DAG that shows how to use CampaignManager.\n",
      "\n",
      "This system test relies on a service account with proper settings in Campaign Manager 360.\n",
      "That's why before running this system test locally, make sure your service account corresponds all the\n",
      "secrets that the DAG reads. If your servic...\n",
      "=========================================\n",
      "\n",
      "üîπ TEST SAMPLE 5\n",
      "üìù PROMPT: <|im_start|>system\n",
      "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs... (truncated)\n",
      "----------------------------------------\n",
      "üß† BASE MODEL:\n",
      "To construct the required data migration workflow using Apache Airflow, we will create tasks for each step in the process, including loading data from cloud object storage (e.g., Google Cloud Storage) into a PostgreSQL database. Here's a sample DAG design:\n",
      "\n",
      "```python\n",
      "from datetime import datetime, t...\n",
      "--------------------\n",
      "üöÄ FINE-TUNED:\n",
      "To create a data migration workflow in Apache Airflow for transferring structured CSV data from cloud object storage into a columnar analytics database, we'll need to follow these steps:\n",
      "\n",
      "1. Define the tasks and dependencies.\n",
      "2. Set up the necessary connections and resources.\n",
      "3. Implement the data l...\n",
      "----------------------------------------\n",
      "‚úÖ GROUND TRUTH:\n",
      "\"\"\"\n",
      "Example DAG using GCSToPrestoOperator.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "from datetime import datetime\n",
      "\n",
      "from airflow import models\n",
      "from airflow.providers.presto.transfers.gcs_to_presto import GCSToPrestoOperator\n",
      "\n",
      "BUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"test28397yeo\")\n",
      "PATH_TO_...\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Force the model into inference mode (Much faster)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# 2. Define 3 distinct test cases\n",
    "test_prompts = [\n",
    "    \"Create a DAG that runs a bash script every morning at 6am.\",\n",
    "    \"Create a DAG with a PythonOperator that pulls data from S3 and pushes to Postgres.\",\n",
    "    \"Create a DAG that branches based on the day of the week.\"\n",
    "]\n",
    "\n",
    "print(\"=== STARTING SMOKE TEST ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"--- TEST CASE {i+1}: {prompt} ---\\n\")\n",
    "\n",
    "    # [CORRECTED SECTION STARTS HERE]\n",
    "    # We create a message list, just like your training dataset structure\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Create an Airflow DAG for: {prompt}\"}\n",
    "    ]\n",
    "\n",
    "    # Apply the template. \n",
    "    # add_generation_prompt=True is the KEY: it adds the \"Assistant:\" start token \n",
    "    # that forces the model to begin answering.\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, \n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 512,\n",
    "        use_cache = True,\n",
    "        # Qwen/ChatML often uses these stop tokens. \n",
    "        # You can also add \"```\" if you want it to stop after code.\n",
    "        stop_strings = [\"<|im_end|>\", \"<|endoftext|>\"] \n",
    "    )\n",
    "    # [CORRECTED SECTION ENDS HERE]\n",
    "\n",
    "    # Decode - skipping the prompt (input_ids) to see only the answer\n",
    "    result = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "print(\"=== TEST COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
