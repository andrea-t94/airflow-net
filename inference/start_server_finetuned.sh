#!/bin/bash
# Starts the C++ Server with Parallel Slots
# Auto-generated by build_llamaserver.sh

# HuggingFace Model Configuration
HF_MODEL_REPO="andrea-t94/qwen2.5-1.5b-airflow-instruct-GGUF"
HF_MODEL_FILE="qwen2.5-1.5b-instruct.Q5_K_M.gguf"

# Server Configuration
N_GPU_LAYERS=99
CTX_SIZE=34816
PARALLEL_SLOTS=8
SERVER_PORT=8000
BATCH_SIZE=2048

echo "ðŸš€ Starting llama-server..."
echo "ðŸ“¦ Model: $HF_MODEL_REPO/$HF_MODEL_FILE"
echo "ðŸ”§ Config: $PARALLEL_SLOTS slots, ctx=$CTX_SIZE, gpu_layers=$N_GPU_LAYERS, port=$SERVER_PORT"
echo ""

./llama.cpp/build/bin/llama-server \
    --hf-repo "$HF_MODEL_REPO" \
    --hf-file "$HF_MODEL_FILE" \
    --n-gpu-layers $N_GPU_LAYERS \
    --ctx-size $CTX_SIZE \
    --batch-size 2048 \
    --parallel $PARALLEL_SLOTS \
    --cont-batching \
    --flash-attn on \
    --port $SERVER_PORT