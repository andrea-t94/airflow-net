{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notebook_header"
   },
   "source": [
    "# Airflow DAG Generation - Model Inference on GPU\n",
    "\n",
    "This notebook runs inference on the test dataset comparing:\n",
    "- **Base Model**: Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
    "- **Fine-tuned Model**: andrea-t94/qwen2.5-1.5b-airflow-instruct (LoRA adapter)\n",
    "\n",
    "**Dataset**: andrea-t94/airflow-dag-dataset (test split)\n",
    "\n",
    "## Setup Instructions\n",
    "1. Runtime â†’ Change runtime type â†’ T4 GPU (or better)\n",
    "2. Run all cells in order\n",
    "3. Models and dataset will be cached for re-runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "## 1. GPU Check and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 17 10:39:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected. Please enable GPU in Runtime â†’ Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "## 2. Install Dependencies (Unsloth for Fast Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.6/180.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ… Unsloth and dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install Unsloth for optimized inference with flash attention\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q datasets>=3.2.0\n",
    "\n",
    "print(\"âœ… Unsloth and dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_dataset"
   },
   "source": [
    "## 3. Load Test Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dataset_loading"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980aec8395894e68b0b06b0af83fcba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0762e865e587492aaf269b2a8b2ecf67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1c8f196aa444eb6baec72540b17a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/560k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef7c33087e491282a2b2a979b11deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/eval-00000-of-00001.parquet:   0%|          | 0.00/574k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8046b9a8578745789864a6385ff8d3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8ccd468582473ca93d40d88a40b41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b94e6a83a6b4cc8b90dc4085a8719b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Loaded 412 test examples\n",
      "\n",
      "Dataset composition:\n",
      "  - Airflow examples: 342 (83.0%)\n",
      "  - Magpie examples: 70 (17.0%)\n",
      "\n",
      "First example:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\\n\\nAirflow Version: 3.0.1\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load test split (will be cached automatically)\n",
    "print(\"Loading test dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\n",
    "    \"andrea-t94/airflow-dag-dataset\",\n",
    "    split=\"test\",\n",
    "    download_mode=\"reuse_cache_if_exists\"  # Use cached version if available\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(dataset)} test examples\")\n",
    "\n",
    "# Show dataset statistics\n",
    "airflow_count = sum(1 for x in dataset if x.get('source') == 'airflow')\n",
    "magpie_count = sum(1 for x in dataset if x.get('source') == 'magpie')\n",
    "\n",
    "print(f\"\\nDataset composition:\")\n",
    "print(f\"  - Airflow examples: {airflow_count} ({airflow_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  - Magpie examples: {magpie_count} ({magpie_count/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "# Preview first example\n",
    "print(f\"\\nFirst example:\")\n",
    "print(json.dumps(dataset[0]['messages'][:2], indent=2))  # Show system + user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\\n\\nAirflow Version: 3.0.1'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\"\"\"\\nExample use of Snowflake Snowpark Python related operators.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom datetime import datetime\\nfrom typing import TYPE_CHECKING\\n\\nif TYPE_CHECKING:\\n    from snowflake.snowpark import Session\\n\\nfrom airflow import DAG\\nfrom airflow.providers.snowflake.operators.snowpark import SnowparkOperator\\n\\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\\nDAG_ID = \"example_snowpark_operator\"\\n\\nwith DAG(\\n    DAG_ID,\\n    start_date=datetime(2024, 1, 1),\\n    schedule=\"@once\",\\n    default_args={\"snowflake_conn_id\": SNOWFLAKE_CONN_ID},\\n    tags=[\"example\"],\\n    catchup=False,\\n) as dag:\\n    # [START howto_operator_snowpark]\\n    def setup_data(session: Session):\\n        # The Snowpark session object is injected as an argument\\n        data = [\\n            (1, 0, 5, \"Product 1\", \"prod-1\", 1, 10),\\n            (2, 1, 5, \"Product 1A\", \"prod-1-A\", 1, 20),\\n            (3, 1, 5, \"Product 1B\", \"prod-1-B\", 1, 30),\\n            (4, 0, 10, \"Product 2\", \"prod-2\", 2, 40),\\n            (5, 4, 10, \"Product 2A\", \"prod-2-A\", 2, 50),\\n            (6, 4, 10, \"Product 2B\", \"prod-2-B\", 2, 60),\\n            (7, 0, 20, \"Product 3\", \"prod-3\", 3, 70),\\n            (8, 7, 20, \"Product 3A\", \"prod-3-A\", 3, 80),\\n            (9, 7, 20, \"Product 3B\", \"prod-3-B\", 3, 90),\\n            (10, 0, 50, \"Product 4\", \"prod-4\", 4, 100),\\n            (11, 10, 50, \"Product 4A\", \"prod-4-A\", 4, 100),\\n            (12, 10, 50, \"Product 4B\", \"prod-4-B\", 4, 100),\\n        ]\\n        columns = [\"id\", \"parent_id\", \"category_id\", \"name\", \"serial_number\", \"key\", \"3rd\"]\\n        df = session.create_dataframe(data, schema=columns)\\n        table_name = \"sample_product_data\"\\n        df.write.save_as_table(table_name, mode=\"overwrite\")\\n        return table_name\\n\\n    setup_data_operator = SnowparkOperator(\\n        task_id=\"setup_data\",\\n        python_callable=setup_data,\\n        dag=dag,\\n    )\\n\\n    def check_num_rows(table_name: str):\\n        # Alternatively, retrieve the Snowpark session object using `get_active_session`\\n        from snowflake.snowpark.context import get_active_session\\n\\n        session = get_active_session()\\n        df = session.table(table_name)\\n        assert df.count() == 12\\n\\n    check_num_rows_operator = SnowparkOperator(\\n        task_id=\"check_num_rows\",\\n        python_callable=check_num_rows,\\n        op_kwargs={\"table_name\": \"{{ task_instance.xcom_pull(task_ids=\\'setup_data\\') }}\"},\\n        dag=dag,\\n    )\\n\\n    setup_data_operator >> check_num_rows_operator\\n    # [END howto_operator_snowpark]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'}],\n",
       " 'metadata': {'file_name': 'dag_1798_variant_1',\n",
       "  'instruction_source': 'claude-3.5',\n",
       "  'variant_number': 1,\n",
       "  'airflow_version': '3.0.1'},\n",
       " 'source': 'airflow'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_models"
   },
   "source": [
    "## 4. Load Models\n",
    "\n",
    "We'll load both:\n",
    "1. Base model: Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
    "2. Fine-tuned model: Base + LoRA adapter from andrea-t94/qwen2.5-1.5b-airflow-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "model_loading"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-444238182.py:1: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading models with Unsloth...\n",
      "============================================================\n",
      "\n",
      "1. Loading BASE model...\n",
      "==((====))==  Unsloth 2025.12.5: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c5382847fa4614aa1c7fdd461e582e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbb64fb241d4dd7a3c563ce63b86233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55026e3f9f8648759ebd5cc183800c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c858d76b7c8442f803b0d758038e423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00d1874def54c92afcbb1498970c653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aba8ed761d24d9e9447be84e4a5c683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46263dfd4f59439f8d977184742d4db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845cb7e2198e40819d7cceb9709cbb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base model loaded: Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "\n",
      "2. Loading FINE-TUNED model...\n",
      "==((====))==  Unsloth 2025.12.5: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82b5476ee684d928ba2531ad60360f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757a5fe09e0b411e8e503abfc65b60bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a196d9fecad45a4a45ad8395d1defb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6ae38de5a400a91c7b0c66114820f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfcb20910434de6a0ee609ab197fc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77558f45ceee42bd86acaf440389ab3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b30f1181e94154aae27a1306125291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca2c7a7e788407cbf553921a73aff3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuned model loaded: andrea-t94/qwen2.5-1.5b-airflow-instruct\n",
      "\n",
      "============================================================\n",
      "âœ… All models loaded.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model identifiers\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "FINETUNED_ADAPTER_ID = \"andrea-t94/qwen2.5-1.5b-airflow-instruct\"\n",
    "\n",
    "# Unsloth configuration\n",
    "max_seq_length = 4096\n",
    "dtype = None            # Auto-detect (float16 for T4)\n",
    "load_in_4bit = False    # False for speed/precision, True for memory savings\n",
    "\n",
    "print(\"Loading models with Unsloth...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Loading the BASE model (Pure Base)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n1. Loading BASE model...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_ID,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "print(f\"âœ… Base model loaded: {BASE_MODEL_ID}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Loading the FINE-TUNED model (Base + Adapter)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n2. Loading FINE-TUNED model...\")\n",
    "# OPTION A: The Unsloth \"Magic\" Way \n",
    "# (Pass the Adapter ID directly; Unsloth finds the base model automatically)\n",
    "finetuned_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = FINETUNED_ADAPTER_ID, # <--- Put Adapter ID here\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(finetuned_model)\n",
    "print(f\"âœ… Fine-tuned model loaded: {FINETUNED_ADAPTER_ID}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… All models loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_setup"
   },
   "source": [
    "## 5. Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference configuration (Unsloth optimized):\n",
      "  max_new_tokens: 4096\n",
      "  temperature: 0.1\n",
      "  top_p: 0.9\n",
      "  do_sample: True\n",
      "  use_cache: True\n"
     ]
    }
   ],
   "source": [
    "# Unsloth TextStreamer configuration for fast generation\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Apply chat template to tokenizer\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",\n",
    ")\n",
    "\n",
    "# Generation parameters optimized for Unsloth\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 4096,  # Reduced for speed\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"use_cache\": True,  # Enable KV cache for speed\n",
    "}\n",
    "\n",
    "print(\"Inference configuration (Unsloth optimized):\")\n",
    "for key, value in GENERATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions"
   },
   "source": [
    "## 6. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helpers"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined (Unsloth optimized)\n"
     ]
    }
   ],
   "source": [
    "def extract_code_from_response(response_text):\n",
    "    \"\"\"Extract Python code from model response.\"\"\"\n",
    "    # Try to find code block\n",
    "    if \"```python\" in response_text:\n",
    "        start_idx = response_text.find(\"```python\") + len(\"```python\")\n",
    "        end_idx = response_text.find(\"```\", start_idx)\n",
    "        if end_idx != -1:\n",
    "            return response_text[start_idx:end_idx].strip()\n",
    "    \n",
    "    # Try generic code block\n",
    "    if \"```\" in response_text:\n",
    "        parts = response_text.split(\"```\")\n",
    "        if len(parts) >= 3:\n",
    "            return parts[1].strip()\n",
    "    \n",
    "    return response_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_inference"
   },
   "source": [
    "## 7. Run Inference on Test Dataset (Optimized)\n",
    "\n",
    "This will process models **separately** to maximize speed and avoid memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "inference"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: True Parallel Batching (Batch Size: 8)\n",
      "\n",
      "ðŸ¤– STEP 1/2: Loading & Running BASE model...\n",
      "==((====))==  Unsloth 2025.12.5: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfe8db08ec04710a83aa4afefe57994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Base Model:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ STEP 2/2: Loading & Running FINETUNED model...\n",
      "==((====))==  Unsloth 2025.12.5: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b186ad7f964d2cb0d5e3226f98531f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuned Model:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… Inference Complete.\n",
      "Total Base Results: 10\n",
      "Total Finetuned Results: 10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "MAX_EXAMPLES = 10  # Set to None for full run\n",
    "# Increase Batch Size for parallel processing \n",
    "# (For a 1.5B model on T4 GPU, you can likely push this to 16 or 32)\n",
    "BATCH_SIZE = 8     \n",
    "\n",
    "# Create the test subset\n",
    "test_examples = dataset.select(range(min(MAX_EXAMPLES, len(dataset)))) if MAX_EXAMPLES else dataset\n",
    "\n",
    "# Helper to slice dataset cleanly\n",
    "def get_batch_as_list(dataset, start_idx, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(dataset))\n",
    "    return dataset.select(range(start_idx, end_idx)).to_list()\n",
    "\n",
    "# Helper to extract code (ensure this is defined)\n",
    "def extract_code_from_response(text):\n",
    "    if \"```python\" in text:\n",
    "        return text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    if \"```\" in text:\n",
    "        return text.split(\"```\")[1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PARALLEL GENERATION FUNCTION (The Speed Upgrade)\n",
    "# ---------------------------------------------------------\n",
    "def generate_parallel(model, tokenizer, examples, model_name=\"model\"):\n",
    "    results = []\n",
    "    \n",
    "    # 1. Prepare Prompts\n",
    "    prompts = []\n",
    "    for example in examples:\n",
    "        messages = example['messages']\n",
    "        # Apply template to inputs only\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages[:-1], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # 2. Batch Tokenize (Left Padding is crucial here!)\n",
    "    # We do this inside the function to ensure the tokenizer config is respected\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,           # Pad to the longest sequence in this batch\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 3. Generate (Processes all 8 rows simultaneously)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **GENERATION_CONFIG,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # 4. Batch Decode\n",
    "    # Slice off the input prompt tokens from the output\n",
    "    generated_ids = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    decoded_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # 5. Reassemble Results\n",
    "    for i, text in enumerate(decoded_texts):\n",
    "        original_msg = examples[i]['messages']\n",
    "        results.append({\n",
    "            'messages': original_msg[:-1] + [{'role': 'assistant', 'content': extract_code_from_response(text)}],\n",
    "            'metadata': {\n",
    "                **(examples[i].get('metadata') or {}),\n",
    "                'model': model_name,\n",
    "                'inference_time': time.time()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "print(f\"Strategy: True Parallel Batching (Batch Size: {BATCH_SIZE})\")\n",
    "\n",
    "# --- STEP 1: BASE MODEL ---\n",
    "print(\"\\nðŸ¤– STEP 1/2: Loading & Running BASE model...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_ID,\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "# CRITICAL: Configure Tokenizer for Batch Generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(test_examples), BATCH_SIZE), desc=\"Base Model\"):\n",
    "    batch_data = get_batch_as_list(test_examples, i, BATCH_SIZE)\n",
    "    batch_results = generate_parallel(base_model, tokenizer, batch_data, \"Qwen2.5-Base\")\n",
    "    base_results.extend(batch_results)\n",
    "\n",
    "# Cleanup\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# --- STEP 2: FINETUNED MODEL ---\n",
    "print(\"\\nâœ¨ STEP 2/2: Loading & Running FINETUNED model...\")\n",
    "finetuned_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=FINETUNED_ADAPTER_ID,\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetuned_model)\n",
    "\n",
    "# CRITICAL: Re-configure Tokenizer (Loading a new model might reset it)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "finetuned_results = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_examples), BATCH_SIZE), desc=\"Finetuned Model\"):\n",
    "    batch_data = get_batch_as_list(test_examples, i, BATCH_SIZE)\n",
    "    batch_results = generate_parallel(finetuned_model, tokenizer, batch_data, \"Qwen2.5-Finetuned\")\n",
    "    finetuned_results.extend(batch_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… Inference Complete.\")\n",
    "print(f\"Total Base Results: {len(base_results)}\")\n",
    "print(f\"Total Finetuned Results: {len(finetuned_results)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\\n\\nAirflow Version: 3.0.1'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'from __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow import DAG\\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nSNOWFLAKE_SAMPLE_TABLE = \"sample\"\\nSNOWFLAKE_CONN_ID = \"snowflake\"\\n\\nwith DAG(\\n    dag_id=f\"example_snowflake_insert_many_{ENV_ID}\",\\n    start_date=datetime(2021, 1, 1),\\n    schedule=None,\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_operator_snowflake]\\n    insert_data_snowflake = SnowflakeOperator(\\n        task_id=\"insert_data_snowflake\",\\n        sql=\"\"\"\\n            INSERT INTO SNOWFLAKE_SAMPLE_TABLE VALUES\\n            (4, \\'Cell Phone\\', 500, \\'2020-03-15 13:00:00\\'),\\n            (3, \\'Personal Computer\\', 700, \\'2020-03-15 13:00:00\\'),\\n            (1, \\'TV\\', 300, \\'2020-03-15 13:00:00\\'),\\n            (2, \\'Radio\\', 200, \\'2020-03-15 13:00:00\\');\\n        \"\"\",\\n    )\\n    # [END howto_operator_snowflake]\\n\\n    (insert_data_snowflake >> SnowflakeOperator(task_id=\"insert_count\", sql=\"SELECT COUNT(*) FROM %s\" % SNOWFLAKE_SAMPLE_TABLE))\\n# [START howto_triggerer_snowflake]\\n    trigger_query = SnowflakeOperator(task_id=\"trigger_query\", sql=\"SELECT COUNT(*) FROM %s\" % SNOWFLAKE_SAMPLE_TABLE)\\n    join_task = SnowflakeOperator(task_id=\"join_task\", sql=\"JOIN {{ task_instance.xcom_pull(\\'trigger_query\\')[\\'returning._cols\\'][0] }} ON 1=1\")\\n# [END howto_triggerer_snowflake]\\n\\n    (trigger_query >> join_task)\\n\\n    # [START howto_run_pipeline]\\n    run_pipeline = SnowflakeOperator(\\n        task_id=\"run_pipeline\",\\n        sql=f\"LIST SCHEMAS;\",\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n    # [END howto_run_pipeline]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_1798_variant_1',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 1,\n",
       "   'airflow_version': '3.0.1',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251381}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'I have a pandas DataFrame `df` with a column `text` that contains strings. I want to create a new column `text_len` that contains the length of each string in the `text` column. How can I achieve this?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"# Assuming df is already defined and has a 'text' column\\n\\n# Calculate the length of each string in the 'text' column\\ndf['text_len'] = df['text'].apply(lambda x: len(x))\\n\\nprint(df)\"}],\n",
       "  'metadata': {'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251493}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Construct an Airflow DAG that showcases the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class. Build a simple workflow that triggers an empty operator after standard work hours to illustrate scheduling flexibility.\\n\\nAirflow Version: 3.0.0'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'from __future__ import annotations\\n\\nimport pendulum\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.standard.operators.empty import EmptyOperator\\nfrom airflow.providers.standard.utils.weekday.timetables import AfterWorkdayTimetable\\nfrom airflow.utils.weekday import WeekDay\\n\\nwith DAG(\\n    dag_id=\"example_after_workday_timetable\",\\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\\n    schedule=\"@daily\",\\n    catchup=False,\\n    tags=[\"example\"],\\n):\\n    run_this = EmptyOperator(task_id=\"run_this\")\\n    AfterWorkdayTimetable(check_time=(WeekDay.FRIDAY, \"09:30\"), task_ids=[\"run_task\"])\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_1063_variant_2',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 2,\n",
       "   'airflow_version': '3.0.0',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251536}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': \"Create a workflow that retrieves Campaign Manager metrics, generates a standard performance report across campaigns, and demonstrates batch conversion tracking and reporting capabilities. The DAG should illustrate how to interact with Campaign Manager's reporting and conversion APIs using Airflow operators.\\n\\nAirflow Version: 3.0.0\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '# [START campaign_manager_gcp_marketing_api_report_job_create_operator]\\nfrom __future__ import annotations\\n\\nimport os\\n\\nfrom datetime import datetime\\nfrom urllib.parse import urlsplit\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.google.marketing_platform.operators.campaign_manager import (\\n    GoogleCampaignManagerBatchInsertReportJobOperator,\\n    GoogleCampaignManagerDeleteReportJobOperator,\\n    GoogleCampaignManagerReportJobCreateOperator,\\n    GoogleCampaignManagerReportJobDeleteOperator,\\n)\\nfrom airflow.providers.google.marketing_platform.sensors.campaign_manager import (\\n    GoogleCampaignManagerReportJobCreateSensor,\\n    GoogleCampaignManagerReportJobDeleteSensor,\\n)\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\\n\\nDAG_ID= os.environ.get(\"GMP_DAG_ID\", \"campaign_manager_report_job\")\\n\\nLOCATION = \"europe-west1\"\\nCROSS_SITE_REPORT = {\\n    \"destination\": {\\n        \"type\": \"COLORED_JSON\",\\n        \"parameters\": {\"file_name\": \"path/to/report.jsonc\"},\\n    },\\n    \"send_time_interval\": 300,\\n}\\nCRITICAL_COLUMNS = [\"CampaignId\", \"AdGroupId\", \"ConversionType\"]\\nREPORT_CONFIG = {\\n    \"location\": LOCATION,\\n    \"reportType\": \"CSV\",\\n    \"dimensions\": [\"CampaignId\", \"CampaignName\"],  # CSV supports multiple dimensions\\n    \"metricssToReport\": [\"ImpressionCount\", \"ConversionCount\"],\\n    \"criticalColumns\": CRITICAL_COLUMNS,\\n}\\nBATCH_REPORT_CONFIG = [\\n    REPORT_CONFIG,\\n    CROSS_SITE_REPORT,\\n]  # Supports batch operation on multiple reports\\n\\n\\nwith DAG(\\n    dag_id=DAG_ID,\\n    start_date=datetime.strptime(os.environ.get(\"DATE_START\", \"2023-01-01\"), \"%Y-%m-%d\"),\\n    schedule=\"@once\",\\n    catchup=False,\\n) as dag:\\n    # [START howto_gmp_report_job_create_operator]\\n    create_report_job = GoogleCampaignManagerReportJobCreateOperator(\\n        task_id=\"create_report_job\",\\n        location=LOCATION,\\n        report_config=REPORT_CONFIG,\\n        google_cloud_project_id=PROJECT_ID,\\n        google_campaign_manager_account_id=\"1234567890\",\\n        delay_delete=True,\\n    )\\n    # [END howto_gmp_report_job_create_operator]\\n\\n    create_batch_insert_report_job = GoogleCampaignManagerBatchInsertReportJobOperator(\\n        task_id=\"create_batch_insert_report_job\",\\n        location=LOCATION,\\n        report_configs=BATCH_REPORT_CONFIG,\\n        google_cloud_project_id=PROJECT_ID,\\n        google_campaign_manager_account_id=\"1234567890\",\\n        batch_size=len(BATCH_REPORT_CONFIG),\\n        async_task=True,\\n    )\\n\\n    # [START howto_gmp_report_job_delete_sensor]\\n    delete_report_job_sensor = GoogleCampaignManagerReportJobDeleteSensor(\\n        task_id=\"delete_report_job_sensor\",\\n        location=LOCATION,\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        poke_interval=60,\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n    # [END howto_gmp_report_job_delete_sensor]\\n\\n    # [START howto_gmp_report_job_create_sensor]\\n    create_report_job_sensor = GoogleCampaignManagerReportJobCreateSensor(\\n        task_id=\"create_report_job_sensor\",\\n        location=LOCATION,\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_batch_insert_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        poke_interval=10,\\n    )\\n    # [END howto_gmp_report_job_create_sensor]\\n\\n    delete_report_job = GoogleCampaignManagerDeleteReportJobOperator(\\n        task_id=\"delete_report_job\",\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n\\n    create_batch_insert_report_job >> delete_report_job\\n\\n    delete_report_job_sensor >> create_batch_insert_report_job\\n    create_report_job_sensor >> delete_report_job\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_1336_variant_2',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 2,\n",
       "   'airflow_version': '3.0.0',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251641}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Construct a data migration workflow that enables seamless transfer of structured CSV data from cloud object storage into a columnar analytics database. Create an Airflow task that supports efficient, automated data loading from cloud storage to a queryable data platform.\\n\\nAirflow Version: 2.9.3'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '\"\"\"\\nExample DAG using GCSToTrinoOperator.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\\n\\n# [START howto_gcs]\\nbucket = \"airflow-system-tests-gcs\"\\nobject_path = \"trino/test-data.csv\"\\n# [END howto_gcs]\\n\\n# [START howto_config_trino]\\ntable = \"test\"\\n# [END howto_config_trino]\\n\\n# [START howto_move_config_trino]\\nmove_table = \"moved_test\"\\n# [END howto_move_config_trino]\\n\\nwith DAG(\\n    \"gcs-to-trino\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_operator_gcs_to_trino]\\n    run_this = GCSToTrinoOperator(\\n        task_id=\"gcs-to-trino-1\",\\n        gcs_bucket=bucket,\\n        source_object=object_path,\\n        trino_table=table,\\n    )\\n\\n    move_this_2 = GCSToTrinoOperator(\\n        task_id=\"gcs-to-trino-2\",\\n        gcs_bucket=bucket,\\n        source_object=object_path,\\n        trino_table=move_table,\\n        move=True,\\n        destination_object=f\"{object_path.replace(\\'.\\', \\'_\")}\",\\n    )\\n    # [END howto_operator_gcs_to_trino]\\n\\n\\nfrom tests.system.utils import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_992_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '2.9.3',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251694}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user', 'content': 'Tell me how to use TensorFlow with Python.'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"import tensorflow as tf\\n\\n# Create a simple neural network model\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Dense(16, activation='relu', input_shape=(None, 1)),  # Input layer\\n    tf.keras.layers.Dense(8, activation='relu'),                          # Hidden layer\\n    tf.keras.layers.Dense(1)                                            # Output layer\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer='adam',\\n              loss=tf.keras.losses.MeanSquaredError(),\\n              metrics=['accuracy'])\\n\\n# Prepare data (example: input shape is (batch_size, 1))\\ninput_data = np.random.rand(100, 1)\\nlabels = np.random.rand(100, 1)\\n\\n# Train the model\\nmodel.fit(input_data, labels, epochs=50, batch_size=32)\\n\\n# Evaluate the model\\ntest_loss, test_acc = model.evaluate(test_input, test_labels)\\nprint(f'Test accuracy: {test_acc}')\"}],\n",
       "  'metadata': {'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251796}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'How can I create a Python function to retrieve the details of an Elastic Network Interface (ENI) using the Boto3 library?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"import boto3\\n\\ndef get_eni_details(eni_id):\\n    # Create an EC2 client\\n    ec2_client = boto3.client('ec2')\\n    \\n    # Describe network interfaces\\n    response = ec2_client.describe_network_interfaces(\\n        Filters=[\\n            {\\n                'Name': 'network-interface-id',\\n                'Values': [eni_id]\\n            },\\n            {\\n                'Name': 'association.subnet-id',\\n                'Values': ['subnet-12345678']  # Example subnet ID\\n            }\\n        ]\\n    )\\n    \\n    # Extract and return the ENI details\\n    eni_details = {}\\n    for interface in response['NetworkInterfaces']:\\n        eni_details[interface['Association']['SubnetId']] = {\\n            'InterfaceId': interface['InterfaceId'],\\n            'PrivateIpAddresses': [\\n                {'PrivateIpAddress': ip} for ip in interface['PrivateIpAddresses']\\n            ],\\n            'Status': interface['Status']\\n        }\\n    \\n    return eni_details\\n\\n# Example usage\\neni_id = 'eni-0ab3c9dfe789abcde'  # Replace with your ENI ID\\nenior_details = get_eni_details(enini_id)\\nprint(enior_details)\"}],\n",
       "  'metadata': {'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5251887}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Design a data management workflow for tracking employee information in a Vertica database. Create a process that demonstrates creating, populating, querying, and cleaning up an employee records table.\\n\\nAirflow Version: 3.0.0'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '\"\"\"\\nExample DAG using VerticaOperator.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.vertica.operators.vertica import VerticaOperator\\n\\n\\ndef setup_connections():\\n    # Connection created automatically when connecting to Vertica\\n    # No need to manually add it\\n    from airflow.utils import db\\n\\n    db.merge_conn(\\n        {\\n            \"conn_id\": \"vertica_default\",\\n            \"conn_type\": \"vertica\",\\n        }\\n    )\\n\\n\\nwith DAG(\\n    \"example_vertica_operator\",\\n    schedule=\"@once\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_vertica_operator]\\n    sql_table = \"VERTICA_OP_TEST_TABLE\"\\n    insert_sql = f\"INSERT INTO {sql_table} VALUES (42, \\'Test Value\\');\"\\n\\n    load_sql = f\"LOAD DATA INPATH \\'/tmp/insert.sql\\' INTO TABLE {sql_table} FIELDS TERMINATED BY \\',\\';\"\\n    delete_sql = f\"DELETE FROM {sql_table};\"\\n    select_sql = f\"SELECT * FROM {sql_table};\"\\n\\n    create_sql = f\"CREATE TABLE {sql_table} (id INT, value VARCHAR(10));\"\\n    drop_sql = f\"DROP TABLE IF EXISTS {sql_table};\"\\n\\n    t1 = VerticaOperator(task_id=\"task1\", vertica_query=create_sql)\\n    t2 = VerticaOperator(task_id=\"task2\", vertica_query=create_sql)\\n    t3 = VerticaOperator(task_id=\"task3\", vertica_query=insert_sql)\\n    t4 = VerticaOperator(task_id=\"task4\", vertica_query=load_sql)\\n    t5 = VerticaOperator(task_id=\"task5\", vertica_query=delete_sql)\\n    t6 = VerticaOperator(task_id=\"task6\", vertica_query=select_sql)\\n    t7 = VerticaOperator(task_id=\"task7\", vertica_query=drop_sql)\\n    t8 = VerticaOperator(task_id=\"task8\", vertica_query=drop_sql)\\n    # [END howto_vertica_operator]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_1418_variant_1',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 1,\n",
       "   'airflow_version': '3.0.0',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970726.5252023}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Develop a data exploration DAG that uses Azure Data Lake Storage to perform a remote file inventory of Parquet-formatted files in a specific directory. Construct an automated task that lists file metadata to support data validation and tracking processes.\\n\\nAirflow Version: 3.0.6'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'from __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSListOperator\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nDAG_ID = \"adls_list_dag\"\\nwith DAG(\\n    dag_id=DAG_ID,\\n    start_date=datetime(2021, 1, 1),\\n    schedule=None,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START adls_list_operator]\\n    adls_list_task = ADLSListOperator(task_id=\"adls_list_task\")\\n    # [END adls_list_operator]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_2154_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '3.0.6',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970766.957391}},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Develop a data migration workflow that transfers files from a cloud storage bucket to an SFTP destination with multiple transfer strategies. Create a pipeline that supports different file transfer modes including copying individual files, moving files, and transferring entire directory contents.\\n\\nAirflow Version: 3.0.0'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '\"\"\"\\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\nfrom pathlib import Path\\n\\nfrom airflow.decorators import task\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\\n\\nDAG_ID = \"gcs_to_sftp\"\\n\\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\\nSFTP_PATH = \"/tmp/sftp-to-gcs\"\\nLOCAL_FILE_PATH = str(Path(__file__).parent / \"resources\" / \"example_transfer.txt\")\\n\\n\\nwith DAG(\\n    DAG_ID,\\n    schedule=\"@once\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\", \"gcs\"],\\n) as dag:\\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\\n\\n    @task\\n    def create_temp_sftp_path():\\n        \"\"\"\\n        Task creates tmp folder in path specified on argument\\n        \"\"\"\\n        from airflow.providers.google.cloud.hooks.sftp_hook import SFTPHook\\n\\n        sftp_hook = SFTPHook()\\n        if sftp_hook.path_exists(SFTP_PATH):\\n            sftp_hook.create_directory(SFTP_PATH)\\n        else:\\n            sftp_hook.create_directory(SFTP_PATH)\\n\\n    create_temp_sftp_path_task = create_temp_sftp_path()\\n\\n    upload_file_1 = SFTPToGCSOperator(\\n        task_id=\"upload_file_1\",\\n        local_filepath=LOCAL_FILE_PATH,\\n        bucket_name=BUCKET_NAME,\\n        sftp_path=SFTP_PATH,\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp]\\n    transfer_files = GCSToSFTTOperator(\\n        task_id=\"transfer_files\",\\n        source_bucket=BUCKET_NAME,\\n        source_object=\"/test/\",\\n        destination_sftp_path=SFTP_PATH,\\n    )\\n    # [END howto_operator_gcs_to_sftp]\\n\\n    download_file_1 = SFTPToGCSOperator(\\n        task_id=\"download_file_1\",\\n        local_filepath=LOCAL_FILE_PATH,\\n        bucket_name=BUCKET_NAME,\\n        sftp_path=SFTP_PATH,\\n    )\\n\\n    delete_bucket = GCSDeleteBucketOperator(\\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\\n    )\\n\\n    (\\n        # TEST SETUP\\n        create_bucket\\n        >> create_temp_sftp_path_task\\n        >> upload_file_1\\n\\n        # TEST BODY\\n        >> transfer_files\\n\\n        # TEST TEARDOWN\\n        >> delete_bucket\\n    )\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'}],\n",
       "  'metadata': {'file_name': 'dag_1298_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '3.0.0',\n",
       "   'model': 'Qwen2.5-Finetuned',\n",
       "   'inference_time': 1765970766.957401}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Result Structure (Corrected):\n",
      "Instruction: Create a data pipeline that demonstrates loading s...\n",
      "------------------------------\n",
      "Original (First 50 chars):  \"\"\"\n",
      "Example use of Snowflake Snowpark Python relat...\n",
      "Base (First 50 chars):      from airflow import DAG\n",
      "from airflow.providers.sno...\n",
      "Finetuned (First 50 chars): from __future__ import annotations\n",
      "\n",
      "import os\n",
      "from...\n",
      "\n",
      "âœ… Successfully combined 10 examples.\n"
     ]
    }
   ],
   "source": [
    "final_consolidated_results = []\n",
    "\n",
    "# Iterate through the original dataset and our two result lists simultaneously\n",
    "for i, example in enumerate(test_examples):\n",
    "    \n",
    "    # 1. Extract inputs (System + User)\n",
    "    input_messages = example['messages'][:-1]\n",
    "    \n",
    "    # 2. Extract Original \"Gold\" Output (The last message in the dataset)\n",
    "    original_target = example['messages'][-1]['content']\n",
    "    \n",
    "    # 3. Extract Generated Outputs (FIX IS HERE)\n",
    "    # We look into the 'messages' list and grab the last item (the Assistant's reply)\n",
    "    base_code = base_results[i]['messages'][-1]['content']\n",
    "    finetuned_code = finetuned_results[i]['messages'][-1]['content']\n",
    "    \n",
    "    # 4. Build the Combined Object\n",
    "    combined_entry = {\n",
    "        'id': i,\n",
    "        'messages': input_messages,  # The prompt context\n",
    "        'responses': {\n",
    "            'original': extract_code_from_response(original_target),  # Truth\n",
    "            'base_model': base_code,                                  # Just the code string\n",
    "            'finetuned': finetuned_code                               # Just the code string\n",
    "        },\n",
    "        'metadata': example.get('metadata', {})\n",
    "    }\n",
    "    \n",
    "    final_consolidated_results.append(combined_entry)\n",
    "\n",
    "# --- VERIFICATION ---\n",
    "print(\"\\nSample Result Structure (Corrected):\")\n",
    "print(f\"Instruction: {final_consolidated_results[0]['messages'][1]['content'][:50]}...\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original (First 50 chars):  {final_consolidated_results[0]['responses']['original'][:50]}...\")\n",
    "print(f\"Base (First 50 chars):      {final_consolidated_results[0]['responses']['base_model'][:50]}...\")\n",
    "print(f\"Finetuned (First 50 chars): {final_consolidated_results[0]['responses']['finetuned'][:50]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully combined {len(final_consolidated_results)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\\n\\nAirflow Version: 3.0.1'}],\n",
       "  'responses': {'original': '\"\"\"\\nExample use of Snowflake Snowpark Python related operators.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom datetime import datetime\\nfrom typing import TYPE_CHECKING\\n\\nif TYPE_CHECKING:\\n    from snowflake.snowpark import Session\\n\\nfrom airflow import DAG\\nfrom airflow.providers.snowflake.operators.snowpark import SnowparkOperator\\n\\nSNOWFLAKE_CONN_ID = \"my_snowflake_conn\"\\nDAG_ID = \"example_snowpark_operator\"\\n\\nwith DAG(\\n    DAG_ID,\\n    start_date=datetime(2024, 1, 1),\\n    schedule=\"@once\",\\n    default_args={\"snowflake_conn_id\": SNOWFLAKE_CONN_ID},\\n    tags=[\"example\"],\\n    catchup=False,\\n) as dag:\\n    # [START howto_operator_snowpark]\\n    def setup_data(session: Session):\\n        # The Snowpark session object is injected as an argument\\n        data = [\\n            (1, 0, 5, \"Product 1\", \"prod-1\", 1, 10),\\n            (2, 1, 5, \"Product 1A\", \"prod-1-A\", 1, 20),\\n            (3, 1, 5, \"Product 1B\", \"prod-1-B\", 1, 30),\\n            (4, 0, 10, \"Product 2\", \"prod-2\", 2, 40),\\n            (5, 4, 10, \"Product 2A\", \"prod-2-A\", 2, 50),\\n            (6, 4, 10, \"Product 2B\", \"prod-2-B\", 2, 60),\\n            (7, 0, 20, \"Product 3\", \"prod-3\", 3, 70),\\n            (8, 7, 20, \"Product 3A\", \"prod-3-A\", 3, 80),\\n            (9, 7, 20, \"Product 3B\", \"prod-3-B\", 3, 90),\\n            (10, 0, 50, \"Product 4\", \"prod-4\", 4, 100),\\n            (11, 10, 50, \"Product 4A\", \"prod-4-A\", 4, 100),\\n            (12, 10, 50, \"Product 4B\", \"prod-4-B\", 4, 100),\\n        ]\\n        columns = [\"id\", \"parent_id\", \"category_id\", \"name\", \"serial_number\", \"key\", \"3rd\"]\\n        df = session.create_dataframe(data, schema=columns)\\n        table_name = \"sample_product_data\"\\n        df.write.save_as_table(table_name, mode=\"overwrite\")\\n        return table_name\\n\\n    setup_data_operator = SnowparkOperator(\\n        task_id=\"setup_data\",\\n        python_callable=setup_data,\\n        dag=dag,\\n    )\\n\\n    def check_num_rows(table_name: str):\\n        # Alternatively, retrieve the Snowpark session object using `get_active_session`\\n        from snowflake.snowpark.context import get_active_session\\n\\n        session = get_active_session()\\n        df = session.table(table_name)\\n        assert df.count() == 12\\n\\n    check_num_rows_operator = SnowparkOperator(\\n        task_id=\"check_num_rows\",\\n        python_callable=check_num_rows,\\n        op_kwargs={\"table_name\": \"{{ task_instance.xcom_pull(task_ids=\\'setup_data\\') }}\"},\\n        dag=dag,\\n    )\\n\\n    setup_data_operator >> check_num_rows_operator\\n    # [END howto_operator_snowpark]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': 'from airflow import DAG\\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\\nfrom airflow.utils.dates import datetime\\n\\n# Define the default arguments for the DAG\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'depends_on_past\\': False,\\n    \\'start_date\\': datetime(2023, 1, 1),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes=5)\\n}\\n\\n# Create the DAG\\ndag = DAG(\\n    \\'load_product_data_to_snowflake\\',\\n    default_args=default_args,\\n    description=\\'A DAG to load product data into Snowflake and validate the load\\',\\n    schedule_interval=None,\\n)\\n\\n# Load product data into Snowflake\\nload_product_data_task = SnowflakeOperator(\\n    task_id=\\'load_product_data\\',\\n    sql=\"\"\"\\n        INSERT INTO products (product_name, price, category)\\n        VALUES (\\'Product A\\', 19.99, \\'Electronics\\'),\\n               (\\'Product B\\', 49.99, \\'Clothing\\'),\\n               (\\'Product C\\', 79.99, \\'Books\\'),\\n               (\\'Product D\\', 29.99, \\'Home Appliances\\'),\\n               (\\'Product E\\', 69.99, \\'Sports\\'),\\n               (\\'Product F\\', 89.99, \\'Toys\\'),\\n               (\\'Product G\\', 39.99, \\'Kitchen Appliances\\'),\\n               (\\'Product H\\', 19.99, \\'Healthcare\\'),\\n               (\\'Product I\\', 49.99, \\'Beauty\\'),\\n               (\\'Product J\\', 79.99, \\'Furniture\\'),\\n               (\\'Product K\\', 29.99, \\'Automotive\\');\\n    \"\"\",\\n    snowflake_conn_id=\\'snowflake_default\\',\\n    dag=dag\\n)\\n\\n# Verify the data load by counting the rows in the products table\\nverify_data_load_task = SnowflakeOperator(\\n    task_id=\\'verify_data_load\\',\\n    sql=\"\"\"\\n        SELECT COUNT(*) AS total_rows\\n        FROM products;\\n    \"\"\",\\n    snowflake_conn_id=\\'snowflake_default\\',\\n    dag=dag\\n)\\n\\n# Set up the dependencies between tasks\\nload_product_data_task >> verify_data_load_task',\n",
       "   'finetuned': 'from __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow import DAG\\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nSNOWFLAKE_SAMPLE_TABLE = \"sample\"\\nSNOWFLAKE_CONN_ID = \"snowflake\"\\n\\nwith DAG(\\n    dag_id=f\"example_snowflake_insert_many_{ENV_ID}\",\\n    start_date=datetime(2021, 1, 1),\\n    schedule=None,\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_operator_snowflake]\\n    insert_data_snowflake = SnowflakeOperator(\\n        task_id=\"insert_data_snowflake\",\\n        sql=\"\"\"\\n            INSERT INTO SNOWFLAKE_SAMPLE_TABLE VALUES\\n            (4, \\'Cell Phone\\', 500, \\'2020-03-15 13:00:00\\'),\\n            (3, \\'Personal Computer\\', 700, \\'2020-03-15 13:00:00\\'),\\n            (1, \\'TV\\', 300, \\'2020-03-15 13:00:00\\'),\\n            (2, \\'Radio\\', 200, \\'2020-03-15 13:00:00\\');\\n        \"\"\",\\n    )\\n    # [END howto_operator_snowflake]\\n\\n    (insert_data_snowflake >> SnowflakeOperator(task_id=\"insert_count\", sql=\"SELECT COUNT(*) FROM %s\" % SNOWFLAKE_SAMPLE_TABLE))\\n# [START howto_triggerer_snowflake]\\n    trigger_query = SnowflakeOperator(task_id=\"trigger_query\", sql=\"SELECT COUNT(*) FROM %s\" % SNOWFLAKE_SAMPLE_TABLE)\\n    join_task = SnowflakeOperator(task_id=\"join_task\", sql=\"JOIN {{ task_instance.xcom_pull(\\'trigger_query\\')[\\'returning._cols\\'][0] }} ON 1=1\")\\n# [END howto_triggerer_snowflake]\\n\\n    (trigger_query >> join_task)\\n\\n    # [START howto_run_pipeline]\\n    run_pipeline = SnowflakeOperator(\\n        task_id=\"run_pipeline\",\\n        sql=f\"LIST SCHEMAS;\",\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n    # [END howto_run_pipeline]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_1798_variant_1',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 1,\n",
       "   'airflow_version': '3.0.1'}},\n",
       " {'id': 1,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'I have a pandas DataFrame `df` with a column `text` that contains strings. I want to create a new column `text_len` that contains the length of each string in the `text` column. How can I achieve this?'}],\n",
       "  'responses': {'original': \"import pandas as pd\\n\\n# Sample DataFrame\\ndata = {'text': ['hello', 'world', 'pandas', 'dataframe']}\\ndf = pd.DataFrame(data)\\n\\n# Add a new column 'text_len' with the length of each string in the 'text' column\\ndf['text_len'] = df['text'].str.len()\\n\\n# Display the DataFrame\\nprint(df)\",\n",
       "   'base_model': \"import pandas as pd\\n\\n# Sample DataFrame creation\\ndata = {\\n    'text': ['Hello', 'World', 'Python', 'Programming']\\n}\\ndf = pd.DataFrame(data)\\n\\n# Adding a new column 'text_len' with the length of each string in 'text'\\ndf['text_len'] = df['text'].str.len()\\n\\nprint(df)\",\n",
       "   'finetuned': \"# Assuming df is already defined and has a 'text' column\\n\\n# Calculate the length of each string in the 'text' column\\ndf['text_len'] = df['text'].apply(lambda x: len(x))\\n\\nprint(df)\"},\n",
       "  'metadata': None},\n",
       " {'id': 2,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Construct an Airflow DAG that showcases the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class. Build a simple workflow that triggers an empty operator after standard work hours to illustrate scheduling flexibility.\\n\\nAirflow Version: 3.0.0'}],\n",
       "  'responses': {'original': 'from __future__ import annotations\\n\\nimport pendulum\\n\\nfrom airflow.example_dags.plugins.workday import AfterWorkdayTimetable\\nfrom airflow.providers.standard.operators.empty import EmptyOperator\\nfrom airflow.sdk import DAG\\n\\nwith DAG(\\n    dag_id=\"example_workday_timetable\",\\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\\n    schedule=AfterWorkdayTimetable(),\\n    tags=[\"example\", \"timetable\"],\\n):\\n    EmptyOperator(task_id=\"run_this\")',\n",
       "   'base_model': 'from datetime import timedelta\\nfrom airflow import DAG\\nfrom airflow.operators.empty_operator import EmptyOperator\\nfrom airflow.timetables.after_workday import AfterWorkdayTimetable\\n\\n# Define the default arguments for the DAG\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'start_date\\': \\'2023-10-01\\',\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes=5),\\n}\\n\\n# Create the DAG object\\ndag = DAG(\\n    \\'after_workday_timetable_example\\',\\n    default_args=default_args,\\n    schedule_interval=\\'@daily\\',  # Adjust this as needed\\n)\\n\\n# Define the task with a custom timetable\\ndef my_task():\\n    print(\"Task executed at:\", dag.execution_date)\\n\\ntask = EmptyOperator(\\n    task_id=\\'my_task\\',\\n    on_failure_callback=my_task,\\n    timetable=AfterWorkdayTimetable(start_time=\\'09:00:00\\', end_time=\\'17:00:00\\'),\\n)\\n\\n# Add the task to the DAG\\ndag.add_task(task)',\n",
       "   'finetuned': 'from __future__ import annotations\\n\\nimport pendulum\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.standard.operators.empty import EmptyOperator\\nfrom airflow.providers.standard.utils.weekday.timetables import AfterWorkdayTimetable\\nfrom airflow.utils.weekday import WeekDay\\n\\nwith DAG(\\n    dag_id=\"example_after_workday_timetable\",\\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\\n    schedule=\"@daily\",\\n    catchup=False,\\n    tags=[\"example\"],\\n):\\n    run_this = EmptyOperator(task_id=\"run_this\")\\n    AfterWorkdayTimetable(check_time=(WeekDay.FRIDAY, \"09:30\"), task_ids=[\"run_task\"])\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_1063_variant_2',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 2,\n",
       "   'airflow_version': '3.0.0'}},\n",
       " {'id': 3,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': \"Create a workflow that retrieves Campaign Manager metrics, generates a standard performance report across campaigns, and demonstrates batch conversion tracking and reporting capabilities. The DAG should illustrate how to interact with Campaign Manager's reporting and conversion APIs using Airflow operators.\\n\\nAirflow Version: 3.0.0\"}],\n",
       "  'responses': {'original': '\"\"\"\\nExample Airflow DAG that shows how to use CampaignManager.\\n\\nThis system test relies on a service account with proper settings in Campaign Manager 360.\\nThat\\'s why before running this system test locally, make sure your service account corresponds all the\\nsecrets that the DAG reads. If your service account doesn\\'t have access but you know another one which has\\nthen simply specify it in the environment variable CM360_IMPERSONATION_CHAIN.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport json\\nimport logging\\nimport os\\nimport time\\nimport uuid\\nfrom datetime import datetime\\nfrom typing import cast\\n\\nfrom google.api_core.exceptions import NotFound\\n\\nfrom airflow.decorators import task\\nfrom airflow.models import Connection\\nfrom airflow.models.dag import DAG\\nfrom airflow.models.xcom_arg import XComArg\\nfrom airflow.providers.google.cloud.hooks.secret_manager import GoogleCloudSecretManagerHook\\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\\nfrom airflow.providers.google.marketing_platform.operators.campaign_manager import (\\n    GoogleCampaignManagerBatchInsertConversionsOperator,\\n    GoogleCampaignManagerBatchUpdateConversionsOperator,\\n    GoogleCampaignManagerDeleteReportOperator,\\n    GoogleCampaignManagerDownloadReportOperator,\\n    GoogleCampaignManagerInsertReportOperator,\\n    GoogleCampaignManagerRunReportOperator,\\n)\\nfrom airflow.providers.google.marketing_platform.sensors.campaign_manager import (\\n    GoogleCampaignManagerReportSensor,\\n)\\nfrom airflow.settings import Session\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nfrom system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\\nCM360_IMPERSONATION_CHAIN = os.environ.get(\"IMPERSONATION_CHAIN\", None)\\n\\nDAG_ID = \"campaign_manager\"\\n\\nSECRET_ACCOUNT_ID = \"cm360_account_id\"\\nSECRET_DCLID = \"cm360_dclid\"\\nSECRET_ENCRYPTION_ENTITY_ID = \"cm360_encryption_entity_id\"\\nSECRET_FLOODLIGHT_ACTIVITY_ID = \"cm360_floodlight_activity_id\"\\nSECRET_FLOODLIGHT_CONFIGURATION_ID = \"cm360_floodlight_configuration_id\"\\nSECRET_USER_PROFILE_ID = \"cm360_user_profile_id\"\\n\\nACCOUNT_ID = \"{{ task_instance.xcom_pull(\\'get_account_id\\') }}\"\\nDCLID = \"{{ task_instance.xcom_pull(\\'get_dclid\\') }}\"\\nENCRYPTION_ENTITY_ID = \"{{ task_instance.xcom_pull(\\'get_encryption_entity_id\\') }}\"\\nFLOODLIGHT_ACTIVITY_ID = \"{{ task_instance.xcom_pull(\\'get_floodlight_activity_id\\') }}\"\\nFLOODLIGHT_CONFIGURATION_ID = \"{{ task_instance.xcom_pull(\\'get_floodlight_configuration_id\\') }}\"\\nUSER_PROFILE_ID = \"{{ task_instance.xcom_pull(\\'get_user_profile_id\\') }}\"\\n\\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\\nREPORT_NAME = f\"report_{DAG_ID}_{ENV_ID}\"\\nFILE_NAME = f\"file_{DAG_ID}_{ENV_ID}\"\\nFORMAT = \"CSV\"\\nCONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"\\n\\n\\n# For more information, please check\\n# https://developers.google.com/doubleclick-advertisers/rest/v4/reports#type\\nREPORT = {\\n    \"kind\": \"dfareporting#report\",\\n    \"type\": \"STANDARD\",\\n    \"name\": REPORT_NAME,\\n    \"fileName\": FILE_NAME,\\n    \"accountId\": ACCOUNT_ID,\\n    \"format\": FORMAT,\\n    \"criteria\": {\\n        \"dateRange\": {\\n            \"kind\": \"dfareporting#dateRange\",\\n            \"relativeDateRange\": \"LAST_365_DAYS\",\\n        },\\n        \"dimensions\": [{\"kind\": \"dfareporting#sortedDimension\", \"name\": \"campaign\"}],\\n        \"metricNames\": [\"activeViewImpressionDistributionViewable\"],\\n    },\\n}\\n\\n# For more information, please check\\n# https://developers.google.com/doubleclick-advertisers/rest/v4/Conversion\\nCONVERSION = {\\n    \"kind\": \"dfareporting#conversion\",\\n    \"floodlightActivityId\": FLOODLIGHT_ACTIVITY_ID,\\n    \"floodlightConfigurationId\": FLOODLIGHT_CONFIGURATION_ID,\\n    \"dclid\": DCLID,\\n    \"ordinal\": \"0\",\\n    \"quantity\": 42,\\n    \"value\": 123.4,\\n    \"timestampMicros\": int(time.time()) * 1000000,\\n    \"customVariables\": [\\n        {\\n            \"kind\": \"dfareporting#customFloodlightVariable\",\\n            \"type\": \"U4\",\\n            \"value\": \"value\",\\n        }\\n    ],\\n}\\n\\nCONVERSION_UPDATE = {\\n    \"kind\": \"dfareporting#conversion\",\\n    \"floodlightActivityId\": FLOODLIGHT_ACTIVITY_ID,\\n    \"floodlightConfigurationId\": FLOODLIGHT_CONFIGURATION_ID,\\n    \"dclid\": DCLID,\\n    \"ordinal\": \"0\",\\n    \"quantity\": 42,\\n    \"value\": 123.4,\\n}\\n\\n\\nlog = logging.getLogger(__name__)\\n\\n\\ndef get_secret(secret_id: str) -> str:\\n    hook = GoogleCloudSecretManagerHook()\\n    if hook.secret_exists(secret_id=secret_id):\\n        return hook.access_secret(secret_id=secret_id).payload.data.decode().strip()\\n    raise NotFound(\"The secret \\'%s\\' not found\", secret_id)\\n\\n\\nwith DAG(\\n    DAG_ID,\\n    schedule=\"@once\",  # Override to match your needs,\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\", \"campaign\"],\\n) as dag:\\n\\n    @task\\n    def create_connection(connection_id: str) -> None:\\n        connection = Connection(\\n            conn_id=connection_id,\\n            conn_type=\"google_cloud_platform\",\\n        )\\n        extras = {\\n            \"scope\": \"https://www.googleapis.com/auth/cloud-platform,\"\\n            \"https://www.googleapis.com/auth/ddmconversions,\"\\n            \"https://www.googleapis.com/auth/dfareporting\",\\n        }\\n        if CM360_IMPERSONATION_CHAIN:\\n            extras[\"impersonation_chain\"] = CM360_IMPERSONATION_CHAIN\\n\\n        conn_extra_json = json.dumps(extras)\\n        connection.set_extra(conn_extra_json)\\n\\n        session = Session()\\n        log.info(\"Removing connection %s if it exists\", connection_id)\\n        query = session.query(Connection).filter(Connection.conn_id == connection_id)\\n        query.delete()\\n\\n        session.add(connection)\\n        session.commit()\\n        log.info(\"Connection %s created\", CONNECTION_ID)\\n\\n    @task\\n    def get_account_id():\\n        return get_secret(secret_id=SECRET_ACCOUNT_ID)\\n\\n    @task\\n    def get_dclid():\\n        return get_secret(secret_id=SECRET_DCLID)\\n\\n    @task\\n    def get_encryption_entity_id():\\n        return get_secret(secret_id=SECRET_ENCRYPTION_ENTITY_ID)\\n\\n    @task\\n    def get_floodlight_activity_id():\\n        return get_secret(secret_id=SECRET_FLOODLIGHT_ACTIVITY_ID)\\n\\n    @task\\n    def get_floodlight_configuration_id():\\n        return get_secret(secret_id=SECRET_FLOODLIGHT_CONFIGURATION_ID)\\n\\n    @task\\n    def get_user_profile_id():\\n        return get_secret(secret_id=SECRET_USER_PROFILE_ID)\\n\\n    create_bucket = GCSCreateBucketOperator(\\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\\n    )\\n\\n    # [START howto_campaign_manager_insert_report_operator]\\n    create_report = GoogleCampaignManagerInsertReportOperator(\\n        profile_id=USER_PROFILE_ID,\\n        report=REPORT,\\n        task_id=\"create_report\",\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    report_id = cast(\"str\", XComArg(create_report, key=\"report_id\"))\\n    # [END howto_campaign_manager_insert_report_operator]\\n\\n    # [START howto_campaign_manager_run_report_operator]\\n    run_report = GoogleCampaignManagerRunReportOperator(\\n        profile_id=USER_PROFILE_ID,\\n        report_id=report_id,\\n        task_id=\"run_report\",\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    file_id = cast(\"str\", XComArg(run_report, key=\"file_id\"))\\n    # [END howto_campaign_manager_run_report_operator]\\n\\n    # [START howto_campaign_manager_wait_for_operation]\\n    wait_for_report = GoogleCampaignManagerReportSensor(\\n        task_id=\"wait_for_report\",\\n        profile_id=USER_PROFILE_ID,\\n        report_id=report_id,\\n        file_id=file_id,\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    # [END howto_campaign_manager_wait_for_operation]\\n\\n    # [START howto_campaign_manager_get_report_operator]\\n    report_name = f\"reports/report_{str(uuid.uuid1())}\"\\n    get_report = GoogleCampaignManagerDownloadReportOperator(\\n        task_id=\"get_report\",\\n        profile_id=USER_PROFILE_ID,\\n        report_id=report_id,\\n        file_id=file_id,\\n        report_name=report_name,\\n        bucket_name=BUCKET_NAME,\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    # [END howto_campaign_manager_get_report_operator]\\n\\n    # [START howto_campaign_manager_delete_report_operator]\\n    delete_report = GoogleCampaignManagerDeleteReportOperator(\\n        profile_id=USER_PROFILE_ID,\\n        report_name=REPORT_NAME,\\n        task_id=\"delete_report\",\\n        trigger_rule=TriggerRule.ALL_DONE,\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    # [END howto_campaign_manager_delete_report_operator]\\n\\n    delete_bucket = GCSDeleteBucketOperator(\\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\\n    )\\n\\n    # [START howto_campaign_manager_insert_conversions]\\n    insert_conversion = GoogleCampaignManagerBatchInsertConversionsOperator(\\n        task_id=\"insert_conversion\",\\n        profile_id=USER_PROFILE_ID,\\n        conversions=[CONVERSION],\\n        encryption_source=\"AD_SERVING\",\\n        encryption_entity_type=\"DCM_ADVERTISER\",\\n        encryption_entity_id=ENCRYPTION_ENTITY_ID,  # type: ignore[arg-type]\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    # [END howto_campaign_manager_insert_conversions]\\n\\n    # [START howto_campaign_manager_update_conversions]\\n    update_conversion = GoogleCampaignManagerBatchUpdateConversionsOperator(\\n        task_id=\"update_conversion\",\\n        profile_id=USER_PROFILE_ID,\\n        conversions=[CONVERSION_UPDATE],\\n        encryption_source=\"AD_SERVING\",\\n        encryption_entity_type=\"DCM_ADVERTISER\",\\n        encryption_entity_id=ENCRYPTION_ENTITY_ID,  # type: ignore[arg-type]\\n        max_failed_updates=1,\\n        gcp_conn_id=CONNECTION_ID,\\n    )\\n    # [END howto_campaign_manager_update_conversions]\\n\\n    @task(task_id=\"delete_connection\")\\n    def delete_connection(connection_id: str) -> None:\\n        session = Session()\\n        log.info(\"Removing connection %s\", connection_id)\\n        query = session.query(Connection).filter(Connection.conn_id == connection_id)\\n        query.delete()\\n        session.commit()\\n\\n    (\\n        # TEST SETUP\\n        create_connection(connection_id=CONNECTION_ID)\\n        >> [\\n            get_account_id(),\\n            get_dclid(),\\n            get_encryption_entity_id(),\\n            get_floodlight_activity_id(),\\n            get_floodlight_configuration_id(),\\n            get_user_profile_id(),\\n        ]\\n        >> create_bucket\\n        # TEST BODY\\n        >> create_report\\n        >> run_report\\n        >> wait_for_report\\n        >> get_report\\n        >> insert_conversion\\n        >> update_conversion\\n        # TEST TEARDOWN\\n        >> delete_report\\n        >> delete_bucket\\n        >> delete_connection(connection_id=CONNECTION_ID)\\n    )\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule is part of the DAG\\n\\n    # Excluding sensor because we expect it to fail due to cancelled operation\\n    [\\n        task\\n        for task in dag.tasks\\n        if task.task_id not in [\"insert_conversion\", \"update_conversion\", \"delete_connection\"]\\n    ] >> watcher()\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': 'from airflow import DAG\\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\\nfrom airflow.providers.google.cloud.hooks.storage import StorageHook\\nfrom airflow.operators.python_operator import PythonOperator\\nfrom datetime import datetime, timedelta\\n\\n# Define the default arguments for the DAG\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'depends_on_past\\': False,\\n    \\'start_date\\': datetime(2023, 1, 1),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes=5)\\n}\\n\\n# Create the DAG\\ndag = DAG(\\n    \\'campaign_manager_reporting_dag\\',\\n    default_args=default_args,\\n    description=\\'A DAG to retrieve Campaign Manager metrics, generate a standard performance report, and demonstrate batch conversion tracking and reporting capabilities.\\',\\n    schedule_interval=timedelta(days=1)\\n)\\n\\n# Function to fetch Campaign Manager metrics\\ndef fetch_campaign_metrics():\\n    hook = BigQueryHook()\\n    query = \"\"\"\\n        SELECT \\n            campaign_id,\\n            date,\\n            impressions,\\n            clicks,\\n            conversions\\n        FROM \\n            `your_project.your_dataset.campaign_metrics`\\n        WHERE \\n            date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY);\\n    \"\"\"\\n    df = hook.get_pandas_df(query)\\n    return df\\n\\n# Function to generate a standard performance report\\ndef generate_performance_report(df):\\n    # Implement your report generation logic here\\n    pass\\n\\n# Function to track and report batch conversions\\ndef track_and_report_conversions(df):\\n    # Implement your conversion tracking and reporting logic here\\n    pass\\n\\n# Task to fetch Campaign Manager metrics\\nfetch_metrics_task = PythonOperator(\\n    task_id=\\'fetch_campaign_metrics\\',\\n    python_callable=fetch_campaign_metrics,\\n    dag=dag\\n)\\n\\n# Task to generate a standard performance report\\ngenerate_report_task = PythonOperator(\\n    task_id=\\'generate_performance_report\\',\\n    python_callable=generate_performance_report,\\n    dag=dag,\\n    trigger_rule=\\'all_success\\'\\n)\\n\\n# Task to track and report batch conversions\\ntrack_and_report_conversion_task = PythonOperator(\\n    task_id=\\'track_and_report_conversions\\',\\n    python_callable=track_and_report_conversions,\\n    dag=dag,\\n    trigger_rule=\\'all_success\\'\\n)\\n\\n# Set up the dependencies between tasks\\nfetch_metrics_task >> generate_report_task >> track_and_report_conversion_task',\n",
       "   'finetuned': '# [START campaign_manager_gcp_marketing_api_report_job_create_operator]\\nfrom __future__ import annotations\\n\\nimport os\\n\\nfrom datetime import datetime\\nfrom urllib.parse import urlsplit\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.google.marketing_platform.operators.campaign_manager import (\\n    GoogleCampaignManagerBatchInsertReportJobOperator,\\n    GoogleCampaignManagerDeleteReportJobOperator,\\n    GoogleCampaignManagerReportJobCreateOperator,\\n    GoogleCampaignManagerReportJobDeleteOperator,\\n)\\nfrom airflow.providers.google.marketing_platform.sensors.campaign_manager import (\\n    GoogleCampaignManagerReportJobCreateSensor,\\n    GoogleCampaignManagerReportJobDeleteSensor,\\n)\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\\n\\nDAG_ID= os.environ.get(\"GMP_DAG_ID\", \"campaign_manager_report_job\")\\n\\nLOCATION = \"europe-west1\"\\nCROSS_SITE_REPORT = {\\n    \"destination\": {\\n        \"type\": \"COLORED_JSON\",\\n        \"parameters\": {\"file_name\": \"path/to/report.jsonc\"},\\n    },\\n    \"send_time_interval\": 300,\\n}\\nCRITICAL_COLUMNS = [\"CampaignId\", \"AdGroupId\", \"ConversionType\"]\\nREPORT_CONFIG = {\\n    \"location\": LOCATION,\\n    \"reportType\": \"CSV\",\\n    \"dimensions\": [\"CampaignId\", \"CampaignName\"],  # CSV supports multiple dimensions\\n    \"metricssToReport\": [\"ImpressionCount\", \"ConversionCount\"],\\n    \"criticalColumns\": CRITICAL_COLUMNS,\\n}\\nBATCH_REPORT_CONFIG = [\\n    REPORT_CONFIG,\\n    CROSS_SITE_REPORT,\\n]  # Supports batch operation on multiple reports\\n\\n\\nwith DAG(\\n    dag_id=DAG_ID,\\n    start_date=datetime.strptime(os.environ.get(\"DATE_START\", \"2023-01-01\"), \"%Y-%m-%d\"),\\n    schedule=\"@once\",\\n    catchup=False,\\n) as dag:\\n    # [START howto_gmp_report_job_create_operator]\\n    create_report_job = GoogleCampaignManagerReportJobCreateOperator(\\n        task_id=\"create_report_job\",\\n        location=LOCATION,\\n        report_config=REPORT_CONFIG,\\n        google_cloud_project_id=PROJECT_ID,\\n        google_campaign_manager_account_id=\"1234567890\",\\n        delay_delete=True,\\n    )\\n    # [END howto_gmp_report_job_create_operator]\\n\\n    create_batch_insert_report_job = GoogleCampaignManagerBatchInsertReportJobOperator(\\n        task_id=\"create_batch_insert_report_job\",\\n        location=LOCATION,\\n        report_configs=BATCH_REPORT_CONFIG,\\n        google_cloud_project_id=PROJECT_ID,\\n        google_campaign_manager_account_id=\"1234567890\",\\n        batch_size=len(BATCH_REPORT_CONFIG),\\n        async_task=True,\\n    )\\n\\n    # [START howto_gmp_report_job_delete_sensor]\\n    delete_report_job_sensor = GoogleCampaignManagerReportJobDeleteSensor(\\n        task_id=\"delete_report_job_sensor\",\\n        location=LOCATION,\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        poke_interval=60,\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n    # [END howto_gmp_report_job_delete_sensor]\\n\\n    # [START howto_gmp_report_job_create_sensor]\\n    create_report_job_sensor = GoogleCampaignManagerReportJobCreateSensor(\\n        task_id=\"create_report_job_sensor\",\\n        location=LOCATION,\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_batch_insert_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        poke_interval=10,\\n    )\\n    # [END howto_gmp_report_job_create_sensor]\\n\\n    delete_report_job = GoogleCampaignManagerDeleteReportJobOperator(\\n        task_id=\"delete_report_job\",\\n        report_job_id=\"{{ task_instance.xcom_pull(\\'create_report_job\\', key=\\'report_job_id\\') }}\",\\n        timeout=60 * 2,\\n        trigger_rule=TriggerRule.ALL_DONE,\\n    )\\n\\n    create_batch_insert_report_job >> delete_report_job\\n\\n    delete_report_job_sensor >> create_batch_insert_report_job\\n    create_report_job_sensor >> delete_report_job\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_1336_variant_2',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 2,\n",
       "   'airflow_version': '3.0.0'}},\n",
       " {'id': 4,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Construct a data migration workflow that enables seamless transfer of structured CSV data from cloud object storage into a columnar analytics database. Create an Airflow task that supports efficient, automated data loading from cloud storage to a queryable data platform.\\n\\nAirflow Version: 2.9.3'}],\n",
       "  'responses': {'original': '\"\"\"\\nExample DAG using GCSToPrestoOperator.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow import models\\nfrom airflow.providers.presto.transfers.gcs_to_presto import GCSToPrestoOperator\\n\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"test28397yeo\")\\nPATH_TO_FILE = os.environ.get(\"GCP_PATH\", \"path/to/file\")\\nPRESTO_TABLE = os.environ.get(\"PRESTO_TABLE\", \"test_table\")\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nDAG_ID = \"example_gcs_to_presto\"\\n\\nwith models.DAG(\\n    dag_id=DAG_ID,\\n    schedule=\"@once\",  # Override to match your needs\\n    start_date=datetime(2022, 1, 1),\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START gcs_csv_to_presto_table]\\n    gcs_csv_to_presto_table = GCSToPrestoOperator(\\n        task_id=\"gcs_csv_to_presto_table\",\\n        source_bucket=BUCKET,\\n        source_object=PATH_TO_FILE,\\n        presto_table=PRESTO_TABLE,\\n    )\\n    # [END gcs_csv_to_presto_table]\\n\\n\\nfrom tests.system.utils import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': \"from airflow import DAG\\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GoogleCloudStorageToBigQueryOperator\\nfrom datetime import datetime\\n\\ndefault_args = {\\n    'owner': 'airflow',\\n    'depends_on_past': False,\\n    'start_date': datetime(2023, 1, 1),\\n    'email_on_failure': False,\\n    'email_on_retry': False,\\n    'retries': 1,\\n}\\n\\ndag = DAG(\\n    'gcs_to_bigquery_migration',\\n    default_args=default_args,\\n    description='A DAG to migrate data from GCS to BigQuery',\\n    schedule_interval=None,  # Set your desired schedule interval here\\n)\\n\\n# Define the source and destination parameters\\nsource_bucket = 'your-source-bucket'\\ndestination_dataset = 'your-destination-dataset'\\ndestination_table = 'your-destination-table'\\n\\n# Task to load data from GCS to BigQuery\\nload_data_task = GoogleCloudStorageToBigQueryOperator(\\n    task_id='load_data_from_gcs_to_bigquery',\\n    bucket=source_bucket,\\n    source_objects=['path/to/your/data.csv'],\\n    destination_project_id='your-project-id',\\n    destination_dataset=destination_dataset,\\n    destination_table=destination_table,\\n    write_disposition='WRITE_TRUNCATE',  # or 'WRITE_APPEND' depending on your needs\\n    schema_fields=[\\n        {'name': 'column1', 'field_type': 'STRING'},\\n        {'name': 'column2', 'field_type': 'INTEGER'},\\n        # Add more fields as needed\\n    ],\\n    dag=dag,\\n)\",\n",
       "   'finetuned': '\"\"\"\\nExample DAG using GCSToTrinoOperator.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\\n\\n# [START howto_gcs]\\nbucket = \"airflow-system-tests-gcs\"\\nobject_path = \"trino/test-data.csv\"\\n# [END howto_gcs]\\n\\n# [START howto_config_trino]\\ntable = \"test\"\\n# [END howto_config_trino]\\n\\n# [START howto_move_config_trino]\\nmove_table = \"moved_test\"\\n# [END howto_move_config_trino]\\n\\nwith DAG(\\n    \"gcs-to-trino\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_operator_gcs_to_trino]\\n    run_this = GCSToTrinoOperator(\\n        task_id=\"gcs-to-trino-1\",\\n        gcs_bucket=bucket,\\n        source_object=object_path,\\n        trino_table=table,\\n    )\\n\\n    move_this_2 = GCSToTrinoOperator(\\n        task_id=\"gcs-to-trino-2\",\\n        gcs_bucket=bucket,\\n        source_object=object_path,\\n        trino_table=move_table,\\n        move=True,\\n        destination_object=f\"{object_path.replace(\\'.\\', \\'_\")}\",\\n    )\\n    # [END howto_operator_gcs_to_trino]\\n\\n\\nfrom tests.system.utils import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_992_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '2.9.3'}},\n",
       " {'id': 5,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user', 'content': 'Tell me how to use TensorFlow with Python.'}],\n",
       "  'responses': {'original': 'import tensorflow as tf\\n\\nprint(\"TensorFlow version:\", tf.__version__)',\n",
       "   'base_model': 'import tensorflow as tf',\n",
       "   'finetuned': \"import tensorflow as tf\\n\\n# Create a simple neural network model\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Dense(16, activation='relu', input_shape=(None, 1)),  # Input layer\\n    tf.keras.layers.Dense(8, activation='relu'),                          # Hidden layer\\n    tf.keras.layers.Dense(1)                                            # Output layer\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer='adam',\\n              loss=tf.keras.losses.MeanSquaredError(),\\n              metrics=['accuracy'])\\n\\n# Prepare data (example: input shape is (batch_size, 1))\\ninput_data = np.random.rand(100, 1)\\nlabels = np.random.rand(100, 1)\\n\\n# Train the model\\nmodel.fit(input_data, labels, epochs=50, batch_size=32)\\n\\n# Evaluate the model\\ntest_loss, test_acc = model.evaluate(test_input, test_labels)\\nprint(f'Test accuracy: {test_acc}')\"},\n",
       "  'metadata': None},\n",
       " {'id': 6,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'How can I create a Python function to retrieve the details of an Elastic Network Interface (ENI) using the Boto3 library?'}],\n",
       "  'responses': {'original': \"import boto3\\n\\ndef get_eni_details(eni_id):\\n    # Create an EC2 client\\n    ec2_client = boto3.client('ec2')\\n    \\n    # Call the describe_network_interfaces method\\n    response = ec2_client.describe_network_interfaces(NetworkInterfaceIds=[eni_id])\\n    \\n    # Extract the ENI details from the response\\n    eni_details = response['NetworkInterfaces'][0]\\n    \\n    return eni_details\\n\\n# Example usage\\neni_id = 'eni-1234567890abcdef0'\\neni_details = get_eni_details(eni_id)\\nprint(eni_details)\",\n",
       "   'base_model': 'import boto3\\n\\ndef get_eni_details(eni_id):\\n    # Create a session using your AWS credentials\\n    session = boto3.Session(\\n        aws_access_key_id=\\'YOUR_ACCESS_KEY\\',\\n        aws_secret_access_key=\\'YOUR_SECRET_KEY\\',\\n        region_name=\\'YOUR_REGION\\'\\n    )\\n    \\n    # Create an EC2 client from the session\\n    ec2_client = session.client(\\'ec2\\')\\n    \\n    # Retrieve the ENI details\\n    response = ec2_client.describe_network_interfaces(NetworkInterfaceIds=[eni_id])\\n    \\n    # Extract and return the details\\n    eni_details = response[\\'NetworkInterfaces\\'][0]\\n    return eni_details\\n\\n# Example usage\\neni_id = \\'eni-1234567890abcdef\\'\\neni_info = get_eni_details(eni_id)\\nprint(\"ENI Details:\", eni_info)',\n",
       "   'finetuned': \"import boto3\\n\\ndef get_eni_details(eni_id):\\n    # Create an EC2 client\\n    ec2_client = boto3.client('ec2')\\n    \\n    # Describe network interfaces\\n    response = ec2_client.describe_network_interfaces(\\n        Filters=[\\n            {\\n                'Name': 'network-interface-id',\\n                'Values': [eni_id]\\n            },\\n            {\\n                'Name': 'association.subnet-id',\\n                'Values': ['subnet-12345678']  # Example subnet ID\\n            }\\n        ]\\n    )\\n    \\n    # Extract and return the ENI details\\n    eni_details = {}\\n    for interface in response['NetworkInterfaces']:\\n        eni_details[interface['Association']['SubnetId']] = {\\n            'InterfaceId': interface['InterfaceId'],\\n            'PrivateIpAddresses': [\\n                {'PrivateIpAddress': ip} for ip in interface['PrivateIpAddresses']\\n            ],\\n            'Status': interface['Status']\\n        }\\n    \\n    return eni_details\\n\\n# Example usage\\neni_id = 'eni-0ab3c9dfe789abcde'  # Replace with your ENI ID\\nenior_details = get_eni_details(enini_id)\\nprint(enior_details)\"},\n",
       "  'metadata': None},\n",
       " {'id': 7,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Design a data management workflow for tracking employee information in a Vertica database. Create a process that demonstrates creating, populating, querying, and cleaning up an employee records table.\\n\\nAirflow Version: 3.0.0'}],\n",
       "  'responses': {'original': '\"\"\"\\nThis is an example DAG for the use of the SQLExecuteQueryOperator with Vertica.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow import DAG\\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nDAG_ID = \"example_vertica\"\\n\\nwith DAG(\\n    dag_id=DAG_ID,\\n    schedule=None,\\n    start_date=datetime(2025, 1, 1),\\n    default_args={\"conn_id\": \"vertica_conn_id\"},\\n    tags=[\"example\"],\\n    catchup=False,\\n) as dag:\\n    # [START howto_operator_vertica]\\n\\n    create_table_vertica_task = SQLExecuteQueryOperator(\\n        task_id=\"create_table_vertica\",\\n        sql=[\\n            \"DROP TABLE IF EXISTS employees;\",\\n            \"\"\"\\n            CREATE TABLE employees (\\n                id IDENTITY,\\n                name VARCHAR(50),\\n                salary NUMERIC(10,2),\\n                hire_date TIMESTAMP DEFAULT NOW()\\n            )\\n            \"\"\",\\n        ],\\n    )\\n\\n    # [END howto_operator_vertica]\\n\\n    insert_data_vertica_task = SQLExecuteQueryOperator(\\n        task_id=\"insert_data_vertica\",\\n        sql=\"\"\"\\n            INSERT INTO employees (name, salary) VALUES (\\'Alice\\', 50000);\\n            INSERT INTO employees (name, salary) VALUES (\\'Bob\\', 60000);\\n        \"\"\",\\n    )\\n\\n    select_data_vertica_task = SQLExecuteQueryOperator(\\n        task_id=\"select_data_vertica\",\\n        sql=\"SELECT * FROM employees\",\\n    )\\n\\n    drop_table_vertica_task = SQLExecuteQueryOperator(\\n        task_id=\"drop_table_vertica\",\\n        sql=\"DROP TABLE IF EXISTS employees\",\\n    )\\n\\n    (\\n        create_table_vertica_task\\n        >> insert_data_vertica_task\\n        >> select_data_vertica_task\\n        >> drop_table_vertica_task\\n    )\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # Ensure test watchers are triggered for success/failure\\n    list(dag.tasks) >> watcher()\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': \"from airflow import DAG\\nfrom airflow.providers.vertica.hooks.vertica_hook import VerticaHook\\nfrom airflow.operators.python_operator import PythonOperator\\nfrom datetime import datetime\\n\\n# Define the default arguments for the DAG\\ndefault_args = {\\n    'owner': 'airflow',\\n    'start_date': datetime(2023, 1, 1),\\n    'retries': 3,\\n    'retry_delay': timedelta(minutes=5)\\n}\\n\\n# Define the DAG\\nwith DAG(\\n    dag_id='employee_management_dag',\\n    description='A DAG to manage employee information in a Vertica database',\\n    schedule_interval=None,  # Set this to your desired schedule interval\\n    default_args=default_args\\n) as dag:\\n\\n    # Task to create the employee records table\\n    create_table_task = PythonOperator(\\n        task_id='create_employee_records_table',\\n        python_callable=create_employee_table,\\n        op_kwargs={\\n            'conn_id': 'vertica_conn'\\n        }\\n    )\\n\\n    # Task to populate the employee records table with initial data\\n    populate_table_task = PythonOperator(\\n        task_id='populate_employee_records_table',\\n        python_callable=populate_employee_table_data,\\n        op_kwargs={\\n            'conn_id': 'vertica_conn'\\n        }\\n    )\\n\\n    # Task to query the employee records table\\n    query_table_task = PythonOperator(\\n        task_id='query_employee_records_table',\\n        python_callable=query_employee_table_data,\\n        op_kwargs={\\n            'conn_id': 'vertica_conn'\\n        }\\n    )\\n\\n    # Task to clean up the employee records table\\n    cleanup_table_task = PythonOperator(\\n        task_id='cleanup_employee_records_table',\\n        python_callable=cleanup_employee_table,\\n        op_kwargs={\\n            'conn_id': 'vertica_conn'\\n        }\\n    )\\n\\n    # Define the dependencies between tasks\\n    create_table_task >> populate_table_task >> query_table_task >> cleanup_table_task\",\n",
       "   'finetuned': '\"\"\"\\nExample DAG using VerticaOperator.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.vertica.operators.vertica import VerticaOperator\\n\\n\\ndef setup_connections():\\n    # Connection created automatically when connecting to Vertica\\n    # No need to manually add it\\n    from airflow.utils import db\\n\\n    db.merge_conn(\\n        {\\n            \"conn_id\": \"vertica_default\",\\n            \"conn_type\": \"vertica\",\\n        }\\n    )\\n\\n\\nwith DAG(\\n    \"example_vertica_operator\",\\n    schedule=\"@once\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_vertica_operator]\\n    sql_table = \"VERTICA_OP_TEST_TABLE\"\\n    insert_sql = f\"INSERT INTO {sql_table} VALUES (42, \\'Test Value\\');\"\\n\\n    load_sql = f\"LOAD DATA INPATH \\'/tmp/insert.sql\\' INTO TABLE {sql_table} FIELDS TERMINATED BY \\',\\';\"\\n    delete_sql = f\"DELETE FROM {sql_table};\"\\n    select_sql = f\"SELECT * FROM {sql_table};\"\\n\\n    create_sql = f\"CREATE TABLE {sql_table} (id INT, value VARCHAR(10));\"\\n    drop_sql = f\"DROP TABLE IF EXISTS {sql_table};\"\\n\\n    t1 = VerticaOperator(task_id=\"task1\", vertica_query=create_sql)\\n    t2 = VerticaOperator(task_id=\"task2\", vertica_query=create_sql)\\n    t3 = VerticaOperator(task_id=\"task3\", vertica_query=insert_sql)\\n    t4 = VerticaOperator(task_id=\"task4\", vertica_query=load_sql)\\n    t5 = VerticaOperator(task_id=\"task5\", vertica_query=delete_sql)\\n    t6 = VerticaOperator(task_id=\"task6\", vertica_query=select_sql)\\n    t7 = VerticaOperator(task_id=\"task7\", vertica_query=drop_sql)\\n    t8 = VerticaOperator(task_id=\"task8\", vertica_query=drop_sql)\\n    # [END howto_vertica_operator]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_1418_variant_1',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 1,\n",
       "   'airflow_version': '3.0.0'}},\n",
       " {'id': 8,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Develop a data exploration DAG that uses Azure Data Lake Storage to perform a remote file inventory of Parquet-formatted files in a specific directory. Construct an automated task that lists file metadata to support data validation and tracking processes.\\n\\nAirflow Version: 3.0.6'}],\n",
       "  'responses': {'original': 'from __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow import models\\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSListOperator\\n\\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\\nREMOTE_FILE_PATH = os.environ.get(\"REMOTE_LOCAL_PATH\", \"remote.txt\")\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nDAG_ID = \"example_adls_list\"\\n\\nwith models.DAG(\\n    DAG_ID,\\n    start_date=datetime(2021, 1, 1),\\n    schedule=None,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START howto_operator_adls_list]\\n    adls_files = ADLSListOperator(\\n        task_id=\"adls_files\",\\n        path=\"folder/output/*.parquet\",\\n        azure_data_lake_conn_id=\"azure_data_lake_default\",\\n    )\\n    # [END howto_operator_adls_list]\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': 'from airflow import DAG\\nfrom airflow.providers.microsoft.azure.hooks.adls import ADLSHook\\nfrom airflow.operators.python_operator import PythonOperator\\nfrom datetime import datetime\\n\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'start_date\\': datetime(2023, 1, 1),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 0,\\n}\\n\\ndag = DAG(\\n    \\'adls_parquet_inventory\\',\\n    default_args=default_args,\\n    description=\\'DAG to list and validate Parquet files in Azure Data Lake Storage\\',\\n    schedule_interval=None,  # Set this to your desired schedule interval\\n)\\n\\n# Hook for Azure ADLS\\nadls_hook = ADLSHook(adls_conn_id=\\'your_adls_connection\\')\\n\\n# Task to list Parquet files\\ndef list_files():\\n    container_name = \\'your_container\\'\\n    file_pattern = \\'*.parquet\\'\\n    files = adls_hook.list_files(container=container_name, pattern=file_pattern)\\n    return files\\n\\nlist_files_task = PythonOperator(\\n    task_id=\\'list_files\\',\\n    python_callable=list_files,\\n    dag=dag,\\n)\\n\\n# Task to validate file metadata\\ndef validate_file_metadata(file_path):\\n    # Example: Check if the file has the expected schema\\n    # Replace with actual schema validation logic\\n    print(f\"Validating metadata for {file_path}\")\\n\\nvalidate_file_metadata_task = PythonOperator(\\n    task_id=\\'validate_file_metadata\\',\\n    python_callable=validate_file_metadata,\\n    op_kwargs={\\'file_path\\': \\'{{ ti.xcom_pull(task_ids=\"list_files\", key=\"return_value\") }}\\'},\\n    dag=dag,\\n)\\n\\n# Dependencies\\nlist_files_task >> validate_file_metadata_task',\n",
       "   'finetuned': 'from __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSListOperator\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nDAG_ID = \"adls_list_dag\"\\nwith DAG(\\n    dag_id=DAG_ID,\\n    start_date=datetime(2021, 1, 1),\\n    schedule=None,\\n    tags=[\"example\"],\\n) as dag:\\n    # [START adls_list_operator]\\n    adls_list_task = ADLSListOperator(task_id=\"adls_list_task\")\\n    # [END adls_list_operator]\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see below)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_2154_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '3.0.6'}},\n",
       " {'id': 9,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Develop a data migration workflow that transfers files from a cloud storage bucket to an SFTP destination with multiple transfer strategies. Create a pipeline that supports different file transfer modes including copying individual files, moving files, and transferring entire directory contents.\\n\\nAirflow Version: 3.0.0'}],\n",
       "  'responses': {'original': '\"\"\"\\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\nfrom pathlib import Path\\n\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nfrom system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\\nDAG_ID = \"gcs_to_sftp\"\\n\\nSFTP_CONN_ID = \"ssh_default\"\\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\\nDESTINATION_PATH_1 = \"/tmp/single-file/\"\\nDESTINATION_PATH_2 = \"/tmp/dest-dir-1/\"\\nDESTINATION_PATH_3 = \"/tmp/dest-dir-2/\"\\nFILE_NAME = GCS_SRC_FILE = \"empty.txt\"\\nUPLOAD_SRC = str(Path(__file__).parent / \"resources\" / FILE_NAME)\\nGCS_SRC_FILE_IN_DIR = f\"dir-1/{FILE_NAME}\"\\nGCS_SRC_DIR = \"dir-2/*\"\\nUPLOAD_IN_DIR_DST = f\"dir-2/{FILE_NAME}\"\\n\\n\\nwith DAG(\\n    DAG_ID,\\n    schedule=\"@once\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\", \"gcs\"],\\n) as dag:\\n    create_bucket = GCSCreateBucketOperator(\\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\\n    )\\n\\n    upload_file_1 = LocalFilesystemToGCSOperator(\\n        task_id=\"upload_file_1\",\\n        src=UPLOAD_SRC,\\n        dst=FILE_NAME,\\n        bucket=BUCKET_NAME,\\n    )\\n    upload_file_2 = LocalFilesystemToGCSOperator(\\n        task_id=\"upload_file_2\",\\n        src=UPLOAD_SRC,\\n        dst=GCS_SRC_FILE_IN_DIR,\\n        bucket=BUCKET_NAME,\\n    )\\n    upload_file_3 = LocalFilesystemToGCSOperator(\\n        task_id=\"upload_file_3\",\\n        src=UPLOAD_SRC,\\n        dst=UPLOAD_IN_DIR_DST,\\n        bucket=BUCKET_NAME,\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp_copy_single_file]\\n    copy_file_from_gcs_to_sftp = GCSToSFTPOperator(\\n        task_id=\"file-copy-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        source_bucket=BUCKET_NAME,\\n        source_object=GCS_SRC_FILE,\\n        destination_path=DESTINATION_PATH_1,\\n    )\\n    # [END howto_operator_gcs_to_sftp_copy_single_file]\\n\\n    check_copy_file_from_gcs_to_sftp = SFTPSensor(\\n        task_id=\"check-file-copy-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        timeout=60,\\n        path=os.path.join(DESTINATION_PATH_1, GCS_SRC_FILE),\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp_move_single_file_destination]\\n    move_file_from_gcs_to_sftp = GCSToSFTPOperator(\\n        task_id=\"file-move-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        source_bucket=BUCKET_NAME,\\n        source_object=GCS_SRC_FILE_IN_DIR,\\n        destination_path=DESTINATION_PATH_1,\\n        move_object=True,\\n    )\\n    # [END howto_operator_gcs_to_sftp_move_single_file_destination]\\n\\n    check_move_file_from_gcs_to_sftp = SFTPSensor(\\n        task_id=\"check-file-move-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        timeout=60,\\n        path=os.path.join(DESTINATION_PATH_1, GCS_SRC_FILE_IN_DIR),\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp_copy_directory]\\n    copy_dir_from_gcs_to_sftp = GCSToSFTPOperator(\\n        task_id=\"dir-copy-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        source_bucket=BUCKET_NAME,\\n        source_object=GCS_SRC_DIR,\\n        destination_path=DESTINATION_PATH_2,\\n    )\\n    # [END howto_operator_gcs_to_sftp_copy_directory]\\n\\n    check_copy_dir_from_gcs_to_sftp = SFTPSensor(\\n        task_id=\"check-dir-copy-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        timeout=60,\\n        path=os.path.join(DESTINATION_PATH_2, \"dir-2\", GCS_SRC_FILE),\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp_move_specific_files]\\n    move_dir_from_gcs_to_sftp = GCSToSFTPOperator(\\n        task_id=\"dir-move-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        source_bucket=BUCKET_NAME,\\n        source_object=GCS_SRC_DIR,\\n        destination_path=DESTINATION_PATH_3,\\n        keep_directory_structure=False,\\n    )\\n    # [END howto_operator_gcs_to_sftp_move_specific_files]\\n\\n    check_move_dir_from_gcs_to_sftp = SFTPSensor(\\n        task_id=\"check-dir-move-gsc-to-sftp\",\\n        sftp_conn_id=SFTP_CONN_ID,\\n        timeout=60,\\n        path=os.path.join(DESTINATION_PATH_3, GCS_SRC_FILE),\\n    )\\n\\n    delete_bucket = GCSDeleteBucketOperator(\\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\\n    )\\n\\n    (\\n        # TEST SETUP\\n        create_bucket\\n        >> (upload_file_1, upload_file_2, upload_file_3)\\n        # TEST BODY\\n        >> copy_file_from_gcs_to_sftp\\n        >> check_copy_file_from_gcs_to_sftp\\n        >> move_file_from_gcs_to_sftp\\n        >> check_move_file_from_gcs_to_sftp\\n        >> copy_dir_from_gcs_to_sftp\\n        >> check_copy_dir_from_gcs_to_sftp\\n        >> move_dir_from_gcs_to_sftp\\n        >> check_move_dir_from_gcs_to_sftp\\n        # TEST TEARDOWN\\n        >> delete_bucket\\n    )\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)',\n",
       "   'base_model': 'from airflow import DAG\\nfrom airflow.operators.python_operator import PythonOperator\\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\\nfrom airflow.providers.sftp.hooks.sftp import SFTPHook\\n\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'depends_on_past\\': False,\\n    \\'start_date\\': \\'2023-01-01\\',\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes=5),\\n}\\n\\ndag = DAG(\\n    \\'data_migration_dag\\',\\n    default_args=default_args,\\n    description=\\'A data migration workflow from S3 to SFTP\\',\\n    schedule_interval=None,\\n)\\n\\n# Define the S3 hook\\ns3_hook = S3Hook(aws_conn_id=\\'aws_default\\')\\n\\n# Define the SFTP hook\\nsftp_hook = SFTPHook(aws_conn_id=\\'sftp_default\\')\\n\\ndef download_file_from_s3(bucket_name, key, local_path):\\n    s3_hook.get_key(key).download_file(local_path)\\n    print(f\"Downloaded {key} to {local_path}\")\\n\\ndef upload_file_to_sftp(local_path, remote_path):\\n    sftp_hook.put(local_path, remote_path)\\n    print(f\"Uploaded {local_path} to {remote_path}\")\\n\\ndef delete_file_from_s3(bucket_name, key):\\n    s3_hook.delete_object(Bucket=bucket_name, Key=key)\\n    print(f\"Deleted {key} from {bucket_name}\")\\n\\n# Download a single file\\ndownload_task = PythonOperator(\\n    task_id=\\'download_file\\',\\n    python_callable=download_file_from_s3,\\n    op_kwargs={\\n        \\'bucket_name\\': \\'source-bucket-name\\',\\n        \\'key\\': \\'path/to/source/file.txt\\',\\n        \\'local_path\\': \\'/tmp/downloaded_file.txt\\'\\n    },\\n    dag=dag\\n)\\n\\n# Upload a single file\\nupload_task = PythonOperator(\\n    task_id=\\'upload_file\\',\\n    python_callable=upload_file_to_sftp,\\n    op_kwargs={\\n        \\'local_path\\': \\'/tmp/downloaded_file.txt\\',\\n        \\'remote_path\\': \\'path/to/destination/file.txt\\'\\n    },\\n    dag=dag\\n)\\n\\n# Delete a single file\\ndelete_task = PythonOperator(\\n    task_id=\\'delete_file\\',\\n    python_callable=delete_file_from_s3,\\n    op_kwargs={\\n        \\'bucket_name\\': \\'source-bucket-name\\',\\n        \\'key\\': \\'path/to/source/file.txt\\'\\n    },\\n    dag=dag\\n)\\n\\n# Download a directory\\ndownload_directory_task = PythonOperator(\\n    task_id=\\'download_directory\\',\\n    python_callable=download_file_from_s3,\\n    op_kwargs={\\n        \\'bucket_name\\': \\'source-bucket-name\\',\\n        \\'key\\': \\'path/to/source/directory/\\',\\n        \\'local_path\\': \\'/tmp/downloaded_directory/\\'\\n    },\\n    dag=dag\\n)\\n\\n# Upload a directory\\nupload_directory_task = PythonOperator(\\n    task_id=\\'upload_directory\\',\\n    python_callable=upload_file_to_sftp,\\n    op_kwargs={\\n        \\'local_path\\': \\'/tmp/downloaded_directory/\\',\\n        \\'remote_path\\': \\'path/to/destination/directory/\\'\\n    },\\n    dag=dag\\n)\\n\\n# Delete a directory\\ndelete_directory_task = PythonOperator(\\n    task_id=\\'delete_directory\\',\\n    python_callable=delete_file_from_s3,\\n    op_kwargs={\\n        \\'bucket_name\\': \\'source-bucket-name\\',\\n        \\'key\\': \\'path/to/source/directory/\\',\\n        \\'local_path\\': \\'/tmp/downloaded_directory/\\'\\n    },\\n    dag=dag\\n)\\n\\n# Set up dependencies\\ndownload_task >> upload_task >> delete_task\\ndownload_directory_task >> upload_directory_task >> delete_directory_task',\n",
       "   'finetuned': '\"\"\"\\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nfrom datetime import datetime\\nfrom pathlib import Path\\n\\nfrom airflow.decorators import task\\nfrom airflow.models.dag import DAG\\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\\nfrom airflow.utils.trigger_rule import TriggerRule\\n\\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\\n\\nDAG_ID = \"gcs_to_sftp\"\\n\\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\\nSFTP_PATH = \"/tmp/sftp-to-gcs\"\\nLOCAL_FILE_PATH = str(Path(__file__).parent / \"resources\" / \"example_transfer.txt\")\\n\\n\\nwith DAG(\\n    DAG_ID,\\n    schedule=\"@once\",\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n    tags=[\"example\", \"gcs\"],\\n) as dag:\\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\\n\\n    @task\\n    def create_temp_sftp_path():\\n        \"\"\"\\n        Task creates tmp folder in path specified on argument\\n        \"\"\"\\n        from airflow.providers.google.cloud.hooks.sftp_hook import SFTPHook\\n\\n        sftp_hook = SFTPHook()\\n        if sftp_hook.path_exists(SFTP_PATH):\\n            sftp_hook.create_directory(SFTP_PATH)\\n        else:\\n            sftp_hook.create_directory(SFTP_PATH)\\n\\n    create_temp_sftp_path_task = create_temp_sftp_path()\\n\\n    upload_file_1 = SFTPToGCSOperator(\\n        task_id=\"upload_file_1\",\\n        local_filepath=LOCAL_FILE_PATH,\\n        bucket_name=BUCKET_NAME,\\n        sftp_path=SFTP_PATH,\\n    )\\n\\n    # [START howto_operator_gcs_to_sftp]\\n    transfer_files = GCSToSFTTOperator(\\n        task_id=\"transfer_files\",\\n        source_bucket=BUCKET_NAME,\\n        source_object=\"/test/\",\\n        destination_sftp_path=SFTP_PATH,\\n    )\\n    # [END howto_operator_gcs_to_sftp]\\n\\n    download_file_1 = SFTPToGCSOperator(\\n        task_id=\"download_file_1\",\\n        local_filepath=LOCAL_FILE_PATH,\\n        bucket_name=BUCKET_NAME,\\n        sftp_path=SFTP_PATH,\\n    )\\n\\n    delete_bucket = GCSDeleteBucketOperator(\\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\\n    )\\n\\n    (\\n        # TEST SETUP\\n        create_bucket\\n        >> create_temp_sftp_path_task\\n        >> upload_file_1\\n\\n        # TEST BODY\\n        >> transfer_files\\n\\n        # TEST TEARDOWN\\n        >> delete_bucket\\n    )\\n\\n    from tests_common.test_utils.watcher import watcher\\n\\n    # This test needs watcher in order to properly mark success/failure\\n    # when \"tearDown\" task with trigger rule is part of the DAG\\n    list(dag.tasks) >> watcher()\\n\\n\\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\\n\\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\\ntest_run = get_test_run(dag)'},\n",
       "  'metadata': {'file_name': 'dag_1298_variant_3',\n",
       "   'instruction_source': 'claude-3.5',\n",
       "   'variant_number': 3,\n",
       "   'airflow_version': '3.0.0'}}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_consolidated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 8. Save Results on drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results saved:\n",
      "   Base model: generated_dags_base_model_20251217_114149.jsonl\n",
      "   Fine-tuned model: generated_dags_finetuned_model_20251217_114149.jsonl\n",
      "\n",
      "Downloading results...\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_5174330b-544d-449d-89ff-c53cdbc08039\", \"generated_dags_base_model_20251217_114149.jsonl\", 20703)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_5c1ee162-69aa-4117-a2e8-c5a672f987a1\", \"generated_dags_finetuned_model_20251217_114149.jsonl\", 23105)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Downloads initiated\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Create a timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"airflow_model_comparison_{timestamp}.jsonl\"\n",
    "\n",
    "# 2. Save the 'final_consolidated_results' list\n",
    "# (This assumes you ran the combining loop from the previous step)\n",
    "print(f\"Saving {len(final_consolidated_results)} examples to {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for entry in final_consolidated_results:\n",
    "        # verify we are saving the clean structure\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"âœ… Comparison data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## 11. Cleanup (Optional)\n",
    "\n",
    "Free up GPU memory if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_code"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete models to free memory\n",
    "del base_model\n",
    "del finetuned_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… GPU memory cleared\")\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
