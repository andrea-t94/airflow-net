{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'retries': 3,\n    'retry_delay': 5 * 60,  # 5 minutes\n}\n\n# Create the DAG\ndag = DAG(\n    'snowflake_product_load_validation',\n    default_args=default_args,\n    description='Load sample product data into Snowflake and validate the data load',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Function to load sample product data into Snowflake\ndef load_sample_products():\n    hook = SnowflakeHook(snowflake_conn_id='your_snowflake_connection')\n    \n    # Sample product data\n    products = [\n        {'product_id': 1, 'name': 'Product A', 'price': 19.99},\n        {'product_id': 2, 'name': 'Product B', 'price': 29.99},\n        {'product_id': 3, 'name': 'Product C', 'price': 39.99},\n        {'product_id': 4, 'name': 'Product D', 'price': 49.99},\n        {'product_id': 5, 'name': 'Product E', 'price': 59.99},\n        {'product_id': 6, 'name': 'Product F', 'price': 69.99},\n        {'product_id': 7, 'name': 'Product G', 'price': 79.99},\n        {'product_id': 8, 'name': 'Product H', 'price': 89.99},\n        {'product_id': 9, 'name': 'Product I', 'price': 99.99},\n        {'product_id': 10, 'name': 'Product J', 'price': 109.99},\n        {'product_id': 11, 'name': 'Product K', 'price': 119.99},\n        {'product_id': 12, 'name': 'Product L', 'price': 129.99}\n    ]\n    \n    # Insert the sample product data into the Snowflake table\n    query = \"\"\"\n    INSERT INTO products (product_id, name, price)\n    VALUES (%(product_id)s, %(name)s, %(price)s);\n    \"\"\"\n    hook.run(query=query, parameters=products)\n\n# Task to load sample product data\nload_task = PythonOperator(\n    task_id='load_sample_products',\n    python_callable=load_sample_products,\n    dag=dag,\n)\n\n# Function to validate the data load\ndef validate_data_load():\n    hook = SnowflakeHook(snowflake_conn_id='your_snowflake_connection')\n    \n    # Query to get the total number of rows in the products table\n    query = \"SELECT COUNT(*) FROM products;\"\n    result = hook.get_pandas_df(query=query)\n    \n    # Expected total number of rows\n    expected_rows = 12\n    \n    # Check if the actual number of rows matches the expected number\n    assert result['count'].values[0] == expected_rows, f\"Expected {expected_rows} rows, but got {result['count'].values[0]}\"\n    print(f\"Data load validation passed. Total rows in the products table: {result['count'].values[0]}\")\n\n# Task to validate the data load\nvalidate_task = PythonOperator(\n    task_id='validate_data_load',\n    python_callable=validate_data_load,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\nload_task >> validate_task"}], "metadata": {"file_name": "dag_1798_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981276.3509831}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a pandas DataFrame `df` with a column `text` that contains strings. I want to create a new column `text_len` that contains the length of each string in the `text` column. How can I achieve this?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Sample DataFrame creation\ndata = {\n    'text': ['Hello', 'World', 'This is a test']\n}\ndf = pd.DataFrame(data)\n\n# Adding a new column 'text_len' with the length of each string in 'text'\ndf['text_len'] = df['text'].str.len()\n\nprint(df)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.3509939}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that showcases the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class. Build a simple workflow that triggers an empty operator after standard work hours to illustrate scheduling flexibility.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.empty_operator import EmptyOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'after_workday_timetable_example',\n    default_args=default_args,\n    description='A DAG that uses AfterWorkdayTimetable for scheduling',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the task that will be triggered after standard work hours\ntask = EmptyOperator(\n    task_id='empty_task',\n    dag=dag\n)\n\n# Set up the AfterWorkdayTimetable\nfrom airflow.timetables.after_workday import AfterWorkdayTimetable\n\n# Schedule the task to run after standard work hours\ntask.set_upstream(None, AfterWorkdayTimetable(start_time=datetime(9, 0), end_time=datetime(17, 0)))"}], "metadata": {"file_name": "dag_1063_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510027}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that retrieves Campaign Manager metrics, generates a standard performance report across campaigns, and demonstrates batch conversion tracking and reporting capabilities. The DAG should illustrate how to interact with Campaign Manager's reporting and conversion APIs using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.campaign_manager import CampaignManagerHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'campaign_manager_reporting_dag',\n    default_args=default_args,\n    description='A DAG to retrieve Campaign Manager metrics, generate a performance report, and demonstrate batch conversion tracking and reporting.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Hook instances\nbigquery_hook = BigQueryHook()\ncm_hook = CampaignManagerHook()\n\n# Function to fetch Campaign Manager metrics\ndef fetch_cm_metrics():\n    cm_data = cm_hook.get_campaign_metrics()\n    return cm_data\n\n# Function to generate a performance report\ndef generate_performance_report(cm_data):\n    report_data = bigquery_hook.generate_performance_report(cm_data)\n    return report_data\n\n# Function to track and report conversions\ndef track_and_report_conversions(report_data):\n    conversion_data = cm_hook.track_conversion(report_data)\n    return conversion_data\n\n# Task to fetch Campaign Manager metrics\nfetch_cm_task = PythonOperator(\n    task_id='fetch_cm_metrics',\n    python_callable=fetch_cm_metrics,\n    dag=dag,\n)\n\n# Task to generate a performance report\ngenerate_report_task = PythonOperator(\n    task_id='generate_performance_report',\n    python_callable=generate_performance_report,\n    dag=dag,\n    depends_on=[fetch_cm_task],\n)\n\n# Task to track and report conversions\ntrack_and_report_conversions_task = PythonOperator(\n    task_id='track_and_report_conversions',\n    python_callable=track_and_report_conversions,\n    dag=dag,\n    depends_on=[generate_report_task],\n)\n\n# Define dependencies between tasks\nfetch_cm_task >> generate_report_task >> track_and_report_conversions_task"}], "metadata": {"file_name": "dag_1336_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510146}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data migration workflow that enables seamless transfer of structured CSV data from cloud object storage into a columnar analytics database. Create an Airflow task that supports efficient, automated data loading from cloud storage to a queryable data platform.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_migration_dag',\n    default_args=default_args,\n    description='A DAG to migrate data from GCS to PostgreSQL',\n    schedule_interval=None,\n)\n\n# Define the task to download data from GCS\ndownload_data_task = GCSHook().get_file(\n    bucket_name=\"your_bucket_name\",\n    object_name=\"path/to/your/file.csv\",\n    local_filename=\"/tmp/data.csv\"\n)\n\n# Define the task to load data into PostgreSQL\nload_data_task = PostgresHook(postgres_conn_id=\"postgres_conn\").run_sql(\n    sql=\"\"\"\n        COPY your_table_name FROM '/tmp/data.csv'\n        DELIMITER ',' CSV HEADER;\n    \"\"\",\n    parameters={\"your_table_name\": \"your_table_name\"}\n)\n\n# Set up the dependencies between tasks\ndownload_data_task >> load_data_task"}], "metadata": {"file_name": "dag_992_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510234}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Tell me how to use TensorFlow with Python."}, {"role": "assistant", "content": "import tensorflow as tf\n\n3. **Create a simple model**:\n   Define a simple neural network model using TensorFlow's Keras API."}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.3510313}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a Python function to retrieve the details of an Elastic Network Interface (ENI) using the Boto3 library?"}, {"role": "assistant", "content": "import boto3\n\n# Configure your AWS credentials\naws_access_key_id = 'YOUR_ACCESS_KEY'\naws_secret_access_key = 'YOUR_SECRET_KEY'\nregion_name = 'us-west-2'  # Replace with your desired region\n\n# Create an EC2 client\nec2_client = boto3.client('ec2', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.351038}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data management workflow for tracking employee information in a Vertica database. Create a process that demonstrates creating, populating, querying, and cleaning up an employee records table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='employee_management',\n    default_args=default_args,\n    description='A simple DAG to manage employee information in a Vertica database',\n    schedule_interval=None,\n) as dag:\n\n    # Task to create the employee records table\n    create_table_task = PythonOperator(\n        task_id='create_employee_records_table',\n        python_callable=create_table,\n        op_kwargs={\n            'table_name': 'employees',\n            'columns': [\n                ('id', 'INT PRIMARY KEY'),\n                ('name', 'VARCHAR(100)'),\n                ('department', 'VARCHAR(100)'),\n                ('salary', 'DECIMAL(10, 2)')\n            ]\n        }\n    )\n\n    # Task to insert data into the employee records table\n    insert_data_task = PythonOperator(\n        task_id='insert_employee_records',\n        python_callable=insert_data,\n        op_kwargs={\n            'table_name': 'employees',\n            'data': [\n                (1, 'John Doe', 'HR', 50000.00),\n                (2, 'Jane Smith', 'Engineering', 60000.00),\n                (3, 'Mike Johnson', 'Finance', 70000.00)\n            ]\n        }\n    )\n\n    # Task to query data from the employee records table\n    query_data_task = PythonOperator(\n        task_id='query_employee_records',\n        python_callable=query_data,\n        op_kwargs={\n            'table_name': 'employees'\n        }\n    )\n\n    # Task to clean up the employee records table\n    cleanup_table_task = PythonOperator(\n        task_id='cleanup_employee_records',\n        python_callable=cleanup_table,\n        op_kwargs={\n            'table_name': 'employees'\n        }\n    )\n\n    # Set the dependencies between tasks\n    create_table_task >> insert_data_task >> query_data_task >> cleanup_table_task"}], "metadata": {"file_name": "dag_1418_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.351049}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data exploration DAG that uses Azure Data Lake Storage to perform a remote file inventory of Parquet-formatted files in a specific directory. Construct an automated task that lists file metadata to support data validation and tracking processes.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.adls import AzureDataLakeHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'adls_parquet_inventory',\n    default_args=default_args,\n    description='DAG for listing Parquet files in Azure Data Lake Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Hook to interact with Azure Data Lake Storage\nadls_hook = AzureDataLakeHook(adls_conn_id='azure_data_lake_connection')\n\ndef list_files():\n    \"\"\"List all Parquet files in the specified ADLS directory.\"\"\"\n    try:\n        files = adls_hook.list_files(container_name='your_container', prefix='your_directory')\n        return files\n    except Exception as e:\n        raise ValueError(f\"Failed to list files: {e}\")\n\ndef extract_metadata(files):\n    \"\"\"Extract metadata for each file.\"\"\"\n    metadata_list = []\n    for file in files:\n        file_path = f'your_container/{file}'\n        metadata = adls_hook.get_file_properties(file_path)\n        metadata_list.append(metadata)\n    return metadata_list\n\nlist_files_task = PythonOperator(\n    task_id='list_files',\n    python_callable=list_files,\n    dag=dag\n)\n\nextract_metadata_task = PythonOperator(\n    task_id='extract_metadata',\n    python_callable=extract_metadata,\n    op_kwargs={'files': '{{ ti.xcom_pull(task_ids=\"list_files\", key=\"return_value\") }}'},\n    dag=dag\n)\n\nlist_files_task >> extract_metadata_task"}], "metadata": {"file_name": "dag_2154_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510592}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration workflow that transfers files from a cloud storage bucket to an SFTP destination with multiple transfer strategies. Create a pipeline that supports different file transfer modes including copying individual files, moving files, and transferring entire directory contents.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'gcs_to_sftp_migration',\n    default_args=default_args,\n    description='Migrates files from GCS to SFTP',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to copy individual files\ncopy_individual_file_task = GCSToSFTPOperator(\n    task_id='copy_individual_file',\n    source_bucket='source-bucket-name',\n    source_object='path/to/source/file.txt',\n    sftp_host='sftp.example.com',\n    sftp_port=22,\n    sftp_username='username',\n    sftp_password='password',\n    sftp_remote_path='/remote/path/to/destination/',\n    dag=dag,\n)\n\n# Task to move files\nmove_files_task = GCSToSFTPOperator(\n    task_id='move_files',\n    source_bucket='source-bucket-name',\n    source_object='path/to/source/file.txt',\n    sftp_host='sftp.example.com',\n    sftp_port=22,\n    sftp_username='username',\n    sftp_password='password',\n    sftp_remote_path='/remote/path/to/destination/',\n    operation='mv',\n    dag=dag,\n)\n\n# Task to transfer entire directory contents\ntransfer_directory_task = GCSToSFTPOperator(\n    task_id='transfer_directory',\n    source_bucket='source-bucket-name',\n    source_object='path/to/source/directory/',\n    sftp_host='sftp.example.com',\n    sftp_port=22,\n    sftp_username='username',\n    sftp_password='password',\n    sftp_remote_path='/remote/path/to/destination/',\n    operation='cp',\n    recursive=True,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1298_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510685}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I want to format the date and time from a Unix timestamp to a human-readable format using Python. How can I do this?"}, {"role": "assistant", "content": "import datetime\n\n# Example Unix timestamp\nunix_timestamp = 1633072800\n\n# Convert Unix timestamp to datetime object\ndt_object = datetime.datetime.fromtimestamp(unix_timestamp)\n\n# Format the datetime object into a readable string\nformatted_date_time = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n\nprint(formatted_date_time)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.3510737}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow for deploying and managing an EMR (Elastic MapReduce) job on an EKS (Elastic Kubernetes Service) cluster using AWS services. Create a data processing pipeline that demonstrates launching a Kubernetes cluster, configuring IAM roles, and running a sample Python computation job.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.emr import EmrHook\nfrom airflow.providers.amazon.aws.sensors.emr import EmrStepSensor\nfrom airflow.providers.amazon.aws.operators.emr import EmrCreateClusterOperator, EmrTerminateClusterOperator\nfrom airflow.providers.amazon.aws.operators.kubernetes import KubernetesPodOperator\nfrom airflow.providers.amazon.aws.operators.ecs import ECSRunTaskOperator\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='emr_eks_dag',\n    default_args=default_args,\n    description='Automated workflow for deploying and managing an EMR job on an EKS cluster',\n    schedule_interval=timedelta(days=1),\n) as dag:\n\n    # Create an EMR cluster\n    create_emr_cluster = EmrCreateClusterOperator(\n        task_id='create_emr_cluster',\n        name='my-emr-cluster',\n        release_label='emr-6.3.0',\n        instance_type='m5.xlarge',\n        num_instances=3,\n        emr_release_version='emr-6.3.0',\n        keep_job_flow_alive_when_no_steps=True,\n        log_uri='s3://my-bucket/logs/emr/',\n        tags={'Environment': 'Development'}\n    )\n\n    # Wait for the EMR cluster to be ready\n    wait_for_emr_cluster = EmrStepSensor(\n        task_id='wait_for_emr_cluster',\n        job_flow_id=create_emr_cluster.output['jobFlowId'],\n        aws_conn_id='aws_default'\n    )\n\n    # Run a sample Python computation job on the EMR cluster\n    run_python_computation_job = KubernetesPodOperator(\n        task_id='run_python_computation_job',\n        namespace='default',\n        image='python:3.9-slim',\n        command=['python', '/app/compute.py'],\n        args=[],\n        env_vars={\n            'EMR_CLUSTER_ID': '{{ task_instance.xcom_pull(task_ids=\"create_emr_cluster\", key=\"output.jobFlowId\") }}'\n        },\n        executor_config={\n            'kubernetes':\n                {\n                    'service_account_name': 'emr-service-account'\n                }\n        },\n        secrets=[\n            {\n                'name': 'aws_access_key_id',\n                'value_from': SecretKeyRef(key='aws_access_key_id')\n            },\n            {\n                'name': 'aws_secret_access_key',\n                'value_from': SecretKeyRef(key='aws_secret_access_key')\n            }\n        ],\n        dag=dag\n    )\n\n    # Terminate the EMR cluster\n    terminate_emr_cluster = EmrTerminateClusterOperator(\n        task_id='terminate_emr_cluster',\n        job_flow_id=create_emr_cluster.output['jobFlowId'],\n        aws_conn_id='aws_default'\n    )"}], "metadata": {"file_name": "dag_390_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981276.3510919}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a data processing workflow with explicit initialization and cleanup stages, using bash commands to represent different phases of data preparation, processing, and finalization. Create a DAG that illustrates how setup tasks can prepare an environment and teardown tasks can perform necessary cleanup operations around the main data processing logic.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-10-01',\n    'retries': 3,\n    'retry_delay': 5 * 60,  # 5 minutes\n}\n\ndag = DAG(\n    'data_processing_dag',\n    default_args=default_args,\n    description='A simple data processing DAG',\n    schedule_interval=None,\n)\n\n# Setup Environment Task\nsetup_environment = BashOperator(\n    task_id='setup_environment',\n    bash_command=\"\"\"\n        # Initialize your environment here\n        echo \"Setting up environment...\"\n        mkdir -p /path/to/data\n        touch /path/to/data/input.txt\n        echo \"Environment setup completed.\"\n    \"\"\",\n    dag=dag,\n)\n\n# Main Data Processing Task\nmain_data_processing = BashOperator(\n    task_id='main_data_processing',\n    bash_command=\"\"\"\n        # Perform your data processing here\n        echo \"Processing data...\"\n        cat /path/to/data/input.txt | awk '{print $1}' > /path/to/data/output.txt\n        echo \"Data processing completed.\"\n    \"\"\",\n    dag=dag,\n)\n\n# Cleanup Environment Task\ncleanup_environment = BashOperator(\n    task_id='cleanup_environment',\n    bash_command=\"\"\"\n        # Clean up your environment here\n        echo \"Cleaning up environment...\"\n        rm -rf /path/to/data\n        echo \"Environment cleanup completed.\"\n    \"\"\",\n    dag=dag,\n)\n\n# Define dependencies between tasks\nsetup_environment >> main_data_processing >> cleanup_environment"}], "metadata": {"file_name": "dag_22_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981276.351103}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need to convert a string to binary in Python. How can I do that?"}, {"role": "assistant", "content": "# Define the string to be converted\ntext = \"Hello, World!\"\n\n# Convert the string to binary\nbinary_representation = bin(text)\n\n# Print the binary representation\nprint(binary_representation)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.35111}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I modify the color of a button in a form using the `setStyle` method in Python with the PyQt5 library?"}, {"role": "assistant", "content": "from PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QLabel\nfrom PyQt5.QtGui import QColor\n\nclass MyForm(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.initUI()\n\n    def initUI(self):\n        # Create a label and set its text\n        self.label = QLabel(\"Click me!\", self)\n        self.label.setGeometry(10, 10, 200, 30)\n\n        # Create a button and set its text\n        self.button = QPushButton(\"Change Color\", self)\n        self.button.setGeometry(10, 40, 100, 30)\n\n        # Set the initial style for the button\n        self.setButtonColor(QColor('blue'))\n\n        # Connect the button's clicked signal to the change_color slot\n        self.button.clicked.connect(self.change_color)\n\n    def setButtonColor(self, color):\n        # Get the current palette of the button\n        palette = self.button.palette()\n        \n        # Set the background color of the button\n        palette.setColor(palette.Button, palette.WindowText, color)\n        \n        # Apply the new palette to the button\n        self.button.setPalette(palette)\n\nif __name__ == '__main__':\n    app = QApplication([])\n    ex = MyForm()\n    ex.show()\n    sys.exit(app.exec_())"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.3511178}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Generate a Python code to create a DataFrame from a dictionary where each key-value pair in the dictionary represents a column and its corresponding values. The dictionary provided is: `{'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'City': ['New York', 'Los Angeles', 'Chicago']}`. How can I handle missing values if any of the lists have a different length?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Define the dictionary with data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30],\n    'City': ['New York', 'Los Angeles']\n}\n\n# Create a DataFrame from the dictionary\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.351123}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kafka-based FizzBuzz game that demonstrates message streaming and event processing. Create a system that generates numbers, checks for divisibility by 3 and 5, and triggers specific actions based on number characteristics.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport random\n\ndef fizz_buzz_task():\n    for i in range(1, 101):\n        if i % 3 == 0 and i % 5 == 0:\n            print(f\"{i} - FizzBuzz\")\n        elif i % 3 == 0:\n            print(f\"{i} - Fizz\")\n        elif i % 5 == 0:\n            print(f\"{i} - Buzz\")\n        else:\n            print(i)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\nwith DAG(\n    dag_id='kafka_fizzbuzz_dag',\n    default_args=default_args,\n    schedule_interval=None,\n) as dag:\n\n    fizz_buzz_task = PythonOperator(\n        task_id='fizz_buzz_task',\n        python_callable=fizz_buzz_task,\n    )"}], "metadata": {"file_name": "dag_1533_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981276.3511317}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how preparatory and cleanup operations can be automatically managed around core task execution. Create a DAG that illustrates task dependency and group management with clear initialization and finalization steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'setup_teardown_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate setup and teardown tasks',\n    schedule_interval=None,\n)\n\n# Setup task\ndef setup():\n    print(\"Setting up the environment\")\n\nsetup_task = PythonOperator(\n    task_id='setup_task',\n    python_callable=setup,\n    dag=dag,\n)\n\n# Task 1\ntask_1 = PythonOperator(\n    task_id='task_1',\n    python_callable=lambda: print(\"Executing task 1\"),\n    dag=dag,\n)\n\n# Task 2\ntask_2 = PythonOperator(\n    task_id='task_2',\n    python_callable=lambda: print(\"Executing task 2\"),\n    dag=dag,\n)\n\n# Task 3\ntask_3 = PythonOperator(\n    task_id='task_3',\n    python_callable=lambda: print(\"Executing task 3\"),\n    dag=dag,\n)\n\n# Teardown task\ndef teardown():\n    print(\"Cleaning up the environment\")\n\nteardown_task = PythonOperator(\n    task_id='teardown_task',\n    python_callable=teardown,\n    dag=dag,\n)\n\n# Set up task as a prerequisite for all other tasks\nsetup_task.set_upstream(task_1)\nsetup_task.set_upstream(task_2)\nsetup_task.set_upstream(task_3)\n\n# Group tasks together\ngrouped_tasks = [task_1, task_2, task_3]\n\n# Add a final task after all grouped tasks have completed\nfinal_task = PythonOperator(\n    task_id='final_task',\n    python_callable=lambda: print(\"All tasks completed\"),\n    dag=dag,\n)\n\n# Set up task as a prerequisite for the final task\nfor task in grouped_tasks:\n    task.set_downstream(final_task)"}], "metadata": {"file_name": "dag_1446_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981276.351141}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u5728Python\u4e2d\uff0c`len()`\u51fd\u6570\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u5b57\u7b26\u4e32\u3001\u5217\u8868\u3001\u5143\u7ec4\u4ee5\u53ca\u5b57\u5178\u7b49\u96c6\u5408\u7c7b\u578b\u7684\u5143\u7d20\u6570\u91cf\u3002\u7136\u800c\uff0c\u5b83\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u6574\u6570\u7c7b\u578b\u7684\u6570\u636e\u3002\u5982\u679c\u6211\u60f3\u8ba1\u7b97\u4e00\u4e2a\u6574\u6570\u7684\u4f4d\u6570\uff0c\u5e94\u8be5\u600e\u4e48\u505a\u5462\uff1f"}, {"role": "assistant", "content": "def count_digits(number):\n    # \u5c06\u6574\u6570\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\n    number_str = str(number)\n    \n    # \u8fd4\u56de\u5b57\u7b26\u4e32\u7684\u957f\u5ea6\n    return len(number_str)\n\n# \u793a\u4f8b\u7528\u6cd5\nnumber = 1234567890\nprint(count_digits(number))  # \u8f93\u51fa: 10"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981276.3511465}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complex cloud infrastructure setup, including creating a Compute Engine instance, configuring a PostgreSQL database, and transferring data to Google Sheets. Showcase a multi-step cloud data workflow with infrastructure provisioning and data migration.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    GKEClusterCreateOperator,\n    GKEClusterDeleteOperator,\n)\nfrom airflow.providers.google.cloud.operators.postgres import (\n    PostgresCreateDatabaseOperator,\n    PostgresExecuteQueryOperator,\n)\nfrom airflow.providers.google.cloud.operators.sheets import (\n    GoogleSheetsUploadOperator,\n)\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Define the main DAG\nwith DAG(\n    dag_id='cloud_infrastructure_setup',\n    default_args=default_args,\n    schedule_interval=None,\n) as dag:\n\n    # Step 1: Create a Compute Engine cluster\n    create_cluster = GKEClusterCreateOperator(\n        task_id='create_cluster',\n        project_id='your-project-id',\n        location='us-central1',\n        cluster_name='my-cluster',\n        num_nodes=2,\n        machine_type='n1-standard-2',\n    )\n\n    # Step 2: Configure a PostgreSQL database\n    create_database = PostgresCreateDatabaseOperator(\n        task_id='create_database',\n        postgres_conn_id='postgres_default',\n        database_name='mydatabase',\n    )\n\n    # Step 3: Execute SQL queries to configure the database\n    execute_queries = PostgresExecuteQueryOperator(\n        task_id='execute_queries',\n        postgres_conn_id='postgres_default',\n        sql=[\n            \"CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, name VARCHAR(100), email VARCHAR(100))\",\n            \"INSERT INTO users (name, email) VALUES ('John Doe', 'john@example.com')\",\n        ],\n    )\n\n    # Step 4: Transfer data from PostgreSQL to Google Sheets\n    upload_to_sheets = GoogleSheetsUploadOperator(\n        task_id='upload_to_sheets',\n        google_cloud_default_project_id='your-project-id',\n        sheet_id='Sheet1',\n        value_input_option='USER_ENTERED',\n        header=True,\n        values=[\n            ['ID', 'Name', 'Email'],\n            [1, 'John Doe', 'john@example.com'],\n        ],\n    )\n\n    # Step 5: Delete the Compute Engine cluster\n    delete_cluster = GKEClusterDeleteOperator(\n        task_id='delete_cluster',\n        project_id='your-project-id',\n        location='us-central1',\n        cluster_name='my-cluster',\n    )\n\n    # Set up dependencies between tasks\n    create_cluster >> create_database >> execute_queries >> upload_to_sheets >> delete_cluster"}], "metadata": {"file_name": "dag_910_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981276.3511572}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Compute Engine instance templates and instance group managers. Create a DAG that demonstrates how to create, copy, and update instance templates with different configurations in a cloud infrastructure environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    GCPComputeInstanceTemplateCreateOperator,\n    GCPComputeInstanceTemplateCopyOperator,\n    GCPComputeInstanceGroupManagerCreateOperator,\n    GCPComputeInstanceGroupManagerUpdateOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'gcp_compute_instance_template_and_group_manager',\n    default_args=default_args,\n    description='Automate Google Compute Engine instance template and group manager management',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Step 1: Create an instance template\ncreate_template_task = GCPComputeInstanceTemplateCreateOperator(\n    task_id='create_instance_template',\n    project='your-project-id',\n    name='my-instance-template',\n    machine_type='n1-standard-1',\n    disks=[\n        {\n            'initialize_params': {\n                'source_image': 'debian-cloud/debian-11'\n            }\n        }\n    ],\n    network_interfaces=[\n        {\n            'network': 'default',\n            'subnetwork': 'default-subnet'\n        }\n    ]\n)\n\n# Step 2: Copy the created instance template\ncopy_template_task = GCPComputeInstanceTemplateCopyOperator(\n    task_id='copy_instance_template',\n    source_project='your-project-id',\n    source_name='my-instance-template',\n    destination_project='your-project-id',\n    destination_name='my-instance-template-copy'\n)\n\n# Step 3: Update the copied instance template\nupdate_template_task = GCPComputeInstanceTemplateUpdateOperator(\n    task_id='update_instance_template',\n    project='your-project-id',\n    name='my-instance-template-copy',\n    machine_type='n1-standard-2'\n)\n\n# Step 4: Create an instance group manager\ncreate_group_manager_task = GCPComputeInstanceGroupManagerCreateOperator(\n    task_id='create_instance_group_manager',\n    project='your-project-id',\n    zone='us-central1-a',\n    name='my-instance-group-manager',\n    target_size=1,\n    base_instance_name='my-base-instance',\n    template='projects/your-project-id/global/compute/disks/my-instance-template'\n)\n\n# Step 5: Update the instance group manager\nupdate_group_manager_task = GCPComputeInstanceGroupManagerUpdateOperator(\n    task_id='update_instance_group_manager',\n    project='your-project-id',\n    zone='us-central1-a',\n    name='my-instance-group-manager',\n    target_size=2\n)\n\n# Define the dependencies between tasks\ncreate_template_task >> copy_template_task >> update_template_task >> create_group_manager_task >> update_group_manager_task"}], "metadata": {"file_name": "dag_1204_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3511672}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data transfer workflow that automatically moves files between source and destination buckets. Create a DAG that demonstrates GCS transfer job creation, file upload, and bucket management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs_operator import GCSToGCSOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'gcs_data_transfer',\n    default_args=default_args,\n    description='A simple Google Cloud Storage data transfer workflow',\n    schedule_interval=timedelta(days=1),\n)"}], "metadata": {"file_name": "dag_1291_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3511732}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-stage workflow that demonstrates task grouping and sequential processing using Airflow. Create a DAG that shows how complex workflows can be organized into logical sections with nested task groups.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-10-01',\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example of a multi-stage workflow',\n    schedule_interval=None,\n)\n\n# Task group 1\nwith dag.create_task_group('group_1'):\n    t1 = PythonOperator(\n        task_id='task_1',\n        python_callable=lambda: print(\"Task 1 executed\"),\n    )\n\n    t2 = PythonOperator(\n        task_id='task_2',\n        python_callable=lambda: print(\"Task 2 executed\"),\n    )\n\n# Task group 2\nwith dag.create_task_group('group_2'):\n    t3 = PythonOperator(\n        task_id='task_3',\n        python_callable=lambda: print(\"Task 3 executed\"),\n    )\n\n    t4 = PythonOperator(\n        task_id='task_4',\n        python_callable=lambda: print(\"Task 4 executed\"),\n    )"}], "metadata": {"file_name": "dag_354_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981276.3511798}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport boto3\nimport pandas as pd\n\n# Define the DAG properties\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'end_date': '2023-04-30',\n    'schedule_interval': '@daily'\n}\n\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='Automated data pipeline from S3 to AWS QuickSight',\n    catchup=False\n)\n\n# Function to download data from S3\ndef download_data_from_s3():\n    s3_client = boto3.client('s3')\n    bucket_name = 'your-s3-bucket-name'\n    file_key = 'path/to/your/file.csv'\n    local_file_path = '/tmp/data.csv'\n\n    s3_client.download_file(bucket_name, file_key, local_file_path)\n    print(f'Data downloaded from {bucket_name}/{file_key} to {local_file_path}')\n\n# Task to download data from S3\ndownload_data_task = PythonOperator(\n    task_id='download_data',\n    python_callable=download_data_from_s3,\n    dag=dag\n)\n\n# Function to upload data to AWS QuickSight\ndef upload_data_to_quicksight(local_file_path):\n    quicksight_client = boto3.client('quicksight')\n    bucket_name = 'your-quicksight-bucket-name'\n    file_key = 'path/to/uploaded/file.csv'\n\n    # Read data into a DataFrame\n    df = pd.read_csv(local_file_path)\n\n    # Upload data to QuickSight\n    response = quicksight_client.create_data_source(\n        DataSourceId='my-data-source',\n        Name='My Data Source',\n        Type='CSV',\n        AwsAccountId='<YOUR_AWS_ACCOUNT_ID>',\n        PhysicalTableMap={\n            'table1': {\n                'Name': 'table1',\n                'Type': 'TABLE',\n                'PhysicalTableDefinition': {\n                    'Catalog': '<YOUR_CATALOG_NAME>',\n                    'Database': '<YOUR_DATABASE_NAME>',\n                    'Schema': '<YOUR_SCHEMA_NAME>',\n                    'TableName': '<YOUR_TABLE_NAME>'\n                }\n            }\n        },\n        ConnectionProperties={\n            'ConnectionMode': 'DIRECT',\n            'AwsIamRoleArn': '<YOUR_IAM_ROLE_ARN>'\n        }\n    )\n\n    # Create a new dataset\n    response = quicksight_client.create_dataset(\n        DataSetId='my-dataset',\n        Name='My Dataset',\n        Source={},\n        Schema={\n            'Columns': [\n                {'Name': 'column1', 'DataType': 'STRING'},\n                {'Name': 'column2', 'DataType': 'INTEGER'}\n            ]\n        }\n    )\n\n    # Add table to dataset\n    response = quicksight_client.add_table_to_data_set(\n        DataSetId='my-dataset',\n        TableId='table1',\n        DataSetSourceId=response['DataSet']['DataSourceId']\n    )\n\n    # Upload data to dataset\n    response = quicksight_client.upload_csv_to_data_set(\n        DataSetId='my-dataset',\n        CsvContent=df.to_csv(index=False).encode('utf-8'),\n        TableName='table1'\n    )\n\n    print(f'Data uploaded to {bucket_name}/{file_key} in QuickSight')\n\n# Task to upload data to AWS QuickSight\nupload_data_task = PythonOperator(\n    task_id='upload_data_to_quicksight',\n    python_callable=upload_data_to_quicksight,\n    op_kwargs={'local_file_path': '/tmp/data.csv'},\n    dag=dag\n)\n\n# Define the dependencies between tasks\ndownload_data_task >> upload_data_task"}], "metadata": {"file_name": "dag_1112_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.351193}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Data Loss Prevention (DLP) workflow that demonstrates the complete lifecycle of a DLP job in Google Cloud, from creation to deletion. Design a DAG that showcases how to programmatically manage sensitive data inspection tasks using Airflow and Google Cloud DLP operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CreateDlpJobOperator,\n    InspectContentOperator,\n    DeleteDlpJobOperator,\n)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'dlp_dag',\n    default_args=default_args,\n    description='A DAG to demonstrate the lifecycle of a DLP job in Google Cloud',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to create a DLP job\ncreate_job_task = CreateDlpJobOperator(\n    task_id='create_dlp_job',\n    project_id='your-project-id',\n    location='us-central1',\n    dlp_job={\n        'inspect_content': {\n            'storage_config': {\n                'cloud_storage_options': {\n                    'uri': 'gs://your-bucket-name/your-file.txt'\n                }\n            },\n            'info_types': [\n                {'name': 'PHONE_NUMBER'},\n                {'name': 'EMAIL_ADDRESS'}\n            ]\n        }\n    },\n    dag=dag,\n)\n\n# Task to inspect content using the created DLP job\ninspect_content_task = InspectContentOperator(\n    task_id='inspect_content',\n    project_id='your-project-id',\n    location='us-central1',\n    dlp_job_name=create_job_task.output,\n    dag=dag,\n)\n\n# Task to delete the DLP job\ndelete_job_task = DeleteDlpJobOperator(\n    task_id='delete_dlp_job',\n    project_id='your-project-id',\n    location='us-central1',\n    dlp_job_name=create_job_task.output,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2017_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981276.351202}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates a complete machine learning model lifecycle, including provisioning AWS resources, configuring SageMaker training jobs, creating model artifacts, and setting up a scalable inference endpoint. The workflow should demonstrate end-to-end machine learning infrastructure management with robust error handling and resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerTrainingOperator,\n    SageMakerModelOperator,\n    SageMakerEndpointOperator,\n)\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'ml_model_lifecycle',\n    default_args=default_args,\n    description='A complete machine learning model lifecycle using Amazon SageMaker',\n    schedule_interval=None,\n)\n\n# Step 1: Provision AWS resources\nprovision_sagemaker_role_task = SageMakerRoleOperator(\n    task_id='provision_sagemaker_role',\n    role_name='MLModelProvisioningRole',\n    dag=dag,\n)\n\n# Step 2: Configure SageMaker training job\ntrain_model_task = SageMakerTrainingOperator(\n    task_id='train_model',\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    image_uri='aws/sagemaker-xgboost:latest',\n    output_path='s3://my-bucket/model-output/',\n    input_data='s3://my-bucket/input-data/',\n    hyperparameters={'max_depth': 6},\n    dag=dag,\n)\n\n# Step 3: Create model artifact\ncreate_model_artifact_task = SageMakerModelOperator(\n    task_id='create_model_artifact',\n    model_name='my-model',\n    primary_container={\n        'image': 'aws/sagemaker-xgboost:latest',\n        'model_data': f's3://{os.getenv(\"S3_BUCKET\")}/model-output/model.tar.gz',\n    },\n    dag=dag,\n)\n\n# Step 4: Set up scalable inference endpoint\ndeploy_endpoint_task = SageMakerEndpointOperator(\n    task_id='deploy_endpoint',\n    endpoint_name='my-endpoint',\n    model_name='my-model',\n    initial_instance_count=1,\n    instance_type='ml.m4.xlarge',\n    dag=dag,\n)\n\n# Step 5: Monitor and manage resources\nmonitor_resources_task = SageMakerMonitoringTask(\n    task_id='monitor_resources',\n    monitoring_job_name='my-monitoring-job',\n    s3_output_location=f's3://{os.getenv(\"S3_BUCKET\")}/monitoring-results/',\n    dag=dag,\n)\n\n# Define dependencies between tasks\nprovision_sagemaker_role_task >> train_model_task >> create_model_artifact_task >> deploy_endpoint_task >> monitor_resources_task"}], "metadata": {"file_name": "dag_1928_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981276.351211}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates a simple file transfer workflow from a local temporary file to an AWS S3 bucket. Create a process that generates a temporary file, transfers it to cloud storage, and then cleans up the temporary resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3UploadFileOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'file_transfer_dag',\n    default_args=default_args,\n    description='A simple file transfer workflow from a local temporary file to an AWS S3 bucket.',\n    schedule_interval=None,\n)\n\n# Step 1: Create an S3 Bucket if it doesn't exist\ncreate_bucket_operator = S3CreateBucketOperator(\n    task_id='create_s3_bucket',\n    bucket_name='your-bucket-name',\n    aws_conn_id='aws_default'\n)\n\n# Step 2: Upload a file to the S3 bucket\nupload_file_operator = S3UploadFileOperator(\n    task_id='upload_file_to_s3',\n    s3_key='path/to/your/file.txt',\n    s3_object_key='your-bucket-name/path/to/your/file.txt',\n    aws_conn_id='aws_default'\n)\n\n# Step 3: Clean up the temporary file (this step can be omitted if you don't need to clean up)\ncleanup_task = BashOperator(\n    task_id='cleanup_temporary_file',\n    bash_command='rm /tmp/temp_file.txt',\n    dag=dag\n)\n\n# Define the dependencies between tasks\ncreate_bucket_operator >> upload_file_operator >> cleanup_task"}], "metadata": {"file_name": "dag_1156_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3512187}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Campaign Manager reporting and conversion tracking workflow that retrieves digital marketing performance metrics and logs conversion events. Create a system that can dynamically pull campaign data, generate reports, and record conversion activities using Campaign Manager 360 APIs.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom google.oauth2.credentials import ServiceAccountCredentials\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'campaign_manager_dag',\n    default_args=default_args,\n    description='Automated Campaign Manager Reporting and Conversion Tracking Workflow',\n    schedule_interval=timedelta(days=1)\n)\n\n# Function to retrieve campaign data from Campaign Manager 360 API\ndef get_campaign_data():\n    # Replace with your actual credentials and API endpoint\n    credentials = ServiceAccountCredentials.from_service_account_file('path/to/your/service-account.json')\n    hook = BigQueryHook(credentials=credentials)\n    \n    query = \"\"\"\n        SELECT *\n        FROM `project.dataset.campaigns`\n    \"\"\"\n    \n    result = hook.run(query=query, use_legacy_sql=False)\n    return result.rows\n\n# Function to generate reports\ndef generate_reports(campaign_data):\n    # Implement report generation logic here\n    print(\"Generating reports...\")\n    # Example: Save the data to a CSV file\n    with open('reports.csv', 'w') as f:\n        for row in campaign_data:\n            f.write(','.join(str(cell) for cell in row) + '\\n')\n\n# Function to record conversion events\ndef record_conversion_events(campaign_data):\n    # Implement conversion event recording logic here\n    print(\"Recording conversion events...\")\n    # Example: Update the campaign status in Campaign Manager 360\n    for row in campaign_data:\n        update_query = f\"\"\"\n            UPDATE `project.dataset.campaigns`\n            SET status = 'converted'\n            WHERE id = {row['id']}\n        \"\"\"\n        hook.run(query=update_query, use_legacy_sql=False)\n\n# Task to retrieve campaign data\nget_campaign_data_task = PythonOperator(\n    task_id='get_campaign_data',\n    python_callable=get_campaign_data,\n    dag=dag\n)\n\n# Task to generate reports\ngenerate_reports_task = PythonOperator(\n    task_id='generate_reports',\n    python_callable=generate_reports,\n    dag=dag,\n    trigger_rule='all_success'\n)\n\n# Task to record conversion events\nrecord_conversion_events_task = PythonOperator(\n    task_id='record_conversion_events',\n    python_callable=record_conversion_events,\n    dag=dag,\n    trigger_rule='all_success'\n)\n\n# Set dependencies between tasks\nget_campaign_data_task >> generate_reports_task >> record_conversion_events_task"}], "metadata": {"file_name": "dag_1733_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981276.3512309}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates basic Pig script execution in a local Hadoop environment. Create a workflow that lists directory contents as a simple example of system interaction.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport subprocess\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'pig_script_dag',\n    default_args=default_args,\n    description='A simple DAG to execute a Pig script and list directory contents',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the task to execute the Pig script\nexecute_pig_script = PythonOperator(\n    task_id='execute_pig_script',\n    python_callable=execute_pig_script_function,\n    dag=dag,\n)\n\n# Define the task to list directory contents\nlist_directory_contents = PythonOperator(\n    task_id='list_directory_contents',\n    python_callable=list_directory_contents_function,\n    dag=dag,\n)\n\n# Set up the dependency between tasks\nexecute_pig_script.set_upstream(list_directory_contents)\n\ndef execute_pig_script_function():\n    # Execute the Pig script\n    subprocess.run(['pig', '-f', '/path/to/your/pig/script.pig'])\n\ndef list_directory_contents_function():\n    # List directory contents\n    import os\n    print(os.listdir('/path/to/directory'))"}], "metadata": {"file_name": "dag_110_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981276.3512385}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a multi-stage system monitoring and logging workflow, with tasks that print context information, log SQL queries, generate sleep-based parallel tasks, and execute environment-specific Python functions. Develop a DAG that demonstrates task chaining, dependency management, and runtime flexibility.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'multi_stage_system_monitoring',\n    default_args=default_args,\n    description='A multi-stage system monitoring and logging workflow',\n    schedule_interval=None  # Set this to your desired schedule interval\n)\n\n# Task to print context information\nprint_context_task = BashOperator(\n    task_id='print_context_info',\n    bash_command='echo \"This is the context information\"',\n    dag=dag\n)\n\n# Task to log SQL queries\nlog_sql_query_task = BashOperator(\n    task_id='log_sql_query',\n    bash_command='echo \"Logging SQL query here\"',\n    dag=dag\n)\n\n# Task to generate sleep-based parallel tasks\nsleep_based_tasks = []\nfor i in range(3):\n    sleep_based_tasks.append(BashOperator(\n        task_id=f'sleep_{i}',\n        bash_command=f'echo \"Sleeping for 5 seconds...\"; sleep 5',\n        dag=dag\n    ))\n\n# Task to execute environment-specific Python functions\nexecute_environment_specific_function_task = PythonOperator(\n    task_id='execute_environment_specific_function',\n    python_callable=execute_environment_specific_function,\n    dag=dag\n)\n\n# Task chaining\nprint_context_task >> log_sql_query_task >> sleep_based_tasks >> execute_environment_specific_function_task\n\ndef execute_environment_specific_function():\n    # Example function to execute environment-specific Python code\n    if os.getenv('ENVIRONMENT') == 'production':\n        print(\"Executing production-specific code\")\n    elif os.getenv('ENVIRONMENT') == 'development':\n        print(\"Executing development-specific code\")\n\n# Define the environment variables\nos.environ['ENVIRONMENT'] = 'production'"}], "metadata": {"file_name": "dag_2217_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981276.3512485}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown tasks for Google Calendar event synchronization. Construct a workflow that showcases connection management, data transfer, and infrastructure cleanup using Google Cloud Platform operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.calendar import CalendarHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'google_calendar_sync_dag',\n    default_args=default_args,\n    description='A DAG to synchronize Google Calendar events with Google BigQuery',\n    schedule_interval=timedelta(days=1),\n)\n\n# Setup task\nsetup_task = PythonOperator(\n    task_id='setup_task',\n    python_callable=set_up_google_calendar_and_bigquery,\n    dag=dag,\n)\n\n# Execution task\nexecution_task = PythonOperator(\n    task_id='execute_task',\n    python_callable=execute_google_calendar_event_synchronization,\n    dag=dag,\n)\n\n# Teardown task\nteardown_task = PythonOperator(\n    task_id='teardown_task',\n    python_callable=cleanup_google_calendar_and_bigquery,\n    dag=dag,\n)\n\n# Dependencies\nsetup_task >> execution_task >> teardown_task\n\ndef set_up_google_calendar_and_bigquery():\n    # Set up Google Calendar API\n    calendar_hook = CalendarHook()\n    calendar_hook.authenticate()\n\n    # Set up Google BigQuery API\n    bigquery_hook = BigQueryHook()\n    bigquery_hook.create_dataset('your_project_id.your_dataset_id')\n\ndef execute_google_calendar_event_synchronization():\n    # Fetch events from Google Calendar\n    calendar_hook = CalendarHook()\n    events = calendar_hook.list_events(calendar_id='primary', max_results=10)\n\n    # Insert events into Google BigQuery\n    bigquery_hook.insert_rows('your_project_id.your_dataset_id.events', events)\n\ndef cleanup_google_calendar_and_bigquery():\n    # Delete events from Google Calendar\n    calendar_hook = CalendarHook()\n    calendar_hook.delete_events(calendar_id='primary', event_ids=['event_id1', 'event_id2'])"}], "metadata": {"file_name": "dag_1643_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981276.351258}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete image processing workflow with Google Cloud services. Develop a systematic approach to handling image data through a series of interconnected tasks from bucket creation to image analysis.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.operators.vision import ImageAnnotateOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'image_processing_dag',\n    default_args=default_args,\n    description='A simple image processing workflow using Google Cloud services',\n    schedule_interval=None,\n)\n\n# Task to create a GCS bucket\ncreate_bucket_task = GCSToGCSOperator(\n    task_id='create_bucket',\n    source_bucket_name='my-image-bucket',\n    destination_bucket_name='my-image-bucket',\n    gcp_conn_id='google_cloud_default',\n    dag=dag\n)\n\n# Task to upload images to the GCS bucket\nupload_images_task = GCSToGCSOperator(\n    task_id='upload_images',\n    source_path='/path/to/images',  # Replace with your local path to images\n    destination_bucket_name='my-image-bucket',\n    gcp_conn_id='google_cloud_default',\n    dag=dag\n)\n\n# Tasks to analyze each image using Google Vision API\nanalyze_image_task = ImageAnnotateOperator(\n    task_id='analyze_image_1',\n    image_uri=f'gs://{create_bucket_task.output}/image1.jpg',\n    gcp_conn_id='google_cloud_default',\n    output_format='json',\n    dag=dag\n)\n\nanalyze_image_task2 = ImageAnnotateOperator(\n    task_id='analyze_image_2',\n    image_uri=f'gs://{create_bucket_task.output}/image2.jpg',\n    gcp_conn_id='google_cloud_default',\n    output_format='json',\n    dag=dag\n)\n\n# Define dependencies between tasks\ncreate_bucket_task >> upload_images_task >> analyze_image_task\ncreate_bucket_task >> upload_images_task >> analyze_image_task2"}], "metadata": {"file_name": "dag_1328_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981276.3512647}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that extracts CSV files from a specified Google Cloud Storage bucket and loads them into a predefined Trino table, supporting efficient and reproducible data ingestion processes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalOperator\nfrom airflow.providers.trino.hooks.trino import TrinoHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'extract_load_trino_dag',\n    default_args=default_args,\n    description='Extracts CSV files from GCS and loads into Trino',\n    schedule_interval='@daily',  # Adjust as needed\n)\n\n# Define the source GCS bucket and destination local directory\nsource_bucket = 'your-source-bucket'\ndestination_dir = '/path/to/local/directory'\n\n# Define the Trino connection details\ntrino_conn_id = 'trino_default'\ntrino_table = 'your_trino_table'\n\n# Extract CSV files from GCS\nextract_csv_task = GCSToLocalOperator(\n    task_id='extract_csv',\n    source_bucket=source_bucket,\n    dest_file_path=f'{destination_dir}/%Y-%m-%d.csv',\n    dag=dag,\n)\n\n# Load CSV files into Trino\nload_csv_task = TrinoHook(trino_conn_id).run_query(f\"\"\"\n    LOAD DATA INPATH '{destination_dir}/*.csv' INTO TABLE {trino_table}\n\"\"\")\n\n# Set up dependencies\nextract_csv_task >> load_csv_task"}], "metadata": {"file_name": "dag_1415_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379105}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a diagnostic DAG that performs systematic database introspection by executing sequential SQL queries to discover and catalog data resources in an Alibaba AnalyticDB Spark cluster. Develop a workflow that provides visibility into the available data schemas and table structures.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'alibaba_analyticdb_spark_introspection',\n    default_args=default_args,\n    description='DAG for performing systematic database introspection in Alibaba AnalyticDB Spark cluster',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the SQL queries to be executed\nsql_queries = [\n    \"SELECT * FROM information_schema.tables WHERE table_schema = 'public'\",\n    \"SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = 'public'\"\n]\n\n# Function to execute SQL query and return results\ndef execute_sql_query(query):\n    spark = SparkSubmitOperator(\n        task_id=f\"execute_sql_{query}\",\n        application=\"spark-submit\",\n        conf={\"spark.master\": \"local[*]\"},\n        py_files=[\"/path/to/spark-submit.py\"],  # Replace with actual path to spark-submit.py\n        args=[f\"--class org.apache.spark.sql.SparkSession --conf spark.sql.warehouse.dir=/tmp --executor-memory=4g --num-executors=2 {query}\"],\n        dag=dag\n    )\n    return spark\n\n# Execute each SQL query and collect results\nfor query in sql_queries:\n    execute_sql_query(query)"}], "metadata": {"file_name": "dag_1068_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.737924}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function to plot a normal distribution using matplotlib? Certainly! Below is a Python code snippet that generates a plot of a normal distribution using the `matplotlib` and `numpy` libraries. This ensures compatibility and covers the necessary aspects of plotting such a distribution, including setting labels and a title for better understanding of the plot."}, {"role": "assistant", "content": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mean=0, std_dev=1, num_samples=1000):\n    \"\"\"\n    Generates a plot of a normal distribution with specified mean and standard deviation.\n    \n    Parameters:\n    - mean: The mean (average) value of the normal distribution.\n    - std_dev: The standard deviation of the normal distribution.\n    - num_samples: The number of samples to generate for the distribution.\n    \n    Returns:\n    None, but displays a plot of the normal distribution.\n    \"\"\"\n    # Generate random data points from the normal distribution\n    x = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n    \n    # Plotting the normal distribution\n    plt.figure(figsize=(8, 6))\n    plt.hist(x, bins=30, density=True, alpha=0.6, color='g', label='Normal Distribution')\n    plt.plot(x, np.exp(-0.5 * ((x - mean) / std_dev) ** 2) / (std_dev * np.sqrt(2 * np.pi)), 'r-', lw=2, label='Probability Density Function')\n    plt.title('Normal Distribution Plot')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Example usage\nplot_normal_distribution(mean=0, std_dev=1, num_samples=1000)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981407.7379339}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates text embedding using the Cohere AI service, showing how to convert text into vector representations. Create a workflow that can process multiple text inputs and generate their corresponding embeddings.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.cohere.hooks.cohere import CohereHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'text_embedding_dag',\n    default_args=default_args,\n    description='DAG to demonstrate text embedding using Cohere AI',\n    schedule_interval=None,\n)\n\ndef embed_text(text):\n    hook = CohereHook(cohere_api_key=\"YOUR_COHERE_API_KEY\")\n    response = hook.embed([text])\n    return response[0]['embedding']\n\nembed_text_task = PythonOperator(\n    task_id='embed_text',\n    python_callable=embed_text,\n    op_kwargs={'text': '{{ ti.xcom_pull(task_ids=\"input_texts\", key=\"texts\") }}'},\n    dag=dag,\n)\n\n# Example input texts\ninput_texts = [\n    \"Hello, world!\",\n    \"This is a test.\",\n    \"Cohere is amazing!\"\n]\n\ninput_texts_task = PythonOperator(\n    task_id='input_texts',\n    python_callable=lambda: input_texts,\n    dag=dag,\n)\n\n# Output results\noutput_results_task = PythonOperator(\n    task_id='output_results',\n    python_callable=lambda: [result['embedding'] for result in embed_text_task.output],\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1959_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379427}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data synchronization workflow that demonstrates file transfer and bucket management techniques. Create a DAG that showcases different synchronization strategies between source and destination buckets.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'gcp_storage_sync_dag',\n    default_args=default_args,\n    description='A simple Google Cloud Storage sync DAG',\n    schedule_interval=None,\n)\n\n# Create source and destination buckets\ncreate_source_bucket = GCSToGCSOperator(\n    task_id='create_source_bucket',\n    source_bucket_name='source-bucket',\n    destination_bucket_name='destination-bucket',\n    gcp_conn_id='google_cloud_default',\n    dag=dag,\n)\n\ncreate_destination_bucket = GCSToGCSOperator(\n    task_id='create_destination_bucket',\n    source_bucket_name='destination-bucket',\n    destination_bucket_name='source-bucket',\n    gcp_conn_id='google_cloud_default',\n    dag=dag,\n)\n\n# Upload files to source bucket\nupload_files_to_source_bucket = LocalToGCSOperator(\n    task_id='upload_files_to_source_bucket',\n    local_file_path='/path/to/local/files/*',\n    destination_bucket_name='source-bucket',\n    object_name_prefix='uploaded/',\n    gcp_conn_id='google_cloud_default',\n    dag=dag,\n)\n\n# Download files from destination bucket\ndownload_files_from_destination_bucket = GCSToLocalOperator(\n    task_id='download_files_from_destination_bucket',\n    source_bucket_name='destination-bucket',\n    object_name_prefix='uploaded/',\n    local_file_path='/path/to/local/downloaded/',\n    gcp_conn_id='google_cloud_default',\n    dag=dag,\n)\n\n# Delete source bucket after download\ndelete_source_bucket = GCSToGCSOperator(\n    task_id='delete_source_bucket',\n    source_bucket_name='source-bucket',\n    destination_bucket_name='source-bucket',\n    gcp_conn_id='google_cloud_default',\n    delete_objects=True,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_211_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379522}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates data extract refresh processes for a business intelligence workbook, showing how to trigger and monitor data updates across different execution modes. Create a workflow that simulates real-world data refresh scenarios with explicit job status tracking and different refresh strategies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import datetime\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_refresh_dag',\n    description='A DAG to demonstrate data extraction, transformation, and refresh processes.',\n    schedule_interval=None,  # Set to None if you want to run manually\n    default_args=default_args\n)\n\n# Task to extract data from a database\ndef extract_data_from_db():\n    print(\"Extracting data from database\")\n\nextract_db_task = PythonOperator(\n    task_id='extract_data_from_db',\n    python_callable=extract_data_from_db,\n    dag=dag\n)\n\n# Task to transform data\ndef transform_data():\n    print(\"Transforming data\")\n\ntransform_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\n# Task to load data into a target system\ndef load_data_into_target():\n    print(\"Loading data into target system\")\n\nload_data_into_target_task = PythonOperator(\n    task_id='load_data_into_target',\n    python_callable=load_data_into_target,\n    dag=dag\n)\n\n# Trigger the tasks in sequence\nextract_db_task >> transform_data_task >> load_data_into_target_task\n\n# Define a function to simulate job status tracking\ndef track_job_status(job_id):\n    print(f\"Job {job_id} has been updated.\")\n\n# Task to monitor job status\ndef monitor_job_status():\n    job_ids = [extract_db_task.task_id, transform_data_task.task_id, load_data_into_target_task.task_id]\n    for job_id in job_ids:\n        track_job_status(job_id)\n\nmonitor_job_status_task = PythonOperator(\n    task_id='monitor_job_status',\n    python_callable=monitor_job_status,\n    dag=dag\n)\n\n# Trigger the job status monitoring task\nmonitor_job_status_task >> extract_db_task >> transform_data_task >> load_data_into_target_task"}], "metadata": {"file_name": "dag_1005_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981407.737963}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases conditional task execution based on the most recent scheduling window. Implement a DAG that uses LatestOnlyOperator to control task execution and prevent redundant processing of older scheduled tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.latest_only_operator import LatestOnlyOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'latest_only_example',\n    default_args=default_args,\n    description='A DAG that uses LatestOnlyOperator to control task execution',\n    schedule_interval=timedelta(hours=1),  # Schedule every hour\n)\n\n# Task 1: Check if it's the latest scheduled task\ncheck_latest_task = LatestOnlyOperator(\n    task_id='check_latest_task',\n    dag=dag,\n    trigger_rule='all_done'\n)\n\n# Task 2: Execute the task if it's the latest scheduled task\nexecute_task = BashOperator(\n    task_id='execute_task',\n    bash_command='echo \"Executing the task\"',\n    dag=dag\n)\n\n# Set up the dependencies\ncheck_latest_task >> execute_task"}], "metadata": {"file_name": "dag_1040_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379763}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates cross-cloud file transfer capabilities, showcasing how to programmatically move data between Azure and Google Cloud storage systems. Implement a workflow that creates a destination bucket, transfers files, and then cleanly removes the temporary storage infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.microsoft.azure.transfers.blob_to_blob import BlobToBlobOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='cross_cloud_file_transfer',\n    description='DAG for transferring files between Azure and Google Cloud',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n) as dag:\n\n    # Step 1: Create a destination bucket in Google Cloud\n    create_bucket_task = GCSToGCSOperator(\n        task_id='create_bucket',\n        source_bucket_name='source-bucket-name',  # Replace with your source bucket name\n        destination_bucket_name='destination-bucket-name',  # Replace with your destination bucket name\n        project_id='your-project-id',  # Replace with your GCP project ID\n        impersonation_chain='serviceAccountEmail'  # Replace with your service account email if needed\n    )\n\n    # Step 2: Upload files to the destination bucket in Google Cloud\n    upload_files_task = GCSToGCSOperator(\n        task_id='upload_files',\n        source_objects=['path/to/source/file.txt'],  # Replace with your source file paths\n        destination_bucket_name='destination-bucket-name',  # Replace with your destination bucket name\n        project_id='your-project-id',  # Replace with your GCP project ID\n        impersonation_chain='serviceAccountEmail'  # Replace with your service account email if needed\n    )\n\n    # Step 3: Clean up by deleting the destination bucket in Google Cloud\n    delete_bucket_task = GCSToGCSOperator(\n        task_id='delete_bucket',\n        source_bucket_name='destination-bucket-name',  # Replace with your destination bucket name\n        destination_bucket_name=None,  # This will delete the bucket\n        project_id='your-project-id',  # Replace with your GCP project ID\n        impersonation_chain='serviceAccountEmail'  # Replace with your service account email if needed\n    )\n\n    # Set dependencies\n    create_bucket_task >> upload_files_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_1568_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379875}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='Automated data pipeline from S3 to AWS QuickSight',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to download data from S3\ndownload_data_task = PythonOperator(\n    task_id='download_data',\n    python_callable=download_data,\n    dag=dag\n)\n\n# Task to upload data to QuickSight\nupload_data_task = PythonOperator(\n    task_id='upload_data',\n    python_callable=upload_data,\n    dag=dag\n)\n\n# Dummy operator to mark the end of the pipeline\nend_task = DummyOperator(\n    task_id='end_task',\n    dag=dag\n)\n\n# Define dependencies\ndownload_data_task >> upload_data_task >> end_task\n\ndef download_data():\n    # Initialize S3 hook\n    s3_hook = S3Hook()\n    \n    # Specify the bucket name and key\n    bucket_name = 'your-s3-bucket-name'\n    key = 'path/to/your/sample/project/data.csv'\n    \n    # Download the file from S3\n    local_file_path = '/tmp/sample_project_data.csv'\n    s3_hook.download_file(bucket_name, key, local_file_path)\n\ndef upload_data():\n    # Initialize QuickSight hook\n    qs_hook = QuickSightHook()\n    \n    # Specify the QuickSight project ID and dataset ID\n    project_id = 'your-quicksight-project-id'\n    dataset_id = 'your-quicksight-dataset-id'\n    \n    # Upload the file to QuickSight\n    qs_hook.upload_csv_to_quick_sight(project_id, dataset_id, '/tmp/sample_project_data.csv')\n\n# Run the DAG\nif __name__ == \"__main__\":\n    dag.cli()"}], "metadata": {"file_name": "dag_404_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981407.7379985}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated reporting workflow for Google Campaign Manager that creates, runs, and processes marketing performance reports. Design a data pipeline that extracts campaign metrics, transforms the data, and prepares it for business intelligence analysis.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'google_campaign_manager_reporting',\n    default_args=default_args,\n    description='Automated Reporting Workflow for Google Campaign Manager',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Extract Campaign Metrics\ndef extract_campaign_metrics():\n    hook = BigQueryHook(project_id='your-project-id')\n    query = \"\"\"\n        SELECT \n            campaign_name,\n            date,\n            impressions,\n            clicks,\n            conversions\n        FROM \n            `your-project-id.your-dataset.campaign_performance`\n        WHERE \n            date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY);\n    \"\"\"\n    df = hook.get_pandas_df(query=query)\n    return df\n\nextract_campaign_metrics_task = PythonOperator(\n    task_id='extract_campaign_metrics',\n    python_callable=extract_campaign_metrics,\n    dag=dag,\n)\n\n# Step 2: Transform Data\ndef transform_data(df):\n    # Add any transformations here\n    return df\n\ntransformed_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag,\n    provide_context=True,\n)\n\n# Step 3: Store Report Files\ndef store_report_files(df):\n    bucket_name = 'your-bucket-name'\n    blob_name = f'campaign_performance_{datetime.now().strftime(\"%Y%m%d\")}.csv'\n    \n    storage_hook = GoogleCloudStorageHook()\n    storage_hook.upload(bucket=bucket_name, object=blob_name, filename=df.to_csv(index=False))\n\nstore_report_files_task = PythonOperator(\n    task_id='store_report_files',\n    python_callable=store_report_files,\n    dag=dag,\n    provide_context=True,\n)\n\n# Step 4: Process Reports\ndef process_reports():\n    # Add any processing logic here\n    pass\n\nprocess_reports_task = PythonOperator(\n    task_id='process_reports',\n    python_callable=process_reports,\n    dag=dag,\n)\n\n# Task Dependencies\nextract_campaign_metrics_task >> transformed_data_task >> store_report_files_task >> process_reports_task"}], "metadata": {"file_name": "dag_276_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380104}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates the technical process of starting and checking Looker PDT builds using specialized Airflow operators for cloud-based data transformation. The DAG should demonstrate task dependencies and asynchronous job monitoring in a data engineering pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.looker.hooks.looker import LookerHook\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'looker_pdt_build_dag',\n    default_args=default_args,\n    description='DAG to start and check Looker PDT builds',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Hook to interact with Looker\nlooker_hook = LookerHook()\n\n# Task to start a PDT build\nstart_pdt_build_task = SparkSubmitOperator(\n    task_id='start_pdt_build',\n    application='/path/to/your/spark_script.py',  # Path to your Spark script\n    name='Start PDT Build',\n    conf={'looker_api_key': looker_hook.get_api_token()},\n    dag=dag,\n)\n\n# Task to wait for the PDT build to finish\nwait_for_pdt_build_task = SparkSubmitOperator(\n    task_id='wait_for_pdt_build',\n    application='/path/to/your/spark_script.py',  # Path to your Spark script\n    name='Wait for PDT Build',\n    conf={'looker_api_key': looker_hook.get_api_token(), 'pdt_job_id': '{{ task_instance.xcom_pull(task_ids=\"start_pdt_build\", key=\"return_value\") }}'},\n    dag=dag,\n)\n\n# Task to check the status of the PDT build\ncheck_pdt_build_status_task = SparkSubmitOperator(\n    task_id='check_pdt_build_status',\n    application='/path/to/your/spark_script.py',  # Path to your Spark script\n    name='Check PDT Build Status',\n    conf={'looker_api_key': looker_hook.get_api_token(), 'pdt_job_id': '{{ task_instance.xcom_pull(task_ids=\"wait_for_pdt_build\", key=\"return_value\") }}'},\n    dag=dag,\n)\n\n# Task dependency\nstart_pdt_build_task >> wait_for_pdt_build_task >> check_pdt_build_status_task"}], "metadata": {"file_name": "dag_703_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380195}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the complete lifecycle of Azure Service Bus messaging operations, from queue and topic creation through message transmission and subscription management. Develop a step-by-step demonstration of enterprise messaging infrastructure using cloud-native service bus capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.service_bus import ServiceBusHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'azure_servicebus_dag',\n    default_args=default_args,\n    description='DAG for Azure Service Bus messaging operations',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Function to create a Service Bus Queue\ndef create_queue(queue_name):\n    hook = ServiceBusHook()\n    hook.create_queue(queue_name)\n    print(f\"Queue {queue_name} created successfully.\")\n\n# Function to create a Service Bus Topic\ndef create_topic(topic_name):\n    hook = ServiceBusHook()\n    hook.create_topic(topic_name)\n    print(f\"Topic {topic_name} created successfully.\")\n\n# Function to send a message to a Service Bus Queue\ndef send_message_to_queue(queue_name, message):\n    hook = ServiceBusHook()\n    hook.send_message(queue_name, message)\n    print(f\"Message sent to queue {queue_name}: {message}\")\n\n# Function to subscribe to a Service Bus Topic\ndef subscribe_to_topic(topic_name, subscription_name):\n    hook = ServiceBusHook()\n    hook.subscribe_to_topic(topic_name, subscription_name)\n    print(f\"Subscription {subscription_name} created successfully.\")\n\n# Task to create a Service Bus Queue\ncreate_queue_task = PythonOperator(\n    task_id='create_queue_task',\n    python_callable=create_queue,\n    op_kwargs={'queue_name': 'my_queue'},\n    dag=dag,\n)\n\n# Task to create a Service Bus Topic\ncreate_topic_task = PythonOperator(\n    task_id='create_topic_task',\n    python_callable=create_topic,\n    op_kwargs={'topic_name': 'my_topic'},\n    dag=dag,\n)\n\n# Task to send a message to a Service Bus Queue\nsend_message_task = PythonOperator(\n    task_id='send_message_task',\n    python_callable=send_message_to_queue,\n    op_kwargs={'queue_name': 'my_queue', 'message': 'Hello, Azure Service Bus!'},\n    dag=dag,\n)\n\n# Task to subscribe to a Service Bus Topic\nsubscribe_to_topic_task = PythonOperator(\n    task_id='subscribe_to_topic_task',\n    python_callable=subscribe_to_topic,\n    op_kwargs={'topic_name': 'my_topic', 'subscription_name': 'my_subscription'},\n    dag=dag,\n)\n\n# Dependencies\ncreate_queue_task >> create_topic_task >> send_message_task >> subscribe_to_topic_task"}], "metadata": {"file_name": "dag_2157_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380314}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that exports a specific Firestore collection to Google Cloud Storage and then creates an external BigQuery table from the exported data. Build an automated pipeline that demonstrates cross-project data transfer and transformation between Firebase, GCS, and BigQuery.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.firestore_to_gcs import FirestoreToGCSOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'export_firestore_to_gcs',\n    default_args=default_args,\n    description='Export Firestore collection to GCS',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the Firestore connection\nfirestore_conn_id = 'google_cloud_default'\n\n# Define the GCS bucket\ngcs_bucket_name = 'your-gcs-bucket-name'\ngcs_object_name = 'path/to/exported/data.csv'\n\n# Define the Firestore query\nquery = \"SELECT * FROM your_collection\"\n\n# Task to export Firestore collection to GCS\nexport_task = FirestoreToGCSOperator(\n    task_id='export_firestore_to_gcs',\n    firestore_conn_id=firestore_conn_id,\n    query=query,\n    gcs_bucket=gcs_bucket_name,\n    gcs_object=gcs_object_name,\n)"}], "metadata": {"file_name": "dag_207_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380426}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a vector data ingestion workflow that populates a Pinecone vector database with predefined embedding vectors. Create a DAG that allows flexible configuration of vector data insertion with support for metadata and custom namespaces.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.pinecone.hooks.pinecone import PineconeHook\nfrom airflow.operators.python_operator import PythonOperator\nimport pandas as pd\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'vector_ingestion_dag',\n    default_args=default_args,\n    description='Ingestion of vector data into Pinecone',\n    schedule_interval=timedelta(hours=1),  # Adjust this interval as needed\n)\n\n# Function to load data from a CSV file\ndef load_data():\n    df = pd.read_csv('path_to_your_vector_data.csv')\n    return df\n\n# Function to insert data into Pinecone\ndef insert_into_pinecone(df):\n    pinecone_hook = PineconeHook()\n    index_name = 'your_index_name'\n    namespace = 'your_namespace'\n\n    for row in df.itertuples(index=False):\n        embedding = row.embedding\n        metadata = {'id': row.id, 'label': row.label}\n        pinecone_hook.upsert(pinecone.Index(index_name), [embedding], [{'metadata': metadata}])\n\n# Task to load data\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag\n)\n\n# Task to insert data into Pinecone\ninsert_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_into_pinecone,\n    dag=dag,\n    trigger_rule='all_success',  # Ensure all tasks succeed before inserting\n)\n\n# Task to notify success\nnotify_success = PythonOperator(\n    task_id='notify_success',\n    python_callable=lambda: print(\"Vector data ingestion completed successfully\"),\n    dag=dag\n)\n\n# Dependencies\nload_task >> insert_task >> notify_success"}], "metadata": {"file_name": "dag_1783_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380533}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Azure Blob Storage monitoring system using Airflow sensors that checks for specific file conditions before initiating downstream data processing tasks. Implement a robust file detection mechanism that allows workflow progression based on precise blob or prefix matching.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.sensors.blob import BlobSensor\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'azure_blob_storage_monitoring',\n    default_args=default_args,\n    description='A DAG to monitor Azure Blob Storage for new files and process them.',\n    schedule_interval='@daily',  # Adjust this interval as needed\n)\n\n# Sensor to check for new files in the specified container\ncheck_new_files = BlobSensor(\n    task_id='check_new_files',\n    azure_container_name='your-container-name',\n    azure_blob_name_pattern='your-file-pattern*',  # Example pattern: '*.txt'\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# Task to process the detected files\nprocess_file = PythonOperator(\n    task_id='process_file',\n    python_callable=process_file_function,\n    op_kwargs={'file_path': '{{ ti.xcom_pull(task_ids=\"check_new_files\", key=\"file_path\") }}'},\n    dag=dag,\n)\n\n# Dependencies between tasks\ncheck_new_files >> process_file"}], "metadata": {"file_name": "dag_975_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380633}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What are the steps to create a `reduce` function in Python that multiplies all elements in a list?"}, {"role": "assistant", "content": "import functools\n\ndef multiply_elements(lst):\n    # Use functools.reduce() with a lambda function that multiplies two numbers\n    return functools.reduce(lambda x, y: x * y, lst)\n\n# Example usage:\nnumbers = [1, 2, 3, 4, 5]\nresult = multiply_elements(numbers)\nprint(result)  # Output: 120"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981407.7380695}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that captures and exports DynamoDB table data to S3 with precise time-based incremental export capabilities. Create a system that enables point-in-time data recovery and demonstrates advanced cloud data transfer techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.dynamodb import DynamoDBHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'dynamodb_to_s3',\n    default_args=default_args,\n    description='A data migration pipeline from DynamoDB to S3',\n    schedule_interval=timedelta(hours=1),  # Adjust as needed\n)\n\n# Define variables for DynamoDB and S3\ndynamodb_table_name = 'your_dynamodb_table'\ns3_bucket_name = 'your_s3_bucket'\ns3_prefix = 'data_exports/'\ns3_export_path = f'{s3_prefix}{datetime.now().strftime(\"%Y%m%d%H%M%S\")}/'\n\n# Hook instances\ndynamodb_hook = DynamoDBHook()\ns3_hook = S3Hook()\n\ndef export_data_to_s3():\n    # Get all items from DynamoDB\n    response = dynamodb_hook.get_paginator('scan').paginate(TableName=dynamodb_table_name)\n    \n    # Export each item to S3\n    for page in response['Items']:\n        s3_hook.load_string(json.dumps(page), f's3://{s3_bucket_name}/{s3_export_path}', replace=True)\n\nexport_task = PythonOperator(\n    task_id='export_data_to_s3',\n    python_callable=export_data_to_s3,\n    dag=dag,\n    trigger_rule=TriggerRule.ALL_SUCCESSFUL\n)\n\ndef restore_data_from_s3():\n    # List all objects in the specified S3 prefix\n    response = s3_hook.list_objects_v2(Bucket=s3_bucket_name, Prefix=s3_export_path)\n    \n    # Restore each object from S3\n    for obj in response.get('Contents', []):\n        s3_hook.download_fileobj(obj['Key'], f'/tmp/{obj[\"Key\"]}')\n        \n        # Load the restored data into DynamoDB\n        dynamodb_hook.put_item(TableName=dynamodb_table_name, Item=json.loads(open(f'/tmp/{obj[\"Key\"]}').read()))\n\nrestore_task = PythonOperator(\n    task_id='restore_data_from_s3',\n    python_callable=restore_data_from_s3,\n    dag=dag,\n    trigger_rule=TriggerRule.ALL_SUCCESSFUL\n)\n\n# Define dependencies\nexport_task >> restore_task"}], "metadata": {"file_name": "dag_1886_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380812}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated pipeline for managing sensitive data detection configurations that demonstrates how to programmatically handle data privacy templates across different stages of processing. The workflow will show how to create, inspect, and clean up data protection configurations with clear, sequential steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_protection_config_management',\n    default_args=default_args,\n    description='Automated pipeline for managing sensitive data detection configurations',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to create a new data protection configuration template\ncreate_template_task = PythonOperator(\n    task_id='create_data_protection_template',\n    python_callable=create_data_protection_template,\n    dag=dag,\n    op_kwargs={\n        'template_name': 'example_template.json'\n    }\n)\n\n# Task to inspect an existing data protection configuration template\ninspect_template_task = PythonOperator(\n    task_id='inspect_data_protection_template',\n    python_callable=inspect_data_protection_template,\n    dag=dag,\n    op_kwargs={\n        'template_name': 'example_template.json'\n    }\n)\n\n# Task to clean up a data protection configuration template\ncleanup_template_task = PythonOperator(\n    task_id='cleanup_data_protection_template',\n    python_callable=cleanup_data_protection_template,\n    dag=dag,\n    op_kwargs={\n        'template_name': 'example_template.json'\n    }\n)\n\n# Define functions to simulate data protection operations\ndef create_data_protection_template(template_name):\n    print(f\"Creating data protection template: {template_name}\")\n\ndef inspect_data_protection_template(template_name):\n    print(f\"Inspecting data protection template: {template_name}\")\n\ndef cleanup_data_protection_template(template_name):\n    print(f\"Cleaning up data protection template: {template_name}\")\n\n# Define dependencies between tasks\ncreate_template_task >> inspect_template_task >> cleanup_template_task"}], "metadata": {"file_name": "dag_1605_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.7380912}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data extraction workflow that performs a targeted graph database query to pull specific node attributes for Tom Hanks on a given date. Configure the pipeline to flexibly retrieve graph data using parameterized Cypher queries within the Airflow orchestration framework.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.neo4j.hooks.neo4j import Neo4jHook\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'neo4j_graph_query_dag',\n    default_args=default_args,\n    description='A DAG to extract specific node attributes from a Neo4j graph database',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the task to execute the Cypher query\ndef execute_cypher_query():\n    hook = Neo4jHook(neo4j_conn_id='neo4j_default')\n    \n    # Specify the date and the node attribute to retrieve\n    date = datetime.now().strftime('%Y-%m-%d')\n    node_attribute = 'name'\n    \n    # Construct the Cypher query\n    cypher_query = f\"MATCH (n) WHERE n.name = '{node_attribute}' AND n.birthdate = '{date}' RETURN n.{node_attribute}\"\n    \n    # Execute the Cypher query and return the result\n    result = hook.run(cypher_query).data()\n    return result\n\n# Register the task as a PythonOperator\nexecute_cypher_task = PythonOperator(\n    task_id='execute_cypher_query',\n    python_callable=execute_cypher_query,\n    dag=dag\n)\n\n# Define the task to log the results\ndef log_results(results):\n    print(f\"Results retrieved: {results}\")\n\n# Register the task as a PythonOperator\nlog_results_task = PythonOperator(\n    task_id='log_results',\n    python_callable=log_results,\n    op_kwargs={'results': execute_cypher_task.output},\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nexecute_cypher_task >> log_results_task"}], "metadata": {"file_name": "dag_1763_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.738101}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data privacy transformation pipeline that uses AWS Comprehend to automatically identify and anonymize sensitive personal information in unstructured text data. Create a workflow that takes raw data, applies intelligent PII masking, and generates a redacted output file.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.comprehend import comprehend_hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'pii_redaction_dag',\n    default_args=default_args,\n    description='A data privacy transformation pipeline using AWS Comprehend',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the function to apply PII masking using AWS Comprehend\ndef apply_pii_masking(text):\n    comprehend_client = comprehend_hook()\n    \n    # Use AWS Comprehend to detect PII entities\n    response = comprehend_client.detect_entities(Text=text, LanguageCode='en')\n    \n    # Mask PII entities\n    masked_text = ''\n    entity_start = 0\n    for entity in response['Entities']:\n        if entity['Type'] == 'PERSON':\n            masked_text += text[entity_start:entity['StartIndex']]\n            masked_text += '*' * len(entity['Text'])\n            entity_start = entity['EndIndex']\n    \n    masked_text += text[entity_start:]\n    \n    return masked_text\n\n# Task to read raw data from S3\nread_raw_data_task = PythonOperator(\n    task_id='read_raw_data',\n    python_callable=lambda: \"Your raw data here\",\n    dag=dag\n)\n\n# Task to apply PII masking\napply_pii_masking_task = PythonOperator(\n    task_id='apply_pii_masking',\n    python_callable=apply_pii_masking,\n    op_kwargs={'text': '{{ ti.xcom_pull(task_ids=\"read_raw_data\") }}'},\n    dag=dag\n)\n\n# Task to write redacted data to S3\nwrite_redacted_data_task = PythonOperator(\n    task_id='write_redacted_data',\n    python_callable=lambda: \"Your redacted data here\",\n    op_kwargs={'text': '{{ ti.xcom_pull(task_ids=\"apply_pii_masking\") }}'},\n    dag=dag\n)\n\n# Dependencies\nread_raw_data_task >> apply_pii_masking_task >> write_redacted_data_task"}], "metadata": {"file_name": "dag_1080_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381115}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases Azure Container Instances deployment patterns, including launching standard Docker images, using custom container registries, and configuring persistent volume attachments. Develop a systematic approach to container deployment that highlights different configuration scenarios.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.aci import AzureContainerInstanceHook\nfrom airflow.providers.microsoft.azure.operators.aci import AzureContainerInstanceCreateOperator\nfrom airflow.providers.microsoft.azure.operators.aci import AzureContainerInstanceDeleteOperator\nfrom airflow.providers.microsoft.azure.sensors.aci import AzureContainerInstanceSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'azure_container_instances_dag',\n    default_args=default_args,\n    description='DAG to deploy Azure Container Instances',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to create a standard Docker image in a custom registry\ncreate_standard_image_task = AzureContainerInstanceCreateOperator(\n    task_id='create_standard_image',\n    aci_conn_id='azure_aci_default',\n    name='standard-image-container',\n    image_name='my-standard-image',\n    image_tag='latest',\n    resource_group_name='my-resource-group',\n    location='eastus',\n    dockerfile_path='/path/to/dockerfile',\n    registry_server='my-custom-registry.azurecr.io',\n    registry_username='my-username',\n    registry_password='my-password'\n)\n\n# Task to delete the created standard Docker image\ndelete_standard_image_task = AzureContainerInstanceDeleteOperator(\n    task_id='delete_standard_image',\n    aci_conn_id='azure_aci_default',\n    name='standard-image-container',\n    resource_group_name='my-resource-group'\n)\n\n# Task to create a persistent volume attachment\ncreate_persistent_volume_attachment_task = AzureContainerInstanceCreateOperator(\n    task_id='create_persistent_volume_attachment',\n    aci_conn_id='azure_aci_default',\n    name='persistent-volume-attachment-container',\n    image_name='my-persistent-volume-attachment-image',\n    image_tag='latest',\n    resource_group_name='my-resource-group',\n    location='eastus',\n    dockerfile_path='/path/to/persistent-volume-attachment-dockerfile',\n    persistent_volume_name='my-persistent-volume',\n    storage_account_name='my-storage-account',\n    storage_account_key='my-storage-account-key'\n)\n\n# Task to delete the created persistent volume attachment\ndelete_persistent_volume_attachment_task = AzureContainerInstanceDeleteOperator(\n    task_id='delete_persistent_volume_attachment',\n    aci_conn_id='azure_aci_default',\n    name='persistent-volume-attachment-container',\n    resource_group_name='my-resource-group'\n)\n\n# Task to monitor the status of the ACI containers\nmonitor_containers_task = AzureContainerInstanceSensor(\n    task_id='monitor_containers',\n    aci_conn_id='azure_aci_default',\n    name='standard-image-container',\n    resource_group_name='my-resource-group',\n    target_state='running'\n)\n\n# Task to monitor the status of the ACI containers\nmonitor_persistent_volume_attachments_task = AzureContainerInstanceSensor(\n    task_id='monitor_persistent_volume_attachments',\n    aci_conn_id='azure_aci_default',\n    name='persistent-volume-attachment-container',\n    resource_group_name='my-resource-group',\n    target_state='running'\n)\n\n# Dependencies between tasks\ncreate_standard_image_task >> delete_standard_image_task\ncreate_persistent_volume_attachment_task >> delete_persistent_volume_attachment_task\nmonitor_containers_task >> monitor_persistent_volume_attachments_task"}], "metadata": {"file_name": "dag_618_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381225}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to find all the numbers in a given list that are divisible by 3 and are even."}, {"role": "assistant", "content": "def find_divisible_by_three_evens(numbers):\n    \"\"\"\n    Finds all numbers in the given list that are divisible by 3 and even.\n    \n    Parameters:\n    numbers (list): A list of integers.\n    \n    Returns:\n    list: A list of integers that are divisible by 3 and even.\n    \"\"\"\n    # Initialize an empty list to store the result\n    result = []\n    \n    # Iterate through each number in the input list\n    for num in numbers:\n        # Check if the number is divisible by 3 and even\n        if num % 3 == 0 and num % 2 == 0:\n            # If true, append it to the result list\n            result.append(num)\n    \n    return result\n\n# Example usage:\ninput_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\noutput_list = find_divisible_by_three_evens(input_list)\nprint(output_list)  # Output: [6, 12]"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981407.73813}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the sequential execution of different Spark operators, showing how complex data workflows can be orchestrated using modular task dependencies. Develop a demonstration pipeline that highlights task chaining and different Spark processing capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.spark.operators.spark_submit_operator import SparkSubmitOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG object\ndag = DAG(\n    'spark_example_dag',\n    description='A simple example of an Airflow DAG with Spark operators',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args,\n)\n\n# Task to read data from a CSV file\nread_csv_task = SparkSubmitOperator(\n    task_id='read_csv',\n    application='/path/to/your/spark_app.py',  # Path to your Spark application script\n    conf={'spark.app.name': 'read_csv'},\n    dag=dag,\n)\n\n# Task to perform some basic transformations (e.g., filtering)\ntransform_data_task = SparkSubmitOperator(\n    task_id='transform_data',\n    application='/path/to/your/spark_app.py',  # Path to your Spark application script\n    conf={'spark.app.name': 'transform_data'},\n    dag=dag,\n    depends_on_past=True,  # This task depends on the previous task\n)\n\n# Task to write the results back to another CSV file\nwrite_csv_task = SparkSubmitOperator(\n    task_id='write_csv',\n    application='/path/to/your/spark_app.py',  # Path to your Spark application script\n    conf={'spark.app.name': 'write_csv'},\n    dag=dag,\n    depends_on_past=True,  # This task depends on the previous task\n)\n\n# Define the task dependencies\nread_csv_task >> transform_data_task >> write_csv_task"}], "metadata": {"file_name": "dag_1147_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381382}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases automated cloud storage interactions, including bucket creation, file transfer between cloud providers, and transfer job management. Develop a demonstration of cloud service integration using Google Cloud Transfer Service and AWS S3 operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3ObjectSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3BucketExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3ObjectSizeSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3PrefixExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3LastModifiedSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3AclSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3NotificationConfigurationSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRuleExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgePermissionExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeRulePolicyExistsSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3EventBridgeTargetPolicy"}], "metadata": {"file_name": "dag_1683_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381551}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical workflow of provisioning and managing cloud infrastructure using Google Cloud Composer operators. Develop a step-by-step process that demonstrates environment configuration, version listing, and operational state management for cloud data processing platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gce import GKEClusterCreateOperator, GKEClusterDeleteOperator\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import KubernetesPodOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryDeleteDatasetOperator\nfrom airflow.providers.google.cloud.operators.dataflow import DataFlowJobSubmitOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_infrastructure_dag',\n    default_args=default_args,\n    description='DAG for provisioning and managing cloud infrastructure',\n    schedule_interval=None,\n)\n\n# Step 1: Create a GKE cluster\ncreate_cluster_operator = GKEClusterCreateOperator(\n    task_id='create_gke_cluster',\n    project_id='your_project_id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    num_nodes=3,\n    machine_type='n1-standard-2',\n    service_account_email='your_service_account_email@example.com',\n    network='projects/your_project_id/global/networks/default',\n    subnetwork='projects/your_project_id/global/subnetworks/default',\n    node_labels={'environment': 'production'},\n    enable_private_nodes=True,\n    min_master_version='1.25.6',\n    max_node_version='1.25.6',\n    oauth_scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\n\n# Step 2: Delete the GKE cluster\ndelete_cluster_operator = GKEClusterDeleteOperator(\n    task_id='delete_gke_cluster',\n    project_id='your_project_id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n)\n\n# Step 3: Submit a Data Flow job\nsubmit_data_flow_job_operator = DataFlowJobSubmitOperator(\n    task_id='submit_data_flow_job',\n    project_id='your_project_id',\n    region='us-central1',\n    template_path='/path/to/dataflow/template.json',\n    parameters={\n        'input_file': 'gs://your_bucket/input.csv',\n        'output_file': 'gs://your_bucket/output.parquet'\n    },\n    labels={'environment': 'production'},\n)\n\n# Step 4: List all datasets in BigQuery\nlist_datasets_operator = BigQueryCreateDatasetOperator(\n    task_id='list_datasets',\n    project_id='your_project_id',\n    dataset_id='my_dataset',\n    schema_fields=[\n        {'name': 'id', 'type': 'INTEGER'},\n        {'name': 'name', 'type': 'STRING'}\n    ],\n)\n\n# Step 5: Delete the dataset from BigQuery\ndelete_dataset_operator = BigQueryDeleteDatasetOperator(\n    task_id='delete_dataset',\n    project_id='your_project_id',\n    dataset_id='my_dataset',\n)\n\n# Define dependencies between tasks\ncreate_cluster_operator >> delete_cluster_operator >> submit_data_flow_job_operator >> list_datasets_operator >> delete_dataset_operator"}], "metadata": {"file_name": "dag_494_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381656}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates automated Google Compute Engine instance lifecycle management, including instance creation, remote command execution via SSH, and instance deletion. The DAG should showcase how to programmatically manage cloud compute resources using OS Login authentication.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    GCPComputeInstanceCreateOperator,\n    GCPComputeSSHCommandOperator,\n    GCPComputeInstanceDeleteOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    'google_compute_lifecycle_management',\n    default_args=default_args,\n    description='Automate Google Compute Engine instance lifecycle management',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Task to create a new Google Compute Engine instance\ncreate_instance_task = GCPComputeInstanceCreateOperator(\n    task_id='create_instance',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    machine_type='n1-standard-1',\n    network_interface={\n        'network': 'default'\n    },\n    tags=['example-tag'],\n    metadata={'description': 'Example instance'},\n    service_account_email='your-service-account-email@your-project-id.iam.gserviceaccount.com',\n    enable_os_login=True,\n    dag=dag,\n)\n\n# Task to execute a remote command via SSH\nexecute_ssh_command_task = GCPComputeSSHCommandOperator(\n    task_id='execute_ssh_command',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    instance_name='your-instance-name',\n    username='your-username',\n    command='echo \"Hello from Airflow!\"',\n    dag=dag,\n)\n\n# Task to delete the Google Compute Engine instance\ndelete_instance_task = GCPComputeInstanceDeleteOperator(\n    task_id='delete_instance',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    instance_name='your-instance-name',\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Define dependencies between tasks\ncreate_instance_task >> execute_ssh_command_task >> delete_instance_task"}], "metadata": {"file_name": "dag_838_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381744}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to trigger a computational task on Azure Batch, showcasing cloud-based distributed computing for executing a simple shell command.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.batch import AzureBatchHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'azure_batch_dag',\n    default_args=default_args,\n    description='DAG to trigger a computational task on Azure Batch',\n    schedule_interval=None,  # Set to None if you want to run it manually\n)\n\n# Function to execute the shell command\ndef execute_shell_command():\n    hook = AzureBatchHook(batch_client)\n    job_id = hook.submit_job(\n        pool_id=\"your_pool_id\",\n        command_line=\"echo Hello from Azure Batch!\",\n        container_image=\"ubuntu:latest\"\n    )\n    print(f\"Job submitted with ID: {job_id}\")\n\n# Task to execute the shell command\nexecute_task = PythonOperator(\n    task_id='execute_shell_command',\n    python_callable=execute_shell_command,\n    dag=dag,\n)\n\n# Trigger the task\ntrigger_task = BashOperator(\n    task_id='trigger_task',\n    bash_command='echo \"Triggering Azure Batch job\"',\n    dag=dag,\n)\n\n# Set up the dependencies\nexecute_task >> trigger_task"}], "metadata": {"file_name": "dag_2153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981407.738182}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of dynamically generating a local file and transferring it to AWS S3 using predefined operators and task chaining. The DAG should highlight task dependencies, error handling, and cloud storage interaction.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom datetime import datetime, timedelta\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'dynamic_s3_upload_dag',\n    default_args=default_args,\n    description='A DAG to upload a local file to AWS S3',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to create a local file\ncreate_local_file_task = PythonOperator(\n    task_id='create_local_file',\n    python_callable=create_local_file,\n    dag=dag\n)\n\n# Function to create a local file\ndef create_local_file():\n    with open('local_file.txt', 'w') as f:\n        f.write(\"This is a test file created by Airflow.\")\n\n# Task to upload the local file to S3\nupload_to_s3_task = PythonOperator(\n    task_id='upload_to_s3',\n    python_callable=upload_to_s3,\n    op_kwargs={'file_path': 'local_file.txt'},\n    dag=dag\n)\n\n# Error handling for the upload task\nupload_to_s3_task.set_upstream(create_local_file_task)\n\n# Hook to interact with S3\ns3_hook = S3Hook()\n\n# Function to upload the file to S3\ndef upload_to_s3(file_path):\n    try:\n        s3_hook.load_file(file_path, bucket_name='your-bucket-name', key='uploaded_file.txt')\n        print(f\"File {file_path} uploaded successfully to S3.\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\n# Task to check if the file exists in S3\ncheck_file_in_s3_task = PythonOperator(\n    task_id='check_file_in_s3',\n    python_callable=check_file_in_s3,\n    op_kwargs={'bucket_name': 'your-bucket-name', 'key': 'uploaded_file.txt'},\n    dag=dag\n)\n\n# Error handling for the check task\ncheck_file_in_s3_task.set_upstream(upload_to_s3_task)\n\n# Function to check if the file exists in S3\ndef check_file_in_s3(bucket_name, key):\n    try:\n        response = s3_hook.list_objects_v2(Bucket=bucket_name, Prefix=key)\n        if response['Contents']:\n            print(f\"File {key} found in S3.\")\n        else:\n            print(f\"File {key} not found in S3.\")\n    except Exception as e:\n        print(f\"Error checking file existence: {e}\")"}], "metadata": {"file_name": "dag_402_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981407.7381938}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow workflow that simulates a multi-stage data processing scenario using Python operators with different runtime environments. Create a pipeline that shows how tasks can be executed in standard, virtual, and external Python contexts while maintaining a clear task dependency structure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'retries': 1,\n}\n\n# Create the DAG object\ndag = DAG(\n    'multi_stage_data_processing',\n    default_args=default_args,\n    description='A DAG demonstrating different Python contexts',\n    schedule_interval=None,\n)\n\n# Task to run in the standard Python context\ndef standard_python_task():\n    print(\"Executing task in standard Python context\")\n\nstandard_python_task = PythonOperator(\n    task_id='standard_python',\n    python_callable=standard_python_task,\n    dag=dag,\n)\n\n# Task to run in the virtual Python context\ndef virtual_python_task():\n    print(\"Executing task in virtual Python context\")\n\nvirtual_python_task = PythonOperator(\n    task_id='virtual_python',\n    python_callable=virtual_python_task,\n    dag=dag,\n    env={\n        \"PYTHONPATH\": \"/path/to/virtualenv/lib/python3.x/site-packages\",\n    },\n)\n\n# Task to run in the external Python context\ndef external_python_task():\n    print(\"Executing task in external Python context\")\n\nexternal_python_task = PythonOperator(\n    task_id='external_python',\n    python_callable=external_python_task,\n    dag=dag,\n    env={\n        \"PYTHONPATH\": \"/path/to/external/env/lib/python3.x/site-packages\",\n    },\n)\n\n# Set up dependencies between tasks\nstandard_python_task >> virtual_python_task >> external_python_task"}], "metadata": {"file_name": "dag_1809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.7382026}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Kubernetes cluster management workflow that demonstrates creating, using, and deleting a Google Kubernetes Engine (GKE) cluster for running sample workloads. The workflow should showcase asynchronous task execution and demonstrate basic pod deployment and cross-communication between tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.gke_kubernetes_engine import (\n    GKEPodExecOperator,\n)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    'gke_cluster_management',\n    default_args=default_args,\n    description='A simple DAG to manage a GKE cluster',\n    schedule_interval=None,\n)\n\n# Create GKE Cluster Operator\ncreate_cluster_task = GKECreateClusterOperator(\n    task_id='create_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='sample-cluster',\n    num_nodes=3,\n    machine_type='n1-standard-2',\n    service_account_email='your-service-account-email@your-project-id.iam.gserviceaccount.com',\n    node_labels={'environment': 'production'},\n    enable_private_nodes=False,\n    enable_public_ip=True,\n    min_master_version='1.24.8-gke.15',\n    max_node_version='1.24.8-gke.15',\n    tags=['example'],\n)\n\n# Deploy Sample Application Operator\ndeploy_sample_app_task = GKEPodExecOperator(\n    task_id='deploy_sample_app',\n    namespace='default',\n    container_name='sample-app',\n    command=['sh', '-c', 'echo \"Hello from Airflow!\" && sleep 60'],\n    cluster_name='sample-cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    use_default_container_image=True,\n    in_cluster=True,\n)\n\n# Delete GKE Cluster Operator\ndelete_cluster_task = GKEDeleteClusterOperator(\n    task_id='delete_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='sample-cluster',\n)\n\n# Define dependencies\ncreate_cluster_task >> deploy_sample_app_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_2078_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981407.7382119}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a document classification training process using AWS services, focusing on creating, monitoring, and cleaning up a machine learning model for medical document categorization. Implement a task group that handles the entire classifier creation workflow with proper error handling and resource management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3UploadFileOperator\nfrom airflow.providers.amazon.aws.operators.ec2 import EC2StartInstanceOperator, EC2StopInstanceOperator\nfrom airflow.providers.amazon.aws.operators.emr import EMRCreateClusterOperator, EMRTerminateClusterOperator\nfrom airflow.providers.amazon.aws.operators.redshift import RedshiftDataOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'medical_document_classifier_dag',\n    default_args=default_args,\n    description='A DAG for orchestrating medical document classification using AWS services.',\n    schedule_interval=None,\n)\n\n# Create S3 Bucket\ncreate_bucket_task = S3CreateBucketOperator(\n    task_id='create_s3_bucket',\n    bucket_name='medical-document-classifier-bucket',\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Upload Data to S3\nupload_data_task = S3UploadFileOperator(\n    task_id='upload_data_to_s3',\n    file_path='/path/to/your/data.csv',\n    bucket_name='medical-document-classifier-bucket',\n    object_key='data.csv',\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Start EC2 Instance\nstart_ec2_instance_task = EC2StartInstanceOperator(\n    task_id='start_ec2_instance',\n    instance_type='t2.micro',\n    subnet_id='subnet-12345678',\n    security_group_ids=['sg-12345678'],\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Train Model on EC2 Instance\ntrain_model_task = RedshiftDataOperator(\n    task_id='train_model_on_ec2',\n    cluster_identifier='medical-document-classifier-cluster',\n    database='medical_document_classifier_db',\n    sql_statement='CREATE TABLE IF NOT EXISTS documents (id INT PRIMARY KEY, text TEXT); COPY documents FROM s3://medical-document-classifier-bucket/data.csv CSV HEADER;',\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Stop EC2 Instance\nstop_ec2_instance_task = EC2StopInstanceOperator(\n    task_id='stop_ec2_instance',\n    instance_id=start_ec2_instance_task.output,\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Terminate EMR Cluster\nterminate_emr_cluster_task = EMRTerminateClusterOperator(\n    task_id='terminate_emr_cluster',\n    cluster_id=train_model_task.output,\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Clean Up S3 Bucket\nclean_up_bucket_task = S3DeleteBucketOperator(\n    task_id='cleanup_s3_bucket',\n    bucket_name='medical-document-classifier-bucket',\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Task Group for Classifier Creation Workflow\nwith dag.create_dag_group('classifier_creation_workflow') as classifier_creation_workflow:\n    create_bucket_task >> upload_data_task >> start_ec2_instance_task >> train_model_task >> stop_ec2_instance_task >> terminate_emr_cluster_task >> clean_up_bucket_task"}], "metadata": {"file_name": "dag_1475_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981407.738222}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer and external table creation, focusing on the technical orchestration of moving metadata between Trino and Google BigQuery. Develop a step-by-step pipeline that demonstrates cloud data integration techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.providers.google.cloud.hooks.gcs import GoogleCloudStorageHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'trino_to_bigquery_dag',\n    default_args=default_args,\n    description='DAG for transferring data from Trino to Google BigQuery',\n    schedule_interval='@daily'\n)\n\n# Step 1: Create a Trino hook\ntrino_hook = TrinoHook(trino_conn_id='trino_default')\n\n# Step 2: Create a Google Cloud Storage hook\ngcs_hook = GoogleCloudStorageHook(gcp_conn_id='google_cloud_default')\n\n# Step 3: Define the SQL query to fetch data from Trino\nsql_query = \"\"\"\nSELECT *\nFROM your_trino_table;\n\"\"\"\n\n# Step 4: Define the destination GCS bucket and file name\ndestination_bucket = 'your_gcs_bucket'\ndestination_file_name = 'data.csv'\n\n# Step 5: Write the data from Trino to a CSV file in GCS\nwrite_data_task = PythonOperator(\n    task_id='write_data_to_gcs',\n    python_callable=write_data_to_gcs,\n    op_kwargs={\n        'trino_conn_id': 'trino_default',\n        'sql_query': sql_query,\n        'destination_bucket': destination_bucket,\n        'destination_file_name': destination_file_name\n    },\n    dag=dag\n)\n\n# Step 6: Read the data from GCS into a DataFrame\nread_data_task = PythonOperator(\n    task_id='read_data_from_gcs',\n    python_callable=read_data_from_gcs,\n    op_kwargs={\n        'gcs_conn_id': 'google_cloud_default',\n        'bucket': destination_bucket,\n        'file_name': destination_file_name\n    },\n    dag=dag\n)\n\n# Step 7: Create an external table in BigQuery\ncreate_external_table_task = PythonOperator(\n    task_id='create_external_table_in_bigquery',\n    python_callable=create_external_table_in_bigquery,\n    op_kwargs={\n        'bigquery_conn_id': 'bigquery_default',\n        'table_name': 'your_bigquery_table',\n        'schema': read_data_task.output_schema,\n        'location': 'US'\n    },\n    dag=dag\n)\n\n# Define the dependencies\nwrite_data_task >> read_data_task >> create_external_table_task"}], "metadata": {"file_name": "dag_1275_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981407.7382321}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates metadata management tasks for data governance and discovery, focusing on systematically creating and configuring entry groups, entry types, and aspect types in a Google Cloud Dataplex environment. Develop a workflow that demonstrates the programmatic setup of data catalog components with clear, configurable parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.dataplex import DataplexHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'dataplex_metadata_management',\n    default_args=default_args,\n    description='DAG to manage Dataplex metadata',\n    schedule_interval=None,\n)\n\n# Define the Dataplex hook\ndataplex_hook = DataplexHook()\n\ndef create_entry_group():\n    entry_group_name = \"my-entry-group\"\n    entry_group_description = \"My Entry Group Description\"\n    \n    response = dataplex_hook.create_entry_group(\n        parent=\"projects/my-project-id/locations/us-central1\",\n        entry_group={\n            \"name\": entry_group_name,\n            \"description\": entry_group_description,\n        }\n    )\n    print(f\"Entry group created: {response.name}\")\n\ndef create_entry_type():\n    entry_type_name = \"my-entry-type\"\n    entry_type_description = \"My Entry Type Description\"\n    \n    response = dataplex_hook.create_entry_type(\n        parent=f\"projects/my-project-id/locations/us-central1/entryGroups/{entry_group_name}\",\n        entry_type={\n            \"name\": entry_type_name,\n            \"description\": entry_type_description,\n        }\n    )\n    print(f\"Entry type created: {response.name}\")\n\ndef create_aspect_type():\n    aspect_type_name = \"my-aspect-type\"\n    aspect_type_description = \"My Aspect Type Description\"\n    \n    response = dataplex_hook.create_aspect_type(\n        parent=f\"projects/my-project-id/locations/us-central1/entryTypes/{entry_type_name}\",\n        aspect_type={\n            \"name\": aspect_type_name,\n            \"description\": aspect_type_description,\n        }\n    )\n    print(f\"Aspect type created: {response.name}\")\n\ncreate_entry_group_task = PythonOperator(\n    task_id='create_entry_group',\n    python_callable=create_entry_group,\n    dag=dag,\n)\n\ncreate_entry_type_task = PythonOperator(\n    task_id='create_entry_type',\n    python_callable=create_entry_type,\n    dag=dag,\n)\n\ncreate_aspect_type_task = PythonOperator(\n    task_id='create_aspect_type',\n    python_callable=create_aspect_type,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1222_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981444.3676033}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated image object detection training system that processes a predefined dataset and trains a machine learning model with configurable training parameters. Implement a scalable workflow that supports data ingestion, model training, and automated resource management in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'image_object_detection_dag',\n    default_args=default_args,\n    description='Automated image object detection training system',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to read images from S3\nread_images_task = PythonOperator(\n    task_id='read_images_from_s3',\n    python_callable=read_images_from_s3,\n    op_kwargs={\n        'bucket_name': 'your_bucket_name',\n        'prefix': 'path/to/images'\n    },\n    dag=dag\n)\n\n# Task to preprocess images\npreprocess_images_task = PythonOperator(\n    task_id='preprocess_images',\n    python_callable=preprocess_images,\n    op_kwargs={\n        'input_dir': '/path/to/preprocessed_images'\n    },\n    dag=dag\n)\n\n# Task to train the model\ntrain_model_task = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model,\n    op_kwargs={\n        'model_type': 'resnet50',\n        'batch_size': 32,\n        'epochs': 10\n    },\n    dag=dag\n)\n\n# Task to evaluate the model\nevaluate_model_task = PythonOperator(\n    task_id='evaluate_model',\n    python_callable=evaluate_model,\n    op_kwargs={\n        'model_path': '/path/to/model.h5'\n    },\n    dag=dag\n)\n\n# Define dependencies\nread_images_task >> preprocess_images_task >> train_model_task >> evaluate_model_task"}], "metadata": {"file_name": "dag_2109_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981444.367618}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python program to simulate the rolling of a six-sided die using the `random` module. The program should prompt the user to input the number of times the die should be rolled and then generate random numbers between 1 and 6 for each roll, displaying the result of each roll. Additionally, the program should calculate and display the frequency of each outcome."}, {"role": "assistant", "content": "import random\n\ndef roll_die(num_rolls):\n    \"\"\"\n    Simulates rolling a six-sided die num_rolls times.\n    \n    Parameters:\n    - num_rolls (int): The number of times the die is to be rolled.\n    \n    Returns:\n    - list: A list containing the results of each roll.\n    \"\"\"\n    outcomes = []\n    for _ in range(num_rolls):\n        outcome = random.randint(1, 6)\n        outcomes.append(outcome)\n    return outcomes\n\ndef count_outcomes(outcomes):\n    \"\"\"\n    Counts the frequency of each outcome in the given list of outcomes.\n    \n    Parameters:\n    - outcomes (list): A list of integers representing the outcomes of dice rolls.\n    \n    Returns:\n    - dict: A dictionary with keys as outcomes and values as their frequencies.\n    \"\"\"\n    outcome_counts = {}\n    for outcome in outcomes:\n        if outcome in outcome_counts:\n            outcome_counts[outcome] += 1\n        else:\n            outcome_counts[outcome] = 1\n    return outcome_counts\n\ndef main():\n    # Prompt the user for the number of rolls\n    num_rolls = int(input(\"Enter the number of times you want to roll the die: \"))\n    \n    # Roll the die and get the results\n    outcomes = roll_die(num_rolls)\n    \n    # Count the frequency of each outcome\n    outcome_counts = count_outcomes(outcomes)\n    \n    # Display the results\n    print(f\"Rolling {num_rolls} times:\")\n    for i, outcome in enumerate(outcomes, start=1):\n        print(f\"Roll {i}: {outcome}\")\n    print(\"\\nOutcome Frequencies:\")\n    for outcome, count in outcome_counts.items():\n        print(f\"{outcome}: {count}\")\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3676293}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I calculate the minimum pairwise Euclidean distance between points in a dataset using Python?"}, {"role": "assistant", "content": "import numpy as np\nfrom scipy.spatial import distance\n\n# Assuming 'points' is your dataset where each row represents a point in n-dimensional space\npoints = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Calculate the Euclidean distances between all pairs of points\ndistances = distance.pdist(points)\n\nprint(distances)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.367636}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Redis for real-time message coordination and signaling between different stages of a workflow. Create a system where one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.redis.hooks.redis import RedisHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01T00:00:00',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'redis_pipeline',\n    default_args=default_args,\n    description='A data pipeline using Redis for real-time message coordination',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to publish a start signal to Redis\npublish_start_signal = PythonOperator(\n    task_id='publish_start_signal',\n    python_callable=publish_to_redis,\n    op_kwargs={'key': 'start_signal', 'value': 'started'},\n    dag=dag,\n)\n\n# Function to publish a value to Redis\ndef publish_to_redis(key, value):\n    redis_conn = RedisHook().get_connection('redis_default')\n    redis_conn.set(key, value)\n    print(f\"Published {value} to Redis with key {key}\")\n\n# Task to wait for a condition (e.g., a key in Redis)\nwait_for_condition = PythonOperator(\n    task_id='wait_for_condition',\n    python_callable=check_redis_key,\n    op_kwargs={'key': 'condition_key'},\n    dag=dag,\n)\n\n# Define the dependencies\npublish_start_signal >> wait_for_condition\n\n# Function to check if a key exists in Redis\ndef check_redis_key(key):\n    redis_conn = RedisHook().get_connection('redis_default')\n    return redis_conn.exists(key)"}], "metadata": {"file_name": "dag_642_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981444.3676455}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that simulates a controlled delay scenario, demonstrating how asynchronous time-based sensors can manage workflow timing without blocking computational resources. Create a simple pipeline that waits for a predetermined time interval before proceeding to the next task.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'delayed_task_dag',\n    default_args=default_args,\n    description='A simple DAG with a delayed task',\n    schedule_interval=timedelta(minutes=1)\n)\n\ndef print_message():\n    print(\"Task executed after a delay\")\n\ntask1 = PythonOperator(\n    task_id='print_message',\n    python_callable=print_message,\n    dag=dag,\n    retries=3,\n    retry_delay=timedelta(seconds=2)\n)\n\ntask2 = PythonOperator(\n    task_id='another_task',\n    python_callable=lambda: print(\"Another task executed\"),\n    dag=dag\n)\n\ntask1 >> task2"}], "metadata": {"file_name": "dag_30_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981444.3676531}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates cloud storage file manipulation, showcasing task dependencies, file transfer between buckets, and executing a custom transformation script with proper error handling and cleanup.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator, GetFileOperator, PythonOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCPSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    dag_id='cloud_storage_file_manipulation',\n    default_args=default_args,\n    description='A simple DAG for manipulating files in Google Cloud Storage',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the source and destination GCS buckets\nsource_bucket_name = 'source-bucket'\ndestination_bucket_name = 'destination-bucket'\n\n# Download the file from the source bucket\ndownload_task = GetFileOperator(\n    task_id='download_file',\n    source_object='path/to/source/file.txt',\n    destination_object='path/to/destination/file.txt',\n    bucket=source_bucket_name,\n    dag=dag\n)\n\n# Execute a Python script on the downloaded file\ntransform_script_path = '/usr/bin/python3 /path/to/your/script.py'\nexecute_script_task = PythonOperator(\n    task_id='execute_script',\n    python_callable=your_custom_script_function,\n    op_kwargs={'input_file': 'path/to/destination/file.txt'},\n    dag=dag\n)\n\n# Clean up the temporary file\ncleanup_task = BashOperator(\n    task_id='cleanup_file',\n    bash_command='rm path/to/destination/file.txt',\n    dag=dag\n)\n\n# Set up the task dependencies\ndownload_task >> execute_script_task >> cleanup_task"}], "metadata": {"file_name": "dag_1651_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981444.367664}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS S3 data management workflow that demonstrates bucket creation, object manipulation, and metadata tagging. Create a DAG that showcases different S3 operations like creating buckets, uploading files, listing contents, and managing bucket tags.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    's3_management_dag',\n    default_args=default_args,\n    description='A simple DAG to manage AWS S3 resources',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to create a new S3 bucket\ncreate_bucket_task = DummyOperator(\n    task_id='create_bucket',\n    dag=dag,\n    params={\n        'bucket_name': 'my-new-s3-bucket'\n    }\n)\n\n# Task to upload a file to the newly created bucket\nupload_file_task = DummyOperator(\n    task_id='upload_file',\n    dag=dag,\n    params={\n        'bucket_name': 'my-new-s3-bucket',\n        'file_path': '/path/to/your/file.txt'\n    }\n)\n\n# Task to list all objects in the bucket\nlist_objects_task = DummyOperator(\n    task_id='list_objects',\n    dag=dag,\n    params={\n        'bucket_name': 'my-new-s3-bucket'\n    }\n)\n\n# Task to tag the bucket\ntag_bucket_task = DummyOperator(\n    task_id='tag_bucket',\n    dag=dag,\n    params={\n        'bucket_name': 'my-new-s3-bucket',\n        'tags': [\n            {'Key': 'Environment', 'Value': 'Production'},\n            {'Key': 'Department', 'Value': 'Finance'}\n        ]\n    }\n)\n\n# Define the order of tasks\ncreate_bucket_task >> upload_file_task >> list_objects_task >> tag_bucket_task"}], "metadata": {"file_name": "dag_92_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981444.3676753}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u8bf7\u95ee\u60a8\u80fd\u8be6\u7ec6\u8bf4\u660e\u4e00\u4e0b\u5982\u4f55\u5728Python\u4e2d\u5b9e\u73b0\u4e00\u4e2a\u6d45\u62f7\u8d1d\u5417\uff1f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u6211\u4eec\u9700\u8981\u4f7f\u7528\u6d45\u62f7\u8d1d\u5462\uff1f"}, {"role": "assistant", "content": "import copy\n\noriginal_list = [1, 2, [3, 4], 'hello']\nshallow_copy = copy.copy(original_list)\n\nprint(\"Original List:\", original_list)\nprint(\"Shallow Copy:\", shallow_copy)\n\n# \u4fee\u6539\u539f\u59cb\u5217\u8868\u4e2d\u7684\u5217\u8868\u5143\u7d20\noriginal_list[2][0] = 99\n\nprint(\"Modified Original List:\", original_list)\nprint(\"Modified Shallow Copy:\", shallow_copy)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3676848}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Blob Storage monitoring workflow that checks for specific file arrivals or file prefix patterns in a cloud storage container. Create an Airflow DAG that can trigger downstream processes based on the presence of expected files or file groups.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.azure.blob_operator import AzureBlobSensor\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'azure_blob_monitoring_dag',\n    default_args=default_args,\n    description='A DAG to monitor Azure Blob Storage for specific file arrivals or file prefix patterns.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Custom operator to check for specific file arrivals or file prefix patterns\ndef check_file_in_container(file_pattern):\n    return AzureBlobSensor(\n        task_id=f'check_{file_pattern}',\n        container_name='your-container-name',\n        blob_name=file_pattern,\n        azure_storage_conn_id='azure_default'\n    )\n\n# Example usage of the custom operator\nfile_pattern_1 = check_file_in_container('example-file-*.txt')\nfile_pattern_2 = check_file_in_container('prefix-*')\n\n# Trigger downstream tasks based on the presence of the files\ntask_1 = DummyOperator(task_id='task_1', dag=dag)\ntask_2 = DummyOperator(task_id='task_2', dag=dag)\n\nfile_pattern_1 >> task_1\nfile_pattern_2 >> task_2"}], "metadata": {"file_name": "dag_49_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981444.3676946}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me with a Python script to merge two CSV files based on a common column and then save the merged file?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Load the first CSV file into a DataFrame\ndf1 = pd.read_csv('file1.csv')\n\n# Load the second CSV file into another DataFrame\ndf2 = pd.read_csv('file2.csv')\n\n# Merge the two DataFrames based on a common column (e.g., 'ID')\nmerged_df = pd.merge(df1, df2, on='common_column_name')\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv('merged_file.csv', index=False)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3677006}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What is the most concise and linear Python code to find the third smallest element in an unsorted list?"}, {"role": "assistant", "content": "def third_smallest_element(lst):\n    if len(lst) < 3:\n        return None  # If there are less than three elements, return None\n\n    # Initialize the first two smallest elements as infinity\n    min1 = float('inf')\n    min2 = float('inf')\n    \n    # Iterate through the list\n    for num in lst:\n        # Update min1 and min2 if current number is smaller than min1 or min2\n        if num < min1:\n            min2 = min(min1, min2)\n            min1 = num\n        elif num < min2:\n            min2 = num\n    \n    # Return the third smallest element\n    return min2\n\n# Example usage:\nlst = [5, 3, 8, 4, 9]\nprint(third_smallest_element(lst))  # Output: 4"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3677073}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced cloud storage transfer techniques using GCS and SFTP operators, with emphasis on handling various file transfer scenarios and implementing robust file transfer verification mechanisms. Develop a workflow that demonstrates different transfer methods while ensuring data integrity through sensor checks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSCredentialsHook\nfrom airflow.providers.ssh.hooks.ssh import SSHHook\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cloud_storage_sftp_transfer',\n    default_args=default_args,\n    description='Transfer files between GCS and SFTP',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Define variables\nbucket_name = 'your_bucket_name'\nsource_file_path = 'path/to/source/file.txt'\ndestination_file_path = 'path/to/destination/file.txt'\nssh_host = 'your_ssh_host'\nssh_port = 22\nssh_username = 'your_ssh_username'\nssh_password = 'your_ssh_password'\n\ndef upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n    gcs_hook = GCSCredentialsHook()\n    bucket = gcs_hook.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_path)\n\ndef download_from_gcs(bucket_name, source_blob_name, destination_file_path):\n    gcs_hook = GCSCredentialsHook()\n    bucket = gcs_hook.get_bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_path)\n\ndef verify_file(file_path):\n    # Implement your file verification logic here\n    print(f\"Verifying file: {file_path}\")\n    return True\n\ndef upload_to_sftp(ssh_host, ssh_port, ssh_username, ssh_password, source_file_path, destination_file_path):\n    sftp_hook = SSHHook(ssh_conn_id='your_ssh_connection')\n    with sftp_hook.connect(host=ssh_host, port=ssh_port, username=ssh_username, password=ssh_password) as conn:\n        sftp_hook.put(local_file=source_file_path, remote_file=destination_file_path)\n\ndef download_from_sftp(ssh_host, ssh_port, ssh_username, ssh_password, source_file_path, destination_file_path):\n    sftp_hook = SSHHook(ssh_conn_id='your_ssh_connection')\n    with sftp_hook.connect(host=ssh_host, port=ssh_port, username=ssh_username, password=ssh_password) as conn:\n        sftp_hook.get(remote_file=source_file_path, local_file=destination_file_path)\n\nupload_task = PythonOperator(\n    task_id='upload_to_gcs',\n    python_callable=upload_to_gcs,\n    op_kwargs={\n        'bucket_name': bucket_name,\n        'source_file_path': source_file_path,\n        'destination_blob_name': 'uploaded_file.txt'\n    },\n    dag=dag\n)\n\ndownload_task = PythonOperator(\n    task_id='download_from_gcs',\n    python_callable=download_from_gcs,\n    op_kwargs={\n        'bucket_name': bucket_name,\n        'source_blob_name': 'uploaded_file.txt',\n        'destination_file_path': 'downloaded_file.txt'\n    },\n    dag=dag\n)\n\nverify_task = PythonOperator(\n    task_id='verify_file',\n    python_callable=verify_file,\n    op_kwargs={\n        'file_path': 'downloaded_file.txt'\n    },\n    dag=dag\n)\n\nupload_task >> download_task >> verify_task"}], "metadata": {"file_name": "dag_2095_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981444.3677294}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms JSON-based Jeopardy question data into vector-enabled database collections using Weaviate's vectorization capabilities. Implement a workflow that shows how to batch import data, handle different vectorization approaches, and manage database collections programmatically.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.weaviate.hooks.weaviate import WeaviateHook\nimport json\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='jeopardy_question_vectorization',\n    default_args=default_args,\n    description='Transform Jeopardy question data into vector-enabled database collections using Weaviate',\n    schedule_interval=None  # Set this to your desired schedule interval\n) as dag:\n\n    # Step 1: Import Jeopardy question data from JSON file\n    def import_jeopardy_questions():\n        with open('jeopardy_questions.json', 'r') as file:\n            questions = json.load(file)\n        return questions\n\n    import_questions_task = PythonOperator(\n        task_id='import_jeopardy_questions',\n        python_callable=import_jeopardy_questions,\n        provide_context=True\n    )\n\n    # Step 2: Vectorize the Jeopardy question data using Weaviate\n    def vectorize_questions(questions):\n        weaviate_hook = WeaviateHook()\n        client = weaviate_hook.get_client()\n\n        # Assuming each question is represented as a dictionary with 'question' and 'answer'\n        for question in questions:\n            embedding = client.vectorizer.text2vector(question['question'])\n            client.data_object.create(\n                class_name=\"JeopardyQuestion\",\n                properties={\n                    \"question\": question['question'],\n                    \"answer\": question['answer'],\n                    \"embedding\": embedding\n                }\n            )\n\n    vectorize_questions_task = PythonOperator(\n        task_id='vectorize_questions',\n        python_callable=vectorize_questions,\n        provide_context=True\n    )\n\n    # Step 3: Manage database collections programmatically\n    def manage_database_collections():\n        weaviate_hook = WeaviateHook()\n        client = weaviate_hook.get_client()\n\n        # List all classes in the Weaviate instance\n        classes = client.schema.get_classes()\n        print(\"Classes in Weaviate:\", classes)\n\n        # Delete a specific class if it exists\n        delete_class_query = \"\"\"\n        DELETE FROM JeopardyQuestion\n        WHERE id = 'your_class_id'\n        \"\"\"\n        client.batch.execute(delete_class_query)\n\n        # Create a new class\n        create_class_query = \"\"\"\n        CREATE CLASS JeopardyQuestion\n        {\n          properties:\n          [\n            {name: \"question\", dataType: \"text\"},\n            {name: \"answer\", dataType: \"text\"},\n            {name: \"embedding\", dataType: \"float\"}\n          ]\n        }\n        \"\"\"\n        client.batch.execute(create_class_query)\n\n    manage_database_collections_task = PythonOperator(\n        task_id='manage_database_collections',\n        python_callable=manage_database_collections,\n        provide_context=True\n    )\n\n    # Task dependencies\n    import_questions_task >> vectorize_questions_task >> manage_database_collections_task"}], "metadata": {"file_name": "dag_1831_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981444.3677433}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that showcases basic data persistence and retrieval techniques using an embedded database technology. Create a sequential task flow that demonstrates reading and writing operations in a lightweight, file-based key-value store.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport sqlite3\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 3,\n    'retry_delay': 5\n}\n\ndag = DAG(\n    'sqlite_example_dag',\n    default_args=default_args,\n    description='A simple example of using SQLite for data persistence',\n    schedule_interval=None,\n)\n\n# Task to insert data into the database\ninsert_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data_to_db,\n    dag=dag,\n    op_kwargs={\n        'db_path': '/tmp/my_database.db'\n    }\n)\n\n# Task to retrieve data from the database\nretrieve_task = PythonOperator(\n    task_id='retrieve_data',\n    python_callable=retrieve_data_from_db,\n    dag=dag,\n    op_kwargs={\n        'db_path': '/tmp/my_database.db'\n    }\n)\n\n# Define the function to insert data into the database\ndef insert_data_to_db(db_path):\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    \n    # Create table if not exists\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS my_table (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL,\n            value REAL NOT NULL\n        )\n    ''')\n    \n    # Insert some sample data\n    cursor.execute(\"INSERT INTO my_table (name, value) VALUES ('Alice', 100)\")\n    cursor.execute(\"INSERT INTO my_table (name, value) VALUES ('Bob', 200)\")\n    \n    conn.commit()\n    conn.close()\n\n# Define the function to retrieve data from the database\ndef retrieve_data_from_db(db_path):\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    \n    # Retrieve all records from the table\n    cursor.execute(\"SELECT * FROM my_table\")\n    rows = cursor.fetchall()\n    \n    for row in rows:\n        print(f\"ID: {row[0]}, Name: {row[1]}, Value: {row[2]}\")\n    \n    conn.close()\n\n# Set up the task dependencies\ninsert_task >> retrieve_task"}], "metadata": {"file_name": "dag_950_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981444.367754}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data repository management workflow that demonstrates creating, updating, and deleting a Databricks repository while executing a data preparation notebook. The workflow should showcase automated repository lifecycle management and notebook execution in a cloud-based data processing environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import (\n    DatabricksSubmitOperator,\n    DatabricksRunNowOperator,\n    DatabricksJobOperator,\n)\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'databricks_repository_management',\n    default_args=default_args,\n    description='A DAG for managing Databricks repositories and running notebooks',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Create a new Databricks repository\ncreate_repo_task = DatabricksCreateRepositoryOperator(\n    task_id='create_databricks_repository',\n    databricks_conn_id='databricks_default',\n    name='MyDataRepository',\n    location='us-west-2',\n    workspace_id=1234567890123456,\n    tags={'environment': 'production'},\n    dag=dag\n)\n\n# Update an existing Databricks repository\nupdate_repo_task = DatabricksUpdateRepositoryOperator(\n    task_id='update_databricks_repository',\n    databricks_conn_id='databricks_default',\n    repository_name='MyDataRepository',\n    location='eu-central-1',\n    workspace_id=1234567890123456,\n    tags={'environment': 'development'},\n    dag=dag\n)\n\n# Delete a Databricks repository\ndelete_repo_task = DatabricksDeleteRepositoryOperator(\n    task_id='delete_databricks_repository',\n    databricks_conn_id='databricks_default',\n    repository_name='MyDataRepository',\n    dag=dag\n)\n\n# Run a data preparation notebook\nrun_notebook_task = DatabricksRunNowOperator(\n    task_id='run_data_preparation_notebook',\n    databricks_conn_id='databricks_default',\n    notebook_path='/path/to/your/notebook.py',\n    parameters={\n        'input_data': '/path/to/input/data.csv',\n        'output_data': '/path/to/output/data.csv'\n    },\n    notebook_params={\n        'param1': 'value1',\n        'param2': 'value2'\n    },\n    dag=dag\n)\n\n# Define dependencies between tasks\ncreate_repo_task >> update_repo_task >> delete_repo_task >> run_notebook_task"}], "metadata": {"file_name": "dag_1161_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981444.3677642}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that illustrates data processing and infrastructure management in a cloud-native Kubernetes environment using dynamic cluster provisioning and ephemeral pod execution. Create a workflow that demonstrates how to programmatically spin up computational resources, execute sample workloads, and cleanly tear down infrastructure after task completion.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.kubernetes.operators.kubernetes_pod_operator import KubernetesPodOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'kubernetes_infrastructure_management',\n    default_args=default_args,\n    description='A DAG for managing Kubernetes infrastructure and executing workflows.',\n    schedule_interval=timedelta(days=1),\n)\n\n# Dynamic Cluster Provisioning\ndynamic_cluster_provisioning_task = KubernetesPodOperator(\n    namespace='default',\n    image='nginx:latest',\n    name='dynamic-cluster-provisioning',\n    labels={'app': 'dynamic-cluster'},\n    dag=dag,\n)\n\n# Ephemeral Pod Execution\nephemeral_pod_execution_task = KubernetesPodOperator(\n    namespace='default',\n    image='python:3.8-slim',\n    command=['python', '-c', 'print(\"Hello, World!\")'],\n    name='ephemeral-pod-execution',\n    labels={'app': 'ephemeral'},\n    dag=dag,\n)\n\n# Data Processing\ndata_processing_task = KubernetesPodOperator(\n    namespace='default',\n    image='python:3.8-slim',\n    command=['python', '-c', 'import pandas as pd; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}); df.to_csv(\"output.csv\", index=False)'],\n    name='data-processing',\n    labels={'app': 'data-processing'},\n    dag=dag,\n)\n\n# Cleanup Infrastructure\ncleanup_infrastructure_task = KubernetesPodOperator(\n    namespace='default',\n    image='bash',\n    command=['echo \"Cleaning up infrastructure...\" && rm output.csv'],\n    name='cleanup-infrastructure',\n    labels={'app': 'cleanup'},\n    dag=dag,\n)\n\n# Task Dependencies\ndynamic_cluster_provisioning_task >> ephemeral_pod_execution_task >> data_processing_task >> cleanup_infrastructure_task"}], "metadata": {"file_name": "dag_900_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981444.3677757}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a sample workflow DAG that simulates a basic system monitoring and testing pipeline, using different system commands and containerized operations to showcase task dependency and execution strategies. Create a sequence of tasks that validate system time, perform wait operations, and execute a containerized process.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 10, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'system_monitoring_test_pipeline',\n    default_args=default_args,\n    description='A simple system monitoring and testing pipeline',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Task to check system time\ncheck_system_time = BashOperator(\n    task_id='check_system_time',\n    bash_command='date'\n)\n\n# Wait operation for 5 minutes\nwait_5_minutes = BashOperator(\n    task_id='wait_5_minutes',\n    bash_command='sleep 5m'\n)\n\n# Containerized process\ncontainer_process = BashOperator(\n    task_id='container_process',\n    bash_command='docker run -it ubuntu /bin/bash'\n)\n\n# Task to verify the output of the containerized process\nverify_output = BashOperator(\n    task_id='verify_output',\n    bash_command='echo \"Container output:\" && docker logs <container_id>'\n)\n\n# Define dependencies between tasks\ncheck_system_time >> wait_5_minutes >> container_process >> verify_output"}], "metadata": {"file_name": "dag_127_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981444.3677948}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across AWS services like RDS, EC2, and S3. Develop a modular pipeline that creates necessary cloud resources, configures security settings, and executes a data replication process.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.hooks.rds import RDSHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aws_cloud_infrastructure_dag',\n    default_args=default_args,\n    description='DAG to orchestrate AWS cloud infrastructure provisioning and data migration',\n    schedule_interval=timedelta(days=1)\n)\n\n# Create EC2 instance\ncreate_ec2_instance_task = PythonOperator(\n    task_id='create_ec2_instance',\n    python_callable=create_ec2_instance,\n    dag=dag\n)\n\n# Create RDS instance\ncreate_rds_instance_task = PythonOperator(\n    task_id='create_rds_instance',\n    python_callable=create_rds_instance,\n    dag=dag\n)\n\n# Create S3 bucket\ncreate_s3_bucket_task = PythonOperator(\n    task_id='create_s3_bucket',\n    python_callable=create_s3_bucket,\n    dag=dag\n)\n\n# Configure security group\nconfigure_security_group_task = PythonOperator(\n    task_id='configure_security_group',\n    python_callable=configure_security_group,\n    dag=dag\n)\n\n# Execute data replication\nexecute_data_replication_task = PythonOperator(\n    task_id='execute_data_replication',\n    python_callable=execute_data_replication,\n    dag=dag\n)\n\n# Define dependencies\ncreate_ec2_instance_task >> create_rds_instance_task >> create_s3_bucket_task >> configure_security_group_task >> execute_data_replication_task"}], "metadata": {"file_name": "dag_1477_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678062}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily Twitter data pipeline that collects tweets from multiple accounts, processes them for insights, and stores the results in a database. Create a workflow that captures incoming and outgoing tweets, performs sentiment analysis, and prepares data for further business intelligence.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.twitter.hooks.twitter import TwitterHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'twitter_data_pipeline',\n    default_args=default_args,\n    description='A daily Twitter data pipeline',\n    schedule_interval='@daily'\n)\n\ndef collect_tweets():\n    twitter_hook = TwitterHook()\n    tweets = twitter_hook.get_home_timeline(screen_name=\"example_account\", count=20)\n    return tweets\n\ndef process_tweets(tweets):\n    # Perform sentiment analysis here\n    pass\n\ndef store_results(results):\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    conn = postgres_hook.get_connection('postgres_default')\n    cursor = conn.cursor()\n    for result in results:\n        cursor.execute(\"INSERT INTO tweets (text, sentiment) VALUES (%s, %s)\", (result['text'], result['sentiment']))\n    conn.commit()\n    cursor.close()\n\ncollect_tweets_task = PythonOperator(\n    task_id='collect_tweets',\n    python_callable=collect_tweets,\n    dag=dag\n)\n\nprocess_tweets_task = PythonOperator(\n    task_id='process_tweets',\n    python_callable=process_tweets,\n    dag=dag\n)\n\nstore_results_task = PythonOperator(\n    task_id='store_results',\n    python_callable=store_results,\n    dag=dag\n)\n\ncollect_tweets_task >> process_tweets_task >> store_results_task"}], "metadata": {"file_name": "dag_1939_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678136}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code example that demonstrates using the `grpcio-reflection` for debugging a gRPC server in Python?"}, {"role": "assistant", "content": "import grpc\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Define the service definition\nclass Calculator_pb2_grpc as calculator_pb2_grpc\nfrom calculator_pb2 import Calculator_pb2\n\nclass CalculatorServicer(calculate_pb2_grpc.CalculatorServicer):\n    def __init__(self):\n        super().__init__()\n\n    def Add(self, request, context):\n        return calculate_pb2.Response(result=request.a + request.b)\n\n    def Multiply(self, request, context):\n        return calculate_pb2.Response(result=request.a * request.b)\n\ndef serve():\n    # Create a gRPC server\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n\n    # Register the service with the server\n    calculator_pb2_grpc.add_CalculatorServicer_to_server(server, CalculatorServicer())\n\n    # Start the server on port 50051\n    server.add_insecure_port('localhost:50051')\n    print(\"Server started at localhost:50051\")\n    server.start()\n\nif __name__ == '__main__':\n    serve()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.367822}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs data transfer operations between BigQuery tables, illustrating how to programmatically create table schemas, copy data between tables, and export data to cloud storage. Implement a complete data workflow that includes setup, data movement, and cleanup stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSCredentialsHook\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'bigquery_data_transfer_dag',\n    default_args=default_args,\n    description='A DAG for transferring data between BigQuery tables and exporting to GCS.',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create a BigQuery hook\nbq_hook = BigQueryHook()\n\n# Create a GCS credentials hook\ngcs_credentials_hook = GCSCredentialsHook()\n\n# Define source and destination BigQuery tables\nsource_table = 'your_project_id.your_dataset.source_table'\ndestination_table = 'your_project_id.your_dataset.destination_table'\n\n# Define source and destination GCS bucket\nsource_bucket = 'your_project_id.your_bucket'\ndestination_bucket = 'your_project_id.your_exported_bucket'\n\n# Create a BigQuery to BigQuery operator to copy data from source to destination\ncopy_data_operator = BigQueryToBigQueryOperator(\n    task_id='copy_data_task',\n    source_project_dataset_table=source_table,\n    destination_project_dataset_table=destination_table,\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag\n)\n\n# Create a BigQuery to GCS operator to export data from BigQuery to GCS\nexport_data_operator = BigQueryToGCSOperator(\n    task_id='export_data_task',\n    source_project_dataset_table=source_table,\n    destination_bucket=destination_bucket,\n    destination_file_format='CSV',\n    export_format='TEXT_FILE',\n    dag=dag\n)\n\n# Create a GCS to BigQuery operator to load data from GCS back into BigQuery\nload_data_operator = GCSToBigQueryOperator(\n    task_id='load_data_task',\n    source_bucket=destination_bucket,\n    destination_project_dataset_table=destination_table,\n    source_file_pattern='*.csv',\n    source_format='TEXT_FILE',\n    dag=dag\n)\n\n# Set dependencies\ncopy_data_operator.set_upstream(load_data_operator)\nload_data_operator.set_upstream(export_data_operator)"}], "metadata": {"file_name": "dag_483_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678312}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates setup and teardown tasks for a cloud infrastructure workflow. Create a DAG that shows how to manage resource allocation and cleanup across multiple task stages.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.s3.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.ec2.hooks.ec2 import EC2Hook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='aws_infrastructure_dag',\n    default_args=default_args,\n    schedule_interval=None,\n) as dag:\n\n    # Define variables\n    vpc_name = \"MyVPC\"\n    subnet_name = \"MySubnet\"\n    security_group_name = \"MySecurityGroup\"\n    ec2_instance_type = \"t2.micro\"\n    image_id = \"ami-0abcdef1234567890\"  # Replace with your AMI ID\n    key_name = \"my-key-pair\"\n\n    # Create VPC\n    create_vpc_task = EC2CreateVPCOperator(\n        task_id=\"create_vpc\",\n        vpc_name=vpc_name,\n        region_name=\"us-west-2\",  # Replace with your AWS region\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Create subnet\n    create_subnet_task = EC2CreateSubnetOperator(\n        task_id=\"create_subnet\",\n        vpc_id=create_vpc_task.output,\n        subnet_name=subnet_name,\n        cidr_block=\"10.0.0.0/24\",\n        availability_zone=\"us-west-2a\",  # Replace with your availability zone\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Create security group\n    create_security_group_task = EC2CreateSecurityGroupOperator(\n        task_id=\"create_security_group\",\n        group_name=security_group_name,\n        description=\"Security group for my infrastructure\",\n        vpc_id=create_vpc_task.output,\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Add rules to security group\n    add_security_group_rule_task = EC2AddSecurityGroupRuleOperator(\n        task_id=\"add_security_group_rule\",\n        group_id=create_security_group_task.output,\n        ip_protocol=\"tcp\",\n        from_port=22,\n        to_port=22,\n        cidr_ip=\"0.0.0.0/0\",\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Launch EC2 instance\n    launch_ec2_instance_task = EC2CreateInstanceOperator(\n        task_id=\"launch_ec2_instance\",\n        image_id=image_id,\n        instance_type=ec2_instance_type,\n        subnet_id=create_subnet_task.output,\n        security_groups=[create_security_group_task.output],\n        key_name=key_name,\n        instance_role=\"arn:aws:iam::123456789012:role/my-instance-role\",  # Replace with your IAM role ARN\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Delete EC2 instance\n    delete_ec2_instance_task = EC2DeleteInstanceOperator(\n        task_id=\"delete_ec2_instance\",\n        instance_ids=launch_ec2_instance_task.output,\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Delete security group\n    delete_security_group_task = EC2DeleteSecurityGroupOperator(\n        task_id=\"delete_security_group\",\n        group_id=create_security_group_task.output,\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Delete subnet\n    delete_subnet_task = EC2DeleteSubnetOperator(\n        task_id=\"delete_subnet\",\n        subnet_id=create_subnet_task.output,\n        aws_conn_id=\"aws_default\"\n    )\n\n    # Delete VPC\n    delete_vpc_task = EC2DeleteVPCOperator(\n        task_id=\"delete_vpc\",\n        vpc_id=create_vpc_task.output,\n        aws_conn_id=\"aws_default\"\n    )"}], "metadata": {"file_name": "dag_23_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678436}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS cloud infrastructure orchestration with dynamic security configuration and EMR cluster management. Develop a workflow that creates an S3 bucket, configures security settings, launches an EMR cluster, executes a Spark step, and handles cluster lifecycle management.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.emr import EmrClientHook\nfrom airflow.providers.amazon.aws.operators.emr import CreateClusterOperator, AddStepOperator, GetClusterStateOperator, DeleteClusterOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'aws_emr_dag',\n    default_args=default_args,\n    description='AWS EMR Cluster Management',\n    schedule_interval=None,\n)\n\n# Step to create an S3 bucket\ncreate_bucket_operator = S3CreateBucketOperator(\n    task_id='create_s3_bucket',\n    bucket_name='my-aws-bucket',\n    aws_conn_id='aws_default',\n    dag=dag,\n)\n\n# Step to configure security settings for the EMR cluster\nconfigure_security_settings_operator = EmrSecuritySettingsOperator(\n    task_id='configure_security_settings',\n    emr_conn_id='emr_default',\n    dag=dag,\n)\n\n# Step to launch an EMR cluster\nlaunch_cluster_operator = CreateClusterOperator(\n    task_id='launch_emr_cluster',\n    name='MyEMRCluster',\n    release_label='emr-5.30.0',\n    instance_type='m5.xlarge',\n    num_instances=3,\n    keep_alive_hours=24,\n    bootstrap_actions=[\n        {\n            'Name': 'Install Python',\n            'ScriptPath': 's3://path/to/install-python.sh'\n        }\n    ],\n    tags={\n        'Environment': 'Production'\n    },\n    dag=dag,\n)\n\n# Step to add a Spark step to the EMR cluster\nadd_spark_step_operator = AddStepOperator(\n    task_id='add_spark_step',\n    job_flow_id=\"{{ task_instance.xcom_pull('launch_emr_cluster', key='return_value') }}\",\n    name='Spark Job',\n    args={'--master': 'yarn', '--deploy-mode': 'cluster'},\n    script_path='s3://path/to/spark-job.py',\n    dag=dag,\n)\n\n# Step to get the state of the EMR cluster\nget_cluster_state_operator = GetClusterStateOperator(\n    task_id='get_cluster_state',\n    cluster_id=\"{{ task_instance.xcom_pull('launch_emr_cluster', key='return_value') }}\",\n    dag=dag,\n)\n\n# Step to delete the EMR cluster\ndelete_cluster_operator = DeleteClusterOperator(\n    task_id='delete_emr_cluster',\n    cluster_id=\"{{ task_instance.xcom_pull('launch_emr_cluster', key='return_value') }}\",\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Define dependencies between tasks\ncreate_bucket_operator >> configure_security_settings_operator >> launch_cluster_operator >> add_spark_step_operator >> get_cluster_state_operator >> delete_cluster_operator"}], "metadata": {"file_name": "dag_389_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678544}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a project where I need to create a function that determines the academic year based on a given date. The academic year typically starts on September 1st and ends on August 31st of the following year. The function should take a date as input and return the academic year in the format \"YYYY-YYYY\". For example, for a date of February 1, 2023, the function should return \"2022-2023\". How can I implement this function in Python?"}, {"role": "assistant", "content": "from datetime import datetime\n\ndef get_academic_year(date):\n    \"\"\"\n    Determines the academic year based on the given date.\n    \n    Parameters:\n    - date (str): A string representing the date in 'YYYY-MM-DD' format.\n    \n    Returns:\n    - str: The academic year in the format 'YYYY-YYYY'.\n    \"\"\"\n    # Convert the input string to a datetime object\n    date_obj = datetime.strptime(date, '%Y-%m-%d')\n    \n    # Extract the year from the datetime object\n    current_year = date_obj.year\n    \n    # Calculate the start of the current academic year\n    start_of_current_year = datetime(current_year, 9, 1)\n    \n    # Calculate the end of the current academic year\n    end_of_current_year = datetime(current_year + 1, 8, 31)\n    \n    # Check if the given date is within the current academic year\n    if start_of_current_year <= date_obj <= end_of_current_year:\n        return f\"{current_year}-{current_year+1}\"\n    else:\n        # If not, calculate the previous academic year\n        previous_year = current_year - 1\n        return f\"{previous_year}-{previous_year+1}\"\n\n# Example usage\ndate_input = \"February 1, 2023\"\nprint(get_academic_year(date_input))  # Output: \"2022-2023\""}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3678637}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates complex Cloud SQL operations including instance creation, data export/import, database configuration changes, and infrastructure cleanup. Develop a systematic workflow that demonstrates enterprise-grade database infrastructure automation.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.sql.operators.sql_create_instance import SqlCreateInstanceOperator\nfrom airflow.providers.google.cloud.sql.operators.sql_delete_instance import SqlDeleteInstanceOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cloud_sql_dag',\n    default_args=default_args,\n    description='A DAG for managing Cloud SQL operations',\n    schedule_interval=None,\n)\n\n# Create a new Cloud SQL instance\ncreate_instance_task = SqlCreateInstanceOperator(\n    task_id='create_instance',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    region='us-central1',\n    machine_type='f1-micro',\n    disk_size_gb=10,\n    database_version='mysql_8_0',\n    service_account_email='your-service-account-email@your-project-id.iam.gserviceaccount.com'\n)\n\n# Export data from Cloud SQL to Google Cloud Storage\nexport_data_task = BigQueryToGCSOperator(\n    task_id='export_data',\n    source_project_id='your-project-id',\n    destination_project_id='your-project-id',\n    dataset_id='your-dataset-id',\n    table_id='your-table-id',\n    destination_bucket='your-bucket-name',\n    destination_file_prefix='data_exported_{{ ds_nodash }}'\n)\n\n# Import data from Google Cloud Storage to Cloud SQL\nimport_data_task = GCSToBigQueryOperator(\n    task_id='import_data',\n    source_project_id='your-project-id',\n    destination_project_id='your-project-id',\n    dataset_id='your-dataset-id',\n    table_id='your-table-id',\n    source_bucket='your-bucket-name',\n    source_file_pattern='data_imported_{{ ds_nodash }}'\n)\n\n# Configure database settings\nconfigure_settings_task = DummyOperator(\n    task_id='configure_settings',\n    dag=dag\n)\n\n# Delete the Cloud SQL instance\ndelete_instance_task = SqlDeleteInstanceOperator(\n    task_id='delete_instance',\n    project_id='your-project-id',\n    instance_id='your-instance-id'\n)\n\n# Define the order of tasks\ncreate_instance_task >> export_data_task >> import_data_task >> configure_settings_task >> delete_instance_task"}], "metadata": {"file_name": "dag_2005_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981444.3678734}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a cloud-based data processing job using Google Dataproc, focusing on the sequential task dependencies of cluster creation, job submission, and cluster deletion. Implement a robust workflow that handles infrastructure provisioning and job execution with clear, predictable task progression.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocClusterCreateOperator,\n    DataprocJobSubmitOperator,\n    DataprocClusterDeleteOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'dataproc_example_dag',\n    default_args=default_args,\n    description='A simple example DAG for running a Dataproc job.',\n    schedule_interval=None,\n)\n\n# Task to create a Dataproc cluster\ncreate_cluster_task = DataprocClusterCreateOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your_project_id',\n    region='us-central1',\n    cluster_name='example-cluster',\n    master_config={\n        'machine_type': 'n1-standard-4',\n        'disk_size_gb': 50,\n    },\n    worker_config={\n        'machine_type': 'n1-standard-8',\n        'disk_size_gb': 50,\n        'num_instances': 2,\n    },\n    init_actions_uris=[\n        'gs://path/to/init-script.sh',  # Path to your initialization script\n    ],\n)\n\n# Task to submit a Spark job to the created cluster\nsubmit_spark_job_task = DataprocJobSubmitOperator(\n    task_id='submit_spark_job',\n    project_id='your_project_id',\n    region='us-central1',\n    cluster_name='example-cluster',\n    main_class='com.example.YourSparkMainClass',\n    jar_file_uri='gs://path/to/your/spark-job.jar',\n    arguments=['arg1', 'arg2'],\n)\n\n# Task to delete the Dataproc cluster\ndelete_cluster_task = DataprocClusterDeleteOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your_project_id',\n    region='us-central1',\n    cluster_name='example-cluster',\n)\n\n# Define the order of tasks\ncreate_cluster_task >> submit_spark_job_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_863_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981444.367882}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow that generates vector embeddings for text data using OpenAI and stores them in a PostgreSQL database with vector capabilities. Create a data pipeline that demonstrates how to transform text into machine-readable vector representations and persist them for potential machine learning or semantic search applications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nimport openai\nimport psycopg2\nimport json\n\n# Set up OpenAI API key\nopenai.api_key = 'your_openai_api_key'\n\n# Set up PostgreSQL connection parameters\npostgres_conn_id = 'postgres_default'\ndb_host = 'localhost'\ndb_port = '5432'\ndb_name = 'vector_db'\ndb_user = 'airflow'\ndb_password = 'airflow'\n\ndef generate_embeddings(text):\n    response = openai.Embedding.create(input=text, engine='text-embedding-ada-002')\n    return response['data'][0]['embedding']\n\ndef insert_vector_to_db(embedding, document_id):\n    conn = psycopg2.connect(\n        host=db_host,\n        port=db_port,\n        dbname=db_name,\n        user=db_user,\n        password=db_password\n    )\n    cursor = conn.cursor()\n    \n    query = \"\"\"\n    INSERT INTO vectors (document_id, embedding)\n    VALUES (%s, %s)\n    ON CONFLICT (document_id) DO UPDATE SET embedding = EXCLUDED.embedding;\n    \"\"\"\n    cursor.execute(query, (document_id, embedding))\n    conn.commit()\n    cursor.close()\n    conn.close()\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'vector_embedding_dag',\n    default_args=default_args,\n    description='A DAG to generate vector embeddings for text data and store them in a PostgreSQL database.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to generate embeddings\ngenerate_embeddings_task = PythonOperator(\n    task_id='generate_embeddings',\n    python_callable=generate_embeddings,\n    op_kwargs={'text': 'Hello, world!'},\n    dag=dag,\n)\n\n# Task to insert embeddings into PostgreSQL\ninsert_embeddings_task = PythonOperator(\n    task_id='insert_embeddings',\n    python_callable=insert_vector_to_db,\n    op_kwargs={\n        'embedding': generate_embeddings('Hello, world!'),\n        'document_id': 1\n    },\n    dag=dag,\n)\n\n# Task to trigger the embedding generation and insertion\ntrigger_embedding_task = PythonOperator(\n    task_id='trigger_embedding',\n    python_callable=lambda: None,\n    dag=dag,\n    on_success_callback=generate_embeddings_task.set_upstream,\n    on_failure_callback=generate_embeddings_task.set_downstream,\n    on_retry_callback=generate_embeddings_task.set_downstream,\n)\n\n# Task to trigger the insertion of embeddings\ntrigger_insertion_task = PythonOperator(\n    task_id='trigger_insertion',\n    python_callable=lambda: None,\n    dag=dag,\n    on_success_callback=insert_embeddings_task.set_upstream,\n    on_failure_callback=insert_embeddings_task.set_downstream,\n    on_retry_callback=insert_embeddings_task.set_downstream,\n)\n\n# Dependencies\ngenerate_embeddings_task >> trigger_embedding_task\ntrigger_embedding_task >> trigger_insertion_task"}], "metadata": {"file_name": "dag_989_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981444.3679044}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that leverages Databricks for scalable data processing, showing how to dynamically create jobs, run notebooks, and manage computational resources for data transformation tasks. Implement a workflow that demonstrates flexible cluster configuration and notebook-based data preparation strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import (\n    DatabricksSubmitOperator,\n    DatabricksRunNotebookOperator,\n)\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'databricks_data_pipeline',\n    default_args=default_args,\n    description='A simple data engineering pipeline using Databricks',\n    schedule_interval=None,\n)\n\n# Define the task to submit a new job\nsubmit_job_task = DatabricksSubmitOperator(\n    task_id='submit_job',\n    databricks_conn_id='databricks_default',\n    notebook_path='/Users/username/transform_data.py',\n    parameters={'input_table': 'my_input_table', 'output_table': 'my_output_table'},\n    spark_version='7.4.x',\n    num_workers=10,\n    driver_memory='8G',\n    executor_cores=4,\n    execution_timeout=60*60,\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Define the task to run a notebook\nrun_notebook_task = DatabricksRunNotebookOperator(\n    task_id='run_notebook',\n    databricks_conn_id='databricks_default',\n    notebook_path='/Users/username/data_preparation.ipynb',\n    parameters={'input_table': 'my_input_table', 'output_table': 'my_output_table'},\n    dag=dag,\n)\n\n# Set up the dependencies between tasks\nsubmit_job_task >> run_notebook_task"}], "metadata": {"file_name": "dag_1555_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981444.3679128}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task chaining and cross-task communication by translating text and then accessing the translation result through XCom. Demonstrate how different operators can interact and pass data between tasks in a sequential workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': days_ago(1),\n}\n\ndag = DAG(\n    'translate_and_access_xcom',\n    default_args=default_args,\n    description='A simple DAG for demonstrating task chaining and cross-task communication using XCom.',\n    schedule_interval=None,\n)\n\n# Task to translate text from English to French\ndef translate_text(text):\n    # Simulate translation logic (replace with actual translation API call)\n    return f\"Translated: {text} -> French\"\n\nt1 = PythonOperator(\n    task_id='translate_task',\n    python_callable=translate_text,\n    op_kwargs={'text': 'Hello, World!'},\n    dag=dag,\n)\n\n# Task to access the translation result through XCom\ndef access_translation_result():\n    # Retrieve the translation result from XCom\n    translation_result = t1.output\n    print(f\"Translation Result: {translation_result}\")\n\nt2 = PythonOperator(\n    task_id='access_translation_task',\n    python_callable=access_translation_result,\n    dag=dag,\n)\n\n# Set up the task dependencies\nt1 >> t2"}], "metadata": {"file_name": "dag_1297_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981444.36792}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me with a Python code to filter and sum a list of numbers based on a condition?"}, {"role": "assistant", "content": "# Define the list of numbers\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Define the condition (e.g., even numbers)\ncondition = lambda x: x % 2 == 0\n\n# Use list comprehension to filter the list based on the condition\neven_numbers = [num for num in numbers if condition(num)]\n\n# Calculate the sum of the filtered list\nsum_of_evens = sum(even_numbers)\n\nprint(\"The sum of even numbers is:\", sum_of_evens)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981444.3679252}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates Google Cloud Spanner's operational capabilities, including instance deployment, database schema evolution, and idempotent database modifications. Develop a comprehensive example that highlights different administrative tasks for cloud database management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.spanner_operator import (\n    CreateInstanceOperator,\n    DeleteInstanceOperator,\n    CreateDatabaseOperator,\n    UpdateDatabaseSchemaOperator,\n    ExecuteSqlQueryOperator,\n)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'google_cloud_spanner_dag',\n    default_args=default_args,\n    description='DAG for managing Google Cloud Spanner instances and databases',\n    schedule_interval=None,\n)\n\n# Task to create a new instance\ncreate_instance_task = CreateInstanceOperator(\n    task_id='create_instance',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    configuration={\n        'config': {\n            'instance_type': 'n1-standard-1',\n            'node_count': 1,\n            'storage_size_gb': 100,\n        },\n    },\n    location='us-central1',\n)\n\n# Task to create a database within the instance\ncreate_database_task = CreateDatabaseOperator(\n    task_id='create_database',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    database_id='your-database-id',\n    ddl_statement='''\n        CREATE TABLE users (\n            user_id INT64 NOT NULL PRIMARY KEY,\n            name STRING(100) NOT NULL,\n            email STRING(100) NOT NULL UNIQUE\n        );\n    ''',\n)\n\n# Task to update the schema of the database\nupdate_schema_task = UpdateDatabaseSchemaOperator(\n    task_id='update_schema',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    database_id='your-database-id',\n    ddl_statement='''\n        ALTER TABLE users ADD COLUMN age INT64;\n    ''',\n)\n\n# Task to execute a SQL query\nexecute_sql_query_task = ExecuteSqlQueryOperator(\n    task_id='execute_sql_query',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    database_id='your-database-id',\n    sql='SELECT * FROM users WHERE age > 30;',\n)\n\n# Task to delete the database\ndelete_database_task = DeleteDatabaseOperator(\n    task_id='delete_database',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n    database_id='your-database-id',\n)\n\n# Task to delete the instance\ndelete_instance_task = DeleteInstanceOperator(\n    task_id='delete_instance',\n    project_id='your-project-id',\n    instance_id='your-instance-id',\n)"}], "metadata": {"file_name": "dag_909_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981444.367938}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that demonstrates multiple generative AI model interactions using Vertex AI, showcasing text generation, embedding, and multimodal processing capabilities.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "bash\nairflow initdb"}], "metadata": {"file_name": "dag_934_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981575.138048}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud file management using Azure operators. Develop a DAG that illustrates how to programmatically upload and remove files from a remote storage system.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.storage import AzureBlobStorageHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'azure_blob_storage_example',\n    default_args=default_args,\n    description='An example DAG for uploading and removing files from Azure Blob Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the function to upload a file to Azure Blob Storage\ndef upload_file_to_azure():\n    hook = AzureBlobStorageHook(azure_conn_id='your_azure_connection')\n    blob_name = 'example.txt'\n    local_path = '/path/to/your/local/file.txt'\n    container_name = 'your_container_name'\n\n    hook.upload_blob(container_name, blob_name, local_path)\n    print(f\"File {local_path} uploaded to Azure Blob Storage as {blob_name}\")\n\n# Define the function to remove a file from Azure Blob Storage\ndef remove_file_from_azure():\n    hook = AzureBlobStorageHook(azure_conn_id='your_azure_connection')\n    blob_name = 'example.txt'\n    container_name = 'your_container_name'\n\n    hook.delete_blob(container_name, blob_name)\n    print(f\"File {blob_name} removed from Azure Blob Storage\")\n\n# Create tasks\nupload_task = PythonOperator(\n    task_id='upload_file',\n    python_callable=upload_file_to_azure,\n    dag=dag,\n)\n\nremove_task = PythonOperator(\n    task_id='remove_file',\n    python_callable=remove_file_from_azure,\n    dag=dag,\n)\n\n# Set up the task sequence\nupload_task >> remove_task"}], "metadata": {"file_name": "dag_1750_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.138062}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a machine learning pipeline that trains a K-Nearest Neighbors classification model using Amazon SageMaker and deploys it as a real-time prediction endpoint. The pipeline should handle data upload, model training, endpoint deployment, and automatic cleanup of resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateModelOperator,\n    SageMakerDeployEndpointOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sagemaker import SageMakerDeleteEndpointOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker import SageMakerBatchTransformOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker import SageMakerUpdateEndpointOperator\nfrom airflow.utils.dates import datetime\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\n# DAG configuration\ndag = DAG(\n    'knn_classification_pipeline',\n    default_args=default_args,\n    description='A pipeline to train a KNN classification model using Amazon SageMaker',\n    schedule_interval=None,  # Set to None if you want to run manually\n)\n\n# Task to upload data to S3\nupload_data_task = BashOperator(\n    task_id='upload_data_to_s3',\n    bash_command='aws s3 cp /path/to/your/data.csv s3://your-s3-bucket/knn-data/',\n    dag=dag,\n)\n\n# Task to create a SageMaker model\ncreate_model_task = SageMakerCreateModelOperator(\n    task_id='create_knn_model',\n    model_name='knn-model',\n    role='arn:aws:iam::your-account-id:role/AirflowRole',  # Replace with your IAM role ARN\n    image_uri='sagemaker-image-url',  # Replace with your SageMaker image URL\n    instance_type='ml.m5.large',\n    instance_count=1,\n    input_config={\n        'DataSource': {\n            'S3DataSource': {\n                'S3Uri': f's3://{YOUR_S3_BUCKET}/knn-data/',\n                'S3DataDistributionType': 'FullyReplicated'\n            }\n        },\n        'ContentType': 'text/csv',\n        'CompressionType': 'None'\n    },\n    output_config={\n        'S3OutputPath': f's3://{YOUR_S3_BUCKET}/knn-model-output/'\n    },\n    transform_input={\n        'DataSource': {\n            'S3DataSource': {\n                'S3Uri': f's3://{YOUR_S3_BUCKET}/knn-data/',\n                'S3DataDistributionType': 'FullyReplicated'\n            }\n        },\n        'ContentType': 'text/csv',\n        'CompressionType': 'None'\n    },\n    transform_output={\n        'S3OutputPath': f's3://{YOUR_S3_BUCKET}/knn-transform-output/'\n    },\n    transform_job_name='knn-transform-job',\n    max_runtime_in_seconds=3600,\n    enable_network_isolation=False,\n    enable_inter_container_traffic_encryption=True,\n    container_entry_point=['python', '/opt/ml/code/train.py'],\n    environment={\n        'MODEL_NAME': 'knn-model',\n        'DATA_PATH': '/opt/ml/input/data',\n        'OUTPUT_PATH': '/opt/ml/output/model',\n        'TRANSFORM_OUTPUT_PATH': '/opt/ml/output/transformed-data'\n    },\n    tags=[\n        {'Key': 'Environment', 'Value': 'Production'}\n    ],\n    dag=dag,\n)\n\n# Task to deploy the SageMaker model\ndeploy_endpoint_task = SageMakerDeployEndpointOperator(\n    task_id='deploy_knn_model',\n    endpoint_name='knn-endpoint',\n    model_name='knn-model',\n    role='arn:aws:iam::your-account-id:role/AirflowRole',  # Replace with your IAM role ARN\n    initial_instance_count=1,\n    instance_type='ml.m5.large',\n    traffic_distribution={\n        'HTTP': 1.0\n    },\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Task to update the SageMaker endpoint\nupdate_endpoint_task = SageMakerUpdateEndpointOperator(\n    task_id='update_knn_model',\n    endpoint_name='knn-endpoint',\n    model_name='knn-model',\n    role='arn:aws:iam::your-account-id:role/AirflowRole',  # Replace with your IAM role ARN\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Task to delete the SageMaker endpoint\ndelete_endpoint_task = SageMakerDeleteEndpointOperator(\n    task_id='delete_knn_model',\n    endpoint_name='knn-endpoint',\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Task to clean up the S3 bucket\ncleanup_bucket_task = BashOperator(\n    task_id='cleanup_s3_bucket',\n    bash_command=f'aws s3 rm --recursive s3://{YOUR_S3_BUCKET}/knn-data/ && aws s3 rm --recursive s3://{YOUR_S3_BUCKET}/knn-model-output/ && aws s3 rm --recursive s3://{YOUR_S3_BUCKET}/knn-transform-output/',\n    dag=dag,\n)\n\n# Dependencies\nupload_data_task >> create_model_task >> deploy_endpoint_task >> update_endpoint_task >> delete_endpoint_task >> cleanup_bucket_task"}], "metadata": {"file_name": "dag_97_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981575.1380785}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data pipeline that showcases vector embedding ingestion with flexible metadata handling and batch processing capabilities. Create a demonstration of how to programmatically insert vector data into a vector database using Airflow's integration with Pinecone.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pinecone\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='vector_embedding_dag',\n    description='Ingest vector embeddings from text data',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args,\n) as dag:\n\n    # Task to load text data\n    def load_text_data():\n        # Replace this with actual data loading logic\n        return [\"This is a sample text.\", \"Another sample text.\"]\n\n    load_text_task = PythonOperator(\n        task_id='load_text_data',\n        python_callable=load_text_data,\n        provide_context=True,\n    )\n\n    # Task to process text data\n    def process_text_data(text):\n        vectorizer = TfidfVectorizer()\n        X = vectorizer.fit_transform([text])\n        return X.toarray()\n\n    process_text_task = PythonOperator(\n        task_id='process_text_data',\n        python_callable=process_text_data,\n        provide_context=True,\n    )\n\n    # Task to store vector data in Pinecone\n    def store_vector_data(vector_data):\n        pinecone.init(api_key=\"YOUR_PINECONE_API_KEY\", environment=\"us-west1\")\n        index_name = \"your_index_name\"\n        collection_name = \"your_collection_name\"\n\n        if not pinecone.Index(index_name).exists():\n            pinecone.create_index(index_name, dimension=len(vector_data[0]))\n\n        pinecone.upsert(collection_name=collection_name, data_points=[{\"id\": i, \"values\": v} for i, v in enumerate(vector_data)])\n\n    store_vector_task = PythonOperator(\n        task_id='store_vector_data',\n        python_callable=store_vector_data,\n        provide_context=True,\n    )\n\n    # Task to trigger the entire pipeline\n    def trigger_pipeline():\n        load_text_task.execute(context=None)\n        process_text_task.execute(context=None)\n        store_vector_task.execute(context=None)\n\n    trigger_pipeline_task = PythonOperator(\n        task_id='trigger_pipeline',\n        python_callable=trigger_pipeline,\n    )"}], "metadata": {"file_name": "dag_639_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.1380918}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the complete lifecycle of a Cloud Data Fusion instance, including creation, pipeline deployment, execution, and cleanup. Implement a comprehensive demonstration of cloud data infrastructure management using Google Cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.datafusion import (\n    CreateDataFusionInstanceOperator,\n    DeployPipelineOperator,\n    ExecutePipelineOperator,\n    DeleteDataFusionInstanceOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'cloud_data_fusion_dag',\n    default_args=default_args,\n    description='DAG for managing Cloud Data Fusion instance',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Create Cloud Data Fusion Instance\ncreate_instance_task = CreateDataFusionInstanceOperator(\n    task_id='create_data_fusion_instance',\n    project_id='your-project-id',\n    location='us-central1',\n    instance_name='my-data-fusion-instance',\n    config={\n        'environment_config': {\n            'service_account': {\n                'email': 'data-fusion-sa@your-project-id.iam.gserviceaccount.com',\n                'key_file_path': '/path/to/data-fusion-sa-key.json',\n            },\n        },\n    },\n)\n\n# Step 2: Deploy Pipeline\ndeploy_pipeline_task = DeployPipelineOperator(\n    task_id='deploy_pipeline',\n    project_id='your-project-id',\n    location='us-central1',\n    instance_name='my-data-fusion-instance',\n    pipeline_definition='/path/to/your/pipeline.yaml',\n    pipeline_name='my-pipeline',\n)\n\n# Step 3: Execute Pipeline\nexecute_pipeline_task = ExecutePipelineOperator(\n    task_id='execute_pipeline',\n    project_id='your-project-id',\n    location='us-central1',\n    instance_name='my-data-fusion-instance',\n    pipeline_name='my-pipeline',\n    parameters={'param1': 'value1', 'param2': 'value2'},\n)\n\n# Step 4: Cleanup\ndelete_instance_task = DeleteDataFusionInstanceOperator(\n    task_id='delete_data_fusion_instance',\n    project_id='your-project-id',\n    location='us-central1',\n    instance_name='my-data-fusion-instance',\n)\n\n# Define dependencies\ncreate_instance_task >> deploy_pipeline_task >> execute_pipeline_task >> delete_instance_task"}], "metadata": {"file_name": "dag_1617_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381023}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms raw geographical and user data into structured database tables with multiple query scenarios. Design a workflow that handles data insertion, supports parameterized queries, and demonstrates different methods of database interaction using MsSql operators and hooks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\nimport pandas as pd\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'geographical_user_data_pipeline',\n    default_args=default_args,\n    description='A data processing pipeline to transform raw geographical and user data into structured database tables.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Function to load data from CSV files\ndef load_data_from_csv():\n    # Load data from CSV files\n    df_geography = pd.read_csv('path_to_geography.csv')\n    df_users = pd.read_csv('path_to_users.csv')\n\n    return df_geography, df_users\n\n# Function to insert data into database\ndef insert_data_into_database(df_geography, df_users):\n    # Hook to connect to SQL Server\n    hook = MsSqlHook(mssql_conn_id='your_mssql_connection_id')\n\n    # Insert data into Geography table\n    geography_table_name = 'GeographyTable'\n    geography_columns = ['Column1', 'Column2', 'Column3']\n    geography_df = df_geography[geography_columns]\n    hook.insert_rows(table=geography_table_name, rows=geography_df.to_dict(orient='records'))\n\n    # Insert data into Users table\n    users_table_name = 'UsersTable'\n    users_columns = ['Column1', 'Column2', 'Column3']\n    users_df = df_users[users_columns]\n    hook.insert_rows(table=users_table_name, rows=users_df.to_dict(orient='records'))\n\n# Task to load data from CSV files\nload_data_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data_from_csv,\n    dag=dag,\n)\n\n# Task to insert data into database\ninsert_data_task >> PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data_into_database,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_630_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381123}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an enterprise-level data extraction and cloud storage transfer process that demonstrates best practices for moving relational database content to a cloud storage platform. Construct an Airflow workflow that handles data export from Oracle, transfers the data to Google Cloud Storage in CSV format, and ensures proper resource management through automated bucket creation and deletion.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.oracle.operators.oracle import OracleOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.dates import days_ago\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'oracle_to_gcs_dag',\n    description='Extract data from Oracle and transfer it to Google Cloud Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args,\n    catchup=False\n)\n\n# Step 1: Extract data from Oracle using OracleOperator\nextract_data_task = OracleOperator(\n    task_id='extract_data_from_oracle',\n    oracle_conn_id='oracle_default',  # Replace with your Oracle connection ID\n    sql='SELECT * FROM your_table',  # Replace with your SQL query\n    dag=dag\n)\n\n# Step 2: Transfer data to Google Cloud Storage using GCSToGCSOperator\ntransfer_data_task = GCSToGCSOperator(\n    task_id='transfer_data_to_gcs',\n    source_bucket_name='your_source_bucket',  # Replace with your source bucket name\n    destination_bucket_name='your_destination_bucket',  # Replace with your destination bucket name\n    source_object_name='data.csv',  # Replace with your source object name\n    destination_object_name='data.csv',  # Replace with your destination object name\n    gcp_conn_id='google_cloud_default',  # Replace with your Google Cloud connection ID\n    dag=dag\n)\n\n# Step 3: Ensure proper resource management by creating and deleting buckets\ncreate_bucket_task = GCSHook.create_bucket(\n    task_id='create_bucket',\n    bucket_name='your_source_bucket',  # Replace with your source bucket name\n    project_id='your_project_id',  # Replace with your project ID\n    location='us-central1'  # Replace with your location\n)\n\ndelete_bucket_task = GCSHook.delete_bucket(\n    task_id='delete_bucket',\n    bucket_name='your_source_bucket',  # Replace with your source bucket name\n    project_id='your_project_id',  # Replace with your project ID\n    location='us-central1'  # Replace with your location\n)\n\n# Define dependencies between tasks\nextract_data_task >> transfer_data_task >> [create_bucket_task, delete_bucket_task]"}], "metadata": {"file_name": "dag_221_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381218}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of AWS SageMaker notebook resources, illustrating how to programmatically control cloud computing infrastructure through defined task dependencies. Develop a step-by-step orchestration that shows creating, manipulating, and cleaning up a computational notebook instance.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n}\n\ndag = DAG(\n    'sagemaker_notebook_dag',\n    default_args=default_args,\n    description='A simple DAG to manage SageMaker Notebook instances',\n    schedule_interval=None,\n)\n\n# Create a SageMaker notebook instance\ncreate_notebook_instance_task = SageMakerCreateNotebookInstanceOperator(\n    task_id='create_notebook_instance',\n    instance_type='ml.t2.micro',\n    role='arn:aws:iam::123456789012:role/AirflowSageMakerRole',\n    image_name='aws/sagemaker-notebooks:pytorch-training',\n    subnet_id='subnet-12345678',\n    security_group_ids=['sg-12345678'],\n    dag=dag,\n)\n\n# Delete the SageMaker notebook instance after some time\ndelete_notebook_instance_task = SageMakerDeleteNotebookInstanceOperator(\n    task_id='delete_notebook_instance',\n    notebook_instance_name='my_notebook_instance',\n    wait_for_completion=True,\n    dag=dag,\n)\n\n# Define the task dependencies\ncreate_notebook_instance_task >> delete_notebook_instance_task"}], "metadata": {"file_name": "dag_1126_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381292}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates calling stored procedures with different parameter passing techniques. Create a DAG that showcases input and output parameter handling for database operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.oracle.hooks.oracle import OracleHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'retries': 1,\n    'retry_delay': 5\n}\n\n# Define the DAG\ndag = DAG(\n    'oracle_stored_procedures_dag',\n    default_args=default_args,\n    description='DAG to demonstrate Oracle stored procedure calls',\n    schedule_interval=None,\n)\n\n# Task to call a stored procedure with positional parameters\ndef call_stored_procedure_with_positional_params():\n    hook = OracleHook()\n    cursor = hook.get_conn().cursor()\n    cursor.callproc('your_package.your_procedure', ('param1', 'param2'))\n    cursor.close()\n\ncall_stored_procedure_with_positional_params_task = PythonOperator(\n    task_id='call_stored_procedure_with_positional_params',\n    python_callable=call_stored_procedure_with_positional_params,\n    dag=dag,\n)\n\n# Task to call a stored procedure with keyword parameters\ndef call_stored_procedure_with_keyword_params():\n    hook = OracleHook()\n    cursor = hook.get_conn().cursor()\n    cursor.callproc('your_package.your_procedure', {'param1': 'value1', 'param2': 'value2'})\n    cursor.close()\n\ncall_stored_procedure_with_keyword_params_task = PythonOperator(\n    task_id='call_stored_procedure_with_keyword_params',\n    python_callable=call_stored_procedure_with_keyword_params,\n    dag=dag,\n)\n\n# Task to call a stored procedure with IN OUT parameters\ndef call_stored_procedure_with_in_out_params():\n    hook = OracleHook()\n    cursor = hook.get_conn().cursor()\n    cursor.callproc('your_package.your_procedure', (None, None))\n    result = cursor.fetchone()\n    print(f'Result from IN OUT parameter: {result}')\n    cursor.close()\n\ncall_stored_procedure_with_in_out_params_task = PythonOperator(\n    task_id='call_stored_procedure_with_in_out_params',\n    python_callable=call_stored_procedure_with_in_out_params,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1770_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381383}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need a Python code snippet to compare the similarity between two text documents using the cosine similarity method."}, {"role": "assistant", "content": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the text from the files\nwith open('document1.txt', 'r') as file:\n    doc1 = file.read()\n\nwith open('document2.txt', 'r') as file:\n    doc2 = file.read()\n\n# Create a CountVectorizer object\nvectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the documents\nX = vectorizer.fit_transform([doc1, doc2])\n\n# Calculate the cosine similarity matrix\nsimilarity_matrix = cosine_similarity(X)\n\n# Print the similarity score\nprint(\"Cosine Similarity:\", similarity_matrix[0][1])"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981575.1381454}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that demonstrates Google Cloud Storage bucket management, including creating, copying, and deleting files and buckets. Create a DAG that showcases file transfer operations between two dynamically generated cloud storage buckets.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_storage_migration_dag',\n    default_args=default_args,\n    description='A DAG for managing Google Cloud Storage buckets and performing file transfers.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the source and destination bucket names\nsource_bucket_name = f'source-bucket-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\ndestination_bucket_name = f'destination-bucket-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n\n# Create the source bucket\ncreate_source_bucket_task = GoogleCloudStorageHook.create_bucket(\n    task_id='create_source_bucket',\n    bucket=source_bucket_name,\n    project_id='your-project-id',  # Replace with your GCP project ID\n)\n\n# Upload a file to the source bucket\nupload_file_task = GoogleCloudStorageHook.upload(\n    task_id='upload_file_to_source_bucket',\n    bucket=source_bucket_name,\n    object=f'example.txt',\n    filename='/path/to/your/example.txt',  # Replace with the path to your local file\n)\n\n# Copy the file from the source bucket to the destination bucket\ncopy_file_task = GCSToGCSOperator(\n    task_id='copy_file_from_source_to_destination',\n    source_bucket=source_bucket_name,\n    source_object='example.txt',\n    destination_bucket=destination_bucket_name,\n    destination_object='example.txt',\n)\n\n# Delete the source bucket\ndelete_source_bucket_task = GoogleCloudStorageHook.delete_bucket(\n    task_id='delete_source_bucket',\n    bucket=source_bucket_name,\n    project_id='your-project-id',  # Replace with your GCP project ID\n)\n\n# Delete the destination bucket\ndelete_destination_bucket_task = GoogleCloudStorageHook.delete_bucket(\n    task_id='delete_destination_bucket',\n    bucket=destination_bucket_name,\n    project_id='your-project-id',  # Replace with your GCP project ID\n)\n\n# Define the dependencies between tasks\ncreate_source_bucket_task >> upload_file_task >> copy_file_task >> [delete_source_bucket_task, delete_destination_bucket_task]"}], "metadata": {"file_name": "dag_880_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381557}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates safe data transaction practices by implementing a complete transaction management scenario in Google Cloud Datastore. Construct a workflow that shows how to start a transaction and then safely roll it back without persisting potentially unwanted data modifications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.hooks.datastore import GoogleCloudDatastoreHook\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'datastore_transaction_example',\n    default_args=default_args,\n    description='A DAG to manage transactions in Google Cloud Datastore',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Function to insert data into Datastore\ndef insert_data():\n    hook = GoogleCloudDatastoreHook()\n    key = hook.key('my_kind', 'my_id')\n    entity = {'name': 'John Doe'}\n    hook.put(key=key, entity=entity)\n    print(f\"Inserted {entity} into Datastore\")\n\n# Function to delete data from Datastore\ndef delete_data():\n    hook = GoogleCloudDatastoreHook()\n    key = hook.key('my_kind', 'my_id')\n    hook.delete(key=key)\n    print(f\"Deleted {key} from Datastore\")\n\n# Task to start a transaction\nstart_transaction_task = PythonOperator(\n    task_id='start_transaction',\n    python_callable=start_transaction,\n    dag=dag,\n)\n\n# Task to insert data within the transaction\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag,\n    trigger_rule='all_success',\n)\n\n# Task to delete data within the transaction\ndelete_data_task = PythonOperator(\n    task_id='delete_data',\n    python_callable=delete_data,\n    dag=dag,\n    trigger_rule='all_success',\n)\n\n# Task to rollback the transaction if any error occurs\nrollback_transaction_task = PythonOperator(\n    task_id='rollback_transaction',\n    python_callable=rollback_transaction,\n    dag=dag,\n    trigger_rule='all_failed',\n)\n\n# Set dependencies between tasks\nstart_transaction_task >> insert_data_task >> delete_data_task >> rollback_transaction_task"}], "metadata": {"file_name": "dag_876_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381674}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data extraction and transfer system that demonstrates extracting data from a PostgreSQL database and moving it to a Google Cloud Storage bucket with full infrastructure provisioning. The solution should highlight how to programmatically create cloud resources, configure database connections, and perform secure data migrations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_migration_dag',\n    default_args=default_args,\n    description='Automated data extraction and transfer from PostgreSQL to Google Cloud Storage',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Create a Google Cloud Storage bucket\ncreate_bucket_task = PythonOperator(\n    task_id='create_bucket',\n    python_callable=create_bucket,\n    op_kwargs={\n        'bucket_name': 'my_data_bucket',\n        'project_id': 'your_gcp_project_id'\n    },\n    dag=dag,\n)\n\n# Step 2: Set up a PostgreSQL connection\npostgres_conn_id = 'postgres_default'\n\n# Step 3: Perform a data migration from PostgreSQL to Google Cloud Storage\ndef migrate_data():\n    postgres_hook = PostgresHook(postgres_conn_id)\n    storage_hook = GoogleCloudStorageHook(gcp_conn_id='google_cloud_default')\n\n    # Query data from PostgreSQL\n    query = \"SELECT * FROM my_table\"\n    df = postgres_hook.get_pandas_df(query)\n\n    # Upload data to Google Cloud Storage\n    blob_name = f\"migrated_data_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\"\n    storage_hook.load_string(df.to_csv(index=False), blob_name, bucket_name=bucket_name)\n\nmigrate_data_task = PythonOperator(\n    task_id='migrate_data',\n    python_callable=migrate_data,\n    dag=dag,\n)\n\n# Define dependencies\ncreate_bucket_task >> migrate_data_task"}], "metadata": {"file_name": "dag_576_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381783}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can wait for specific conditions before triggering downstream tasks. Create a DAG that illustrates time-based, file-based, bash command, and Python function sensors to help learners understand conditional task execution.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.http import HttpSensor\nfrom airflow.sensors.python import PythonSensor\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'sensor_example_dag',\n    description='A DAG demonstrating various sensor types in Apache Airflow',\n    schedule_interval=timedelta(hours=1),\n    default_args=default_args\n)\n\n# Task to check if a file exists\ndef file_exists(file_path):\n    return os.path.exists(file_path)\n\nfile_sensor = FileSensor(\n    task_id='check_file_exists',\n    filepath='/path/to/your/file.txt',\n    dag=dag,\n    poke_interval=60,\n    timeout=300\n)\n\n# Task to check if a HTTP request returns a status code of 200\nhttp_sensor = HttpSensor(\n    task_id='check_http_status',\n    endpoint='https://api.example.com/data',\n    method='GET',\n    dag=dag,\n    poke_interval=60,\n    timeout=300\n)\n\n# Task to execute a Bash command\nbash_command_task = BashOperator(\n    task_id='execute_bash_command',\n    bash_command='echo \"Hello from Bash\"',\n    dag=dag\n)\n\n# Task to call a Python function\npython_function_task = PythonOperator(\n    task_id='call_python_function',\n    python_callable=file_exists,\n    op_kwargs={'file_path': '/path/to/your/file.txt'},\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nfile_sensor >> http_sensor >> bash_command_task >> python_function_task"}], "metadata": {"file_name": "dag_1049_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381884}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that monitors a specific Azure CosmosDB collection for document availability and triggers a subsequent document insertion task. Implement a dependency-based workflow that ensures document existence before performing write operations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cosmos_db_monitoring_pipeline',\n    default_args=default_args,\n    description='A pipeline to monitor Azure CosmosDB collection for document availability and trigger document insertion.',\n    schedule_interval=None,  # Set to None if you want to run it manually\n)\n\n# Function to check if a document exists in CosmosDB\ndef check_document_exists(cosmos_client, database_name, container_name, document_id):\n    try:\n        cosmos_client.read_item(container_name, document_id)\n        return True\n    except Exception as e:\n        print(f\"Document {document_id} does not exist: {e}\")\n        return False\n\n# Task to check document existence\ncheck_document_task = PythonOperator(\n    task_id='check_document_existence',\n    python_callable=check_document_exists,\n    op_kwargs={\n        'cosmos_client': cosmos_client,\n        'database_name': 'your_database_name',\n        'container_name': 'your_container_name',\n        'document_id': 'your_document_id'\n    },\n    dag=dag\n)\n\n# Task to insert document into CosmosDB if it does not exist\ninsert_document_task = SparkSubmitOperator(\n    task_id='insert_document',\n    application='/path/to/your/spark_script.py',  # Replace with your Spark script path\n    conf={'spark.driver.extraClassPath': '/path/to/your/spark_jar.jar'},  # Replace with your Spark JAR path\n    dag=dag\n)\n\n# Dependency between tasks\ncheck_document_task >> insert_document_task"}], "metadata": {"file_name": "dag_968_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981575.1381996}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code snippet to calculate the factorial of a number using recursion?"}, {"role": "assistant", "content": "def factorial(n):\n    # Base case: factorial of 0 or 1 is 1\n    if n == 0 or n == 1:\n        return 1\n    else:\n        # Recursive case: n * factorial of (n-1)\n        return n * factorial(n-1)\n\n# Example usage:\nnumber = 5\nresult = factorial(number)\nprint(f\"The factorial of {number} is {result}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981575.1382043}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data monitoring workflow that extracts specific column data from a SQL table and automatically sends the results to a designated Slack channel for real-time reporting and team awareness.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.slack.hooks.slack import SlackHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_monitoring_dag',\n    default_args=default_args,\n    description='Extracts specific column data from a SQL table and sends it to a designated Slack channel.',\n    schedule_interval='@daily',\n)\n\n# Define the SQL query to extract specific column data\nsql_query = \"\"\"\nSELECT column_name FROM your_table_name;\n\"\"\"\n\n# Create a Postgres hook to execute the SQL query\npostgres_hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n\n# Execute the SQL query and get the result\nresult = postgres_hook.get_pandas_df(sql_query)\n\n# Create a Slack hook to send the results to a designated Slack channel\nslack_hook = SlackHook(webhook_url='https://hooks.slack.com/services/your_slack_webhook_url')\n\n# Define a Python operator to send the results to the Slack channel\nsend_results_to_slack = PythonOperator(\n    task_id='send_results_to_slack',\n    python_callable=lambda: slack_hook.send_message(text=result.to_string()),\n    dag=dag,\n)\n\n# Set up the dependencies between tasks\nextract_data >> send_results_to_slack"}], "metadata": {"file_name": "dag_651_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.1382113}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow that demonstrates transferring a local text file to an Amazon S3 bucket using Airflow, showcasing file management and cloud storage integration. Create a DAG that generates a sample text file, uploads it to S3, and then performs cleanup operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='upload_to_s3_dag',\n    description='A simple DAG to upload a local text file to an S3 bucket',\n    default_args=default_args,\n    schedule_interval=None,  # Set this to your desired schedule interval\n) as dag:\n\n    # Task to generate a sample text file\n    generate_text_file = PythonOperator(\n        task_id='generate_text_file',\n        python_callable=generate_text_file_function,\n        op_kwargs={'file_path': '/tmp/sample.txt'},\n        dag=dag\n    )\n\n    # Task to upload the generated text file to S3\n    upload_to_s3 = PythonOperator(\n        task_id='upload_to_s3',\n        python_callable=upload_to_s3_function,\n        op_kwargs={\n            'bucket_name': 'your-s3-bucket-name',\n            'object_key': 'sample.txt',\n            'file_path': '/tmp/sample.txt'\n        },\n        dag=dag\n    )\n\n    # Task to clean up the temporary file\n    cleanup_temp_file = PythonOperator(\n        task_id='cleanup_temp_file',\n        python_callable=cleanup_temp_file_function,\n        op_kwargs={'file_path': '/tmp/sample.txt'},\n        dag=dag\n    )\n\n    # Define the dependencies between tasks\n    generate_text_file >> upload_to_s3 >> cleanup_temp_file"}], "metadata": {"file_name": "dag_1503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.1382213}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that validates and explores HTTP service connectivity through sensor checks and sequential HTTP request tasks. Implement a workflow that first checks service availability, then executes a series of HTTP interactions with different configuration parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.http_sensor import HttpSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'retries': 3,\n    'retry_delay': 5 * 60,  # 5 minutes\n}\n\ndag = DAG(\n    'http_service_validation_dag',\n    default_args=default_args,\n    description='DAG to validate and explore HTTP service connectivity',\n    schedule_interval=None,  # Set to None for testing purposes\n)\n\n# Sensor to check if the HTTP service is available\ncheck_http_service = HttpSensor(\n    task_id='check_http_service',\n    http_conn_id='http_default',  # Replace with your actual connection ID\n    endpoint='/healthz',  # Replace with your actual health check endpoint\n    timeout=10,  # Timeout in seconds\n    poke_interval=5,  # Interval between pokes in seconds\n    dag=dag,\n)\n\n# Sequential HTTP requests\ndef make_http_request(url):\n    from requests import get\n    response = get(url)\n    return response.status_code\n\nmake_http_request_task_1 = PythonOperator(\n    task_id='make_http_request_task_1',\n    python_callable=make_http_request,\n    op_kwargs={'url': 'http://example.com/api/data'},\n    dag=dag,\n)\n\nmake_http_request_task_2 = PythonOperator(\n    task_id='make_http_request_task_2',\n    python_callable=make_http_request,\n    op_kwargs={'url': 'http://example.com/api/data?param=value'},\n    dag=dag,\n)\n\nmake_http_request_task_3 = PythonOperator(\n    task_id='make_http_request_task_3',\n    python_callable=make_http_request,\n    op_kwargs={'url': 'http://example.com/api/data?param=value&another_param=new_value'},\n    dag=dag,\n)\n\n# Define the dependencies\ncheck_http_service >> [make_http_request_task_1, make_http_request_task_2, make_http_request_task_3]"}], "metadata": {"file_name": "dag_1346_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.138231}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data protection workflow that enables dynamic configuration of sensitive data detection rules using Google Cloud's Data Loss Prevention service. Create a pipeline that uploads custom dictionaries, defines information type patterns, and demonstrates how to programmatically manage data classification rules.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.dlp import DlpHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'data_protection',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'upload_custom_dictionaries',\n    default_args=default_args,\n    description='Upload custom dictionaries to DLP',\n    schedule_interval=None,\n)\n\ndef upload_dictionary():\n    dlp_hook = DlpHook()\n    dictionary = {\n        \"name\": \"MyCustomDictionary\",\n        \"dictionaryEntries\": [\n            {\"entryType\": \"STRING\", \"value\": \"SensitiveData\"},\n            {\"entryType\": \"NUMBER\", \"value\": \"1234567890\"}\n        ]\n    }\n    response = dlp_hook.create_custom_dictionary(dictionary)\n    print(response)\n\nupload_dictionary_task = PythonOperator(\n    task_id='upload_custom_dictionary',\n    python_callable=upload_dictionary,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1211_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.1382427}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a cloud-based data discovery pipeline that leverages Google Dataproc to execute a simple Hive query across a dynamically created multi-node cluster. Construct a demonstration of scalable, on-demand data infrastructure that can quickly spin up and down for lightweight analytical tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    GoogleCloudDataprocCreateClusterOperator,\n    GoogleCloudDataprocDeleteClusterOperator,\n)\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dataproc_hive_query',\n    default_args=default_args,\n    description='A simple DAG to demonstrate a cloud-based data discovery pipeline using Google Dataproc.',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create a Dataproc cluster\ncreate_cluster_task = GoogleCloudDataprocCreateClusterOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n    master_config={\n        'machine_type': 'n1-standard-4',\n        'num_instances': 2,\n    },\n    worker_config={\n        'machine_type': 'n1-standard-4',\n        'num_instances': 2,\n    },\n    init_actions_uris=[\n        'gs://path/to/init-script.sh',  # Replace with your init script URI\n    ],\n)\n\n# Run a Hive query\nhive_query_task = GoogleCloudDataprocSubmitJobOperator(\n    task_id='run_hive_query',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n    job={\n        'query': \"\"\"\n            SELECT * FROM my_table;\n        \"\"\",\n        'language': 'sql',\n    },\n)\n\n# Delete the Dataproc cluster\ndelete_cluster_task = GoogleCloudDataprocDeleteClusterOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n)\n\n# Define the dependencies between tasks\ncreate_cluster_task >> hive_query_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_1234_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.1382518}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I use AWS SDK for Python (Boto3) to filter and retrieve specific VPCs based on tags?"}, {"role": "assistant", "content": "import boto3\nfrom botocore.exceptions import ClientError"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981575.1382565}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates various Google Cloud Storage sensor techniques, showcasing synchronous and asynchronous methods for detecting file upload completions and object changes. Build a workflow that illustrates different monitoring strategies for cloud storage events and file transfer validations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.sensors.gcs import (\n    GCSObjectExistenceSensor,\n    GCSFileUploadCompletedSensor,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n}\n\ndag = DAG(\n    'gcp_storage_sensor_dag',\n    default_args=default_args,\n    description='DAG to demonstrate Google Cloud Storage sensor techniques',\n    schedule_interval=None,\n)\n\n# Sensor to check if a specific object exists in GCS\nobject_exists_task = GCSObjectExistenceSensor(\n    task_id='check_object_existence',\n    bucket_name='your_bucket_name',\n    object_name='your_object_name',\n    dag=dag,\n)\n\n# Sensor to check if a specific file has been uploaded to GCS\nfile_upload_completed_task = GCSFileUploadCompletedSensor(\n    task_id='check_file_upload_completion',\n    bucket_name='your_bucket_name',\n    object_name='your_object_name',\n    dag=dag,\n)\n\n# Transfer file from GCS to local\ntransfer_file_task = GCSToLocalOperator(\n    task_id='transfer_file',\n    source_bucket='your_bucket_name',\n    source_object='your_object_name',\n    destination_path='/path/to/local/directory/',\n    dag=dag,\n)\n\n# Monitoring strategy using GCSObjectExistenceSensor\nmonitoring_strategy_1 = GCSObjectExistenceSensor(\n    task_id='monitoring_strategy_1',\n    bucket_name='your_bucket_name',\n    object_name='your_object_name',\n    dag=dag,\n)\n\n# Monitoring strategy using GCSFileUploadCompletedSensor\nmonitoring_strategy_2 = GCSFileUploadCompletedSensor(\n    task_id='monitoring_strategy_2',\n    bucket_name='your_bucket_name',\n    object_name='your_object_name',\n    dag=dag,\n)\n\n# Task dependencies\nobject_exists_task >> file_upload_completed_task >> transfer_file_task\nmonitoring_strategy_1 >> monitoring_strategy_2"}], "metadata": {"file_name": "dag_1647_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.138265}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello Qwen, I want to know more about the use of conditional statements in Python. Can you provide me with some examples?"}, {"role": "assistant", "content": "x = 5\n   if x > 0:\n       print(\"x is positive\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981575.1382713}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated file monitoring and processing workflow that detects new files in a source directory and moves them to a destination directory using Docker containers. Create a system that checks for file availability, moves the first detected file, and then prints its contents.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom datetime import datetime\n\n# Define the DAG\ndag = DAG(\n    'file_monitoring_and_processing',\n    description='Automated file monitoring and processing workflow',\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n)\n\n# Task to monitor the source directory for new files\ndef check_for_new_files():\n    # This function should be implemented to check for new files in the source directory\n    pass\n\ncheck_for_new_files_task = PythonOperator(\n    task_id='check_for_new_files',\n    python_callable=check_for_new_files,\n    dag=dag,\n)\n\n# Task to move the first detected file to the destination directory using Docker containers\ndef move_file_to_destination(file_path):\n    # This function should be implemented to move the file to the destination directory using Docker containers\n    pass\n\nmove_file_to_destination_task = DockerOperator(\n    task_id='move_file_to_destination',\n    image='your_docker_image',  # Replace with your Docker image name\n    command=f'mv {file_path} /path/to/destination',  # Replace with the actual command to move the file\n    volumes=[\n        ('/path/to/source:/path/to/destination'),  # Replace with the actual paths\n    ],\n    dag=dag,\n)\n\n# Task to print the contents of the moved file\ndef print_file_contents(file_path):\n    # This function should be implemented to print the contents of the file\n    pass\n\nprint_file_contents_task = PythonOperator(\n    task_id='print_file_contents',\n    python_callable=print_file_contents,\n    op_args=[file_path],\n    dag=dag,\n)\n\n# Set up the dependencies between the tasks\ncheck_for_new_files_task >> move_file_to_destination_task >> print_file_contents_task"}], "metadata": {"file_name": "dag_452_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.138282}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential execution of Trino database operations, illustrating how to create schemas, define tables, insert data, and perform template-based and parameterized queries. The workflow should highlight the dependency chaining and error handling capabilities of Airflow with a Trino data source.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'trino_dag',\n    description='DAG to demonstrate Trino database operations',\n    schedule_interval=None,  # Set to None if you want to run it manually\n    default_args=default_args\n)\n\n# Function to create schema\ndef create_schema():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        CREATE SCHEMA IF NOT EXISTS my_schema;\n    \"\"\"\n    hook.run(query=query)\n\n# Function to create table\ndef create_table():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_schema.my_table (\n            id INT PRIMARY KEY,\n            name STRING,\n            age INT\n        );\n    \"\"\"\n    hook.run(query=query)\n\n# Function to insert data\ndef insert_data():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        INSERT INTO my_schema.my_table (id, name, age) VALUES (%s, %s, %s);\n    \"\"\"\n    data = [(1, 'Alice', 30), (2, 'Bob', 25)]\n    hook.run(query=query, parameters=data)\n\n# Function to perform template-based query\ndef template_based_query():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        SELECT * FROM my_schema.my_table WHERE age > {{ params.age }};\n    \"\"\"\n    params = {'age': 25}\n    hook.run(query=query, parameters=params)\n\n# Function to perform parameterized query\ndef parameterized_query():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        SELECT * FROM my_schema.my_table WHERE age > :age;\n    \"\"\"\n    params = {'age': 25}\n    hook.run(query=query, parameters=params)\n\n# Task definitions\ncreate_schema_task = PythonOperator(\n    task_id='create_schema',\n    python_callable=create_schema,\n    dag=dag\n)\n\ncreate_table_task = PythonOperator(\n    task_id='create_table',\n    python_callable=create_table,\n    dag=dag\n)\n\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag\n)\n\ntemplate_based_query_task = PythonOperator(\n    task_id='template_based_query',\n    python_callable=template_based_query,\n    dag=dag\n)\n\nparameterized_query_task = PythonOperator(\n    task_id='parameterized_query',\n    python_callable=parameterized_query,\n    dag=dag\n)\n\n# Dependency chaining\ncreate_schema_task >> create_table_task >> insert_data_task >> [template_based_query_task, parameterized_query_task]"}], "metadata": {"file_name": "dag_657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981575.1382933}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of creating cloud resources, performing a data transfer, and then cleaning up those resources. Develop a systematic workflow that demonstrates provisioning, data movement, and resource teardown in a cloud environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.microsoft.azure.hooks.azure_storage import AzureBlobStorageHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cloud_resource_management_dag',\n    default_args=default_args,\n    description='A DAG to manage cloud resources',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Provisioning AWS S3 bucket\naws_s3_bucket_provisioner = PythonOperator(\n    task_id='aws_s3_bucket_provisioner',\n    python_callable=provision_aws_s3_bucket,\n    dag=dag,\n    op_kwargs={\n        'bucket_name': 'my-aws-bucket'\n    }\n)\n\n# Step 2: Data transfer from AWS S3 to Google Cloud Storage\naws_s3_to_gcp_data_transfer = PythonOperator(\n    task_id='aws_s3_to_gcp_data_transfer',\n    python_callable=data_transfer_from_aws_s3_to_gcp,\n    dag=dag,\n    op_kwargs={\n        'source_bucket_name': 'my-aws-bucket',\n        'destination_bucket_name': 'my-gcp-bucket'\n    }\n)\n\n# Step 3: Cleaning up AWS S3 bucket\naws_s3_bucket_cleanup = PythonOperator(\n    task_id='aws_s3_bucket_cleanup',\n    python_callable=cleanup_aws_s3_bucket,\n    dag=dag,\n    op_kwargs={\n        'bucket_name': 'my-aws-bucket'\n    }\n)\n\n# Step 4: Cleaning up Google Cloud Storage bucket\ngcp_bucket_cleanup = PythonOperator(\n    task_id='gcp_bucket_cleanup',\n    python_callable=cleanup_gcp_bucket,\n    dag=dag,\n    op_kwargs={\n        'bucket_name': 'my-gcp-bucket'\n    }\n)\n\n# Define dependencies\naws_s3_bucket_provisioner >> aws_s3_to_gcp_data_transfer >> aws_s3_bucket_cleanup\naws_s3_to_gcp_data_transfer >> gcp_bucket_cleanup"}], "metadata": {"file_name": "dag_1579_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.1383097}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that transforms raw text data into dense vector representations using Cohere's embedding technology, preparing unstructured text for advanced natural language processing tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.cohere.hooks.cohere import CohereHook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cohere_embedding_dag',\n    default_args=default_args,\n    description='Transform raw text data into dense vector representations using Cohere',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\ndef embed_text(text):\n    hook = CohereHook(cohere_api_key=\"YOUR_COHERE_API_KEY\")\n    response = hook.embed(text=text, model=\"embeddings-english-v2\")\n    return response['embeddings']\n\n# Define the task to embed text\nembed_text_task = PythonOperator(\n    task_id='embed_text',\n    python_callable=embed_text,\n    op_kwargs={'text': \"Your raw text here\"},\n    dag=dag\n)\n\n# Define the task to store embeddings (this part depends on your storage system)\nstore_embeddings_task = PythonOperator(\n    task_id='store_embeddings',\n    python_callable=lambda x: print(f\"Stored embeddings: {x}\"),\n    op_kwargs={'x': embed_text_task.output},\n    dag=dag\n)\n\n# Define the task to trigger the embedding process\ntrigger_embedding_task = PythonOperator(\n    task_id='trigger_embedding',\n    python_callable=lambda: embed_text_task.execute(),\n    dag=dag\n)\n\n# Define the workflow\ntrigger_embedding_task >> embed_text_task >> store_embeddings_task"}], "metadata": {"file_name": "dag_1548_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981575.138319}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates basic SQL query execution and stored procedure interactions. Create a DAG that showcases different methods of executing database operations using Oracle connection parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.oracle.hooks.oracle import OracleHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'oracle_database_workflow',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Oracle database operations',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to connect to Oracle database and execute a SQL query\ndef execute_sql_query():\n    hook = OracleHook()\n    conn = hook.get_conn()\n    cursor = conn.cursor()\n    \n    # Execute a sample SQL query\n    cursor.execute(\"SELECT * FROM employees\")\n    results = cursor.fetchall()\n    \n    print(results)\n    \n    # Close the cursor and connection\n    cursor.close()\n    conn.close()\n\nexecute_sql_task = PythonOperator(\n    task_id='execute_sql_query',\n    python_callable=execute_sql_query,\n    dag=dag\n)\n\n# Task to connect to Oracle database and call a stored procedure\ndef call_stored_procedure():\n    hook = OracleHook()\n    conn = hook.get_conn()\n    cursor = conn.cursor()\n    \n    # Call a stored procedure\n    cursor.callproc('my_package.my_procedure', [param1, param2])\n    \n    # Commit the transaction\n    conn.commit()\n    \n    # Close the cursor and connection\n    cursor.close()\n    conn.close()\n\ncall_stored_procedure_task = PythonOperator(\n    task_id='call_stored_procedure',\n    python_callable=call_stored_procedure,\n    dag=dag\n)\n\n# Set up the task dependencies\nexecute_sql_task >> call_stored_procedure_task"}], "metadata": {"file_name": "dag_708_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981575.1383293}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases cloud infrastructure management by dynamically creating a GCS bucket, transferring data using a specialized operator, and then cleaning up cloud resources after the transfer is complete.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSUploadObjectFromLocalFileOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'cloud_infrastructure_management',\n    default_args=default_args,\n    description='DAG for managing cloud infrastructure',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create a GCS bucket\ncreate_bucket_task = GCSCreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Upload data to the GCS bucket\nupload_data_task = GCSUploadObjectFromLocalFileOperator(\n    task_id='upload_data',\n    source_file_path='/path/to/local/data.csv',\n    destination_blob_name='data.csv',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Transfer data from one GCS bucket to another\ntransfer_data_task = GCSToGCSOperator(\n    task_id='transfer_data',\n    source_bucket_name='my-gcs-bucket',\n    destination_bucket_name='another-gcs-bucket',\n    source_object_names=['data.csv'],\n    destination_object_names=['data.csv'],\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Clean up resources (optional)\ncleanup_resources_task = GCSCreateBucketOperator(\n    task_id='cleanup_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    delete_objects=True,\n    dag=dag,\n)\n\n# Define dependencies\ncreate_bucket_task >> upload_data_task >> transfer_data_task >> cleanup_resources_task"}], "metadata": {"file_name": "dag_1264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981575.1383376}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that sequentially executes SQL tasks with clear dependency management, showing how tasks can be chained together to perform a complete database operation lifecycle. The DAG should illustrate task ordering and atomic database interactions.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'sql_task_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate sequential SQL operations',\n    schedule_interval=None,\n)\n\n# Task to create a table\ncreate_table_task = PythonOperator(\n    task_id='create_table',\n    python_callable=create_table,\n    dag=dag,\n)\n\n# Task to insert data into the table\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag,\n    depends_on=[create_table_task],\n)\n\n# Task to select data from the table\nselect_data_task = PythonOperator(\n    task_id='select_data',\n    python_callable=select_data,\n    dag=dag,\n    depends_on=[insert_data_task],\n)\n\ndef create_table():\n    hook = PostgresHook(postgres_conn_id='postgres_default')\n    query = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_table (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100) NOT NULL\n        );\n    \"\"\"\n    hook.run(query)\n\ndef insert_data():\n    hook = PostgresHook(postgres_conn_id='postgres_default')\n    query = \"\"\"\n        INSERT INTO my_table (name) VALUES ('Alice'), ('Bob');\n    \"\"\"\n    hook.run(query)\n\ndef select_data():\n    hook = PostgresHook(postgres_conn_id='postgres_default')\n    query = \"SELECT * FROM my_table;\"\n    result = hook.get_pandas_df(query)\n    print(result)\n\n# Define the dependencies between tasks\ncreate_table_task >> insert_data_task >> select_data_task"}], "metadata": {"file_name": "dag_2180_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981575.1383467}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that performs time-span based file processing in Google Cloud Storage, applying a custom transformation script to selected files from a source bucket and transferring the results to a destination bucket. Create a systematic approach to data processing that includes automated setup, transformation, and cleanup of cloud storage resources.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'time_span_based_file_processing',\n    default_args=default_args,\n    description='A DAG for time-span based file processing in Google Cloud Storage',\n    schedule_interval=timedelta(days=1)\n)\n\n# Function to apply transformation script\ndef transform_file(file_path):\n    hook = GoogleCloudStorageHook()\n    # Apply transformation logic here\n    print(f\"Transforming {file_path}\")\n\n# Task to list files in the source bucket\nlist_files_task = PythonOperator(\n    task_id='list_files',\n    python_callable=list_files,\n    op_kwargs={'bucket_name': 'source-bucket', 'prefix': 'data/'},\n    dag=dag\n)\n\n# Task to apply transformation to each file\ntransform_files_tasks = []\nfor i in range(10):  # Assuming there are 10 files in the prefix\n    file_path = f'data/file_{i}.csv'\n    transform_files_tasks.append(PythonOperator(\n        task_id=f'transform_file_{i}',\n        python_callable=transform_file,\n        op_kwargs={'file_path': file_path},\n        dag=dag\n    ))\n\n# Task to transfer transformed files to the destination bucket\ntransfer_files_task = PythonOperator(\n    task_id='transfer_files',\n    python_callable=transfer_files,\n    op_kwargs={'source_bucket': 'source-bucket', 'destination_bucket': 'destination-bucket'},\n    dag=dag\n)\n\n# Dependencies\nlist_files_task >> transform_files_tasks >> transfer_files_task"}], "metadata": {"file_name": "dag_2063_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981575.1383574}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Azure Data Lake Storage (ADLS) workflow that demonstrates file upload and immediate deletion. Design a simple DAG that showcases basic file management operations in a cloud storage environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.azure_blob_storage import AzureBlobStorageHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    dag_id='adls_workflow',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Azure Blob Storage operations',\n)\n\n# Define the connection to Azure Blob Storage\nstorage_conn_id = 'your_azure_connection'\nblob_client = AzureBlobStorageHook(azure_conn_id=storage_conn_id)\n\n# Define the source and destination containers\nsource_container_name = 'source_container'\ndestination_container_name = 'destination_container'\n\n# Task to upload a file to ADLS\nupload_task = blob_client.upload_file_to_container(\n    bucket=source_container_name,\n    object='input.txt',\n    filename='/path/to/local/file.txt',\n    overwrite=True\n)\n\n# Task to wait for the file to be available in ADLS\nwait_for_upload_task = blob_client.wait_for_blob_availability(\n    bucket=source_container_name,\n    object='input.txt',\n    timeout=60\n)\n\n# Task to delete the uploaded file from ADLS\ndelete_task = blob_client.delete_blob(\n    bucket=source_container_name,\n    object='input.txt'\n)\n\n# Define the task dependencies\nupload_task >> wait_for_upload_task >> delete_task"}], "metadata": {"file_name": "dag_962_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981604.301946}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases inter-service communication using Redis as a message broker and synchronization mechanism. Develop a workflow that publishes a start signal and then waits for a confirmation key before advancing to subsequent tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n}\n\ndag = DAG(\n    'RedisCommunicationDAG',\n    default_args=default_args,\n    description='A DAG demonstrating inter-service communication using Redis',\n    schedule_interval=timedelta(minutes=5)\n)\n\n# Task to publish a start signal\npublish_start_signal = DummyOperator(\n    task_id='Publish Start Signal',\n    dag=dag,\n    python_callable=publish_start_signal_function,\n    op_kwargs={'message': 'Start signal received'}\n)\n\n# Task to wait for a confirmation key\nwait_for_confirmation = DummyOperator(\n    task_id='Wait for Confirmation',\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on=publish_start_signal\n)\n\ndef publish_start_signal_function(message):\n    # Simulate publishing a start signal to Redis\n    from redis import Redis\n    r = Redis(host='localhost', port=6379, db=0)\n    r.set('start_signal', message)\n\ndef confirm_signal():\n    # Simulate waiting for a confirmation key\n    from redis import Redis\n    r = Redis(host='localhost', port=6379, db=0)\n    return r.get('confirmation') == b'confirmed'\n\n# Task to process the confirmation\nprocess_confirmation = PythonOperator(\n    task_id='Process Confirmation',\n    dag=dag,\n    python_callable=confirm_signal,\n    trigger_rule='one_failed'\n)\n\n# Define the dependencies between tasks\npublish_start_signal >> wait_for_confirmation >> process_confirmation"}], "metadata": {"file_name": "dag_2197_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3019595}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data workflow automation process that leverages Azure Synapse for executing a predefined data processing pipeline on a daily schedule. Configure the DAG to reliably trigger and monitor the pipeline execution with appropriate error handling and logging.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MSSQLHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'synapse_etl_pipeline',\n    default_args=default_args,\n    description='A simple ETL pipeline using Azure Synapse',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the task to connect to Azure Synapse\ndef load_data_from_synapse():\n    # Replace these values with your actual connection details\n    conn_id = \"your_connection_id\"\n    server_name = \"your_server_name\"\n    database_name = \"your_database_name\"\n    username = \"your_username\"\n    password = \"your_password\"\n\n    hook = MSSQLHook(mssql_conn_id=conn_id)\n    \n    # Execute a query to fetch data from Azure Synapse\n    df = hook.get_pandas_df(query=\"SELECT * FROM your_table\")\n\n    return df\n\n# Define the task to perform some transformation on the data\ndef transform_data(df):\n    # Perform your transformations here\n    # For this example, we'll just print the first few rows\n    print(df.head())\n\n# Define the task to load the transformed data back to Azure Synapse\ndef load_data_to_synapse(df):\n    # Replace these values with your actual connection details\n    conn_id = \"your_connection_id\"\n    server_name = \"your_server_name\"\n    database_name = \"your_database_name\"\n    username = \"your_username\"\n    password = \"your_password\"\n\n    hook = MSSQLHook(mssql_conn_id=conn_id)\n    \n    # Insert the transformed data into Azure Synapse\n    hook.insert_rows(table=\"your_target_table\", dataframe=df)\n\n# Define the tasks in the DAG\nload_task = PythonOperator(\n    task_id='Load Data from Synapse',\n    python_callable=load_data_from_synapse,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id='Transform Data',\n    python_callable=transform_data,\n    op_kwargs={'df': load_task.output},\n    dag=dag\n)\n\nload_task >> transform_task"}], "metadata": {"file_name": "dag_1364_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981604.301971}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed task automation DAG that leverages WinRM to execute multiple system commands across a remote Windows infrastructure. Create a workflow that simulates real-world remote server management scenarios with parallel task execution and dependency tracking.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.microsoft.windows.winrm import WinRMOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='remote_command_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(hours=1),\n) as dag:\n\n    # Define the list of remote hosts\n    remote_hosts = ['server1', 'server2', 'server3']\n\n    # Define the list of commands to be executed on each host\n    commands = [\n        \"dir\",\n        \"net user administrator /add username=myuser password=mypassword\",\n        \"ipconfig\"\n    ]\n\n    # Loop through each host and command pair and execute them using WinRMOperator\n    for host, cmd in zip(remote_hosts, commands):\n        winrm_task = WinRMOperator(\n            task_id=f\"run_cmd_{host}\",\n            hostname=host,\n            username=\"Administrator\",\n            password=\"mypassword\",\n            command=cmd,\n            log_cli=True,\n            catch_exceptions=True,\n            dag=dag\n        )\n\n    # Set up dependencies between tasks\n    for i in range(len(commands)):\n        if i < len(commands) - 1:\n            winrm_tasks[i].set_upstream_of(winrm_tasks[i + 1])"}], "metadata": {"file_name": "dag_977_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981604.3019805}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure pipeline that automates the provisioning and management of a scalable NoSQL database environment using Cloud Bigtable. Design a workflow that demonstrates dynamic resource allocation, replication monitoring, and systematic teardown of database infrastructure with configurable parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigtable import BigQueryToBigtableOperator\nfrom airflow.providers.google.cloud.sensors.bigtable import BigtableInstanceExistsSensor\nfrom airflow.providers.google.cloud.sensors.bigtable import BigtableTableExistsSensor\nfrom airflow.providers.google.cloud.sensors.bigtable import BigtableTableCreationSensor\nfrom airflow.providers.google.cloud.sensors.bigtable import BigtableTableDeletionSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='bigtable_dag',\n    default_args=default_args,\n    schedule_interval=None,\n) as dag:\n\n    # Task to check if the Bigtable instance exists\n    check_instance_exists = BigtableInstanceExistsSensor(\n        task_id='check_instance_exists',\n        project_id='your-project-id',\n        instance_id='your-instance-id'\n    )\n\n    # Task to create the Bigtable instance\n    create_instance = BigtableInstanceOperator(\n        task_id='create_instance',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        display_name='Your Instance Name',\n        zone='us-central1-a',\n        cluster_config={\n            'num_nodes': 3,\n            'storage_type': 'PROFILER'\n        }\n    )\n\n    # Task to check if the Bigtable table exists\n    check_table_exists = BigtableTableExistsSensor(\n        task_id='check_table_exists',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        table_id='your-table-id'\n    )\n\n    # Task to create the Bigtable table\n    create_table = BigtableTableCreationOperator(\n        task_id='create_table',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        table_id='your-table-id',\n        column_family_id='cf1',\n        column_family_id='cf2',\n        column_family_id='cf3'\n    )\n\n    # Task to load data into the Bigtable table\n    load_data = BigQueryToBigtableOperator(\n        task_id='load_data',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        table_id='your-table-id',\n        source_project_id='your-source-project-id',\n        source_dataset_id='your-source-dataset-id',\n        source_table_id='your-source-table-id',\n        destination_table_id='your-table-id',\n        source_format='CSV',\n        write_disposition='APPEND'\n    )\n\n    # Task to monitor the health of the Bigtable table\n    monitor_table_health = BigtableTableExistsSensor(\n        task_id='monitor_table_health',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        table_id='your-table-id'\n    )\n\n    # Task to delete the Bigtable table\n    delete_table = BigtableTableDeletionOperator(\n        task_id='delete_table',\n        project_id='your-project-id',\n        instance_id='your-instance-id',\n        table_id='your-table-id'\n    )\n\n    # Task to delete the Bigtable instance\n    delete_instance = BigtableInstanceDeletionOperator(\n        task_id='delete_instance',\n        project_id='your-project-id',\n        instance_id='your-instance-id'\n    )"}], "metadata": {"file_name": "dag_821_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981604.3019917}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data transfer pipeline that moves a sample CSV file between cloud storage platforms while implementing robust testing and teardown mechanisms. The pipeline should showcase how to programmatically create, transfer, and delete cloud storage resources using infrastructure-as-code principles.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example DAG to transfer a CSV file between S3 and GCS',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the task to upload the file to S3\nupload_to_s3 = BashOperator(\n    task_id='upload_to_s3',\n    bash_command='aws s3 cp /path/to/local/file.csv s3://your-bucket-name/your-file-name.csv',\n    dag=dag\n)\n\n# Define the task to download the file from S3\ndownload_from_s3 = BashOperator(\n    task_id='download_from_s3',\n    bash_command='aws s3 cp s3://your-bucket-name/your-file-name.csv /path/to/local/downloaded_file.csv',\n    dag=dag\n)\n\n# Define the task to delete the file from S3\ndelete_from_s3 = BashOperator(\n    task_id='delete_from_s3',\n    bash_command='aws s3 rm s3://your-bucket-name/your-file-name.csv',\n    dag=dag\n)\n\n# Define the task to delete the file from GCS\ndelete_from_gcs = BashOperator(\n    task_id='delete_from_gcs',\n    bash_command='gsutil rm gs://your-bucket-name/your-file-name.csv',\n    dag=dag\n)\n\n# Set up the dependencies\nupload_to_s3 >> download_from_s3 >> delete_from_s3"}], "metadata": {"file_name": "dag_1756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3019996}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Analytics admin tasks, including creating, listing, and deleting properties and data streams. Build a comprehensive DAG that demonstrates interaction with Google Analytics Admin API using Airflow's operators and task dependencies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.bigquery_operator import BigQueryCreateDatasetOperator, BigQueryDeleteDatasetOperator\nfrom airflow.providers.google.cloud.operators.bigquery_operator import BigQueryInsertRowsOperator\nfrom airflow.providers.google.cloud.operators.bigquery_operator import BigQueryUpdateDatasetOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'ga_admin_tasks',\n    default_args=default_args,\n    description='DAG to manage Google Analytics admin tasks',\n    schedule_interval='@daily'\n)\n\n# Create dataset\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    project_id='your-project-id',\n    dataset_id='my_dataset',\n    schema=[\n        {'name': 'property_id', 'type': 'STRING'},\n        {'name': 'stream_id', 'type': 'STRING'}\n    ],\n    location='US'\n)\n\n# List datasets\nlist_datasets_task = BigQueryListDatasetsOperator(\n    task_id='list_datasets',\n    project_id='your-project-id'\n)\n\n# Insert rows into dataset\ninsert_rows_task = BigQueryInsertRowsOperator(\n    task_id='insert_rows',\n    destination_project_id='your-project-id',\n    destination_dataset_id='my_dataset',\n    source_uris=['gs://your-bucket/my_data.csv'],\n    skip_leading_row=True,\n    write_disposition='WRITE_APPEND'\n)\n\n# Update dataset\nupdate_dataset_task = BigQueryUpdateDatasetOperator(\n    task_id='update_dataset',\n    project_id='your-project-id',\n    dataset_id='my_dataset',\n    dataset_schema={\n        'fields': [\n            {'name': 'property_id', 'type': 'STRING'},\n            {'name': 'stream_id', 'type': 'STRING'},\n            {'name': 'status', 'type': 'STRING'}\n        ]\n    }\n)\n\n# Delete dataset\ndelete_dataset_task = BigQueryDeleteDatasetOperator(\n    task_id='delete_dataset',\n    project_id='your-project-id',\n    dataset_id='my_dataset'\n)\n\n# Define task dependencies\ncreate_dataset_task >> list_datasets_task >> insert_rows_task >> update_dataset_task >> delete_dataset_task"}], "metadata": {"file_name": "dag_1730_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020096}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a simple Yandex Cloud data query workflow that demonstrates basic Airflow orchestration. Design a DAG that executes a trivial SQL query and marks its completion with a final empty operator.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    dag_id='example_bigquery_dag',\n    description='A simple DAG to demonstrate Airflow orchestration with BigQuery',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the task to run a SQL query against BigQuery\nrun_query = BashOperator(\n    task_id='run_sql_query',\n    bash_command=\"echo 'SELECT * FROM your_project.your_dataset.your_table WHERE condition\",\n    dag=dag,\n)\n\n# Define the final empty operator to mark the end of the DAG\nend_task = BashOperator(\n    task_id='end_of_dag',\n    bash_command='',\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2243_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020184}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that demonstrates advanced branching techniques using different Python execution contexts, showing how to dynamically select task paths and handle complex workflow routing. Implement a DAG that explores random selection, external Python execution, and virtual environment task execution with clear demonstration of conditional task routing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nimport random\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n}\n\ndag = DAG(\n    'Advanced Branching DAG',\n    default_args=default_args,\n    description='A DAG demonstrating advanced branching techniques in Airflow',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the first task that generates a random number\ngenerate_random_number = PythonOperator(\n    task_id='Generate Random Number',\n    python_callable=random.randint,\n    op_kwargs={'low': 1, 'high': 10},\n    dag=dag\n)\n\n# Define the second task that checks if the generated number is even or odd\ncheck_even_odd = PythonOperator(\n    task_id='Check Even/Odd',\n    python_callable=lambda x: 'Even' if x % 2 == 0 else 'Odd',\n    op_kwargs={'x': generate_random_number.output},\n    dag=dag\n)\n\n# Define the third task that prints the result\nprint_result = PythonOperator(\n    task_id='Print Result',\n    python_callable=lambda x: print(f'The number is {x}'),\n    op_kwargs={'x': check_even_odd.output},\n    dag=dag\n)\n\n# Define the fourth task that executes a command in a virtual environment\nexecute_command = BashOperator(\n    task_id='Execute Command',\n    bash_command='echo \"Executing command in virtual environment\"',\n    env={\n        'VIRTUAL_ENV': '/path/to/your/virtualenv'\n    },\n    dag=dag\n)\n\n# Define the fifth task that sends an email notification\nsend_email = EmailOperator(\n    task_id='Send Email',\n    to_emails=['recipient@example.com'],\n    subject='Advanced Branching DAG Execution',\n    html_content='<p>The result of the DAG execution is: {{ ti.xcom_pull(task_ids=\"Print Result\") }}</p>',\n    dag=dag\n)\n\n# Set up the dependencies\ngenerate_random_number >> check_even_odd >> print_result >> execute_command >> send_email"}], "metadata": {"file_name": "dag_1803_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020287}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a controlled database transaction process with explicit start and cancellation steps in a Google Cloud environment. Create a systematic demonstration of transaction management that ensures data consistency and prevents unintended persistent changes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 5,\n}\n\ndag = DAG(\n    dag_id='transaction_management_dag',\n    default_args=default_args,\n    description='A DAG to manage transactions in Google BigQuery',\n)\n\n# Define the task to start a transaction\nstart_task = BashOperator(\n    task_id='start_transaction',\n    bash_command='gcloud bigquery bq query --project your-project-id --dataset your-dataset --table your-table \"BEGIN TRANSACTION;\"'\n)\n\n# Define the task to perform some operations\noperation_task = BashOperator(\n    task_id='perform_operation',\n    bash_command='gcloud bigquery bq query --project your-project-id --dataset your-dataset --table your-table \"UPDATE your_table SET column_name = value WHERE condition;\"'\n)\n\n# Define the task to commit the transaction\ncommit_task = BashOperator(\n    task_id='commit_transaction',\n    bash_command='gcloud bigquery bq query --project your-project-id --dataset your-dataset --table your-table \"COMMIT;\"\n)\n\n# Define the task to roll back the transaction\nrollback_task = BashOperator(\n    task_id='rollback_transaction',\n    bash_command='gcloud bigquery bq query --project your-project-id --dataset your-dataset --table your-table \"ROLLBACK;\"\n)\n\n# Set up the dependencies\nstart_task >> operation_task >> commit_task >> rollback_task"}], "metadata": {"file_name": "dag_1249_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981604.302037}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a custom prompt for a text generation model using Python?"}, {"role": "assistant", "content": "from transformers import pipeline\n\n# Load the GPT-4 model\nmodel = pipeline('text-generation')\n\n# Define the input text\ninput_text = \"Hello, how are you?\"\n\n# Create a custom prompt\ncustom_prompt = f\"Generate a short summary of {input_text}\"\n\n# Generate the response\nresponse = model(custom_prompt)\n\n# Print the generated response\nprint(response)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981604.302043}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates automated vector data insertion with configurable parameters like index name, namespace, and batch size. Construct a workflow that shows how to dynamically load vector embeddings from environment variables or predefined lists.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom elasticsearch import Elasticsearch\n\n# Define the default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'vector_insert_dag',\n    default_args=default_args,\n    description='Inserting vectors into Elasticsearch',\n    schedule_interval=None,\n)\n\n# Custom hook for interacting with Elasticsearch\nclass ElasticsearchHook:\n    def __init__(self, host='localhost', port=9200):\n        self.es = Elasticsearch([{'host': host, 'port': port}])\n\n    def insert_vector(self, index_name, namespace, vectors):\n        body = {'mappings': {\n            'properties': {\n                'embedding': {\n                    'type': 'dense_vector',\n                    'dims': 128\n                }\n            }\n        }}\n        self.es.indices.create(index=index_name, body=body)\n        \n        for vector in vectors:\n            self.es.index(index=index_name, id=vector['id'], body={'embedding': vector})\n\n# Task to insert vectors\ninsert_vectors_task = PythonOperator(\n    task_id='insert_vectors',\n    python_callable=ElasticsearchHook().insert_vector,\n    op_kwargs={\n        'index_name': 'your_index_name',\n        'namespace': 'your_namespace',\n        'vectors': [\n            {'id': 'vector1', 'embedding': [0.1, 0.2, 0.3, ..., 0.1]},\n            {'id': 'vector2', 'embedding': [0.2, 0.3, 0.4, ..., 0.2]}\n            # Add more vectors here...\n        ]\n    },\n    dag=dag\n)\n\n# Run the task\ninsert_vectors_task"}], "metadata": {"file_name": "dag_639_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020535}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates enterprise-level data extract refresh techniques for business intelligence reporting systems. Create a workflow that shows how to trigger workbook refreshes and monitor their completion status using different execution strategies.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\nimport pandas as pd\n\n# Define the DAG\ndag = DAG(\n    'data_refresh_dag',\n    description='A DAG for refreshing data in a business intelligence reporting system',\n    schedule_interval=timedelta(days=1),\n)\n\n# Load data from CSV file\nload_data = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data_from_csv,\n    op_kwargs={'file_path': '/path/to/your/data.csv'},\n    dag=dag\n)\n\n# Transform data\ntransform_data = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data_function,\n    op_kwargs={'input_df': '{{ ti.xcom_pull(task_ids=['load_data']) }}',\n    dag=dag\n)\n\n# Write data to database\nwrite_data = PythonOperator(\n    task_id='write_data',\n    python_callable=write_data_to_database,\n    op_kwargs={'df': '{{ ti.xcom_pull(task_ids=['transform_data']) }}',\n    dag=dag\n)"}], "metadata": {"file_name": "dag_2224_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020613}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS Glue DataBrew data profiling workflow that demonstrates creating, executing, and cleaning up a data processing job using Airflow. Build a DAG that showcases end-to-end data preparation and profiling with automated resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.providers.amazon.aws.hooks.glue import GlueHook\nfrom airflow.providers.amazon.aws.s3.hooks.s3 import S3Hook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='glue_data_profiling_dag',\n    default_args=default_args,\n    description='A DAG to profile data using AWS Glue DataBrew',\n    schedule_interval=timedelta(days=1),\n) as dag:\n\n    # Define variables\n    s3_bucket_name = \"your-s3-bucket-name\"\n    glue_catalog_table_name = \"your-glue-catalog-table-name\"\n\n    # Step 1: Copy data from S3 to Glue Catalog\n    copy_to_glue_catalog_task = BashOperator(\n        task_id='copy_to_glue_catalog',\n        bash_command=f\"aws glue get-connection --name 'your-glue-job' --query 'ConnectionArn'\",\n        env_vars={\n            'BUCKET_NAME': s3_bucket_name,\n            'TABLE_NAME': glue_catalog_table_name\n        },\n        dag=dag\n    )\n\n    # Step 2: Run Glue DataBrew Profile Dataset Action\n    run_profile_dataset_action_task = BashOperator(\n        task_id='run_profile_dataset_action',\n        bash_command=f\"aws glue start-job --job-name 'your-glue-job' --arguments '--profile-dataset'\",\n        env_vars={\n            'JOB_NAME': 'your-glue-job'\n        },\n        dag=dag\n    )\n\n    # Step 3: Clean up by deleting the Glue Catalog Table\n    delete_glue_catalog_table_task = BashOperator(\n        task_id='delete_glue_catalog_table',\n        bash_command=f\"aws glue delete-table --database-name 'your-glue-database-name' --table-name '{glue_catalog_table_name}'\",\n        env_vars={\n            'DATABASE_NAME': 'your-glue-database-name',\n            'TABLE_NAME': glue_catalog_table_name\n        },\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_1904_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020716}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to fine-tune a Google Vertex AI generative model using supervised learning techniques. The DAG will showcase how to take a base Gemini model and customize it with specific training data to improve its performance for a particular use case.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.gcloud import GoogleCloudStorageToBigQueryOperator\nfrom airflow.providers.google.cloud.ml_engine.operators.mlengine import MlEngineTrainOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    dag_id='fine_tune_gemini_model',\n    default_args=default_args,\n    description='Fine-tunes a Gemini model using supervised learning techniques.',\n    schedule_interval='@daily'\n)\n\n# Define the input and output paths\ninput_bucket = 'gs://your-bucket-name/input-data'\noutput_bucket = 'gs://your-bucket-name/output-data'\n\n# Upload input data to Google Cloud Storage\nupload_data_task = GoogleCloudStorageToBigQueryOperator(\n    task_id='upload_data_to_gcs',\n    bucket=output_bucket,\n    source_objects=['input.csv'],\n    destination_project='your-project-id',\n    destination_dataset='your-dataset',\n    destination_table='your-table'\n)\n\n# Train the Gemini model using Google Cloud ML Engine\ntrain_model_task = MlEngineTrainOperator(\n    task_id='train_gemini_model',\n    project='your-project-id',\n    region='us-central1',\n    job_name='gemini-fine-tuning-job',\n    model_name='your-model-name',\n    dataset_id='your-dataset',\n    train_steps=1000,\n    evaluation_steps=500,\n    machine_type='n1-standard-8',\n    master_config={\n        'machineType': 'n1-standard-8'\n    },\n    worker_config={\n        'machineType': 'n1-standard-8'\n    }\n)\n\n# Define the final step of the DAG\nfinal_step = PythonOperator(\n    task_id='final_step',\n    python_callable=your_final_function,\n    op_kwargs={'model_path': 'gs://your-bucket-name/output-data/gemini-fine-tuned-model'}\n)\n\n# Set up the dependencies\nupload_data_task >> train_model_task >> final_step"}], "metadata": {"file_name": "dag_2123_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020794}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that sequentially executes SQL operations against a Trino database, demonstrating task dependencies and progressive data transformation steps. The workflow should showcase how to create schemas, generate tables, insert data, and perform template-based and parameterized queries.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.trino.hooks.trino import TrinoHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'trino_example',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Trino operations',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create a Trino hook\ntrino_hook = TrinoHook(trino_conn_id='trino_default')\n\ndef create_schema():\n    sql = \"\"\"\n        CREATE SCHEMA IF NOT EXISTS my_schema;\n    \"\"\"\n    trino_hook.run(sql)\n\ndef create_table():\n    sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_schema.my_table (\n            id INT,\n            name STRING,\n            age INT\n        );\n    \"\"\"\n    trino_hook.run(sql)\n\ndef insert_data():\n    sql = \"\"\"\n        INSERT INTO my_schema.my_table (id, name, age) VALUES (%s, %s, %s);\n    \"\"\"\n    trino_hook.run(sql, parameters=(1, 'Alice', 30))\n\ndef query_data():\n    sql = \"\"\"\n        SELECT * FROM my_schema.my_table WHERE age > %s;\n    \"\"\"\n    result = trino_hook.get_records(sql, parameters=(30,))\n    for row in result:\n        print(row)\n\ncreate_schema_task = PythonOperator(\n    task_id='create_schema',\n    python_callable=create_schema,\n    dag=dag\n)\n\ncreate_table_task = PythonOperator(\n    task_id='create_table',\n    python_callable=create_table,\n    dag=dag\n)\n\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag\n)\n\nquery_data_task = PythonOperator(\n    task_id='query_data',\n    python_callable=query_data,\n    dag=dag\n)\n\ncreate_schema_task >> create_table_task >> insert_data_task >> query_data_task"}], "metadata": {"file_name": "dag_1824_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3020906}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an ETL process that automatically ingests CSV files from an external data source into a predefined Snowflake table using Airflow's Snowflake integration. Implement a robust data movement strategy that supports flexible file parsing and consistent data loading.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import FileSensor, BashOperator\nfrom datetime import timedelta\n\n# Define the DAG\ndag = DAG(\n    'snowflake_etl',\n    description='An ETL process to load CSV files from S3 to Snowflake',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1)\n)\n\n# Define the task to check if there are new CSV files in the S3 bucket\ncheck_files = FileSensor(\n    task_id='check_new_files',\n    filepath='s3://your-bucket-name/your-folder/*.csv',\n    dag=dag,\n    poke_interval=60\n)\n\n# Define the task to load the CSV files into Snowflake\nload_data = BashOperator(\n    task_id='load_data_to_snowflake',\n    bash_command='aws s3 cp s3://your-bucket-name/your-folder/*.csv /tmp/',\n    dag=dag,\n    trigger_rule='all_success'\n)\n\n# Define the task to load the CSV files into Snowflake\nload_data_to_snowflake >> check_files"}], "metadata": {"file_name": "dag_1796_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.302099}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates scheduling and executing a data pipeline in Microsoft Fabric, highlighting the process of triggering data transformation and movement tasks through a cloud-native orchestration mechanism.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom datetime import timedelta\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom azure.mgmt.datafactory import DataFactoryManagementClient\nfrom azure.mgmt.datafactory.models import PipelineResource, PipelineRunRequest, PipelineTriggerType\n\n# Define variables\nstorage_account_name = \"your_storage_account_name\"\ncontainer_name = \"your_container_name\"\nblob_names = [\"input.csv\", \"output.csv\"]\ndatabase_connection_string = \"your_database_connection_string\"\n\n# Create a credential object\ncredential = DefaultAzureCredential()\n\n# Create a client for Data Factory\ndata_factory_client = DataFactoryManagementClient(credential, subscription_id=\"your_subscription_id\")\n\n# Define the pipeline\npipeline_name = \"DataPipeline\"\n\n# Define the pipeline resource\npipeline_resource = PipelineResource(\n    properties={\n        \"activities\": [\n            {\n                \"name\": \"CopyBlobToSql\",\n                \"type\": \"CopyActivity\",\n                \"inputs\": {\n                    \"datasetReference\": {\n                        \"referenceName\": \"InputDataset\",\n                        \"parameters\": {}\n                    }\n                },\n                \"outputs\": {\n                    \"datasetReference\": {\n                        \"referenceName\": \"OutputDataset\",\n                        \"parameters\": {}\n                    }\n                },\n                \"linkedServiceReference\": {\n                    \"referenceName\": \"StorageLinkedService\",\n                    \"parameters\": {}\n                }\n            }\n        ]\n    }\n)\n\n# Create the pipeline\nresponse = data_factory_client.pipelines.create_or_update(resource_group_name=\"your_resource_group_name\", factory_name=your_data_factory_name, pipeline=pipeline_resource)\nprint(response)\n\n# Schedule the pipeline\ntrigger = PipelineTriggerRequest(\n    type=PipelineTriggerType.SCHEDULED,\n    recurrence={\"frequency\": \"Hourly\", \"interval\": 1}\n)\n\nresponse = data_factory_client.triggers.create_or_update(resource_group_name=\"your_resource_group_name\", factory_name=your_data_factory_name, trigger=trigger)\nprint(response)"}], "metadata": {"file_name": "dag_1359_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021104}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS service integration by creating, executing, and cleaning up a DataSync task between two dynamically generated S3 buckets. Implement a complete lifecycle management workflow that demonstrates task creation, execution, and resource cleanup using Python and AWS SDK.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aws_s3_data_sync_dag',\n    default_args=default_args,\n    description='A simple DAG for AWS S3 DataSync',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create S3 buckets\ncreate_bucket_task = DummyOperator(\n    task_id='create_buckets',\n    dag=dag,\n    python_callable=create_buckets,\n    op_kwargs={\n        'bucket_name_1': 'source-bucket-name',\n        'bucket_name_2': 'destination-bucket-name'\n    }\n)\n\nupload_to_source_bucket_task = DummyOperator(\n    task_id='upload_to_source_bucket',\n    dag=dag,\n    python_callable=upload_to_source_bucket,\n    op_kwargs={\n        'bucket_name': 'source-bucket-name',\n        'file_path': '/path/to/your/data.csv'\n    }\n)\n\nsync_data_task = DummyOperator(\n    task_id='sync_data',\n    dag=dag,\n    python_callable=sync_data,\n    op_kwargs={\n        'source_bucket_name': 'source-bucket-name',\n        'destination_bucket_name': 'destination-bucket-name'\n    }\n)\n\ndelete_buckets_task = DummyOperator(\n    task_id='delete_buckets',\n    dag=dag,\n    python_callable=delete_buckets,\n    op_kwargs={\n        'bucket_name_1': 'source-bucket-name',\n        'bucket_name_2': 'destination-bucket-name'\n    }\n)\n\ncreate_bucket_task >> upload_to_source_bucket_task >> sync_data_task >> delete_buckets_task"}], "metadata": {"file_name": "dag_59_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021212}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates file transfer techniques between cloud storage buckets with explicit error handling and resource management. Construct a workflow that shows how to programmatically move files, manage cloud storage resources, and ensure reliable data transfer across different storage locations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.s3_operator import S3CreateBucketOperator, S3DeleteObjectOperator, S3DownloadOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    's3_file_transfer_dag',\n    default_args=default_args,\n    description='A simple DAG for transferring files between S3 buckets',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the source and destination S3 buckets\nsource_bucket_name = 'source-bucket'\ndestination_bucket_name = 'destination-bucket'\n\n# Create the S3 bucket if it doesn't exist\ncreate_source_bucket = S3CreateBucketOperator(\n    task_id='create-source-bucket',\n    bucket_name=source_bucket_name,\n    dag=dag\n)\n\nupload_task = S3UploadOperator(\n    task_id='upload-file-to-source-bucket',\n    s3_key='example.txt',\n    s3_object_key='example.txt',\n    bucket_name=source_bucket_name,\n    dag=dag\n)\n\ndownload_task = S3DownloadOperator(\n    task_id='download-file-from-source-bucket',\n    s3_key='example.txt',\n    s3_object_key='example.txt',\n    bucket_name=source_bucket_name,\n    dag=dag\n)\n\ndelete_source_bucket = S3DeleteObjectOperator(\n    task_id='delete-source-bucket',\n    bucket_name=source_bucket_name,\n    dag=dag\n)\n\n# Set up dependencies\ncreate_source_bucket >> upload_task >> download_task >> delete_source_bucket"}], "metadata": {"file_name": "dag_1684_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021295}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure automation DAG that simulates a scalable cloud computing task execution environment. Create a workflow that programmatically provisions a compute resource, executes predefined remote commands, and then systematically tears down the infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.amazon.aws.ssh_operator import SSHOperator\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='aws_ec2_dag',\n    default_args=default_args,\n    description='A simple DAG to provision and tear down an EC2 instance',\n    schedule_interval=timedelta(days=1)\n) as dag:\n    \n    # Step 1: Provision an EC2 instance\n    start_task = DummyOperator(task_id='start', dag=dag)\n\n    # Step 2: Run a command on the EC2 instance\n    ssh_command = SSHOperator(\n        task_id='run_remote_command',\n        command='echo \"Hello from Airflow!\"',\n        ssh_conn_id='aws_default'\n    )\n\n    # Step 3: Terminate the EC2 instance\n    stop_task = DummyOperator(\n        task_id='stop_instance',\n        bash_command='aws ec2 terminate-instances --instance-ids <INSTANCE_ID>',\n        env_vars={'AWS_ACCESS_KEY_ID': '<YOUR_AWS_ACCESS_KEY>', 'AWS_SECRET_ACCESS_KEY': '<YOUR_AWS_SECRET_KEY>'},\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_172_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021386}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates interaction with InfluxDB using the InfluxDBOperator, showing how to connect to a time-series database and execute a Flux query. Develop a task that retrieves data from a predefined bucket with a configurable time window.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01T00:00:00Z',\n    'retries': 5,\n    'retry_delay': 60\n}\n\ndag = DAG(\n    'influxdb_example',\n    default_args=default_args,\n    description='A simple DAG to interact with InfluxDB',\n    schedule_interval=None,\n)\n\n# Define the task to connect to InfluxDB and run a Flux query\nrun_query_task = BashOperator(\n    task_id='run_influxdb_query',\n    bash_command='influxql -host localhost -port 8086 -database my_database -query \"SELECT * FROM my_measurement WHERE time > now() - 1h\"',\n    dag=dag\n)"}], "metadata": {"file_name": "dag_612_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021457}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need help with a specific calculation in Python. I want to determine the interest accumulated on a bank account over a period of years, assuming a fixed annual interest rate and compound interest is applied. The account starts with an initial deposit. The formula for compound interest is A = P(1 + r/n)^(nt), where A is the amount of money accumulated after n years, including interest, P is the principal amount (initial deposit), r is the annual interest rate (decimal), n is the number of times that interest is compounded per year, and t is the time the money is invested for in years. Can you help me implement this in Python?"}, {"role": "assistant", "content": "def calculate_compound_interest(principal, annual_rate, compounding_periods, years):\n    \"\"\"\n    Calculate the compound interest.\n\n    Parameters:\n    - principal: The initial amount of money.\n    - annual_rate: The annual interest rate as a decimal (e.g., 0.05 for 5%).\n    - compounding_periods: The number of times interest is compounded per year.\n    - years: The number of years the money is invested for.\n\n    Returns:\n    - The total amount accumulated after the specified number of years.\n    \"\"\"\n    # Calculate the compound interest using the formula A = P(1 + r/n)^(nt)\n    amount = principal * (1 + annual_rate / compounding_periods) ** (compounding_periods * years)\n    return amount\n\n# Example usage:\nprincipal = 1000  # Initial deposit\nannual_rate = 0.05  # Annual interest rate (5%)\ncompounding_periods = 12  # Compounded monthly\nyears = 5  # Investment duration\n\ntotal_amount = calculate_compound_interest(principal, annual_rate, compounding_periods, years)\nprint(f\"Total amount accumulated after {years} years: ${total_amount:.2f}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981604.3021529}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the difference between a tuple and a list in Python?"}, {"role": "assistant", "content": "# Define a tuple\nmy_tuple = (1, 2, 3)\n\n# Attempting to modify the tuple will result in a TypeError\ntry:\n    my_tuple[0] = 4\nexcept TypeError as e:\n    print(e)  # Output: 'tuple' object does not support item assignment\n\n# Define a list\nmy_list = [1, 2, 3]\n\n# Modifying the list is allowed\nmy_list[0] = 4\nprint(my_list)  # Output: [4, 2, 3]"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981604.3021576}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I obtain the A10 Networks AXAPI version 2.1 using the python-axapi library?"}, {"role": "assistant", "content": "from axapi import Client\n\n# Create a client object with your A10 Networks device details\nclient = Client('https://your-a10-ip', username='admin', password='password')\n\n# Send a GET request to get the AXAPI version\nresponse = client.get('/axapi/v2/axapi/system/version')\n\n# Print the response\nprint(response.json())"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981604.3021624}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow monitoring DAG that validates the completion status of a specific cloud task queue in a Google Cloud environment. Implement a sensor task that pauses workflow execution until the designated queue is completely empty.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\nfrom airflow.providers.google.cloud.sensors.gcp_taskqueue import CloudTaskQueueSensor\n\ndefault_args = {\n    'owner': 'your_email@example.com',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'task_queue_monitoring',\n    default_args=default_args,\n    description='DAG to monitor the completion status of a specific Cloud Task Queue',\n    schedule_interval='@daily'\n)\n\n# Define the name of the Cloud Task Queue\nqueue_name = 'your_queue_name'\n\n# Define the task to check the queue\ncheck_queue_status = CloudTaskQueueSensor(\n    task_id='check_queue_status',\n    project='your_project_id',\n    location='us-central1',  # Replace with your GCP region\n    queue_name=queue_name,\n    timeout=60,  # Timeout in seconds\n    poke_interval=10,  # Polling interval in seconds\n    dag=dag\n)\n\n# Define the action to be taken when the queue is empty\nif check_queue_status.past_state == 'SUCCEEDED':\n    print('The queue has been successfully processed.')\nelse:\n    print(f'The queue is still processing. Current state: {check_queue_status.state}')"}], "metadata": {"file_name": "dag_699_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021712}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates remote task scheduling and execution using WinRM hooks, demonstrating how to connect to and run commands on Windows machines programmatically. Develop a parallel execution pattern that shows different command types running concurrently with a final consolidation task.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'winrm_example',\n    default_args=default_args,\n    description='A simple DAG for executing tasks remotely using WinRM',\n    schedule_interval=None,\n)\n\n# Define the WinRM hook\nwinrm_hook = WinRMHook(winrm_host='your_winrm_server', username='username', password='password')\n\ndef run_command(command):\n    return winrm_hook.run(command).stdout.decode('utf-8').strip()\n\nwith dag:\n    # Task to run a command on the remote machine\n    task1 = BashOperator(\n        task_id='task1',\n        bash_command=run_command('echo \"Hello from task 1\"'),\n        dag=dag\n    )\n\n    task2 = BashOperator(\n        task_id='task2',\n        bash_command=run_command('echo \"Hello from task 2\"'),\n        dag=dag\n    )\n\n    task3 = BashOperator(\n        task_id='task3',\n        bash_command=run_command('echo \"Hello from task 3\"'),\n        dag=dag\n    )\n\n    # Task to consolidate the output of all tasks\n    consolidate_task = BashOperator(\n        task_id='consolidate_output',\n        bash_command=f'echo \"Tasks completed: {task1.output}, {task2.output}, {task3.output}',\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_1761_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021798}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases the complete lifecycle management of containerized jobs using Google Cloud Run operators. Develop a systematic workflow that demonstrates job creation, execution, modification, and cleanup with robust error checking and cross-task dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.run.operators import GCPJobOperator\nfrom airflow.providers.google.cloud.functions.operators import GoogleCloudFunctionOperator\nfrom airflow.providers.google.cloud.storage.operators import GoogleCloudStorageToBigQueryOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='gcp_container_job_dag',\n    default_args=default_args,\n    schedule_interval='@daily'\n) as dag:\n\n    # Create a new service in Google Cloud Run\n    create_service = GCPRunOperator(\n        task_id='create_service',\n        project='your-project-id',\n        location='us-central1',\n        image='gcr.io/your-project-id/your-service-name',\n        region='us-central1',\n        name='my-service',\n        request_body={\n            \"spec\": {\n                \"template\": {\n                    \"containers\": [\n                        {\n                            \"image\": \"gcr.io/your-project-id/your-container-image\",\n                            \"ports\": [{\"containerPort\": 80}]\n                        }\n                    ]\n                },\n                \"metadata\": {\n                    \"labels\": {\n                        \"environment\": \"production\"\n                    }\n                }\n            }\n        }\n    )\n\n    # Deploy the service to Cloud Run\n    deploy_service = GCPRunOperator(\n        task_id='deploy_service',\n        project='your-project-id',\n        location='us-central1',\n        service_name='my-service',\n        request_body={\n            \"spec\": {\n                \"template\": {\n                    \"containers\": [\n                        {\n                            \"image\": \"gcr.io/your-project-id/your-container-image\",\n                            \"ports\": [{\"containerPort\": 80}]\n                        }\n                    ]\n                },\n                \"metadata\": {\n                    \"labels\": {\n                        \"environment\": \"production\"\n                    }\n                }\n            }\n        }\n    )\n\n    # Scale the service up\n    scale_up_service = GCPRunOperator(\n        task_id='scale_up_service',\n        project='your-project-id',\n        location='us-central1',\n        service_name='my-service',\n        request_body={\n            \"spec\": {\n                \"replicas\": 3\n            }\n        }\n    )\n\n    # Scale the service down\n    scale_down_service = GCPRunOperator(\n        task_id='scale_down_service',\n        project='your-project-id',\n        location='us-central1',\n        service_name='my-service',\n        request_body={\n            \"spec\": {\n                \"replicas\": 1\n            }\n        }\n    )\n\n    # Delete the service\n    delete_service = GCPRunOperator(\n        task_id='delete_service',\n        project='your-project-id',\n        location='us-central1',\n        service_name='my-service',\n        request_body={}\n    )"}], "metadata": {"file_name": "dag_2003_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.3021934}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning lifecycle for a predictive model, demonstrating how to manage cloud resources, transfer data, create datasets, and handle model training and deployment. Showcase how to integrate multiple cloud services and operators to create a robust, scalable machine learning infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.amazon.aws.s3.hooks.s3 import S3Hook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.google.cloud.bigquery.hooks.bigquery import BigQueryHook\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2021-01-01',\n    'retries': 3,\n    'retry_delay': 60\n}\n\n# Create the DAG object\ndag = DAG(\n    'ml_lifecycle_dag',\n    description='A simple DAG for managing a machine learning lifecycle',\n    schedule_interval='@daily',\n    default_args=default_args\n)\n\n# Task to upload data from S3 to PostgreSQL\nupload_data_to_postgres = DummyOperator(\n    task_id='upload_data_to_postgres',\n    dag=dag,\n    params={\n        'bucket_name': 'my_bucket',\n        'file_path': 'data.csv'\n    }\n)\n\n# Task to create a dataset in BigQuery\ncreate_dataset_in_bigquery = DummyOperator(\n    task_id='create_dataset_in_bigquery',\n    dag=dag,\n    params={\n        'dataset_id': 'my_dataset',\n        'table_id': 'my_table'\n    }\n)\n\n# Task to train a machine learning model in TensorFlow\ntrain_model_with_tensorflow = DummyOperator(\n    task_id='train_model_with_tensorflow',\n    dag=dag,\n    params={\n        'model_type': 'linear_regression',\n        'input_table': 'my_dataset.my_table',\n        'output_table': 'my_dataset.my_trained_model'\n    }\n)\n\n# Task to deploy the trained model to AWS SageMaker\ndeploy_model_to_sagemaker = DummyOperator(\n    task_id='deploy_model_to_sagemaker',\n    dag=dag,\n    params={\n        'model_name': 'my_model',\n        'role_arn': 'arn:aws:iam::123456789012:role/my_role'\n    }\n)\n\n# Task to monitor the deployed model\nmonitor_model_performance = DummyOperator(\n    task_id='monitor_model_performance',\n    dag=dag,\n    params={\n        'model_name': 'my_model',\n        'endpoint_name': 'my_endpoint'\n    }\n)\n\n# Set up dependencies\nupload_data_to_postgres >> create_dataset_in_bigquery >> train_model_with_tensorflow >> deploy_model_to_sagemaker >> monitor_model_performance"}], "metadata": {"file_name": "dag_264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981604.302203}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data processing system that performs daily data validation and generates reports with error tracking. Create a workflow that can automatically detect data integrity issues and trigger appropriate error communication or reporting mechanisms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\nimport pandas as pd\n\n# Define the DAG\ndag = DAG(\n    'data_validation_dag',\n    description='A DAG to validate data and send error reports',\n    schedule_interval=timedelta(days=1),\n)\n\n# Load data from CSV\nload_data = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data_from_csv,\n    op_kwargs={'file_path': '/path/to/your/data.csv'},\n    dag=dag\n)\n\n# Validate data\nvalidate_data = PythonOperator(\n    task_id='validate_data',\n    python_callable=validate_data_function,\n    op_kwargs={'df': load_data.output},\n    dag=dag\n)\n\n# Send error report\nsend_error_report = PythonOperator(\n    task_id='send_error_report',\n    python_callable=send_error_report_function,\n    op_kwargs={'df': validate_data.output},\n    dag=dag\n)\n\n# Define the function to load data from CSV\ndef load_data_from_csv(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Define the function to validate data\ndef validate_data_function(df):\n    # Example validation logic (e.g., checking if all columns exist)\n    missing_columns = [col for col in df.columns if len(df[col]) == 0]\n    if missing_columns:\n        raise ValueError(f'Missing columns: {missing_columns}')\n    return df\n\n# Define the function to send error report\ndef send_error_report(df):\n    # Example error report generation\n    error_message = f\"Error detected in data:\\n{df}\"\n    print(error_message)\n    # Here you would typically use an SMTP client to send this message\n    # For simplicity, we'll just print it to the console"}], "metadata": {"file_name": "dag_1842_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981604.302213}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data workflow that uses a sensor task to simulate checking for data availability or external system readiness before triggering subsequent processing steps. Create a DAG that demonstrates how to conditionally control task execution using Airflow's sensor decorator.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.sensors.http import HttpSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    dag_id='example_data_workflow',\n    default_args=default_args,\n    schedule_interval=timedelta(minutes=1)\n)\n\n# Define the sensor task to check for data availability\ncheck_data_availability = HttpSensor(\n    task_id='check_data_availability',\n    http_conn_id='data_source_api',\n    endpoint='/api/data/availability',\n    timeout=60,\n    poke_interval=60,\n    retries=5,\n    retry_delay=timedelta(seconds=10)\n)\n\n# Define the task to process the data\nprocess_data = DummyOperator(\n    task_id='process_data',\n    trigger_rule='all_success'\n)\n\n# Set up the dependencies between the tasks\ncheck_data_availability >> process_data"}], "metadata": {"file_name": "dag_1810_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981604.3022184}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates SQL-based data validation techniques using column and table check operators against a predefined database table. Implement sequential validation tasks that check specific data quality metrics.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.postgres_operator import PostgresOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n}\n\ndag = DAG(\n    dag_id='validate_data_quality',\n    default_args=default_args,\n    description='DAG for validating data in a PostgreSQL database',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the connection to the PostgreSQL database\npostgres_conn = \"postgresql://username:password@localhost/mydatabase\"\n\n# Task to check if a column has unique values\ncheck_unique_values = PostgresOperator(\n    task_id='check_unique_values',\n    sql=\"\"\"\n        SELECT COUNT(*) FROM my_table WHERE column_name IS NOT NULL AND column_name IN (\n            SELECT column_name FROM my_table GROUP BY column_name HAVING COUNT(column_name) > 1;\n        );\n    \"\"\",\n    postgres_conn_id=postgres_conn,\n    dag=dag\n)\n\n# Task to check if a column contains null values\ncheck_null_values = PostgresOperator(\n    task_id='check_null_values',\n    sql=\"\"\"\n        SELECT COUNT(*) FROM my_table WHERE column_name IS NULL;\n    \"\"\",\n    postgres_conn_id=postgres_conn,\n    dag=dag\n)\n\n# Task to check if a column meets certain conditions\ncheck_conditions = PostgresOperator(\n    task_id='check_conditions',\n    sql=\"\"\"\n        SELECT COUNT(*) FROM my_table WHERE column_name < 0 OR column_name > 100;\n    \"\"\",\n    postgres_conn_id=postgres_conn,\n    dag=dag\n)\n\n# Set up dependencies between tasks\ncheck_unique_values.set_upstream(check_null_values)\ncheck_null_values.set_upstream(check_conditions)"}], "metadata": {"file_name": "dag_120_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981604.3022268}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates a complete cloud data transfer process, including bucket creation, data extraction, and post-transfer cleanup with error handling and observability.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cloud_data_transfer_dag',\n    default_args=default_args,\n    description='A complete cloud data transfer process using Google Cloud Storage',\n    schedule_interval=None,\n)\n\n# Create a GCS hook\ngcs_hook = GCSHook()\n\n# Define the source and destination buckets\nsource_bucket_name = 'source-bucket'\ndestination_bucket_name = 'destination-bucket'\n\n# Create a task to create the source bucket if it does not exist\ncreate_source_bucket_task = gcs_hook.create_bucket_if_not_exists(\n    bucket_name=source_bucket_name,\n    project_id='your-project-id',  # Replace with your Google Cloud project ID\n    location='us-central1'  # Replace with your desired location\n)\n\n# Create a task to extract data from the source bucket\nextract_data_task = GCSToGCSOperator(\n    task_id='extract_data',\n    source_bucket=source_bucket_name,\n    source_object='data.csv',\n    destination_bucket=destination_bucket_name,\n    destination_object='extracted_data.csv',\n    google_cloud_conn_id='google-cloud-default',  # Replace with your Google Cloud connection ID\n    dag=dag\n)\n\n# Create a task to clean up the source bucket after data extraction\ncleanup_source_bucket_task = gcs_hook.delete_bucket(\n    bucket_name=source_bucket_name,\n    project_id='your-project-id',  # Replace with your Google Cloud project ID\n    force=True  # Set to True to delete all objects in the bucket\n)\n\n# Define dependencies between tasks\ncreate_source_bucket_task >> extract_data_task >> cleanup_source_bucket_task"}], "metadata": {"file_name": "dag_1658_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981639.7526693}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates task dependencies and lifecycle management, using nested task groups and explicit setup/teardown decorators to show how complex workflows can be structured with clear initialization and finalization steps. Develop a demonstration that highlights Airflow's ability to manage task relationships and automatic resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example DAG with task dependencies and lifecycle management.',\n    schedule_interval=None,\n)\n\n# Setup decorator for task groups\ndef setup():\n    print(\"Setting up task group\")\n\n# Teardown decorator for task groups\ndef teardown():\n    print(\"Tearing down task group\")\n\n# Task group 1\nwith dag.create_task_group(group_id=\"task_group_1\"):\n    # Setup task group\n    setup()\n\n    # Task within task group 1\n    t1 = PythonOperator(\n        task_id='t1',\n        python_callable=lambda: print(\"Task 1\"),\n    )\n\n    # Task within task group 1\n    t2 = PythonOperator(\n        task_id='t2',\n        python_callable=lambda: print(\"Task 2\"),\n    )\n\n    # Set up dependency between tasks within task group 1\n    t1.set_upstream(t2)\n\n    # Teardown task group\n    teardown()\n\n# Task group 2\nwith dag.create_task_group(group_id=\"task_group_2\"):\n    # Setup task group\n    setup()\n\n    # Task within task group 2\n    t3 = PythonOperator(\n        task_id='t3',\n        python_callable=lambda: print(\"Task 3\"),\n    )\n\n    # Task within task group 2\n    t4 = PythonOperator(\n        task_id='t4',\n        python_callable=lambda: print(\"Task 4\"),\n    )\n\n    # Set up dependency between tasks within task group 2\n    t3.set_upstream(t4)\n\n    # Teardown task group\n    teardown()"}], "metadata": {"file_name": "dag_1054_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981639.752682}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an image data processing pipeline that transfers image files between cloud storage buckets and prepares them for machine learning-based product recognition. Create a reproducible workflow that demonstrates how to programmatically manage image datasets and product metadata using cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers import gcs_to_gcs_operator\nfrom airflow.providers.google.cloud.hooks import gcs_hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'image_processing_pipeline',\n    default_args=default_args,\n    description='A simple image processing pipeline',\n    schedule_interval=None,\n)\n\n# Define the source and destination GCS buckets\nsource_bucket = 'your-source-bucket'\ndestination_bucket = 'your-destination-bucket'\n\n# Transfer images from source to destination bucket\ntransfer_task = gcs_to_gcs_operator.GCSToGCSOperator(\n    task_id='transfer_images',\n    source_bucket=source_bucket,\n    destination_bucket=destination_bucket,\n    source_objects=['**/*.jpg'],  # Adjust this pattern as needed\n    move_object=True,\n    dag=dag\n)\n\n# Preprocess images using TensorFlow\ndef preprocess_image(image_path):\n    # Placeholder for actual preprocessing logic\n    print(f\"Preprocessing {image_path}\")\n\npreprocess_task >> [PythonOperator(\n    task_id=f'preprocess_{image}',\n    python_callable=preprocess_image,\n    op_kwargs={'image_path': image},\n) for image in transfer_task.output]  # Assuming output is a list of image paths"}], "metadata": {"file_name": "dag_598_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.752691}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with clear sequential steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.sqs_operator import AwsSQSServiceOperator\nfrom airflow.providers.amazon.aws.sensors.sqs_sensor import AwsSQSSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aws_sqs_stack_management',\n    default_args=default_args,\n    description='DAG to manage AWS SQS Stack',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create SQS Queue\ncreate_queue_task = AwsSQSServiceOperator(\n    task_id='create_sqs_queue',\n    aws_conn_id='aws_default',\n    queue_name='my-sqs-queue',\n    state='active',\n    region_name='us-east-1'\n)\n\n# Wait for SQS Queue to be Created\nwait_for_queue_created = AwsSQSSensor(\n    task_id='wait_for_queue_created',\n    aws_conn_id='aws_default',\n    queue_name='my-sqs-queue',\n    state='active',\n    region_name='us-east-1',\n    poke_interval=60,\n    timeout=3600\n)\n\n# Delete SQS Queue\ndelete_queue_task = AwsSQSServiceOperator(\n    task_id='delete_sqs_queue',\n    aws_conn_id='aws_default',\n    queue_name='my-sqs-queue',\n    state='inactive',\n    region_name='us-east-1'\n)\n\n# Define dependencies\ncreate_queue_task >> wait_for_queue_created >> delete_queue_task"}], "metadata": {"file_name": "dag_58_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.752699}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can conditionally trigger downstream tasks. Create a DAG that illustrates waiting mechanisms, file checks, time-based conditions, and different sensor behaviors.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.time_based import ExternalTimeSensor\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'sensor_example_dag',\n    default_args=default_args,\n    description='A DAG demonstrating various sensor types in Apache Airflow',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Sensor to wait until a specific date\nwait_until_date = ExternalTimeSensor(\n    task_id='wait_until_date',\n    target_time=datetime(2023, 1, 5),\n    dag=dag\n)\n\n# Sensor to check if a file exists\nfile_exists_sensor = FileSensor(\n    task_id='file_exists_sensor',\n    filepath='/path/to/your/file.txt',\n    dag=dag\n)\n\n# Sensor to wait until a directory is created\ndirectory_created_sensor = FileSensor(\n    task_id='directory_created_sensor',\n    filepath='/path/to/your/directory',\n    poke_interval=60,  # Check every minute\n    dag=dag\n)\n\n# Task that depends on all sensors\ndef execute_task():\n    print(\"All sensors have been met!\")\n\nexecute_task_task = PythonOperator(\n    task_id='execute_task',\n    python_callable=execute_task,\n    dag=dag\n)\n\n# Set up dependencies\nwait_until_date >> file_exists_sensor >> directory_created_sensor >> execute_task_task"}], "metadata": {"file_name": "dag_344_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527075}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Google Dataproc workflow template for running a Pig job on a managed Hadoop cluster. The workflow should showcase asynchronous job instantiation and template management in a cloud data processing environment.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    CreateDataprocClusterOperator,\n    DeleteDataprocClusterOperator,\n    SubmitJobToDataprocOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'dataproc_pig_workflow',\n    default_args=default_args,\n    description='A DAG to create and execute a Google Dataproc workflow template for running a Pig job.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the Dataproc cluster configuration\ncluster_config = {\n    \"masterConfig\": {\n        \"numInstances\": 1,\n        \"machineTypeUri\": \"n1-standard-4\",\n    },\n    \"workerConfig\": {\n        \"numInstances\": 2,\n        \"machineTypeUri\": \"n1-standard-8\",\n    },\n    \"softwareConfig\": {\n        \"imageVersion\": \"2.9.0\",\n    },\n}\n\n# Define the Pig job configuration\npig_job_config = {\n    \"mainJarFileUri\": \"gs://path/to/your/pig/script.jar\",\n    \"args\": [\"-f\", \"input.txt\"],\n}\n\n# Create a Dataproc cluster operator\ncreate_cluster_operator = CreateDataprocClusterOperator(\n    task_id=\"create_dataproc_cluster\",\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n    config=cluster_config,\n)\n\n# Submit a Pig job to the Dataproc cluster\nsubmit_job_operator = SubmitJobToDataprocOperator(\n    task_id=\"submit_pig_job\",\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n    job=pig_job_config,\n)\n\n# Delete the Dataproc cluster after the job completes\ndelete_cluster_operator = DeleteDataprocClusterOperator(\n    task_id=\"delete_dataproc_cluster\",\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n)\n\n# Define the dependencies between tasks\ncreate_cluster_operator >> submit_job_operator >> delete_cluster_operator"}], "metadata": {"file_name": "dag_195_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.752719}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates secure and controlled file movement between cloud storage systems with explicit lifecycle management. Create a process that initializes a temporary storage container, executes a cross-platform file transfer, and ensures complete resource cleanup regardless of task outcomes.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToFilesystemOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    dag_id='data_migration_dag',\n    default_args=default_args,\n    description='A data migration DAG using Google Cloud Storage',\n    schedule_interval=None,\n)\n\n# Initialize temporary storage container\ninit_container_task = BashOperator(\n    task_id='initialize_temp_storage',\n    bash_command='gsutil mb gs://temp-storage'\n)\n\n# Execute cross-platform file transfer\ntransfer_file_task = BashOperator(\n    task_id='transfer_file',\n    bash_command='cp /path/to/local/file gs://source-bucket/source-file'\n)\n\n# Ensure complete resource cleanup\ncleanup_task = BashOperator(\n    task_id='cleanup_resources',\n    bash_command='rm -rf /path/to/local/file gs://source-bucket/source-file gs://temp-storage'\n)\n\n# Define dependencies\ninit_container_task >> transfer_file_task >> cleanup_task"}], "metadata": {"file_name": "dag_93_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527344}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates basic SQL query execution against an Airflow metadata database, showing how to connect to a Postgres database and run multiple SQL statements. Create a workflow that showcases the SQLExecuteQueryOperator's capabilities for executing database queries.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.postgres.operators.sql import SQLExecuteQueryOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2021-01-01',\n}\n\n# Create the DAG\ndag = DAG(\n    'postgres_sql_example',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\n# Task to create a table in the PostgreSQL database\ncreate_table_task = SQLExecuteQueryOperator(\n    task_id='create_table',\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS employees (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            age INT,\n            department VARCHAR(50)\n        );\n    \"\"\",\n    postgres_conn_id='postgres_default',  # Replace with your actual connection ID\n    dag=dag,\n)\n\n# Task to insert data into the table\ninsert_data_task = SQLExecuteQueryOperator(\n    task_id='insert_data',\n    sql=\"\"\"\n        INSERT INTO employees (name, age, department) VALUES\n        ('John Doe', 30, 'HR'),\n        ('Jane Smith', 28, 'Engineering');\n    \"\"\",\n    postgres_conn_id='postgres_default',  # Replace with your actual connection ID\n    dag=dag,\n)\n\n# Task to select all records from the table\nselect_records_task = SQLExecuteQueryOperator(\n    task_id='select_records',\n    sql=\"\"\"\n        SELECT * FROM employees;\n    \"\"\",\n    postgres_conn_id='postgres_default',  # Replace with your actual connection ID\n    dag=dag,\n)\n\n# Set dependencies between tasks\ncreate_table_task >> insert_data_task >> select_records_task"}], "metadata": {"file_name": "dag_1158_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527432}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete data pipeline from NoSQL database export to cloud storage staging and then into a queryable data warehouse table. Develop a systematic workflow that shows how different cloud services can be integrated for seamless data processing.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='A complete data pipeline from NoSQL database export to cloud storage staging and then into a queryable data warehouse table.',\n    schedule_interval=timedelta(days=1)\n)\n\n# Step 1: Export data from NoSQL database to S3\nexport_to_s3_task = PythonOperator(\n    task_id='export_to_s3',\n    python_callable=export_data_to_s3,\n    dag=dag\n)\n\n# Step 2: Upload data from S3 to Google Cloud Storage\nupload_to_gcs_task = PythonOperator(\n    task_id='upload_to_gcs',\n    python_callable=upload_data_to_gcs,\n    dag=dag\n)\n\n# Step 3: Load data from Google Cloud Storage into Snowflake\nload_into_snowflake_task = SnowflakeOperator(\n    task_id='load_into_snowflake',\n    sql=\"INSERT INTO my_table (column1, column2) VALUES (%s, %s)\",\n    parameters=[('value1', 'value2')],\n    dag=dag\n)\n\n# Define the dependencies between tasks\nexport_to_s3_task >> upload_to_gcs_task >> load_into_snowflake_task\n\ndef export_data_to_s3():\n    # Implement the logic to export data from NoSQL database to S3\n    pass\n\ndef upload_data_to_gcs():\n    # Implement the logic to upload data from S3 to Google Cloud Storage\n    pass"}], "metadata": {"file_name": "dag_535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527533}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that simulates real-world S3 file processing and storage management tasks, including creating buckets, uploading sample data, and performing metadata operations. Design a learning exercise that demonstrates how to programmatically interact with cloud storage using Apache Airflow and AWS S3 providers.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3UploadFileOperator,\n    S3ListObjectsOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    dag_id='aws_s3_pipeline',\n    default_args=default_args,\n    schedule_interval=None,\n)\n\n# Create a bucket\ncreate_bucket_task = S3CreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my-aws-bucket',\n    aws_conn_id='aws_default',\n)\n\n# Upload a file to the bucket\nupload_file_task = S3UploadFileOperator(\n    task_id='upload_file',\n    bucket_name='my-aws-bucket',\n    key='sample.txt',\n    filename='/path/to/sample.txt',\n    aws_conn_id='aws_default',\n)\n\n# List all objects in the bucket\nlist_objects_task = S3ListObjectsOperator(\n    task_id='list_objects',\n    bucket_name='my-aws-bucket',\n    aws_conn_id='aws_default',\n)\n\n# Define dependencies\ncreate_bucket_task >> upload_file_task >> list_objects_task"}], "metadata": {"file_name": "dag_1117_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527595}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the relationship between `ExponentialBackoff` in Python and its use with `urllib3` for handling HTTP requests?"}, {"role": "assistant", "content": "import urllib3\nfrom tenacity import ExponentialBackoff, wait_fixed\n\n# Create an instance of ExponentialBackoff with custom settings\nbackoff = ExponentialBackoff(\n    wait=wait_fixed(1),  # Initial delay before the first retry\n    max_delay=60,       # Maximum delay between retries\n    multiplier=2        # Backoff factor (doubling the delay)\n)\n\ndef make_request(url):\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('GET', url)\n        return response.status_code\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        raise\n\n# Use the ExponentialBackoff wrapper\ntry:\n    status_code = make_request('http://example.com')\n    print(f\"Status Code: {status_code}\")\nexcept Exception as e:\n    print(e)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981639.7527695}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure DAG that demonstrates system-level and query-level interactions with Apache Pinot for data exploration and management. Create a workflow that executes administrative commands and retrieves database information using specialized Pinot hooks.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache_pinot.hooks.pinot import PinotHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='pinot_dag',\n    default_args=default_args,\n    description='DAG to interact with Apache Pinot',\n    schedule_interval=None,  # Set this to your desired schedule interval\n) as dag:\n\n    # Task to execute administrative command\n    execute_admin_command = PythonOperator(\n        task_id='execute_admin_command',\n        python_callable=execute_admin_command_function,\n        op_kwargs={\n            'command': 'admin --help',  # Example command to list available admin commands\n        },\n    )\n\n    # Task to retrieve database information\n    retrieve_database_info = PythonOperator(\n        task_id='retrieve_database_info',\n        python_callable=retrieve_database_info_function,\n        op_kwargs={\n            'database_name': 'your_database_name',  # Replace with your actual database name\n        },\n    )\n\n    # Define dependencies between tasks\n    execute_admin_command >> retrieve_database_info\n\ndef execute_admin_command_function(command):\n    hook = PinotHook()\n    result = hook.run_query(query=command)\n    print(f\"Admin Command Result: {result}\")\n\ndef retrieve_database_info_function(database_name):\n    hook = PinotHook()\n    result = hook.get_table_metadata(table_name=database_name)\n    print(f\"Database Information: {result}\")"}], "metadata": {"file_name": "dag_773_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.7527907}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data transformation pipeline that converts CSV files to AVRO format using Google Dataflow templates. Create a workflow that uploads source files, manages GCS buckets, and executes flexible data conversion jobs.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.templates.dataflow import DataFlowTemplateOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'data_conversion_pipeline',\n    default_args=default_args,\n    description='A pipeline to convert CSV files to AVRO format using Google Dataflow',\n    schedule_interval='@daily',\n)\n\n# Upload source files to GCS bucket\nupload_source_files = GCSToGCSOperator(\n    task_id='upload_source_files',\n    source_bucket='source-bucket',\n    destination_bucket='destination-bucket',\n    source_objects=['path/to/source/*.csv'],\n    move_object=True,\n    dag=dag,\n)\n\n# Execute Google Dataflow template\nexecute_dataflow_template = DataFlowTemplateOperator(\n    task_id='execute_dataflow_template',\n    gcp_conn_id='google_cloud_default',\n    template_path='/path/to/dataflow/template.py',\n    parameters={\n        'input_file_pattern': 'gs://source-bucket/path/to/source/*.csv',\n        'output_file_pattern': 'gs://destination-bucket/path/to/output/*.avro',\n    },\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2023_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528007}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the lifecycle of cloud infrastructure, showing how to programmatically create a Google Kubernetes Engine (GKE) cluster and then spin up a Dataproc cluster within that Kubernetes environment. Demonstrate infrastructure-as-code principles by defining cluster configurations and managing their creation and deletion as part of a coordinated workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.gke import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n)\nfrom airflow.providers.apache.dataproc.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'gke_dataproc_dag',\n    default_args=default_args,\n    description='DAG for creating a GKE cluster and Dataproc cluster',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create GKE Cluster\ncreate_gke_cluster = GKECreateClusterOperator(\n    task_id='create_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    node_count=3,\n    machine_type='n1-standard-2',\n    service_account='your-service-account@your-project-id.iam.gserviceaccount.com',\n    network='projects/your-project-id/global/networks/default',\n    subnetwork='projects/your-project-id/global/subnetworks/default',\n    tags=['gke', 'dataproc'],\n)\n\n# Delete GKE Cluster\ndelete_gke_cluster = GKEDeleteClusterOperator(\n    task_id='delete_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n)\n\n# Create Dataproc Cluster\ncreate_dataproc_cluster = DataprocCreateClusterOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n    master_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 100,\n    },\n    worker_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 100,\n    },\n    num_workers=2,\n    image_version='latest',\n)\n\n# Delete Dataproc Cluster\ndelete_dataproc_cluster = DataprocDeleteClusterOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n)\n\n# Define dependencies\ncreate_gke_cluster >> delete_gke_cluster\ncreate_dataproc_cluster >> delete_dataproc_cluster"}], "metadata": {"file_name": "dag_1626_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981639.752811}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data extraction and loading process that captures a specific Firestore collection, exports it to cloud storage, and prepares it for analytical querying in BigQuery. Design a scalable data movement solution that transforms unstructured NoSQL data into a queryable external table format.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.sensors.firestore import GoogleCloudFirestoreSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'firestore_to_bq_dag',\n    default_args=default_args,\n    description='Extracts Firestore data, exports to GCS, and loads into BigQuery',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Sensor to wait for Firestore data to be available\nfirestore_sensor = GoogleCloudFirestoreSensor(\n    task_id='wait_for_firestore_data',\n    project_id='your-project-id',\n    database_id='your-database-id',\n    collection_id='your-collection-id',\n    poke_interval=60,\n    timeout=3600\n)\n\n# Export Firestore data to GCS\ngcs_export_task = GCSToBigQueryOperator(\n    task_id='export_firestore_to_gcs',\n    bucket_name='your-bucket-name',\n    source_objects=['path/to/firestore/data.json'],\n    destination_project_id='your-project-id',\n    destination_dataset_id='your-dataset-id',\n    destination_table_id='your-table-id',\n    schema_fields=[\n        {'name': 'id', 'field_type': 'STRING'},\n        # Add more fields as needed\n    ],\n    write_disposition='WRITE_TRUNCATE'\n)\n\n# Load GCS data into BigQuery\nbq_load_task = GCSToBigQueryOperator(\n    task_id='load_gcs_to_bq',\n    bucket_name='your-bucket-name',\n    source_objects=['path/to/firestore/data.json'],\n    destination_project_id='your-project-id',\n    destination_dataset_id='your-dataset-id',\n    destination_table_id='your-table-id',\n    schema_fields=[\n        {'name': 'id', 'field_type': 'STRING'},\n        # Add more fields as needed\n    ],\n    write_disposition='WRITE_APPEND'\n)\n\n# Define dependencies\nfirestore_sensor >> gcs_export_task >> bq_load_task"}], "metadata": {"file_name": "dag_207_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528207}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that retrieves tweets from different Twitter accounts, transforms the raw data through cleaning and analysis stages, and prepares aggregated insights for storage. Develop a systematic approach to collecting, processing, and summarizing social media data with clear transformation steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'twitter_data_pipeline',\n    default_args=default_args,\n    description='A simple Twitter data pipeline',\n    schedule_interval=timedelta(days=1)\n)\n\n# Function to fetch tweets\ndef fetch_tweets():\n    # Replace with your actual Twitter credentials\n    consumer_key = 'your_consumer_key'\n    consumer_secret = 'your_consumer_secret'\n    access_token = 'your_access_token'\n    access_token_secret = 'your_access_token_secret'\n\n    import tweepy\n\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    api = tweepy.API(auth)\n\n    # Fetch tweets from specific accounts\n    tweets = []\n    for account in ['account1', 'account2']:\n        tweets.extend(api.user_timeline(screen_name=account, count=10))\n\n    return tweets\n\n# Function to clean and analyze tweets\ndef clean_and_analyze(tweets):\n    import pandas as pd\n\n    df = pd.DataFrame([tweet._json for tweet in tweets])\n    df['created_at'] = pd.to_datetime(df['created_at'])\n    df['text'] = df['text'].str.lower()\n\n    # Example: Counting words in each tweet\n    word_counts = df['text'].apply(lambda x: len(x.split()))\n\n    return word_counts\n\n# Task to fetch tweets\nfetch_tweets_task = PythonOperator(\n    task_id='fetch_tweets',\n    python_callable=fetch_tweets,\n    dag=dag\n)\n\n# Task to clean and analyze tweets\nclean_and_analyze_task = PythonOperator(\n    task_id='clean_and_analyze',\n    python_callable=clean_and_analyze,\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on=[fetch_tweets_task]\n)\n\n# Task to store aggregated insights\nstore_insights_task = PythonOperator(\n    task_id='store_insights',\n    python_callable=lambda: print(\"Insights stored\"),\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on=[clean_and_analyze_task]\n)\n\n# Set dependencies between tasks\nfetch_tweets_task >> clean_and_analyze_task >> store_insights_task"}], "metadata": {"file_name": "dag_426_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528317}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a simple data storage workflow using LevelDB that demonstrates basic key-value retrieval and insertion operations. Create a DAG that shows how to get and put data into a lightweight embedded key-value database.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport leveldb\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'leveldb_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate LevelDB operations',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to initialize the LevelDB database\ninit_db_task = PythonOperator(\n    task_id='init_db',\n    python_callable=initialize_db,\n    dag=dag,\n)\n\n# Task to insert data into the LevelDB database\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag,\n)\n\n# Task to retrieve data from the LevelDB database\nretrieve_data_task = PythonOperator(\n    task_id='retrieve_data',\n    python_callable=retrieve_data,\n    dag=dag,\n)\n\n# Task to clean up the LevelDB database\ncleanup_db_task = PythonOperator(\n    task_id='cleanup_db',\n    python_callable=cleanup_db,\n    dag=dag,\n)\n\n# Define the functions to interact with LevelDB\ndef initialize_db():\n    db_path = '/tmp/leveldb_example'\n    db = leveldb.LevelDB(db_path)\n    print(f\"LevelDB initialized at {db_path}\")\n\ndef insert_data():\n    db_path = '/tmp/leveldb_example'\n    db = leveldb.LevelDB(db_path)\n    db.put(b'key1', b'value1')\n    db.put(b'key2', b'value2')\n    print(\"Data inserted into LevelDB\")\n\ndef retrieve_data():\n    db_path = '/tmp/leveldb_example'\n    db = leveldb.LevelDB(db_path)\n    value1 = db.get(b'key1')\n    value2 = db.get(b'key2')\n    print(f\"Retrieved values: key1={value1}, key2={value2}\")\n    db.close()\n\ndef cleanup_db():\n    db_path = '/tmp/leveldb_example'\n    db = leveldb.LevelDB(db_path)\n    db.delete(b'key1')\n    db.delete(b'key2')\n    db.close()\n    print(\"Database cleaned up\")\n\n# Define the dependencies between tasks\ninit_db_task >> insert_data_task >> retrieve_data_task >> cleanup_db_task"}], "metadata": {"file_name": "dag_950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528405}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that automatically collects, downloads, and transfers digital advertising performance data from Display Video 360 to a centralized data warehouse for comprehensive marketing analytics and reporting.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.display_video360_to_bigquery import DisplayVideo360ToBigQueryOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'display_video_360_to_bigquery_dag',\n    default_args=default_args,\n    description='Automate the collection, download, and transfer of Display Video 360 performance data to BigQuery.',\n    schedule_interval='@daily',  # Adjust this as needed\n)\n\n# Define the task to fetch data from Display Video 360\nfetch_data_task = DisplayVideo360ToBigQueryOperator(\n    task_id='fetch_display_video_data',\n    display_video_account_id='YOUR_DISPLAY_VIDEO_ACCOUNT_ID',\n    dataset_id='YOUR_DATASET_ID',\n    table_id='YOUR_TABLE_ID',\n    bigquery_conn_id='google_cloud_default',\n    dag=dag,\n)\n\n# Define the task to load data into BigQuery\nload_data_task = BigQueryOperator(\n    task_id='load_display_video_data_into_bigquery',\n    sql=f\"INSERT INTO `{YOUR_PROJECT_ID}.{YOUR_DATASET_ID}.{YOUR_TABLE_ID}` SELECT * FROM `YOUR_DISPLAY_VIDEO_ACCOUNT_ID`.performance_data\",\n    bigquery_conn_id='google_cloud_default',\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_46_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528486}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and communication using XComs. Create a workflow that shows how different tasks can push and pull data between each other using various methods like direct pushing, returning values, and bash operator interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'xcom_example_dag',\n    default_args=default_args,\n    description='An example DAG demonstrating cross-task data sharing using XComs.',\n    schedule_interval=None,\n)\n\n# Task 1: Push data to XCom\ndef push_data_to_xcom():\n    xcom.set(key='data_from_task1', value='Hello from Task 1')\n\npush_task1 = PythonOperator(\n    task_id='push_data_to_xcom',\n    python_callable=push_data_to_xcom,\n    dag=dag,\n)\n\n# Task 2: Pull data from XCom\ndef pull_data_from_xcom():\n    pulled_data = xcom.get(key='data_from_task1')\n    print(f'Data from Task 1: {pulled_data}')\n\npull_task2 = PythonOperator(\n    task_id='pull_data_from_xcom',\n    python_callable=pull_data_from_xcom,\n    dag=dag,\n)\n\n# Task 3: Use the pulled data in another task\ndef use_pulled_data():\n    pulled_data = xcom.get(key='data_from_task1')\n    print(f'Using data from Task 1 in Task 3: {pulled_data}')\n\nuse_task3 = PythonOperator(\n    task_id='use_pulled_data',\n    python_callable=use_pulled_data,\n    dag=dag,\n)\n\n# Set up the dependencies\npush_task1 >> pull_task2 >> use_task3"}], "metadata": {"file_name": "dag_32_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.752856}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that dynamically creates an SQS queue, publishes multiple messages, and demonstrates different message retrieval strategies using sensors. Implement a complete workflow that covers queue creation, message publishing, message sensing, and queue deletion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.sqs import SQSHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='sqs_example_dag',\n    description='A simple DAG to demonstrate SQS operations',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1)\n) as dag:\n\n    # Step 1: Create an SQS queue\n    create_queue_task = PythonOperator(\n        task_id='create_sqs_queue',\n        python_callable=create_sqs_queue,\n        op_kwargs={\n            'queue_name': 'example_queue'\n        }\n    )\n\n    # Step 2: Publish multiple messages to the queue\n    publish_messages_task = PythonOperator(\n        task_id='publish_messages',\n        python_callable=publish_messages,\n        op_kwargs={\n            'queue_url': 'https://sqs.us-west-2.amazonaws.com/your-account-id/example_queue',\n            'messages': [\n                {'body': 'Message 1'},\n                {'body': 'Message 2'},\n                {'body': 'Message 3'}\n            ]\n        }\n    )\n\n    # Step 3: Use a sensor to wait for messages in the queue\n    receive_messages_sensor = PythonOperator(\n        task_id='receive_messages_sensor',\n        python_callable=receive_messages_sensor,\n        op_kwargs={\n            'queue_url': 'https://sqs.us-west-2.amazonaws.com/your-account-id/example_queue'\n        }\n    )\n\n    # Step 4: Delete the SQS queue\n    delete_queue_task = PythonOperator(\n        task_id='delete_sqs_queue',\n        python_callable=delete_sqs_queue,\n        op_kwargs={\n            'queue_url': 'https://sqs.us-west-2.amazonaws.com/your-account-id/example_queue'\n        }\n    )\n\n    # Define the dependencies between tasks\n    create_queue_task >> publish_messages_task >> receive_messages_sensor >> delete_queue_task"}], "metadata": {"file_name": "dag_423_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528665}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data science workflow that transforms raw training data into a deployable machine learning model with automated infrastructure provisioning and prediction capabilities. The solution should demonstrate how to take structured CSV data, train a classification model, and expose it as a real-time prediction service with automatic resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.kubernetes.operators.kubernetes_pod import KubernetesPodOperator\nfrom airflow.providers.http.sensors.http import HttpSensor\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_science_workflow',\n    default_args=default_args,\n    description='A data science workflow for transforming raw training data into a deployable machine learning model.',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to check if the data source is available\ncheck_data_source = HttpSensor(\n    task_id='check_data_source',\n    http_conn_id='http_default',\n    endpoint='/api/data-source',\n    response_check=lambda response: response.status_code == 200,\n    timeout=30,\n    poke_interval=5\n)\n\n# Task to download the raw data from the data source\ndownload_raw_data = SimpleHttpOperator(\n    task_id='download_raw_data',\n    http_conn_id='http_default',\n    endpoint='/api/download-data',\n    method='GET',\n    output_file_path='/path/to/raw_data.csv'\n)\n\n# Task to preprocess the raw data\npreprocess_data = PythonOperator(\n    task_id='preprocess_data',\n    python_callable=preprocess_data_function,\n    op_kwargs={'input_file_path': '/path/to/raw_data.csv', 'output_file_path': '/path/to/preprocessed_data.csv'}\n)\n\n# Task to train the classification model\ntrain_model = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model_function,\n    op_kwargs={'input_file_path': '/path/to/preprocessed_data.csv', 'model_output_path': '/path/to/model.pkl'}\n)\n\n# Task to deploy the trained model as a REST API\ndeploy_model = KubernetesPodOperator(\n    task_id='deploy_model',\n    namespace='default',\n    image='your-deployment-image',\n    command=['python', 'app.py'],\n    env_vars={\n        'MODEL_PATH': '/path/to/model.pkl',\n        'API_PORT': '8080'\n    },\n    labels={'app': 'ml-model'},\n    resources={\n        'limits': {'cpu': '1', 'memory': '2Gi'},\n        'requests': {'cpu': '0.5', 'memory': '1Gi'}\n    }\n)\n\n# Define the dependencies between tasks\ncheck_data_source >> download_raw_data >> preprocess_data >> train_model >> deploy_model"}], "metadata": {"file_name": "dag_1928_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981639.7528772}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task management of a Google Cloud DLP job trigger, demonstrating how to create, list, retrieve, update, and delete a data inspection trigger. The workflow will serve as a technical reference for managing data protection tasks through a structured, automated pipeline.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.dlp import DlpHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='gcp_dlp_trigger_management',\n    description='DAG to manage Google Cloud DLP data inspection triggers',\n    default_args=default_args,\n    schedule_interval=None  # Set this to your desired schedule interval\n) as dag:\n\n    # Function to create a data inspection trigger\n    def create_data_inspection_trigger():\n        hook = DlpHook()\n        response = hook.create_data_inspection_trigger(\n            parent='projects/your-project-id',\n            trigger={\n                'display_name': 'My Data Inspection Trigger',\n                'inspect_job': {\n                    'storage_config': {\n                        'cloud_storage_options': {\n                            'uri': 'gs://your-bucket-name/path/to/data'\n                        }\n                    },\n                    'info_types': [\n                        {'name': 'PHONE_NUMBER'},\n                        {'name': 'EMAIL_ADDRESS'}\n                    ]\n                }\n            }\n        )\n        print(f\"Created trigger with ID: {response.name}\")\n\n    # Task to create the data inspection trigger\n    create_trigger_task = PythonOperator(\n        task_id='create_data_inspection_trigger',\n        python_callable=create_data_inspection_trigger\n    )\n\n    # Function to list all data inspection triggers\n    def list_data_inspection_triggers():\n        hook = DlpHook()\n        response = hook.list_data_inspection_triggers(parent='projects/your-project-id')\n        print(\"List of triggers:\")\n        for trigger in response.triggers:\n            print(trigger.display_name)\n\n    # Task to list all data inspection triggers\n    list_triggers_task = PythonOperator(\n        task_id='list_data_inspection_triggers',\n        python_callable=list_data_inspection_triggers\n    )\n\n    # Function to retrieve a specific data inspection trigger\n    def get_data_inspection_trigger():\n        hook = DlpHook()\n        response = hook.get_data_inspection_trigger(name='projects/your-project-id/triggers/your-trigger-id')\n        print(f\"Retrieved trigger: {response.display_name}\")\n\n    # Task to retrieve a specific data inspection trigger\n    get_trigger_task = PythonOperator(\n        task_id='get_data_inspection_trigger',\n        python_callable=get_data_inspection_trigger\n    )\n\n    # Function to update a data inspection trigger\n    def update_data_inspection_trigger():\n        hook = DlpHook()\n        response = hook.update_data_inspection_trigger(\n            name='projects/your-project-id/triggers/your-trigger-id',\n            trigger={\n                'display_name': 'Updated Data Inspection Trigger'\n            }\n        )\n        print(f\"Updated trigger with ID: {response.name}\")\n\n    # Task to update a data inspection trigger\n    update_trigger_task = PythonOperator(\n        task_id='update_data_inspection_trigger',\n        python_callable=update_data_inspection_trigger\n    )\n\n    # Function to delete a data inspection trigger\n    def delete_data_inspection_trigger():\n        hook = DlpHook()\n        hook.delete_data_inspection_trigger(name='projects/your-project-id/triggers/your-trigger-id')\n\n    # Task to delete a data inspection trigger\n    delete_trigger_task = PythonOperator(\n        task_id='delete_data_inspection_trigger',\n        python_callable=delete_data_inspection_trigger\n    )\n\n    # Define the task dependencies\n    create_trigger_task >> list_triggers_task >> get_trigger_task >> update_trigger_task >> delete_trigger_task"}], "metadata": {"file_name": "dag_2015_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981639.752893}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that illustrates the interaction between local file systems and cloud storage using Azure-specific operators for file transfer and deletion. Develop a simple demonstration of how data can be moved and managed programmatically in a cloud infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.storage import AzureBlobStorageHook\nfrom airflow.providers.microsoft.azure.operators.storage import (\n    AzureBlobFileDownloadOperator,\n    AzureBlobFileUploadOperator,\n    AzureBlobDeleteOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 10, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'azure_blob_storage_dag',\n    default_args=default_args,\n    description='A DAG to demonstrate file transfer and deletion from Azure Blob Storage',\n    schedule_interval=None,\n)\n\n# Define variables\nsource_container_name = 'source-container'\ndestination_container_name = 'destination-container'\nlocal_file_path = '/path/to/local/file.txt'\nblob_name = 'file.txt'\n\n# Create Azure Blob Storage hooks\nstorage_hook = AzureBlobStorageHook(\n    azure_conn_id='azure_default',  # Replace with your Azure connection ID\n    resource_group_name='your-resource-group-name',  # Replace with your resource group name\n    account_name='your-account-name',  # Replace with your Azure account name\n)\n\n# Task to upload a file from local to Azure Blob Storage\nupload_task = AzureBlobFileUploadOperator(\n    task_id='upload_to_azure',\n    source_file=local_file_path,\n    destination_container=destination_container_name,\n    blob_name=blob_name,\n    hook=storage_hook,\n)\n\n# Task to download a file from Azure Blob Storage to local\ndownload_task = AzureBlobFileDownloadOperator(\n    task_id='download_from_azure',\n    container_name=destination_container_name,\n    blob_name=blob_name,\n    destination_file='/path/to/downloaded/file.txt',\n    hook=storage_hook,\n)\n\n# Task to delete a file from Azure Blob Storage\ndelete_task = AzureBlobDeleteOperator(\n    task_id='delete_from_azure',\n    container_name=destination_container_name,\n    blob_name=blob_name,\n    hook=storage_hook,\n)\n\n# Set dependencies\nupload_task >> download_task >> delete_task"}], "metadata": {"file_name": "dag_963_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529025}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task execution pattern for cloud infrastructure management, specifically demonstrating how to programmatically interact with Alibaba OSS bucket resources. Develop a simple workflow that shows task dependencies and cloud service interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.alibaba.cloud.hooks.oss import OssHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'alibaba_cloud_oss_example',\n    default_args=default_args,\n    description='A simple example of interacting with Alibaba Cloud OSS',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Create a new OSS bucket\ncreate_bucket_task = PythonOperator(\n    task_id='create_bucket',\n    python_callable=create_bucket,\n    op_kwargs={\n        'bucket_name': 'my-new-bucket',\n        'endpoint': 'http://oss-cn-hangzhou.aliyuncs.com',  # Replace with your actual endpoint\n        'access_key_id': 'your-access-key-id',  # Replace with your actual access key ID\n        'access_key_secret': 'your-access-key-secret'  # Replace with your actual access key secret\n    },\n    dag=dag,\n)\n\n# Upload a file to the OSS bucket\nupload_file_task = PythonOperator(\n    task_id='upload_file',\n    python_callable=upload_file,\n    op_kwargs={\n        'bucket_name': 'my-new-bucket',\n        'object_key': 'example.txt',\n        'file_path': '/path/to/your/example.txt',  # Replace with your actual file path\n        'endpoint': 'http://oss-cn-hangzhou.aliyuncs.com',  # Replace with your actual endpoint\n        'access_key_id': 'your-access-key-id',  # Replace with your actual access key ID\n        'access_key_secret': 'your-access-key-secret'  # Replace with your actual access key secret\n    },\n    dag=dag,\n)\n\n# Delete the OSS bucket\ndelete_bucket_task = PythonOperator(\n    task_id='delete_bucket',\n    python_callable=delete_bucket,\n    op_kwargs={\n        'bucket_name': 'my-new-bucket',\n        'endpoint': 'http://oss-cn-hangzhou.aliyuncs.com',  # Replace with your actual endpoint\n        'access_key_id': 'your-access-key-id',  # Replace with your actual access key ID\n        'access_key_secret': 'your-access-key-secret'  # Replace with your actual access key secret\n    },\n    dag=dag,\n)\n\n# Define task dependencies\ncreate_bucket_task >> upload_file_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_54_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529118}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing workflow that shows how to execute database transactions using Oracle connections, highlighting different parameter input strategies for stored procedures. Create a DAG that explores various methods of executing database operations with input and output parameter handling.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.oracle.hooks.oracle import OracleHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'oracle_database_operations',\n    default_args=default_args,\n    description='A DAG to demonstrate Oracle database operations',\n    schedule_interval=None,\n)\n\n# Task to insert data into the database\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag,\n    op_kwargs={\n        'connection_id': 'oracle_conn',\n        'procedure_name': 'INSERT_DATA',\n        'params': ('John Doe', 30)\n    }\n)\n\n# Task to select data from the database\nselect_data_task = PythonOperator(\n    task_id='select_data',\n    python_callable=select_data,\n    dag=dag,\n    op_kwargs={\n        'connection_id': 'oracle_conn',\n        'procedure_name': 'SELECT_DATA',\n        'params': ()\n    }\n)\n\n# Set up the dependencies between tasks\ninsert_data_task.set_upstream(select_data_task)\n\ndef insert_data(connection_id, procedure_name, params):\n    hook = OracleHook(conn_id=connection_id)\n    cursor = hook.get_cursor()\n    try:\n        cursor.callproc(procedure_name, params)\n        print(f\"Data inserted successfully.\")\n    except Exception as e:\n        print(f\"Error inserting data: {e}\")\n    finally:\n        cursor.close()\n\ndef select_data(connection_id, procedure_name, params):\n    hook = OracleHook(conn_id=connection_id)\n    cursor = hook.get_cursor()\n    try:\n        cursor.callproc(procedure_name, params)\n        result = cursor.fetchone()\n        if result:\n            print(f\"Selected data: {result}\")\n        else:\n            print(\"No data found.\")\n    except Exception as e:\n        print(f\"Error selecting data: {e}\")\n    finally:\n        cursor.close()"}], "metadata": {"file_name": "dag_708_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529213}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data ingestion pipeline that transforms a local CSV file containing US state information into a structured BigQuery table using Google Cloud data transfer services. Implement a systematic approach to moving structured data from local storage to cloud-based analytical platforms.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_ingestion_pipeline',\n    default_args=default_args,\n    description='A data ingestion pipeline to transform a local CSV file into a structured BigQuery table',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Download CSV File\ndownload_csv_task = BashOperator(\n    task_id='download_csv',\n    bash_command='curl -O https://example.com/us_states.csv',\n    dag=dag,\n)\n\n# Step 2: Upload CSV File to Google Cloud Storage\nupload_csv_to_gcs_task = GCSFileTransferOperator(\n    task_id='upload_csv_to_gcs',\n    source_file_path='us_states.csv',\n    destination_bucket_name='your-bucket-name',\n    destination_blob_name='us_states.csv',\n    dag=dag,\n)\n\n# Step 3: Create BigQuery Table\ncreate_bigquery_table_task = BigQueryCreateTableOperator(\n    task_id='create_bigquery_table',\n    project_id='your-project-id',\n    dataset_id='your-dataset-id',\n    table_id='us_states_table',\n    schema=[\n        {'name': 'state', 'type': 'STRING'},\n        {'name': 'population', 'type': 'INTEGER'},\n        {'name': 'capital', 'type': 'STRING'},\n        # Add more columns as needed\n    ],\n    dag=dag,\n)\n\n# Step 4: Load Data into BigQuery\nload_data_into_bigquery_task = GCSToBigQueryOperator(\n    task_id='load_data_into_bigquery',\n    source_file_path='gs://{}/{}'.format('your-bucket-name', 'us_states.csv'),\n    destination_project_id='your-project-id',\n    destination_dataset_id='your-dataset-id',\n    destination_table_id='us_states_table',\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag,\n)\n\n# Define dependencies\ndownload_csv_task >> upload_csv_to_gcs_task >> create_bigquery_table_task >> load_data_into_bigquery_task"}], "metadata": {"file_name": "dag_809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.752932}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure-as-code workflow for dynamically creating and managing a distributed caching service with precise configuration controls. Develop a DAG that illustrates how to programmatically set up a scalable in-memory data storage solution with specific node and memory configurations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='distributed_cache_service',\n    description='A DAG to manage a distributed caching service',\n    default_args=default_args,\n    schedule_interval=timedelta(hours=1),\n) as dag:\n\n    # Step 1: Pull the Docker image for the cache service\n    pull_image = DockerOperator(\n        task_id='pull_docker_image',\n        image='cache-service-image',\n        auto_remove=True,\n        network_mode='host'\n    )\n\n    # Step 2: Start the cache service container\n    start_container = DockerOperator(\n        task_id='start_cache_service_container',\n        image='cache-service-image',\n        command=['python', 'app.py'],\n        environment={\n            'NODE_COUNT': '4',\n            'MEMORY_SIZE': '8G'\n        },\n        auto_remove=True,\n        network_mode='host'\n    )\n\n    # Step 3: Stop the cache service container\n    stop_container = DockerOperator(\n        task_id='stop_cache_service_container',\n        image='cache-service-image',\n        command=['python', 'cleanup.py'],\n        environment={\n            'NODE_COUNT': '4',\n            'MEMORY_SIZE': '8G'\n        },\n        auto_remove=True,\n        network_mode='host'\n    )\n\n    # Step 4: Remove the Docker image\n    remove_image = DockerOperator(\n        task_id='remove_docker_image',\n        image='cache-service-image',\n        auto_remove=True,\n        network_mode='host'\n    )\n\n    # Task dependencies\n    pull_image >> start_container >> stop_container >> remove_image"}], "metadata": {"file_name": "dag_1588_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529414}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical process of cloud file operations, including creating a GCS bucket, uploading a local file, downloading the file, and performing cleanup operations. Implement the workflow to demonstrate inter-system file transfer and management using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSUploadFileOperator,\n    GCSDownloadFileOperator,\n    GCSDeleteBucketOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\ndag = DAG(\n    'gcs_file_operations',\n    default_args=default_args,\n    description='DAG for GCS file operations',\n    schedule_interval=None,\n)\n\n# Create a GCS bucket\ncreate_bucket_task = GCSCreateBucketOperator(\n    task_id='create_gcs_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n)\n\n# Upload a local file to GCS\nupload_file_task = GCSUploadFileOperator(\n    task_id='upload_file_to_gcs',\n    bucket_name='my-gcs-bucket',\n    object_name='local-file.txt',\n    source_path='/path/to/local/file.txt',\n)\n\n# Download the file from GCS\ndownload_file_task = GCSDownloadFileOperator(\n    task_id='download_file_from_gcs',\n    bucket_name='my-gcs-bucket',\n    object_name='local-file.txt',\n    destination_path='/path/to/downloaded/file.txt',\n)\n\n# Delete the GCS bucket\ndelete_bucket_task = GCSDeleteBucketOperator(\n    task_id='delete_gcs_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n)\n\n# Define the dependencies between tasks\ncreate_bucket_task >> upload_file_task >> download_file_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_1657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529485}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an event-driven Airflow DAG that demonstrates message production and consumption using Kafka, with a focus on creating a sensor that triggers specific actions based on number characteristics. Implement a data pipeline that showcases asynchronous message processing and conditional event handling.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\n# Define the default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'kafka_event_driven_dag',\n    default_args=default_args,\n    description='A DAG to demonstrate Kafka message production and consumption',\n    schedule_interval=timedelta(minutes=5)\n)\n\n# Function to produce messages to Kafka\ndef produce_messages():\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.appName(\"KafkaProducer\").getOrCreate()\n    \n    # Define the schema for the data\n    schema = StructType([\n        StructField('id', IntegerType()),\n        StructField('value', IntegerType())\n    ])\n    \n    # Data to be produced\n    data = [(i, i * 2) for i in range(10)]\n    \n    # Write data to Kafka\n    df = spark.createDataFrame(data, schema)\n    df.write.format(\"kafka\").option(\"topic\", \"my_topic\").option(\"bootstrap.servers\", \"localhost:9092\").save()\n    \n    spark.stop()\n\n# Task to produce messages\nproduce_task = PythonOperator(\n    task_id='produce_messages',\n    python_callable=produce_messages,\n    dag=dag\n)\n\n# Function to consume messages from Kafka\ndef consume_messages():\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.appName(\"KafkaConsumer\").getOrCreate()\n    \n    # Read data from Kafka\n    df = spark.read.format(\"kafka\").option(\"topic\", \"my_topic\").option(\"bootstrap.servers\", \"localhost:9092\").load()\n    \n    # Process the data\n    processed_df = df.filter(col('value') > 10).select('id')\n    \n    # Print the processed data\n    processed_df.show()\n    \n    spark.stop()\n\n# Task to consume messages\nconsume_task = PythonOperator(\n    task_id='consume_messages',\n    python_callable=consume_messages,\n    dag=dag\n)\n\n# Sensor to trigger tasks based on condition\nsensor = PythonOperator(\n    task_id='sensor',\n    python_callable=lambda: True,  # Replace with actual condition logic\n    dag=dag\n)\n\n# Set up dependencies\nproduce_task >> sensor >> consume_task"}], "metadata": {"file_name": "dag_1942_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981639.75296}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates converting text to speech using Google Cloud Text-to-Speech service and storing the audio file in a Google Cloud Storage bucket. The DAG should showcase creating a bucket, generating an audio file from sample text, and then cleaning up the bucket.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.text_to_speech import GoogleCloudTextToSpeechSynthesizeOperator\nfrom airflow.providers.google.cloud.sensors.storage import GoogleCloudStorageObjectExistsSensor\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_text_to_speech_dag',\n    default_args=default_args,\n    description='Convert text to speech using Google Cloud Text-to-Speech and store the audio file in a Google Cloud Storage bucket.',\n    schedule_interval=None,  # Set to None for testing purposes\n)\n\n# Create a bucket\ncreate_bucket_operator = GoogleCloudStorageBucketCreateOperator(\n    task_id='create_bucket',\n    bucket_name='my-text-to-speech-bucket',\n    project_id='your-project-id',  # Replace with your GCP project ID\n    dag=dag,\n)\n\n# Generate an audio file from sample text\ngenerate_audio_operator = GoogleCloudTextToSpeechSynthesizeOperator(\n    task_id='generate_audio',\n    input_text='Hello, this is a test text to speech conversion.',\n    voice_name='en-US-Standard-A',\n    output_uri=f'gs://{create_bucket_operator.output_bucket}/output.mp3',\n    project_id='your-project-id',  # Replace with your GCP project ID\n    dag=dag,\n)\n\n# Check if the audio file exists in the bucket\ncheck_audio_exists_operator = GoogleCloudStorageObjectExistsSensor(\n    task_id='check_audio_exists',\n    bucket_name=create_bucket_operator.output_bucket,\n    object_name='output.mp3',\n    project_id='your-project-id',  # Replace with your GCP project ID\n    dag=dag,\n)\n\n# Clean up the bucket\ncleanup_bucket_operator = GoogleCloudStorageBucketDeleteOperator(\n    task_id='cleanup_bucket',\n    bucket_name=create_bucket_operator.output_bucket,\n    project_id='your-project-id',  # Replace with your GCP project ID\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_572_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529688}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that demonstrates dynamic table creation and transformation in BigQuery with multiple data object types. Implement a workflow that shows how to create, modify, and manage different table representations including standard tables, views, and materialized views.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'bigquery_table_operations',\n    default_args=default_args,\n    description='DAG for managing BigQuery tables',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Hook for interacting with BigQuery\nbq_hook = BigQueryHook()\n\n# Task to create a standard table\ncreate_standard_table_task = PythonOperator(\n    task_id='create_standard_table',\n    python_callable=create_standard_table,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your_project_id',\n        'dataset_id': 'your_dataset_id',\n        'table_id': 'standard_table',\n        'schema': [\n            {'name': 'id', 'field_type': 'INTEGER'},\n            {'name': 'name', 'field_type': 'STRING'},\n            {'name': 'age', 'field_type': 'INTEGER'}\n        ]\n    }\n)\n\n# Task to create a view\ncreate_view_task = PythonOperator(\n    task_id='create_view',\n    python_callable=create_view,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your_project_id',\n        'dataset_id': 'your_dataset_id',\n        'table_id': 'view',\n        'source_table': 'standard_table',\n        'sql_query': \"\"\"\n            SELECT id, name, age * 2 AS doubled_age\n            FROM `your_project_id.your_dataset_id.standard_table`\n        \"\"\"\n    }\n)\n\n# Task to create a materialized view\ncreate_materialized_view_task = PythonOperator(\n    task_id='create_materialized_view',\n    python_callable=create_materialized_view,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your_project_id',\n        'dataset_id': 'your_dataset_id',\n        'table_id': 'materialized_view',\n        'source_table': 'standard_table',\n        'sql_query': \"\"\"\n            CREATE MATERIALIZED VIEW `your_project_id.your_dataset_id.materialized_view`\n            AS\n            SELECT id, name, age * 2 AS doubled_age\n            FROM `your_project_id.your_dataset_id.standard_table`\n        \"\"\"\n    }\n)\n\n# Task to drop a table\ndrop_table_task = PythonOperator(\n    task_id='drop_table',\n    python_callable=drop_table,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your_project_id',\n        'dataset_id': 'your_dataset_id',\n        'table_id': 'view'\n    }\n)\n\n# Define dependencies between tasks\ncreate_standard_table_task >> create_view_task >> create_materialized_view_task >> drop_table_task"}], "metadata": {"file_name": "dag_151_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529795}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that moves spreadsheet data between Google Sheets and Google Cloud Storage using Airflow's task orchestration capabilities. Create a systematic process for extracting spreadsheet data, transferring it to cloud storage, and then reimporting the data into a new spreadsheet environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSCredentialsHook\nfrom airflow.providers.google.cloud.hooks.sheets import SheetsHook\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_migration_dag',\n    default_args=default_args,\n    description='A data migration pipeline from Google Sheets to Google Cloud Storage and back',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to extract data from Google Sheets\nextract_data_task = SheetsHook().get_pandas_dataframe(\n    spreadsheet_id='YOUR_SPREADSHEET_ID',\n    sheet_name='Sheet1'\n)\n\n# Task to transfer data to Google Cloud Storage\ntransfer_to_gcs_task = GCSCredentialsHook().create_client()\ngcs_hook = GCSCredentialsHook(google_cloud_storage_conn_id='your_gcs_connection')\ngcs_hook.upload_object_from_string(\n    bucket_name='your_bucket_name',\n    object_name='path/to/destination/file.csv',\n    data=extract_data_task.to_csv(index=False).encode('utf-8'),\n    content_type='text/csv'\n)\n\n# Task to import data into BigQuery\nimport_data_task = BigQueryHook().load_table_from_dataframe(\n    dataframe=extract_data_task,\n    destination_dataset_table='your_project.your_dataset.your_table',\n    source_format='CSV'\n)\n\n# Task to clean up (optional)\ncleanup_task = GCSCredentialsHook().delete_objects_in_bucket(\n    bucket_name='your_bucket_name',\n    objects=['path/to/destination/file.csv']\n)\n\n# Dependencies\nextract_data_task >> transfer_to_gcs_task >> import_data_task >> cleanup_task"}], "metadata": {"file_name": "dag_895_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981639.7529905}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I modify the given Python code to handle a list of dictionaries with potential missing keys? When processing each dictionary, I want to ensure that if a key is missing, it returns a default value instead of throwing an error."}, {"role": "assistant", "content": "# Define a function to process a list of dictionaries with optional missing keys\ndef process_dicts(dict_list, default_value=None):\n    # Iterate over each dictionary in the list\n    for item in dict_list:\n        # Use get() to safely access values from the dictionary\n        name = item.get('name', default_value)\n        age = item.get('age', default_value)\n        \n        # Print the processed data\n        print(f\"Name: {name}, Age: {age}\")\n\n# Example usage\ndata = [\n    {'name': 'Alice', 'age': 30},\n    {'name': 'Bob'},\n    {'age': 25}\n]\n\nprocess_dicts(data)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2647903}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a MySQL database to Google Cloud Storage using an automated, one-time batch process. Create a DAG that demonstrates cloud data extraction and storage with built-in bucket management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.mysql.hooks.mysql import MySqlHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'mysql_to_gcs_dag',\n    default_args=default_args,\n    description='A DAG to migrate data from MySQL to Google Cloud Storage',\n    schedule_interval=None,\n)\n\n# Function to fetch data from MySQL\ndef fetch_data_from_mysql():\n    mysql_hook = MySqlHook(mysql_conn_id='mysql_conn_id')\n    query = \"SELECT * FROM your_table\"\n    df = mysql_hook.get_pandas_df(query)\n    return df\n\n# Function to upload data to Google Cloud Storage\ndef upload_data_to_gcs(df):\n    gcs_hook = GCSHook(google_cloud_storage_conn_id='google_cloud_storage_conn_id')\n    bucket_name = 'your_bucket_name'\n    object_name = 'your_object_name.csv'\n    gcs_hook.load_string(df.to_csv(index=False), object_name, bucket=bucket_name)\n\n# Task to fetch data from MySQL\nfetch_data_task = PythonOperator(\n    task_id='fetch_data_task',\n    python_callable=fetch_data_from_mysql,\n    dag=dag\n)\n\n# Task to upload data to Google Cloud Storage\nupload_data_task = PythonOperator(\n    task_id='upload_data_task',\n    python_callable=upload_data_to_gcs,\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nfetch_data_task >> upload_data_task"}], "metadata": {"file_name": "dag_220_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648025}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data movement process that extracts salary records from a BigQuery table and loads them into a PostgreSQL destination table. Develop a DAG that handles data transfer with configurable parameters and includes robust error handling and resource management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import GoogleCloudBigQueryHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'bigquery_to_postgres_dag',\n    default_args=default_args,\n    description='Automate data movement from BigQuery to PostgreSQL',\n    schedule_interval=timedelta(days=1)\n)\n\n# Function to extract data from BigQuery\ndef extract_data_from_bigquery():\n    hook = GoogleCloudBigQueryHook()\n    query = \"\"\"\n        SELECT *\n        FROM `your_project.your_dataset.your_table`\n    \"\"\"\n    df = hook.get_pandas_df(query=query)\n    return df\n\n# Function to load data into PostgreSQL\ndef load_data_into_postgresql(df):\n    hook = PostgresHook(postgres_conn_id='postgres_default')\n    conn = hook.get_connection('postgres_default')\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    create_table_query = \"\"\"\n        CREATE TABLE IF NOT EXISTS your_table (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            salary DECIMAL(10, 2)\n        )\n    \"\"\"\n    cursor.execute(create_table_query)\n\n    # Insert data into the table\n    insert_query = \"\"\"\n        INSERT INTO your_table (name, salary)\n        VALUES (%s, %s)\n    \"\"\"\n    for index, row in df.iterrows():\n        cursor.execute(insert_query, (row['name'], row['salary']))\n\n    cursor.close()\n    conn.commit()\n\n# Task to execute the extraction\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data_from_bigquery,\n    dag=dag\n)\n\n# Task to execute the loading\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data_into_postgresql,\n    dag=dag\n)\n\n# Set up dependencies\nextract_task >> load_task"}], "metadata": {"file_name": "dag_823_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648153}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide me with a Python code snippet that demonstrates how to use the `pandas` library to read a CSV file and filter rows based on a specific condition?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Filter rows where the age is greater than 30\nfiltered_df = df[df['Age'] > 30]\n\n# Display the filtered DataFrame\nprint(filtered_df)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2648213}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates parameter passing and environment variable testing in Airflow, showcasing different methods of injecting runtime configuration into tasks. Create a DAG that illustrates how parameters can be dynamically passed and accessed across Python and Bash tasks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'parameter_passing_and_environment_variable_test',\n    default_args=default_args,\n    description='A DAG to demonstrate parameter passing and environment variable testing in Airflow.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to set environment variables\nset_env_vars = BashOperator(\n    task_id='set_env_vars',\n    bash_command='export MY_VAR=\"Hello from Airflow\"',\n    dag=dag,\n)\n\n# Task to read environment variables\nread_env_vars = BashOperator(\n    task_id='read_env_vars',\n    bash_command='echo $MY_VAR',\n    dag=dag,\n)\n\n# Task to pass parameters to a Python operator\npass_params_to_python = PythonOperator(\n    task_id='pass_params_to_python',\n    python_callable=print_params,\n    op_kwargs={'param1': 'value1', 'param2': 'value2'},\n    dag=dag,\n)\n\n# Task to access parameters in a Bash operator\naccess_params_in_bash = BashOperator(\n    task_id='access_params_in_bash',\n    bash_command=f'echo \"Param1: {params[\"param1\"]}, Param2: {params[\"param2\"]}\"',\n    params=params,\n    dag=dag,\n)\n\n# Define a function to print parameters\ndef print_params(**kwargs):\n    params = kwargs.get('params')\n    if params:\n        print(\"Parameters:\", params)\n    else:\n        print(\"No parameters provided.\")\n\n# Define a dictionary with parameters\nparams = {'param1': 'value1', 'param2': 'value2'}\n\n# Add dependencies between tasks\nset_env_vars >> read_env_vars >> pass_params_to_python >> access_params_in_bash"}], "metadata": {"file_name": "dag_339_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648308}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that uploads a local file to Azure Data Lake Storage and then immediately deletes the uploaded file. Create a DAG that demonstrates file movement and cleanup in a cloud storage environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.adls import AzureDataLakeHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'azure_data_lake_upload_and_delete',\n    default_args=default_args,\n    description='Uploads a local file to Azure Data Lake Storage and deletes it.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the function to upload and delete the file\ndef upload_and_delete_file():\n    adls_hook = AzureDataLakeHook(adls_conn_id='adls_default')\n    \n    # Upload the file to ADLS\n    adls_hook.load_file(\n        filename='/path/to/local/file.txt', \n        container_name='your_container_name', \n        overwrite=True\n    )\n    \n    # Delete the file from ADLS\n    adls_hook.delete_file(\n        filename='/path/to/local/file.txt', \n        container_name='your_container_name'\n    )\n\n# Create a PythonOperator to execute the upload_and_delete_file function\nupload_task = PythonOperator(\n    task_id='upload_to_adls',\n    python_callable=upload_and_delete_file,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1750_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648401}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data processing pipeline that demonstrates Google Cloud Dataflow's workflow for batch text analysis, using a word count example with a sample text file. Design an Airflow DAG that manages the entire lifecycle of creating, running, and deleting a Dataflow pipeline while handling Google Cloud Storage operations.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    BigQueryCreateDatasetOperator,\n    BigQueryLoadTableFromTextOperator,\n    DataflowJobStartOperator,\n    DataflowJobDeleteOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'google_cloud_dataflow_word_count',\n    default_args=default_args,\n    description='A simple Dataflow pipeline for word count',\n    schedule_interval=None,\n)\n\n# Create a dataset in BigQuery\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='word_count_dataset',\n    location='US',\n)\n\n# Load text data into BigQuery\nload_text_data_task = BigQueryLoadTableFromTextOperator(\n    task_id='load_text_data',\n    destination_project_id='your-project-id',\n    destination_dataset_id='word_count_dataset',\n    destination_table_id='text_data',\n    source_uris=['gs://your-bucket/path/to/textfile.txt'],\n    schema=[\n        {'name': 'content', 'field_type': 'STRING'},\n    ],\n)\n\n# Start the Dataflow job\nstart_dataflow_job_task = DataflowJobStartOperator(\n    task_id='start_dataflow_job',\n    project_id='your-project-id',\n    region='us-central1',\n    template_path='gs://your-bucket/path/to/dataflow-template.json',\n    parameters={\n        'inputFilePattern': 'gs://your-bucket/path/to/textfile.txt',\n        'outputTable': 'word_count_dataset.text_output',\n    },\n)\n\n# Delete the Dataflow job\ndelete_dataflow_job_task = DataflowJobDeleteOperator(\n    task_id='delete_dataflow_job',\n    project_id='your-project-id',\n    region='us-central1',\n    job_id=start_dataflow_job_task.output['jobId'],\n)\n\n# Define dependencies\ncreate_dataset_task >> load_text_data_task >> start_dataflow_job_task >> delete_dataflow_job_task"}], "metadata": {"file_name": "dag_1216_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648497}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data workflow that illustrates how different data assets can be managed and scheduled using modern asset-based decorators and S3 storage. Create a pipeline that shows the relationship between data producers and consumers with minimal but clear processing steps.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_workflow',\n    default_args=default_args,\n    description='A data workflow that manages and schedules data assets using modern asset-based decorators and S3 storage.',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the data producers\n@task(task_id='producer_1')\ndef producer_1():\n    # Simulate producing data\n    return \"Data from producer 1\"\n\n@task(task_id='producer_2')\ndef producer_2():\n    # Simulate producing data\n    return \"Data from producer 2\"\n\n# Define the data consumers\n@task(task_id='consumer_1', depends_on=[producer_1])\ndef consumer_1(data):\n    # Process the data\n    print(f\"Processing data from producer 1: {data}\")\n\n@task(task_id='consumer_2', depends_on=[producer_2])\ndef consumer_2(data):\n    # Process the data\n    print(f\"Processing data from producer 2: {data}\")\n\n# Define the data storage\ns3_hook = S3Hook()\n\n# Define the data upload tasks\n@task(task_id='upload_data_1', depends_on=[producer_1])\ndef upload_data_1(data):\n    # Upload data to S3\n    s3_hook.load_string(data=data, bucket_name='my-bucket', key='data/producer_1.txt')\n\n@task(task_id='upload_data_2', depends_on=[producer_2])\ndef upload_data_2(data):\n    # Upload data to S3\n    s3_hook.load_string(data=data, bucket_name='my-bucket', key='data/producer_2.txt')\n\n# Define the data download tasks\n@task(task_id='download_data_1', depends_on=[upload_data_1])\ndef download_data_1():\n    # Download data from S3\n    data = s3_hook.read_key(key='data/producer_1.txt', bucket_name='my-bucket')\n    return data\n\n@task(task_id='download_data_2', depends_on=[upload_data_2])\ndef download_data_2():\n    # Download data from S3\n    data = s3_hook.read_key(key='data/producer_2.txt', bucket_name='my-bucket')\n    return data\n\n# Define the data processing tasks\n@task(task_id='process_data_1', depends_on=[download_data_1])\ndef process_data_1(data):\n    # Process the data\n    print(f\"Processing data from producer 1: {data}\")\n\n@task(task_id='process_data_2', depends_on=[download_data_2])\ndef process_data_2(data):\n    # Process the data\n    print(f\"Processing data from producer 2: {data}\")"}], "metadata": {"file_name": "dag_1844_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981677.26486}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data pipeline that illustrates how to orchestrate system-level tasks, including time-based scheduling, command execution, and basic error handling in a distributed computing environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example DAG',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to execute a command\ndef run_command():\n    print(\"Executing command...\")\n    # Replace this with your actual command\n    result = subprocess.run(['ls', '-l'], capture_output=True, text=True)\n    print(result.stdout)\n\nrun_command_task = PythonOperator(\n    task_id='run_command',\n    python_callable=run_command,\n    dag=dag\n)\n\n# Task to handle errors\ndef handle_error():\n    print(\"Handling error...\")\n\nhandle_error_task = PythonOperator(\n    task_id='handle_error',\n    python_callable=handle_error,\n    dag=dag\n)\n\n# Set up dependencies\nrun_command_task.set_upstream(handle_error_task)"}], "metadata": {"file_name": "dag_648_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981677.264869}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud infrastructure management by dynamically creating a Google Cloud Storage bucket, transferring data, and then cleaning up the bucket after the operation. Implement a sequential task flow that handles resource provisioning, data movement, and resource teardown.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalToGCSOperator\nfrom airflow.providers.google.cloud.operators.storage import CreateBucketOperator, DeleteBucketOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'google_cloud_storage_dag',\n    default_args=default_args,\n    description='A simple DAG to manage Google Cloud Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to create a Google Cloud Storage bucket\ncreate_bucket_task = CreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Task to upload data from local to GCS\nupload_data_task = LocalToGCSOperator(\n    task_id='upload_data',\n    source_path='/path/to/local/data.txt',\n    destination_path='my-gcs-bucket/my-data.txt',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Task to delete the Google Cloud Storage bucket\ndelete_bucket_task = DeleteBucketOperator(\n    task_id='delete_bucket',\n    bucket_name='my-gcs-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Define the task flow\ncreate_bucket_task >> upload_data_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_894_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648776}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that uses multimodal machine learning models to process and evaluate text and image data. The pipeline should leverage Google Cloud services to perform embedding, generation, and safety-filtered content evaluation across different input types.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\nfrom airflow.providers.google.cloud.operators.kubernetes import KubernetesPodOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.kubernetes import KubernetesPodHook\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'content_analysis_pipeline',\n    default_args=default_args,\n    description='An AI-powered content analysis pipeline using multimodal machine learning models',\n    schedule_interval=None,\n)\n\n# Create dataset if it doesn't exist\ncreate_dataset_operator = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='content_analysis_dataset',\n    project_id='your_project_id'\n)\n\n# Insert job into BigQuery\ninsert_job_operator = BigQueryInsertJobOperator(\n    task_id='insert_job',\n    configuration={\n        'load': {\n            'sourceUris': ['gs://your_bucket/path/to/input_data'],\n            'destinationTable': {\n                'projectId': 'your_project_id',\n                'datasetId': 'content_analysis_dataset',\n                'tableId': 'input_table'\n            },\n            'schema': {\n                'fields': [\n                    {'name': 'id', 'type': 'STRING'},\n                    {'name': 'text', 'type': 'STRING'},\n                    {'name': 'image_url', 'type': 'STRING'}\n                ]\n            }\n        }\n    },\n    project_id='your_project_id'\n)\n\n# Define the Kubernetes Pod Operator for running the content analysis pipeline\nkubernetes_pod_operator = KubernetesPodOperator(\n    task_id='run_content_analysis',\n    namespace='default',\n    image='gcr.io/your_gcp_project/content-analysis-pipeline:v1',\n    arguments=[\n        '--input-data', 'gs://your_bucket/path/to/input_data',\n        '--output-data', 'gs://your_bucket/path/to/output_data'\n    ],\n    secrets=[{'name': 'google-cloud-keyfile-json', 'value': '/path/to/google-cloud-keyfile.json'}],\n    env_vars={'GOOGLE_APPLICATION_CREDENTIALS': '/path/to/google-cloud-keyfile.json'}\n)\n\n# Define the Kubernetes Pod Operator for running the safety filtering pipeline\nkubernetes_safety_filtering_operator = KubernetesPodOperator(\n    task_id='run_safety_filtering',\n    namespace='default',\n    image='gcr.io/your_gcp_project/safety-filtering-pipeline:v1',\n    arguments=[\n        '--input-data', 'gs://your_bucket/path/to/output_data',\n        '--output-data', 'gs://your_bucket/path/to/final_output_data'\n    ],\n    secrets=[{'name': 'google-cloud-keyfile-json', 'value': '/path/to/google-cloud-keyfile.json'}],\n    env_vars={'GOOGLE_APPLICATION_CREDENTIALS': '/path/to/google-cloud-keyfile.json'}\n)\n\n# Define the dependencies between tasks\ncreate_dataset_operator >> insert_job_operator >> kubernetes_pod_operator >> kubernetes_safety_filtering_operator"}], "metadata": {"file_name": "dag_1714_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981677.2648907}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data orchestration process that extracts tabular data from a Google Spreadsheet and securely transfers it to a cloud storage solution with dynamic environment-based naming. Create a repeatable template for extracting structured data from cloud-based spreadsheet services and preparing it for downstream processing.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.sheets.hooks.gsheets import GoogleSheetsHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nimport os\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'extract_and_transfer_data',\n    default_args=default_args,\n    description='Extracts tabular data from Google Sheets and transfers it to AWS S3',\n    schedule_interval=None,\n)\n\n# Define the function to extract data from Google Sheets\ndef extract_data_from_google_sheets():\n    # Set up Google Sheets hook\n    sheets_hook = GoogleSheetsHook(google_cloud_default_project_id=\"your-project-id\")\n    \n    # Get the sheet ID\n    sheet_id = \"your-sheet-id\"\n    \n    # Extract data from the sheet\n    df = sheets_hook.get_pandas_dataframe(sheet_id=sheet_id)\n    \n    return df\n\n# Define the function to transfer data to AWS S3\ndef transfer_data_to_aws_s3(df):\n    # Set up S3 hook\n    s3_hook = S3Hook(aws_conn_id=\"your-aws-s3-conn-id\")\n    \n    # Define the bucket name\n    bucket_name = \"your-bucket-name\"\n    \n    # Define the file name\n    file_name = f\"extracted_data_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\"\n    \n    # Write the dataframe to a CSV file\n    df.to_csv(os.path.join(\"/tmp\", file_name), index=False)\n    \n    # Upload the file to S3\n    s3_hook.load_file(file_path=os.path.join(\"/tmp\", file_name), key=file_name, bucket=bucket_name, replace=True)\n\n# Define the task to extract data from Google Sheets\nextract_data_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data_from_google_sheets,\n    dag=dag\n)\n\n# Define the task to transfer data to AWS S3\ntransfer_data_task = PythonOperator(\n    task_id='transfer_data',\n    python_callable=transfer_data_to_aws_s3,\n    dag=dag\n)\n\n# Define the dependency between tasks\nextract_data_task >> transfer_data_task"}], "metadata": {"file_name": "dag_1905_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649012}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases a systematic cloud storage transfer process with clear setup, execution, and teardown stages. Implement a workflow that creates a temporary S3 bucket, transfers data from Azure Blob Storage, and then deletes the bucket after completion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.storage import AzureBlobStorageHook\nfrom airflow.providers.apache.sftp.hooks.sftp import SFTPHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'cloud_storage_transfer_dag',\n    default_args=default_args,\n    description='A DAG to transfer data between Azure Blob Storage and Amazon S3',\n    schedule_interval=timedelta(days=1)\n)\n\n# Step 1: Set up a temporary S3 bucket\nsetup_s3_bucket_task = PythonOperator(\n    task_id='setup_s3_bucket',\n    python_callable=set_up_s3_bucket,\n    dag=dag\n)\n\n# Step 2: Transfer data from Azure Blob Storage to Amazon S3\ntransfer_data_task = PythonOperator(\n    task_id='transfer_data',\n    python_callable=transfer_data,\n    dag=dag\n)\n\n# Step 3: Delete the temporary S3 bucket\ndelete_s3_bucket_task = PythonOperator(\n    task_id='delete_s3_bucket',\n    python_callable=delete_s3_bucket,\n    dag=dag\n)\n\n# Define functions for each step\ndef set_up_s3_bucket():\n    # Code to set up a temporary S3 bucket\n    print(\"Setting up a temporary S3 bucket\")\n\ndef transfer_data():\n    # Code to transfer data from Azure Blob Storage to Amazon S3\n    azure_blob_storage_hook = AzureBlobStorageHook(azure_conn_id='azure_default')\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n\n    # Example: Copying a file from Azure Blob Storage to Amazon S3\n    blob_name = 'example.txt'\n    container_name = 'source_container'\n    object_key = f'{container_name}/{blob_name}'\n    local_file_path = '/path/to/local/file.txt'\n\n    azure_blob_storage_hook.download_file(container_name, blob_name, local_file_path)\n    s3_hook.load_file(local_file_path, object_key, bucket_name='destination_bucket', replace=True)\n\ndef delete_s3_bucket():\n    # Code to delete the temporary S3 bucket\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    bucket_name = 'temporary_bucket'\n    s3_hook.delete_bucket(bucket_name=bucket_name)\n\n# Define dependencies\nsetup_s3_bucket_task >> transfer_data_task >> delete_s3_bucket_task"}], "metadata": {"file_name": "dag_1071_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981677.264912}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a reproducible machine learning data processing pipeline for housing price prediction that leverages cloud infrastructure and automated workflow management. Create a system that transforms local housing data into a cloud-based training job with standardized dataset preparation and model training steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.redshift import RedshiftOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='housing_price_prediction_pipeline',\n    description='A pipeline for predicting housing prices using machine learning',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1)\n) as dag:\n\n    # Task to download housing data from S3\n    download_data_task = PythonOperator(\n        task_id='download_housing_data',\n        python_callable=download_housing_data,\n        op_kwargs={\n            'bucket_name': 'your-housing-data-bucket',\n            'file_key': 'housing_data.csv'\n        }\n    )\n\n    # Task to preprocess the downloaded data\n    preprocess_data_task = PythonOperator(\n        task_id='preprocess_housing_data',\n        python_callable=preprocess_housing_data,\n        op_kwargs={\n            'input_file_path': '/path/to/downloaded/housing_data.csv',\n            'output_file_path': '/path/to/preprocessed/housing_data.csv'\n        }\n    )\n\n    # Task to load preprocessed data into Redshift\n    load_data_to_redshift_task = RedshiftOperator(\n        task_id='load_data_to_redshift',\n        redshift_conn_id='redshift_default',\n        sql_statement=load_data_sql,\n        parameters={\n            'table_name': 'housing_data',\n            'input_file_path': '/path/to/preprocessed/housing_data.csv'\n        }\n    )\n\n    # Task to train the machine learning model\n    train_model_task = PythonOperator(\n        task_id='train_model',\n        python_callable=train_model,\n        op_kwargs={\n            'model_type': 'random_forest',\n            'training_table': 'housing_data',\n            'target_column': 'price'\n        }\n    )\n\n    # Task to evaluate the trained model\n    evaluate_model_task = PythonOperator(\n        task_id='evaluate_model',\n        python_callable=evaluate_model,\n        op_kwargs={\n            'model_type': 'random_forest',\n            'test_table': 'housing_data_test',\n            'target_column': 'price'\n        }\n    )\n\n    # Task to upload evaluation results to S3\n    upload_results_task = PythonOperator(\n        task_id='upload_evaluation_results',\n        python_callable=upload_results,\n        op_kwargs={\n            'bucket_name': 'your-evaluation-results-bucket',\n            'file_key': 'evaluation_results.json'\n        }\n    )\n\n    # Define the dependencies between tasks\n    download_data_task >> preprocess_data_task >> load_data_to_redshift_task >> train_model_task >> evaluate_model_task >> upload_results_task"}], "metadata": {"file_name": "dag_259_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981677.264924}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me extract specific elements from a JSON document using Python?"}, {"role": "assistant", "content": "import json\n\n# Sample JSON data as a string\njson_data = '''\n{\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"city\": \"New York\",\n    \"hobbies\": [\"reading\", \"traveling\", \"coding\"],\n    \"address\": {\n        \"street\": \"123 Main St\",\n        \"city\": \"Anytown\",\n        \"zip\": \"12345\"\n    }\n}\n'''\n\n# Parse the JSON string into a Python dictionary\ndata = json.loads(json_data)\n\n# Extracting specific elements\nname = data['name']\nage = data['age']\nhobbies = data['hobbies'][0]\nstreet = data['address']['street']\n\nprint(\"Name:\", name)\nprint(\"Age:\", age)\nprint(\"Hobby:\", hobbies)\nprint(\"Street:\", street)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2649293}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a machine learning pipeline that demonstrates Vertex AI dataset creation and pipeline job management using Google Cloud resources. Create a workflow that synchronizes housing data, creates a tabular dataset, runs a predefined ML pipeline, and then manages the pipeline job lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.vertex_ai import (\n    CreateDatasetOperator,\n    RunPipelineJobOperator,\n    DeleteDatasetOperator,\n)\nfrom datetime import datetime, timedelta\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='vertex_ai_pipeline_dag',\n    default_args=default_args,\n    description='A DAG to demonstrate Vertex AI dataset creation, pipeline execution, and lifecycle management.',\n    schedule_interval=timedelta(days=1),\n) as dag:\n\n    # Step 1: Create a Vertex AI Dataset\n    create_dataset = CreateDatasetOperator(\n        task_id='create_dataset',\n        project_id='your-project-id',  # Replace with your GCP project ID\n        display_name='Housing Data Dataset',\n        metadata={\n            'labels': {'environment': 'production'},\n        },\n    )\n\n    # Step 2: Run a Predefined ML Pipeline\n    run_pipeline_job = RunPipelineJobOperator(\n        task_id='run_pipeline_job',\n        project_id='your-project-id',  # Replace with your GCP project ID\n        location='us-central1',  # Replace with your desired region\n        pipeline_resource_id='your-pipeline-resource-id',  # Replace with your pipeline resource ID\n        parameters={\n            'input_data_uri': 'gs://your-bucket/housing-data.csv',  # Replace with your input data URI\n        },\n    )\n\n    # Step 3: Manage the Pipeline Job Lifecycle\n    delete_dataset = DeleteDatasetOperator(\n        task_id='delete_dataset',\n        project_id='your-project-id',  # Replace with your GCP project ID\n        dataset_id='housing-data-dataset',  # Replace with your dataset ID\n    )\n\n    # Set dependencies between tasks\n    create_dataset >> run_pipeline_job >> delete_dataset"}], "metadata": {"file_name": "dag_1327_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649388}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a string that contains multiple pieces of information about a location, separated by commas. The string looks something like this: \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\". I need to extract the name, location, category, and rating from this string and store them in separate variables. How can I do this in Python?"}, {"role": "assistant", "content": "# Given string containing location details\nlocation_string = \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\"\n\n# Splitting the string using comma as the delimiter\ndetails_list = location_string.split(',')\n\n# Extracting individual pieces of information\nname = details_list[0].strip()  # Removing any leading/trailing whitespace\nlocation = details_list[1].strip()\ncategory = details_list[2].strip()\nrating = details_list[3].strip()\n\n# Printing the extracted information\nprint(\"Name:\", name)\nprint(\"Location:\", location)\nprint(\"Category:\", category)\nprint(\"Rating:\", rating)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2649443}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the core principles of task sequencing and SQL query execution using the SQLExecuteQueryOperator. Develop a workflow that connects to a Presto database and demonstrates fundamental data manipulation tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.presto.operators.sql_execute_query import SQLExecuteQueryOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'presto_example_dag',\n    default_args=default_args,\n    description='A simple DAG to execute a SQL query against a Presto database',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the SQL query\nsql_query = \"\"\"\nSELECT * FROM your_presto_table;\n\"\"\"\n\n# Create the SQLExecuteQueryOperator\nexecute_sql_task = SQLExecuteQueryOperator(\n    task_id='execute_sql_task',\n    sql=sql_query,\n    presto_conn_id='your_presto_conn',  # Replace with your Presto connection ID\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2192_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649531}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates how to ingest data into Weaviate vector database using custom vectors and different data input methods. Create a workflow that shows how to create a Weaviate class, prepare data, and load it into the vector database using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.weaviate.hooks.weaviate import WeaviateHook\nfrom airflow.providers.http.operators.http import HttpOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='weaviate_ingestion_dag',\n    description='DAG to ingest data into Weaviate vector database',\n    default_args=default_args,\n    schedule_interval=None  # Set this to your desired schedule interval\n) as dag:\n\n    # Step 1: Create a Weaviate class\n    create_class_task = PythonOperator(\n        task_id='create_weaviate_class',\n        python_callable=create_weaviate_class,\n        op_kwargs={\n            'class_name': 'MyClass',\n            'properties': [\n                {'name': 'text', 'dataType': ['text']},\n                {'name': 'vector', 'dataType': ['float']}\n            ]\n        }\n    )\n\n    # Step 2: Prepare data\n    prepare_data_task = PythonOperator(\n        task_id='prepare_data',\n        python_callable=prepare_data,\n        op_kwargs={\n            'data': [\n                {'text': 'Hello, World!', 'vector': [0.1, 0.2, 0.3, 0.4, 0.5]},\n                {'text': 'This is an example.', 'vector': [0.6, 0.7, 0.8, 0.9, 1.0]}\n            ]\n        }\n    )\n\n    # Step 3: Load data into Weaviate\n    load_data_task = HttpOperator(\n        task_id='load_data_to_weaviate',\n        method='POST',\n        endpoint='http://localhost:8080/v1/batches',\n        headers={'Content-Type': 'application/json'},\n        json={\n            \"requests\": [\n                {\n                    \"class\": \"MyClass\",\n                    \"data\": [\n                        {\"text\": \"Hello, World!\", \"vector\": [0.1, 0.2, 0.3, 0.4, 0.5]},\n                        {\"text\": \"This is an example.\", \"vector\": [0.6, 0.7, 0.8, 0.9, 1.0]}\n                    ]\n                }\n            ]\n        },\n        dag=dag\n    )\n\n    # Define dependencies\n    create_class_task >> prepare_data_task >> load_data_task"}], "metadata": {"file_name": "dag_1016_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649648}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a BigQuery data pipeline that demonstrates creating datasets, tables, and executing SQL queries for data insertion and retrieval. Build a workflow that showcases different BigQuery operators and demonstrates basic data management techniques in a cloud data warehouse environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryInsertJobOperator,\n    BigQueryExecuteQueryOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'bigquery_data_pipeline',\n    default_args=default_args,\n    description='A simple BigQuery data pipeline',\n    schedule_interval=None,\n)\n\n# Create Dataset\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='my_dataset',\n    project_id='your_project_id',\n    location='US',  # Replace with your desired location\n)\n\n# Create Table\ncreate_table_task = BigQueryCreateTableOperator(\n    task_id='create_table',\n    dataset_id='my_dataset',\n    table_id='my_table',\n    schema=[\n        ('id', 'INTEGER'),\n        ('name', 'STRING'),\n        ('age', 'INTEGER'),\n    ],\n    project_id='your_project_id',\n    location='US',  # Replace with your desired location\n)\n\n# Insert Data into Table\ninsert_data_task = BigQueryInsertJobOperator(\n    task_id='insert_data',\n    destination_dataset_table='my_dataset.my_table',\n    configuration={\n        'load': {\n            'sourceUris': ['gs://your_bucket/path/to/data.csv'],\n            'schema': {\n                'fields': [\n                    {'name': 'id', 'type': 'INTEGER'},\n                    {'name': 'name', 'type': 'STRING'},\n                    {'name': 'age', 'type': 'INTEGER'},\n                ]\n            },\n        }\n    },\n    project_id='your_project_id',\n    location='US',  # Replace with your desired location\n)\n\n# Execute Query\nexecute_query_task = BigQueryExecuteQueryOperator(\n    task_id='execute_query',\n    sql=\"\"\"\n        SELECT * FROM my_dataset.my_table;\n    \"\"\",\n    use_legacy_sql=False,\n    project_id='your_project_id',\n    location='US',  # Replace with your desired location\n)\n\n# Define dependencies\ncreate_dataset_task >> create_table_task >> insert_data_task >> execute_query_task"}], "metadata": {"file_name": "dag_471_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649753}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What are the differences between synchronous and asynchronous programming in Python? Can you provide a simple example to illustrate the difference?"}, {"role": "assistant", "content": "# Synchronous Programming Example\ndef print_numbers(n):\n    for i in range(n):\n        print(i)\n\nprint_numbers(5)  # This will wait for each number to be printed before printing the next\n\n# Asynchronous Programming Example using asyncio\nimport asyncio\n\nasync def print_number_async(n):\n    for i in range(n):\n        await asyncio.sleep(1)  # Simulate some delay\n        print(i)\n\nasync def main():\n    await asyncio.gather(print_number_async(5), print_number_async(5))\n\nasyncio.run(main())"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2649815}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one DAG can initiate another DAG with a predefined configuration message.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "# Import necessary modules\n   from datetime import timedelta\n   from airflow import DAG\n   from airflow.providers.apache_airflow.hooks.base import BaseHook\n   from airflow.operators.trigger_dagrun_operator import TriggerDagRunOperator\n\n   # Define the default arguments for the DAG\n   default_args = {\n       'owner': 'airflow',\n       'depends_on_past': False,\n       'start_date': datetime(2023, 1, 1),\n       'email_on_failure': False,\n       'email_on_retry': False,\n       'retries': 1,\n       'retry_delay': timedelta(minutes=5)\n   }\n\n   # Create the primary DAG\n   with DAG(\n       dag_id='primary_dag',\n       default_args=default_args,\n       description='A simple example of triggering a secondary DAG',\n       schedule_interval=timedelta(days=1)\n   ) as dag:\n\n       # Function to send a message to the secondary DAG\n       def send_message_to_secondary():\n           hook = BaseHook.get_hook('my_base_hook')\n           hook.send_message('Hello from primary DAG!')\n\n       # Trigger the secondary DAG using TriggerDagRunOperator\n       trigger_secondary_dag = TriggerDagRunOperator(\n           task_id='trigger_secondary_dag',\n           dag_id='secondary_dag',\n           conf={'message': 'This is a test message'},\n           wait_for_completion=True,\n           poke_interval=60\n       )\n\n       # Execute the function to send the message\n       send_message_to_secondary()"}], "metadata": {"file_name": "dag_1056_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981677.2649994}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Dataproc workflow template in Google Cloud, showing how to set up a Hadoop/Spark cluster and run a Pig job. The workflow should illustrate the lifecycle of a cloud data processing job from template creation to execution.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocSubmitJobOperator,\n    DataprocDeleteClusterOperator,\n)\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'dataproc_pig_workflow',\n    default_args=default_args,\n    description='A DAG to create and execute a Dataproc workflow template for running a Pig job.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Step 1: Create a Dataproc cluster\ncreate_cluster_task = DataprocCreateClusterOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='pig-cluster',\n    master_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 50,\n    },\n    worker_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 50,\n    },\n    num_workers=2,\n)\n\n# Step 2: Submit a Pig job to the cluster\nsubmit_job_task = DataprocSubmitJobOperator(\n    task_id='submit_pig_job',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='pig-cluster',\n    main_class='org.apache.pig.Pig',\n    jar_file_uri='gs://path/to/pig-job.jar',\n    args=['-f', 'input.txt'],\n)\n\n# Step 3: Delete the Dataproc cluster\ndelete_cluster_task = DataprocDeleteClusterOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='pig-cluster',\n)\n\n# Define the dependencies between tasks\ncreate_cluster_task >> submit_job_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_525_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981677.2650073}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete cloud data transfer process, including creating a DynamoDB table, enabling point-in-time recovery, and performing a segmented export to S3. Implement a workflow that showcases dynamic resource management and cloud service interactions.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.dynamodb import DynamoDBHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.dynamodb import CreateTableOperator, UpdateTableOperator\nfrom airflow.providers.amazon.aws.operators.s3 import CopyObjectOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'cloud_data_transfer_dag',\n    default_args=default_args,\n    description='A DAG for orchestrating a complete cloud data transfer process.',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Create DynamoDB table\ncreate_table_task = CreateTableOperator(\n    task_id='create_dynamodb_table',\n    aws_conn_id='aws_default',\n    dynamodb_table_name='my_dynamodb_table',\n    key_schema=[\n        {'AttributeName': 'id', 'KeyType': 'HASH'},\n    ],\n    attribute_definitions=[\n        {'AttributeName': 'id', 'AttributeType': 'S'},\n    ],\n    provisioned_throughput={\n        'ReadCapacityUnits': 5,\n        'WriteCapacityUnits': 5,\n    },\n    dag=dag,\n)\n\n# Enable point-in-time recovery\nenable_point_in_time_recovery_task = UpdateTableOperator(\n    task_id='enable_point_in_time_recovery',\n    aws_conn_id='aws_default',\n    dynamodb_table_name='my_dynamodb_table',\n    update_expression='SET PointInTimeRecoveryEnabled = :p',\n    expression_attribute_values={':p': True},\n    dag=dag,\n)\n\n# Perform segmented export to S3\ncopy_object_task = CopyObjectOperator(\n    task_id='copy_segmented_export_to_s3',\n    aws_conn_id='aws_default',\n    s3_bucket_name='my_s3_bucket',\n    s3_key_prefix='segmented_exports/',\n    source_file_path='/path/to/your/data.csv',\n    destination_file_path='segmented_exports/my_data.csv',\n    dag=dag,\n)\n\n# Define dependencies\ncreate_table_task >> enable_point_in_time_recovery_task >> copy_object_task"}], "metadata": {"file_name": "dag_722_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2650168}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Could you provide a detailed explanation of how to build a recommendation system using collaborative filtering in Python? Building a recommendation system can be quite complex, and I'm looking for a step-by-step guide."}, {"role": "assistant", "content": "import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.neighbors import NearestNeighbors"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2650254}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data integrity verification workflow that uses SQL operators to perform systematic checks on a database table's structure and content. Build a DAG that validates column-level constraints and confirms the expected number of rows in the target table.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.sql import SQLOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-10-01',\n    'retries': 3,\n    'retry_delay': 5\n}\n\ndag = DAG(\n    'verify_database_integrity',\n    default_args=default_args,\n    description='A DAG to verify the integrity of a database table',\n    schedule_interval=None,\n)\n\n# Define the SQL query to check column-level constraints\ncheck_column_constraints_query = \"\"\"\nSELECT \n    column_name, \n    constraint_type, \n    constraint_expression \nFROM information_schema.constraint_columns \nWHERE table_name = 'your_table_name';\n\"\"\"\n\n# Define the SQL query to count the number of rows in the target table\ncount_rows_query = \"\"\"\nSELECT COUNT(*) FROM your_table_name;\n\"\"\"\n\n# Create an instance of the PostgresHook\npostgres_hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n\n# Execute the SQL query to check column-level constraints\ncheck_column_constraints_task = SQLOperator(\n    task_id='check_column_constraints',\n    sql=check_column_constraints_query,\n    postgres_conn_id='your_postgres_connection',\n    dag=dag\n)\n\n# Execute the SQL query to count the number of rows in the target table\ncount_rows_task = SQLOperator(\n    task_id='count_rows',\n    sql=count_rows_query,\n    postgres_conn_id='your_postgres_connection',\n    dag=dag\n)\n\n# Set up the dependencies between tasks\ncheck_column_constraints_task.set_upstream(count_rows_task)"}], "metadata": {"file_name": "dag_784_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.265034}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates complex marketing data operations across Google Cloud Platform services, focusing on extracting advertising performance metrics and managing data transfer between Google Display Video 360 and BigQuery. Implement tasks for downloading line items, uploading files, and performing data transformations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDownloadObjectOperator,\n)\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'google_display_video_to_bigquery_dag',\n    default_args=default_args,\n    description='DAG to orchestrate Google Display Video to BigQuery operations',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Create a bucket in GCS if it doesn't exist\ncreate_bucket_task = GCSCreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='display-video-to-bigquery-bucket',\n)\n\n# Download line items from Google Display Video 360\ndownload_line_items_task = GCSDownloadObjectOperator(\n    task_id='download_line_items',\n    object_name='line-items.csv',\n    bucket_name='display-video-to-bigquery-bucket',\n    filename='/path/to/local/downloaded/file.csv',\n)\n\n# Process the downloaded file using Python script\nprocess_file_task = BashOperator(\n    task_id='process_file',\n    bash_command='python /path/to/your/process_line_items.py',\n    env={\n        'INPUT_FILE': '/path/to/local/downloaded/file.csv',\n        'OUTPUT_FILE': '/path/to/local/processed/file.csv'\n    }\n)\n\n# Upload the processed file to BigQuery\nupload_to_bigquery_task = BigQueryInsertJobOperator(\n    task_id='upload_to_bigquery',\n    configuration={\n        'load': {\n            'sourceUris': ['gs://display-video-to-bigquery-bucket/processed/file.csv'],\n            'destinationTable': {\n                'projectId': 'your-project-id',\n                'datasetId': 'your-dataset-id',\n                'tableId': 'your-table-id'\n            },\n            'schema': {\n                'fields': [\n                    {'name': 'id', 'type': 'STRING'},\n                    {'name': 'name', 'type': 'STRING'},\n                    {'name': 'status', 'type': 'STRING'},\n                    # Add more fields as needed\n                ]\n            }\n        }\n    }\n)\n\n# Define the dependencies\ncreate_bucket_task >> download_line_items_task >> process_file_task >> upload_to_bigquery_task"}], "metadata": {"file_name": "dag_711_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2650445}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Data Catalog tag template management workflow that demonstrates creating, retrieving, and modifying metadata templates for data governance. Build an Airflow DAG that showcases the lifecycle of a tag template with different operational steps.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.datacatalog import DataCatalogClient\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_catalog_tag_template_management',\n    default_args=default_args,\n    description='Google Cloud Data Catalog Tag Template Management Workflow',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to create a tag template\ncreate_tag_template_task = DataCatalogClient.create_tag_template(\n    parent=\"projects/your-project-id/locations/global/catalogs/default\",\n    tag_template_id=\"my-tag-template\",\n    tag_template={\n        \"schema\": {\n            \"fields\": [\n                {\"name\": \"id\", \"type\": \"STRING\"},\n                {\"name\": \"name\", \"type\": \"STRING\"},\n                {\"name\": \"description\", \"type\": \"STRING\"}\n            ]\n        }\n    },\n    dag=dag\n)\n\n# Task to retrieve a tag template\nretrieve_tag_template_task = DataCatalogClient.get_tag_template(\n    name=f\"projects/your-project-id/locations/global/catalogs/default/tagTemplates/my-tag-template\",\n    dag=dag\n)\n\n# Task to update a tag template\nupdate_tag_template_task = DataCatalogClient.update_tag_template(\n    name=f\"projects/your-project-id/locations/global/catalogs/default/tagTemplates/my-tag-template\",\n    tag_template={\n        \"schema\": {\n            \"fields\": [\n                {\"name\": \"id\", \"type\": \"STRING\"},\n                {\"name\": \"name\", \"type\": \"STRING\"},\n                {\"name\": \"description\", \"type\": \"STRING\"},\n                {\"name\": \"status\", \"type\": \"STRING\"}\n            ]\n        }\n    },\n    update_mask={\"paths\": [\"schema.fields.status\"]}\n)"}], "metadata": {"file_name": "dag_947_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981677.2650602}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that calculates the total cost of a list of items, where each item is represented as a dictionary with keys 'name', 'quantity', and 'price_per_unit'. The function should return the total cost of all items."}, {"role": "assistant", "content": "# Define a function to calculate the total cost of items in a list\ndef calculate_total_cost(items):\n    # Initialize the total cost to 0\n    total_cost = 0\n    \n    # Iterate over each item in the list\n    for item in items:\n        # Calculate the cost of the current item and add it to the total cost\n        total_cost += item['quantity'] * item['price_per_unit']\n    \n    # Return the total cost\n    return total_cost\n\n# Example usage of the function\nitems = [\n    {'name': 'Apple', 'quantity': 5, 'price_per_unit': 1.2},\n    {'name': 'Banana', 'quantity': 3, 'price_per_unit': 0.8},\n    {'name': 'Cherry', 'quantity': 7, 'price_per_unit': 1.5}\n]\n\ntotal_cost = calculate_total_cost(items)\nprint(f\"The total cost of the items is: ${total_cost:.2f}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2650676}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data movement and management pipeline that transfers files between cloud storage buckets while implementing systematic tracking and cleanup procedures. Create a workflow that demonstrates how data can be safely moved, listed, and deleted using cloud-native orchestration techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.transfers.s3_to_s3_operator import S3ToS3Operator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    's3_to_s3_dag',\n    default_args=default_args,\n    description='A simple DAG to move files between S3 buckets',\n    schedule_interval=None,\n)\n\n# Define the source and destination S3 buckets\nsource_bucket = 'source-bucket'\ndestination_bucket = 'destination-bucket'\n\n# Define the key prefix in the source bucket\nkey_prefix = 'data/'\n\n# Define the task to copy files from source to destination\ncopy_task = S3ToS3Operator(\n    task_id='copy_files',\n    s3_source_key=key_prefix,\n    s3_destination_key=key_prefix,\n    aws_conn_id='aws_default',\n    replace=True,\n    dag=dag,\n)\n\n# Define the task to log the number of copied files\nlog_task = BashOperator(\n    task_id='log_files',\n    bash_command=f'echo \"Copied {copy_task.output} files\"',\n    dag=dag,\n)\n\n# Define the task to delete the source files after copying\ndelete_task = BashOperator(\n    task_id='delete_source_files',\n    bash_command=f'aws s3 rm --recursive s3://{source_bucket}/{key_prefix}',\n    dag=dag,\n)\n\n# Set up dependencies\ncopy_task >> log_task >> delete_task"}], "metadata": {"file_name": "dag_2055_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981677.265076}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that takes a string and returns a new string with the first letter of each word capitalized."}, {"role": "assistant", "content": "def capitalize_words(text):\n    \"\"\"\n    Capitalize the first letter of each word in a given string.\n\n    Args:\n    text (str): The input string to be processed.\n\n    Returns:\n    str: A new string with the first letter of each word capitalized.\n    \"\"\"\n    # Split the string into words\n    words = text.split()\n    \n    # Capitalize the first letter of each word\n    capitalized_words = [word.capitalize() for word in words]\n    \n    # Join the capitalized words back into a single string\n    return ' '.join(capitalized_words)\n\n# Example usage:\ninput_text = \"hello world this is a test\"\noutput_text = capitalize_words(input_text)\nprint(output_text)  # Output: Hello World This Is A Test"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981677.2650824}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data profiling pipeline that uses AWS Glue DataBrew to analyze a JSON dataset stored in S3, generating insights and managing computational resources dynamically. Construct a workflow that shows how to programmatically profile data with full lifecycle management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.glue_data_brew import GlueDataBrewHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'glue_data_profiling_pipeline',\n    default_args=default_args,\n    description='Automated data profiling pipeline using AWS Glue DataBrew',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to set up AWS Glue DataBrew\nsetup_glue_data_brew = PythonOperator(\n    task_id='setup_glue_data_brew',\n    python_callable=set_up_glue_data_brew,\n    dag=dag\n)\n\n# Task to run the AWS Glue DataBrew job\nrun_glue_data_brew_job = PythonOperator(\n    task_id='run_glue_data_brew_job',\n    python_callable=run_glue_data_brew_job,\n    dag=dag\n)\n\n# Task to clean up resources\ncleanup_resources = PythonOperator(\n    task_id='cleanup_resources',\n    python_callable=cleanup_resources,\n    dag=dag\n)\n\n# Set up dependencies\nsetup_glue_data_brew >> run_glue_data_brew_job >> cleanup_resources"}], "metadata": {"file_name": "dag_1904_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981677.2650907}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers CSV files from Google Cloud Storage to a Presto table, enabling seamless data movement between cloud storage and analytics platforms.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_presto import GCSToPrestoOperator\nfrom airflow.providers.presto.hooks.presto import PrestoHook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'gcs_to_presto_dag',\n    default_args=default_args,\n    description='Transfer CSV files from GCS to Presto',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the source bucket and destination table\nsource_bucket = 'your-source-bucket'\ndestination_table = 'your-destination-table'\n\n# Create a hook for Presto\npresto_hook = PrestoHook()\n\n# Define the task to transfer CSV files from GCS to Presto\ntransfer_task = GCSToPrestoOperator(\n    task_id='transfer_csv_to_presto',\n    presto_conn_id='your-presto-conn-id',  # Replace with your Presto connection ID\n    sql=f\"INSERT INTO {destination_table} SELECT * FROM @{source_bucket}/path/to/your/csv/files/*.csv\",\n    gcp_conn_id='your-gcp-conn-id',  # Replace with your Google Cloud Storage connection ID\n    dag=dag\n)\n\n# Define the task to check if the Presto table exists\ncheck_table_exists_task = PrestoHook().get_presto_table_info(destination_table)\n\n# Define the task to log the result of the check\nlog_result_task = BashOperator(\n    task_id='log_result',\n    bash_command=f'echo \"Table {destination_table} exists: {check_table_exists_task.table_exists}\"',\n    dag=dag\n)\n\n# Define the dependencies between tasks\ntransfer_task >> check_table_exists_task >> log_result_task"}], "metadata": {"file_name": "dag_301_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.570559}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates conditional task execution using short-circuit logic, where certain tasks only run based on predefined boolean conditions. Create a DAG that showcases how tasks can be dynamically skipped or executed depending on specific criteria.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n}\n\ndag = DAG(\n    'conditional_task_execution_dag',\n    default_args=default_args,\n    schedule_interval='@daily'\n)\n\ndef check_condition():\n    # This function checks if the condition is true or false\n    return True\n\ndef task_a():\n    print(\"Task A is running\")\n\ndef task_b():\n    print(\"Task B is running\")\n\ndef task_c():\n    print(\"Task C is running\")\n\n# Define the tasks\ntask_a = PythonOperator(task_id='task_a', python_callable=task_a)\ntask_b = PythonOperator(task_id='task_b', python_callable=task_b)\ntask_c = PythonOperator(task_id='task_c', python_callable=task_c)\n\n# Set up the dependencies\nif check_condition():\n    task_a.set_upstream(None)\n    task_b.set_upstream(task_a)\nelse:\n    task_b.set_upstream(None)\n\nif check_condition() and not check_condition():\n    task_c.set_upstream(task_b)"}], "metadata": {"file_name": "dag_24_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.5705693}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates container-based task scheduling and execution using Docker Swarm operator. Create a workflow that launches a CentOS container, executes a brief sleep operation, and automatically removes the container after completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'docker_swarm_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Docker Swarm with Airflow',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Task to launch a CentOS container\nlaunch_container_task = DockerOperator(\n    task_id='launch_centos_container',\n    image='centos:latest',\n    command=['sleep', '60'],\n    docker_url='unix:///var/run/docker.sock',\n    network_mode='bridge',\n    dag=dag\n)\n\n# Task to remove the container\nremove_container_task = DockerOperator(\n    task_id='remove_centos_container',\n    image='centos:latest',\n    command=['rm', '-f', '/var/lib/containers/*/containerd.io/*'],\n    docker_url='unix:///var/run/docker.sock',\n    network_mode='bridge',\n    dag=dag\n)\n\n# Define dependencies between tasks\nlaunch_container_task >> remove_container_task"}], "metadata": {"file_name": "dag_453_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.5705812}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that monitors a specified input directory for new files, validates file presence, and performs a controlled file transfer using containerized operations. Implement a robust mechanism for detecting, validating, and processing files with built-in error handling and logging.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'file_transfer_dag',\n    default_args=default_args,\n    description='A DAG to monitor a specified input directory for new files, validate file presence, and perform a controlled file transfer using containerized operations.',\n    schedule_interval=timedelta(hours=1),  # Adjust this as needed\n)\n\n# Sensor to wait for new files in the input directory\ninput_directory_sensor = FileSensor(\n    task_id='wait_for_new_files',\n    filepath='/path/to/input/directory',  # Replace with your input directory path\n    poke_interval=60,  # Check every minute\n    dag=dag\n)\n\n# Container Operator to validate and transfer files\ncontainer_operator = DockerOperator(\n    task_id='validate_and_transfer_file',\n    image='your-docker-image',  # Replace with your Docker image name\n    command=['/bin/bash', '-c', 'your-validation-and-transfer-script.sh'],  # Replace with your script\n    volumes=[('/path/to/input/directory:/data'), ('/path/to/output/directory:/output')],  # Mount directories\n    dag=dag\n)\n\n# Log sensor to log the status of each task\nlog_sensor = LogSensor(\n    task_id='log_task_status',\n    dag=dag\n)\n\n# Task dependencies\ninput_directory_sensor >> container_operator >> log_sensor"}], "metadata": {"file_name": "dag_794_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981730.570591}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data quality workflow that ingests raw data, performs initial analysis, and checks data integrity. Create a branching process that either saves clean data and generates a report, or handles errors by describing issues and sending notifications.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_quality_workflow',\n    default_args=default_args,\n    description='A data quality workflow to ingest raw data, perform initial analysis, and check data integrity.',\n    schedule_interval=None,\n)\n\n# Task to ingest raw data\ningest_data_task = PythonOperator(\n    task_id='ingest_raw_data',\n    python_callable=lambda: print(\"Ingesting raw data...\"),\n    dag=dag\n)\n\n# Task to perform initial analysis\ninitial_analysis_task = PythonOperator(\n    task_id='perform_initial_analysis',\n    python_callable=lambda: print(\"Performing initial analysis...\"),\n    dag=dag\n)\n\n# Task to check data integrity\ncheck_data_integrity_task = PythonOperator(\n    task_id='check_data_integrity',\n    python_callable=lambda: print(\"Checking data integrity...\"),\n    dag=dag\n)\n\n# Branching logic to save clean data and generate a report if everything passes, or handle errors\nsave_clean_data_and_report_task = PythonOperator(\n    task_id='save_clean_data_and_report',\n    python_callable=lambda: print(\"Saving clean data and generating a report...\"),\n    dag=dag\n)\n\nhandle_errors_task = PythonOperator(\n    task_id='handle_errors',\n    python_callable=lambda: print(\"Handling errors by describing issues and sending notifications...\"),\n    dag=dag\n)\n\n# Dependencies\ningest_data_task >> initial_analysis_task >> check_data_integrity_task >> [save_clean_data_and_report_task, handle_errors_task]"}], "metadata": {"file_name": "dag_325_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706005}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data enrichment pipeline that uses OpenAI to generate descriptive information about a static list of items, leveraging Airflow's task expansion and deferred execution capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport openai\n\n# Set up the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='enrichment_pipeline',\n    default_args=default_args,\n    description='An automated data enrichment pipeline using OpenAI',\n    schedule_interval=None,  # Set this to your desired schedule interval\n) as dag:\n\n    def generate_descriptive_info(item):\n        response = openai.Completion.create(\n            engine=\"text-davinci-003\",\n            prompt=f\"Generate a descriptive sentence about {item}.\",\n            max_tokens=100,\n            temperature=0.7,\n        )\n        return response.choices[0].text.strip()\n\n    # List of items to enrich\n    items = ['apple', 'banana', 'cherry']\n\n    # Create a task for each item in the list\n    tasks = [PythonOperator(\n        task_id=f'generate_description_{item}',\n        python_callable=generate_descriptive_info,\n        op_kwargs={'item': item},\n    ) for item in items]\n\n    # Set up the task dependencies\n    for i in range(len(tasks) - 1):\n        tasks[i].set_upstream(tasks[i + 1])"}], "metadata": {"file_name": "dag_2174_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706115}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a cloud-based data extraction and loading pipeline that captures data from a NoSQL database, exports it to a neutral storage format, and prepares it for analytical querying. Create a workflow that transforms unstructured data into a structured, analyzable format across different Google Cloud platforms.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cloud_data_pipeline',\n    default_args=default_args,\n    description='A pipeline to extract data from a NoSQL database, export it to a neutral storage format, and prepare it for analytical querying.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the tasks in the DAG\ndef extract_data():\n    gcs_hook = GCSHook()\n    bucket_name = 'your_bucket_name'\n    object_name = 'your_object_name'\n    \n    # Extract data from NoSQL database (example using Snowflake)\n    snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_default')\n    query = \"SELECT * FROM your_table\"\n    df = snowflake_hook.get_pandas_df(query)\n    \n    # Upload data to Google Cloud Storage\n    gcs_hook.load_string(df.to_csv(index=False), object_name, bucket=bucket_name)\n\ndef transform_data():\n    gcs_hook = GCSHook()\n    bucket_name = 'your_bucket_name'\n    object_name = 'your_object_name'\n    \n    # Download data from Google Cloud Storage\n    df = gcs_hook.read_local_file(object_name, bucket=bucket_name)\n    \n    # Transform data (example: convert to JSON)\n    transformed_df = df.to_json(orient='records')\n    \n    # Upload transformed data back to Google Cloud Storage\n    gcs_hook.load_string(transformed_df, object_name, bucket=bucket_name)\n\ndef load_data_to_bigquery():\n    gcs_hook = GCSHook()\n    bucket_name = 'your_bucket_name'\n    object_name = 'your_object_name'\n    \n    # Download data from Google Cloud Storage\n    df = gcs_hook.read_local_file(object_name, bucket=bucket_name)\n    \n    # Load data to BigQuery\n    bigquery_hook = BigQueryHook(bigquery_conn_id='bigquery_default')\n    table_ref = bigquery_hook.get_table('your_project.your_dataset.your_table')\n    bigquery_hook.insert_rows(table_ref, df.to_dict('records'))\n\n# Define the operators\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\nload_task = PythonOperator(\n    task_id='load_data_to_bigquery',\n    python_callable=load_data_to_bigquery,\n    dag=dag\n)\n\n# Define the dependencies\nextract_task >> transform_task >> load_task"}], "metadata": {"file_name": "dag_1250_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706224}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that retrieves specific information about Tom Hanks from a Neo4j graph database on a daily basis. Design the DAG to execute a Cypher query targeting a node with Tom Hanks' details, using the current date as a dynamic parameter.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.neo4j.hooks.neo4j import Neo4jHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'neo4j_tom_hanks_info_pipeline',\n    default_args=default_args,\n    description='A simple DAG to retrieve Tom Hanks info from Neo4j',\n    schedule_interval=timedelta(days=1)\n)\n\n# Function to fetch Tom Hanks info from Neo4j\ndef get_tom_hanks_info():\n    hook = Neo4jHook(neo4j_conn_id='neo4j_default')\n    # Assuming the Cypher query is stored in a variable named tom_hanks_query\n    result = hook.run(tom_hanks_query, return_all=True)\n    return result[0]\n\n# Task to run the Cypher query and store the result\nget_tom_hanks_info_task = PythonOperator(\n    task_id='get_tom_hanks_info',\n    python_callable=get_tom_hanks_info,\n    dag=dag\n)\n\n# Print the result of the Cypher query\nprint_result_task = PythonOperator(\n    task_id='print_result',\n    python_callable=lambda x: print(x),\n    op_kwargs={'x': get_tom_hanks_info_task.output},\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nget_tom_hanks_info_task >> print_result_task"}], "metadata": {"file_name": "dag_2172_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981730.570631}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code to query the current weather data from a weather API?"}, {"role": "assistant", "content": "import requests\n\ndef get_weather(city_name):\n    # Replace 'YOUR_API_KEY' with your actual OpenWeatherMap API key\n    api_key = 'YOUR_API_KEY'\n    \n    # Construct the URL for the API request\n    url = f'http://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={api_key}'\n    \n    # Send a GET request to the API\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the JSON response\n        weather_data = response.json()\n        \n        # Extract relevant information\n        temperature = weather_data['main']['temp']\n        description = weather_data['weather'][0]['description']\n        \n        # Print the weather details\n        print(f\"Weather in {city_name}:\")\n        print(f\"Temperature: {temperature} Kelvin\")\n        print(f\"Description: {description}\")\n    else:\n        print(\"Failed to retrieve weather data\")\n\n# Example usage\nget_weather('London')"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981730.5706384}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates how to interact with Databricks SQL endpoints for creating tables, inserting data, and performing data transformations. The pipeline should showcase different SQL operations like table creation, data selection, file export, and data import using Airflow and Databricks operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks_sql import DatabricksSQLOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'databricks_data_pipeline',\n    default_args=default_args,\n    description='A simple Data Pipeline using Airflow and Databricks',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to create a table in Databricks\ncreate_table_task = DatabricksSQLOperator(\n    task_id='create_table',\n    sql=\"CREATE TABLE IF NOT EXISTS my_database.my_table (id INT, name STRING)\",\n    databricks_conn_id='databricks_default'\n)\n\n# Task to insert data into the table\ninsert_data_task = DatabricksSQLOperator(\n    task_id='insert_data',\n    sql=\"INSERT INTO my_database.my_table VALUES (1, 'Alice'), (2, 'Bob')\",\n    databricks_conn_id='databricks_default'\n)\n\n# Task to export data from the table to a CSV file\nexport_to_csv_task = DatabricksSQLOperator(\n    task_id='export_to_csv',\n    sql=\"SELECT * FROM my_database.my_table\",\n    output_mode='overwrite',\n    output_format='csv',\n    output_file_path='/tmp/my_table.csv',\n    databricks_conn_id='databricks_default'\n)\n\n# Task to import data from a CSV file into the table\nimport_from_csv_task = DatabricksSQLOperator(\n    task_id='import_from_csv',\n    sql=\"LOAD DATA INPATH '/tmp/my_table.csv' INTO TABLE my_database.my_table\",\n    databricks_conn_id='databricks_default'\n)\n\n# Set up dependencies between tasks\ncreate_table_task >> insert_data_task >> export_to_csv_task >> import_from_csv_task"}], "metadata": {"file_name": "dag_1163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981730.570647}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated database management workflow that demonstrates cross-database interaction and query execution in a Cloud SQL environment. Create a data pipeline that showcases table creation, data insertion, and table deletion across PostgreSQL and MySQL instances.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.sql import (\n    PostgresOperator,\n    MySqlOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\ndag = DAG(\n    'database_management_dag',\n    default_args=default_args,\n    description='A DAG to manage a database using Python operators for cross-database interaction and query execution.',\n    schedule_interval=None,\n)\n\n# Task to create a table in PostgreSQL\ncreate_postgres_table = PostgresOperator(\n    task_id='create_postgres_table',\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS mydb.mytable (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100) NOT NULL,\n            age INT\n        );\n    \"\"\",\n    postgres_conn_id='postgres_default',\n    dag=dag,\n)\n\n# Task to insert data into PostgreSQL\ninsert_data_postgres = PostgresOperator(\n    task_id='insert_data_postgres',\n    sql=\"\"\"\n        INSERT INTO mydb.mytable (name, age) VALUES ('Alice', 30), ('Bob', 25);\n    \"\"\",\n    postgres_conn_id='postgres_default',\n    dag=dag,\n)\n\n# Task to delete a table from PostgreSQL\ndelete_postgres_table = PostgresOperator(\n    task_id='delete_postgres_table',\n    sql=\"\"\"\n        DROP TABLE IF EXISTS mydb.mytable;\n    \"\"\",\n    postgres_conn_id='postgres_default',\n    dag=dag,\n)\n\n# Task to create a table in MySQL\ncreate_mysql_table = MySqlOperator(\n    task_id='create_mysql_table',\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS mydb.mytable (\n            id INT AUTO_INCREMENT PRIMARY KEY,\n            name VARCHAR(100) NOT NULL,\n            age INT\n        );\n    \"\"\",\n    mysql_conn_id='mysql_default',\n    dag=dag,\n)\n\n# Task to insert data into MySQL\ninsert_data_mysql = MySqlOperator(\n    task_id='insert_data_mysql',\n    sql=\"\"\"\n        INSERT INTO mydb.mytable (name, age) VALUES ('Charlie', 35), ('David', 30);\n    \"\"\",\n    mysql_conn_id='mysql_default',\n    dag=dag,\n)\n\n# Task to delete a table from MySQL\ndelete_mysql_table = MySqlOperator(\n    task_id='delete_mysql_table',\n    sql=\"\"\"\n        DROP TABLE IF EXISTS mydb.mytable;\n    \"\"\",\n    mysql_conn_id='mysql_default',\n    dag=dag,\n)\n\n# Dependencies\ncreate_postgres_table.set_upstream(create_mysql_table)\ninsert_data_postgres.set_upstream(create_postgres_table)\ndelete_postgres_table.set_upstream(insert_data_postgres)\ncreate_mysql_table.set_upstream(delete_mysql_table)\ninsert_data_mysql.set_upstream(create_mysql_table)\ndelete_mysql_table.set_upstream(insert_data_mysql)"}], "metadata": {"file_name": "dag_36_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706596}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates cloud data processing tasks, including bucket creation, file uploads, template-based data processing, and automated cleanup of temporary resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\nfrom airflow.providers.google.cloud.operators.storage import GoogleCloudStorageBucketCreateOperator\nfrom airflow.providers.google.cloud.operators.storage import GoogleCloudStorageFileUploadOperator\nfrom airflow.providers.google.cloud.operators.dataflow import DataFlowTemplateOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    dag_id='cloud_data_processing_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\n# Create GCS Bucket\ncreate_bucket_task = GoogleCloudStorageBucketCreateOperator(\n    task_id='create_bucket',\n    bucket_name='my-cloud-bucket',\n    project_id='my-project-id',\n)\n\n# Upload files to GCS\nupload_files_task = GoogleCloudStorageFileUploadOperator(\n    task_id='upload_files',\n    source_objects=['data/*.csv'],\n    destination_bucket='my-cloud-bucket',\n    destination_object_prefix='processed/',\n    project_id='my-project-id',\n)\n\n# Define Dataflow Template\ntemplate_params = {\n    'input_file_pattern': 'gs://my-cloud-bucket/processed/*.csv',\n    'output_file_pattern': 'gs://my-cloud-bucket/output/*.parquet',\n}\n\ndataflow_template_task = DataFlowTemplateOperator(\n    task_id='dataflow_template',\n    template_path='/path/to/dataflow/template.py',\n    parameters=template_params,\n    project_id='my-project-id',\n    gcp_conn_id='google_cloud_default',\n    location='us-central1',\n)\n\n# Insert data into BigQuery\ninsert_data_task = BigQueryInsertJobOperator(\n    task_id='insert_data',\n    configuration={\n        'load': {\n            'sourceUris': ['gs://my-cloud-bucket/output/*.parquet'],\n            'destinationTable': {\n                'projectId': 'my-project-id',\n                'datasetId': 'my_dataset',\n                'tableId': 'my_table',\n            },\n            'schemaUpdateOptions': ['ALLOW_FIELD_ADDITION', 'ALLOW_FIELD_DELETION'],\n        }\n    },\n    gcp_conn_id='google_cloud_default',\n    use_legacy_sql=False,\n)\n\n# Cleanup temporary resources\ncleanup_task = GCSToGCSOperator(\n    task_id='cleanup_temporary_resources',\n    source_objects=['gs://my-cloud-bucket/processed/*', 'gs://my-cloud-bucket/output/*'],\n    destination_bucket='my-cloud-bucket',\n    destination_object_prefix='',\n    project_id='my-project-id',\n)\n\n# Set dependencies\ncreate_bucket_task >> upload_files_task >> dataflow_template_task >> insert_data_task >> cleanup_task"}], "metadata": {"file_name": "dag_178_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706692}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that showcases the end-to-end process of creating, managing, and analyzing image resources in a cloud environment. Develop a systematic workflow that demonstrates Google Cloud Vision's product and image management capabilities.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.hooks.vision import VisionHook\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'google_cloud_vision_dag',\n    default_args=default_args,\n    description='An example DAG for Google Cloud Vision',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to upload images to Google Cloud Storage\nupload_images_task = GCSToGCSOperator(\n    task_id='upload_images',\n    source_bucket='your-source-bucket',\n    destination_bucket='your-destination-bucket',\n    source_objects=['path/to/your/images/*.jpg'],\n    dag=dag\n)\n\n# Task to process images with Google Cloud Vision\nprocess_images_task = PythonOperator(\n    task_id='process_images',\n    python_callable=process_images,\n    op_kwargs={\n        'source_bucket': 'your-source-bucket',\n        'destination_bucket': 'your-destination-bucket'\n    },\n    dag=dag\n)\n\n# Task to store processed results in BigQuery\nstore_results_task = PythonOperator(\n    task_id='store_results',\n    python_callable=store_results,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'dataset_id': 'your-dataset-id',\n        'table_id': 'your-table-id',\n        'destination_bucket': 'your-destination-bucket'\n    },\n    dag=dag\n)\n\n# Define the dependencies between tasks\nupload_images_task >> process_images_task >> store_results_task\n\ndef process_images(source_bucket, destination_bucket):\n    vision_hook = VisionHook()\n    bigquery_hook = BigQueryHook()\n\n    # List all files in the source bucket\n    blobs = vision_hook.list_blobs(bucket_name=source_bucket)\n\n    for blob in blobs:\n        if blob.name.endswith('.jpg'):\n            # Upload the image to Google Cloud Storage\n            vision_hook.copy_blob(blob, destination_bucket, blob.name)\n\n            # Process the image with Google Cloud Vision\n            response = vision_hook.annotate_image(image_uri=f'gs://{destination_bucket}/{blob.name}')\n\n            # Store the processed result in BigQuery\n            table_ref = bigquery_hook.dataset_table_ref(project_id, dataset_id, table_id)\n            row_data = {\n                'image_name': blob.name,\n                'label': response.label_annotations[0].description,\n                'confidence': response.label_annotations[0].score\n            }\n            bigquery_hook.insert_rows(table_ref, [row_data])\n\ndef store_results(project_id, dataset_id, table_id, destination_bucket):\n    bigquery_hook = BigQueryHook()\n    query = f\"\"\"\n        SELECT *\n        FROM `{project_id}.{dataset_id}.{table_id}`\n    \"\"\"\n    df = bigquery_hook.get_pandas_df(query=query)\n    df.to_csv(f'{destination_bucket}/processed_results.csv', index=False)"}], "metadata": {"file_name": "dag_267_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706816}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data backup workflow that transfers data from a DynamoDB table to an S3 bucket with point-in-time recovery capabilities. Create a system that demonstrates incremental data export and backup strategies for cloud-based database management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'backup_dag',\n    default_args=default_args,\n    description='Automated data backup workflow',\n    schedule_interval=timedelta(days=1),\n)\n\n# Function to export data from DynamoDB to S3\ndef export_to_s3():\n    s3_hook = S3Hook()\n    dynamodb_table_name = 'your-dynamodb-table-name'\n    bucket_name = 'your-s3-bucket-name'\n\n    # Export data from DynamoDB to S3\n    response = s3_hook.list_objects_v2(Bucket=bucket_name)\n    if not response['Contents']:\n        print(\"No objects found in the bucket.\")\n        return\n\n    for obj in response['Contents']:\n        key = obj['Key']\n        s3_hook.download_file(bucket_name, key, f'/tmp/{key}')\n\n    print(f\"Data exported successfully to {bucket_name}.\")\n\n# Task to export data to S3\nexport_task = PythonOperator(\n    task_id='export_data_to_s3',\n    python_callable=export_to_s3,\n    dag=dag,\n)\n\n# Function to perform point-in-time recovery\ndef restore_from_s3():\n    s3_hook = S3Hook()\n    dynamodb_table_name = 'your-dynamodb-table-name'\n    bucket_name = 'your-s3-bucket-name'\n\n    # Restore data from S3\n    s3_hook.upload_file('/tmp/your-dynamodb-table-name.csv', bucket_name, 'restored-data.csv')\n\n    print(f\"Point-in-time recovery completed successfully.\")\n\n# Task to perform point-in-time recovery\nrestore_task = PythonOperator(\n    task_id='restore_data_from_s3',\n    python_callable=restore_from_s3,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\nexport_task >> restore_task"}], "metadata": {"file_name": "dag_1473_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981730.5706923}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Google Cloud Data Catalog workflow that demonstrates creating and managing metadata for a Google Cloud Storage bucket. Design a DAG that showcases how to programmatically register, tag, and catalog cloud storage resources for improved data governance and discoverability.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    GoogleCloudDataCatalogCreateEntryOperator,\n    GoogleCloudDataCatalogCreateTagTemplateOperator,\n    GoogleCloudDataCatalogUpdateTagTemplateOperator,\n    GoogleCloudDataCatalogCreateEntryGroupOperator,\n    GoogleCloudDataCatalogCreateTagOperator,\n    GoogleCloudDataCatalogCreateTagBindingOperator,\n)\nfrom airflow.providers.google.cloud.hooks.datacatalog import GoogleCloudDataCatalogHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'gcp_data_catalog_dag',\n    default_args=default_args,\n    description='Google Cloud Data Catalog Workflow',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create Entry Group\ncreate_entry_group = GoogleCloudDataCatalogCreateEntryGroupOperator(\n    task_id='create_entry_group',\n    entry_group_id='my-entry-group',\n    parent='projects/my-project-id/locations/us-central1',\n    dag=dag,\n)\n\n# Create Tag Template\ncreate_tag_template = GoogleCloudDataCatalogCreateTagTemplateOperator(\n    task_id='create_tag_template',\n    tag_template_id='my-tag-template',\n    parent='projects/my-project-id/locations/us-central1',\n    schema={\n        'fields': [\n            {'name': 'id', 'type': 'STRING'},\n            {'name': 'name', 'type': 'STRING'},\n            {'name': 'created_at', 'type': 'TIMESTAMP'},\n        ],\n    },\n    dag=dag,\n)\n\n# Update Tag Template\nupdate_tag_template = GoogleCloudDataCatalogUpdateTagTemplateOperator(\n    task_id='update_tag_template',\n    tag_template_id='my-tag-template',\n    schema={\n        'fields': [\n            {'name': 'id', 'type': 'STRING'},\n            {'name': 'name', 'type': 'STRING'},\n            {'name': 'updated_at', 'type': 'TIMESTAMP'},\n        ],\n    },\n    dag=dag,\n)\n\n# Create Entry\ncreate_entry = GoogleCloudDataCatalogCreateEntryOperator(\n    task_id='create_entry',\n    entry_id='my-entry',\n    parent='projects/my-project-id/locations/us-central1/entryGroups/my-entry-group',\n    schema={\n        'fields': [\n            {'name': 'id', 'type': 'STRING'},\n            {'name': 'name', 'type': 'STRING'},\n            {'name': 'created_at', 'type': 'TIMESTAMP'},\n        ],\n    },\n    tags=[\n        {\n            'tag_template': 'projects/my-project-id/locations/us-central1/tagTemplates/my-tag-template',\n            'field_values': {\n                'id': '123',\n                'name': 'My Entry',\n                'created_at': datetime.now(),\n            },\n        },\n    ],\n    dag=dag,\n)\n\n# Create Tag\ncreate_tag = GoogleCloudDataCatalogCreateTagOperator(\n    task_id='create_tag',\n    tag_id='my-tag',\n    parent='projects/my-project-id/locations/us-central1/tagTemplates/my-tag-template',\n    field_values={\n        'id': '123',\n        'name': 'My Tag',\n        'created_at': datetime.now(),\n    },\n    dag=dag,\n)\n\n# Create Tag Binding\ncreate_tag_binding = GoogleCloudDataCatalogCreateTagBindingOperator(\n    task_id='create_tag_binding',\n    tag_binding_id='my-tag-binding',\n    entry_id='my-entry',\n    tag_id='my-tag',\n    dag=dag,\n)\n\n# Define dependencies\ncreate_entry_group >> create_tag_template >> update_tag_template >> create_entry >> create_tag >> create_tag_binding"}], "metadata": {"file_name": "dag_271_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981730.5707068}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an educational data processing pipeline that explores Airflow's task flexibility by implementing a sequence of tasks that print context, log SQL queries, perform time-based operations, and demonstrate isolated Python environment executions.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.postgres_operator import PostgresOperator\nfrom airflow.operators.bash_operator import BashOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'educational_data_processing_pipeline',\n    default_args=default_args,\n    description='A simple educational data processing pipeline using Airflow',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to print context\nprint_context_task = PythonOperator(\n    task_id='print_context',\n    python_callable=lambda: print(\"This is the context from the DAG\"),\n    dag=dag\n)\n\n# Task to log SQL query\nlog_sql_query_task = BashOperator(\n    task_id='log_sql_query',\n    bash_command=\"echo 'Executing SQL query...' && echo 'SELECT * FROM my_table'\",\n    dag=dag\n)\n\n# Task to perform time-based operation\ntime_based_operation_task = BashOperator(\n    task_id='time_based_operation',\n    bash_command=\"sleep 5\",\n    dag=dag\n)\n\n# Task to execute isolated Python environment\nexecute_isolated_python_env_task = BashOperator(\n    task_id='execute_isolated_python_env',\n    bash_command=\"python -c 'import sys; print(sys.executable)'\",\n    dag=dag\n)\n\n# Set up dependencies between tasks\nprint_context_task >> log_sql_query_task >> time_based_operation_task >> execute_isolated_python_env_task"}], "metadata": {"file_name": "dag_1808_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981730.570715}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need to convert a large dataset of 1 million rows from a CSV file into a pandas DataFrame. How can I handle this process efficiently?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Specify the path to your CSV file\nfile_path = 'large_dataset.csv'\n\n# Use read_csv to load the data into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Display the first few rows of the DataFrame to verify the loading\nprint(df.head())"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981730.5707202}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates advanced cloud service integration by copying an image file between buckets and performing multiple types of computer vision analysis using different Google Cloud Vision operators. Showcase how complex data processing workflows can be orchestrated with modular, configurable task chains.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    GoogleCloudVisionAnnotateImageOperator,\n    GoogleCloudVisionDetectTextOperator,\n    GoogleCloudVisionLabelDetectionOperator,\n    GoogleCloudVisionObjectLocalizationOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    dag_id='google_cloud_vision_dag',\n    default_args=default_args,\n    description='DAG for Google Cloud Vision operations',\n    schedule_interval=None,\n)\n\n# Copy image from source bucket to destination bucket\ncopy_image_task = GCSToGCSOperator(\n    task_id='copy_image',\n    source_bucket='source-bucket-name',\n    source_object='image.jpg',\n    destination_bucket='destination-bucket-name',\n    destination_object='image.jpg',\n    dag=dag,\n)\n\n# Annotate image with labels\nannotate_image_labels_task = GoogleCloudVisionAnnotateImageOperator(\n    task_id='annotate_image_labels',\n    image_uri=f'gs://{copy_image_task.output}',\n    features=[\n        {'type': 'LABEL_DETECTION', 'max_results': 5},\n    ],\n    output_format='json',\n    dag=dag,\n)\n\n# Detect text in image\ndetect_text_task = GoogleCloudVisionDetectTextOperator(\n    task_id='detect_text',\n    image_uri=f'gs://{copy_image_task.output}',\n    output_format='json',\n    dag=dag,\n)\n\n# Label detection in image\nlabel_detection_task = GoogleCloudVisionLabelDetectionOperator(\n    task_id='label_detection',\n    image_uri=f'gs://{copy_image_task.output}',\n    output_format='json',\n    dag=dag,\n)\n\n# Object localization in image\nobject_localization_task = GoogleCloudVisionObjectLocalizationOperator(\n    task_id='object_localization',\n    image_uri=f'gs://{copy_image_task.output}',\n    output_format='json',\n    dag=dag,\n)\n\n# Define dependencies\ncopy_image_task >> annotate_image_labels_task\ncopy_image_task >> detect_text_task\ncopy_image_task >> label_detection_task\ncopy_image_task >> object_localization_task"}], "metadata": {"file_name": "dag_597_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.5707376}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete data transfer lifecycle, including bucket and dataset creation, data extraction from a Trino information schema, and external table generation in BigQuery. Implement a robust, idempotent workflow that handles resource provisioning and cleanup automatically.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_transfer_dag',\n    default_args=default_args,\n    description='A DAG to orchestrate data transfer between Trino and BigQuery',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Create bucket task\ncreate_bucket_task = PythonOperator(\n    task_id='create_bucket',\n    python_callable=create_bucket,\n    dag=dag,\n    op_kwargs={\n        'bucket_name': 'my_data_bucket'\n    }\n)\n\n# Create dataset task\ncreate_dataset_task = PythonOperator(\n    task_id='create_dataset',\n    python_callable=create_dataset,\n    dag=dag,\n    op_kwargs={\n        'dataset_name': 'my_data_dataset'\n    }\n)\n\n# Extract data from Trino task\nextract_data_task = PythonOperator(\n    task_id='extract_data_from_trino',\n    python_callable=extract_data_from_trino,\n    dag=dag,\n    op_kwargs={\n        'trino_conn_id': 'trino_default',\n        'table_name': 'my_table'\n    }\n)\n\n# Generate external table in BigQuery task\ngenerate_external_table_task = PythonOperator(\n    task_id='generate_external_table_in_bigquery',\n    python_callable=generate_external_table_in_bigquery,\n    dag=dag,\n    op_kwargs={\n        'bigquery_conn_id': 'google_cloud_default',\n        'project_id': 'my_project',\n        'dataset_id': 'my_data_dataset',\n        'table_id': 'my_external_table',\n        'schema': 'my_schema'\n    }\n)\n\n# Define dependencies\ncreate_bucket_task >> create_dataset_task >> extract_data_task >> generate_external_table_task"}], "metadata": {"file_name": "dag_2074_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981730.5707483}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates end-to-end image processing and machine learning operations using Google Cloud Vision APIs. Develop a pipeline that transfers images, creates product catalogs, and performs automated image analysis.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    GoogleCloudVisionAnnotateImageOperator,\n    GoogleCloudVisionCreateProductCatalogOperator,\n    GoogleCloudVisionCreateProductOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_vision_dag',\n    default_args=default_args,\n    description='DAG to demonstrate Google Cloud Vision API operations',\n    schedule_interval=None,\n)\n\n# Task to transfer images from source bucket to destination bucket\ntransfer_images_task = GCSToGCSOperator(\n    task_id='transfer_images',\n    source_bucket='source-bucket-name',\n    destination_bucket='destination-bucket-name',\n    gcp_conn_id='gcp_default',\n)\n\n# Task to create a product catalog in Google Cloud Vision\ncreate_product_catalog_task = GoogleCloudVisionCreateProductCatalogOperator(\n    task_id='create_product_catalog',\n    parent='projects/project-id/locations/global',\n    gcp_conn_id='gcp_default',\n)\n\n# Task to create a product in Google Cloud Vision\ncreate_product_task = GoogleCloudVisionCreateProductOperator(\n    task_id='create_product',\n    parent=f'projects/project-id/locations/global/catalogs/{create_product_catalog_task.output}',\n    display_name='Sample Product',\n    description='This is a sample product.',\n    gcp_conn_id='gcp_default',\n)\n\n# Task to annotate an image with Google Cloud Vision\nannotate_image_task = GoogleCloudVisionAnnotateImageOperator(\n    task_id='annotate_image',\n    image_uri=transfer_images_task.output,\n    features=[\n        {\n            'type': 'LABEL_DETECTION',\n            'max_results': 5,\n        },\n        {\n            'type': 'TEXT_DETECTION',\n            'max_results': 5,\n        },\n    ],\n    gcp_conn_id='gcp_default',\n)\n\n# Define dependencies between tasks\ntransfer_images_task >> create_product_catalog_task >> create_product_task >> annotate_image_task"}], "metadata": {"file_name": "dag_596_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.5707576}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for provisioning and managing an Amazon EKS (Elastic Kubernetes Service) cluster with dynamic nodegroup creation. Create a system that demonstrates infrastructure-as-code deployment of Kubernetes resources using Airflow's AWS provider operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.operators.ec2 import EC2CreateInstanceOperator\nfrom airflow.providers.amazon.aws.operators.ec2 import EC2TerminateInstanceOperator\nfrom airflow.providers.amazon.aws.operators.ec2 import EC2WaitConditionOperator\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.operators.kubernetes import KubernetesPodOperator\nfrom airflow.providers.amazon.aws.sensors.kubernetes import KubernetesPodSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'provision_eks_cluster',\n    default_args=default_args,\n    description='Provision an Amazon EKS cluster',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create an EC2 instance for the EKS control plane\ncreate_control_plane_instance = EC2CreateInstanceOperator(\n    task_id='create_control_plane_instance',\n    image_id='ami-0c55b159cbf575a81',  # Ubuntu AMI\n    instance_type='t2.micro',\n    subnet_id='subnet-12345678',  # Replace with your subnet ID\n    security_group_ids=['sg-12345678'],  # Replace with your security group ID\n    key_name='my-key-pair',  # Replace with your key pair name\n    dag=dag,\n)\n\n# Wait for the control plane instance to be running\nwait_for_control_plane_instance = EC2WaitConditionOperator(\n    task_id='wait_for_control_plane_instance',\n    state='running',\n    timeout=3600,\n    polling_period=60,\n    aws_conn_id='aws_default',\n    resource_id=f'instance/{create_control_plane_instance.instance_id}',\n    dag=dag,\n)\n\n# Create an EC2 instance for the EKS worker nodes\ncreate_worker_node_instance = EC2CreateInstanceOperator(\n    task_id='create_worker_node_instance',\n    image_id='ami-0c55b159cbf575a81',  # Ubuntu AMI\n    instance_type='t2.micro',\n    subnet_id='subnet-12345678',  # Replace with your subnet ID\n    security_group_ids=['sg-12345678'],  # Replace with your security group ID\n    key_name='my-key-pair',  # Replace with your key pair name\n    dag=dag,\n)\n\n# Wait for the worker node instances to be running\nwait_for_worker_node_instances = EC2WaitConditionOperator(\n    task_id='wait_for_worker_node_instances',\n    state='running',\n    timeout=3600,\n    polling_period=60,\n    aws_conn_id='aws_default',\n    resource_id=f'instance/{create_worker_node_instance.instance_id}',\n    dag=dag,\n)\n\n# Terminate the control plane instance\nterminate_control_plane_instance = EC2TerminateInstanceOperator(\n    task_id='terminate_control_plane_instance',\n    instance_id=create_control_plane_instance.instance_id,\n    dag=dag,\n)\n\n# Terminate the worker node instances\nterminate_worker_node_instances = EC2TerminateInstanceOperator(\n    task_id='terminate_worker_node_instances',\n    instance_id=create_worker_node_instance.instance_id,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\ncreate_control_plane_instance >> wait_for_control_plane_instance >> terminate_control_plane_instance\ncreate_worker_node_instance >> wait_for_worker_node_instances >> terminate_worker_node_instances"}], "metadata": {"file_name": "dag_732_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981730.570771}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates a complete machine learning workflow with Google Cloud services, showcasing task dependencies, cloud storage interactions, and automated model training processes. Construct a step-by-step orchestration that moves data, creates datasets, and trains a translation model with clear operational stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\nfrom airflow.providers.google.cloud.operators.storage import GoogleCloudStorageToBigQueryOperator\nfrom airflow.providers.google.cloud.operators.endpoints import GoogleCloudEndpointsDeployServiceOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# DAG configuration\ndag = DAG(\n    'ml_workflow',\n    default_args=default_args,\n    description='A complete machine learning workflow using Google Cloud services',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Task to upload data from GCS to BigQuery\nupload_data_to_bq = GoogleCloudStorageToBigQueryOperator(\n    task_id='upload_data_to_bq',\n    source_bucket='your-gcs-bucket-name',\n    destination_project_id='your-project-id',\n    destination_dataset_id='your-dataset-id',\n    destination_table_id='your-table-id',\n    source_objects=['path/to/your/data/*.csv'],\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag,\n)\n\n# Task to create a dataset in BigQuery\ncreate_dataset = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='translation_model_dataset',\n    location='us-central1',  # Replace with your preferred region\n    dag=dag,\n)\n\n# Task to train a translation model using TensorFlow Hub\ntrain_translation_model = PythonOperator(\n    task_id='train_translation_model',\n    python_callable=train_translation_model_function,\n    op_kwargs={\n        'model_name': 'tensorflow/tensorflow-hub:tf_text_en_to_fr'\n    },\n    dag=dag,\n)\n\n# Task to deploy the trained model to Google Cloud Endpoints\ndeploy_model = GoogleCloudEndpointsDeployServiceOperator(\n    task_id='deploy_model',\n    service_name='translation-model-service',\n    version_name='v1',\n    endpoint_config_path='path/to/endpoint-config.json',\n    dag=dag,\n)\n\n# Task dependencies\nupload_data_to_bq >> create_dataset >> train_translation_model >> deploy_model"}], "metadata": {"file_name": "dag_461_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.570798}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data pipeline that demonstrates task chaining, system command execution, and containerized task running, highlighting Airflow's ability to coordinate different types of computational tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.docker_operator import DockerOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example DAG with task chaining, system command execution, and containerized task running.',\n    schedule_interval=None,\n)\n\n# Task 1: Execute a system command\nexecute_system_command = PythonOperator(\n    task_id='execute_system_command',\n    python_callable=print,\n    op_kwargs={'message': 'Hello from system command!'},\n    dag=dag,\n)\n\n# Task 2: Run a Docker container\nrun_docker_container = DockerOperator(\n    task_id='run_docker_container',\n    image='nginx:latest',\n    container_name='my_nginx_container',\n    network_mode='host',\n    dag=dag,\n)\n\n# Task 3: Execute another system command after the Docker container is up\nexecute_after_docker = PythonOperator(\n    task_id='execute_after_docker',\n    python_callable=print,\n    op_kwargs={'message': 'Nginx server is running!'},\n    dag=dag,\n)\n\n# Task 4: Chain the tasks together\nexecute_system_command >> run_docker_container >> execute_after_docker"}], "metadata": {"file_name": "dag_1563_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708055}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that showcases data cube transformation and maintenance processes, demonstrating how to systematically process, update, and manage analytical data structures using Kylin operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.kylin.operators.kylin_load_operator import KylinLoadOperator\nfrom airflow.providers.apache.kylin.operators.kylin_transform_operator import KylinTransformOperator\nfrom airflow.providers.apache.kylin.operators.kylin_update_operator import KylinUpdateOperator\nfrom airflow.providers.apache.kylin.operators.kylin_delete_operator import KylinDeleteOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'kylin_data_cube_management',\n    default_args=default_args,\n    description='A sample DAG for managing data cubes in Kylin',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Task to load data into Kylin\nload_task = KylinLoadOperator(\n    task_id='load_data_into_kylin',\n    kylin_conn_id='kylin_conn',\n    sql_file_path='/path/to/your/load_script.sql',\n    warehouse_name='your_warehouse',\n    database_name='your_database',\n    table_name='your_table',\n    dag=dag,\n)\n\n# Task to transform data in Kylin\ntransform_task = KylinTransformOperator(\n    task_id='transform_data_in_kylin',\n    kylin_conn_id='kylin_conn',\n    sql_file_path='/path/to/your/transform_script.sql',\n    warehouse_name='your_warehouse',\n    database_name='your_database',\n    table_name='your_table',\n    dag=dag,\n)\n\n# Task to update data in Kylin\nupdate_task = KylinUpdateOperator(\n    task_id='update_data_in_kylin',\n    kylin_conn_id='kylin_conn',\n    sql_file_path='/path/to/your/update_script.sql',\n    warehouse_name='your_warehouse',\n    database_name='your_database',\n    table_name='your_table',\n    dag=dag,\n)\n\n# Task to delete data from Kylin\ndelete_task = KylinDeleteOperator(\n    task_id='delete_data_from_kylin',\n    kylin_conn_id='kylin_conn',\n    sql_file_path='/path/to/your/delete_script.sql',\n    warehouse_name='your_warehouse',\n    database_name='your_database',\n    table_name='your_table',\n    dag=dag,\n)\n\n# Define dependencies between tasks\nload_task >> transform_task >> update_task >> delete_task"}], "metadata": {"file_name": "dag_1944_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708153}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kubernetes volume management workflow that automatically creates and deletes a persistent volume claim (PVC) with a 5GB storage allocation. The workflow should run daily and demonstrate basic Kubernetes resource lifecycle management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.kubernetes.operators.kubernetes_pod_operator import KubernetesPodOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'k8s_volume_management_workflow',\n    default_args=default_args,\n    description='Automate PVC creation and deletion',\n    schedule_interval=timedelta(days=1),\n    catchup=False,\n)\n\n# Create PVC Operator\ncreate_pvc_task = KubernetesPodOperator(\n    namespace='default',  # Replace with your desired namespace\n    image='busybox',\n    command=['sh', '-c', 'echo \"Creating PVC\" && kubectl apply -f pvc.yaml'],\n    name='create-pvc',\n    dag=dag\n)\n\n# Delete PVC Operator\ndelete_pvc_task = KubernetesPodOperator(\n    namespace='default',  # Replace with your desired namespace\n    image='busybox',\n    command=['sh', '-c', 'echo \"Deleting PVC\" && kubectl delete pvc my-pvc'],\n    name='delete-pvc',\n    dag=dag\n)\n\n# Set up dependencies\ncreate_pvc_task >> delete_pvc_task"}], "metadata": {"file_name": "dag_1153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708237}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a BigQuery table to a Google Cloud Storage bucket. Create an Airflow DAG that demonstrates how to extract data from a BigQuery dataset and export it to a cloud storage location.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_migration',\n    default_args=default_args,\n    description='Data Migration from BigQuery to Google Cloud Storage',\n    schedule_interval=timedelta(days=1),\n)\n\ndef extract_data_from_bigquery():\n    hook = BigQueryHook()\n    query = \"\"\"\n        SELECT *\n        FROM `your_project.your_dataset.your_table`\n    \"\"\"\n    df = hook.get_pandas_df(query)\n    return df\n\ndef load_data_to_gcs(df):\n    hook = GoogleCloudStorageHook()\n    blob_name = f\"your_bucket/{df.columns[0]}.csv\"\n    hook.load_string(df.to_csv(index=False), blob_name, \"gs\")\n\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data_from_bigquery,\n    dag=dag,\n)\n\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data_to_gcs,\n    dag=dag,\n    provide_context=True,\n    op_kwargs={\n        'df': \"{{ ti.xcom_pull(task_ids='extract_data', key='return_value') }}\"\n    },\n)\n\nextract_task >> load_task"}], "metadata": {"file_name": "dag_1579_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708342}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates a complete data integration process from external file sources to a Databricks SQL environment, including steps for table creation, data import, transformation, and output generation. The workflow should highlight the flexibility of connecting and manipulating data across different storage systems and compute environments.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_integration_dag',\n    description='A complete data integration process from external file sources to a Databricks SQL environment',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args\n)\n\n# Step 1: Read data from CSV files\nread_csv_task = SparkSubmitOperator(\n    task_id='read_csv_task',\n    application='/usr/local/spark/examples/src/main/python/README.md',\n    name='Read CSV Task',\n    executor_cores=4,\n    executor_memory='8g',\n    driver_cores=2,\n    driver_memory='4g',\n    conf={'spark.sql.warehouse.dir': '/mnt/training'},\n    dag=dag\n)\n\n# Step 2: Transform data using PySpark\ntransform_data_task = SparkSubmitOperator(\n    task_id='transform_data_task',\n    application='/usr/local/spark/examples/src/main/python/README.md',\n    name='Transform Data Task',\n    executor_cores=4,\n    executor_memory='8g',\n    driver_cores=2,\n    driver_memory='4g',\n    conf={'spark.sql.warehouse.dir': '/mnt/training'},\n    dag=dag\n)\n\n# Step 3: Write transformed data to Databricks SQL\nwrite_to_db_task = DatabricksSubmitOperator(\n    task_id='write_to_db_task',\n    notebook_path='/Users/your_username/databricks/notebooks/data_integration.py',\n    spark_version='7.3.x',\n    cluster_id='your_cluster_id',\n    parameters={\n        'input_table': 'input_table',\n        'output_table': 'output_table'\n    },\n    dag=dag\n)\n\n# Set up dependencies\nread_csv_task >> transform_data_task >> write_to_db_task"}], "metadata": {"file_name": "dag_791_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708442}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a streaming data processing system that captures real-time taxi ride events, transforms them using Apache Beam, and routes the processed data through Google Cloud Pub/Sub topics with automated infrastructure management. Create a scalable and repeatable data streaming solution that demonstrates cloud-native data processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from apache_beam import Pipeline\nfrom apache_beam.io.gcp.pubsub import ReadFromPubSub\nfrom apache_beam.io.gcp.bigquery import WriteToBigQuery\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\n# Define the pipeline options\noptions = PipelineOptions()\n\n# Create the pipeline\nwith Pipeline(options=options) as p:\n    # Read from Google Cloud Pub/Sub\n    taxi_rides = (\n        p\n        | \"Read from Pub/Sub\" >> ReadFromPubSub(topic=\"projects/your-project-id/topics/taxi-rides\")\n        | \"Parse Taxi Rides\" >> beam.Map(lambda x: json.loads(x))\n    )\n\n    # Transform the data (example: extract pickup and dropoff locations)\n    transformed_rides = (\n        taxi_rides\n        | \"Extract Pickup Location\" >> beam.Map(lambda x: {\"pickup_location\": x[\"pickup_location\"]})\n        | \"Extract Dropoff Location\" >> beam.Map(lambda x: {\"dropoff_location\": x[\"dropoff_location\"]})\n    )\n\n    # Write to BigQuery\n    transformed_rides |\n        \"Write to BigQuery\" >> WriteToBigQuery(\n            table=\"your-project-id.taxi_rides\",\n            schema=[\n                {\"name\": \"pickup_location\", \"field_type\": \"STRING\"},\n                {\"name\": \"dropoff_location\", \"field_type\": \"STRING\"}\n            ],\n            write_disposition=beam.WriteDisposition.WRITE_APPEND\n        )"}], "metadata": {"file_name": "dag_1215_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708559}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure deployment pipeline that automatically provisions a Kubernetes cluster with secure metadata handling and demonstrates the ability to run containerized workloads. Create a scalable and repeatable process for initializing cloud-native compute resources with built-in compliance and verification steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.kubernetes.operators.kubernetes_pod_operator import KubernetesPodOperator\nfrom airflow.providers.google.cloud.operators.gcloud_kubernetes_engine import GKECreateClusterOperator, GKEDeleteClusterOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    dag_id='kubernetes_infrastructure_deployment_pipeline',\n    default_args=default_args,\n    schedule_interval=None,\n)\n\n# Step 1: Provision Kubernetes Cluster\nprovision_cluster_task = GKECreateClusterOperator(\n    task_id='provision_kubernetes_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='your-cluster-name',\n    num_nodes=3,\n    node_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 50,\n    },\n    service_account_email='your-service-account-email@example.com',\n    network='projects/your-project-id/global/networks/default',\n    subnetwork='projects/your-project-id/global/subnetworks/default',\n    enable_private_nodes=True,\n    enable_public_ip=False,\n    enable_autorepair=True,\n    enable_autoscaling=True,\n    min_node_count=1,\n    max_node_count=5,\n    tags=['airflow', 'kubernetes'],\n    metadata={'key1': 'value1', 'key2': 'value2'},\n    gcp_conn_id='google_cloud_default',\n)\n\n# Step 2: Run Containerized Workload\nrun_containerized_workload_task = KubernetesPodOperator(\n    task_id='run_containerized_workload',\n    namespace='default',\n    image='nginx:latest',\n    name='my-container',\n    command=['sh', '-c', 'echo \"Hello from Airflow!\"'],\n    secrets=[],\n    env_vars={},\n    resource_request_memory='512Mi',\n    resource_limit_memory='1Gi',\n    gcp_conn_id='google_cloud_default',\n)\n\n# Step 3: Delete Kubernetes Cluster\ndelete_cluster_task = GKEDeleteClusterOperator(\n    task_id='delete_kubernetes_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='your-cluster-name',\n    gcp_conn_id='google_cloud_default',\n)\n\n# Define dependencies\nprovision_cluster_task >> run_containerized_workload_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_387_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981730.570866}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering solution that demonstrates how to ingest raw data from an external source, transform it using a cloud-based data preparation tool, and generate a clean, structured output in a specified cloud storage location. Develop a reproducible data processing workflow that can be easily configured and scaled.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.apache.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSEndpointHook\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_engineering_dag',\n    description='A data engineering pipeline to ingest raw data, transform it, and load into BigQuery',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args\n)\n\n# Step 3: Ingest Raw Data from External Source (Example: CSV File)\ningest_data_task = SnowflakeOperator(\n    task_id='ingest_raw_data',\n    sql=\"\"\"\n        COPY INTO my_schema.my_table\n        FROM 's3://my-bucket/my-file.csv'\n        CREDENTIALS (aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-key')\n        FILE_FORMAT = CSV\n        HEADER = TRUE;\n    \"\"\",\n    dag=dag\n)\n\n# Step 4: Transform Data Using a Cloud-Based Data Preparation Tool (Example: Pandas)\ntransform_data_task = BashOperator(\n    task_id='transform_data',\n    bash_command=\"python /path/to/transform_script.py\",\n    dag=dag\n)\n\n# Step 5: Load Cleaned Data into BigQuery\nload_data_task = GCSToBigQueryOperator(\n    task_id='load_data_into_bigquery',\n    bucket='my-bucket',\n    object_name='cleaned-data.csv',\n    destination_project_dataset_table='my-project:my-dataset.my-table',\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag\n)\n\n# Define dependencies between tasks\ningest_data_task >> transform_data_task >> load_data_task"}], "metadata": {"file_name": "dag_1225_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708764}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs a complete data transfer operation between two database tables, highlighting the steps of source data preparation, data movement, and destination table validation. Demonstrate a systematic approach to moving data with clear, traceable steps from source to destination.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_transfer_dag',\n    description='A simple data transfer DAG',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args\n)\n\n# Step 1: Source Data Preparation (Python Operator)\ndef prepare_source_data():\n    # Implement your data preparation logic here\n    print(\"Preparing source data...\")\n\nprepare_source_task = PythonOperator(\n    task_id='prepare_source_data',\n    python_callable=prepare_source_data,\n    dag=dag\n)\n\n# Step 2: Data Movement (Python Operator)\ndef move_data_to_destination():\n    # Implement your data movement logic here\n    print(\"Moving data to destination...\")\n\nmove_data_task = PythonOperator(\n    task_id='move_data_to_destination',\n    python_callable=move_data_to_destination,\n    dag=dag\n)\n\n# Step 3: Destination Table Validation (Python Operator)\ndef validate_destination_table():\n    # Implement your destination table validation logic here\n    print(\"Validating destination table...\")\n\nvalidate_destination_task = PythonOperator(\n    task_id='validate_destination_table',\n    python_callable=validate_destination_table,\n    dag=dag\n)\n\n# Define the dependencies\nprepare_source_task >> move_data_task >> validate_destination_task"}], "metadata": {"file_name": "dag_1827_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708857}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a multi-step cloud-based audio processing workflow, showing how to chain together Google Cloud operators for text-to-speech synthesis and speech translation. The DAG will showcase task dependencies, cloud service integration, and automated workflow management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.text_to_speech import TextToSpeechSynthesizeOperator\nfrom airflow.providers.google.cloud.operators.speech import SpeechTranslateOperator\nfrom airflow.providers.google.cloud.operators.storage import StorageUploadOperator\nfrom airflow.providers.google.cloud.operators.storage import StorageDownloadOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'audio_processing_dag',\n    default_args=default_args,\n    description='A multi-step audio processing workflow using Google Cloud services',\n    schedule_interval=None,\n)\n\n# Step 1: Text-to-Speech Synthesis\ntext_to_speech_synthesize_task = TextToSpeechSynthesizeOperator(\n    task_id='text_to_speech_synthesize',\n    text='Hello, this is a test text.',\n    voice_name='en-US-Wavenet-A',\n    output_uri=f'gs://{BUCKET_NAME}/output.mp3',\n    dag=dag,\n)\n\n# Step 2: Speech Translation\nspeech_translate_task = SpeechTranslateOperator(\n    task_id='speech_translate',\n    source_language_code='en-US',\n    target_language_code='fr-FR',\n    input_uri=f'gs://{BUCKET_NAME}/input.mp3',\n    output_uri=f'gs://{BUCKET_NAME}/translated.mp3',\n    dag=dag,\n)\n\n# Step 3: Audio File Conversion\nstorage_upload_task = StorageUploadOperator(\n    task_id='upload_audio_file',\n    bucket_name=BUCKET_NAME,\n    object_name='converted.mp3',\n    filename='converted.mp3',\n    dag=dag,\n)\n\n# Step 4: Downloading the Converted Audio File\nstorage_download_task = StorageDownloadOperator(\n    task_id='download_converted_audio',\n    bucket_name=BUCKET_NAME,\n    object_name='converted.mp3',\n    filename='downloaded_converted.mp3',\n    dag=dag,\n)\n\n# Task Dependencies\ntext_to_speech_synthesize_task >> speech_translate_task >> storage_upload_task >> storage_download_task"}], "metadata": {"file_name": "dag_2108_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981730.5708935}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can we determine the mode of a dataset in Python, especially for a small dataset like [4, 5, 2, 1, 4, 5, 4, 3]?"}, {"role": "assistant", "content": "from collections import Counter\n\ndef calculate_mode(data):\n    \"\"\"\n    Calculate the mode of a given list of data.\n    \n    Parameters:\n    - data: A list of integers or floats.\n    \n    Returns:\n    - The mode of the data as an integer or float.\n    \"\"\"\n    # Count the occurrences of each element in the data\n    counts = Counter(data)\n    \n    # Find the maximum occurrence\n    max_count = max(counts.values())\n    \n    # Find all elements with the maximum occurrence\n    modes = [num for num, count in counts.items() if count == max_count]\n    \n    # Return the first mode found (since there might be multiple modes)\n    return modes[0]\n\n# Example usage\ndata = [4, 5, 2, 1, 4, 5, 4, 3]\nmode_value = calculate_mode(data)\nprint(f\"The mode of the dataset is: {mode_value}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7982142}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for deploying and managing a simple containerized task on AWS ECS Fargate. Create a DAG that demonstrates creating an ECS cluster, registering a task definition, running a container with a basic command, and then cleaning up resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.ecs import (\n    EcsCreateClusterOperator,\n    EcsRegisterTaskDefinitionOperator,\n    EcsRunTaskOperator,\n    EcsDeleteClusterOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n}\n\ndag = DAG(\n    'ecs_fargate_dag',\n    default_args=default_args,\n    description='Deploying and managing a simple containerized task on AWS ECS Fargate',\n    schedule_interval=None,\n)\n\n# Step 1: Create an ECS Cluster\ncreate_cluster_operator = EcsCreateClusterOperator(\n    task_definition_family='simple_task',\n    cluster_name='simple-cluster',\n    dag=dag,\n)\n\n# Step 2: Register a Task Definition\nregister_task_definition_operator = EcsRegisterTaskDefinitionOperator(\n    family='simple_task',\n    container_definitions=[\n        {\n            'name': 'simple_container',\n            'image': 'nginx:latest',\n            'command': ['echo', 'Hello from ECS!'],\n        },\n    ],\n    network_mode='bridge',\n    volumes=[],\n    tags=[{'key': 'Name', 'value': 'simple-task'}],\n    dag=dag,\n)\n\n# Step 3: Run a Container\nrun_task_operator = EcsRunTaskOperator(\n    cluster='simple-cluster',\n    task_definition='simple_task:1',\n    count=1,\n    launch_type='FARGATE',\n    network_configuration={\n        'awsvpcConfiguration': {\n            'subnets': ['subnet-12345678'],\n            'securityGroups': ['sg-12345678'],\n        }\n    },\n    dag=dag,\n)\n\n# Step 4: Clean Up Resources\ndelete_cluster_operator = EcsDeleteClusterOperator(\n    cluster_name='simple-cluster',\n    dag=dag,\n)\n\n# Define dependencies\ncreate_cluster_operator >> register_task_definition_operator >> run_task_operator >> delete_cluster_operator"}], "metadata": {"file_name": "dag_1087_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982252}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Given a string of words, write a Python function to split the string into words and sort them lexicographically. Assume that the words are separated by spaces."}, {"role": "assistant", "content": "def sort_words(input_string):\n    # Split the input string into a list of words using space as the delimiter\n    words = input_string.split()\n    \n    # Sort the list of words lexicographically\n    sorted_words = sorted(words)\n    \n    return sorted_words\n\n# Example usage:\ninput_str = \"hello world this is python\"\nsorted_list = sort_words(input_str)\nprint(sorted_list)  # Output: ['hello', 'is', 'python', 'this', 'world']"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7982326}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an employee salary data processing pipeline that transforms raw employee data from BigQuery using Google Cloud Dataflow. Create a workflow that loads employee salary information, applies transformations, and writes results to a new BigQuery table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.operators.dataflow import DataFlowTemplateOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'employee_salary_processing',\n    default_args=default_args,\n    schedule_interval='@daily',\n)\n\n# Define the DataFlow template\ndataflow_template = DataFlowTemplateOperator(\n    task_id='process_employee_salaries',\n    template_path='/path/to/dataflow/template.py',  # Path to your DataFlow template\n    parameters={\n        'input_table': 'your-project-id.your-dataset.your-table-name',\n        'output_table': 'your-project-id.your-dataset.processed-salary-data'\n    },\n    dag=dag,\n)\n\n# Define the BigQuery transfer operator\nbigquery_transfer = BigQueryToBigQueryOperator(\n    task_id='load_processed_data_to_bq',\n    source_project_dataset_table='your-project-id.your-dataset.processed-salary-data',\n    destination_project_dataset_table='your-project-id.your-dataset.final-salary-data',\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1218_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982426}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data engineering workflow that transforms raw city data by creating schemas, generating tables with predefined structures, and inserting sample city records using Trino as the data processing platform. The pipeline should demonstrate how to systematically build and populate database objects with incremental complexity.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'city_data_pipeline',\n    description='A data engineering pipeline to transform raw city data into structured tables.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args\n)\n\n# Function to create schema\ndef create_schema(trino_conn_id):\n    trino_hook = TrinoHook(trino_conn_id=trino_conn_id)\n    query = \"\"\"\n        CREATE SCHEMA IF NOT EXISTS city_data;\n    \"\"\"\n    trino_hook.run(query=query)\n\n# Function to create table\ndef create_table(trino_conn_id):\n    trino_hook = TrinoHook(trino_conn_id=trino_conn_id)\n    query = \"\"\"\n        CREATE TABLE IF NOT EXISTS city_data.city (\n            id INT PRIMARY KEY,\n            name VARCHAR(255) NOT NULL,\n            population INT,\n            area FLOAT,\n            latitude DECIMAL(9, 6),\n            longitude DECIMAL(9, 6)\n        );\n    \"\"\"\n    trino_hook.run(query=query)\n\n# Function to insert sample data\ndef insert_sample_data(trino_conn_id):\n    trino_hook = TrinoHook(trino_conn_id=trino_conn_id)\n    query = \"\"\"\n        INSERT INTO city_data.city (id, name, population, area, latitude, longitude)\n        VALUES\n            (1, 'New York', 8419000, 783.8, 40.7128, -74.0060),\n            (2, 'Los Angeles', 3990456, 163.8, 34.0522, -118.2437),\n            (3, 'Chicago', 2705994, 89.4, 41.8781, -87.6298);\n    \"\"\"\n    trino_hook.run(query=query)\n\n# Task to create schema\ncreate_schema_task = PythonOperator(\n    task_id='create_schema',\n    python_callable=create_schema,\n    op_kwargs={'trino_conn_id': 'trino_default'},\n    dag=dag\n)\n\n# Task to create table\ncreate_table_task = PythonOperator(\n    task_id='create_table',\n    python_callable=create_table,\n    op_kwargs={'trino_conn_id': 'trino_default'},\n    dag=dag\n)\n\n# Task to insert sample data\ninsert_sample_data_task = PythonOperator(\n    task_id='insert_sample_data',\n    python_callable=insert_sample_data,\n    op_kwargs={'trino_conn_id': 'trino_default'},\n    dag=dag\n)\n\n# Dependencies\ncreate_schema_task >> create_table_task >> insert_sample_data_task"}], "metadata": {"file_name": "dag_2235_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982545}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data movement pipeline that generates random binary files and demonstrates cloud storage data transfer and replication strategies. Create an automated workflow that shows how to programmatically manage file uploads, bucket interactions, and cross-bucket synchronization using Airflow and Google Cloud Storage operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.operators.storage import GCSBucketCreateOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'data_movement_dag',\n    default_args=default_args,\n    description='A simple data movement pipeline using Airflow and Google Cloud Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Create a bucket for storing the generated files\ncreate_bucket_task = GCSBucketCreateOperator(\n    task_id='create_bucket',\n    bucket_name='source_bucket',\n    project_id='your_project_id',  # Replace with your Google Cloud project ID\n    dag=dag,\n)\n\n# Upload a random binary file to the source bucket\nupload_file_task = GCSToGCSOperator(\n    task_id='upload_file',\n    source_bucket='source_bucket',\n    source_object='random_binary_file.bin',\n    destination_bucket='destination_bucket',\n    destination_object='random_binary_file.bin',\n    dag=dag,\n)\n\n# Synchronize the files between the source and destination buckets\nsync_files_task = GCSToGCSOperator(\n    task_id='sync_files',\n    source_bucket='source_bucket',\n    source_object='random_binary_file.bin',\n    destination_bucket='destination_bucket',\n    destination_object='random_binary_file.bin',\n    dag=dag,\n)\n\n# Define the order of tasks\ncreate_bucket_task >> upload_file_task >> sync_files_task"}], "metadata": {"file_name": "dag_1650_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982647}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates end-to-end machine learning orchestration with clear task dependencies and cloud-based infrastructure management. Build a workflow that creates a dataset, trains an AutoML model, and automatically cleans up resources after model training.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\nfrom airflow.providers.google.cloud.operators.mlengine import MLEngineDeployModelOperator, MLEngineDeleteModelOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'ml_automl_dag',\n    default_args=default_args,\n    description='End-to-end ML orchestration using Airflow',\n    schedule_interval=None,\n)\n\n# Task to create a dataset in BigQuery\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    project_id='your_project_id',\n    dataset_id='your_dataset_id',\n    location='us-central1',\n)\n\n# Task to insert data into the dataset\ninsert_data_task = BigQueryInsertJobOperator(\n    task_id='insert_data',\n    project_id='your_project_id',\n    dataset_id='your_dataset_id',\n    table_id='your_table_id',\n    schema_fields=[\n        {'name': 'feature1', 'type': 'STRING'},\n        {'name': 'feature2', 'type': 'INTEGER'},\n    ],\n    source_uris=['gs://path/to/your/data.csv'],\n)\n\n# Task to train an AutoML model\ntrain_model_task = MLEngineDeployModelOperator(\n    task_id='train_model',\n    project_id='your_project_id',\n    model_name='your_model_name',\n    region='us-central1',\n    package_path='gs://path/to/your/model.tar.gz',\n    runtime_version='2.15',\n    python_version='3.8',\n    machine_type='n1-standard-4',\n    accelerator_count=1,\n    accelerator_type='NVIDIA_TESLA_K80',\n    deployment_config={\n        'machine_spec': {\n            'machine_type': 'n1-standard-4',\n            'accelerator_count': 1,\n            'accelerator_type': 'NVIDIA_TESLA_K80',\n        },\n        'disk_spec': {\n            'size_gb': 100,\n        },\n    },\n)\n\n# Task to delete the model\ndelete_model_task = MLEngineDeleteModelOperator(\n    task_id='delete_model',\n    project_id='your_project_id',\n    model_name='your_model_name',\n)\n\n# Task dependencies\ncreate_dataset_task >> insert_data_task >> train_model_task >> delete_model_task"}], "metadata": {"file_name": "dag_2115_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982745}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u8bf7\u7ed9\u6211\u4e00\u4e2aPython\u4ee3\u7801\u793a\u4f8b\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e24\u4e2a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3002"}, {"role": "assistant", "content": "# \u5b9a\u4e49\u4e24\u4e2a\u6570\u5b57\nnum1 = 10\nnum2 = 20\n\n# \u8ba1\u7b97\u5e73\u5747\u503c\naverage = (num1 + num2) / 2\n\n# \u8f93\u51fa\u7ed3\u679c\nprint(\"\u4e24\u4e2a\u6570\u5b57\u7684\u5e73\u5747\u503c\u662f:\", average)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7982807}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data pipeline that transfers cocktail recipe data from an S3 bucket to a Redshift database, demonstrating end-to-end cloud data migration. Create a workflow that provisions cloud resources, loads sample data, and establishes necessary connections for data transfer.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.redshift import RedshiftHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# DAG configuration\ndag = DAG(\n    'cocktail_recipe_pipeline',\n    default_args=default_args,\n    description='A DAG to transfer cocktail recipe data from S3 to Redshift',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to provision Redshift cluster\nprovision_redshift_task = PythonOperator(\n    task_id='provision_redshift_cluster',\n    python_callable=provision_redshift,\n    dag=dag\n)\n\n# Task to load sample data into Redshift\nload_sample_data_task = PythonOperator(\n    task_id='load_sample_data_into_redshift',\n    python_callable=load_sample_data,\n    dag=dag\n)\n\n# Task to establish connection between S3 and Redshift\nestablish_connection_task = PythonOperator(\n    task_id='establish_connection_between_s3_and_redshift',\n    python_callable=establish_connection,\n    dag=dag\n)\n\n# Task to transfer data from S3 to Redshift\ntransfer_data_task = PythonOperator(\n    task_id='transfer_data_from_s3_to_redshift',\n    python_callable=transfer_data,\n    dag=dag\n)\n\n# Task to clean up resources (optional)\ncleanup_resources_task = PythonOperator(\n    task_id='cleanup_resources',\n    python_callable=cleanup_resources,\n    dag=dag\n)\n\n# Dependencies\nprovision_redshift_task >> load_sample_data_task >> establish_connection_task >> transfer_data_task >> cleanup_resources_task"}], "metadata": {"file_name": "dag_755_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981779.7982929}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the step-by-step operational control of a cloud-based machine learning notebook, showcasing how to orchestrate infrastructure management tasks using AWS SageMaker operators. Develop a workflow that demonstrates dynamic instance management with clear, sequential task dependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n    SageMakerStartNotebookInstanceOperator,\n    SageMakerStopNotebookInstanceOperator,\n)\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'aws_sagemaker_notebook_management',\n    default_args=default_args,\n    description='DAG for managing SageMaker Notebooks',\n    schedule_interval=None,\n)\n\n# Create Notebook Instance\ncreate_notebook_instance_task = SageMakerCreateNotebookInstanceOperator(\n    task_id='create_notebook_instance',\n    instance_type='ml.t2.micro',\n    role_arn='arn:aws:iam::123456789012:role/AirflowRole',\n    image_name='aws-sagemaker-notebook-ami-1.0.0',\n    subnet_id='subnet-12345678',\n    security_group_ids=['sg-12345678'],\n    dag=dag,\n)\n\n# Start Notebook Instance\nstart_notebook_instance_task = SageMakerStartNotebookInstanceOperator(\n    task_id='start_notebook_instance',\n    notebook_instance_name='my-notebook-instance',\n    dag=dag,\n)\n\n# Stop Notebook Instance\nstop_notebook_instance_task = SageMakerStopNotebookInstanceOperator(\n    task_id='stop_notebook_instance',\n    notebook_instance_name='my-notebook-instance',\n    dag=dag,\n)\n\n# Delete Notebook Instance\ndelete_notebook_instance_task = SageMakerDeleteNotebookInstanceOperator(\n    task_id='delete_notebook_instance',\n    notebook_instance_name='my-notebook-instance',\n    dag=dag,\n)\n\n# Define task dependencies\ncreate_notebook_instance_task >> start_notebook_instance_task\nstart_notebook_instance_task >> stop_notebook_instance_task\nstop_notebook_instance_task >> delete_notebook_instance_task"}], "metadata": {"file_name": "dag_759_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981779.7983022}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with proper error handling and sequential task execution.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.sqs import AwsSQSCreateQueueOperator, AwsSQSDeleteQueueOperator\nfrom airflow.providers.amazon.aws.sensors.sqs import AwsSQSSensor\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'aws_sqs_stack_management',\n    default_args=default_args,\n    description='A DAG to manage an AWS SQS queue',\n    schedule_interval=None,  # Set to None if you want to run it manually\n)\n\n# Create SQS Queue\ncreate_queue_task = AwsSQSCreateQueueOperator(\n    task_id='create_sqs_queue',\n    aws_conn_id='aws_default',  # Replace with your AWS connection ID\n    queue_name='my-sqs-queue',\n    region_name='us-east-1',  # Replace with your AWS region\n    dag=dag,\n)\n\n# Wait for SQS Queue to be created\nwait_for_queue_created = AwsSQSSensor(\n    task_id='wait_for_queue_created',\n    aws_conn_id='aws_default',  # Replace with your AWS connection ID\n    queue_name='my-sqs-queue',\n    region_name='us-east-1',  # Replace with your AWS region\n    timeout=600,  # Timeout in seconds\n    poke_interval=5,  # Polling interval in seconds\n    dag=dag,\n)\n\n# Delete SQS Queue\ndelete_queue_task = AwsSQSDeleteQueueOperator(\n    task_id='delete_sqs_queue',\n    aws_conn_id='aws_default',  # Replace with your AWS connection ID\n    queue_name='my-sqs-queue',\n    region_name='us-east-1',  # Replace with your AWS region\n    dag=dag,\n)\n\n# Define dependencies\ncreate_queue_task >> wait_for_queue_created >> delete_queue_task"}], "metadata": {"file_name": "dag_1878_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.7983098}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need a function that calculates the factorial of a number in Python. Can you provide one?"}, {"role": "assistant", "content": "def factorial(n):\n    # Base case: factorial of 0 or 1 is 1\n    if n == 0 or n == 1:\n        return 1\n    else:\n        # Recursive case: n * factorial of (n-1)\n        return n * factorial(n-1)\n\n# Example usage:\nnumber = 5\nresult = factorial(number)\nprint(f\"The factorial of {number} is {result}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7983153}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning workflow from data ingestion to model training and cleanup. Develop a systematic task sequence that handles Google Cloud Storage operations, dataset creation, model training, and resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.storage import (\n    GoogleCloudStorageCreateBucketOperator,\n    GoogleCloudStorageDeleteBucketOperator,\n)\nfrom airflow.providers.google.cloud.operators.mlengine import (\n    GoogleCloudMLEngineTrainOperator,\n    GoogleCloudMLEngineDeployModelOperator,\n)\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'ml_workflow',\n    default_args=default_args,\n    description='A complete ML workflow using Google Cloud Storage and ML Engine',\n    schedule_interval=None,\n)\n\n# Create bucket for storing data\ncreate_bucket_task = GoogleCloudStorageCreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my_ml_project_data',\n    project_id='your-project-id',\n)\n\n# Upload data to bucket\nupload_data_task = GoogleCloudStorageUploadFileOperator(\n    task_id='upload_data',\n    bucket_name='my_ml_project_data',\n    object_name='data.csv',\n    file_path='/path/to/data.csv',\n    project_id='your-project-id',\n)\n\n# Train model\ntrain_model_task = GoogleCloudMLEngineTrainOperator(\n    task_id='train_model',\n    job_dir=f'gs://{create_bucket_task.output}',\n    package_uris=['gs://your-gcs-bucket/path/to/model.tar.gz'],\n    python_module='train.py',\n    region='us-central1',\n    runtime_version='2.5',\n    framework_version='tensorflow',\n    master_config={\n        'machine_type': 'n1-standard-4'\n    },\n    worker_config={\n        'machine_type': 'n1-standard-4'\n    },\n    hyperparameters={\n        'learning_rate': 0.001,\n        'batch_size': 64\n    }\n)\n\n# Deploy model\ndeploy_model_task = GoogleCloudMLEngineDeployModelOperator(\n    task_id='deploy_model',\n    model_name=train_model_task.outputs['model_name'],\n    version_name='v1',\n    region='us-central1',\n    package_uris=['gs://{create_bucket_task.output}'],\n    runtime_version='2.5',\n    framework_version='tensorflow',\n    master_config={\n        'machine_type': 'n1-standard-4'\n    },\n    worker_config={\n        'machine_type': 'n1-standard-4'\n    }\n)\n\n# Delete bucket after deployment\ndelete_bucket_task = GoogleCloudStorageDeleteBucketOperator(\n    task_id='delete_bucket',\n    bucket_name='my_ml_project_data',\n    project_id='your-project-id',\n)\n\n# Define dependencies\ncreate_bucket_task >> upload_data_task >> train_model_task >> deploy_model_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_1312_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.798324}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u5982\u4f55\u5728Python\u4e2d\u4f7f\u7528\u88c5\u9970\u5668\u6765\u68c0\u67e5\u6743\u9650\uff1f"}, {"role": "assistant", "content": "# \u5b9a\u4e49\u4e00\u4e2a\u7b80\u5355\u7684\u7528\u6237\u8ba4\u8bc1\u7cfb\u7edf\nclass User:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\ndef authenticate(func):\n    def wrapper(*args, **kwargs):\n        # \u8fd9\u91cc\u53ef\u4ee5\u6dfb\u52a0\u6743\u9650\u68c0\u67e5\u903b\u8f91\n        if args[0].username == 'admin' and args[0].password == 'secret':\n            return func(*args, **kwargs)\n        else:\n            print(\"Access denied\")\n            return None\n    return wrapper\n\n@authenticate\ndef admin_only_function():\n    print(\"This function is only accessible to administrators.\")\n\n# \u521b\u5efa\u4e00\u4e2a\u7528\u6237\u5bf9\u8c61\u5e76\u8c03\u7528\u88c5\u9970\u540e\u7684\u51fd\u6570\nuser = User('admin', 'secret')\nadmin_only_function(user)  # \u8f93\u51fa: This function is only accessible to administrators."}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7983303}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete machine learning lifecycle for image classification, showcasing how to programmatically manage cloud machine learning infrastructure and model deployment processes. The DAG should illustrate task dependencies and cloud service interactions for an end-to-end ML workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.mlengine import (\n    MlEngineDeployModelOperator,\n    MlEngineListModelsOperator,\n    MlEngineTrainModelOperator,\n)\nfrom airflow.providers.google.cloud.hooks.mlengine import MlEngineHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\nml_engine_hook = MlEngineHook()\n\ndef preprocess_data():\n    # Implement data preprocessing logic here\n    print(\"Data preprocessing completed.\")\n\ndef train_model():\n    # Implement model training logic here\n    print(\"Model training completed.\")\n\ndef evaluate_model():\n    # Implement model evaluation logic here\n    print(\"Model evaluation completed.\")\n\ndef deploy_model():\n    # Implement model deployment logic here\n    print(\"Model deployed successfully.\")\n\nwith DAG(\n    dag_id='image_classification_dag',\n    default_args=default_args,\n    schedule_interval=None,\n) as dag:\n\n    preprocess_task = PythonOperator(\n        task_id='preprocess_data',\n        python_callable=preprocess_data,\n    )\n\n    train_task = PythonOperator(\n        task_id='train_model',\n        python_callable=train_model,\n    )\n\n    evaluate_task = PythonOperator(\n        task_id='evaluate_model',\n        python_callable=evaluate_model,\n    )\n\n    deploy_task = MlEngineDeployModelOperator(\n        task_id='deploy_model',\n        project_id='your-project-id',\n        region='us-central1',\n        model_name='your-model-name',\n        package_path='/path/to/your/model.tar.gz',\n        runtime_version='2.4',\n        framework_version='tf2',\n        serving_container_image_uri='gcr.io/cloud-ai-platform/prediction/tf2-cpu:latest',\n    )\n\n    preprocess_task >> train_task >> evaluate_task >> deploy_task"}], "metadata": {"file_name": "dag_1317_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7983398}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates basic Pig script execution in a local Hadoop environment. Create a DAG that runs a simple Pig command to list directory contents as a learning example of task configuration.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.pig_operator import PigOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'pig_script_dag',\n    default_args=default_args,\n    description='A simple DAG to run a Pig script',\n    schedule_interval=None\n)\n\n# Define the Pig task\nlist_directory_contents_task = PigOperator(\n    task_id='list_directory_contents',\n    pig_command='ls /path/to/directory',\n    dag=dag\n)\n\n# Define the end of the DAG\nend_task = DummyOperator(task_id='end', dag=dag)\n\n# Set up the dependencies between tasks\nlist_directory_contents_task >> end_task"}], "metadata": {"file_name": "dag_1536_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981779.798346}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated metadata management pipeline that allows dynamic configuration and manipulation of data catalog tag templates for standardizing data asset descriptions and classifications. The workflow will enable systematic tracking and enhancement of metadata across cloud data environments.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'metadata_management_pipeline',\n    default_args=default_args,\n    description='Automated metadata management pipeline',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to generate or update the tag template\ndef generate_tag_template():\n    # Implement logic to generate or update the tag template\n    print(\"Generating or updating tag template\")\n\ngenerate_tag_template_task = PythonOperator(\n    task_id='generate_tag_template',\n    python_callable=generate_tag_template,\n    dag=dag\n)\n\n# Task to assign the tag template to data assets\ndef assign_tag_to_data_assets():\n    # Implement logic to assign the tag template to data assets\n    print(\"Assigning tag template to data assets\")\n\nassign_tag_to_data_assets_task = PythonOperator(\n    task_id='assign_tag_to_data_assets',\n    python_callable=assign_tag_to_data_assets,\n    dag=dag\n)\n\n# Task to track changes in metadata\ndef track_metadata_changes():\n    # Implement logic to track changes in metadata\n    print(\"Tracking metadata changes\")\n\ntrack_metadata_changes_task = PythonOperator(\n    task_id='track_metadata_changes',\n    python_callable=track_metadata_changes,\n    dag=dag\n)\n\n# Task to send alerts\ndef send_alerts():\n    # Implement logic to send alerts\n    print(\"Sending alerts\")\n\nsend_alerts_task = PythonOperator(\n    task_id='send_alerts',\n    python_callable=send_alerts,\n    dag=dag\n)\n\n# Set up dependencies between tasks\ngenerate_tag_template_task >> assign_tag_to_data_assets_task >> track_metadata_changes_task >> send_alerts_task"}], "metadata": {"file_name": "dag_1333_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.798356}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data transformation workflow that triggers dbt Cloud jobs for data processing and monitoring their execution status. Design a DAG that demonstrates different methods of running and tracking dbt Cloud job runs with synchronous and asynchronous sensors.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.dbt.cloud.operators.dbt_cloud import DbtCloudRunJobOperator\nfrom airflow.sensors.dbt_cloud import DbtCloudJobStatusSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'dbt_cloud_dag',\n    default_args=default_args,\n    description='A DAG to run dbt Cloud jobs and monitor their execution status',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the dbt Cloud project name and profile name\nproject_name = \"my_dbt_project\"\nprofile_name = \"my_dbt_profile\"\n\n# Define the dbt Cloud job names\njob_names = [\"job1\", \"job2\"]\n\n# Create a task to run each dbt Cloud job synchronously\nfor job in job_names:\n    run_job_task = DbtCloudRunJobOperator(\n        task_id=f\"run_{job}\",\n        project_name=project_name,\n        profile_name=profile_name,\n        job_name=job,\n        dag=dag,\n    )\n\n# Create a sensor to check the status of each dbt Cloud job asynchronously\nfor job in job_names:\n    job_status_sensor = DbtCloudJobStatusSensor(\n        task_id=f\"check_{job}_status\",\n        project_name=project_name,\n        profile_name=profile_name,\n        job_name=job,\n        dag=dag,\n    )"}], "metadata": {"file_name": "dag_792_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981779.798364}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Google Cloud Compute Engine template management workflow that demonstrates how to create, copy, and update instance templates for a scalable infrastructure deployment. The workflow should showcase best practices for managing compute resources programmatically using Airflow.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    GCPComputeInstanceTemplateCreateOperator,\n    GCPComputeInstanceTemplateCopyOperator,\n    GCPComputeInstanceTemplateUpdateOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'compute_template_management',\n    default_args=default_args,\n    description='Automate Google Cloud Compute Engine template management',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create Instance Template\ncreate_instance_template = GCPComputeInstanceTemplateCreateOperator(\n    task_id='create_instance_template',\n    project='your-project-id',\n    name='my-instance-template',\n    machine_type='n1-standard-1',\n    disks=[\n        {\n            'initialize_params': {\n                'source_image': 'debian-cloud/debian-11',\n            },\n        },\n    ],\n    network_interfaces=[\n        {\n            'network': 'default',\n            'subnetwork': 'default',\n        },\n    ],\n)\n\n# Copy Instance Template\ncopy_instance_template = GCPComputeInstanceTemplateCopyOperator(\n    task_id='copy_instance_template',\n    source_project='your-source-project-id',\n    source_name='my-source-instance-template',\n    destination_project='your-project-id',\n    destination_name='my-copied-instance-template',\n)\n\n# Update Instance Template\nupdate_instance_template = GCPComputeInstanceTemplateUpdateOperator(\n    task_id='update_instance_template',\n    project='your-project-id',\n    name='my-copied-instance-template',\n    machine_type='n1-standard-2',\n    disks=[\n        {\n            'initialize_params': {\n                'source_image': 'ubuntu-os-cloud/ubuntu-2004-lts',\n            },\n        },\n    ],\n)\n\n# Define dependencies\ncreate_instance_template >> copy_instance_template >> update_instance_template"}], "metadata": {"file_name": "dag_2008_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.798375}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates the technical orchestration of data cube operations using the KylinCubeOperator, demonstrating how complex data processing tasks can be chained together with precise time-based segmentation and automated management. The DAG will highlight task dependencies, XCom data passing, and systematic execution of build, refresh, merge, disable, and purge commands.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.apache.kylin.operators.kylin_cube_operator import KylinCubeOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'kylin_data_cube_operations',\n    default_args=default_args,\n    description='DAG to orchestrate Kylin data cube operations',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to build a data cube\nbuild_cube_task = KylinCubeOperator(\n    task_id='build_cube',\n    kylin_conn_id='kylin_conn',\n    cube_name='my_cube',\n    operation_type='BUILD'\n)\n\n# Task to refresh a data cube\nrefresh_cube_task = KylinCubeOperator(\n    task_id='refresh_cube',\n    kylin_conn_id='kylin_conn',\n    cube_name='my_cube',\n    operation_type='REFRESH'\n)\n\n# Task to merge a data cube\nmerge_cube_task = KylinCubeOperator(\n    task_id='merge_cube',\n    kylin_conn_id='kylin_conn',\n    cube_name='my_cube',\n    operation_type='MERGE'\n)\n\n# Task to disable a data cube\ndisable_cube_task = KylinCubeOperator(\n    task_id='disable_cube',\n    kylin_conn_id='kylin_conn',\n    cube_name='my_cube',\n    operation_type='DISABLE'\n)\n\n# Task to purge a data cube\npurge_cube_task = KylinCubeOperator(\n    task_id='purge_cube',\n    kylin_conn_id='kylin_conn',\n    cube_name='my_cube',\n    operation_type='PURGE'\n)\n\n# Task to segment data cubes by date\nsegment_by_date_task = PythonOperator(\n    task_id='segment_by_date',\n    python_callable=segment_data_cubes_by_date,\n    op_kwargs={'date_range': ['2023-01-01', '2023-01-31']}\n)\n\n# Define the function to segment data cubes by date\ndef segment_data_cubes_by_date(date_range):\n    # Implement your logic here to segment data cubes by date\n    print(f\"Segmenting data cubes between {date_range[0]} and {date_range[1]}\")\n\n# Set up the task dependencies\nbuild_cube_task >> refresh_cube_task >> merge_cube_task >> disable_cube_task >> purge_cube_task >> segment_by_date_task"}], "metadata": {"file_name": "dag_1143_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.798384}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you suggest a way to modify the given Python code to ensure that a list is only modified if the new value is not already present in the list?"}, {"role": "assistant", "content": "def add_unique_element(lst, element):\n    # Check if the element is not already in the list\n    if element not in lst:\n        # If not, append it to the list\n        lst.append(element)\n        return True\n    else:\n        # If it is, return False indicating the element was not added\n        return False\n\n# Example usage\nmy_list = [1, 2, 3]\nnew_element = 4\n\nif add_unique_element(my_list, new_element):\n    print(\"Element added successfully.\")\nelse:\n    print(\"Element already exists in the list.\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7983894}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that ensures only the most recent workflow run is processed, preventing redundant task executions across multiple scheduling intervals. Create a workflow that demonstrates how to skip unnecessary task runs when a newer workflow instance is already in progress.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='A data pipeline that processes and validates data.',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Task to process the data\nprocess_data_task = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    dag=dag\n)\n\n# Task to validate the data\nvalidate_data_task = PythonOperator(\n    task_id='validate_data',\n    python_callable=validate_data,\n    dag=dag\n)\n\n# Task to check if a new workflow run is needed\ndef check_new_workflow_run():\n    # Get the current DagRun\n    current_dag_run = dag.get_current_dagrun()\n    \n    # Check if there's an existing DagRun with the same key (workflow name)\n    existing_dag_run = dag.get_active_runs(key=current_dag_run.key)[0] if current_dag_run else None\n    \n    return existing_dag_run is None\n\n# Task to skip tasks if a newer workflow instance is already in progress\ndef skip_tasks_if_needed():\n    if not check_new_workflow_run():\n        process_data_task.skip()\n        validate_data_task.skip()\n\nskip_tasks_if_needed_task = PythonOperator(\n    task_id='skip_tasks_if_needed',\n    python_callable=skip_tasks_if_needed,\n    dag=dag\n)\n\n# Set up the dependencies\nprocess_data_task.set_upstream(skip_tasks_if_needed_task)\nvalidate_data_task.set_upstream(process_data_task)"}], "metadata": {"file_name": "dag_335_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981779.7983985}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transformation pipeline that demonstrates dynamic task mapping by taking a list of numbers, incrementing each number, and then calculating their total sum. Create a workflow that shows how Airflow can dynamically generate tasks based on input data.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'dynamic_task_mapping_dag',\n    default_args=default_args,\n    description='A DAG that demonstrates dynamic task mapping',\n    schedule_interval=None,  # Set to None if you want to run it manually\n)\n\n# Function to increment a number and calculate its sum\ndef process_number(number):\n    incremented_number = number + 1\n    return incremented_number * incremented_number\n\n# List of numbers to be processed\nnumbers = [1, 2, 3, 4, 5]\n\n# Dynamic task generator function\ndef generate_tasks(numbers):\n    tasks = []\n    for number in numbers:\n        task = PythonOperator(\n            task_id=f'increment_and_square_{number}',\n            python_callable=process_number,\n            op_kwargs={'number': number},\n        )\n        tasks.append(task)\n    return tasks\n\n# Generate tasks based on the input data\ntasks = generate_tasks(numbers)\n\n# Add all tasks to the DAG\nfor task in tasks:\n    dag.add_task(task)"}], "metadata": {"file_name": "dag_1849_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984061}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I'm looking for a Python function that can send an email with an attachment. Can you provide a sample code?"}, {"role": "assistant", "content": "import smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.base import MIMEBase\nfrom email import encoders\n\ndef send_email_with_attachment(sender_email, receiver_email, password, subject, body, filename):\n    # Create a multipart message\n    msg = MIMEMultipart()\n    \n    # Set up the sender's email details\n    msg['From'] = sender_email\n    msg['To'] = receiver_email\n    msg['Subject'] = subject\n    \n    # Attach the body of the email\n    msg.attach(MIMEText(body, 'plain'))\n    \n    # Open the file to be attached\n    with open(filename, \"rb\") as attachment:\n        part = MIMEBase('application', 'octet-stream')\n        part.set_payload((attachment).read())\n        \n    # Encode the payload into base64\n    encoders.encode_base64(part)\n    \n    # Add header as per RFC 2183\n    part.add_header('Content-Disposition', f'attachment; filename= {filename}')\n    \n    # Attach the instance 'part' to instance 'msg'\n    msg.attach(part)\n    \n    # Create SMTP session\n    server = smtplib.SMTP('smtp.gmail.com', 587)  # For Gmail, use smtp.gmail.com and port 587\n    server.starttls()  # Secure the connection\n    server.login(sender_email, password)  # Login to your email account\n    text = msg.as_string()\n    server.sendmail(sender_email, receiver_email, text)  # Send the email\n    server.quit()\n\n# Example usage\nsender_email = \"your-email@gmail.com\"\nreceiver_email = \"recipient-email@example.com\"\npassword = \"your-password\"\nsubject = \"Test Email with Attachment\"\nbody = \"This is a test email sent from Python.\"\nfilename = \"example.txt\"\n\nsend_email_with_attachment(sender_email, receiver_email, password, subject, body, filename)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981779.7984152}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data processing workflow with multiple interconnected tasks, including cloud resource management, data import, transformation, and export. Implement a systematic approach to handling data pipeline tasks with robust error handling and connection management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_processing_dag',\n    default_args=default_args,\n    description='A complex data processing workflow using Apache Airflow',\n    schedule_interval=None,\n)\n\n# Create dataset in Google Cloud BigQuery\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='my_dataset',\n    project_id='my_project',\n    dag=dag,\n)\n\n# Import data from CSV into Google Cloud Storage\nimport_csv_task = SparkSubmitOperator(\n    task_id='import_csv',\n    application='/usr/bin/spark-submit',\n    main_class='com.example.MySparkApp',\n    spark_conf={'spark.jars.packages': 'org.apache.hadoop:hadoop-common:3.3.4'},\n    py_files=['/path/to/my_script.py'],\n    files=['/path/to/data.csv'],\n    name='import_csv',\n    dag=dag,\n)\n\n# Transform data in Google Cloud BigQuery\ntransform_data_task = BigQueryInsertJobOperator(\n    task_id='transform_data',\n    configuration={\n        'query': \"\"\"\n            SELECT *\n            FROM `my_project.my_dataset.my_table`\n            WHERE column_name = 'value'\n        \"\"\"\n    },\n    destination_dataset_id='my_dataset',\n    destination_table_id='transformed_table',\n    project_id='my_project',\n    dag=dag,\n)\n\n# Export data to PostgreSQL database\nexport_to_postgres_task = PostgresOperator(\n    task_id='export_to_postgres',\n    postgres_conn_id='postgres_conn',\n    sql='INSERT INTO my_table (column_name) VALUES (%(value)s)',\n    parameters={'value': 'new_value'},\n    dag=dag,\n)\n\n# Define dependencies between tasks\ncreate_dataset_task >> import_csv_task >> transform_data_task >> export_to_postgres_task"}], "metadata": {"file_name": "dag_1225_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984235}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a MySQL database cleanup workflow that demonstrates dropping tables using Airflow's SQL operators. Create a DAG that shows how to remove database tables programmatically and sequentially.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.mysql.hooks.mysql import MySQLHook\nfrom airflow.operators.sql_operator import SQLOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 3,\n    'retry_delay': 5 * 60,  # 5 minutes\n}\n\n# Create the DAG\ndag = DAG(\n    'mysql_database_cleanup',\n    default_args=default_args,\n    description='DAG to clean up MySQL database tables',\n    schedule_interval=None,  # Set to None if you want to run manually\n)\n\n# Function to drop a table\ndef drop_table(table_name):\n    hook = MySQLHook(mysql_conn_id='your_mysql_connection')\n    query = f'DROP TABLE IF EXISTS {table_name}'\n    hook.run(query=query)\n\n# List of tables to be dropped\ntables_to_drop = [\n    'old_data_table',\n    'historical_data',\n    'temp_table'\n]\n\n# Create tasks to drop each table\nfor table in tables_to_drop:\n    task = SQLOperator(\n        task_id=f'drop_{table}',\n        sql=drop_table(table),\n        mysql_conn_id='your_mysql_connection',\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_1368_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.798431}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that explores different task execution strategies by implementing parallel pipeline branches with configurable success conditions. Construct a DAG that demonstrates how tasks can be conditionally executed or skipped based on predefined trigger rules.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    dag_id='parallel_pipeline_branches',\n    default_args=default_args,\n    schedule_interval=None,\n)\n\n# Task 1: Check if the input number is even\ndef check_even_number(number):\n    return number % 2 == 0\n\ncheck_even_task = PythonOperator(\n    task_id='check_even_number',\n    python_callable=check_even_number,\n    op_kwargs={'number': 4},\n    dag=dag,\n)\n\n# Task 2: If the number is even, execute this task\neven_task = PythonOperator(\n    task_id='even_task',\n    python_callable=lambda x: print(f'Number {x} is even'),\n    dag=dag,\n    depends_on=[check_even_task],\n)\n\n# Task 3: If the number is odd, skip this task\nodd_task = PythonOperator(\n    task_id='odd_task',\n    python_callable=lambda x: print(f'Number {x} is odd'),\n    dag=dag,\n    depends_on=[check_even_task],\n    trigger_rule='all_failed'\n)\n\n# Branching logic: If the number is even, go to even_task; otherwise, go to odd_task\nbranch_task = BranchPythonOperator(\n    task_id='branch_task',\n    python_callable=lambda x: 'even_task' if check_even_task.output else 'odd_task',\n    op_kwargs={'number': 5},\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_689_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984385}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Data Lake Storage (ADLS) file management workflow that demonstrates uploading a local file and then immediately deleting it. Create a DAG that showcases automated file transfer and cleanup processes in a cloud storage environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.adls import AzureDataLakeHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'upload_file_to_adls',\n    default_args=default_args,\n    description='Uploads a local file to Azure Data Lake Storage',\n    schedule_interval=timedelta(days=1)\n)\n\n# Define the hook for Azure Data Lake\nadls_hook = AzureDataLakeHook(adls_conn_id='azure_data_lake_connection')\n\n# Define the task to upload the file\nupload_task = PythonOperator(\n    task_id='upload_file',\n    python_callable=adls_hook.upload_file,\n    op_kwargs={\n        'container_name': 'my_container',\n        'file_path': '/path/to/local/file.txt',\n        'adls_file_path': '/path/in/adls/file.txt'\n    },\n    dag=dag\n)"}], "metadata": {"file_name": "dag_1353_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984467}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the complete lifecycle of an AutoML image training job, from dataset initialization through model training to resource deletion. Showcase how to programmatically manage machine learning infrastructure using cloud services and workflow orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gcloud_aiplatform import (\n    GCloudAutoMLImageTrainingOperator,\n    GCloudAutoMLImageDeployModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcloud_storage import (\n    GCSUploadOperator,\n    GCSDownloadOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'automl_image_training_dag',\n    default_args=default_args,\n    description='Automl Image Training DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Upload dataset to Google Cloud Storage\nupload_dataset_task = GCSUploadOperator(\n    task_id='upload_dataset',\n    bucket='your-bucket-name',\n    object_name='path/to/your/dataset.zip',\n    file_path='/path/to/your/dataset.zip',\n    dag=dag,\n)\n\n# Step 2: Initialize AutoML Image Training\ninit_auto_ml_training_task = GCloudAutoMLImageTrainingOperator(\n    task_id='initialize_auto_ml_training',\n    project_id='your-project-id',\n    display_name='my-auto-ml-training-job',\n    input_config={\n        'gcs_source': {\n            'input_uris': ['gs://your-bucket-name/path/to/your/dataset.zip'],\n        },\n    },\n    output_config={\n        'model_deployment_uri': 'gs://your-bucket-name/model-deployment-uri',\n    },\n    dag=dag,\n)\n\n# Step 3: Train AutoML Image Model\ntrain_auto_ml_model_task = GCloudAutoMLImageTrainingOperator(\n    task_id='train_auto_ml_model',\n    project_id='your-project-id',\n    display_name='my-auto-ml-model',\n    input_config={\n        'gcs_source': {\n            'input_uris': ['gs://your-bucket-name/path/to/your/dataset.zip'],\n        },\n    },\n    output_config={\n        'model_deployment_uri': 'gs://your-bucket-name/model-deployment-uri',\n    },\n    dag=dag,\n)\n\n# Step 4: Deploy trained model\ndeploy_model_task = GCloudAutoMLImageDeployModelOperator(\n    task_id='deploy_model',\n    project_id='your-project-id',\n    model_id='your-model-id',\n    endpoint_id='your-endpoint-id',\n    deployment_config={\n        'machine_spec': {\n            'machine_type': 'n1-standard-8',\n        },\n    },\n    dag=dag,\n)\n\n# Step 5: Delete resources\ndelete_resources_task = GCloudAutoMLImageTrainingOperator(\n    task_id='delete_resources',\n    project_id='your-project-id',\n    display_name='my-auto-ml-training-job',\n    delete=True,\n    dag=dag,\n)\n\n# Define dependencies\nupload_dataset_task >> init_auto_ml_training_task >> train_auto_ml_model_task >> deploy_model_task >> delete_resources_task"}], "metadata": {"file_name": "dag_2114_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984571}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that captures and transforms data between two sequential assets, demonstrating how Airflow can manage data dependencies and trigger downstream processes. Create a pipeline that shows how one data asset can trigger and inform the processing of a subsequent asset.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='data_processing_dag',\n    description='A simple data processing workflow',\n    default_args=default_args,\n    schedule_interval=None  # Set to None if you want to run manually\n) as dag:\n\n    # Task to generate source data\n    def generate_source_data():\n        print(\"Generating source data...\")\n        return \"Sample Data\"\n\n    source_data_task = PythonOperator(\n        task_id='generate_source_data',\n        python_callable=generate_source_data,\n        provide_context=True\n    )\n\n    # Task to transform source data\n    def transform_source_data(context):\n        source_data = context['task_instance'].xcom_pull(task_ids='generate_source_data')\n        transformed_data = f\"Transformed: {source_data}\"\n        print(transformed_data)\n        return transformed_data\n\n    transformed_data_task = PythonOperator(\n        task_id='transform_source_data',\n        python_callable=transformed_data,\n        provide_context=True\n    )\n\n    # Trigger the transformation task after the source data task completes\n    source_data_task.set_upstream(transformed_data_task)"}], "metadata": {"file_name": "dag_1434_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984672}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated Kubernetes cluster management workflow that demonstrates creating a GKE cluster, running test pods with different configurations, and then cleaning up the cluster. The workflow should showcase how to dynamically create and delete Kubernetes infrastructure while executing sample workloads.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.kubernetes_kubelet import (\n    KubeletStartPodOperator,\n    KubeletStopPodOperator,\n)\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'gke_cluster_management',\n    default_args=default_args,\n    description='Automate GKE cluster management',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create GKE cluster\ncreate_cluster_task = GKECreateClusterOperator(\n    task_id='create_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    num_nodes=3,\n    machine_type='n1-standard-4',\n    service_account='your-service-account@your-project-id.iam.gserviceaccount.com',\n)\n\n# Start test pod with different configurations\ntest_pod_1 = KubeletStartPodOperator(\n    task_id='start_test_pod_1',\n    namespace='default',\n    image='nginx:latest',\n    command=['sleep', '60'],\n    labels={'app': 'test-pod-1'},\n)\n\ntest_pod_2 = KubeletStartPodOperator(\n    task_id='start_test_pod_2',\n    namespace='default',\n    image='busybox',\n    command=['echo', 'Hello from busybox'],\n    labels={'app': 'test-pod-2'},\n)\n\n# Stop test pod\nstop_test_pod_1 = KubeletStopPodOperator(\n    task_id='stop_test_pod_1',\n    namespace='default',\n    pod_name='test-pod-1',\n)\n\nstop_test_pod_2 = KubeletStopPodOperator(\n    task_id='stop_test_pod_2',\n    namespace='default',\n    pod_name='test-pod-2',\n)\n\n# Delete GKE cluster\ndelete_cluster_task = GKEDeleteClusterOperator(\n    task_id='delete_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n)\n\n# Define dependencies\ncreate_cluster_task >> [test_pod_1, test_pod_2]\n[test_pod_1, test_pod_2] >> stop_test_pod_1 >> stop_test_pod_2\nstop_test_pod_1 >> stop_test_pod_2 >> delete_cluster_task"}], "metadata": {"file_name": "dag_1273_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984765}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates cloud storage data movement and management techniques using Alibaba OSS operators. Create a workflow that captures the essential steps of file lifecycle management: transferring data between local and cloud storage, and executing precise object deletion strategies.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.alibaba.cloud.hooks.oss import OssHook\nfrom airflow.providers.apache.hadoop.hdfs.operators.hdfs import HdfsCreateDirOperator, HdfsUploadFileOperator, HdfsDownloadFileOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'alibaba_cloud_oss_dag',\n    default_args=default_args,\n    description='A simple Airflow DAG to manage data in Alibaba Cloud OSS',\n    schedule_interval=None,\n)\n\n# Define variables\nlocal_file_path = '/path/to/local/file.txt'\noss_bucket_name = 'your-oss-bucket-name'\noss_object_key = 'file.txt'\n\n# Task to upload file from local to OSS\nupload_task = HdfsUploadFileOperator(\n    task_id='upload_to_oss',\n    hdfs_conn_id='hdfs_default',\n    path=local_file_path,\n    filename=oss_object_key,\n    bucket=oss_bucket_name,\n    dag=dag\n)\n\n# Task to download file from OSS to local\ndownload_task = HdfsDownloadFileOperator(\n    task_id='download_from_oss',\n    hdfs_conn_id='hdfs_default',\n    path=oss_object_key,\n    filename='/path/to/downloaded/file.txt',\n    bucket=oss_bucket_name,\n    dag=dag\n)\n\n# Task to delete object from OSS\ndelete_task = HdfsDeleteDirOperator(\n    task_id='delete_from_oss',\n    hdfs_conn_id='hdfs_default',\n    path=oss_object_key,\n    bucket=oss_bucket_name,\n    dag=dag\n)\n\n# Set up dependencies\nupload_task >> download_task >> delete_task"}], "metadata": {"file_name": "dag_55_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981779.7984858}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that illustrates the basic mechanics of Azure Blob Storage interaction using Airflow operators, focusing on the sequential upload and deletion of a file in a cloud container.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.blob_storage import AzureBlobStorageHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'azure_blob_storage_example',\n    default_args=default_args,\n    description='A simple example of uploading and deleting files from Azure Blob Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the hook\nblob_hook = AzureBlobStorageHook(azure_conn_id='your_azure_connection')\n\n# Task to upload a file\nupload_file_task = PythonOperator(\n    task_id='upload_file_to_container',\n    python_callable=upload_file_to_container,\n    op_kwargs={\n        'container_name': 'your_container_name',\n        'file_path': '/path/to/your/local/file.txt',\n        'blob_name': 'uploaded_file.txt'\n    },\n    dag=dag,\n)\n\n# Task to delete the uploaded file\ndelete_file_task = PythonOperator(\n    task_id='delete_file_from_container',\n    python_callable=delete_file_from_container,\n    op_kwargs={\n        'container_name': 'your_container_name',\n        'blob_name': 'uploaded_file.txt'\n    },\n    dag=dag,\n)\n\n# Define the function to upload a file\ndef upload_file_to_container(container_name, file_path, blob_name):\n    blob_client = blob_hook.get_blob_client(container_name, blob_name)\n    with open(file_path, \"rb\") as data:\n        blob_client.upload_blob(data)\n\n# Define the function to delete a file\ndef delete_file_from_container(container_name, blob_name):\n    blob_client = blob_hook.get_blob_client(container_name, blob_name)\n    blob_client.delete_blob()\n\n# Set up the dependencies between tasks\nupload_file_task >> delete_file_task"}], "metadata": {"file_name": "dag_2159_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.9300778}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing orchestration DAG that provisions a multi-node Dataproc cluster with preemptible workers and custom initialization scripts for data analysis tasks. Create a reproducible cloud data processing environment that automatically manages computational resources and supports flexible metadata and package installations.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.dates import datetime\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'dataproc_data_analysis_dag',\n    default_args=default_args,\n    description='A DAG to provision a Dataproc cluster with preemptible workers and run a data analysis task.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Step 1: Create a Dataproc cluster\ncreate_cluster_task = DataprocCreateClusterOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your_project_id',  # Replace with your Google Cloud project ID\n    region='us-central1',  # Replace with your preferred region\n    cluster_name='my-dataproc-cluster',\n    num_workers=4,\n    worker_machine_type='n1-standard-2',\n    master_machine_type='n1-standard-2',\n    preemptible=True,\n    init_actions_uris=[\n        'gs://path/to/your/init_script.sh',  # Replace with your init script URI\n    ],\n    metadata={\n        'gce_metadata': {\n            'startup-script': 'gs://path/to/your/startup_script.sh',  # Replace with your startup script URI\n        },\n    },\n)\n\n# Step 2: Submit a job to the Dataproc cluster\nsubmit_job_task = DataprocSubmitJobOperator(\n    task_id='submit_dataproc_job',\n    project_id='your_project_id',  # Replace with your Google Cloud project ID\n    region='us-central1',  # Replace with your preferred region\n    cluster_name='my-dataproc-cluster',\n    main_class='com.example.MyDataAnalysisClass',  # Replace with your main class\n    jar_file_uri='gs://path/to/your/jar_file.jar',  # Replace with your JAR file URI\n    args=['arg1', 'arg2'],  # Replace with your arguments\n)\n\n# Step 3: Delete the Dataproc cluster\ndelete_cluster_task = DataprocDeleteClusterOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your_project_id',  # Replace with your Google Cloud project ID\n    region='us-central1',  # Replace with your preferred region\n    cluster_name='my-dataproc-cluster',\n)\n\n# Define dependencies\ncreate_cluster_task >> submit_job_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_2031_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.9300904}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Could you provide a detailed explanation of how the `input()` function works in Python, including its syntax and typical use cases, and perhaps some examples?"}, {"role": "assistant", "content": "input(prompt)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981847.930099}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data processing and machine learning pipeline that takes raw image data from Google Cloud Storage, prepares it for object detection training, and creates a deployable machine learning model. Implement a systematic approach to handling data ingestion, model creation, and infrastructure management using Airflow and Google Cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.operators.storage import (\n    GoogleCloudStorageDownloadOperator,\n)\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_ingestion',\n    default_args=default_args,\n    description='Ingests images from Google Cloud Storage',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the task to download images\ndownload_task = GoogleCloudStorageDownloadOperator(\n    task_id='download_images',\n    bucket_name='your_bucket_name',\n    object_name='path/to/image.jpg',\n    local_file_path='/path/to/local/image.jpg',\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_462_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301128}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a modular Airflow pipeline that uses task decorators to execute two distinct Pinot interaction tasks with different connection methods. Implement a DAG that demonstrates parallel task execution using Pinot admin and database API hooks.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache_pinot.hooks.pinot_admin_hook import PinotAdminHook\nfrom airflow.providers.apache_pinot.hooks.pinot_database_hook import PinotDatabaseHook\nfrom airflow.decorators import dag, task\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 3,\n    'retry_delay': 5 * 60,\n}\n\n@dag(\n    dag_id='pinot_interaction_dag',\n    description='DAG to interact with Pinot using both admin and database APIs',\n    default_args=default_args,\n    schedule_interval=None,\n)\ndef pinot_interaction_dag():\n    \n    # Task to interact with Pinot Admin API\n    @task(task_id='pinot_admin_task')\n    def pinot_admin_task():\n        hook = PinotAdminHook()\n        # Example operation: Get all tables in Pinot\n        tables = hook.get_tables()\n        print(f\"Pinot Tables: {tables}\")\n        \n    # Task to interact with Pinot Database API\n    @task(task_id='pinot_database_task')\n    def pinot_database_task():\n        hook = PinotDatabaseHook()\n        # Example operation: Query data from a table\n        query_result = hook.query_data('your_table_name', 'SELECT * FROM your_table_name')\n        print(f\"Query Result: {query_result}\")\n\n    # Execute tasks in parallel\n    pinot_admin_task() >> pinot_database_task()"}], "metadata": {"file_name": "dag_112_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301214}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that transforms unstructured trivia data into a semantically searchable vector database using AI-powered embedding techniques. Create an end-to-end pipeline that demonstrates how to load, vectorize, query, and manage structured knowledge data.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.apache_spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.openai.hooks.openai import OpenAIHook\nimport os\n\n# Define the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'trivia_data_processing',\n    default_args=default_args,\n    description='A data processing workflow for trivia data',\n    schedule_interval=timedelta(days=1)\n)\n\n# Load trivia data\nload_trivia_data_task = PythonOperator(\n    task_id='load_trivia_data',\n    python_callable=load_trivia_data,\n    dag=dag\n)\n\n# Vectorize trivia data\nvectorize_trivia_data_task = SparkSubmitOperator(\n    task_id='vectorize_trivia_data',\n    application=os.path.join(os.getcwd(), 'vectorize_trivia_data.py'),\n    executor_class=\"local\",\n    conf={\"spark.driver.memory\": \"8g\", \"spark.executor.memory\": \"8g\"},\n    dag=dag\n)\n\n# Query trivia data\nquery_trivia_data_task = PythonOperator(\n    task_id='query_trivia_data',\n    python_callable=query_trivia_data,\n    dag=dag\n)\n\n# Manage structured knowledge data\nmanage_knowledge_data_task = PythonOperator(\n    task_id='manage_knowledge_data',\n    python_callable=manage_knowledge_data,\n    dag=dag\n)\n\n# Define dependencies\nload_trivia_data_task >> vectorize_trivia_data_task >> query_trivia_data_task >> manage_knowledge_data_task"}], "metadata": {"file_name": "dag_1833_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981847.930134}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated metadata registration pipeline that enables systematic tracking and organization of data assets across different systems and repositories using Google Cloud Dataplex. Design a workflow that captures metadata attributes, defines entry types, and creates catalog entries to support comprehensive data discovery and governance initiatives.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    CreateAssetOperator,\n    UpdateAssetOperator,\n    DeleteAssetOperator,\n)\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dataplex_metadata_registration',\n    default_args=default_args,\n    description='Automated Metadata Registration Pipeline for Google Cloud Dataplex',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the asset details\nasset_details = {\n    \"name\": \"my_dataset\",\n    \"type\": \"DATASET\",\n    \"description\": \"This is my dataset\",\n    \"location\": \"us-central1:my_project_id/my_dataset_location\"\n}\n\n# Create the asset operator\ncreate_asset_operator = CreateAssetOperator(\n    task_id=\"create_asset\",\n    project_id=\"your_project_id\",\n    location=\"us-central1\",\n    body=asset_details,\n    dag=dag,\n)\n\n# Update the asset operator (optional)\nupdate_asset_operator = UpdateAssetOperator(\n    task_id=\"update_asset\",\n    project_id=\"your_project_id\",\n    location=\"us-central1\",\n    asset_name=\"my_dataset\",\n    body={\"description\": \"Updated description\"},\n    dag=dag,\n)\n\n# Delete the asset operator (optional)\ndelete_asset_operator = DeleteAssetOperator(\n    task_id=\"delete_asset\",\n    project_id=\"your_project_id\",\n    location=\"us-central1\",\n    asset_name=\"my_dataset\",\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2026_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.930143}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration pipeline that moves files from cloud object storage to a secure file transfer protocol destination with complete lifecycle tracking. Create a workflow that demonstrates secure, reliable data transfer between different cloud storage systems with comprehensive task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_s3 import S3ToS3Operator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'data_migration_dag',\n    default_args=default_args,\n    description='A simple data migration pipeline from S3 to GCS',\n    schedule_interval=None,  # Set this to your desired frequency\n)\n\n# Define the source and target buckets\nsource_bucket = 'your-source-bucket'\ntarget_bucket = 'your-target-bucket'\n\n# Task to list files in the source bucket\nlist_files_task = GCSToGCSOperator(\n    task_id='list_files',\n    source_bucket=source_bucket,\n    destination_bucket=target_bucket,\n    destination_object_prefix='',\n    dag=dag,\n)\n\n# Task to download files from the source bucket\ndownload_files_task = S3ToS3Operator(\n    task_id='download_files',\n    source_bucket=source_bucket,\n    destination_bucket=target_bucket,\n    destination_object_prefix='',\n    dag=dag,\n)\n\n# Task to upload files to the target bucket\nupload_files_task = GCSToGCSOperator(\n    task_id='upload_files',\n    source_bucket=target_bucket,\n    destination_bucket=source_bucket,\n    destination_object_prefix='',\n    dag=dag,\n)\n\n# Task to set expiration date for uploaded files\nset_expiration_date_task = GCSToGCSOperator(\n    task_id='set_expiration_date',\n    source_bucket=target_bucket,\n    destination_bucket=target_bucket,\n    destination_object_prefix='',\n    destination_object='path/to/your/object',  # Replace with the actual object path\n    expiration_days=30,  # Set the number of days before expiration\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301527}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that creates a schema and multiple tables in Trino for storing city information, demonstrating basic database schema creation and data insertion techniques. The pipeline should showcase different query methods including templated and parameterized queries.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.trino.hooks.trino import TrinoHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'trino_data_pipeline',\n    description='A simple data pipeline to create a schema and tables in Trino',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args\n)\n\n# Function to create a schema in Trino\ndef create_schema():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    query = \"\"\"\n        CREATE SCHEMA IF NOT EXISTS my_schema;\n    \"\"\"\n    hook.run(query=query)\n\n# Function to create tables in Trino\ndef create_tables():\n    hook = TrinoHook(trino_conn_id='trino_default')\n    \n    # Table 1: City Information\n    table_1_query = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_schema.city_info (\n            id INT PRIMARY KEY,\n            name VARCHAR(255) NOT NULL,\n            population INT,\n            country VARCHAR(255)\n        );\n    \"\"\"\n    hook.run(query=table_1_query)\n    \n    # Table 2: Weather Data\n    table_2_query = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_schema.weather_data (\n            id INT PRIMARY KEY,\n            city_id INT,\n            temperature FLOAT,\n            humidity INT,\n            date DATE,\n            FOREIGN KEY (city_id) REFERENCES my_schema.city_info(id)\n        );\n    \"\"\"\n    hook.run(query=table_2_query)\n\n# Task to create the schema\ncreate_schema_task = PythonOperator(\n    task_id='create_schema',\n    python_callable=create_schema,\n    dag=dag\n)\n\n# Task to create the tables\ncreate_tables_task = PythonOperator(\n    task_id='create_tables',\n    python_callable=create_tables,\n    dag=dag\n)\n\n# Task dependencies\ncreate_schema_task >> create_tables_task"}], "metadata": {"file_name": "dag_320_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301627}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated configuration generation and artifact retrieval workflow that uses Jenkins to trigger a job for creating configuration files and then downloads the generated artifact. The workflow should demonstrate inter-system communication and automated artifact management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.jenkins.operators.jenkins import JenkinsOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'jenkins_config_workflow',\n    default_args=default_args,\n    description='Automated configuration generation and artifact retrieval workflow',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the task to trigger Jenkins job\ntrigger_jenkins_job = JenkinsOperator(\n    jenkins_conn_id='your_jenkins_connection_id',  # Replace with your Jenkins connection ID\n    job_name='generate_configurations',\n    dag=dag,\n)\n\n# Define the task to download the artifact from Jenkins\ndownload_artifact = PythonOperator(\n    task_id='download_artifact',\n    python_callable=download_artifact_from_jenkins,\n    dag=dag,\n)\n\n# Define the function to download the artifact from Jenkins\ndef download_artifact_from_jenkins():\n    # Implement the logic to download the artifact from Jenkins\n    # For example, using the Jenkins API or downloading the file directly from the Jenkins server\n    pass\n\n# Define the dependencies between tasks\ntrigger_jenkins_job >> download_artifact"}], "metadata": {"file_name": "dag_960_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301724}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data transformation workflow using Google Dataprep, showcasing task dependencies, connection management, and cloud resource provisioning. Implement a systematic approach to creating, running, and monitoring data processing jobs across different cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataprep import (\n    DataprepCreateProjectOperator,\n    DataprepImportDataOperator,\n    DataprepTransformDataOperator,\n    DataprepExportDataOperator,\n    DataprepDeleteProjectOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'google_dataprep_workflow',\n    default_args=default_args,\n    description='A DAG to orchestrate a complex data transformation workflow using Google Dataprep.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to create a new Google Cloud project\ncreate_project_task = DataprepCreateProjectOperator(\n    task_id='create_project',\n    project_name='my_data_processing_project',\n    dag=dag,\n)\n\n# Task to import data from a source into Dataprep\nimport_data_task = DataprepImportDataOperator(\n    task_id='import_data',\n    project_id='my_data_processing_project',\n    dataset_id='my_dataset',\n    table_id='my_table',\n    file_path='/path/to/your/data.csv',\n    dag=dag,\n)\n\n# Task to transform data in Dataprep\ntransform_data_task = DataprepTransformDataOperator(\n    task_id='transform_data',\n    project_id='my_data_processing_project',\n    dataset_id='my_dataset',\n    table_id='my_transformed_table',\n    dag=dag,\n)\n\n# Task to export data from Dataprep back to a destination\nexport_data_task = DataprepExportDataOperator(\n    task_id='export_data',\n    project_id='my_data_processing_project',\n    dataset_id='my_dataset',\n    table_id='my_exported_table',\n    file_path='/path/to/exported/data.csv',\n    dag=dag,\n)\n\n# Task to delete the Google Cloud project\ndelete_project_task = DataprepDeleteProjectOperator(\n    task_id='delete_project',\n    project_id='my_data_processing_project',\n    dag=dag,\n)\n\n# Define task dependencies\ncreate_project_task >> import_data_task >> transform_data_task >> export_data_task >> delete_project_task"}], "metadata": {"file_name": "dag_1619_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301827}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a monitoring and alerting workflow for Google Cloud infrastructure that allows creating, managing, and configuring alert policies and notification channels. Build a system that can dynamically enable, disable, and track infrastructure health alerts across different Google Cloud resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.gce import GCECreateInstanceOperator, GCENetworkInterfaceOperator\nfrom airflow.providers.google.cloud.operators.gke import GKEClusterOperator, GKENodePoolOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSUploadFileOperator\nfrom airflow.providers.google.cloud.operators.gcp_alertmanager import (\n    GCPAlertManagerCreatePolicyOperator,\n    GCPAlertManagerCreateChannelOperator,\n    GCPAlertManagerUpdatePolicyOperator,\n    GCPAlertManagerUpdateChannelOperator,\n)\nfrom airflow.providers.google.cloud.sensors.gce import GCEInstanceSensor\nfrom airflow.providers.google.cloud.sensors.gke import GKEClusterSensor\nfrom airflow.providers.google.cloud.sensors.gcs import GCSSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    'google_cloud_monitoring_alerting',\n    default_args=default_args,\n    description='Monitoring and Alerting Workflow for Google Cloud Infrastructure',\n    schedule_interval=None,\n)\n\n# Create GCE instance\ncreate_instance_task = GCECreateInstanceOperator(\n    task_id='create_gce_instance',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    machine_type='n1-standard-1',\n    name='my-gce-instance',\n    image_family='ubuntu-os-cloud/ubuntu-2004-lts',\n)\n\n# Create GCE network interface\ncreate_network_interface_task = GCENetworkInterfaceOperator(\n    task_id='create_gce_network_interface',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    network='default',\n    subnetwork='default',\n    access_config='external',\n    name='my-gce-network-interface',\n)\n\n# Create GKE cluster\ncreate_cluster_task = GKEClusterOperator(\n    task_id='create_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    num_nodes=3,\n    node_labels={'environment': 'production'},\n)\n\n# Create GKE node pool\ncreate_node_pool_task = GKENodePoolOperator(\n    task_id='create_gke_node_pool',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    node_count=2,\n    node_labels={'environment': 'production'},\n)\n\n# Upload configuration files to GCS\nupload_config_files_task = GCSUploadFileOperator(\n    task_id='upload_config_files',\n    bucket='your-bucket-name',\n    object_key='alert-manager.yaml',\n    file_path='/path/to/alert-manager.yaml',\n)\n\n# Create GCP Alert Manager policy\ncreate_policy_task = GCPAlertManagerCreatePolicyOperator(\n    task_id='create_gcp_alert_manager_policy',\n    project_id='your-project-id',\n    policy_file='alert-manager.yaml',\n)\n\n# Create GCP Alert Manager channel\ncreate_channel_task = GCPAlertManagerCreateChannelOperator(\n    task_id='create_gcp_alert_manager_channel',\n    project_id='your-project-id',\n    channel_file='channel.yaml',\n)\n\n# Update GCP Alert Manager policy\nupdate_policy_task = GCPAlertManagerUpdatePolicyOperator(\n    task_id='update_gcp_alert_manager_policy',\n    project_id='your-project-id',\n    policy_file='updated-alert-manager.yaml',\n)\n\n# Update GCP Alert Manager channel\nupdate_channel_task = GCPAlertManagerUpdateChannelOperator(\n    task_id='update_gcp_alert_manager_channel',\n    project_id='your-project-id',\n    channel_file='updated-channel.yaml',\n)\n\n# Enable GCE instance\nenable_instance_task = GCEInstanceSensor(\n    task_id='enable_gce_instance',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    resource_name='projects/your-project-id/zones/us-central1-a/instances/my-gce-instance',\n)\n\n# Disable GCE instance\ndisable_instance_task = GCEInstanceSensor(\n    task_id='disable_gce_instance',\n    project_id='your-project-id',\n    zone='us-central1-a',\n    resource_name='projects/your-project-id/zones/us-central1-a/instances/my-gce-instance',\n)\n\n# Enable GKE cluster\nenable_cluster_task = GKEClusterSensor(\n    task_id='enable_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    resource_name='projects/your-project-id/locations/us-central1/clusters/my-gke-cluster',\n)\n\n# Disable GKE cluster\ndisable_cluster_task = GKEClusterSensor(\n    task_id='disable_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    resource_name='projects/your-project-id/locations/us-central1/clusters/my-gke-cluster',\n)\n\n# Enable GKE node pool\nenable_node_pool_task = GKENodePoolSensor(\n    task_id='enable_gke_node_pool',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    resource_name='projects/your-project-id/locations/us-central1/clusters/my-gke-cluster/nodePools/default',\n)\n\n# Disable GKE node pool\ndisable_node_pool_task = GKENodePoolSensor(\n    task_id='disable_gke_node_pool',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    resource_name='projects/your-project-id/locations/us-central1/clusters/my-gke-cluster/nodePools/default',\n)\n\n# Track infrastructure health alerts\ntrack_health_alerts_task = BashOperator(\n    task_id='track_health_alerts',\n    bash_command='echo \"Health alerts tracked successfully\"',\n)\n\n# Define dependencies\ncreate_instance_task >> create_network_interface_task >> create_cluster_task >> create_node_pool_task >> upload_config_files_task >> create_policy_task >> create_channel_task >> update_policy_task >> update_channel_task >> enable_instance_task >> disable_instance_task >> enable_cluster_task >> disable_cluster_task >> enable_node_pool_task >> disable_node_pool_task >> track_health_alerts_task"}], "metadata": {"file_name": "dag_1287_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.9301975}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data pipeline that illustrates how to interact with Databricks SQL environments, performing table creation, data insertion, transformation, file-based export, and cross-storage data import using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.databricks.operators.databricks_sql import (\n    DatabricksSqlCreateTableOperator,\n    DatabricksSqlInsertIntoTableOperator,\n    DatabricksSqlQueryOperator,\n    DatabricksSqlExportTableToParquetOperator,\n    DatabricksSqlImportTableFromParquetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSToGCSOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'databricks_data_pipeline',\n    description='A simple data pipeline using Airflow and Databricks SQL',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args,\n)\n\n# Step 1: Create a table in Databricks\ncreate_table_task = DatabricksSqlCreateTableOperator(\n    task_id='create_table',\n    database_name='my_database',\n    schema_name='my_schema',\n    table_name='my_table',\n    sql=\"CREATE TABLE my_table (id INT, name STRING)\",\n    dag=dag,\n)\n\n# Step 2: Insert data into the table\ninsert_data_task = DatabricksSqlInsertIntoTableOperator(\n    task_id='insert_data',\n    database_name='my_database',\n    schema_name='my_schema',\n    table_name='my_table',\n    values=[(1, 'Alice'), (2, 'Bob')],\n    dag=dag,\n)\n\n# Step 3: Query data from the table\nquery_data_task = DatabricksSqlQueryOperator(\n    task_id='query_data',\n    database_name='my_database',\n    schema_name='my_schema',\n    table_name='my_table',\n    query=\"SELECT * FROM my_table\",\n    dag=dag,\n)\n\n# Step 4: Export table to Parquet format\nexport_to_parquet_task = DatabricksSqlExportTableToParquetOperator(\n    task_id='export_to_parquet',\n    database_name='my_database',\n    schema_name='my_schema',\n    table_name='my_table',\n    output_path='gs://my-bucket/my-table.parquet',\n    dag=dag,\n)\n\n# Step 5: Import Parquet data back into Databricks\nimport_from_parquet_task = DatabricksSqlImportTableFromParquetOperator(\n    task_id='import_from_parquet',\n    database_name='my_database',\n    schema_name='my_schema',\n    input_path='gs://my-bucket/my-table.parquet',\n    table_name='my_table',\n    dag=dag,\n)\n\n# Task dependencies\ncreate_table_task >> insert_data_task >> query_data_task >> export_to_parquet_task >> import_from_parquet_task"}], "metadata": {"file_name": "dag_1558_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302068}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an observability pipeline that showcases system interaction tasks, including date printing, sleep operations, and containerized command execution with clear task interdependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 10, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'observability_pipeline',\n    default_args=default_args,\n    description='A simple observability pipeline',\n    schedule_interval=timedelta(minutes=5)\n)\n\n# Task to print current date\nprint_date_task = PythonOperator(\n    task_id='print_date',\n    python_callable=lambda: print(f\"Current Date: {datetime.now()}\"),\n    dag=dag\n)\n\n# Task to sleep for 5 seconds\nsleep_5_seconds_task = PythonOperator(\n    task_id='sleep_5_seconds',\n    python_callable=lambda: time.sleep(5),\n    dag=dag\n)\n\n# Task to execute a containerized command (example using Docker)\nexecute_container_task = BashOperator(\n    task_id='execute_container',\n    bash_command='docker run --rm -it ubuntu /bin/bash',\n    dag=dag\n)\n\n# Set up the dependencies\nprint_date_task >> sleep_5_seconds_task >> execute_container_task"}], "metadata": {"file_name": "dag_999_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.930214}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS SQS message queue workflow that demonstrates publishing and consuming messages across multiple tasks. Create a DAG that showcases how to interact with Amazon SQS by sending messages, reading messages in single and batch modes, and managing queue lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.sqs import SQSHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    'aws_sqs_workflow',\n    default_args=default_args,\n    description='A simple workflow to demonstrate interactions with Amazon SQS',\n    schedule_interval=None,\n)\n\n# Function to publish a message to SQS\ndef publish_message():\n    sqs_hook = SQSHook()\n    queue_url = 'YOUR_SQS_QUEUE_URL'\n    message_body = 'Hello from Airflow!'\n    sqs_hook.put_message(queue_url=queue_url, message_body=message_body)\n    print(f\"Message published successfully: {message_body}\")\n\n# Task to publish a message\npublish_task = PythonOperator(\n    task_id='publish_message',\n    python_callable=publish_message,\n    dag=dag,\n)\n\n# Function to read messages from SQS\ndef read_messages():\n    sqs_hook = SQSHook()\n    queue_url = 'YOUR_SQS_QUEUE_URL'\n    max_number_of_messages = 10\n    visibility_timeout = 60\n    messages = sqs_hook.receive_message(\n        QueueUrl=queue_url,\n        MaxNumberOfMessages=max_number_of_messages,\n        VisibilityTimeout=visibility_timeout,\n    )\n    if messages:\n        for message in messages:\n            print(f\"Received message: {message.body}\")\n            sqs_hook.delete_message(QueueUrl=queue_url, ReceiptHandle=message.receipt_handle)\n    else:\n        print(\"No messages received.\")\n\n# Task to read messages in batch mode\nread_batch_task = PythonOperator(\n    task_id='read_messages_batch',\n    python_callable=read_messages,\n    dag=dag,\n)\n\n# Task to delete the SQS queue\ndef delete_queue():\n    sqs_hook = SQSHook()\n    queue_url = 'YOUR_SQS_QUEUE_URL'\n    sqs_hook.delete_queue(QueueUrl=queue_url)\n    print(f\"Queue deleted successfully.\")\n\n# Task to delete the SQS queue\ndelete_queue_task = PythonOperator(\n    task_id='delete_queue',\n    python_callable=delete_queue,\n    dag=dag,\n)\n\n# Dependencies\npublish_task >> read_messages_task >> read_batch_task >> delete_queue_task"}], "metadata": {"file_name": "dag_1133_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.930224}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that illustrates advanced scheduling mechanisms using dataset dependencies, showcasing how tasks can be triggered based on dataset updates, time schedules, and complex logical conditions. Develop a series of DAGs that demonstrate dataset-based scheduling with various dependency scenarios.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'dataset_based_scheduling_time_schedule',\n    default_args=default_args,\n    description='A DAG that runs every hour',\n    schedule_interval=timedelta(hours=1)\n)\n\ndef print_current_time():\n    print(\"Current Time:\", datetime.now())\n\ntask = PythonOperator(\n    task_id='print_current_time',\n    python_callable=print_current_time,\n    dag=dag\n)"}], "metadata": {"file_name": "dag_671_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302323}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data pipeline that filters and exports specific documents from a MongoDB collection to an S3 storage system using predefined query criteria. Construct a workflow that selectively moves data based on a status condition, ensuring only relevant documents are transferred between different data storage platforms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.mongodb.hooks.mongodb import MongoDBHook\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'mongodb_to_s3_pipeline',\n    default_args=default_args,\n    description='A data pipeline that filters and exports specific documents from a MongoDB collection to an S3 storage system.',\n    schedule_interval=None,\n)\n\n# Define MongoDB connection\nmongo_conn_id = \"mongodb_default\"\nmongo_hook = MongoDBHook(mongo_conn_id=mongo_conn_id)\n\n# Define S3 connection\ns3_conn_id = \"s3_default\"\ns3_hook = S3Hook(s3_conn_id=s3_conn_id)\n\ndef filter_and_export_documents(**kwargs):\n    # Get MongoDB connection\n    mongo_client = mongo_hook.get_connection()\n    \n    # Define MongoDB query\n    query = {\"status\": \"relevant\"}\n    \n    # Filter documents from MongoDB\n    filtered_docs = mongo_client.your_database.your_collection.find(query)\n    \n    # Define S3 bucket and key\n    s3_bucket_name = \"your-bucket-name\"\n    s3_key_prefix = \"your-key-prefix/\"\n    \n    # Upload documents to S3\n    for doc in filtered_docs:\n        file_path = f\"{s3_key_prefix}{doc['_id']}.json\"\n        s3_hook.load_string(str(doc), file_path, bucket_name=s3_bucket_name)\n\nfilter_and_export_task = PythonOperator(\n    task_id='filter_and_export_documents',\n    python_callable=filter_and_export_documents,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_1910_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302387}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data engineering workflow that leverages Google Cloud Build for automated software deployment and infrastructure provisioning. Create a scalable DAG that demonstrates dynamic trigger creation, project number retrieval, and cloud resource orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import GKEPodOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\nfrom airflow.utils.dates import days_ago\nimport os\n\n# Set default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'data_engineering_workflow',\n    default_args=default_args,\n    description='A data engineering workflow using Google Cloud Build',\n    schedule_interval=None,  # Use manual triggering\n)\n\n# Task to upload data from GCS to BigQuery\nupload_data_task = GCSToBigQueryOperator(\n    task_id='upload_data_to_bigquery',\n    source_bucket='your-source-bucket',\n    destination_dataset_table='your-dataset.your-table',\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag,\n)\n\n# Task to build Docker image\nbuild_docker_image_task = GKEPodOperator(\n    task_id='build_docker_image',\n    namespace='default',\n    image_name='your-image-name',\n    command=['bash', '-c', 'docker build -t gcr.io/your-project-id/your-image-name .'],\n    dag=dag,\n)\n\n# Task to push Docker image to Google Container Registry (GCR)\npush_image_task = GKEPodOperator(\n    task_id='push_image_to_gcr',\n    namespace='default',\n    image_name='gcr.io/your-project-id/your-image-name',\n    command=['bash', '-c', 'docker push gcr.io/your-project-id/your-image-name'],\n    dag=dag,\n)\n\n# Task to deploy the Docker container to GKE\ndeploy_container_task = GKEPodOperator(\n    task_id='deploy_container_to_gke',\n    namespace='default',\n    image_name='gcr.io/your-project-id/your-image-name',\n    command=['kubectl', 'apply', '-f', 'deployment.yaml'],\n    dag=dag,\n)\n\n# Task to delete the GKE cluster after use\ndelete_cluster_task = GKEPodOperator(\n    task_id='delete_cluster_after_use',\n    namespace='default',\n    image_name='gcr.io/cloud-builders/kubectl',\n    command=['kubectl', 'delete', 'cluster', 'your-cluster-name'],\n    dag=dag,\n)\n\n# Define the dependencies between tasks\nupload_data_task >> build_docker_image_task >> push_image_task >> deploy_container_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_1997_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.93025}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates a basic sensor mechanism for waiting on upstream data readiness before proceeding with downstream tasks. Design a simple workflow that simulates checking for data availability and then executing a subsequent operation.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='A simple data pipeline using sensors',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task 1: Simulate data generation\ndef generate_data():\n    print(\"Data generated\")\n\ntask1 = PythonOperator(\n    task_id='generate_data',\n    python_callable=generate_data,\n    dag=dag,\n)\n\n# Task 2: Check for data availability\ndef check_data_availability():\n    return True  # Replace with actual data availability check logic\n\nsensor = FileSensor(\n    task_id='check_data_availability',\n    filepath='/path/to/data',  # Replace with your actual file path\n    poke_interval=60,  # Check every minute\n    dag=dag,\n)\n\n# Task 3: Execute downstream operations\ndef execute_downstream_operations():\n    print(\"Executing downstream operations\")\n\ntask3 = PythonOperator(\n    task_id='execute_downstream_operations',\n    python_callable=execute_downstream_operations,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\ntask1 >> sensor >> task3"}], "metadata": {"file_name": "dag_19_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302576}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a multi-stage data orchestration process using task dependencies and different operators like BashOperator and HiveOperator. Implement a workflow that shows how data can be transferred between local storage, HDFS, Hive tables, and a MySQL database.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.providers.apache.hive.operators.hive import HiveOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'data_orchestration_dag',\n    description='A multi-stage data orchestration process',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args\n)\n\n# Task to transfer data from local storage to HDFS\nlocal_to_hdfs_task = BashOperator(\n    task_id='local_to_hdfs',\n    bash_command='hdfs dfs -put /path/to/local/data /path/to/hdfs/data',\n    dag=dag\n)\n\n# Task to create a Hive table in HDFS\nhive_create_table_task = HiveOperator(\n    task_id='hive_create_table',\n    hql='CREATE TABLE IF NOT EXISTS my_table (id INT, name STRING)',\n    dag=dag\n)\n\n# Task to insert data into Hive table\nhive_insert_data_task = HiveOperator(\n    task_id='hive_insert_data',\n    hql='INSERT INTO TABLE my_table VALUES (1, \"Alice\"), (2, \"Bob\")',\n    dag=dag\n)\n\n# Task to transfer data from Hive table to MySQL\nmysql_transfer_task = BashOperator(\n    task_id='mysql_transfer',\n    bash_command='mysql -u username -p mydatabase < /path/to/mysql_script.sql',\n    dag=dag\n)\n\n# Task to clean up temporary files\ncleanup_task = BashOperator(\n    task_id='cleanup',\n    bash_command='rm -rf /path/to/temporary/files',\n    dag=dag\n)\n\n# Set up task dependencies\nlocal_to_hdfs_task >> hive_create_table_task >> hive_insert_data_task >> mysql_transfer_task >> cleanup_task"}], "metadata": {"file_name": "dag_1939_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302657}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that simulates a data processing scenario with explicit resource management, showing how to handle cluster initialization, data transformation, and cluster teardown in a single coordinated workflow. Construct a DAG that demonstrates best practices for managing computational resources across different task stages.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.spark.operators.cluster_manager import ClusterManagerOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'spark_data_processing_dag',\n    default_args=default_args,\n    description='A simple DAG for data processing using Apache Spark',\n    schedule_interval=None,\n)\n\n# Task to initialize the Spark cluster\ninit_cluster_task = ClusterManagerOperator(\n    task_id='initialize_cluster',\n    command='spark-submit --master yarn --deploy-mode client /path/to/your/spark_script.py',\n    dag=dag,\n)\n\n# Task to transform the data\ntransform_data_task = ClusterManagerOperator(\n    task_id='transform_data',\n    command='spark-submit --master yarn --deploy-mode client /path/to/your/data_transform_script.py',\n    dag=dag,\n)\n\n# Task to tear down the Spark cluster\nteardown_cluster_task = ClusterManagerOperator(\n    task_id='teardown_cluster',\n    command='spark-submit --master yarn --deploy-mode client /path/to/your/teardown_script.py',\n    dag=dag,\n)\n\n# Set up dependencies between tasks\ninit_cluster_task >> transform_data_task >> teardown_cluster_task"}], "metadata": {"file_name": "dag_1054_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302738}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that showcases the sequential execution of Kylin cube operations, highlighting task dependencies, time-based data segmentation, and the progression of data transformation steps from build to purge.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'kylin_cube_operations',\n    default_args=default_args,\n    description='A DAG for executing Kylin cube operations',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to build a cube\nbuild_cube_task = PythonOperator(\n    task_id='build_cube',\n    python_callable=build_cube_function,\n    dag=dag\n)\n\n# Task to segment data\nsegment_data_task = PythonOperator(\n    task_id='segment_data',\n    python_callable=segment_data_function,\n    dag=dag\n)\n\n# Task to purge old data\npurge_old_data_task = PythonOperator(\n    task_id='purge_old_data',\n    python_callable=purge_old_data_function,\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nbuild_cube_task.set_upstream(segment_data_task)\nsegment_data_task.set_upstream(purge_old_data_task)"}], "metadata": {"file_name": "dag_1535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302804}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a sequence of cloud tasks involving bucket creation, audio generation, speech recognition, and bucket cleanup as a systematic demonstration of Google Cloud Speech API interactions. Showcase how different cloud operators can be chained together to create a comprehensive speech processing workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.speech import (\n    GoogleCloudSpeechCreateAudioOperator,\n    GoogleCloudSpeechRecognizeOperator,\n    GoogleCloudStorageDeleteBucketOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_speech_workflow',\n    default_args=default_args,\n    description='A simple workflow for Google Cloud Speech API',\n    schedule_interval=None,\n)\n\n# Step 1: Create a bucket\ncreate_bucket_task = GoogleCloudStorageCreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my-speech-processing-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Step 2: Generate audio\ngenerate_audio_task = GoogleCloudSpeechCreateAudioOperator(\n    task_id='generate_audio',\n    audio_content=b'your-audio-content-here',  # Replace with actual audio content\n    bucket_name='my-speech-processing-bucket',\n    filename='audio.wav',\n    format='WAV',\n    encoding='LINEAR16',\n    sample_rate_hertz=44100,\n    language_code='en-US',\n    dag=dag,\n)\n\n# Step 3: Recognize speech\nrecognize_speech_task = GoogleCloudSpeechRecognizeOperator(\n    task_id='recognize_speech',\n    audio_uri=f\"gs://{create_bucket_task.output}/audio.wav\",\n    model=\"video\",\n    language_code='en-US',\n    max_alternatives=1,\n    enable_automatic_punctuation=True,\n    output_config={\n        \"encoding\": \"UTF-8\",\n        \"sample_rate_hertz\": 44100,\n        \"enable_word_time_offsets\": True,\n    },\n    dag=dag,\n)\n\n# Step 4: Delete bucket\ndelete_bucket_task = GoogleCloudStorageDeleteBucketOperator(\n    task_id='delete_bucket',\n    bucket_name='my-speech-processing-bucket',\n    project_id='your-project-id',\n    dag=dag,\n)\n\n# Define dependencies\ncreate_bucket_task >> generate_audio_task >> recognize_speech_task >> delete_bucket_task"}], "metadata": {"file_name": "dag_1680_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981847.9302883}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates the complete lifecycle of data catalog operations, including creating entry groups, registering file entries, and attaching custom metadata tags. Develop a systematic workflow that illustrates how to interact with Google Cloud Data Catalog using Airflow operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CreateEntryGroupOperator,\n    RegisterFileEntriesOperator,\n    AttachCustomMetadataTagsOperator,\n)\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\ndag = DAG(\n    'data_catalog_operations_dag',\n    default_args=default_args,\n    description='DAG to demonstrate data catalog operations',\n    schedule_interval=None,\n)\n\n# Step 1: Create Entry Group\ncreate_entry_group_task = CreateEntryGroupOperator(\n    task_id='create_entry_group',\n    project_id='your_project_id',\n    location='us-central1',\n    entry_group_id='my_entry_group',\n    display_name='My Entry Group',\n)\n\n# Step 2: Register File Entries\nregister_file_entries_task = RegisterFileEntriesOperator(\n    task_id='register_file_entries',\n    project_id='your_project_id',\n    location='us-central1',\n    entry_group_id='my_entry_group',\n    file_paths=['gs://your_bucket/path/to/file1.csv', 'gs://your_bucket/path/to/file2.csv'],\n    schema={\n        'fields': [\n            {'name': 'id', 'type': 'STRING'},\n            {'name': 'name', 'type': 'STRING'},\n            {'name': 'age', 'type': 'INTEGER'},\n        ],\n    },\n)\n\n# Step 3: Attach Custom Metadata Tags\nattach_custom_metadata_tags_task = AttachCustomMetadataTagsOperator(\n    task_id='attach_custom_metadata_tags',\n    project_id='your_project_id',\n    location='us-central1',\n    entry_group_id='my_entry_group',\n    tag_keys=['tag1', 'tag2'],\n    tag_values=['value1', 'value2'],\n)\n\n# Define dependencies\ncreate_entry_group_task >> register_file_entries_task >> attach_custom_metadata_tags_task"}], "metadata": {"file_name": "dag_2135_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.930297}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me understand how to solve a system of linear equations using Python? Certainly! To solve a system of linear equations, you can use several methods in Python, including NumPy, which provides a convenient and efficient way to handle such computations."}, {"role": "assistant", "content": "import numpy as np\n\n# Define the coefficients matrix A and the constant matrix b\nA = np.array([[2, 1], [3, -1]])\nb = np.array([5, 4])\n\n# Use NumPy's linalg.solve function to find the solution\nsolution = np.linalg.solve(A, b)\n\nprint(\"The solution to the system of equations is:\")\nprint(solution)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981847.9303017}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown stages for Google Calendar data synchronization. Implement a workflow that creates necessary cloud resources, handles authentication, transfers data, and then systematically removes all temporary infrastructure components.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.hooks.calendar import GoogleCalendarHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'google_calendar_sync',\n    default_args=default_args,\n    description='A DAG to sync Google Calendar data',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to create Google Calendar service account key\ncreate_key_task = PythonOperator(\n    task_id='create_key_task',\n    python_callable=create_google_calendar_service_account_key,\n    dag=dag,\n)\n\n# Task to authenticate with Google Calendar API\nauthenticate_task = PythonOperator(\n    task_id='authenticate_task',\n    python_callable=authenticate_with_google_calendar_api,\n    dag=dag,\n)\n\n# Task to list calendars\nlist_calendars_task = PythonOperator(\n    task_id='list_calendars_task',\n    python_callable=list_calendars,\n    dag=dag,\n)\n\n# Task to transfer events from Google Calendar to another system (example: CSV)\ntransfer_events_task = PythonOperator(\n    task_id='transfer_events_task',\n    python_callable=transfer_events_to_csv,\n    dag=dag,\n)\n\n# Task to clean up temporary files\ncleanup_task = PythonOperator(\n    task_id='cleanup_task',\n    python_callable=cleanup_temporary_files,\n    dag=dag,\n)\n\n# Dependencies\ncreate_key_task >> authenticate_task >> list_calendars_task >> transfer_events_task >> cleanup_task"}], "metadata": {"file_name": "dag_1248_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.930311}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates file transfer from a local temporary storage to an AWS S3 bucket, showcasing how to programmatically create, move, and clean up temporary files in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3MoveObjectOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.local_filesystem import LocalFileSensor\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'file_transfer_to_s3',\n    default_args=default_args,\n    description='A simple DAG to transfer a file from local to S3',\n    schedule_interval=None,\n)\n\n# Task to check if the local file exists\ncheck_local_file = LocalFileSensor(\n    task_id='check_local_file',\n    filepath='/path/to/local/file.txt',\n    dag=dag\n)\n\n# Task to create an S3 bucket (if it doesn't exist)\ncreate_bucket = S3CreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my-temp-bucket',\n    region_name='us-west-2',  # Replace with your desired region\n    dag=dag\n)\n\n# Task to upload the file to S3\nupload_file = S3UploadOperator(\n    task_id='upload_file',\n    s3_key='my-file.txt',\n    s3_object_key='my-file.txt',\n    bucket_name='my-temp-bucket',\n    replace=True,\n    dag=dag\n)\n\n# Task to move the file from local to S3\nmove_file = S3MoveObjectOperator(\n    task_id='move_file',\n    source_bucket_name='my-temp-bucket',\n    source_key='my-file.txt',\n    destination_bucket_name='my-temp-bucket',\n    destination_key='my-file.txt',\n    dag=dag\n)\n\n# Task to delete the local file\ndelete_local_file = LocalFileRemoveOperator(\n    task_id='delete_local_file',\n    filepath='/path/to/local/file.txt',\n    dag=dag\n)\n\n# Task to delete the S3 bucket\ndelete_bucket = S3DeleteBucketOperator(\n    task_id='delete_bucket',\n    bucket_name='my-temp-bucket',\n    force_delete=True,\n    dag=dag\n)\n\n# Define the dependencies between tasks\ncheck_local_file >> create_bucket >> upload_file >> move_file >> delete_local_file >> delete_bucket"}], "metadata": {"file_name": "dag_1957_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981847.9303184}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that copies employee salary data from one BigQuery table to another within the same dataset. Create an Airflow DAG that demonstrates table creation, data transfer, and cleanup for a simple HR data management scenario.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'employee_salary_migration',\n    description='Migrates employee salary data from one BigQuery table to another',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args,\n    catchup=False\n)\n\n# Hook for interacting with Google Cloud services\nbigquery_hook = BigQueryHook()\n\n# Step 1: Create the source table if it doesn't exist\ncreate_source_table_task = BigQueryCreateTableOperator(\n    task_id='create_source_table',\n    project_id='your_project_id',\n    dataset_id='hr_data',\n    table_id='source_employee_salaries',\n    schema=[\n        {'name': 'id', 'type': 'INTEGER'},\n        {'name': 'first_name', 'type': 'STRING'},\n        {'name': 'last_name', 'type': 'STRING'},\n        {'name': 'salary', 'type': 'FLOAT'}\n    ],\n    dag=dag\n)\n\n# Step 2: Create the destination table if it doesn't exist\ncreate_destination_table_task = BigQueryCreateTableOperator(\n    task_id='create_destination_table',\n    project_id='your_project_id',\n    dataset_id='hr_data',\n    table_id='destination_employee_salaries',\n    schema=[\n        {'name': 'id', 'type': 'INTEGER'},\n        {'name': 'first_name', 'type': 'STRING'},\n        {'name': 'last_name', 'type': 'STRING'},\n        {'name': 'salary', 'type': 'FLOAT'}\n    ],\n    dag=dag\n)\n\n# Step 3: Copy data from source to destination table\ncopy_data_task = BigQueryToBigQueryOperator(\n    task_id='copy_data',\n    project_id='your_project_id',\n    source_dataset_id='hr_data',\n    source_table_id='source_employee_salaries',\n    destination_dataset_id='hr_data',\n    destination_table_id='destination_employee_salaries',\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag\n)\n\n# Step 4: Clean up temporary tables (optional)\ncleanup_temp_tables_task = BigQueryDeleteTablesOperator(\n    task_id='cleanup_temp_tables',\n    project_id='your_project_id',\n    dataset_id='hr_data',\n    table_ids=['temp_source_employee_salaries', 'temp_destination_employee_salaries'],\n    dag=dag\n)\n\n# Define the dependencies between tasks\ncreate_source_table_task >> create_destination_table_task >> copy_data_task >> cleanup_temp_tables_task"}], "metadata": {"file_name": "dag_816_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.9303286}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an end-to-end machine learning data pipeline that demonstrates model training, throughput allocation, and model lifecycle management using AWS Bedrock services. Create a systematic approach to preparing, training, and deploying custom AI models with automated monitoring and cleanup.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockModelHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'ml_pipeline',\n    default_args=default_args,\n    description='A simple ML pipeline using AWS Bedrock',\n    schedule_interval=None,\n)\n\n# Define tasks\ndef prepare_data():\n    s3_hook = S3Hook()\n    bucket_name = 'your-bucket-name'\n    key = 'path/to/your/data.csv'\n    local_path = '/tmp/data.csv'\n    s3_hook.download_file(bucket_name, key, local_path)\n\ndef train_model():\n    bedrock_model_hook = BedrockModelHook(model_id='amazon.titanic')\n    input_data = open('/tmp/data.csv', 'rb')\n    output_data = bedrock_model_hook.invoke(input_data=input_data)\n    output_data.save('output.json')\n\nprepare_data_task = PythonOperator(\n    task_id='prepare_data',\n    python_callable=prepare_data,\n    dag=dag\n)\n\ntrain_model_task = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model,\n    dag=dag\n)\n\nprepare_data_task >> train_model_task"}], "metadata": {"file_name": "dag_1075_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.930338}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how to manage resource allocation and cleanup for a multi-stage data processing pipeline. Create a DAG that includes preparatory tasks, main work tasks, and cleanup tasks to simulate a complete operational workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_processing_pipeline',\n    description='A simple data processing pipeline with setup and teardown tasks.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args\n)\n\n# Setup task\nsetup_task = PythonOperator(\n    task_id='setup_task',\n    python_callable=setup_function,\n    dag=dag\n)\n\n# Main work task\nmain_work_task = PythonOperator(\n    task_id='main_work_task',\n    python_callable=main_work_function,\n    dag=dag\n)\n\n# Cleanup task\ncleanup_task = PythonOperator(\n    task_id='cleanup_task',\n    python_callable=cleanup_function,\n    dag=dag\n)\n\n# Define dependencies\nsetup_task >> main_work_task >> cleanup_task"}], "metadata": {"file_name": "dag_1054_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981847.9303467}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I handle the issue of receiving a 500 Internal Server Error when attempting to delete a record using an API endpoint in Python?"}, {"role": "assistant", "content": "import requests\n\n# Define the API endpoint URL\nurl = 'https://api.example.com/records/123'\n\n# Set up the headers with appropriate authentication or authorization\nheaders = {\n    'Authorization': 'Bearer your_access_token',\n    'Content-Type': 'application/json'\n}\n\ntry:\n    # Send the DELETE request\n    response = requests.delete(url, headers=headers)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        print(\"Record deleted successfully.\")\n    elif response.status_code == 500:\n        print(\"Internal Server Error:\", response.text)\n    else:\n        print(\"Unexpected error:\", response.status_code)\nexcept requests.exceptions.RequestException as e:\n    print(\"An error occurred:\", e)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981847.9303553}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering pipeline that demonstrates dynamic cluster provisioning and management for big data processing workloads, showing how to dynamically allocate and deallocate computational resources based on processing requirements. The workflow should highlight the flexibility of cloud-based data processing infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.batch import AwsBatchHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'dynamic_cluster_management',\n    default_args=default_args,\n    description='A data engineering pipeline for dynamic cluster provisioning and management',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Task to provision a cluster\ndef provision_cluster():\n    hook = AwsBatchHook()\n    response = hook.create_job_definition(\n        jobDefinitionName='my-cluster-definition',\n        type='container',\n        containerProperties={\n            'image': 'ubuntu:latest',\n            'command': ['sleep', '60']\n        }\n    )\n    print(f\"Cluster definition created with ID: {response['jobDefinitionArn']}\")\n\nprovision_cluster_task = PythonOperator(\n    task_id='provision_cluster',\n    python_callable=provision_cluster,\n    dag=dag\n)\n\n# Task to manage the cluster\ndef manage_cluster():\n    hook = AwsBatchHook()\n    response = hook.describe_jobs(jobNames=['my-cluster-definition'])\n    if response['jobs'][0]['status'] == 'COMPLETED':\n        print(\"Cluster is completed. Deallocating resources...\")\n        hook.delete_job_definition(jobDefinitionName='my-cluster-definition')\n    else:\n        print(\"Cluster is still running. Waiting for completion...\")\n\nmanage_cluster_task = PythonOperator(\n    task_id='manage_cluster',\n    python_callable=manage_cluster,\n    dag=dag\n)\n\n# Define the dependency between tasks\nprovision_cluster_task >> manage_cluster_task"}], "metadata": {"file_name": "dag_858_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981847.930365}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that moves files from Google Cloud Storage to a Samba file share, demonstrating different transfer scenarios like single file copy, file move, directory copy, and selective file movement. Create a DAG that showcases flexible file migration strategies between cloud and network storage systems.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSCredentialsHook\nfrom airflow.providers.samba.hooks.samba import SMBHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'gcs_to_samba_transfer',\n    description='Transfer files from GCS to Samba',\n    schedule_interval=timedelta(hours=1),  # Adjust this interval as needed\n    default_args=default_args,\n    catchup=False\n)\n\n# Function to upload a file to Samba\ndef upload_file_to_samba(file_path, samba_url):\n    hook = SMBHook(samba_url)\n    with open(file_path, 'rb') as f:\n        hook.put(f.read(), file_path.split('/')[-1])\n\n# Task to upload a single file to Samba\nupload_single_file_task = PythonOperator(\n    task_id='upload_single_file',\n    python_callable=upload_file_to_samba,\n    op_kwargs={\n        'file_path': '/path/to/your/local/file.txt',  # Replace with your local file path\n        'samba_url': 'smb://username:password@your-samba-server/share'\n    },\n    dag=dag\n)\n\n# Task to upload a directory to Samba\nupload_directory_task = PythonOperator(\n    task_id='upload_directory',\n    python_callable=upload_file_to_samba,\n    op_kwargs={\n        'file_path': '/path/to/your/local/directory',  # Replace with your local directory path\n        'samba_url': 'smb://username:password@your-samba-server/share'\n    },\n    dag=dag\n)\n\n# Task to move a file in GCS to Samba\nmove_file_task = PythonOperator(\n    task_id='move_file',\n    python_callable=upload_file_to_samba,\n    op_kwargs={\n        'file_path': 'gs://your-bucket-name/your-file.txt',  # Replace with your GCS file path\n        'samba_url': 'smb://username:password@your-samba-server/share'\n    },\n    dag=dag\n)\n\n# Task to delete a file in GCS after moving it to Samba\ndelete_file_task = PythonOperator(\n    task_id='delete_file',\n    python_callable=lambda: None,\n    op_kwargs={\n        'file_path': 'gs://your-bucket-name/your-file.txt'  # Replace with your GCS file path\n    },\n    dag=dag\n)\n\n# Task to move a directory in GCS to Samba\nmove_directory_task = PythonOperator(\n    task_id='move_directory',\n    python_callable=upload_file_to_samba,\n    op_kwargs={\n        'file_path': 'gs://your-bucket-name/your-directory',  # Replace with your GCS directory path\n        'samba_url': 'smb://username:password@your-samba-server/share'\n    },\n    dag=dag\n)\n\n# Task to delete a directory in GCS after moving it to Samba\ndelete_directory_task = PythonOperator(\n    task_id='delete_directory',\n    python_callable=lambda: None,\n    op_kwargs={\n        'file_path': 'gs://your-bucket-name/your-directory'  # Replace with your GCS directory path\n    },\n    dag=dag\n)\n\n# Set up dependencies\nupload_single_file_task >> upload_directory_task >> move_file_task >> delete_file_task\nupload_single_file_task >> upload_directory_task >> move_directory_task >> delete_directory_task"}], "metadata": {"file_name": "dag_646_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.2783806}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates scalable cloud data movement techniques, focusing on transferring files between cloud storage environments with robust error management and cleanup procedures. Create a workflow that shows how to programmatically manage cloud storage resources and perform reliable file transfers using Google Cloud Transfer Service.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'google_cloud_transfer_dag',\n    default_args=default_args,\n    description='Transfer files from one Google Cloud Storage bucket to another using Google Cloud Transfer Service',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define the source and destination buckets\nsource_bucket = 'your-source-bucket'\ndestination_bucket = 'your-destination-bucket'\n\n# Define the transfer configuration\ntransfer_config = {\n    'description': 'Example transfer configuration',\n    'source_bucket_name': source_bucket,\n    'destination_bucket_name': destination_bucket,\n    'job_name_prefix': 'example-transfer-job',\n    'file_types': ['*'],\n    'schedule_interval': None,  # Set this to your desired schedule interval\n    'status_quo': True,\n    'notification_emails': [],\n    'encryption_key': None,\n    'storage_class': 'STANDARD',\n    'object_conditions': {},\n    'operation_conditions': {}\n}\n\n# Create the transfer operation\ntransfer_operation = GCSToGCSOperator(\n    task_id='transfer_files',\n    source_bucket=source_bucket,\n    destination_bucket=destination_bucket,\n    config=transfer_config,\n    dag=dag\n)\n\n# Define the cleanup task\ncleanup_task = GCSToGCSOperator(\n    task_id='cleanup_transfer_job',\n    source_bucket=source_bucket,\n    destination_bucket=destination_bucket,\n    job_name_prefix=transfer_config['job_name_prefix'],\n    config=transfer_config,\n    dag=dag\n)\n\n# Define the success task\nsuccess_task = GCSToGCSOperator(\n    task_id='success_message',\n    source_bucket=source_bucket,\n    destination_bucket=destination_bucket,\n    job_name_prefix=transfer_config['job_name_prefix'],\n    config=transfer_config,\n    dag=dag\n)\n\n# Define the failure task\nfailure_task = GCSToGCSOperator(\n    task_id='failure_message',\n    source_bucket=source_bucket,\n    destination_bucket=destination_bucket,\n    job_name_prefix=transfer_config['job_name_prefix'],\n    config=transfer_config,\n    dag=dag\n)\n\n# Define the dependencies\ntransfer_operation >> cleanup_task >> success_task >> failure_task"}], "metadata": {"file_name": "dag_1290_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981978.2783945}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data workflow that monitors and validates file uploads to a Google Cloud Storage bucket. Design a process that checks for completed file transfers and triggers subsequent data processing steps based on file arrival and update conditions.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'data_workflow',\n    default_args=default_args,\n    description='A data workflow that monitors and validates file uploads to a Google Cloud Storage bucket.',\n    schedule_interval=timedelta(hours=1),  # Adjust this as needed\n)\n\ndef validate_file_upload(bucket_name, object_name):\n    hook = GoogleCloudStorageHook()\n    try:\n        hook.get_object(Bucket=bucket_name, Name=object_name)\n        print(f\"File {object_name} uploaded successfully.\")\n    except Exception as e:\n        print(f\"Failed to upload file {object_name}: {e}\")\n\ndef process_data(file_path):\n    print(f\"Processing data from {file_path}\")\n    # Add your data processing logic here\n\nvalidate_task = PythonOperator(\n    task_id='validate_file_upload',\n    python_callable=validate_file_upload,\n    op_kwargs={\n        'bucket_name': 'your_bucket_name',  # Replace with your bucket name\n        'object_name': 'path/to/your/file.csv',  # Replace with your file path\n    },\n    dag=dag,\n)\n\nprocess_data_task = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    op_kwargs={\n        'file_path': '/path/to/processed/data.csv',  # Replace with your processed data path\n    },\n    dag=dag,\n)\n\nvalidate_task >> process_data_task"}], "metadata": {"file_name": "dag_1253_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784061}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Life Sciences pipeline that demonstrates basic workflow orchestration using Airflow. Create a DAG that uploads a file to a GCS bucket, runs a simple bash command, and then performs a multi-step pipeline action.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalOperator\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'google_cloud_life_sciences_pipeline',\n    default_args=default_args,\n    description='A Google Cloud Life Sciences Pipeline',\n    schedule_interval=None,\n)\n\n# Upload a file to a GCS bucket\nupload_file_task = GCSToLocalOperator(\n    task_id='upload_file',\n    source_bucket_name='your-source-bucket-name',\n    source_object_name='your-source-object-name',\n    destination_path='/path/to/local/destination/file.txt',\n    dag=dag,\n)\n\n# Run a simple bash command\nbash_command_task = BashOperator(\n    task_id='run_bash_command',\n    bash_command='echo \"Hello from Airflow!\"',\n    dag=dag,\n)\n\n# Perform a multi-step pipeline action (e.g., run a BigQuery query)\nbigquery_hook = BigQueryHook()\nquery_result = bigquery_hook.run_query('SELECT * FROM your-bigquery-table')\n\n# Print the result of the BigQuery query\nprint_bigquery_result_task = PythonOperator(\n    task_id='print_bigquery_result',\n    python_callable=lambda: print(query_result),\n    dag=dag,\n)\n\n# Define dependencies between tasks\nupload_file_task >> bash_command_task >> print_bigquery_result_task"}], "metadata": {"file_name": "dag_228_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784138}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data asset registration and tagging pipeline that enables automated metadata management for cloud storage resources. The pipeline will demonstrate how to dynamically create catalog entries, assign custom tags, and prepare data assets for discovery and governance.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSUploadFileOperator\nfrom airflow.providers.apache.hadoop.hdfs.operators.hdfs import HDFSUploadOperator\nfrom airflow.utils.dates import days_ago\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'data_asset_registration_tagging_pipeline',\n    default_args=default_args,\n    description='A pipeline to register and tag data assets in cloud storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Step 1: Create a bucket in Google Cloud Storage\ncreate_bucket_task = GCSCreateBucketOperator(\n    task_id='create_bucket',\n    bucket_name='my_data_assets_bucket'\n)\n\n# Step 2: Upload a file to Google Cloud Storage\nupload_file_task = GCSUploadFileOperator(\n    task_id='upload_file',\n    bucket_name='my_data_assets_bucket',\n    object_name='example.csv',\n    filepath='/path/to/example.csv'\n)\n\n# Step 3: Submit a Spark job to process the uploaded file\nspark_job_task = SparkSubmitOperator(\n    task_id='spark_job',\n    application='/path/to/spark_script.py',\n    conf={'spark.driver.memory': '4g', 'spark.executor.memory': '8g'},\n    files=['/path/to/example.csv']\n)\n\n# Step 4: Create a dataset in BigQuery\ncreate_dataset_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='my_data_assets_dataset'\n)\n\n# Step 5: Insert data into BigQuery\ninsert_data_task = BigQueryInsertJobOperator(\n    task_id='insert_data',\n    configuration={\n        'load': {\n            'sourceUris': ['gs://my_data_assets_bucket/example.csv'],\n            'destinationTable': {\n                'projectId': 'your_project_id',\n                'datasetId': 'my_data_assets_dataset',\n                'tableId': 'example_table'\n            }\n        }\n    },\n    location='US'\n)\n\n# Step 6: Register the data asset in the Airflow metadata database\nregister_data_asset_task = SnowflakeOperator(\n    task_id='register_data_asset',\n    sql=\"\"\"\n        INSERT INTO my_data_assets_db.my_data_assets (\n            asset_id,\n            asset_type,\n            asset_path,\n            asset_tags\n        )\n        VALUES (\n            'example_asset',\n            'csv',\n            '/path/to/example.csv',\n            '{\"tag1\": \"value1\", \"tag2\": \"value2\"}'\n        );\n    \"\"\"\n)\n\n# Step 7: Tag the data asset in the Airflow metadata database\ntag_data_asset_task = SnowflakeOperator(\n    task_id='tag_data_asset',\n    sql=\"\"\"\n        UPDATE my_data_assets_db.my_data_assets\n        SET asset_tags = '{\"tag1\": \"value1\", \"tag2\": \"value2\"}'\n        WHERE asset_id = 'example_asset';\n    \"\"\"\n)\n\n# Define the order of tasks\ncreate_bucket_task >> upload_file_task >> spark_job_task >> create_dataset_task >> insert_data_task >> register_data_asset_task >> tag_data_asset_task"}], "metadata": {"file_name": "dag_2136_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784264}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a controlled waiting period in a data processing workflow, using an asynchronous time delta sensor to introduce a deliberate delay before task completion. Develop a DAG that shows how time-based synchronization can be achieved without occupying worker slots during the waiting phase.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.time_delta import TimeDeltaSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'time_based_synchronization_dag',\n    default_args=default_args,\n    description='A DAG demonstrating time-based synchronization with an asynchronous time delta sensor.',\n    schedule_interval=None,  # Use this for testing purposes only.\n)\n\n# Task to wait for a specified duration\nwait_for_duration_task = PythonOperator(\n    task_id='wait_for_duration_task',\n    python_callable=lambda: print(\"Waiting for 5 minutes...\"),\n    dag=dag,\n)\n\n# Asynchronous time delta sensor to introduce a delay\nasync_time_delta_sensor = TimeDeltaSensor(\n    task_id='async_time_delta_sensor',\n    delta=timedelta(minutes=5),\n    poke_interval=10,\n    dag=dag,\n)\n\n# Task to execute after the delay\nexecute_after_delay_task = PythonOperator(\n    task_id='execute_after_delay_task',\n    python_callable=lambda: print(\"Task executed after 5 minutes.\"),\n    dag=dag,\n)\n\n# Set up the dependencies\nwait_for_duration_task >> async_time_delta_sensor >> execute_after_delay_task"}], "metadata": {"file_name": "dag_1862_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784343}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain how the Chinese Remainder Theorem (CRT) can be effectively used to solve systems of congruences? Could you also provide a Python code example to demonstrate its application?"}, {"role": "assistant", "content": "import sympy as sp\n\n# Define the moduli and their corresponding remainders\nmoduli = [3, 5, 7]\nremainders = [2, 3, 4]\n\n# Calculate the product of all moduli\nM = sp.prod(moduli)\n\n# Find the modular inverses of the moduli\ninverse_moduli = [sp.mod_inverse(m, M) for m in moduli]\n\n# Construct the solution\nsolution = sum(a * x_i for a, x_i in zip(remainders, inverse_moduli)) % M\n\nprint(solution)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.2784421}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that showcases the sequential steps of interacting with a cloud-based NoSQL database, focusing on transaction management and query preparation. Build a workflow that illustrates the technical mechanics of database interaction in a cloud environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'mongodb_dag',\n    description='A simple DAG to interact with MongoDB',\n    schedule_interval=None,  # Set to None if you want it to run once\n    default_args=default_args\n)\n\n# Task to connect to MongoDB\ndef connect_to_mongodb():\n    from pymongo import MongoClient\n    client = MongoClient('mongodb://localhost:27017/')\n    db = client['test_db']\n    collection = db['test_collection']\n    return client, db, collection\n\nconnect_task = PythonOperator(\n    task_id='connect_to_mongodb',\n    python_callable=connect_to_mongodb,\n    dag=dag\n)\n\n# Task to insert data into MongoDB\ndef insert_data(client, db, collection):\n    data = {'name': 'John Doe', 'age': 30}\n    collection.insert_one(data)\n    print(\"Data inserted successfully\")\n\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    op_kwargs={'client': connect_task.output, 'db': connect_task.output[1], 'collection': connect_task.output[2]},\n    dag=dag\n)\n\n# Task to read data from MongoDB\ndef read_data(client, db, collection):\n    documents = collection.find()\n    for doc in documents:\n        print(doc)\n\nread_data_task = PythonOperator(\n    task_id='read_data',\n    python_callable=read_data,\n    op_kwargs={'client': connect_task.output, 'db': connect_task.output[1], 'collection': connect_task.output[2]},\n    dag=dag\n)\n\n# Task to update data in MongoDB\ndef update_data(client, db, collection):\n    filter_query = {'name': 'John Doe'}\n    update_query = {'$set': {'age': 31}}\n    collection.update_one(filter_query, update_query)\n    print(\"Data updated successfully\")\n\nupdate_data_task = PythonOperator(\n    task_id='update_data',\n    python_callable=update_data,\n    op_kwargs={'client': connect_task.output, 'db': connect_task.output[1], 'collection': connect_task.output[2]},\n    dag=dag\n)\n\n# Task to delete data from MongoDB\ndef delete_data(client, db, collection):\n    filter_query = {'name': 'John Doe'}\n    collection.delete_one(filter_query)\n    print(\"Data deleted successfully\")\n\ndelete_data_task = PythonOperator(\n    task_id='delete_data',\n    python_callable=delete_data,\n    op_kwargs={'client': connect_task.output, 'db': connect_task.output[1], 'collection': connect_task.output[2]},\n    dag=dag\n)\n\n# Task to close the connection\ndef close_connection(client):\n    client.close()\n\nclose_connection_task = PythonOperator(\n    task_id='close_connection',\n    python_callable=close_connection,\n    op_kwargs={'client': connect_task.output},\n    dag=dag\n)\n\n# Define the dependencies between tasks\nconnect_task >> insert_data_task >> read_data_task >> update_data_task >> delete_data_task >> close_connection_task"}], "metadata": {"file_name": "dag_532_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784538}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases the integration between Airflow's task decorators and Snowflake's Snowpark session management, highlighting how to programmatically create a DataFrame and write it to a Snowflake table. Implement a validation task that checks the data integrity of the newly created table.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.decorators import dag, task\nfrom airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.utils.dates import days_ago\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(2),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'snowflake_integration_dag',\n    default_args=default_args,\n    description='DAG to showcase Snowflake integration with Airflow',\n    schedule_interval=None,\n)\n\n# Task to create a DataFrame\n@task(task_id='create_dataframe')\ndef create_dataframe():\n    # Initialize Snowflake hook\n    snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_default')\n    \n    # Create a Snowflake session\n    session = snowflake_hook.get_session()\n    \n    # Execute SQL query to create a temporary table\n    session.execute(\"\"\"\n        CREATE TEMPORARY TABLE IF NOT EXISTS temp_table (\n            id INT,\n            name STRING,\n            age INT\n        )\n    \"\"\")\n    \n    # Close the session\n    session.close()\n\n# Task to write data to Snowflake table\n@task(task_id='write_to_snowflake')\ndef write_to_snowflake():\n    # Initialize Snowflake hook\n    snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_default')\n    \n    # Create a Snowflake session\n    session = snowflake_hook.get_session()\n    \n    # Insert data into the temporary table\n    session.execute(\"\"\"\n        INSERT INTO temp_table (id, name, age) VALUES (1, 'Alice', 30), (2, 'Bob', 25)\n    \"\"\")\n    \n    # Close the session\n    session.close()\n\n# Task to validate data integrity\n@task(task_id='validate_data_integrity')\ndef validate_data_integrity():\n    # Initialize Snowflake hook\n    snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_default')\n    \n    # Create a Snowflake session\n    session = snowflake_hook.get_session()\n    \n    # Query the temporary table to verify data\n    result = session.execute(\"SELECT * FROM temp_table\").fetchall()\n    \n    # Check if the data matches expected values\n    expected_data = [(1, 'Alice', 30), (2, 'Bob', 25)]\n    assert result == expected_data, \"Data integrity check failed\"\n    \n    # Close the session\n    session.close()\n\n# Set up dependencies\ncreate_dataframe >> write_to_snowflake >> validate_data_integrity"}], "metadata": {"file_name": "dag_2206_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784655}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a single-task data transfer mechanism between cloud storage and a distributed query engine. Develop a scheduled job that moves data from GCS to Presto with minimal configuration and maximum reliability.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_presto import GoogleCloudStorageToPrestoOperator\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='gcs_to_presto_dag',\n    default_args=default_args,\n    description='A DAG to move data from GCS to Presto',\n    schedule_interval=timedelta(hours=1),  # Run every hour\n    catchup=False,\n) as dag:\n\n    # Task to copy data from GCS to Presto\n    gcs_to_presto_task = GoogleCloudStorageToPrestoOperator(\n        task_id='gcs_to_presto_task',\n        source_bucket='your-source-bucket-name',\n        source_object='path/to/your/data.csv',\n        destination_table='your-presto-table-name',\n        presto_conn_id='presto_default',  # Replace with your Presto connection ID\n        presto_schema='public',  # Replace with your Presto schema\n        presto_database='your-presto-database'  # Replace with your Presto database\n    )"}], "metadata": {"file_name": "dag_992_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784755}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a demonstration DAG that showcases basic task orchestration and Docker integration, showing how different types of tasks can be sequenced and executed in an Airflow workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.docker.operators.docker_operator import DockerOperator\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'docker_dag',\n    default_args=default_args,\n    description='A simple DAG using Docker containers',\n    schedule_interval=timedelta(minutes=5)\n)\n\n# Define the tasks\ntask1 = DockerOperator(\n    task_id='docker_task1',\n    image='ubuntu:latest',\n    command=['echo', 'Hello from Docker Task 1'],\n    dag=dag\n)\n\ntask2 = DockerOperator(\n    task_id='docker_task2',\n    image='ubuntu:latest',\n    command=['echo', 'Hello from Docker Task 2'],\n    dag=dag\n)\n\ntask3 = DockerOperator(\n    task_id='docker_task3',\n    image='ubuntu:latest',\n    command=['echo', 'Hello from Docker Task 3'],\n    dag=dag\n)\n\n# Set up the dependencies between tasks\ntask1.set_upstream(task2)\ntask2.set_upstream(task3)"}], "metadata": {"file_name": "dag_1563_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981978.2784832}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you design a Python class that includes methods for arithmetic operations and logging? The class should perform addition, subtraction, multiplication, and division, and it should log each operation with a timestamp and the result. The logging should be handled by a separate logger class."}, {"role": "assistant", "content": "import datetime\n\nclass Logger:\n    def __init__(self):\n        self.log_file = \"operation_log.txt\"\n\n    def log_operation(self, operation_type, num1, num2, result):\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        log_entry = f\"{timestamp} - {operation_type}: {num1} {operation_type} {num2} = {result}\\n\"\n        with open(self.log_file, 'a') as file:\n            file.write(log_entry)\n\nclass Calculator:\n    def __init__(self):\n        pass\n\n    def add(self, num1, num2):\n        result = num1 + num2\n        Logger().log_operation(\"addition\", num1, num2, result)\n        return result\n\n    def subtract(self, num1, num2):\n        result = num1 - num2\n        Logger().log_operation(\"subtraction\", num1, num2, result)\n        return result\n\n    def multiply(self, num1, num2):\n        result = num1 * num2\n        Logger().log_operation(\"multiplication\", num1, num2, result)\n        return result\n\n    def divide(self, num1, num2):\n        if num2 == 0:\n            raise ValueError(\"Cannot divide by zero.\")\n        result = num1 / num2\n        Logger().log_operation(\"division\", num1, num2, result)\n        return result\n\n# Example usage\ncalc = Calculator()\nprint(calc.add(5, 3))  # Output: 8\nprint(calc.subtract(10, 4))  # Output: 6\nprint(calc.multiply(7, 2))  # Output: 14\nprint(calc.divide(9, 3))  # Output: 3"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.2784903}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to calculate the smallest common multiple (SCM) of two numbers. The function should take two integers as input and return their SCM."}, {"role": "assistant", "content": "import math\n\ndef lcm(a, b):\n    \"\"\"\n    Calculate the smallest common multiple (SCM) of two integers.\n    \n    Parameters:\n    a (int): First integer\n    b (int): Second integer\n    \n    Returns:\n    int: The SCM of a and b\n    \"\"\"\n    # Calculate the GCD using the math.gcd function\n    gcd_value = math.gcd(a, b)\n    \n    # Calculate the LCM using the formula\n    lcm_value = abs(a * b) // gcd_value\n    \n    return lcm_value\n\n# Example usage:\nprint(lcm(4, 6))  # Output: 12\nprint(lcm(21, 6)) # Output: 42"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.2784967}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can I inherit from a class in Python that doesn't have a `__init__` method?"}, {"role": "assistant", "content": "# Define a base class without __init__\nclass BaseClass:\n    pass\n\n# Define a subclass that inherits from BaseClass\nclass SubClass(BaseClass):\n    # No __init__ method defined here\n\n# Create an instance of SubClass\nobj = SubClass()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.278502}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud storage interactions using Azure Data Lake Storage operators. Construct a DAG that demonstrates how to programmatically upload and remove files with explicit dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.adls import AzureDataLakeStorageHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'azure_adls_example',\n    default_args=default_args,\n    description='A simple example of uploading and removing files from Azure ADLS',\n    schedule_interval=None,\n)\n\n# Define the hook\nadls_hook = AzureDataLakeStorageHook(\n    azure_data_lake_conn_id='your_azure_datalake_connection_id'\n)\n\n# Upload a file\nupload_file_task = PythonOperator(\n    task_id='upload_file',\n    python_callable=upload_file_to_adls,\n    op_kwargs={\n        'hook': adls_hook,\n        'container_name': 'your_container_name',\n        'file_path': 'path/to/your/file.txt',\n        'local_file_path': '/path/to/local/file.txt'\n    },\n    dag=dag\n)\n\n# Remove a file\nremove_file_task = PythonOperator(\n    task_id='remove_file',\n    python_callable=remove_file_from_adls,\n    op_kwargs={\n        'hook': adls_hook,\n        'container_name': 'your_container_name',\n        'file_path': 'path/to/your/file.txt'\n    },\n    dag=dag\n)\n\n# Set up dependencies\nupload_file_task.set_upstream(remove_file_task)"}], "metadata": {"file_name": "dag_2151_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765981978.2785113}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed command execution workflow that leverages WinRM to run system-level commands across remote Windows environments, highlighting inter-task dependencies and parallel processing capabilities. Create a DAG that simulates real-world remote system management and monitoring scenarios.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'winrm_command_execution_dag',\n    default_args=default_args,\n    description='A distributed command execution workflow using WinRM',\n    schedule_interval=None,\n)\n\ndef execute_command(host, command):\n    hook = WinRMHook(winrm_conn_id='winrm_default')\n    result = hook.run(command, host=host)\n    return result.stdout.decode('utf-8')\n\n# Define the list of hosts and commands\nhosts = ['remote_host_1', 'remote_host_2', 'remote_host_3']\ncommands = ['dir', 'get-process']\n\n# Create tasks for each host and command pair\nfor host, command in zip(hosts, commands):\n    task = PythonOperator(\n        task_id=f'execute_command_{host}',\n        python_callable=execute_command,\n        op_kwargs={'host': host, 'command': command},\n        dag=dag,\n    )"}], "metadata": {"file_name": "dag_1761_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981978.27852}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and XCom communication techniques using both Python and Bash operators. Create a workflow that shows how different tasks can push and pull data between each other using Airflow's XCom mechanism.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-04-01',\n    'retries': 1,\n}\n\ndag = DAG(\n    'xcom_example_dag',\n    default_args=default_args,\n    description='An example DAG demonstrating XCom communication',\n    schedule_interval=None,\n)\n\n# Task A: Push data into XCom\ndef push_data_to_xcom():\n    # Simulate pushing data to XCom\n    xcom_push('my_key', 'Hello from task A!')\n\npush_task = PythonOperator(\n    task_id='push_data_to_xcom',\n    python_callable=push_data_to_xcom,\n    dag=dag,\n)\n\n# Task B: Pull data from XCom\ndef pull_data_from_xcom():\n    # Simulate pulling data from XCom\n    pulled_data = xcom_pull(key='my_key')\n    print(f\"Data pulled from XCom: {pulled_data}\")\n\npull_task = BashOperator(\n    task_id='pull_data_from_xcom',\n    bash_command=f\"xcom_pull -k my_key\",\n    dag=dag,\n)\n\n# Define the dependencies\npush_task >> pull_task"}], "metadata": {"file_name": "dag_1454_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981978.278528}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hi, I have a dataset with numerical features and a target variable that is categorical. I want to perform classification. Can you guide me on how to approach this problem using Python?"}, {"role": "assistant", "content": "pip install pandas scikit-learn matplotlib seaborn"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.2785347}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration workflow that seamlessly moves structured CSV data from cloud storage into a distributed query engine for further analysis. Set up an Airflow DAG that enables efficient, one-time data transfer between cloud storage and a Presto database table.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.presto.hooks.presto import PrestoHook\nimport pandas as pd\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_migration_dag',\n    default_args=default_args,\n    description='Automated data migration from cloud storage to Presto',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to download the CSV file from cloud storage\ndownload_csv_task = PythonOperator(\n    task_id='download_csv',\n    python_callable=download_csv,\n    dag=dag\n)\n\n# Task to load the CSV file into a Presto database table\nload_to_presto_task = PythonOperator(\n    task_id='load_to_presto',\n    python_callable=load_to_presto,\n    dag=dag\n)\n\n# Define the dependencies\ndownload_csv_task >> load_to_presto_task\n\ndef download_csv():\n    # Replace with your actual cloud storage credentials and bucket name\n    cloud_storage_bucket_name = 'your-cloud-storage-bucket-name'\n    csv_file_path = 'path/to/your/csv/file.csv'\n\n    # Download the CSV file from cloud storage\n    # This is a placeholder for actual implementation\n    print(f\"Downloading CSV file from {cloud_storage_bucket_name}/{csv_file_path}\")\n\ndef load_to_presto():\n    # Replace with your actual Presto connection details\n    presto_host = 'your-presto-host'\n    presto_port = 8080\n    presto_user = 'your-presto-user'\n    presto_password = 'your-presto-password'\n    presto_database = 'your-presto-database'\n    presto_table = 'your-presto-table'\n\n    # Load the CSV file into a Presto database table\n    hook = PrestoHook(host=presto_host, port=presto_port, username=presto_user, password=presto_password)\n    \n    # Read the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Convert the DataFrame to a list of tuples\n    rows = [tuple(row) for row in df.values]\n    \n    # Insert the data into the Presto table\n    hook.insert_rows(table=presto_table, rows=rows)\n\n# Run the DAG\nif __name__ == \"__main__\":\n    dag.cli()"}], "metadata": {"file_name": "dag_644_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.2785456}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the differences between `sys.exit()` and `os._exit()` in Python, and provide examples of how to use each?"}, {"role": "assistant", "content": "import sys\n\ndef main():\n    print(\"Starting the program...\")\n    # Simulate some operations\n    if some_condition:\n        sys.exit(1)  # Exit with status code 1 indicating failure\n    else:\n        sys.exit(0)  # Exit with status code 0 indicating success\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.2785516}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated workflow that demonstrates provisioning an Amazon EKS cluster with Fargate compute resources and running a simple test pod. Design a DAG that showcases the end-to-end process of creating, testing, and cleaning up an EKS cluster infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    EC2CreateClusterOperator,\n    EC2DeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EKSCreateFargateProfileOperator,\n    EKSDeployPodOperator,\n    EKSCleanupPodOperator,\n)\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.hooks.eks import EKSHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-04-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\ndag = DAG(\n    'eks_fargate_test_dag',\n    default_args=default_args,\n    description='Automated workflow to provision an EKS cluster with Fargate and run a test pod',\n    schedule_interval=None,\n)\n\n# Create EKS Cluster\ncreate_cluster_task = EC2CreateClusterOperator(\n    task_id='create_eks_cluster',\n    aws_conn_id='aws_default',\n    name='my-eks-cluster',\n    version='1.21',\n    subnets=['subnet-12345678', 'subnet-98765432'],\n    security_groups=['sg-12345678'],\n    role_arn='arn:aws:iam::123456789012:role/eks-fargate-role',\n    fargate_profile_name='my-fargate-profile',\n    tags={'Environment': 'Development'},\n)\n\n# Deploy Test Pod\ndeploy_pod_task = EKSDeployPodOperator(\n    task_id='deploy_test_pod',\n    aws_conn_id='aws_default',\n    cluster_name='my-eks-cluster',\n    namespace='default',\n    pod_spec={\n        'apiVersion': 'v1',\n        'kind': 'Pod',\n        'metadata': {\n            'name': 'test-pod',\n        },\n        'spec': {\n            'containers': [\n                {\n                    'name': 'test-container',\n                    'image': 'nginx:latest',\n                }\n            ]\n        }\n    },\n)\n\n# Cleanup Test Pod\ncleanup_pod_task = EKSCleanupPodOperator(\n    task_id='cleanup_test_pod',\n    aws_conn_id='aws_default',\n    cluster_name='my-eks-cluster',\n    namespace='default',\n    pod_name='test-pod',\n)\n\n# Delete EKS Cluster\ndelete_cluster_task = EC2DeleteClusterOperator(\n    task_id='delete_eks_cluster',\n    aws_conn_id='aws_default',\n    cluster_name='my-eks-cluster',\n)\n\n# Define dependencies\ncreate_cluster_task >> deploy_pod_task >> cleanup_pod_task >> delete_cluster_task"}], "metadata": {"file_name": "dag_1481_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765981978.2785609}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "In Python, how can I ensure that a function is called and its result is checked for a specific condition without using decorators?"}, {"role": "assistant", "content": "def divide(a, b):\n    # Check if the divisor is not zero\n    assert b != 0, \"Cannot divide by zero\"\n    \n    # Perform division\n    return a / b\n\n# Example usage\nresult = divide(10, 2)\nprint(result)  # Output: 5.0\n\ntry:\n    result = divide(10, 0)\nexcept AssertionError as e:\n    print(e)  # Output: Cannot divide by zero"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.278566}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Windows Remote Management (WinRM) task orchestration DAG that demonstrates parallel remote command execution across multiple Windows servers. Create a workflow that showcases executing different shell commands simultaneously and tracking their completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'winrm_parallel_commands',\n    default_args=default_args,\n    description='DAG for executing parallel WinRM commands on multiple Windows servers',\n    schedule_interval=None,\n)\n\n# List of Windows servers\nservers = ['server1', 'server2', 'server3']\n\n# Function to execute a command on a server using WinRM\ndef execute_command(server):\n    hook = WinRMHook(server=server)\n    result = hook.run('echo Hello from {}!'.format(server))\n    return result.stdout.decode()\n\n# Task to execute a command on each server in parallel\ntasks = [PythonOperator(\n    task_id=f'execute_command_{server}',\n    python_callable=execute_command,\n    op_kwargs={'server': server},\n    dag=dag\n) for server in servers]\n\n# Set up dependencies between tasks\nfor i in range(len(tasks)):\n    if i < len(tasks) - 1:\n        tasks[i].set_upstream(tasks[i + 1])"}], "metadata": {"file_name": "dag_628_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.2785735}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Is there any library available in Python for parsing and interpreting natural language instructions for date and time extraction?"}, {"role": "assistant", "content": "import dateparser\n\n# Sample text containing date information\ntext = \"I want to book a flight on June 15th, 2023.\"\n\n# Parse the date from the text\nparsed_date = dateparser.parse(text)\n\nprint(parsed_date)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.278579}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates event-driven message processing using Kafka as a message broker, with tasks that produce sequential numeric data and a sensor that dynamically responds to specific numeric conditions. Implement a real-time event listener that can detect and react to numbers divisible by 3, 5, or 15.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.kafka import KafkaSensor\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'event_driven_message_processing',\n    default_args=default_args,\n    description='A DAG that demonstrates event-driven message processing using Kafka',\n    schedule_interval=None  # Set to None for continuous execution\n)\n\n# Producer Task\ndef producer():\n    for i in range(1, 101):\n        print(f\"Producing number: {i}\")\n        # Simulate producing a message\n        time.sleep(1)\n\nproducer_task = PythonOperator(\n    task_id='producer',\n    python_callable=producer,\n    dag=dag\n)\n\n# Sensor Task\ndef check_number(number):\n    return number % 3 == 0 or number % 5 == 0 or number % 15 == 0\n\nsensor_task = KafkaSensor(\n    task_id='sensor',\n    topic='number_topic',\n    value_fn=lambda x: int(x.decode('utf-8')),\n    poke_interval=10,\n    timeout=60,\n    dag=dag\n)\n\n# Event Listener\ndef event_listener():\n    while True:\n        try:\n            number = sensor_task.get()\n            if check_number(number):\n                print(f\"Number {number} meets the condition!\")\n                # Perform actions here, e.g., send a notification\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n\nevent_listener_task = PythonOperator(\n    task_id='event_listener',\n    python_callable=event_listener,\n    dag=dag\n)\n\n# Define the dependencies between tasks\nproducer_task >> sensor_task >> event_listener_task"}], "metadata": {"file_name": "dag_427_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.2785895}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you describe how to use the `numpy` library in Python to create a random 3x3 matrix and then find its eigenvalues and eigenvectors?"}, {"role": "assistant", "content": "import numpy as np"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.278595}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that represents a lightweight, reproducible method for executing ephemeral computational tasks using Azure Container Instances. Create a workflow that demonstrates how to trigger a container-based job with standardized resource allocation and dynamic naming conventions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.aci import AzureContainerInstanceHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n}\n\ndag = DAG(\n    'azure_aci_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Azure Container Instances',\n    schedule_interval=None,\n)\n\n# Function to create a new ACI instance\ndef create_aci_instance():\n    hook = AzureContainerInstanceHook()\n    # Define the container definition\n    container_definition = {\n        \"name\": f\"task_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n        \"image\": \"ubuntu\",\n        \"resources\": {\n            \"requests\": {\n                \"cpu\": 0.5,\n                \"memoryMiB\": 256\n            },\n            \"limits\": {\n                \"cpu\": 1,\n                \"memoryMiB\": 512\n            }\n        }\n    }\n    # Create the container\n    response = hook.create_container(resource_group_name=\"my-resource-group\", \n                                     aci_name=container_definition[\"name\"], \n                                     container_definition=container_definition)\n    print(f\"Created ACI instance {response.name}\")\n\n# Task to create a new ACI instance\ncreate_aci_task = PythonOperator(\n    task_id='create_aci_task',\n    python_callable=create_aci_instance,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_286_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981978.278605}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer mechanisms, highlighting task chaining, cloud service interactions, and dynamic environment configuration. Develop a systematic workflow that demonstrates creating, transferring, and deleting resources across Google Cloud and AWS infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_s3 import S3ToS3Operator\nfrom airflow.providers.google.cloud.transfers.gcs_to_s3 import GCSBucketToS3Operator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3ObjectToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs import S3BucketToGCSOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_gcs"}], "metadata": {"file_name": "dag_734_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765981978.2786176}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated data infrastructure pipeline that demonstrates database provisioning, configuration, and data migration strategies for a cloud-based database environment. Develop a systematic approach to managing database resources with reproducible and configurable deployment steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.hooks.rds import RDSHook\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator\nfrom airflow.providers.amazon.aws.operators.rds import RDSCreateDBInstanceOperator, RDSModifyDBInstanceOperator\nfrom airflow.providers.amazon.aws.operators.redshift import RedshiftCopyTableOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'database_migration_dag',\n    default_args=default_args,\n    description='Automate database provisioning, configuration, and data migration',\n    schedule_interval=None,\n)\n\n# Step 1: Create an EC2 instance for RDS\ncreate_rds_instance = RDSCreateDBInstanceOperator(\n    task_id='create_rds_instance',\n    db_name='mydb',\n    master_username='admin',\n    master_password='password',\n    engine='mysql',\n    instance_class='db.t2.micro',\n    availability_zone='us-west-2a',\n    port=3306,\n    dag=dag,\n)\n\n# Step 2: Wait until the RDS instance is available\nwait_for_rds_instance = S3KeySensor(\n    task_id='wait_for_rds_instance',\n    bucket_name='mybucket',\n    key='rds-instance-available.txt',\n    poke_interval=60,\n    dag=dag,\n)\n\n# Step 3: Create a bucket for storing data\ncreate_s3_bucket = S3CreateBucketOperator(\n    task_id='create_s3_bucket',\n    bucket_name='mybucket',\n    acl='public-read-write',\n    dag=dag,\n)\n\n# Step 4: Copy data from source database to S3\ncopy_data_to_s3 = RedshiftCopyTableOperator(\n    task_id='copy_data_to_s3',\n    table_name='source_table',\n    s3_bucket='mybucket',\n    s3_key='data.csv',\n    copy_options='CSV HEADER',\n    dag=dag,\n)\n\n# Step 5: Modify the RDS instance to use the new bucket\nmodify_rds_instance = RDSModifyDBInstanceOperator(\n    task_id='modify_rds_instance',\n    db_instance_identifier='mydb',\n    new_db_cluster_identifier='mydb-new',\n    new_vpc_security_group_ids=['sg-1234567890abcdef'],\n    new_subnet_group_name='mysubnetgroup',\n    new_storage_type='gp2',\n    new_allocated_storage=20,\n    dag=dag,\n)\n\n# Step 6: Wait until the RDS instance is modified\nwait_for_rds_modification = S3KeySensor(\n    task_id='wait_for_rds_modification',\n    bucket_name='mybucket',\n    key='rds-instance-modified.txt',\n    poke_interval=60,\n    dag=dag,\n)\n\n# Step 7: Delete the old bucket\ndelete_old_bucket = S3DeleteBucketOperator(\n    task_id='delete_old_bucket',\n    bucket_name='mybucket-old',\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_492_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.278627}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I write a Python function that takes a list of integers and returns a new list with each element incremented by 2?"}, {"role": "assistant", "content": "def increment_elements_by_two(numbers):\n    \"\"\"\n    This function takes a list of integers and returns a new list where each element is incremented by 2.\n    \n    Parameters:\n    numbers (list): A list of integers.\n    \n    Returns:\n    list: A new list with each element incremented by 2.\n    \"\"\"\n    # Using list comprehension to increment each element by 2\n    return [num + 2 for num in numbers]\n\n# Example usage:\noriginal_list = [1, 2, 3, 4, 5]\nincremented_list = increment_elements_by_two(original_list)\nprint(incremented_list)  # Output: [3, 4, 5, 6, 7]"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765981978.278632}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud service integration by creating a temporary GCP connection, uploading files to a Google Cloud Storage bucket, and then transferring those files to Google Drive using different configuration options. Implement a workflow that demonstrates file transfer techniques across cloud platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gdrive import GCSToGoogleDriveOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_drive import GoogleCloudStorageToGoogleDriveOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\ndag = DAG(\n    'gcp_cloud_service_dag',\n    default_args=default_args,\n    description='DAG for integrating with Google Cloud services',\n    schedule_interval=None,\n)\n\n# Create a temporary GCP connection\ncreate_connection_task = BashOperator(\n    task_id='create_connection',\n    bash_command='gcloud config set project YOUR_PROJECT_ID'\n)\n\n# Upload local files to Google Cloud Storage\nupload_files_task = LocalFilesystemToGCSOperator(\n    task_id='upload_files',\n    src_dir='/path/to/local/files',\n    dst_bucket='YOUR_BUCKET_NAME',\n    dst_path_prefix='local_files/'\n)\n\n# Transfer files from Google Cloud Storage to Google Drive\ntransfer_files_task = GoogleCloudStorageToGoogleDriveOperator(\n    task_id='transfer_files',\n    source_bucket='YOUR_BUCKET_NAME',\n    destination_folder='drive_files/',\n    source_objects=['local_files/*'],\n    drive_file_name_template='{filename}',\n    impersonate_user='YOUR_SERVICE_ACCOUNT_EMAIL'\n)\n\n# Define the order of tasks\ncreate_connection_task >> upload_files_task >> transfer_files_task"}], "metadata": {"file_name": "dag_541_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765981978.27864}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that transforms raw JSON employee data into a structured Parquet format for improved query performance and storage efficiency. Create an Airflow task that uses Apache Drill to perform the data conversion and table management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.apache.drill.operators.drill import DrillOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'employee_data_pipeline',\n    default_args=default_args,\n    description='A data engineering pipeline to transform raw JSON employee data into a structured Parquet format.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to load raw JSON data from S3\nload_json_data_task = DrillOperator(\n    task_id='load_json_data',\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS employees (\n            id INT,\n            name STRING,\n            department STRING,\n            salary DECIMAL(10, 2)\n        ) USING json LOCATION '/path/to/your/raw/json/data';\n    \"\"\",\n    drill_conn_id='drill_default'\n)\n\n# Task to convert JSON data to Parquet format using Drill\nconvert_to_parquet_task = DrillOperator(\n    task_id='convert_to_parquet',\n    sql=\"\"\"\n        ALTER TABLE employees SET TBLPROPERTIES (format='parquet');\n    \"\"\",\n    drill_conn_id='drill_default'\n)\n\n# Task to verify the Parquet file has been created\nverify_parquet_file_task = DrillOperator(\n    task_id='verify_parquet_file',\n    sql=\"\"\"\n        SELECT COUNT(*) FROM employees;\n    \"\"\",\n    drill_conn_id='drill_default',\n    do_xcom_push=True\n)\n\n# Define dependencies between tasks\nload_json_data_task >> convert_to_parquet_task >> verify_parquet_file_task"}], "metadata": {"file_name": "dag_105_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765981978.2786486}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that monitors a CosmosDB collection for a specific document and then inserts a new document when the condition is met. Design a workflow that demonstrates document tracking and insertion in a cloud-based NoSQL database environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\nimport json\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'cosmosdb_monitor_and_insert',\n    default_args=default_args,\n    description='DAG to monitor CosmosDB for a specific document and insert a new one if found',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Define the CosmosDB connection details\nconnection_string = \"your_cosmosdb_connection_string\"\ndatabase_name = \"your_database_name\"\ncollection_name = \"your_collection_name\"\ndocument_id = \"specific_document_id\"\n\ndef check_document_exists():\n    # Hook to connect to CosmosDB\n    hook = MsSqlHook(mssql_conn_id=\"mssql_default\")\n    \n    # Query to check if the document exists\n    query = f\"SELECT * FROM {collection_name} WHERE id = '{document_id}'\"\n    result = hook.get_pandas_df(query=query)\n    \n    return not result.empty\n\ndef insert_new_document():\n    # Hook to connect to CosmosDB\n    hook = MsSqlHook(mssql_conn_id=\"mssql_default\")\n    \n    # Data to be inserted\n    new_document = {\n        \"id\": document_id,\n        \"name\": \"New Document\",\n        \"value\": \"This is a new document.\"\n    }\n    \n    # Insert the new document\n    hook.create_item(collection_name, new_document)\n\nwith dag as dag:\n    # Check if the document exists\n    check_task = PythonOperator(\n        task_id='check_document_exists',\n        python_callable=check_document_exists,\n        dag=dag\n    )\n    \n    # Insert the new document if it doesn't exist\n    insert_task = PythonOperator(\n        task_id='insert_new_document',\n        python_callable=insert_new_document,\n        dag=dag\n    )\n    \n    # Set the order of tasks\n    check_task >> insert_task"}], "metadata": {"file_name": "dag_1745_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982020.0857177}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated machine learning pipeline for sales forecasting using Google Vertex AI that transforms raw sales data into predictive models. Create a workflow that can ingest time-series data, train a forecasting model, and generate batch predictions for business planning.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.operators.ml_engine_operator import GoogleCloudMLTrainingOperator\nfrom airflow.providers.google.cloud.operators.ml_engine_operator import GoogleCloudMLPredictionOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'SalesForecastingDAG',\n    default_args=default_args,\n    description='A simple DAG for training and predicting sales data using Google Cloud ML Engine',\n    schedule_interval='@daily'\n)\n\n# Define the input and output paths\ninput_bucket = \"your-input-bucket\"\noutput_bucket = \"your-output-bucket\"\n\n# Step 1: Load data from GCS\nload_data_task = PythonOperator(\n    task_id='LoadData',\n    python_callable=load_sales_data,\n    op_kwargs={\n        'bucket': input_bucket,\n        'prefix': 'sales_data.csv'\n    },\n    dag=dag\n)\n\n# Step 2: Train a linear regression model\ntrain_model_task = GoogleCloudMLTrainingOperator(\n    task_id='TrainModel',\n    project='your-project-id',\n    region='us-central1',\n    job_dir=f'gs://{output_bucket}/models',\n    package_uris=['gs://your-gcs-path/to/linear_regression_model.zip'],\n    job_name='LinearRegressionJob',\n    scale_tier='BASIC',\n    hyperparameters={\n        'learning_rate': 0.01,\n        'hidden_units': [64, 32]\n    }\n)\n\n# Step 3: Predict using the trained model\npredict_task = GoogleCloudMLPredictionOperator(\n    task_id='Predict',\n    project='your-project-id',\n    region='us-central1',\n    model_name='your-model-name',\n    instances=[{'features': [[1, 2, 3], [4, 5, 6]]],\n    payload_format='JSONLINES',\n    gcp_conn_id='gcloud_default'\n)\n\n# Define the order of tasks\nload_data_task >> train_model_task >> predict_task"}], "metadata": {"file_name": "dag_932_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.0857394}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data catalog management workflow in Google Cloud Dataplex that demonstrates creating and managing metadata catalog entries, entry types, and aspect types. Design a DAG that showcases the full lifecycle of metadata catalog objects from creation to potential deletion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\n\n# Define the DAG properties\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_catalog_management',\n    default_args=default_args,\n    description='A DAG for managing data catalog entries in Google Cloud Dataplex',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to create a data catalog\ncreate_data_catalog_task = PythonOperator(\n    task_id='create_data_catalog',\n    python_callable=create_data_catalog,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'location': 'us-central1'\n    }\n)\n\n# Task to add entries\nadd_entry_task = PythonOperator(\n    task_id='add_entries',\n    python_callable=add_entries,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'location': 'us-central1',\n        'catalog_name': 'your-catalog-name',\n        'entry_type': 'TABLE',\n        'entry_name': 'your-entry-name',\n        'description': 'This is a sample table entry.'\n    }\n)\n\n# Task to update entries\nupdate_entry_task = PythonOperator(\n    task_id='update_entries',\n    python_callable=update_entries,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'location': 'us-central1',\n        'catalog_name': 'your-catalog-name',\n        'entry_name': 'your-entry-name',\n        'new_description': 'Updated description for the entry.'\n    }\n)\n\n# Task to list entries\nlist_entries_task = PythonOperator(\n    task_id='list_entries',\n    python_callable=list_entries,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'location': 'us-central1',\n        'catalog_name': 'your-catalog-name'\n    }\n)\n\n# Task to delete entries\ndelete_entry_task = PythonOperator(\n    task_id='delete_entries',\n    python_callable=delete_entries,\n    dag=dag,\n    op_kwargs={\n        'project_id': 'your-project-id',\n        'location': 'us-central1',\n        'catalog_name': 'your-catalog-name',\n        'entry_name': 'your-entry-name'\n    }\n)\n\n# Define dependencies\ncreate_data_catalog_task >> add_entry_task >> update_entry_task >> list_entries_task >> delete_entry_task"}], "metadata": {"file_name": "dag_1222_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982020.0857532}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that retrieves the current server's external IP address and sends an email notification about the IP. Design a workflow that automatically tracks and communicates server connection details.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.http_operator import SimpleHttpOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'ServerIPDAG',\n    default_args=default_args,\n    description='A DAG to retrieve server IP and send an email',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Define the task to get the server's external IP address\nget_server_ip = SimpleHttpOperator(\n    task_id='Get Server IP',\n    http_conn_id='http_default',\n    endpoint='/api/v1/server/ip',\n    method='GET',\n    do_xcom_push=True,\n    dag=dag\n)\n\n# Define the task to send an email with the server's IP\nsend_email = EmailOperator(\n    task_id='Send Email',\n    smtp_host='smtp.example.com',\n    smtp_port=587,\n    smtp_username='your-email@example.com',\n    smtp_password='your-password',\n    to_emails=['recipient@example.com'],\n    subject='Server IP Notification',\n    html_content=f'The current server IP is: {get_server_ip.output}',\n    dag=dag\n)\n\n# Set up the dependencies\nget_server_ip >> send_email"}], "metadata": {"file_name": "dag_673_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.0857618}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to find the maximum possible sum of a subarray within a given array of integers. The array can contain both positive and negative numbers."}, {"role": "assistant", "content": "def max_subarray_sum(arr):\n    \"\"\"\n    Finds the maximum sum of a contiguous subarray within a given array of integers.\n    \n    Args:\n    arr (list): A list of integers.\n    \n    Returns:\n    int: The maximum sum of a contiguous subarray.\n    \"\"\"\n    if not arr:\n        return 0\n    \n    # Initialize variables to keep track of the current sum and the maximum sum found so far\n    current_sum = max_sum = arr[0]\n    \n    # Iterate through the array starting from the second element\n    for num in arr[1:]:\n        # Decide whether to start a new subarray or continue the existing one\n        current_sum = max(num, current_sum + num)\n        \n        # Update the maximum sum if the current sum is greater than the maximum sum found so far\n        max_sum = max(max_sum, current_sum)\n    \n    return max_sum\n\n# Example usage:\narr = [-2, -3, 4, -1, -5, 6]\nprint(max_subarray_sum(arr))  # Output: 8"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.08577}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that automates the process of preparing, training, and managing a text classification dataset using cloud-based machine learning infrastructure. Create a systematic workflow that handles data synchronization, dataset import, model training, and resource cleanup in a single, orchestrated pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.ml_engine.operators.mlengine import MLEngineTrainingOperator\nfrom airflow.providers.google.cloud.bigquery.operators.bigquery import BigQueryCreateDatasetOperator, BigQueryInsertJobOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'text_classification_dag',\n    default_args=default_args,\n    description='A DAG for text classification using Google Cloud services',\n    schedule_interval=None,  # Set to your desired schedule interval\n)\n\n# Step 1: Synchronize data from GCS to another GCS bucket\nsync_data_task = GCSToGCSOperator(\n    task_id='sync_data',\n    source_bucket='source-bucket-name',\n    destination_bucket='destination-bucket-name',\n    source_objects=['path/to/source/data/*'],\n    destination_object_prefix='processed-data/',\n    dag=dag,\n)\n\n# Step 2: Import data into BigQuery\nimport_data_task = BigQueryCreateDatasetOperator(\n    task_id='create_dataset',\n    dataset_id='text_classification_dataset',\n    location='us-central1',  # Replace with your preferred region\n    dag=dag,\n)\n\nimport_data_task >> sync_data_task\n\n# Step 3: Train the model using Google Cloud ML Engine\ntrain_model_task = MLEngineTrainingOperator(\n    task_id='train_model',\n    project='your-project-id',\n    job_id='text-classification-job',\n    package_uris=['gs://your-gcs-bucket/your-model-package.tar.gz'],  # Replace with your model package URI\n    python_package='your-python-package-name',  # Replace with your Python package name\n    runtime_version='2.7',  # Replace with your required runtime version\n    framework_version='tensorflow',  # Replace with your required framework version\n    region='us-central1',  # Replace with your preferred region\n    scale_tier='BASIC',  # Replace with your preferred scale tier\n    hyperparameters={\n        'learning_rate': 0.001,\n        'batch_size': 64,\n    },\n    labels={'environment': 'production'},\n    dag=dag,\n)\n\ntrain_model_task >> import_data_task\n\n# Step 4: Clean up resources (optional)\ncleanup_task = PythonOperator(\n    task_id='cleanup_resources',\n    python_callable=cleanup_function,\n    dag=dag,\n)\n\ndef cleanup_function():\n    # Add your cleanup logic here\n    print(\"Cleaning up resources...\")\n\ncleanup_task >> train_model_task"}], "metadata": {"file_name": "dag_927_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.085793}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Please write a Python function that takes a list of lists (a 2D list) and returns the longest sub-list in terms of the number of elements. If there are multiple sub-lists with the same maximum length, the function should return the first one encountered. For example, if the input is `[[1, 2, 3], [4, 5], [6, 7, 8, 9], [10, 11, 12]]`, the function should return `[6, 7, 8, 9]`."}, {"role": "assistant", "content": "def find_longest_sublist(list_of_lists):\n    \"\"\"\n    Finds the longest sublist within a given 2D list.\n    \n    Parameters:\n    - list_of_lists: A list containing other lists as its elements.\n    \n    Returns:\n    - The longest sublist found in the input list. If there are multiple sublists\n      with the same maximum length, the first one encountered is returned.\n    \"\"\"\n    # Initialize variables to keep track of the longest sublist\n    max_length = 0\n    longest_sublist = []\n    \n    # Iterate through each sublist in the main list\n    for sublist in list_of_lists:\n        # Update the longest sublist if the current sublist is longer\n        if len(sublist) > max_length:\n            max_length = len(sublist)\n            longest_sublist = sublist\n    \n    return longest_sublist\n\n# Example usage\ninput_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9], [10, 11, 12]]\nprint(find_longest_sublist(input_list))  # Output: [6, 7, 8, 9]"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0858}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow workflow that showcases asynchronous batch processing using OpenAI's API, with dynamic task generation for multiple Pok\u00e9mon descriptions and deferred execution. Implement a scalable pattern for sending multiple AI generation requests and managing the batch processing lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndef generate_pokemon_description(pokemon_name):\n    # Simulate generating a Pok\u00e9mon description using OpenAI's GPT-4 API\n    import openai\n    openai.api_key = \"YOUR_OPENAI_API_KEY\"\n    \n    response = openai.Completion.create(\n        engine=\"text-davinci-instruct\",\n        prompt=f\"Generate a detailed description of {pokemon_name}.\",\n        max_tokens=2000\n    )\n    \n    return response.choices[0].text\n\nwith DAG(\n    dag_id='async_batch_pokemon_descriptions',\n    default_args=default_args,\n    schedule_interval=timedelta(hours=1),\n) as dag:\n\n    def process_pokemon(pokemon_name):\n        task = PythonOperator(\n            task_id=pokemon_name,\n            python_callable=generate_pokemon_description,\n            op_kwargs={'pokemon_name': pokemon_name},\n            retries=2,\n            retry_delay=timedelta(minutes=5)\n        )\n        return task\n\n    # List of Pok\u00e9mon names\n    pokemon_names = [\"Pikachu\", \"Charmander\", \"Squirtle\", \"Bulbasaur\"]\n\n    # Create tasks for each Pok\u00e9mon name\n    tasks = [process_pokemon(name) for name in pokemon_names]\n\n    # Set up dependencies between tasks\n    for i in range(len(tasks)-1):\n        tasks[i].set_upstream(tasks[i+1])"}], "metadata": {"file_name": "dag_1371_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982020.0858107}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced translation task orchestration using cloud services, with clear task dependencies and resource management. Develop a workflow that creates a temporary bucket, performs translations, and then cleanly deletes the bucket after processing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.google.cloud.translations import GoogleCloudTranslateOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'translate_text_dag',\n    default_args=default_args,\n    description='A simple DAG to translate text using Google Cloud Translate API',\n    schedule_interval=None,\n)\n\n# Create a temporary bucket\ncreate_bucket_task = DummyOperator(\n    task_id='create_bucket',\n    dag=dag,\n    python_callable=create_bucket,\n    op_kwargs={\n        'bucket_name': 'temp-translation-bucket'\n    }\n)\n\n# Define the source and target languages\nsource_language = 'en'  # English\ntarget_languages = ['es', 'fr', 'de']  # Spanish, French, German\n\n# Perform translations\nfor lang in target_languages:\n    translate_task = GoogleCloudTranslateOperator(\n        task_id=f'translate_{lang}',\n        dag=dag,\n        project_id='your-project-id',  # Replace with your GCP project ID\n        source_language=source_language,\n        target_language=lang,\n        source_text='Hello, world!',\n        destination_language=lang,\n        output_format='text',\n        model='nmt',\n        credentials_path='/path/to/your/service-account-key.json'  # Path to your service account key file\n    )\n\n# Set up dependencies\ncreate_bucket_task >> translate_task\n\n# Delete the bucket after all translations are completed\ndelete_bucket_task = DummyOperator(\n    task_id='delete_bucket',\n    dag=dag,\n    python_callable=delete_bucket,\n    op_kwargs={\n        'bucket_name': 'temp-translation-bucket'\n    }\n)"}], "metadata": {"file_name": "dag_1698_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982020.085821}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "The user claims there is a 'globalmon' module, but it does not seem to be part of any official Python libraries or widely recognized third-party packages. It could be a custom or private module. How can I help the user determine if they have access to this module and how it can be used?"}, {"role": "assistant", "content": "import importlib.util\n\n# Check if 'globalmon' is installed\nspec = importlib.util.find_spec('globalmon')\nif spec is None:\n    print(\"The 'globalmon' module is not installed.\")\nelse:\n    print(\"The 'globalmon' module is installed.\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0858285}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily automated workflow for deploying and testing containerized applications in Azure Container Instances. Create a DAG that demonstrates launching different container configurations with varying registry sources and volume mounting strategies.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.azure_container_instances import AzureContainerInstancesHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\nwith DAG(\n    dag_id='deploy_and_test_containers',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n) as dag:\n    # Define the Azure Container Instance hook\n    az_cli = AzureContainerInstancesHook()\n\n    # Task to list all running containers\n    list_running_containers = az_cli.list_running_containers()\n\n    # Task to start a new container\n    start_container = az_cli.start_container(\n        resource_group_name='your_resource_group_name',\n        instance_name='your_instance_name',\n        image='your_image_name',\n        ports=[{'container_port': 80}],\n        volumes=[\n            {'name': 'my_volume', 'source_container_path': '/app/data'}\n        ]\n    )\n\n    # Task to stop a specific container\n    stop_container = az_cli.stop_container(\n        resource_group_name='your_resource_group_name',\n        instance_name='your_instance_name',\n        container_name='your_container_name'\n    )\n\n    # Task to delete a specific container\n    delete_container = az_cli.delete_container(\n        resource_group_name='your_resource_group_name',\n        instance_name='your_instance_name',\n        container_name='your_container_name'\n    )"}], "metadata": {"file_name": "dag_618_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982020.085839}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that calculates the sum of the squares of the first n odd natural numbers."}, {"role": "assistant", "content": "def sum_of_squares(n):\n    \"\"\"\n    Calculate the sum of the squares of the first n odd natural numbers.\n    \n    Args:\n    n (int): The number of initial odd natural numbers to consider.\n    \n    Returns:\n    int: The sum of the squares of the first n odd natural numbers.\n    \"\"\"\n    # Formula to calculate the sum of squares of the first n odd natural numbers\n    return n * (2*n - 1) * (2*n + 1) // 3\n\n# Test cases to verify the correctness of the function\nassert sum_of_squares(1) == 1\nassert sum_of_squares(2) == 10\nassert sum_of_squares(3) == 35\nassert sum_of_squares(4) == 84\nassert sum_of_squares(5) == 165"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0858464}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS-integrated Airflow pipeline that showcases sensor functionality for tracking data availability in DynamoDB, including table creation, value checking with different matching strategies, and automatic table deletion. Implement a systematic workflow that tests DynamoDB item value detection using Airflow's task chaining and sensor capabilities.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dynamodb_operator import CreateTableOperator, DeleteTableOperator\nfrom airflow.sensors.dynamodb_sensor import DynamodbKeyExistsSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'dynamodb_pipeline',\n    default_args=default_args,\n    description='A simple AWS DynamoDB pipeline',\n    schedule_interval=None,\n)\n\n# Create DynamoDB table\ncreate_table = CreateTableOperator(\n    task_id='create_dynamodb_table',\n    aws_conn_id='aws_default',\n    table_name='my_dynamodb_table',\n    key_schema=[\n        {'AttributeName': 'id', 'KeyType': 'HASH'},\n        {'AttributeName': 'timestamp', 'KeyType': 'RANGE'}\n    ],\n    attribute_definitions=[\n        {'AttributeName': 'id', 'AttributeType': 'S'},\n        {'AttributeName': 'timestamp', 'AttributeType': 'N'}\n    ],\n    provisioned_throughput={\n        'ReadCapacityUnits': 5,\n        'WriteCapacityUnits': 5\n    }\n)\n\n# Check if the table exists\ntable_exists = DynamodbKeyExistsSensor(\n    task_id='check_table_exists',\n    aws_conn_id='aws_default',\n    table_name='my_dynamodb_table'\n)\n\n# Insert a new item into the table\ninsert_item = DDBPutItemOperator(\n    task_id='insert_item',\n    aws_conn_id='aws_default',\n    table_name='my_dynamodb_table',\n    item={'id': '1', 'timestamp': 1678945678},\n    return_value=True\n)\n\n# Delete the table if it exists\ndelete_table_if_exists = DeleteTableOperator(\n    task_id='delete_table_if_exists',\n    aws_conn_id='aws_default',\n    table_name='my_dynamodb_table',\n    force_delete=True\n)\n\n# Define the dependencies\ncreate_table >> table_exists >> insert_item >> delete_table_if_exists"}], "metadata": {"file_name": "dag_1472_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982020.0858562}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello, how can I create a Python script that can handle HTTP POST requests to receive data and store it in a local JSON file?"}, {"role": "assistant", "content": "import requests\nimport json\n\n# Define the URL where the POST request will be sent\nurl = 'https://api.example.com/data'\n\n# Data to be sent in the POST request\ndata = {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n}\n\n# Send the POST request\nresponse = requests.post(url, json=data)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the JSON response\n    response_data = response.json()\n    \n    # Write the data to a local JSON file\n    with open('received_data.json', 'w') as file:\n        json.dump(response_data, file)\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.085863}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a Python script to check the status of a website?"}, {"role": "assistant", "content": "import requests\nfrom bs4 import BeautifulSoup\n\ndef check_website_status(url):\n    # Send a GET request to the specified URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        print(f\"The website is up and running.\")\n    else:\n        print(f\"The website is down with status code: {response.status_code}\")\n\n# Example usage\ncheck_website_status('https://www.example.com')"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0858696}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-DAG workflow that demonstrates dependency management between two related data processing pipelines, where one DAG triggers and monitors the execution of tasks in another DAG. Create a parent-child DAG relationship that showcases how external task dependencies can be established and monitored in Airflow.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'multi_dag',\n    default_args=default_args,\n    description='A multi-DAG with two related data processing pipelines',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to generate dummy data\ngenerate_data = PythonOperator(\n    task_id='generate_data',\n    python_callable=generate_dummy_data,\n    dag=dag\n)\n\n# Task to process the generated data\nprocess_data = PythonOperator(\n    task_id='process_data',\n    python_callable=process_data,\n    dag=dag,\n    depends_on=[generate_data]\n)\n\ndef generate_dummy_data():\n    print(\"Generating dummy data...\")\n    # Simulate generating dummy data\n    return \"Dummy Data\"\n\ndef process_data(dummy_data):\n    print(f\"Processing {dummy_data}...\")"}], "metadata": {"file_name": "dag_13_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982020.0858772}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I want to create a Python Flask application that allows users to register and log in using their email and password. How can I implement this functionality?"}, {"role": "assistant", "content": "from flask import Flask, request, render_template, redirect, url_for, session\n\n   app = Flask(__name__)\n\n# Define routes for registration and login\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        # Here you would typically save the username and password securely to a database\n        return f\"User {username} registered successfully!\"\n    else:\n        return render_template('register.html')\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        # Here you would typically check the username and password against a database\n        if username == 'admin' and password == 'password':\n            session['logged_in'] = True\n            return \"Logged in successfully!\"\n        else:\n            return \"Invalid credentials\"\n\n# Route to check if the user is logged in\n@app.route('/')\ndef home():\n    if 'logged_in' in session:\n        return \"Welcome back, {}!\".format(session['username'])\n    else:\n        return redirect(url_for('login'))\n\nif __name__ == '__main__':\n    app.run(debug=True)"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0858889}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that monitors a Google Cloud Tasks queue to ensure all tasks have been processed before proceeding with downstream operations. Create an Airflow DAG that checks queue status as a critical checkpoint in a distributed task processing system.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators import BashOperator\nfrom datetime import timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='monitor_google_cloud_tasks_queue',\n    description='DAG to monitor Google Cloud Tasks Queue',\n    default_args=default_args,\n    schedule_interval=timedelta(hours=1),\n) as dag:\n\n    # Task to check if all tasks in the queue are completed\n    check_all_tasks_completed = BashOperator(\n        task_id='check_all_tasks_completed',\n        bash_command='gcloud tasks list --project YOUR_PROJECT_ID --queue YOUR_QUEUE_NAME | grep \"State: DONE\" | wc -l',\n        dag=dag\n    )\n\n    # Task to perform downstream operations after all tasks are completed\n    downstream_task = BashOperator(\n        task_id='downstream_task',\n        bash_command='echo \"All tasks completed. Proceeding with downstream operations.\"',\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_699_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.0858986}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an inter-DAG dependency workflow that demonstrates how two separate Airflow DAGs can synchronize task execution using external task markers and sensors. Create a parent DAG that signals completion and a child DAG that waits for the parent's specific task to finish before proceeding.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\nparent_dag = DAG(\n    dag_id='parent_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1)\n)\n\ndef mark_parent_complete():\n    return \"Parent DAG completed\"\n\nparent_task = PythonOperator(\n    task_id='mark_parent_complete',\n    python_callable=mark_parent_complete,\n    dag=parent_dag\n)"}], "metadata": {"file_name": "dag_674_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.085905}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an end-to-end machine learning workflow for automated model training and deployment using Google Vertex AI. Create a comprehensive data science pipeline that demonstrates training, evaluation, and deployment of machine learning models across different data types and use cases.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "# Import necessary libraries\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\nimport pandas as pd\n\n# Initialize the Google Cloud client\nclient = automl_v1beta1.AutoMlClient()\n\n# Define the project ID and bucket name\nproject_id = 'your-project-id'\nbucket_name = 'your-bucket-name'\n\n# Define the input file path\ninput_file_path = 'gs://{}/your-dataset.csv'.format(bucket_name)\n\n# Define the output file path\noutput_file_path = 'gs://{}/trained-model'.format(bucket_name)\n\n# Train the model\nresponse = client.train_model(\n    parent='projects/{}/locations/us-central1'.format(project_id),\n    model={\n        'display_name': 'My Machine Learning Model',\n        'dataset': {\n            'name': 'projects/{}/datasets/your-dataset-id'.format(project_id)\n        },\n        'training_parameters': {\n            'budget_microseconds': 60000000,\n            'hyperparameters': {\n                'learning_rate': 0.001,\n                'epochs': 10\n            }\n        }\n    }\n)\n\nprint('Training started...')\noperation = response.result()\nprint('Training completed.')\n\n# Evaluate the model\nevaluation = client.evaluate_model(\n    name=operation.name\n)\n\nprint('Evaluation started...')\nevaluation_result = evaluation.result()\nprint('Evaluation completed.')\n\n# Deploy the model\ndeployed_model = client.deploy_model(\n    name=operation.name,\n    deployed_model={\n        'display_name': 'My Deployed Model',\n        'model': {\n            'name': 'projects/{}/models/your-trained-model-id'.format(project_id)\n        },\n        'deployment_config': {\n            'machine_spec': {\n                'machine_type': 'n1-standard-8'\n            }\n        }\n    }\n)\n\nprint('Deployment started...')\ndeployed_model = deployed_model.result()\nprint('Deployment completed.')"}], "metadata": {"file_name": "dag_45_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982020.0859168}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that demonstrates Azure Synapse integration with Apache Airflow, showing how enterprise data orchestration can be achieved through automated pipeline triggering. Implement a simple DAG that connects cloud data processing services with workflow management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.synapse import SynapseHook\nfrom airflow.providers.microsoft.azure.hooks.storage import AzureBlobHook\nfrom airflow.sensors.http import HttpSensor\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'synapse_blob_to_synapse',\n    default_args=default_args,\n    description='A DAG to transfer data from Azure Blob Storage to Azure Synapse',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the source blob sensor\nsource_blob_sensor = HttpSensor(\n    task_id='check_source_blob_exists',\n    http_conn_id='azure_blob_connection',\n    endpoint_url='https://<your-azure-blob-account>.blob.core.windows.net/<your-container>/<your-blob-name>',\n    method='HEAD',\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# Define the destination synapse hook\ndestination_synapse_hook = SynapseHook(\n    server='<your-azure-synapse-server>',\n    username='<your-azure-synapse-username>',\n    password='<your-azure-synapse-password>',\n    database='<your-azure-synapse-database>',\n    schema='<your-azure-synapse-schema>',\n    auth_type='USER',\n    use_encryption=True,\n    encryption_algorithm='AES_256_CBC'\n)\n\n# Define the destination synapse table operator\ndestination_table_operator = PythonOperator(\n    task_id='insert_data_to_synapse',\n    python_callable=insert_data_to_synapse,\n    op_kwargs={\n        'table_name': '<your-azure-synapse-table>'\n    },\n    dag=dag,\n)\n\n# Define the task to copy the blob to the storage\ncopy_blob_task = PythonOperator(\n    task_id='copy_blob_from_storage',\n    python_callable=copy_blob_from_storage,\n    op_kwargs={\n        'container_name': '<your-azure-blob-container>',\n        'blob_name': '<your-blob-name>',\n        'storage_account_name': '<your-azure-storage-account>',\n        'storage_account_key': '<your-azure-storage-key>'\n    },\n    dag=dag,\n)\n\n# Define the task to trigger the Synapse job\ntrigger_synapse_job_task = PythonOperator(\n    task_id='trigger_synapse_job',\n    python_callable=trigger_synapse_job,\n    op_kwargs={\n        'job_name': '<your-azure-synapse-job-name>'\n    },\n    dag=dag,\n)\n\n# Define the task to monitor the status of the Synapse job\nmonitor_synapse_job_status_task = PythonOperator(\n    task_id='monitor_synapse_job_status',\n    python_callable=monitor_synapse_job_status,\n    op_kwargs={\n        'job_name': '<your-azure-synapse-job-name>'\n    },\n    dag=dag,\n)\n\n# Define the task to clean up the temporary files\ncleanup_task = PythonOperator(\n    task_id='cleanup_temp_files',\n    python_callable=cleanup_temp_files,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\nsource_blob_sensor >> copy_blob_task >> trigger_synapse_job_task >> monitor_synapse_job_status_task >> cleanup_task"}], "metadata": {"file_name": "dag_973_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.0859313}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code to extract sentences from a text while keeping only the words that contain only alphabetic characters?"}, {"role": "assistant", "content": "import re\n\ndef extract_alpha_sentences(text):\n    # Use regular expression to find all sentences containing only alphabetic characters\n    alpha_sentences = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    \n    return alpha_sentences\n\n# Example usage:\ntext = \"Hello, world! This is a test sentence with some numbers 123 and special characters @#$%^&*().\"\nprint(extract_alpha_sentences(text))"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0859373}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function that converts a given word to Pig Latin?"}, {"role": "assistant", "content": "def pig_latin(word):\n    \"\"\"\n    Convert a given word to Pig Latin.\n    \n    Parameters:\n    word (str): The word to be converted.\n    \n    Returns:\n    str: The Pig Latin version of the input word.\n    \"\"\"\n    vowels = 'aeiouAEIOU'\n    if word[0] in vowels:\n        return word + 'way'\n    else:\n        first_letter = word[0]\n        rest_of_word = word[1:]\n        return rest_of_word + first_letter + 'ay'\n\n# Example usage:\nprint(pig_latin('hello'))  # Output: \"ellohay\"\nprint(pig_latin('world'))  # Output: \"orldway\""}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0859427}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Asana task management workflow that demonstrates creating, finding, updating, and deleting tasks programmatically using Airflow. Build a DAG that showcases the full lifecycle of task management operations in an automated, sequential manner.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'asana_dag',\n    default_args=default_args,\n    description='A simple DAG for managing tasks in Asana',\n    schedule_interval='@daily'\n)\n\n# Create a task to create a new task in Asana\ncreate_task = PythonOperator(\n    task_id='create_task',\n    python_callable=create_task_in_asana,\n    op_kwargs={\n        'workspace_id': 'YOUR_WORKSPACE_ID',\n        'project_id': 'YOUR_PROJECT_ID',\n        'task_name': 'My New Task',\n        'description': 'This is a new task created via Airflow.'\n    },\n    dag=dag\n)\n\n# Find a task in Asana\nfind_task = PythonOperator(\n    task_id='find_task',\n    python_callable=find_task_in_asana,\n    op_kwargs={\n        'workspace_id': 'YOUR_WORKSPACE_ID',\n        'task_id': 'My New Task'\n    },\n    dag=dag\n)\n\n# Update a task in Asana\nupdate_task = PythonOperator(\n    task_id='update_task',\n    python_callable=update_task_in_asana,\n    op_kwargs={\n        'workspace_id': 'YOUR_WORKSPACE_ID',\n        'task_id': 'My New Task',\n        'new_description': 'Updated description for My New Task.'\n    },\n    dag=dag\n)\n\n# Delete a task from Asana\ndelete_task = PythonOperator(\n    task_id='delete_task',\n    python_callable=delete_task_from_asana,\n    op_kwargs={\n        'workspace_id': 'YOUR_WORKSPACE_ID',\n        'task_id': 'My New Task'\n    },\n    dag=dag\n)\n\n# Define dependencies between tasks\ncreate_task >> find_task >> update_task >> delete_task"}], "metadata": {"file_name": "dag_1950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982020.085953}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello Qwen, how can I retrieve data from a SQL database using Python?"}, {"role": "assistant", "content": "import sqlite3\n\n# Connect to the SQLite database (or create it if it doesn't exist)\nconn = sqlite3.connect('example.db')\n\n# Create a cursor object using the connection\ncursor = conn.cursor()\n\n# Execute a SELECT query\ncursor.execute(\"SELECT * FROM your_table_name\")\n\n# Fetch all rows from the query result\nrows = cursor.fetchall()\n\n# Print each row\nfor row in rows:\n    print(row)\n\n# Close the cursor and connection\ncursor.close()\nconn.close()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0859592}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data privacy workflow that automatically de-identifies sensitive phone numbers in tabular data using Google Cloud's Data Loss Prevention (DLP) service. Create an Airflow pipeline that demonstrates how to mask, template, and optionally reverse sensitive information transformations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from google.cloud import dlp_v2\n\ndef de_identify_phone_numbers(data):\n    # Initialize the DLP client\n    dlp_client = dlp_v2.DlpServiceClient()\n\n    # Define the content item to be inspected\n    content_item = {\n        'text': data,\n        'type_': dlp_v2.ContentItem.Type.TEXT\n    }\n\n    # Configure the inspection job\n    inspect_job_config = {\n        'inspect_config': {\n            'info_types': [\n                {'name': 'PHONE_NUMBER'}\n            ],\n            'include_quote': True,\n            'min_likelihood': dlp_v2.Likelihood.LIKELIHOOD_UNSPECIFIED\n        }\n    }\n\n    # Run the inspection job\n    response = dlp_client.inspect_content(parent='projects/your-project-id', inspect_job=inspect_job_config)\n\n    # Extract and print the results\n    findings = response.result().findings\n    for finding in findings:\n        if finding.info_type.name == 'PHONE_NUMBER':\n            print(f\"Found phone number: {finding.quote}\")"}], "metadata": {"file_name": "dag_503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982020.0859687}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function that checks if a given number is a prime number? If the number is prime, the function should return `True`; otherwise, it should return `False`."}, {"role": "assistant", "content": "def is_prime(n):\n    \"\"\"\n    Check if a number is prime.\n\n    Args:\n    n (int): The number to check.\n\n    Returns:\n    bool: True if the number is prime, False otherwise.\n    \"\"\"\n    # Handle edge cases\n    if n <= 1:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    # Check for factors from 3 up to the square root of n\n    for i in range(3, int(n**0.5) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\n# Example usage\nnumber = 29\nprint(is_prime(number))  # Output: True"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982020.0859745}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the complete lifecycle of a file transfer task, including dynamic environment setup, S3 bucket creation, local file generation, file upload, and subsequent resource cleanup. Implement a systematic approach to demonstrate cloud file transfer best practices.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aws_s3_file_transfer_dag',\n    default_args=default_args,\n    description='A simple DAG for transferring files between AWS S3 buckets.',\n    schedule_interval=None,\n)\n\n# Task to set up the environment\nsetup_environment = DummyOperator(\n    task_id='setup_environment',\n    dag=dag,\n    description='Set up the AWS environment'\n)\n\n# Task to create an S3 bucket\ncreate_bucket = DummyOperator(\n    task_id='create_bucket',\n    dag=dag,\n    description='Create an S3 bucket'\n)\n\n# Task to generate a local file\ngenerate_local_file = DummyOperator(\n    task_id='generate_local_file',\n    dag=dag,\n    description='Generate a local file'\n)\n\n# Task to upload the file to S3\nupload_to_s3 = DummyOperator(\n    task_id='upload_to_s3',\n    dag=dag,\n    description='Upload the file to S3'\n)\n\n# Task to clean up the resources\ncleanup_resources = DummyOperator(\n    task_id='cleanup_resources',\n    dag=dag,\n    description='Clean up the resources'\n)\n\n# Define dependencies\nsetup_environment >> create_bucket >> generate_local_file >> upload_to_s3 >> cleanup_resources"}], "metadata": {"file_name": "dag_1503_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982020.0859816}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated machine learning pipeline for training and deploying a housing price prediction model using Google Vertex AI. Create a workflow that handles dataset creation, model training, versioning, and deployment with robust error handling and cloud infrastructure management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom google.cloud import storage\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom google.cloud import bigquery\nfrom google.cloud import storage\n\n# Define the default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'housing_price_prediction',\n    default_args=default_args,\n    description='A simple example of a housing price prediction pipeline',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task to download the dataset\ndownload_data = PythonOperator(\n    task_id='download_data',\n    python_callable=download_dataset,\n    dag=dag\n)\n\n# Task to preprocess the data\npreprocess_data = PythonOperator(\n    task_id='preprocess_data',\n    python_callable=preprocess_data_function,\n    dag=dag\n)\n\n# Task to split the data into training and testing sets\nsplit_data = PythonOperator(\n    task_id='split_data',\n    python_callable=split_data_function,\n    dag=dag\n)\n\n# Task to train the model\ntrain_model = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model_function,\n    dag=dag\n)\n\n# Task to evaluate the model\nevaluate_model = PythonOperator(\n    task_id='evaluate_model',\n    python_callable=evaluate_model_function,\n    dag=dag\n)\n\n# Task to deploy the model\ndeploy_model = PythonOperator(\n    task_id='deploy_model',\n    python_callable=deploy_model_function,\n    dag=dag\n)\n\n# Task to upload the model to Google Cloud Storage\nupload_model = PythonOperator(\n    task_id='upload_model',\n    python_callable=upload_model_function,\n    dag=dag\n)\n\n# Task to delete the local model directory\ndelete_local_model = PythonOperator(\n    task_id='delete_local_model',\n    python_callable=delete_local_model_function,\n    dag=dag\n)\n\n# Define functions\ndef download_dataset():\n    # Replace with actual dataset download logic\n    print(\"Downloading dataset...\")\n\ndef preprocess_data_function():\n    # Replace with actual preprocessing logic\n    print(\"Preprocessing data...\")\n\ndef split_data_function():\n    # Replace with actual splitting logic\n    print(\"Splitting data...\")\n\ndef train_model_function():\n    # Replace with actual training logic\n    print(\"Training model...\")\n\ndef evaluate_model_function():\n    # Replace with actual evaluation logic\n    print(\"Evaluating model...\")\n\ndef deploy_model_function():\n    # Replace with actual deployment logic\n    print(\"Deploying model...\")\n\ndef upload_model_function():\n    # Replace with actual uploading logic\n    print(\"Uploading model to GCS...\")\n\ndef delete_local_model_function():\n    # Replace with actual deleting logic\n    print(\"Deleting local model directory...\")"}], "metadata": {"file_name": "dag_1718_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982020.0859935}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates remote task scheduling and execution using Azure's cloud infrastructure, showing how to configure a computational job with specific VM specifications and a basic command line task. Illustrate the process of defining a cloud-based computational workflow with precise infrastructure and execution parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.compute import AzureComputeHook\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'azure_compute_example',\n    default_args=default_args,\n    description='A simple example of running a command on an Azure virtual machine',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define a task to create a virtual machine\ncreate_vm_task = AzureComputeHook().run_command(\n    task_id='create_vm',\n    cluster_name='my-vm',\n    vm_size='Standard_DS1_v2',\n    image='Ubuntu_Linux_18_04_LTS',\n    admin_username='adminuser',\n    ssh_private_key_path='/path/to/private/key.pem'\n)\n\n# Define a task to run a command on the virtual machine\nrun_command_task = PythonOperator(\n    task_id='run_command',\n    python_callable=run_command,\n    op_kwargs={'command': 'echo \"Hello from Azure!\"'},\n    dag=dag\n)\n\n# Set up dependencies between tasks\ncreate_vm_task >> run_command_task"}], "metadata": {"file_name": "dag_965_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982020.0860038}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that manages Power BI workspace and dataset operations, focusing on retrieving workspace information and refreshing datasets automatically. Create a workflow that can scan workspace modifications, track dataset refresh status, and handle administrative Power BI tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\nfrom airflow.providers.microsoft.powerbi.hooks.power_bi import PowerBIHook\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'powerbi_dag',\n    default_args=default_args,\n    description='A DAG to manage Power BI workspace and dataset operations',\n    schedule_interval=None,\n)\n\n# Hook instances\nmssql_hook = MsSqlHook(mssql_conn_id='your_mssql_connection')\npowerbi_hook = PowerBIHook(powerbi_conn_id='your_powerbi_connection')\n\ndef get_workspace_info(workspace_name):\n    query = f\"SELECT * FROM powerbi.workspaces WHERE name = '{workspace_name}'\"\n    return mssql_hook.get_pandas_df(query)\n\ndef refresh_dataset(dataset_name):\n    query = f\"UPDATE powerbi.datasets SET last_refreshed_on = GETDATE() WHERE name = '{dataset_name}'\"\n    mssql_hook.run_query(query)\n\ndef create_or_update_workspace(workspace_name):\n    # Implement logic to create or update the workspace\n    pass\n\ndef delete_workspace(workspace_name):\n    query = f\"DELETE FROM powerbi.workspaces WHERE name = '{workspace_name}'\"\n    mssql_hook.run_query(query)\n\ndef create_or_update_dataset(dataset_name):\n    # Implement logic to create or update the dataset\n    pass\n\ndef handle_admin_tasks():\n    # Implement logic to handle administrative tasks like creating or deleting workspaces and datasets\n    pass\n\nwith dag as dag:\n    # Task to retrieve workspace information\n    get_workspace_task = PythonOperator(\n        task_id='get_workspace_info',\n        python_callable=get_workspace_info,\n        op_kwargs={'workspace_name': 'YourWorkspaceName'},\n        dag=dag\n    )\n\n    # Task to refresh datasets\n    refresh_datasets_task = PythonOperator(\n        task_id='refresh_datasets',\n        python_callable=refresh_dataset,\n        op_kwargs={'dataset_name': 'YourDatasetName'},\n        dag=dag\n    )\n\n    # Task to create or update workspace\n    create_or_update_workspace_task = PythonOperator(\n        task_id='create_or_update_workspace',\n        python_callable=create_or_update_workspace,\n        op_kwargs={'workspace_name': 'YourWorkspaceName'},\n        dag=dag\n    )\n\n    # Task to delete workspace\n    delete_workspace_task = PythonOperator(\n        task_id='delete_workspace',\n        python_callable=delete_workspace,\n        op_kwargs={'workspace_name': 'YourWorkspaceName'},\n        dag=dag\n    )\n\n    # Task to handle administrative tasks\n    handle_admin_tasks_task = PythonOperator(\n        task_id='handle_admin_tasks',\n        python_callable=handle_admin_tasks,\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_2163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982020.0860167}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Papermill to dynamically execute a Jupyter notebook with date-based parameters, generating a new output notebook for each workflow run. The pipeline should verify the notebook's execution by checking a specific message generated during the process.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom datetime import timedelta\nfrom airflow.operators.python_operator import PythonOperator\nimport papermill\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'dynamic_notebook_execution',\n    default_args=default_args,\n    description='A DAG to dynamically execute a Jupyter notebook with date-based parameters',\n    schedule_interval=timedelta(days=1)\n)\n\ndef execute_notebook(notebook_path, output_notebook_path):\n    # Execute the notebook using papermill\n    papermill.execute_notebook(notebook_path, output_notebook_path)\n\n# Define the tasks\nfor year in range(2023, 2024):  # Example years to process\n    for month in range(1, 13):  # Example months to process\n        task_name = f'execute_notebook_{year}_{month}'\n        task = PythonOperator(\n            task_id=task_name,\n            python_callable=execute_notebook,\n            op_kwargs={\n                'notebook_path': 'path/to/data_analysis.ipynb',  # Path to your Jupyter notebook\n                'output_notebook_path': f'path/to/output/{year}/{month}/data_analysis_{year}_{month}.ipynb'\n            },\n            dag=dag\n        )"}], "metadata": {"file_name": "dag_1381_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982020.086026}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of a cloud computing resource, showing how to programmatically control a SageMaker notebook instance through different operational states. Illustrate the step-by-step process of resource provisioning, activation, deactivation, and cleanup using AWS SageMaker operators.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n)\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'sagemaker_notebook_management_dag',\n    default_args=default_args,\n    description='DAG to manage a SageMaker Notebook Instance',\n    schedule_interval=timedelta(days=1),\n)\n\n# Step 1: Create a SageMaker Notebook Instance\ncreate_notebook_instance_task = SageMakerCreateNotebookInstanceOperator(\n    task_id='create_notebook_instance',\n    instance_type='ml.t2.micro',\n    role='arn:aws:iam::123456789012:role/AirflowRole',\n    image_name='aws/sagemaker-notebooks:pytorch-training-1.12',\n    name='my-sagemaker-notebook-instance',\n    dag=dag,\n)\n\n# Step 2: Activate the SageMaker Notebook Instance\nactivate_notebook_instance_task = SageMakerHook().run_notebook_instance(\n    notebook_instance_name='my-sagemaker-notebook-instance',\n    state='InService',\n    wait_for_completion=True,\n    max_wait_time=3600,\n    dag=dag,\n)\n\n# Step 3: Deactivate the SageMaker Notebook Instance\ndeactivate_notebook_instance_task = SageMakerHook().run_notebook_instance(\n    notebook_instance_name='my-sagemaker-notebook-instance',\n    state='Stopped',\n    wait_for_completion=True,\n    max_wait_time=3600,\n    dag=dag,\n)\n\n# Step 4: Delete the SageMaker Notebook Instance\ndelete_notebook_instance_task = SageMakerDeleteNotebookInstanceOperator(\n    task_id='delete_notebook_instance',\n    notebook_instance_name='my-sagemaker-notebook-instance',\n    wait_for_completion=True,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_418_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982065.9952207}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates parallel task execution and infrastructure management using Airflow operators for Google Cloud Compute Engine. The DAG will demonstrate creating a VM instance, running parallel SSH commands with different authentication techniques, and then cleaning up the infrastructure.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.compute_engine import (\n    GKEClusterCreateOperator,\n    GKEClusterDeleteOperator,\n    GKENodePoolCreateOperator,\n    GKENodePoolDeleteOperator,\n    GKEPodExecOperator,\n)\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\ndag = DAG(\n    'gke_compute_engine_dag',\n    default_args=default_args,\n    description='A DAG to create a GKE cluster, run parallel SSH commands, and delete the cluster.',\n    schedule_interval=None,\n)\n\n# Create GKE Cluster\ncreate_cluster_operator = GKEClusterCreateOperator(\n    task_id='create_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    num_nodes=3,\n    node_config={\n        'machine_type': 'n1-standard-2',\n        'disk_size_gb': 50,\n    },\n)\n\n# Create Node Pool\ncreate_node_pool_operator = GKENodePoolCreateOperator(\n    task_id='create_gke_node_pool',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    node_pool_name='my-node-pool',\n    machine_type='n1-standard-2',\n    disk_size_gb=50,\n)\n\n# Run SSH Commands in Parallel\nssh_command_1 = \"\"\"\necho \"Hello from first SSH command\"\n\"\"\"\n\nssh_command_2 = \"\"\"\necho \"Hello from second SSH command\"\n\"\"\"\n\nssh_command_3 = \"\"\"\necho \"Hello from third SSH command\"\n\"\"\"\n\nssh_task_1 = GKEPodExecOperator(\n    task_id='ssh_command_1',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    pod_name='my-pod',\n    container_name='my-container',\n    command=['sh', '-c', ssh_command_1],\n    use_sudo=True,\n)\n\nssh_task_2 = GKEPodExecOperator(\n    task_id='ssh_command_2',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    pod_name='my-pod',\n    container_name='my-container',\n    command=['sh', '-c', ssh_command_2],\n    use_sudo=True,\n)\n\nssh_task_3 = GKEPodExecOperator(\n    task_id='ssh_command_3',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    pod_name='my-pod',\n    container_name='my-container',\n    command=['sh', '-c', ssh_command_3],\n    use_sudo=True,\n)\n\n# Delete Node Pool\ndelete_node_pool_operator = GKENodePoolDeleteOperator(\n    task_id='delete_gke_node_pool',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n    node_pool_name='my-node-pool',\n)\n\n# Delete GKE Cluster\ndelete_cluster_operator = GKEClusterDeleteOperator(\n    task_id='delete_gke_cluster',\n    project_id='your-project-id',\n    location='us-central1',\n    cluster_name='my-gke-cluster',\n)\n\n# Define dependencies\ncreate_cluster_operator >> create_node_pool_operator >> [ssh_task_1, ssh_task_2, ssh_task_3] >> delete_node_pool_operator >> delete_cluster_operator"}], "metadata": {"file_name": "dag_2011_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.995234}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data validation pipeline that monitors a specific Cassandra keyspace and table for data availability and record-level conditions. Create a sensor-based workflow that acts as a gatekeeper to ensure data quality and completeness before initiating downstream data transformation or analysis tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.cassandra.hooks.cassandra import CassandraHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.cassandra import CassandraRowSensor\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'data_validation_dag',\n    default_args=default_args,\n    description='Data Validation Pipeline for Cassandra Keyspace and Table',\n    schedule_interval=timedelta(hours=1)\n)\n\n# Cassandra Hook\ncassandra_hook = CassandraHook()\n\n# Sensor to check if the Cassandra keyspace exists\ncheck_keyspace_sensor = CassandraRowSensor(\n    task_id='check_keyspace_exists',\n    cassandra_conn_id='cassandra_default',\n    keyspace_name='your_keyspace_name',\n    query=\"SELECT * FROM system.keyspaces WHERE keyspace_name = %s\",\n    parameters=('your_keyspace_name',),\n    poke_interval=60,\n    timeout=300\n)\n\n# Sensor to check if the Cassandra table exists\ncheck_table_sensor = CassandraRowSensor(\n    task_id='check_table_exists',\n    cassandra_conn_id='cassandra_default',\n    keyspace_name='your_keyspace_name',\n    table_name='your_table_name',\n    query=\"SELECT * FROM information_schema.columns WHERE table_schema = %s AND table_name = %s\",\n    parameters=('your_keyspace_name', 'your_table_name'),\n    poke_interval=60,\n    timeout=300\n)\n\n# Data Validation Task\ndef validate_data():\n    # Example data validation logic\n    try:\n        # Query the Cassandra table\n        rows = cassandra_hook.get_pandas_df('SELECT * FROM your_keyspace_name.your_table_name')\n        \n        # Check if the table is empty\n        if rows.empty:\n            raise ValueError(\"Table is empty\")\n        \n        # Check if all columns exist\n        expected_columns = ['column1', 'column2', 'column3']  # Replace with actual column names\n        if not set(expected_columns).issubset(rows.columns):\n            raise ValueError(\"Missing columns\")\n        \n        print(\"Data validation passed!\")\n    except Exception as e:\n        print(f\"Data validation failed: {e}\")\n\nvalidate_data_task = PythonOperator(\n    task_id='validate_data',\n    python_callable=validate_data,\n    dag=dag\n)\n\n# Dependencies between sensors and data validation task\ncheck_keyspace_sensor >> check_table_sensor >> validate_data_task"}], "metadata": {"file_name": "dag_1935_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.9952478}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates transferring files between Google Drive and Google Cloud Storage, showcasing cross-platform file movement and synchronization capabilities. Create a DAG that illustrates how enterprise data can be seamlessly migrated between different cloud storage platforms.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gdrive import GCSToGoogleDriveOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'transfer_files_between_google_drive_and_gcs',\n    default_args=default_args,\n    description='Transfer files between Google Drive and Google Cloud Storage',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the source and destination paths\nsource_bucket_name = 'source-bucket'\ndestination_bucket_name = 'destination-bucket'\nsource_folder_path = '/path/to/source/folder'\ndestination_folder_path = '/path/to/destination/folder'\n\n# Task to upload files from Google Drive to Google Cloud Storage\nupload_task = GCSToGoogleDriveOperator(\n    task_id='upload_files',\n    source_bucket=source_bucket_name,\n    source_object=source_folder_path,\n    destination_bucket=destination_bucket_name,\n    destination_object=destination_folder_path,\n    gcp_conn_id='google_cloud_default',\n)\n\n# Task to download files from Google Cloud Storage to Google Drive\ndownload_task = GCSToGoogleDriveOperator(\n    task_id='download_files',\n    source_bucket=destination_bucket_name,\n    source_object=destination_folder_path,\n    destination_bucket=source_bucket_name,\n    destination_object=source_folder_path,\n    gcp_conn_id='google_cloud_default',\n)\n\n# Define the order of tasks\nupload_task >> download_task"}], "metadata": {"file_name": "dag_1690_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982065.9952588}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing workflow that transforms raw project metadata stored in S3 into a QuickSight-ready dataset for business intelligence purposes. Create a systematic approach to extract, transfer, and load (ETL) project information with automated provisioning and cleanup of AWS resources.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\ndag = DAG(\n    'project_metadata_to_quicksight',\n    description='Transforms raw project metadata from S3 into a QuickSight-ready dataset',\n    schedule_interval=timedelta(days=1),\n    default_args=default_args,\n    catchup=False\n)\n\n# Step 1: Extract data from S3\nextract_data_task = PythonOperator(\n    task_id='extract_data_from_s3',\n    python_callable=extract_data_from_s3,\n    op_kwargs={\n        'bucket_name': 'your-bucket-name',\n        'file_path': 'path/to/your/project-metadata.csv'\n    },\n    dag=dag\n)\n\n# Step 2: Transform data using Python function\ntransform_data_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    op_kwargs={\n        'input_file_path': '/tmp/project-metadata.csv',\n        'output_file_path': '/tmp/processed-project-metadata.csv'\n    },\n    dag=dag\n)\n\n# Step 3: Load data into PostgreSQL\nload_data_task = PythonOperator(\n    task_id='load_data_into_postgresql',\n    python_callable=load_data_into_postgresql,\n    op_kwargs={\n        'table_name': 'project_metadata',\n        'input_file_path': '/tmp/processed-project-metadata.csv'\n    },\n    dag=dag\n)\n\n# Step 4: Create QuickSight dataset\ncreate_dataset_task = PythonOperator(\n    task_id='create_quicksight_dataset',\n    python_callable=create_quicksight_dataset,\n    op_kwargs={\n        'database_name': 'your-database-name',\n        'schema_name': 'public',\n        'table_name': 'project_metadata'\n    },\n    dag=dag\n)\n\n# Task dependencies\nextract_data_task >> transform_data_task >> load_data_task >> create_dataset_task"}], "metadata": {"file_name": "dag_746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982065.995271}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an enterprise messaging system DAG that illustrates the complete lifecycle of Azure Service Bus communication, including queue and topic management, message transmission strategies, and subscription handling. Develop a technical demonstration of cloud messaging infrastructure using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.hooks.service_bus import ServiceBusHook\nfrom airflow.operators.python_operator import PythonOperator\nimport pandas as pd\n\n# Define the default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='enterprise_messaging_system',\n    description='DAG for managing Azure Service Bus resources',\n    schedule_interval=timedelta(hours=1),\n    default_args=default_args\n) as dag:\n\n    # Function to create a new queue\n    def create_queue(queue_name):\n        hook = ServiceBusHook()\n        hook.create_queue(queue_name)\n\n    # Function to delete a queue\n    def delete_queue(queue_name):\n        hook = ServiceBusHook()\n        hook.delete_queue(queue_name)\n\n    # Function to send a message to a queue\n    def send_message_to_queue(queue_name, message):\n        hook = ServiceBusHook()\n        hook.send_message(queue_name, message)\n\n    # Function to receive messages from a queue\n    def receive_messages_from_queue(queue_name):\n        hook = ServiceBusHook()\n        return hook.receive_messages(queue_name)\n\n    # Function to create a new topic\n    def create_topic(topic_name):\n        hook = ServiceBusHook()\n        hook.create_topic(topic_name)\n\n    # Function to delete a topic\n    def delete_topic(topic_name):\n        hook = ServiceBusHook()\n        hook.delete_topic(topic_name)\n\n    # Function to publish a message to a topic\n    def publish_message_to_topic(topic_name, message):\n        hook = ServiceBusHook()\n        hook.publish_message(topic_name, message)\n\n    # Function to subscribe to a topic\n    def subscribe_to_topic(topic_name, subscription_name):\n        hook = ServiceBusHook()\n        hook.subscribe_to_topic(topic_name, subscription_name)\n\n    # Function to unsubscribe from a topic\n    def unsubscribe_from_topic(topic_name, subscription_name):\n        hook = ServiceBusHook()\n        hook.unsubscribe_from_topic(topic_name, subscription_name)\n\n    # Task to create a new queue\n    create_queue_task = PythonOperator(\n        task_id='create_queue_task',\n        python_callable=create_queue,\n        op_kwargs={'queue_name': 'my_queue'}\n    )\n\n    # Task to delete a queue\n    delete_queue_task = PythonOperator(\n        task_id='delete_queue_task',\n        python_callable=delete_queue,\n        op_kwargs={'queue_name': 'my_queue'}\n    )\n\n    # Task to send a message to a queue\n    send_message_to_queue_task = PythonOperator(\n        task_id='send_message_to_queue_task',\n        python_callable=send_message_to_queue,\n        op_kwargs={'queue_name': 'my_queue', 'message': 'Hello, Azure Service Bus!'}\n    )\n\n    # Task to receive messages from a queue\n    receive_messages_from_queue_task = PythonOperator(\n        task_id='receive_messages_from_queue_task',\n        python_callable=receive_messages_from_queue,\n        op_kwargs={'queue_name': 'my_queue'}\n    )\n\n    # Task to create a new topic\n    create_topic_task = PythonOperator(\n        task_id='create_topic_task',\n        python_callable=create_topic,\n        op_kwargs={'topic_name': 'my_topic'}\n    )\n\n    # Task to delete a topic\n    delete_topic_task = PythonOperator(\n        task_id='delete_topic_task',\n        python_callable=delete_topic,\n        op_kwargs={'topic_name': 'my_topic'}\n    )\n\n    # Task to publish a message to a topic\n    publish_message_to_topic_task = PythonOperator(\n        task_id='publish_message_to_topic_task',\n        python_callable=publish_message_to_topic,\n        op_kwargs={'topic_name': 'my_topic', 'message': 'Hello, Azure Service Bus!'}\n    )\n\n    # Task to subscribe to a topic\n    subscribe_to_topic_task = PythonOperator(\n        task_id='subscribe_to_topic_task',\n        python_callable=subscribe_to_topic,\n        op_kwargs={'topic_name': 'my_topic', 'subscription_name': 'my_subscription'}\n    )\n\n    # Task to unsubscribe from a topic\n    unsubscribe_from_topic_task = PythonOperator(\n        task_id='unsubscribe_from_topic_task',\n        python_callable=unsubscribe_from_topic,\n        op_kwargs={'topic_name': 'my_topic', 'subscription_name': 'my_subscription'}\n    )\n\n    # Task to wait for messages to be received\n    wait_for_messages_task = PythonOperator(\n        task_id='wait_for_messages_task',\n        python_callable=lambda: print(\"Waiting for messages...\"),\n        trigger_rule='none_failed'\n    )\n\n    # Task to clean up resources\n    cleanup_resources_task = PythonOperator(\n        task_id='cleanup_resources_task',\n        python_callable=lambda: print(\"Cleaning up resources...\")\n    )\n\n    # Task dependencies\n    create_queue_task >> delete_queue_task >> send_message_to_queue_task >> receive_messages_from_queue_task >> \\\n        create_topic_task >> delete_topic_task >> publish_message_to_topic_task >> subscribe_to_topic_task >> \\\n        unsubscribe_from_topic_task >> wait_for_messages_task >> cleanup_resources_task"}], "metadata": {"file_name": "dag_1349_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982065.9952893}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a time series forecasting system that processes retail sales data with granular geographic attributes like city, zip code, and county. Develop an automated pipeline that transforms raw sales data into predictive insights with a 30-day forecast horizon.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom prophet import Prophet\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-01-01',\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'retail_sales_forecasting',\n    default_args=default_args,\n    description='A time series forecasting system for retail sales data',\n    schedule_interval=timedelta(days=1),\n)\n\n# Function to load data from PostgreSQL\ndef load_data():\n    hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n    query = \"\"\"\n        SELECT \n            date_column AS ds,\n            city,\n            zip_code,\n            county,\n            sales_amount\n        FROM \n            your_retail_sales_table;\n    \"\"\"\n    df = hook.get_pandas_df(query)\n    return df\n\n# Function to preprocess data\ndef preprocess_data(df):\n    # Example preprocessing steps (e.g., handling missing values, scaling)\n    df.fillna(0, inplace=True)\n    return df\n\n# Function to train Prophet model\ndef train_prophet_model(df):\n    model = Prophet()\n    model.fit(df[['ds', 'sales_amount']])\n    return model\n\n# Function to make predictions\ndef make_predictions(model, future_dates):\n    future = model.make_future_dataframe(periods=30)\n    forecast = model.predict(future)\n    return forecast\n\n# Task to load data\nload_data_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag,\n)\n\n# Task to preprocess data\npreprocess_data_task = PythonOperator(\n    task_id='preprocess_data',\n    python_callable=preprocess_data,\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on_past=False,\n)\n\n# Task to train Prophet model\ntrain_prophet_model_task = PythonOperator(\n    task_id='train_prophet_model',\n    python_callable=train_prophet_model,\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on_past=False,\n    provide_context=True,\n)\n\n# Task to make predictions\nmake_predictions_task = PythonOperator(\n    task_id='make_predictions',\n    python_callable=make_predictions,\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on_past=False,\n    provide_context=True,\n)\n\n# Task to save predictions\nsave_predictions_task = PythonOperator(\n    task_id='save_predictions',\n    python_callable=lambda context: context['task_instance'].xcom_pull(task_ids='make_predictions'),\n    dag=dag,\n    trigger_rule='all_success',\n    depends_on_past=False,\n)\n\n# Dependencies\nload_data_task >> preprocess_data_task >> train_prophet_model_task >> make_predictions_task >> save_predictions_task"}], "metadata": {"file_name": "dag_2113_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.995302}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end infrastructure orchestration DAG that demonstrates the complete lifecycle of an EKS cluster, including creation, validation, and pod deployment. Implement a robust workflow that shows how to programmatically manage cloud infrastructure using Airflow's declarative task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    EC2CreateClusterOperator,\n    EC2WaitForClusterStateOperator,\n)\nfrom airflow.providers.amazon.aws.operators.kubernetes import (\n    KubernetesPodOperator,\n)\nfrom airflow.utils.dates import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n}\n\neks_cluster_name = \"my-eks-cluster\"\npod_name = \"my-pod\"\n\neks_cluster_config = {\n    \"clusterName\": eks_cluster_name,\n    \"version\": \"1.21\",\n    \"securityGroupIds\": [\"sg-1234567890abcdef\"],\n    \"subnetIds\": [\"subnet-1234567890abcdef\"],\n    \"keyPairName\": \"my-key-pair\",\n    \"instanceTypes\": [\"t2.micro\"],\n    \"desiredCapacity\": 1,\n    \"maxSize\": 1,\n    \"minSize\": 1,\n    \"tags\": [\n        {\"Key\": \"Environment\", \"Value\": \"Production\"},\n    ],\n}\n\nkubernetes_pod_spec = {\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"name\": pod_name,\n        \"labels\": {\n            \"app\": \"my-app\",\n        },\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"name\": \"my-container\",\n                \"image\": \"nginx:latest\",\n                \"ports\": [\n                    {\n                        \"containerPort\": 80,\n                    },\n                ],\n            },\n        ],\n    },\n}\n\ndag = DAG(\n    dag_id=\"eks_cluster_dag\",\n    default_args=default_args,\n    schedule_interval=None,\n)\n\ncreate_eks_cluster = EC2CreateClusterOperator(\n    task_id=\"create_eks_cluster\",\n    aws_conn_id=\"aws_default\",\n    cluster_name=eks_cluster_name,\n    cluster_config=eks_cluster_config,\n)\n\nwait_for_eks_cluster_state = EC2WaitForClusterStateOperator(\n    task_id=\"wait_for_eks_cluster_state\",\n    aws_conn_id=\"aws_default\",\n    cluster_name=eks_cluster_name,\n    desired_status=\"running\",\n)\n\ndeploy_kubernetes_pod = KubernetesPodOperator(\n    task_id=\"deploy_kubernetes_pod\",\n    namespace=\"default\",\n    image=kubernetes_pod_spec[\"apiVersion\"] + \"/\" + kubernetes_pod_spec[\"kind\"],\n    command=[\"sh\", \"-c\", \"echo Hello from Kubernetes!\"],\n    labels={\"app\": \"my-app\"},\n    name=pod_name,\n    in_cluster=True,\n    aws_conn_id=\"aws_default\",\n)\n\ncreate_eks_cluster >> wait_for_eks_cluster_state >> deploy_kubernetes_pod"}], "metadata": {"file_name": "dag_732_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953125}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data transformation pipeline that translates multilingual documents across different file formats (XLSX, DOCX, PDF) using cloud translation services. Implement a workflow that handles language conversion, manages input and output configurations, and demonstrates scalable document processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.translations import GoogleCloudTranslationOperator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSEndpointHook\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'translate_documents',\n    default_args=default_args,\n    description='Automated data transformation pipeline for translating multilingual documents',\n    schedule_interval=timedelta(hours=1),\n)\n\n# Define the Google Cloud endpoint hook\ngcs_hook = GCSEndpointHook(project_id='your-project-id')\n\n# Define the source bucket and destination bucket\nsource_bucket = 'your-source-bucket'\ndestination_bucket = 'your-destination-bucket'\n\n# Define the list of input files\ninput_files = [\n    {'file_name': 'document1.xlsx', 'format': 'xlsx'},\n    {'file_name': 'document2.docx', 'format': 'docx'},\n    {'file_name': 'document3.pdf', 'format': 'pdf'}\n]\n\n# Define the target language\ntarget_language = 'en'\n\n# Define the function to translate a single file\ndef translate_file(file_path, format):\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        content = f.read()\n    \n    # Translate the content\n    translated_content = gcs_hook.translate_text(content, target_language=target_language)\n    \n    # Write the translated content to a new file\n    output_file_path = f'translated_{file_path}'\n    with open(output_file_path, 'wb') as f:\n        f.write(translated_content)\n    \n    return output_file_path\n\n# Define the task to translate each input file\nfor file_info in input_files:\n    file_path = f'{source_bucket}/{file_info[\"file_name\"]}.{file_info[\"format\"]}'\n    output_file_path = translate_file(file_path, file_info[\"format\"])\n    \n    # Upload the translated file to the destination bucket\n    gcs_hook.upload_blob(destination_bucket, output_file_path, file_info[\"file_name\"])\n\n# Define the task to delete the original files\ndelete_task = BashOperator(\n    task_id='delete_original_files',\n    bash_command=f'rm -rf {source_bucket}/*',\n    dag=dag\n)"}], "metadata": {"file_name": "dag_1301_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953234}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across multiple AWS services. Develop a sequence of tasks that create security groups, configure database endpoints, and set up a replication instance for seamless data transfer.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.hooks.rds import RDSHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aws_infrastructure_dag',\n    default_args=default_args,\n    description='DAG to orchestrate AWS infrastructure provisioning and data migration',\n    schedule_interval=timedelta(days=1)\n)\n\n# Create Security Group\ncreate_security_group_task = PythonOperator(\n    task_id='create_security_group',\n    python_callable=create_security_group,\n    dag=dag\n)\n\n# Configure Database Endpoint\nconfigure_db_endpoint_task = PythonOperator(\n    task_id='configure_db_endpoint',\n    python_callable=configure_db_endpoint,\n    dag=dag\n)\n\n# Set Up Replication Instance\nset_up_replication_instance_task = PythonOperator(\n    task_id='set_up_replication_instance',\n    python_callable=set_up_replication_instance,\n    dag=dag\n)\n\n# Define Functions\ndef create_security_group():\n    ec2_hook = EC2Hook()\n    response = ec2_hook.create_security_group(\n        GroupName='MySecurityGroup',\n        Description='My Security Group'\n    )\n    print(f\"Created Security Group: {response['GroupId']}\")\n\ndef configure_db_endpoint():\n    rds_hook = RDSHook()\n    response = rds_hook.create_db_instance(\n        DBInstanceIdentifier='mydbinstance',\n        MasterUsername='admin',\n        MasterUserPassword='password',\n        DBInstanceClass='db.t2.micro',\n        Engine='mysql',\n        Port=3306,\n        VpcId='vpc-1234567890abcdef'\n    )\n    print(f\"Created Database Endpoint: {response['DBInstance']['Endpoint']['Address']}\")\n\ndef set_up_replication_instance():\n    rds_hook = RDSHook()\n    response = rds_hook.create_replication_instance(\n        ReplicationInstanceIdentifier='myreplicationinstance',\n        SourceDBInstanceIdentifier='mydbinstance',\n        ReplicationInstanceClass='db.r4.large',\n        AvailabilityZone='us-west-2a',\n        MultiAZ=True\n    )\n    print(f\"Set Up Replication Instance: {response['ReplicationInstance']['Endpoint']['Address']}\")"}], "metadata": {"file_name": "dag_60_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982065.995334}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates the complete lifecycle of a cloud-based distributed computing cluster, highlighting the sequential task dependencies and operational states of a Dataproc cluster. The DAG should illustrate how to programmatically manage cloud infrastructure using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocClusterCreateOperator,\n    DataprocClusterDeleteOperator,\n)\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dataproc_cluster_dag',\n    default_args=default_args,\n    description='DAG for managing Dataproc clusters',\n    schedule_interval=timedelta(days=1),\n)\n\n# Create Dataproc Cluster Operator\ncreate_dataproc_cluster = DataprocClusterCreateOperator(\n    task_id='create_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n    master_config={\n        'machine_type': 'n1-standard-4',\n        'num_instances': 2,\n    },\n    worker_config={\n        'machine_type': 'n1-standard-8',\n        'num_instances': 4,\n    },\n    init_actions_uris=[\n        'gs://path/to/init-script.sh',\n    ],\n    labels={\n        'environment': 'production',\n    },\n)\n\n# Delete Dataproc Cluster Operator\ndelete_dataproc_cluster = DataprocClusterDeleteOperator(\n    task_id='delete_dataproc_cluster',\n    project_id='your-project-id',\n    region='us-central1',\n    cluster_name='my-dataproc-cluster',\n)\n\n# Define task dependencies\ncreate_dataproc_cluster >> delete_dataproc_cluster"}], "metadata": {"file_name": "dag_858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953442}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data streaming pipeline that demonstrates Kafka cluster management and message processing using Google Cloud Managed Kafka. Create a workflow that showcases creating a Kafka cluster, producing messages, and consuming those messages with a specific consumer group.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.kafka import KafkaProducerOperator, KafkaConsumerOperator\nfrom airflow.providers.google.cloud.sensors.kafka import KafkaSensor\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-01-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\ndag = DAG(\n    'google_cloud_kafka_pipeline',\n    default_args=default_args,\n    description='A simple example of a Kafka pipeline in Airflow',\n    schedule_interval=None,\n)\n\n# Step 1: Create a Kafka cluster\ncreate_cluster_task = KafkaProducerOperator(\n    task_id='create_kafka_cluster',\n    bootstrap_servers='kafka.googleapis.com:9092',\n    topic='test-topic',\n    num_partitions=1,\n    num_replicas=1,\n    dag=dag,\n)\n\n# Step 2: Produce messages to the Kafka cluster\nproduce_messages_task = KafkaProducerOperator(\n    task_id='produce_messages',\n    bootstrap_servers='kafka.googleapis.com:9092',\n    topic='test-topic',\n    value='Hello, Kafka!',\n    dag=dag,\n)\n\n# Step 3: Consume messages from the Kafka cluster\nconsume_messages_task = KafkaConsumerOperator(\n    task_id='consume_messages',\n    bootstrap_servers='kafka.googleapis.com:9092',\n    topic='test-topic',\n    group_id='my-consumer-group',\n    auto_offset_reset='earliest',\n    dag=dag,\n)\n\n# Step 4: Check if the messages were consumed successfully\ncheck_consumption_task = KafkaSensor(\n    task_id='check_consumption',\n    bootstrap_servers='kafka.googleapis.com:9092',\n    topic='test-topic',\n    group_id='my-consumer-group',\n    check_mode=True,\n    dag=dag,\n)"}], "metadata": {"file_name": "dag_2082_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.995352}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data migration pipeline that checks for the existence of a file in an Azure container and then transfers that file to a specified Google Cloud Storage location, showcasing cross-cloud data synchronization and movement capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.microsoft.azure.hooks.storage import AzureBlobStorageHook\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_migration_pipeline',\n    default_args=default_args,\n    description='A data migration pipeline from Azure Blob Storage to Google Cloud Storage',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Define variables\nsource_container_name = 'your_source_container'\nsource_blob_name = 'your_source_blob'\ndestination_bucket_name = 'your_destination_bucket'\ndestination_blob_name = 'your_destination_blob'\n\n# Hook for Azure Blob Storage\nazure_storage_hook = AzureBlobStorageHook(\n    azure_conn_id='azure_default',  # Replace with your Azure connection ID\n    resource_group_name='your_resource_group',  # Replace with your resource group name\n    account_name='your_account_name'  # Replace with your Azure storage account name\n)\n\n# Hook for Google Cloud Storage\ngcs_hook = GoogleCloudStorageHook(\n    google_cloud_conn_id='google_cloud_default'  # Replace with your Google Cloud connection ID\n)\n\n# Task to check if the source blob exists\ncheck_blob_exists_task = azure_storage_hook.check_for_blob(\n    container_name=source_container_name,\n    blob_name=source_blob_name,\n    task_id='check_blob_exists'\n)\n\n# Task to copy the blob from Azure to Google Cloud Storage\ncopy_blob_task = GCSToGCSOperator(\n    task_id='copy_blob',\n    source_bucket=source_container_name,\n    source_object=source_blob_name,\n    destination_bucket=destination_bucket_name,\n    destination_object=destination_blob_name,\n    replace=True,\n    gcp_conn_id='google_cloud_default'\n)\n\n# Dependencies between tasks\ncheck_blob_exists_task >> copy_blob_task"}], "metadata": {"file_name": "dag_1176_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Base", "inference_time": 1765982065.995362}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases task dependencies and sequential PostgreSQL operations, illustrating how different database tasks can be chained together with clear dependency management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': '2023-10-01',\n    'retries': 1,\n    'retry_delay': 5\n}\n\n# Create the DAG\ndag = DAG(\n    'postgres_dag',\n    description='A simple DAG to demonstrate PostgreSQL operations',\n    schedule_interval=None,  # Set this to your desired schedule interval\n    default_args=default_args\n)\n\n# Task to create a table in PostgreSQL\ncreate_table_task = PythonOperator(\n    task_id='create_table',\n    python_callable=create_table,\n    dag=dag\n)\n\n# Task to insert data into the table\ninsert_data_task = PythonOperator(\n    task_id='insert_data',\n    python_callable=insert_data,\n    dag=dag\n)\n\n# Task to select data from the table\nselect_data_task = PythonOperator(\n    task_id='select_data',\n    python_callable=select_data,\n    dag=dag\n)\n\n# Define the dependencies between tasks\ncreate_table_task.set_upstream(None)\ninsert_data_task.set_upstream(create_table_task)\nselect_data_task.set_upstream(insert_data_task)\n\ndef create_table():\n    hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n    query = \"\"\"\n        CREATE TABLE IF NOT EXISTS my_table (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            age INT\n        );\n    \"\"\"\n    hook.run(query=query)\n\ndef insert_data():\n    hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n    query = \"\"\"\n        INSERT INTO my_table (name, age) VALUES ('Alice', 30), ('Bob', 25);\n    \"\"\"\n    hook.run(query=query)\n\ndef select_data():\n    hook = PostgresHook(postgres_conn_id='your_postgres_connection')\n    query = \"SELECT * FROM my_table;\"\n    result = hook.get_pandas_df(query=query)\n    print(result)"}], "metadata": {"file_name": "dag_302_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953728}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data governance pipeline that tracks the creation, validation, and cleanup of metadata entries in a systematic workflow. Construct a DAG that simulates real-world metadata management processes with clear create, get, list, and delete operations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'metadata_governance_pipeline',\n    default_args=default_args,\n    description='A simple data governance pipeline for tracking metadata entries',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to create a new metadata entry\ncreate_metadata_task = PythonOperator(\n    task_id='create_metadata_entry',\n    python_callable=create_metadata,\n    dag=dag,\n)\n\n# Task to get a metadata entry by ID\nget_metadata_task = PythonOperator(\n    task_id='get_metadata_entry',\n    python_callable=get_metadata,\n    dag=dag,\n)\n\n# Task to list all metadata entries\nlist_metadata_task = PythonOperator(\n    task_id='list_metadata_entries',\n    python_callable=list_metadata,\n    dag=dag,\n)\n\n# Task to delete a metadata entry by ID\ndelete_metadata_task = PythonOperator(\n    task_id='delete_metadata_entry',\n    python_callable=delete_metadata,\n    dag=dag,\n)\n\n# Define functions to simulate CRUD operations\ndef create_metadata():\n    print(\"Creating metadata entry\")\n\ndef get_metadata(metadata_id):\n    print(f\"Getting metadata entry with ID {metadata_id}\")\n\ndef list_metadata():\n    print(\"Listing all metadata entries\")\n\ndef delete_metadata(metadata_id):\n    print(f\"Deleting metadata entry with ID {metadata_id}\")\n\n# Define dependencies between tasks\ncreate_metadata_task >> get_metadata_task >> list_metadata_task >> delete_metadata_task"}], "metadata": {"file_name": "dag_326_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953809}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that shows how to programmatically manage and manipulate data in Google BigQuery using Airflow operators. Create a workflow that transforms raw data, performs quality checks, and demonstrates different query execution strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQueryDeleteTableOperator,\n    BigQueryExecuteQueryOperator,\n)\nfrom airflow.utils.dates import datetime\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Define the DAG\nwith DAG(\n    dag_id='bigquery_data_pipeline',\n    description='A simple data processing pipeline using Airflow with Google BigQuery',\n    default_args=default_args,\n    schedule_interval=None,  # Set to your desired schedule interval\n) as dag:\n\n    # Task to create a dataset if it doesn't exist\n    create_dataset = BigQueryCreateDatasetOperator(\n        task_id='create_dataset',\n        dataset_id='my_dataset',\n        project_id='your_project_id',\n    )\n\n    # Task to insert data into a table\n    insert_data = BigQueryInsertJobOperator(\n        task_id='insert_data',\n        destination_table_id='my_dataset.my_table',\n        schema_fields=[\n            {'name': 'id', 'type': 'INTEGER'},\n            {'name': 'name', 'type': 'STRING'},\n            {'name': 'age', 'type': 'INTEGER'},\n        ],\n        source_uris=['gs://path/to/your/data.csv'],\n        project_id='your_project_id',\n    )\n\n    # Task to delete a table\n    delete_table = BigQueryDeleteTableOperator(\n        task_id='delete_table',\n        table_id='my_dataset.my_table',\n        project_id='your_project_id',\n    )\n\n    # Task to execute a query\n    execute_query = BigQueryExecuteQueryOperator(\n        task_id='execute_query',\n        sql=\"SELECT * FROM my_dataset.my_table WHERE age > 30\",\n        use_legacy_sql=False,\n        project_id='your_project_id',\n    )\n\n    # Task to perform quality checks (example: check if the count of rows is correct)\n    check_count = BigQueryExecuteQueryOperator(\n        task_id='check_count',\n        sql=\"SELECT COUNT(*) AS row_count FROM my_dataset.my_table\",\n        use_legacy_sql=False,\n        project_id='your_project_id',\n        do_xcom_push=True,\n    )\n\n    # Task to log the result of the quality check\n    log_result = BashOperator(\n        task_id='log_result',\n        bash_command=f\"echo 'Row count: {{ ti.xcom_pull(task_ids='check_count') }}'\",\n        env={'AIRFLOW_HOME': '/usr/local/airflow'},\n    )\n\n    # Define dependencies\n    create_dataset >> insert_data >> delete_table >> execute_query >> [check_count, log_result]"}], "metadata": {"file_name": "dag_1573_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982065.9953918}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases different task execution strategies for Looker data transformations, highlighting the ability to start PDT builds with both blocking and non-blocking approaches. Implement a workflow that demonstrates task chaining and sensor-based job monitoring.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.sensors.looker import LookerJobSensor\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Task 1: Start PDT build with blocking approach\ndef start_pdt_build_blocking():\n    # Replace 'your_looker_instance_url' and 'your_api_token' with your actual Looker instance URL and API token\n    looker_instance_url = 'https://your_looker_instance_url'\n    api_token = 'your_api_token'\n\n    # Start PDT build\n    command = f\"curl -X POST '{looker_instance_url}/api/49/pdts/build?api_key={api_token}'\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"PDT build started successfully\")\n    else:\n        print(f\"PDT build failed: {result.stderr}\")\n\nstart_pdt_build_task = PythonOperator(\n    task_id='start_pdt_build_blocking',\n    python_callable=start_pdt_build_blocking,\n    dag=dag\n)\n\n# Task 2: Start PDT build with non-blocking approach\ndef start_pdt_build_non_blocking():\n    # Replace 'your_looker_instance_url' and 'your_api_token' with your actual Looker instance URL and API token\n    looker_instance_url = 'https://your_looker_instance_url'\n    api_token = 'your_api_token'\n\n    # Start PDT build\n    command = f\"curl -X POST '{looker_instance_url}/api/49/pdts/build?api_key={api_token}&non_blocking=true'\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"Non-blocking PDT build started successfully\")\n    else:\n        print(f\"Non-blocking PDT build failed: {result.stderr}\")\n\nstart_pdt_build_non_blocking_task = PythonOperator(\n    task_id='start_pdt_build_non_blocking',\n    python_callable=start_pdt_build_non_blocking,\n    dag=dag\n)\n\n# Task 3: Monitor PDT build status using sensor\ndef monitor_pdt_build_status(job_id):\n    # Replace 'your_looker_instance_url' and 'your_api_token' with your actual Looker instance URL and API token\n    looker_instance_url = 'https://your_looker_instance_url'\n    api_token = 'your_api_token'\n\n    # Check PDT build status\n    command = f\"curl -X GET '{looker_instance_url}/api/49/pdts/{job_id}?api_key={api_token}'\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        build_status = result.stdout.strip()\n        print(f\"PDT build status: {build_status}\")\n    else:\n        print(f\"Failed to retrieve PDT build status: {result.stderr}\")\n\nmonitor_pdt_build_status_task = LookerJobSensor(\n    task_id='monitor_pdt_build_status',\n    job_id=\"{{ task_instance.xcom_pull(task_ids='start_pdt_build_blocking', key='return_value') }}\",\n    poke_interval=60,\n    timeout=3600,\n    dag=dag\n)\n\n# Task 4: Chaining tasks\ndef chain_tasks():\n    # Replace 'your_looker_instance_url' and 'your_api_token' with your actual Looker instance URL and API token\n    looker_instance_url = 'https://your_looker_instance_url'\n    api_token = 'your_api_token'\n\n    # Chain tasks\n    command = f\"curl -X POST '{looker_instance_url}/api/49/pdts/chains?api_key={api_token}'\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"Task chaining successful\")\n    else:\n        print(f\"Task chaining failed: {result.stderr}\")\n\nchain_tasks_task = PythonOperator(\n    task_id='chain_tasks',\n    python_callable=chain_tasks,\n    dag=dag\n)\n\n# Task dependencies\nstart_pdt_build_blocking_task >> monitor_pdt_build_status_task >> chain_tasks_task"}], "metadata": {"file_name": "dag_42_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954045}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one workflow initiates another workflow with a predefined configuration message.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'primary_dag',\n    default_args=default_args,\n    description='A simple DAG to trigger secondary DAG',\n    schedule_interval=timedelta(days=1)\n)\n\ndef trigger_secondary_dag():\n    # Define the configuration message\n    config_message = \"Hello from Primary DAG\"\n    \n    # Trigger the secondary DAG\n    secondary_dag_run = SecondaryDAGRunOperator(\n        task_id='trigger_secondary_dag',\n        dag=dag,\n        conf={'config_message': config_message}\n    )\n\ntrigger_secondary_dag()"}], "metadata": {"file_name": "dag_2223_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954119}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that illustrates the infrastructure-as-code approach to setting up a scalable job scheduling environment in Kubernetes. Create a workflow that demonstrates provisioning computational resources, defining job queues, and preparing a system for parallel workload processing.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.kubernetes.operators.kubernetes_pod_operator import KubernetesPodOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n}\n\n# Create the DAG\ndag = DAG(\n    'kubernetes_job_scheduling_dag',\n    default_args=default_args,\n    description='A simple DAG to demonstrate Kubernetes job scheduling',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task to provision a Kubernetes cluster\nprovision_cluster_task = KubernetesPodOperator(\n    namespace='default',\n    image='alpine:latest',\n    command=['sh', '-c', 'echo \"Provisioning Kubernetes cluster\"'],\n    dag=dag,\n    task_id='provision_cluster'\n)\n\n# Task to define a job queue\ndefine_queue_task = KubernetesPodOperator(\n    namespace='default',\n    image='alpine:latest',\n    command=['sh', '-c', 'echo \"Defining job queue\"'],\n    dag=dag,\n    task_id='define_queue'\n)\n\n# Task to prepare a system for parallel workload processing\nprepare_system_task = KubernetesPodOperator(\n    namespace='default',\n    image='alpine:latest',\n    command=['sh', '-c', 'echo \"Preparing system for parallel workload processing\"'],\n    dag=dag,\n    task_id='prepare_system'\n)\n\n# Task to run a sample job\nrun_sample_job_task = KubernetesPodOperator(\n    namespace='default',\n    image='alpine:latest',\n    command=['sh', '-c', 'echo \"Running sample job\"'],\n    dag=dag,\n    task_id='run_sample_job'\n)\n\n# Task to clean up resources\ncleanup_resources_task = KubernetesPodOperator(\n    namespace='default',\n    image='alpine:latest',\n    command=['sh', '-c', 'echo \"Cleaning up resources\"'],\n    dag=dag,\n    task_id='cleanup_resources'\n)\n\n# Define the dependencies between tasks\nprovision_cluster_task >> define_queue_task >> prepare_system_task >> run_sample_job_task >> cleanup_resources_task"}], "metadata": {"file_name": "dag_902_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Base", "inference_time": 1765982065.995421}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates integration with InfluxDB using the InfluxDBOperator for time-bounded data extraction. Build a task that pulls recent time-series data using a dynamic time range relative to the current execution date.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from datetime import timedelta\nfrom airflow import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\nfrom airflow.operators.python_operator import PythonOperator\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG\ndag = DAG(\n    'influxdb_time_bounded_data_extraction',\n    default_args=default_args,\n    description='A DAG to extract recent time-series data from InfluxDB',\n    schedule_interval=timedelta(hours=1)  # Adjust this interval as needed\n)\n\n# Function to fetch data from InfluxDB\ndef fetch_data_from_influxdb():\n    hook = InfluxDBHook(influxdb_conn_id='influxdb_default')\n    \n    # Get the current date and time\n    now = datetime.now()\n    \n    # Calculate the start and end times for the last hour\n    start_time = now - timedelta(hours=1)\n    end_time = now\n    \n    # Query the database\n    query = f\"SELECT * FROM your_measurement WHERE time >= '{start_time}' AND time <= '{end_time}'\"\n    result = hook.run_query(query)\n    \n    return result\n\n# Task to execute the function\nfetch_data_task = PythonOperator(\n    task_id='fetch_data_task',\n    python_callable=fetch_data_from_influxdb,\n    dag=dag\n)\n\n# Example usage of the fetched data (this part depends on what you want to do with the data)\n# For demonstration purposes, we'll just print it out\nprint_data_task = PythonOperator(\n    task_id='print_data_task',\n    python_callable=lambda x: print(x),\n    op_kwargs={'x': fetch_data_task.output},\n    dag=dag\n)\n\n# Set up the dependencies between tasks\nfetch_data_task >> print_data_task"}], "metadata": {"file_name": "dag_2145_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954302}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Dataproc cluster management workflow that demonstrates creating, starting, stopping, and deleting a Dataproc cluster with specific configuration parameters. The workflow should showcase handling of existing clusters and provide a complete lifecycle management example for big data processing infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'dataproc_cluster_management',\n    default_args=default_args,\n    description='A simple DAG to manage Google Cloud Dataproc clusters',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the Dataproc cluster configuration\ncluster_config = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"size_gb\": 100\n        }\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type\": \"n1-standard-8\",\n        \"disk_config\": {\n            \"size_gb\": 100\n        }\n    },\n    \"gce_metadata\": {\n        \"startup_script\": \"gs://path/to/startup-script.sh\"\n    }\n}\n\n# Create a Dataproc cluster operator\ncreate_cluster = DataprocCreateClusterOperator(\n    task_id='create_dataproc_cluster',\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n    config=cluster_config,\n)\n\n# Submit a job to the cluster\nsubmit_job = DataprocSubmitJobOperator(\n    task_id='submit_job_to_cluster',\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n    main_class=\"com.example.MainClass\",\n    jar_file_uri=\"gs://path/to/your-jar-file.jar\",\n    args=[\"arg1\", \"arg2\"],\n)\n\n# Delete the Dataproc cluster operator\ndelete_cluster = DataprocDeleteClusterOperator(\n    task_id='delete_dataproc_cluster',\n    project_id=\"your-project-id\",\n    region=\"us-central1\",\n    cluster_name=\"my-dataproc-cluster\",\n)\n\n# Define the dependencies between tasks\ncreate_cluster >> submit_job >> delete_cluster"}], "metadata": {"file_name": "dag_1622_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954402}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task dependencies and lifecycle management, including nested task groups with explicit setup and teardown operations. Develop a workflow that demonstrates how to link tasks with clear initialization and finalization steps, using both decorator-based and direct task relationship methods.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple example DAG demonstrating task dependencies, lifecycle management, nested task groups, and explicit setup and teardown operations.',\n    schedule_interval=timedelta(days=1)\n)\n\n# Task group for setup operations\nwith dag.create_task_group('setup_operations'):\n    # Setup task\n    setup_task = PythonOperator(\n        task_id='setup_task',\n        python_callable=lambda: print(\"Setup operation completed\"),\n        dag=dag\n    )\n\n# Task group for main workflow\nwith dag.create_task_group('main_workflow'):\n    # Main task\n    main_task = PythonOperator(\n        task_id='main_task',\n        python_callable=lambda: print(\"Main task completed\"),\n        dag=dag\n    )\n\n    # Finalize task\n    finalize_task = PythonOperator(\n        task_id='finalize_task',\n        python_callable=lambda: print(\"Finalize operation completed\"),\n        dag=dag\n    )\n\n# Set up task dependency\nsetup_task.set_upstream(main_task)\n\n# Set up task dependency between main and finalize tasks\nmain_task.set_downstream(finalize_task)\n\n# Print task IDs for verification\nprint(f\"Task ID for setup operation: {setup_task.task_id}\")\nprint(f\"Task ID for main task: {main_task.task_id}\")\nprint(f\"Task ID for finalize operation: {finalize_task.task_id}\")"}], "metadata": {"file_name": "dag_1858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954495}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data processing workflow that simulates a multi-stage bash script execution environment, demonstrating how different computational tasks can be sequenced and managed through Airflow's task scheduling mechanism. Create a DAG that shows how simple shell commands can be orchestrated with complex dependency relationships.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': '2023-10-01',\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'bash_script_dag',\n    default_args=default_args,\n    description='A DAG to simulate a multi-stage bash script execution environment.',\n    schedule_interval=None,  # Set this to your desired schedule interval\n)\n\n# Task 1: Execute the first bash script\ntask_1 = BashOperator(\n    task_id='task_1',\n    bash_command='echo \"Executing task 1\"',\n    dag=dag,\n)\n\n# Task 2: Execute the second bash script, dependent on task 1\ntask_2 = BashOperator(\n    task_id='task_2',\n    bash_command='echo \"Executing task 2\" && ./script2.sh',\n    depends_on_past=False,\n    dag=dag,\n)\n\n# Task 3: Execute the third bash script, dependent on task 2\ntask_3 = BashOperator(\n    task_id='task_3',\n    bash_command='echo \"Executing task 3\" && ./script3.sh',\n    depends_on_past=False,\n    dag=dag,\n)\n\n# Define the dependencies between tasks\ntask_1.set_upstream(task_2)\ntask_2.set_upstream(task_3)"}], "metadata": {"file_name": "dag_328_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954576}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that leverages Azure Container Instances for scalable, ephemeral compute tasks. Create a DAG that demonstrates launching containerized workloads with configurable resources, environment variables, and storage volume integrations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.microsoft.azure.operators.aci_operator import ACIOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'aci_example_dag',\n    default_args=default_args,\n    description='A simple DAG to launch Azure Container Instances',\n    schedule_interval=timedelta(days=1),\n)\n\n# Define the task to launch an ACI container\nlaunch_aci_task = ACIOperator(\n    task_id='launch_aci_task',\n    image_name='nginx:latest',  # Replace with your desired Docker image\n    resource_requests={'cpu': '0.5', 'memory': '512Mi'},  # Adjust as needed\n    resource_limits={'cpu': '1', 'memory': '1Gi'},\n    environment_variables={'MY_VAR': 'my_value'},\n    volumes=[\n        {'name': 'data_volume', 'source': '/mnt/data'}\n    ],\n    acr_credentials={\n        'username': 'your_azure_username',\n        'password': 'your_azure_password'\n    },\n    dag=dag,\n)\n\n# Define the task to check the status of the ACI container\ncheck_aci_status_task = ACIOperator(\n    task_id='check_aci_status_task',\n    container_name='your_container_name',  # Replace with your container name\n    acr_credentials={\n        'username': 'your_azure_username',\n        'password': 'your_azure_password'\n    },\n    dag=dag,\n)\n\n# Set up the workflow\nlaunch_aci_task >> check_aci_status_task"}], "metadata": {"file_name": "dag_1746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954667}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that demonstrates a complete data transfer process from Google Ads to Google Cloud Storage, including bucket creation, data extraction using a custom query, listing of advertising accounts, and automatic bucket cleanup. The workflow should showcase inter-service orchestration and demonstrate best practices for cloud data migration.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.providers.google.ads.hooks.adwords import GoogleAdsHook\nfrom airflow.providers.google.cloud.hooks.storage import GoogleCloudStorageHook\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\n# Define default arguments\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# DAG configuration\ndag = DAG(\n    'google_ads_to_gcs_data_transfer',\n    default_args=default_args,\n    description='A DAG to transfer data from Google Ads to Google Cloud Storage',\n    schedule_interval=timedelta(days=1),\n)\n\n# Task to create a Google Cloud Storage bucket\ncreate_bucket_task = GoogleCloudStorageHook.create_bucket_task(\n    task_id='create_bucket',\n    bucket_name='your-google-cloud-storage-bucket-name'\n)\n\n# Task to extract data from Google Ads using a custom SQL query\nextract_data_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data_from_google_ads,\n    op_kwargs={\n        'query': 'SELECT * FROM adwords.account_performance_report WHERE campaign_id = \"YOUR_CAMPAIGN_ID\"'\n    }\n)\n\n# Task to list advertising accounts\nlist_accounts_task = PythonOperator(\n    task_id='list_accounts',\n    python_callable=list_advertising_accounts,\n    op_kwargs={}\n)\n\n# Task to clean up the bucket after the data transfer\ncleanup_bucket_task = GoogleCloudStorageHook.cleanup_bucket_task(\n    task_id='cleanup_bucket',\n    bucket_name='your-google-cloud-storage-bucket-name'\n)\n\n# Task dependencies\ncreate_bucket_task >> extract_data_task >> list_accounts_task >> cleanup_bucket_task\n\ndef extract_data_from_google_ads(query):\n    # Implement your logic to execute the Google Ads query and return the results\n    pass\n\ndef list_advertising_accounts():\n    # Implement your logic to list advertising accounts and return the results\n    pass"}], "metadata": {"file_name": "dag_454_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954774}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an interactive Airflow DAG that demonstrates dynamic parameter configuration for workflow triggering, showcasing how users can customize input parameters with various input types and validation rules.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Create the DAG\ndag = DAG(\n    'dynamic_parameter_dag',\n    default_args=default_args,\n    description='A DAG demonstrating dynamic parameter configuration',\n    schedule_interval=None,\n)\n\n# Task to accept user inputs\ndef get_user_inputs():\n    # Simulate getting user inputs from a web form or other source\n    return {\n        'input_type': 'string',\n        'input_value': 'Hello, World!',\n        'validation_rule': 'required'\n    }\n\n# Task to validate the user inputs\ndef validate_input(input_data):\n    if input_data['validation_rule'] == 'required' and not input_data['input_value']:\n        raise ValueError('Input value is required')\n    return True\n\n# Task to trigger another task with the validated input\ndef trigger_task_with_input(**kwargs):\n    input_data = kwargs.get('input_data', {})\n    if validate_input(input_data):\n        print(f\"Triggering task with input: {input_data}\")\n        # Here you would add code to trigger another task with the validated input\n        pass\n\n# Task to display the result of the triggered task\ndef display_result(result):\n    print(f\"Result of triggered task: {result}\")\n\n# Tasks in the DAG\nget_user_inputs_task = PythonOperator(\n    task_id='get_user_inputs',\n    python_callable=get_user_inputs,\n    dag=dag,\n)\n\nvalidate_input_task = PythonOperator(\n    task_id='validate_input',\n    python_callable=validate_input,\n    provide_context=True,\n    dag=dag,\n)\n\ntrigger_task_with_input_task = PythonOperator(\n    task_id='trigger_task_with_input',\n    python_callable=trigger_task_with_input,\n    provide_context=True,\n    dag=dag,\n)\n\ndisplay_result_task = PythonOperator(\n    task_id='display_result',\n    python_callable=display_result,\n    provide_context=True,\n    dag=dag,\n)\n\n# Set up the dependencies between tasks\nget_user_inputs_task >> validate_input_task >> trigger_task_with_input_task >> display_result_task"}], "metadata": {"file_name": "dag_16_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Base", "inference_time": 1765982065.9954882}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates error handling and external alerting by implementing a bash task with integrated failure notifications to an incident management platform.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.utils.dates import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Create the DAG object\ndag = DAG(\n    'bash_error_handling_dag',\n    default_args=default_args,\n    description='A simple DAG demonstrating error handling and external alerting',\n    schedule_interval=None,\n)\n\n# Define the Bash task\nbash_task = BashOperator(\n    task_id='run_bash_command',\n    bash_command='echo \"Hello, World!\" && exit 1',  # This command will fail\n    dag=dag,\n)\n\n# Define the email alert task\nalert_task = BashOperator(\n    task_id='send_email_alert',\n    bash_command='curl -X POST https://api.example.com/alerts -H \"Content-Type: application/json\" -d \"{\\\"message\\\": \\\"Bash command failed\\\"}\"',\n    dag=dag,\n    trigger_rule='all_failed'\n)\n\n# Set up the dependencies between tasks\nbash_task >> alert_task"}], "metadata": {"file_name": "dag_2179_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Base", "inference_time": 1765982065.995496}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a dataset with multiple time-series data points for different categories, and I want to visualize the trends over time for these categories using Python. I have started using Matplotlib and Pandas. Can you guide me on how to plot multiple time-series data on the same graph to compare their trends over time?"}, {"role": "assistant", "content": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the data\ndata = pd.read_csv('your_data.csv')\n\n# Step 2: Plotting multiple time-series\n# Let's say 'category1' and 'category2' are the columns containing the time-series data\nplt.figure(figsize=(10, 6))  # Set the figure size\n\n# Plotting category1\nplt.plot(data['date'], data['category1'], label='Category 1', color='blue')\n\n# Plotting category2\nplt.plot(data['date'], data['category2'], label='Category 2', color='red')\n\n# Adding title and labels\nplt.title('Time Series Comparison')\nplt.xlabel('Date')\nplt.ylabel('Value')\n\n# Adding legend\nplt.legend()\n\n# Display the plot\nplt.show()"}], "metadata": {"model": "Qwen2.5-Base", "inference_time": 1765982065.9955032}}
