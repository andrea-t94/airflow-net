{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates loading sample product data into a Snowflake table and validating the data load. The pipeline should insert 12 product records and then verify the total number of rows matches the expected count.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nSNOWFLAKE_SAMPLE_TABLE = \"sample\"\nSNOWFLAKE_CONN_ID = \"snowflake\"\n\nwith DAG(\n    f\"example_snowflake_insert_many_{ENV_ID}\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_snowflake]\n    snowflake_insert_many = SnowflakeOperator(\n        task_id=\"insert_many_into_snowflake_table\",\n        sql=\"\"\"\n            INSERT INTO %s VALUES\n            ( 'prod', '123456789012345', '2020-01-01' ),\n            ( 'prod', '123456789012345', '2020-01-02' ),\n            ( 'prod', '123456789012345', '2020-01-03' ),\n            ( 'prod', '123456789012345', '2020-01-04' ),\n            ( 'prod', '123456789012345', '2020-01-05' ),\n            ( 'prod', '123456789012345', '2020-01-06' ),\n            ( 'prod', '123456789012345', '2020-01-07' ),\n            ( 'prod', '123456789012345', '2020-01-08' ),\n            ( 'prod', '123456789012345', '2020-01-09' ),\n            ( 'prod', '123456789012345', '2020-01-10' ),\n            ( 'prod', '123456789012345', '2020-01-11' ),\n            ( 'prod', '123456789012345', '2020-01-12' ),\n            ( 'prod', '123456789012345', '2020-01-13' ),\n            ( 'prod', '123456789012345', '2020-01-14' ),\n            ( 'prod', '123456789012345', '2020-01-15' ),\n            ( 'prod', '123456789012345', '2020-01-16' ),\n            ( 'prod', '123456789012345', '2020-01-17' ),\n            ( 'prod', '123456789012345', '2020-01-18' ),\n            ( 'prod', '123456789012345', '2020-01-19' ),\n            ( 'prod', '123456789012345', '2020-01-20' );\n        \"\"\" % SNOWFLAKE_SAMPLE_TABLE,\n    )\n    # [END howto_operator_snowflake]\n\n    snowflake_insert_many >> TriggerRule.ALL_DONE\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1798_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299076}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a pandas DataFrame `df` with a column `text` that contains strings. I want to create a new column `text_len` that contains the length of each string in the `text` column. How can I achieve this?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Sample data creation (replace this with actual data loading)\ndata = {\n    'text': ['hello world', 'this is a test', 'pandas is great']\n}\ndf = pd.DataFrame(data)\n\n# Calculate the length of each string and store it in a new column 'text_len'\ndf['text_len'] = df['text'].apply(len)\n\nprint(df)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299207}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that showcases the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class. Build a simple workflow that triggers an empty operator after standard work hours to illustrate scheduling flexibility.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.sdk import DAG\nfrom airflow.timetables.after_workday import AfterWorkdayTimetable\nfrom airflow.utils.weekday import WeekDay\n\n\nwith DAG(\n    dag_id=\"example_after_workday_timetable\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@daily\",\n    catchup=False,\n    timetables={\n        \"default\": AfterWorkdayTimetable(days=(WeekDay.MONDAY, WeekDay.TUESDAY)),\n    },\n):\n    run_this = EmptyOperator(task_id=\"run_this\")"}], "metadata": {"file_name": "dag_1063_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299248}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that retrieves Campaign Manager metrics, generates a standard performance report across campaigns, and demonstrates batch conversion tracking and reporting capabilities. The DAG should illustrate how to interact with Campaign Manager's reporting and conversion APIs using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "# [START campaign_manager_import]\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.campaign_manager import (\n    GoogleCampaignManagerBatchInsertOperator,\n    GoogleCampaignManagerDeleteOperator,\n    GoogleCampaignManagerGetOperator,\n    GoogleCampaignManagerListOperator,\n    GoogleCampaignManagerUpdateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GMP_PROJECT_ID\", \"default\")\n\nDAG_ID = \"example_campaign_manager\"\n\nPATH_TO_CLOUD_SAVED_LIST_FILE = \"gs://airflow-system-tests-resources/campaign-manager/saved_list.json\"\nINSERT.INSTANCE_ID = str(int(os.environ.get(\"AIRFLOW_EXTRACTOR_INSTANCE_ID\", 1)))\nUPDATE.INSTANCE_ID = \"cm_test_id_2\"\nDELETE.ID = \"cm_test_id_3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcp_marketing_platform_campaing_manager_batch_insert]\n    batch_insert = GoogleCampaignManagerBatchInsertOperator(\n        task_id=\"batch_insert\",\n        project_id=PROJECT_ID,\n        body={\n            \"ids\": [\n                \"i-1\",\n            ],\n            \"operations\": [\n                {\n                    \"add\": {\n                        \"campaign\": {\"id\": INSERT.INSTANCE_ID},\n                        \"type\": \"CAMPAIGN\",\n                        \"parameters\": {\"parameter\": {\"string\": \"value\"}},\n                    }\n                },\n            ],\n        },\n    )\n    # [END howto_operator_gcp_marketing_platform_campaing_manager_batch_insert]\n\n    # [START howto_operator_gcp_marketing_platform_campaing_manager_get]\n    get = GoogleCampaignManagerGetOperator(task_id=\"get\", path=\"cm_test_id_1\")\n    # [END howto_operator_gcp_marketing_platform_campaing_manager_get]\n\n    # [START howto_operator_gcp_marketing_platform_campaing_manager_update]\n    update = GoogleCampaignManagerUpdateOperator(\n        task_id=\"update\",\n        path=\"cm_test_id_2\",\n        body={\"params\": [{\"parameter\": {\"string\": \"value\"}}]}),\n    )\n    # [END howto_operator_gcp_marketing_platform_campaing_manager_update]\n\n    # [START howto_operator_gcp_marketing_platform_campaing_manager_list]\n    list_ = GoogleCampaignManagerListOperator(\n        task_id=\"list_\",\n        paths=[\"campaignmanager/adwords/CM_1589764111\"),\n    )\n    # [END howto_operator_gcp_marketing_platform_campaing_manager_list]\n\n    batch_insert >> get >> update >> list_\n\n    batch_insert_dep = batch_insert.output_task_ids()\n    get_dep = get.output_task_ids()\n    update_dep = update.output_task_ids()\n    list_.output_task_ids_like(batch_insert_dep)\n\n    batch_insert >> get >> update >> list_\n    delete_dep = delete.output_task_ids()\n\n    # [START howto_operator_gcp_marketing_platform_campaing_manager_delete]\n    delete = GoogleCampaignManagerDeleteOperator(\n        task_id=\"delete\", path=\"cm_test_id_3\", trigger_rule=TriggerRule.ALL_DONE)\n    # [END howto_operator_gcp_marketing_platform_campaing_manager_delete]\n\n    (batch_insert >> get >> update >> list_) >> delete\n\n    (delete >> [batch_insert_dep, get_dep, update_dep, list_])\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1336_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299353}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data migration workflow that enables seamless transfer of structured CSV data from cloud object storage into a columnar analytics database. Create an Airflow task that supports efficient, automated data loading from cloud storage to a queryable data platform.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToTrinoOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\n\n# [START howto_gcs]\n# TODO: Update this example\nGCS_BUCKET = \"s3a://bucket-name/\"\nGCS_OBJECT_KEY = \"example/file.txt\"\n# [END howto_gcs]\n\n# [START howto_trino]\nTRINO_TABLE = \"test_table\"\n# [END howto_trino]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcs_to_trino_default\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcs_to_trino]\n    gcs_to_trino = GCSToTrinoOperator(\n        task_id=\"gcs_to_trino_task\",\n        gcs_bucket=GCS_BUCKET,\n        gcs_object_key=GCS_OBJECT_KEY,\n        trino_table=TRINO_TABLE,\n    )\n    # [END howto_operator_gcs_to_trino]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_992_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299405}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Tell me how to use TensorFlow with Python."}, {"role": "assistant", "content": "import tensorflow as tf"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299517}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a Python function to retrieve the details of an Elastic Network Interface (ENI) using the Boto3 library?"}, {"role": "assistant", "content": "import boto3\n\ndef get_eni_details(eni_id):\n    # Create an EC2 client\n    ec2_client = boto3.client('ec2')\n    \n    # Call describe_network_interfaces to fetch ENI details\n    response = ec2_client.describe_network_interfaces(\n        Filters=[\n            {\n                'Name': 'network-interface-id',\n                'Values': [eni_id]\n            },\n            {\n                'Name': 'association.subnet-id',\n                'Values': ['subnet-12345678']  # Example subnet ID\n            }\n        ]\n    )\n    \n    # Extract and return the ENI details\n    eni_details = {}\n    if 'NetworkInterfaces' in response:\n        for interface in response['NetworkInterfaces']:\n            eni_details[interface['Association']['SubnetId']] = {\n                'InterfaceId': interface['InterfaceId'],\n                'PrivateDnsName': interface.get('PrivateDnsName'),\n                'PrivateIpV4Address': interface.get('PrivateIpAddress'),\n                'Status': interface['Status']\n            }\n    return eni_details\n\n# Example usage\neni_id = 'eni-0123456789abcdef0'\nenior_details = get_eni_details(enini_id)\nprint(enior_details)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299639}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data management workflow for tracking employee information in a Vertica database. Create a process that demonstrates creating, populating, querying, and cleaning up an employee records table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using VerticaOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.vertica.operators.vertica import VerticaOperator\n\n\ndef load_connections():\n    \"\"\"\n    Connection parameters for the example dag below are defined in examples/connections.yaml\n    \"\"\"\n    from airflow.utils.importlibLoader import import_loader\n\n    return import_loader(\"examples.connections\").load_connection(\"vertica_conn\")\n\n\nwith DAG(\n    \"vertica_example_dag\",\n    default_args={\"vertica_conn_id\": \"vertica_conn\"},\n    catchup=False,\n    start_date=datetime(2021, 1, 1),\n) as dag:\n    connect_info = load_connections()\n\n    # [START howto_vertica_operator]\n    task_get_all = VerticaOperator(task_id=\"get_all_data\", sql=\"SELECT * FROM employees;\", conn_id=connect_info)\n    # [END howto_vertica_operator]\n\n    # [START howto_vertica_operator_with_params]\n    task_get_all_params = VerticaOperator(\n        task_id=\"get_all_data_with_params\",\n        sql=\"SELECT * FROM employees WHERE dept_name LIKE %s\",\n        params=(\"xyz%\", ),\n        conn_id=connect_info,\n    )\n    # [END howto_vertica_operator_with_params]\n\n    task_get_all_empty = VerticaOperator(\n        task_id=\"get_all_data_empty\",\n        sql=\"SELECT * FROM employees;\",\n        conn_id=connect_info,\n    )\n\n    task_getall_from_table = VerticaOperator(\n        task_id=\"getall_from_table\",\n        sql=\"SELECT * FROM dbc.current_database();\",\n        use_task_defaults=True,\n    )\n\n\ndag >> task_get_all >> task_getall_from_table\ndag >> task_get_all_params >> task_getall_from_table"}], "metadata": {"file_name": "dag_1418_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299696}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data exploration DAG that uses Azure Data Lake Storage to perform a remote file inventory of Parquet-formatted files in a specific directory. Construct an automated task that lists file metadata to support data validation and tracking processes.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSListOperator\n\ntry:\n    from airflow.providers.microsoft.azure.transfers.adls_to_sftp import ADLSToSFTPTempFileOperator\nexcept ImportError:\n    from airflow.providers.microsoft.azure.transfers.adls_to_local import ADLSToLocalOperator  # noqa: E402\n\n# [START howto_operator_adls_list]\ndag = DAG(\n    \"adls_list\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n)\ntmp_path = ADLSToSFTPTempFileOperator(task_id=\"tmp_path_task\", adls_path=\"/path/\")\nadls_list_path = ADLSListOperator(task_id=\"adls_list_path_task\", adls_path=\"/path\")\n# [END howto_operator_adls_list]\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2154_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299734}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration workflow that transfers files from a cloud storage bucket to an SFTP destination with multiple transfer strategies. Create a pipeline that supports different file transfer modes including copying individual files, moving files, and transferring entire directory contents.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using FTPToSFTPServerOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.ftp.transfers.ftp_to_sftp import FTPToSFTPServerOperator\n\n\ndef load_env_variables():\n    \"\"\"\n    Environment variables:\n        FTP_TO_SFTP_DESTINATION_PATH - Path under sftp where files will be uploaded\n    \"\"\"\n\n    from airflow.utils.env import SystemEnvironmentVariable\n\n    # This should match the path to the folder holding this test case.\n    test_folder = os.path.dirname(os.path.realpath(__file__))\n    # Absolute test folder containing tmp file\n    absolute_test_folder = os.path.normpath(test_folder)\n    # Final relative path to tmp file\n    final_path = absolute_test_folder if \"/\" not in absolute_test_folder else absolute_test_folder[1:]\n\n    # We set the env variable so that we can ensure the test uses the same environment as written.\n    SystemEnvironmentVariable.set(\"FTP_TO_SFTP_DESTINATION_PATH\", final_path)\n\n\nwith DAG(\n    \"example_ftp_to_sftp_server\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n):\n    # [START howto_operator_ftp_to_sftp]\n    upload_file = FTPToSFTPServerOperator(\n        task_id=\"upload_file\",\n        ftp_conn_id=\"ftp_default\",\n        sftp_conn_id=\"sftp_default\",\n        src=\"/tmp/python_airflow/test.tar.xz\",\n        dest=\"/tmp/ftp_to_sftp/\",\n    )\n    # [END howto_operator_ftp_to_sftp]\n\n    # [START howto_operator_move_ftp_to_sftp]\n    move_file = FTPToSFTPServerOperator(\n        task_id=\"move_file\",\n        ftp_conn_id=\"ftp_default\",\n        sftp_conn_id=\"sftp_default\",\n        src=\"/tmp/python_airflow/test.tar.xz\",\n        dest=\"/tmp/ftp_to_sftp/\",\n        move_dest=True,\n    )\n    # [END howto_operator_move_ftp_to_sftp]\n\n    # [START howto_operator_upload_dir_ftp_to_sftp]\n    upload_dir = FTPToSFTPServerOperator(\n        task_id=\"upload_dir\",\n        ftp_conn_id=\"ftp_default\",\n        sftp_conn_id=\"sftp_default\",\n        src=\"/tmp/dataset\",\n        dest=\"/tmp/ftp_to_sftp/\",\n    )\n    # [END howto_operator_upload_dir_ftp_to_sftp]\n\n    # [START howto_operator_move_dir_ftp_to_sftp]\n    move_dir = FTPToSFTPServerOperator(\n        task_id=\"move_dir\",\n        ftp_conn_id=\"ftp_default\",\n        sftp_conn_id=\"sftp_default\",\n        src=\"/tmp/dataset\",\n        dest=\"/tmp/ftp_to_sftp/\",\n        move_dest=True,\n    )\n    # [END howto_operator_move_dir_ftp_to_sftp]"}], "metadata": {"file_name": "dag_1298_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299818}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I want to format the date and time from a Unix timestamp to a human-readable format using Python. How can I do this?"}, {"role": "assistant", "content": "from datetime import datetime\n\n# Example: Convert a Unix timestamp to a human-readable string\ntimestamp = 1633072800  # Replace with your Unix timestamp\nhuman_readable_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\nprint(human_readable_date)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6299875}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow for deploying and managing an EMR (Elastic MapReduce) job on an EKS (Elastic Kubernetes Service) cluster using AWS services. Create a data processing pipeline that demonstrates launching a Kubernetes cluster, configuring IAM roles, and running a sample Python computation job.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterAlreadyExistsError, ClusterHook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyJobFlowOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Saturday\nfrom airflow.utils.timezone import datetime as date_time\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr\"\nEXTRA_ROLE_ARNS = [\"\"]\nEXECUTION_ROLE_ARN = \"\"\nJOB_FLOW_OVERRIDES = {\n    \"Name\": DAG_ID,\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Hadoop\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"LogGroupName\": \"\",\n                \"LogDriver\": \"awslogs\",\n                \"TerminationProtected\": False,\n                \"MetadataOptions\": {\"HttpTokens\": \"required\"},\n            },\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"CORE\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"LogGroupName\": \"\",\n                \"LogDriver\": \"awslogs\",\n                \"TerminationProtected\": False,\n                \"MetadataOptions\": {\"HttpTokens\": \"required\"},\n            },\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": True,\n    },\n    \"Steps\": [\n        {\n            \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n            \"Application\": \"HADOOP\",\n            \"Name\": \"create-hdfs\",\n            \"Args\": [\n                \"-D.hadoop.root.dir=/hdfs\",\n                \"-c\",\n                \"/usr/lib/hadoop/bin/hdfs',\n                'dfs', '-mkdir', '-p /hdfs/input\",\n                'dfs', '-mkdir', '-p /hdfs/output'\",\n            ],\n        },\n        {\n            \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n            \"BreakerTimeoutInMinutes\": 15,\n            \"Classification\": \"emr.stepdefinition\",\n            \"Name\": \"PyWordCount\",\n            \"HadoopJarStep\": {\n                \"JAR\": \"s3n://aws-demography-jars/wordcount.jar\",\n                \"Args\": [\"s3n://amazon-reviews-polarity-segmentation-task-S3/output\", \"s3n://amazon-reviews-polarity-segmentation-task-S3/input/\", \"s3n://amazon-reviews-polarity-segmentation-task-S3/output/\"],\n            },\n        },\n    ],\n    \"ServiceRole\": \"EMR_NAMED_SERVICE_ROLE\",\n}\nROLE_ARN_KEY = \"role_arn\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    bucket_name = f\"{env_id}-emr-bucket\"\n    s3_bucket = S3CreateBucketOperator(task_id=\"s3_create_bucket_task\", bucket_name=bucket_name)\n\n    @task\n    def create_eks_cluster(cluster_name: str, vpc_id: str, region: str):\n        # `'us-east-1'` is the default when creating a cluster from the UI or the console.\n        # If you want to choose a different region you need to explicitly set it in the ClusterHook constructor.\n        hook = ClusterHook(credential_role_arn=role_arn, client_region=region)\n        eks_cluster = {\n            \"clusterName\": cluster_name,\n            \"vpcConfiguration\": {\n                \"subnetIds\": [vpc_id],\n                \"endpointPublicAccess\": True,\n                \"endpointPrivateAccess\": False,\n            },\n        }\n        try:\n            hook.create_cluster(**eks_cluster)\n        except ClusterAlreadyExistsError:\n            print(f\"Cluster {cluster_name} already exists.\")\n\n    create_eks_cluster(cluster_name=\"test-eks-cluster\", vpc_id=f\"{env_id}-vpc-id\", region=\"us-east-1\")\n\n    # [START howto_operator_emr_create_job_flow]\n    start_job = EmrCreateJobFlowOperator(\n        task_id=\"start-job-flow-task\",\n        job_flow_name=DAG_ID,\n        emr_conn_id=\"emr_default\",\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n    )\n    # [END howto_operator_emr_create_job_flow]\n\n    # [START howto_sensor_emr_job_flow_state]\n    check_job_state = EmrSensor(\n        task_id=\"check-job-state\",\n        job_flow_id=start_job.output,\n        target_states=[\"RUNNING\"],\n        poke_interval=5,\n        timeout=30 * 60,\n    )\n    # [END howto_sensor_emr_job_flow_state]\n\n    # [START howto_operator_emr_modify_job_flow]\n    modify_job = EmrModifyJobFlowOperator(\n        task_id=\"modify-job\",\n        job_flow_id=start_job.output,\n        name=DAG_ID,\n        steps=[\n            {\"Name\": \"PyWordCount\", \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\", \"HadoopJarStep\": {\"Args\": [\"\"]}},\n        ],\n    )\n    # [END howto_operator_emr_modify_job_flow]\n\n    # [START howto_operator_emr_add_steps]\n    add_step = EmrAddStepsOperator(\n        task_id=\"add-step\",\n        job_flow_id=DAG_ID,\n        steps=[\n            {\n                \"Name\": \"PyWordCount\",\n                \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n                \"HadoopJarStep\": {\n                    \"Jar\": \"s3n://amazon-emr-wordcount-steps/wordcount.jar\",\n                    \"Args\": [\"s3n://amazon-emr-wordcount-steps/input\", \"s3n://amazon-emr-wordcount-steps/output/\"],\n                },\n            }\n        ],\n    )\n    # [END howto_operator_emr_add_steps]\n\n    # [START howto_operator_emr_terminate_job_flow]\n    terminate_job = EmrTerminateJobFlowOperator(\n        task_id=\"terminate-job\",\n        job_flow_id=DAG_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_emr_terminate_job_flow]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        s3_bucket,\n        create_eks_cluster(cluster_name=\"test-eks-cluster2\", vpc_id=f\"{env_id}-vpc-id\"),  # new cluster\n        # TEST BODY\n        start_job,\n        check_job_state,\n        modify_job,\n        add_step,\n        # TEST TEARDOWN\n        terminate_job,\n        delete_s3_bucket=s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) | watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_390_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300063}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a data processing workflow with explicit initialization and cleanup stages, using bash commands to represent different phases of data preparation, processing, and finalization. Create a DAG that illustrates how setup tasks can prepare an environment and teardown tasks can perform necessary cleanup operations around the main data processing logic.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\n\n\n@task(task_id=\"test_singularity\")\ndef test_singularity():\n    print(\"Hello World! (Singularity)\")\n\n\nwith DAG(\n    dag_id=\"example_singularity\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    test_singularity()\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_22_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300104}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need to convert a string to binary in Python. How can I do that?"}, {"role": "assistant", "content": "def string_to_binary(input_string):\n    # Convert each character to its ASCII value and then to binary\n    return ''.join(format(ord(char), '08b') for char in input_string)\n\n# Example usage:\ninput_str = \"Hello World\"\nbinary_representation = string_to_binary(input_str)\nprint(binary_representation)  # Output: 0100100001100101011011000111010101110011011011110111000001101111011011110110011001100111011101101110011001110110011101110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110011101100111011001110110"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.63002}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I modify the color of a button in a form using the `setStyle` method in Python with the PyQt5 library?"}, {"role": "assistant", "content": "from PyQt5.QtWidgets import QApplication, QWidget, QPushButton\n\n# Create a simple application instance\napp = QApplication([])\n\n# Create a window and a button\nwindow = QWidget()\nbutton = QPushButton('Click Me')\n\n# Set the initial style sheet (background color)\nbutton.setStyleSheet(\"QPushButton { background-color: blue; }\")\n\n# Optionally, set other properties like font or text color\nbutton.setFont(QFont('Arial', 12))\nbutton.setTextColor(Qt.red)\n\n# Show the window\nwindow.show()\n\n# Execute the application event loop\napp.exec_()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300266}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Generate a Python code to create a DataFrame from a dictionary where each key-value pair in the dictionary represents a column and its corresponding values. The dictionary provided is: `{'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'City': ['New York', 'Los Angeles', 'Chicago']}`. How can I handle missing values if any of the lists have a different length?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Sample dictionary with mixed-length lists\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', None]\n}\n\n# Convert the dictionary into a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the original DataFrame\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Check if all lists have the same length\nif len(df) == len(df['Name']) != len(df['Age']):\n    raise ValueError(\"All lists must have the same length.\")\n\n# Handle missing values by filling them with NaN (Not a Number)\ndf.fillna(method='ffill', inplace=True)  # Forward fill strategy\n\n# Display the DataFrame after handling missing values\nprint(\"\\nDataFrame after handling missing values:\")\nprint(df)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630034}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kafka-based FizzBuzz game that demonstrates message streaming and event processing. Create a system that generates numbers, checks for divisibility by 3 and 5, and triggers specific actions based on number characteristics.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from airflow.providers.standard.operators.bash import BashOperator\n\nfor loop = range(1,11):\n    if loop % 3 == 0:\n        print(\"fizz\")\n    elif loop % 5 == 0:\n        print(\"buzz\")\n    else:\n        print(loop)"}], "metadata": {"file_name": "dag_1533_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630037}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how preparatory and cleanup operations can be automatically managed around core task execution. Create a DAG that illustrates task dependency and group management with clear initialization and finalization steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\n\n@task(task_id=\"print_hello\")\ndef print_hello():\n    print(\"Hello\")\n\n\nwith DAG(\n    \"example_setup_teardown\",\n    schedule=pendulum.timeperiod(datetime(2020, 1, 1), datetime(2020, 1, 2)),\n    start_date=pendulum.datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    print_world = BashOperator(task_id=\"print_world\", bash_command=\"echo \" + \"World\")\n    print_hello()\n    print_world\n    print_hello()"}], "metadata": {"file_name": "dag_1446_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630041}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u5728Python\u4e2d\uff0c`len()`\u51fd\u6570\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u5b57\u7b26\u4e32\u3001\u5217\u8868\u3001\u5143\u7ec4\u4ee5\u53ca\u5b57\u5178\u7b49\u96c6\u5408\u7c7b\u578b\u7684\u5143\u7d20\u6570\u91cf\u3002\u7136\u800c\uff0c\u5b83\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u6574\u6570\u7c7b\u578b\u7684\u6570\u636e\u3002\u5982\u679c\u6211\u60f3\u8ba1\u7b97\u4e00\u4e2a\u6574\u6570\u7684\u4f4d\u6570\uff0c\u5e94\u8be5\u600e\u4e48\u505a\u5462\uff1f"}, {"role": "assistant", "content": "def count_digits(number):\n    if number == 0:\n        return 1  # \u7279\u6b8a\u60c5\u51b5\uff0c0\u67091\u4e2a\u6570\u5b57\n    \n    digits = 0\n    while number > 0:\n        number //= 10\n        digits += 1\n    return digits\n\n# \u793a\u4f8b\u7528\u6cd5\nnumber = 123456789\nprint(f\"The number {number} has {count_digits(number)} digits.\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300483}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complex cloud infrastructure setup, including creating a Compute Engine instance, configuring a PostgreSQL database, and transferring data to Google Sheets. Showcase a multi-step cloud data workflow with infrastructure provisioning and data migration.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using PostgresToGoogleSheetsOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.sheets import GoogleSheetsHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n)\nfrom airflow.providers.google.sheets.transfers.postgres_to_sheets import PostgresToGoogleSheetsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"postgres_to_sheets\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nRESOURCE_NAME = \"instance-1\"\nPOSTGRES_DATABASE_NAME = \"testdb\"\nPOSTGRES_INSTANCE_NAME = f\"{RESOURCE_NAME}-postgres\"\nSHEETS_TABLE_NAME = \"test_table\"\n\n# Instance template:\n# [START howto_postgres_instance_template]\nPOSTGRES_TEMPLATE = {\n    \"name\": RESOURCE_NAME + \"-postgres\",\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/n1-standard-1\",\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": True,\n            \"device_name\": POSTGRES_INSTANCE_NAME + \"-postgres\",\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"pd-balanced-{LOCATION}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/ppa\",\n            },\n        }\n    ],\n    \"network_interfaces\": [{\"network\": f\"global/networks/default\", \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}]}],\n    \"disks\": [\n        {\n            \"auto_delete\": True,\n            \"boot\": False,\n            \"device_name\": POSTGRES_INSTANCE_NAME + \"-postgres\",\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"pd-balanced-{LOCATION}\",\n                \"source_image_uri\": \"projects/debian-cloud/global/addresses/ppa\",\n            },\n        }\n    ],\n}\n# [END howto_postgres_instance_template]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_instance(template_name: str):\n        hook = ComputeEngineHook()\n        request = {\"project\": PROJECT_ID, \"zone\": LOCATION + \":\" + REGION, \"body\": {}}\n        request[\"body\"] = template_name\n        response = hook.client.make_request(request=request)\n        return response[\"name\"]\n\n    instance_name = create_instance(POSTGRES_TEMPLATE[\"name\"])\n\n    # [START howto_postgres_to_sheets_operator]\n    rename_sheet = PostgresToGoogleSheetsOperator(\n        task_id=\"rename_sheet\",\n        sql=f\"ALTER TABLE {POSTGRES_DATABASE_NAME}.public.t1 RENAME TO {SHEETS_TABLE_NAME};\",\n        google_sheets_config={\n            \"target_sheets_property\": \"spreadsheet.addSheet\",\n            \"properties\": {\n                \"title\": SHEETS_TABLE_NAME,\n                \"description\": \"Sheet containing table data.\",\n            },\n        },\n        postgres_conn_id=\"postgres_default\",\n        project_id=PROJECT_ID,\n        zone=LOCATION + \":\" + REGION,\n        instance_name=instance_name,\n        use_legacy_sql=False,\n        use_kubernetes_engine=False,\n    )\n    # [END howto_postgres_to_sheets_operator]\n\n    # [START howto_insert_data_to_sheets]\n    insert_data = PostgresToGoogleSheetsOperator(\n        task_id=\"insert_data\",\n        sql=f\"INSERT INTO {POSTGRES_DATABASE_NAME}.public.{SHEETS_TABLE_NAME} VALUES (42, 'TEST'\",  # noqa\n             google_sheets_config={\n                 \"target_sheets_property\": \"spreadsheet.insertRow\",\n                 \"properties\": {\n                     \"values\": [[\"42\", \"TEST\"]],\n                 },\n             },\n         ),\n        postgres_conn_id=\"postgres_default\",\n        project_id=PROJECT_ID,\n        zone=LOCATION + \":\" + REGION,\n        instance_name=instance_name,\n        use_legacy_sql=False,\n        use_kubernetes_engine=False,\n    )\n    # [END howto_insert_data_to_sheets]\n\n    delete_instance = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"delete_instance\",\n        template=POSTGRES_TEMPLATE,\n        zone=LOCATION + \":\" + REGION,\n        project_id=PROJECT_ID,\n        resource_name=RESOURCE_NAME + \"-instance\",\n        ignore_missing=True,\n        wait_for_termination=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_instance >> rename_sheet >> insert_data\n\n    delete_instance_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance_instance\",\n        name=instance_name,\n        zone=LOCATION + \":\" + REGION,\n        wait_for_completion=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    rename_sheet >> insert_data >> delete_instance_instance\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_910_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300623}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Compute Engine instance templates and instance group managers. Create a DAG that demonstrates how to create, copy, and update instance templates with different configurations in a cloud infrastructure environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Compute Intergroup Manager Operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineCopyInstanceOperator,\n    ComputeEngineDeleteInstanceGroupManagerOperator,\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceGroupManagerOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineInstanceGroupManagerFromCopyOperator,\n)\nfrom airflow.utils.context import Context\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nNETWORK = \"default\"\n\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nDAG_ID = \"example_gce_igm\"\n\nINSTANCE_NAME = \"igm-test\"\n.INSTANCE = {\n    \"name\": INSTANCE_NAME,\n    \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"source_image\": f\"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n                \"disk_name\": INSTANCE_NAME,\n            },\n        }\n    ],\n    \"network_interfaces\": [{\"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}}, {\"name\": \"8080\", \"subnetwork\": f\"regions/{REGION}/subnetworks/default\"}]},\n}\nIMAGRANK_NAME = \"img-rank-test\"\nIMAGRANK = {\"name\": IMAGRANK_NAME}\nIMAGRANK[\"network_interfaces\"] = [\n    {\n        \"access_configs\": [\n            {\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"},\n        ],\n        \"network\": f\"global/networks/{NETWORK}\",\n    }\n]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gce_insert_instance]\n    create_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"create_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=INSTANCE,\n    )\n    # [END howto_operator_gce_insert_instance]\n\n    # [START howto_operator_gce_copy_instance]\n    copy_instance = ComputeEngineCopyInstanceOperator(\n        task_id=\"copy_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=INSTANCE_NAME,\n        body={\"sourceInstance\": f\"zones/{LOCATION}/instances/{INSTANCE_NAME}\"},\n        new_resource_id=\"test-copy\",\n        new_zone=LOCATION,\n        new_body={},\n    )\n    # [END howto_operator_gce_copy_instance]\n\n    # [START howto_operator_gce_delete_instance]\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=INSTANCE_NAME,\n    )\n    # [END howto_operator_gce_delete_instance]\n\n    # [START howto_operator_gce_igm_create]\n    create_igm = ComputeEngineInsertInstanceGroupManagerOperator(\n        task_id=\"create_igm\", project_id=PROJECT_ID, region=REGION, body=IMAGRANK_NAME, body={\"size\": 1}\n    )\n    # [END howto_operator_gce_igm_create]\n\n    # [START howto_operator_gce_igm_from_copy]\n    igm_from_copy = ComputeEngineInstanceGroupManagerFromCopyOperator(\n        task_id=\"igm_from_copy\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body={\"resourceId\": \"test-copy\"},\n        repair=True,\n      zone=LOCATION,\n      instance_group_manager=IMAGRANK_NAME,\n    )\n    # [END howto_operator_gce_igm_from_copy]\n\n    # [START howto_operator_gce_igm_delete]\n    delete_igm = ComputeEngineDeleteInstanceGroupManagerOperator(\n        task_id=\"delete_igm\", project_id=PROJECT_ID, region=REGION, instance_group_manager=IMAGRANK_NAME\n    )\n    # [END howto_operator_gce_igm_delete]\n\n    (create_instance >> copy_instance >> create_igm >> igm_from_copy >> delete_igm >> delete_instance)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1204_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300735}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data transfer workflow that automatically moves files between source and destination buckets. Create a DAG that demonstrates GCS transfer job creation, file upload, and bucket management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSToGCSOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcs_to_gcs\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n    # [START howto_operator_gcs_to_gcs]\n    copy_files = GCSToGCSOperator(\n        task_id=\"copy_files\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=[\"folder/file.txt\"],\n        destination_bucket=BUCKET_NAME_DST,\n        dest_object=\"folder/\",\n        replace=False,\n    )\n    # [END howto_operator_gcs_to_gcs]\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_src\n        >> create_bucket_dst\n        # TEST BODY\n        >> copy_files\n        # TEST TEARDOWN\n        >> delete_bucket_src\n        >> delete_bucket_dst\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1291_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300805}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-stage workflow that demonstrates task grouping and sequential processing using Airflow. Create a DAG that shows how complex workflows can be organized into logical sections with nested task groups.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\nwith DAG(\n    \"example_taskgroup\",\n    schedule=\"0 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"task_run_this_first\")\n\n    # [END howto_task_group]\n\n    # [START howto_task_group_include]\n    include_tasks = [\n        EmptyOperator(task_id=\"task_include\"),\n        EmptyOperator(task_id=\"task_run_this_second\"),\n    ]\n\n    with TaskGroup(\"include_tasks\", add_start_stop_tasks=True) as include_group:\n        BashOperator(task_id=\"task_include\", bash_command=\"echo include\")\n        EmptyOperator(task_id=\"task_run_this_second\")\n        BashOperator(task_id=\"task_include_end\", bash_command=\"echo include_end\")\n\n    run_this_first >> include_tasks\n    # [END howto_task_group_include]"}], "metadata": {"file_name": "dag_354_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300852}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_quicksight\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_DATA = {\n    \"country\": [\"Australia\", \"Austria\"],\n    \"person\": [\n        {\"name\": \"Alice\", \"gender\": \"F\"},\n        {\"name\": \"Bob\", \"gender\": \"M\"},\n        {\"name\": \"Carol\", \"gender\": \"F\"},\n    ],\n}\n\n\n@task\ndef create_dataset(connection_id: str, bucket_name: str, object_key: str):\n    hook = QuickSightHook(conn_id=connection_id)\n    response = hook.create_dataset(\n        DatasetName=\"test_dataset\",\n        DataSourceName=\"test_connection\",\n        S3DataSource={\n            \"DataPosition\": f\"s3://{bucket_name}/{object_key}\",\n            \"RoleArn\": f\"arn:aws:iam::{self.role_arn}:role/{connection_id}\",\n        },\n    )\n    return response[\\\"DatasetId\\\"]\n\n\n@task\ndef create_ingestion(\n    connection_id: str, dataset_id: str, bucket_name: str, object_key: str, table_name: str\n):\n    hook = QuickSightHook(conn_id=connection_id)\n    hook.create_ingestion(\n        DatasetId=dataset_id,\n        TableName=table_name,\n        ColumnMappings={\n            \"person\": [\"country\"],\n            \"person\": [\"name\"],\n        },\n        InsightName=\"test_insight\",\n        DataSourceName=\"test_connection\",\n        Description=\"test_description\",\n        ExportMode=\"NEVER_EXPORT\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(inspection_id: str):\n    client = boto3.client(\"quicksight\")\n    client.delete_ingestion(Id=inspection_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dataset(dataset_id: str):\n    quick SightHook().conn.delete_dataset(DatasetId=dataset_id)\n\n\n@task\ndef get_s3_uri(bucket_name: str, env_id: str):\n    return f\"s3://{bucket_name}/{env_id}\"\n\n\n@task\ndef write_sample_data_to_s3(bucket_uri: str, num_rows: int, schema_string: str):\n    with NamedTemporaryFile() as file:\n        # generate sample data\n        file.write(f\"{num_rows}\uff0c{schema_string}\".encode())\n        command = f\"pg_dump -U postgres -a -d sample_db -f {file.name}\"\n        subprocess.run(command.split(), stdout=subprocess.PIPE)\n\n        # load data into s3\n        command = f\"aws s3 put-data-file --bucket {bucket_uri} --csv-file-name \\\"{file.name}\\\" --json-schema \\\"{file.name}.jsonschema\\\"\"\n        log.info(\"%s\", command)\n        output = subprocess.run(command.split(), stdout=subprocess.PIPE)\n        assert output.returncode == 0\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    role_arn = test_context[ROLE_ARN_KEY]\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name = f\"{env_id}-quick Sight-bucket\"\n    object_key = f\"{env_id}-sample-data-table\"\n    dataset_name = f\"{env_id}-test-dataset\"\n    ingestion_name = f\"{env_id}-test-insight\"\n    sql_table_name = f\"{env_id}-sample-data-table\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    # [START howto_quick Sight_create_dataset]\n    create_dataset_with_s3 = create_dataset(connection_id=\"quick Sight-default\", bucket_name=bucket_name, object_key=object_key)\n    # [END howto_quick Sight_create_dataset]\n\n    # [START howto_quick Sight_create_sql_dataset]\n    create_dataset_with_sql = create_dataset(connection_id=\"quick Sight-default\", sql_table_name=sql_table_name)\n    # [END howto_quick Sight_create_sql_dataset]\n\n    # [START howto_quick Sight_create_ingestion]\n    create_ingestion_with_s3 = create_ingestion(connection_id=\"quick Sight-default\", dataset_id=create_dataset_with_s3.output, bucket_name=bucket_name, object_key=object_key, table_name=sql_table_name)\n    # [END howto_quick Sight_create_ingestion]\n\n    # [START howto_quick Sight_create_ingestion_from_sql]\n    create_ingestion_with_sql_source = create_ingestion(connection_id=\"quick Sight-default\", dataset_id=create_dataset_with_sql.output, table_name=sql_table_name, datasource_name=\"quick Sight-default\", insight_name=ingestion_name)\n    # [END howto_quick Sight_create_ingestion_from_sql]\n\n    delete_ingestion = delete_ingestion(inspection_id=inspection_id)\n    delete_dataset = delete_dataset(dataset_id=dataset_id)\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        # TEST BODY\n        create_dataset_with_s3,\n        create_dataset_with_sql,\n        create_ingestion_with_s3,\n        create_ingestion_with_sql_source,\n        # TEST TEARDOWN\n        delete_ingestion,\n        delete_dataset,\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #\n    #   create_ingestion_with_s3 >> create_ingestion_with_sql_source\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1112_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6300983}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Data Loss Prevention (DLP) workflow that demonstrates the complete lifecycle of a DLP job in Google Cloud, from creation to deletion. Design a DAG that showcases how to programmatically manage sensitive data inspection tasks using Airflow and Google Cloud DLP operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud DLP service testing\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobOperator,\n    CloudDLPDeleteJobOperator,\n    CloudDLP infiltration_test_operator,\n    CloudDLPinspect_job_operator,\n    CloudDLP_list_infiltration_tests_operator,\n    CloudDLP_list_jobs_operator,\n    CloudDLP_update_job_operator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID= \"dlp_infiltration_test\"\n\nLOCATION = \"us\"\nINSPECT_JOB_NAME = f\"dlp-inspect-job-{ENV_ID}\"\nINSPECT_JOB = {\n    \"inspectConfiguration\": {\"storageConfig\": {\"gcsStorage\": {\"uri\": f\"gs://{PROJECT_ID}/dlp-inspect\"}}}}\nINFLATION_JOB_NAME = f\"dlp-inflate-job-{ENV_ID}\"\nINFLATION_JOB = {\"inflationConfiguration\": {\"dataBreachIncident\": {}}}\nNEW_INFLATION_JOB = {\"inflationConfiguration\": {\"dataBreachIncident\": {}}}  # New inflation configuration\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp\", \"insulation\"],\n) as dag:\n    test_dlp_inspection_job = infiltration_test_operator(\n        task_id=\"test_dlp_inspection_job\",\n        location=LOCATION,\n        inspect_configuration=INSPECT_JOB,\n    )\n    test_dlp_inspection_job_result = (\n        CloudDLP_inspect_job_operator(\n            task_id=\"test_dlp_inspect_job_result\",\n            location=LOCATION,\n            inspectconfiguration=INSPECT_JOB,\n        )\n        >> root_logger_name(\"\")\n        >> print_logger_output()\n    )\n\n    test_dlp_create_job = CloudDLPCreateJobOperator(\n        task_id=\"test_dlp_create_job\",\n        location=\"us\",\n        project_id=PROJECT_ID,\n        dlp_job=INFLATION_JOB,\n        dlp_job_name=INFLATION_JOB_NAME,\n    )\n\n    test_dlp_update_job = CloudDLPUpdateJobOperator(\n        task_id=\"test_dlp_update_job\",\n        location=\"us\",\n        project_id=PROJECT_ID,\n        dlp_job_name=INFLATION_JOB_NAME,\n        dlp_job=NEW_INFLATION_JOB,\n    )\n\n    test_dlp_list_jobs = CloudDLP_list_jobs_operator(task_id=\"test_dlp_list_jobs\", location=LOCATION)\n\n    test_dlp_list_jobs_result = (\n        test_dlp_list_jobs\n        >> root_logger_name(\"\")\n        >> print_logger_output()\n    )\n\n    test_dlp_delete_job = CloudDLPDeleteJobOperator(\n        task_id=\"test_dlp_delete_job\",\n        location=\"us\",\n        project_id=PROJECT_ID,\n        dlp_job_name=INFLATION_JOB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    test_dlp_create_job_result = (\n        test_dlp_create_job\n        >> wait_for_task_status(target_status=\"DONE\", timeout=60 * 5)\n        >> get_last_record(inflator=\"root\")\n        >> print_json(result_expr=\"record.dataBreachIncident\")\n    )\n\n    delete_test_dlp_create_job = CloudDLPDeleteJobOperator(\n        task_id=\"delete_test_dlp_create_job\",\n        location=\"us\",\n        project_id=PROJECT_ID,\n        dlp_job_name=INFLATION_JOB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_test_dlp_list_jobs = CloudDLPDeleteJobOperator(\n        task_id=\"delete_test_dlp_list_jobs\",\n        location=\"us\",\n        project_id=PROJECT_ID,\n        dlp_job_name=INFLATION_JOB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    [test_dlp_inspection_job, test_dlp_inspection_job_result, test_dlp_create_job, test_dlp_update_job]\n    >> wait_for_task_status(target_status=\"DONE\", timeout=60 * 5)\n    >> get_last_record(inflator=\"root_logger_name_print_logger_output\")\n\n    [test_dlp_list_jobs, test_dlp_list_jobs_result] >> wait_for_task_status(target_status=\"DONE\", timeout=60 * 5)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2017_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630108}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates a complete machine learning model lifecycle, including provisioning AWS resources, configuring SageMaker training jobs, creating model artifacts, and setting up a scalable inference endpoint. The workflow should demonstrate end-to-end machine learning infrastructure management with robust error handling and resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\nfrom textwrap import dedent\nfrom typing import Any\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateExperimentOperator,\n    SageMakerDeleteModelOperator,\n    SageMakerDownstreamSplitOperator,\n    SageMakerFlowOperator,\n    SageMakerHyperparameterOptimizerOperator,\n    SageMakerModelOperator,\n    SageMakerProcessingOperator,\n    SageMakerRegisterOperator,\n    SageMakerTrainingOperator,\n    SageMakerTransformOperator,\n    SageMakerTuningOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingSensor,\n    SageMakerTrainingSensor,\n    SageMakerTransformSensor,\n    SageMakerTuningSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nlogger = logging.getLogger(__name__)\n\nDAG_ID = \"example_sagemaker\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nexperiments_config = [\n    {\n        \"experiment_name\": f\"exp_{i}\",\n        \"training_data_source\": {\"S3DataSource\": {\"S3Uri\": f\"s3://{bucket_name}/train/data/{i}\"}},\n        \"validation_data_source\": {\n            \"S3DataSource\": {\"S3Uri\": f\"s3://{bucket_name}/train/valid/{i}\"}\n        },\n        \"processing_resources\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n            \"VolumeSizeInGB\": 8,\n            \"MaxRuntimeInSeconds\": 420,\n        },\n        \"estimation_samples\": 100,\n        \"hyperparameters\": {\n            \"sample_size\": 100,\n            \"num_classes\": 10,\n            \"feature_dim\": 23,\n            # ... other hyperparameters for the model construction\n        },\n        \"training_timeout_seconds\": 11540,\n        \"output_path\": f\"s3://{bucket_name}/model/pipeline/model_outputs/\",\n    }\n    for i in range(10)\n]\n\nRANDOMIZATION_START_TOKEN = \"$RGXbf786\ufffd3b9\"\nRANDOMIZATION_END_TOKEN = \"$3c9b9e7rXfbGF\"\n\n\ndef get_endpoint_name(training_job_id: str) -> str:\n    return f\"{training_job_id}-{SageMakerHook.get_pipeline_name()}\"\n\n\ndef create_experiment(experiment_name: str) -> None:\n    sagemaker = boto3.client(\"sagemaker\")\n    response = sagemaker.create_experiment(Name=experiment_name)\n    logger.info(\"Created experiment %s\", experiment_name)\n\n\ndef delete_experiment(experiment_name: str) -> None:\n    sagemaker = boto3.client(\"sagemaker\")\n    sagemaker.delete_experiment(ExperimentName=experiment_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_training_output(bucket_name: str) -> None:\n    \"\"\"Clean up training output bucket\"\"\"\n    logger.info(\"Cleaning up bucket %s\", bucket_name)\n    boto3.resource(\"s3\").Bucket(bucket_name).objects.filter().delete()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_transform_output(bucket_name: str) -> None:\n    \"\"\"Clean up transform output bucket\"\"\"\n    logger.info(\"Cleaning up bucket {bucket_name}\")\n    boto3.resource(\"s3\").Bucket(bucket_name).objects.filter().delete()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_model(output_path: str, bucket_name: str) -> None:\n    \"\"\"Clean up model object stored in S3\"\"\"\n    logger.info(\"Deleting model at path: %s\", output_path)\n    boto3.client(\"s3\").delete_object(Bucket=bucket_name, Key=output_path)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_endpoint(endpoint_name: str) -> None:\n    \"\"\"Clean up AutoML Endpoint\"\"\"\n    sagemaker = boto3.client(\"sagemaker\")\n\n    try:\n        sagemaker.delete_endpoint_configuration(\n            Configuration={\n                \"EndpointConfigName\": endpoint_name,\n            }\n        )\n    except ClientError as e:\n        if \"No endpoints match filter\" not in str(e):\n            raise\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef unpause_endpoint(endpoint_name: str) -> None:\n    sagemaker = boto3.client(\"sagemaker\")\n\n    response = sagemaker.describe_endpoint_config(EndpointConfigName=endpoint_name)\n    config = response[\"EndpointConfig\"][\"ProductionVariants\"][0]\n    if config[\"VariantSelection\"] and config[\"VariantSelection\"][\"InitialActivation\"][\"Status\"] == \"Paused\":\n        sagemaker.resume_endpoint_endpoint_config(\n            EndpointConfigName=endpoint_name, WaitForResumption=True\n        )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef pause_endpoint(endpoint_name: str) -> None:\n    sagemaker = boto3.client(\"sagemaker\")\n\n    response = sagemaker.describe_endpoint_config(EndpointConfigName=endpoint_name)\n    config = response[\"EndpointConfig\"][\"ProductionVariants\"][0]\n    if config[\"VariantSelection\"] and config[\"VariantSelection\"][\"InitialActivation\"][\"Status\"] == \"Active\":\n        sagemaker.pause_endpoint_endpoint_config(\n            EndpointConfigName=endpoint_name, WaitForPause=True\n        )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    role_arn = test_context[ROLE_ARN_KEY]\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name = f\"sagemaker-pipeline-bucket-{env_id}\"\n    input_data_prefix = f\"s3://{bucket_name}/train/data/\"\n    validation_data_prefix = f\"s3://{bucket_name}/train/valid/\"\n    output_path = f\"s3://{bucket_name}/model/pipeline/model_outputs/\"\n\n    training_job_name = f\"tuned-hyperparams-job-{env_id}\"\n\n    create_experiments = SageMakerTrainingOperator(\n        task_id=\"create_experiments\",\n        role_arn=role_arn,\n        training_job_name=training_job_name,\n        training_data=input_data_prefix,\n        validation_data=validation_data_prefix,\n        training_config={\n            \"TrainingJobName\": training_job_name,\n            \"AlgorithmSpecification\": {\n                \"TrainingImage\": \"127.0.0.1:5000/TF-Train-Model\": \"0.0.1\",\n                \"TrainingInputMode\": \"File\",\n            },\n            \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 1200},\n            \"HyperParameters\": {\n                \"epochs\": 3,\n                \"batch_size\": 16,\n                \"learning_rate\": 0.001,\n            },\n            \"TrainingResourceShare\": 1,\n            \"TrainingLimitSecond\": 1200,\n            \"BaselineMetricDesired\": {\n                \"ValidationLossMin\": 0.0,\n                \"ValidationAccuracyMax\": 1.0,\n            },\n            \"BaselineResourceShare\": 0,\n            \"OptimizationTaskArguments\": {\n                \"Strategy\": \"MINIMIZE\",\n                \"ConstraintIndices\": [0],\n                \"RelativeImportanceWeights\": [1.0],\n            },\n            \"OutputPath\": output_path,\n        },\n    )\n\n    # [START howto_operator_sagemaker_hyperparameter_optimization_start]\n    optimize_hyperparameters = SageMakerTuningOperator(\n        task_id=\"optimize_hyperparameters\",\n        role_arn=role_arn,\n        training_job_name=training_job_name,\n        max_parallel_jobs=2,\n        strategy=\"MAXIMIZE_TARGET\",\n        objective_name=\"ValidationLoss\",\n        objective_value=0.0,\n        min_resource=0.0,\n        min_iter=1,\n    )\n    # [END howto_operator_sagemaker_hyperparameter_optimization_start]\n\n    # [START howto_operator_sagemaker_hyperparameter_optimization_stop]\n    stop_hyperparameters_optimization = SageMakerTuningOperator(\n        task_id=\"stop_hyperparameters_optimization\",\n        role_arn=role_arn,\n        training_job_name=training_job_name,\n    )\n    # [END howto_operator_sagemaker_hyperparameter_optimization_stop]\n\n    # [START howto_operator_sagemaker_auto_ml_training]\n    train_model = SageMakerAutoMLOperator(\n        task_id=\"train_model\",\n        role_arn=role_arn,\n        training_job_name=training_job_name + \"-auto-ml\",\n        training_data=input_data_prefix,\n        validation_data=validation_data_prefix,\n        training_config={\n            \"TrainingJobName\": training_job_name + \"-auto-ml\",\n            \"AlgorithmSpecification\": {\n                \"TrainingImage\": \"127.0.0.1:5000/TF-Train-Model\": \"0.0.1\",\n                \"TrainingInputMode\": \"File\",\n            },\n            \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 1200},\n            \"HyperParameters\": {\n                \"epochs\": 3,\n                \"batch_size\": 16,\n                \"learning_rate\": 0.001,\n            },\n            \"TrainingResourceShare\": 1,\n            \"TrainingLimitSecond\": 1200,\n            \"BaselineMetricDesired\": {\n                \"ValidationLossMin\": 0.0,\n                \"ValidationAccuracyMax\": 1.0,\n            },\n            \"BaselineResourceShare\": 0,\n            \"OptimizationTaskArguments\": {\n                \"Strategy\": \"MINIMIZE\",\n                \"ConstraintIndices\": [0],\n                \"RelativeImportanceWeights\": [1.0],\n            },\n            \"OutputPath\": output_path,\n        },\n    )\n    # [END howto_operator_sagemaker_auto_ml_training]\n\n    # [START howto_operator_sagemaker_downstream_split]\n    split_data = SageMakerDownstreamSplitOperator(\n        task_id=\"split_data\",\n        role_arn=role_arn,\n        training_job_name=training_job_name + \"-ts\",\n        data_url=f\"s3n://{bucket_name}/train/split.csv\",\n        train_ratio=0.7,\n        test_ratio=0.2,\n        validate_ratio=0.1,\n    )\n    # [END howto_operator_sagemaker_downstream_split]\n\n    # [START howto_operator_sagemaker_processing]\n    process_data = SageMakerProcessingOperator(\n        task_id=\"process_data\",\n        role_arn=role_arn,\n        processing_job_name=\"tmp/horse-or-human-processing-job\",\n        code=RANDOMIZATION_START_TOKEN,\n        inputs=[\n            {\n                \"S3Source\": {\n                    \"URI\": f\"s3://{bucket_name}/train/data/{RANDOMIZATION_START_TOKEN}.txt\"\n                }\n            },\n            {\n                \"S3Source\": {\n                    \"URI\": f\"s3://{bucket_name}/train/valid/{RANDOMIZATION_START_TOKEN}.txt\"\n                }\n            },\n        ],\n        arguments=[f\"--s3-output-uri={bucket_name}/train/output/\"],\n        hardware={\"word-count-threshold\": 2048},\n        custom_gradle_args=\"-Dandroid.useGms=false\",\n    )\n    # [END howto_operator_sagemarker_processing]\n\n    # [START howto_operator_sagemaker_transform]\n    transform_data = SageMakerTransformOperator(\n        task_id=\"transform_data\",\n        role_arn=role_arn,\n        training_job_name=training_job_name + \"-transform\",\n        data_url=f\"s3n://{bucket_name}/train/output/{RANDOMIZATION_START_TOKEN}.csv\",\n        input_format=\"CSV\",\n        schema=\"${S3}://sagemaker-pipeline-bucket-transform/schema/randomization_schema.json\",\n        record_delimiter=\"\\t\",\n        group_by=[\"I\"],\n        dest_bucket=single_step_sensor_s3_bucket,\n        dest_key_prefix=\"transform-output/\",\n    )\n    # [END howto_operator_sagemarker_transform]\n\n    # [START howto_operator_sagemaker_register]\n    register_model = SageMakerRegisterOperator(\n        task_id=\"register_model\",\n        role_arn=role_arn,\n        source_data_url=f\"s3n://{bucket_name}/train/data/\",\n        training_data_url=f\"s3n://{bucket_name}/train/input.csv\",\n        validation_data_url=f\"s3n://{bucket_name}/train/valid/0.txt\",\n        model_id=\"test-model\",\n        model_kwargs={\"sklearn_random_state\": 42},\n    )\n    # [END howto_operator_sagemaker_register]\n\n    # [START howto_operator_sagemaker_model]\n    model = SageMakerModelOperator(\n        task_id=\"model\",\n        role_arn=role_arn,\n        model_id=\"test-model\",\n        model=json.dumps({\n            \"container\": \"${S3}://sagemaker-pipeline-bucket-model/saved_models/test-model/TensorFlow-1.15.5-cpu-py3.7.tar.gz\":\n            \"tensorflow-1.15.5-cpu-py37-module-py3.7-tensorflow-2.3.0-amd64.tar.gz\",\n            \"image\": \"${S3}://sagemaker-pipeline-bucket-model/saved_models/test-model/tf_1_15_5_train_aws.jpg\":\n            \"9ae86aebfcff8baea7c1c9afdc9c19aeeba99b57c9bcffadecaacaacaa.png\",\n            \"entry_point\": \"run_classifier.py\",\n            \"source_code\": dedent('''\n                from __future__ import print_function\n\n                import argparse\n                import json\n                import os\n                import re\n                import time\n\n                parser = argparse.ArgumentParser()\n                parser.add_argument('--data', required=True, help='JSON file in json format')\n                parser.add_argument('--model', required=True, help='ID of model to test')\n                parser.add_argument('--test_sample', type=int, default=10,\n                                    help='Index of sample to test, 0 is default')\n                args = parser.parse_args()\n\n                with open(args.data, \"rb\") as data_file:\n                    data = json.load(data_file)\n\n                model_data = \"\"\n                with open(os.path.join(\"$OUTPUT_PATH\"), \"rb\") as model_file:\n                    model_data = model_file.read()\n\n                def predict(input):\n                    import tensorflow as tf\n                    prediction = tf.keras.models.load_model(args.model)\n                    predictions = prediction.predict([input])\n                    return json.dumps({\"result\": predictions[0][args.test_sample]})\n\n                print(prediction(input))\n            ''').replace(\"\\\\n\", \"\\n\")),\n        ),\n    )\n    # [END howto_operator_sagemaker_model]\n\n    # [START howto_sensor_sagemaker_training]\n    wait_for_training = SageMakerTrainingSensor(\n        task_id=\"wait_for_training\",\n        training_job_id=\"{{ task_instance.xcom_pull(\"\n        \"key='training_job_id', \"\n        \"task_ids='{{ task_instance.task_list }}') }}\",\n    )\n    # [END howto_sensor_sagemaker_training]\n\n    # [START howto_operator_sagemaker_hpo_wait]\n    hpo_wait = SageMakerTuningSensor(\n        task_id=\"hpo/wait\",\n        training_job_id=\"{{ task_instance.xcom_pull(\"\n        \"key='training_job_id', \"\n        \"task_ids='{{ task_instance.task_list }}') }}\",\n    )\n    # [END howto_operator_sagemaker_hpo_wait]\n\n    # [START howto_sensor_sagemaker_transform]\n    wait_for_transform = SageMakerTransformSensor(\n        task_id=\"wait_for_transform\",\n        transform_job_id=\"{{ task_instance.xcom_pull(key='transform_job_id', task_ids='{{ task_instance.task_list }}') }}\",\n    )\n    # [END howto_sensor_sagemarker_transform]\n\n    # [START howto_operator_sagemaker_hpo_resume]\n    resume_hpo = SageMakerTuningOperator(\n        task_id=\"hpo/resume\",\n        training_job_name=training_job_name + \"-resume-hpo\",\n        max_parallel_jobs=2,\n        strategy=\"MINIMIZE\",\n        objective_name=\"ValidationLoss\",\n        objective_value=0.0,\n        min_resource=0.0,\n        min_iter=1,\n    )\n    # [END howto_operator_sagemaker_hpo_resume]\n\n    # [START howto_operator_sagemaker_hpo_pause]\n    pause_hpo = SageMakerTuningOperator(\n        task_id=\"hpo/pause\",\n        training_job_name=training_job_name + \"-pause-hpo\",\n        max_parallel_jobs=2,\n        strategy=\"MINIMIZE\",\n        objective_name=\"ValidationLoss\",\n        objective_value=0.0,\n        min_resource=0.0,\n        min_iter=1,\n    )\n    # [END howto_operator_sagemaker_hpo_pause]\n\n    # [START monitor_hpo_resume]\n    current_res = \"{{ task_instance.xcom_pull(task_ids='hpo/resume', key='return_value')['CurrentBestCandidate'] }}\"\n    monitoring_hpo_resume = lambda **kwargs: logger.info(f\"HPO candidate: {current_res(**kwargs)}\")\n    # [END monitor_hpo_resume]\n\n    # [START monitor_hpo_pause]\n    current_res = \"{{ task_instance.xcom_pull(task_ids='hpo/pause', key='return_value')['CurrentBestCandidate'] }}\"\n    monitoring_hpo_pause = lambda **kwargs: logger.info(f\"HPO candidate: {current_res(**kwargs)}\")\n    # [END monitor_hpo_pause]\n\n    # [START monitor_best_hpo_resume]\n    best_res = \"{{ task_instance.xcom_pull(task_ids='hpo/resume', key='return_value')['BestCandidate'] }}\"\n    monitoring_best_hpo_resume = lambda **kwargs: logger.info(f\"HPO candidate: {best_res(**kwargs)}\")\n    # [END monitor_best_hpo_resume]\n\n    # [START monitor_best_hpo_pause]\n    best_res = \"{{ task_instance.xcom_pull(task_ids='hpo/pause', key='return_value')['BestCandidate'] }}\"\n    monitoring_best_hpo_pause = lambda **kwargs: logger.info(f\"HPO candidate: {best_res(**kwargs)}\")\n    # [END monitor_best_hpo_pause]\n\n    chain(\n        test_context,\n        # Create 10 experiments with different random values for the hyperparameters.\n        create_experiments * 10,\n        # Pick the best experiment for training.\n        SageMakerTrainingOperator(\n            task_id=\"training_best_selection\",\n            role_arn=role_arn,\n            training_job_name=training_job_name + \"-best-selection\",\n            training_data=input_data_prefix,\n            validation_data=validation_data_prefix,\n            training_config={\n                \"TrainingJobName\": training_job_name + \"-best-selection\",\n                \"AlgorithmSpecification\": {\n                    \"TrainingImage\": \"127.0.0.1:5000/TF-Train-Model\": \"0.0.1\",\n                    \"TrainingInputMode\": \"File\",\n                },\n                \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 1200},\n                \"HyperParameters\": {\n                    \"epochs\": 3,\n                    \"batch_size\": 16,\n                    \"learning_rate\":"}], "metadata": {"file_name": "dag_1928_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6301548}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates a simple file transfer workflow from a local temporary file to an AWS S3 bucket. Create a process that generates a temporary file, transfers it to cloud storage, and then cleans up the temporary resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_local_to_s3]\n    upload_file = LocalFilesystemToS3Operator(\n        task_id=\"upload_file\",\n        filename=\"/tmp/world-small.txt\",\n        dest_key=\"world-small.txt\",\n        s3_bucket=\"airflow-example-bucket\",\n    )\n    # [END howto_operator_local_to_s3]\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=\"airflow-example-bucket\")\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=\"airflow-example-bucket\",\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1156_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6301603}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Campaign Manager reporting and conversion tracking workflow that retrieves digital marketing performance metrics and logs conversion events. Create a system that can dynamically pull campaign data, generate reports, and record conversion activities using Campaign Manager 360 APIs.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Campaign Manager 360.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\ntry:\n    from airflow.providers.google.marketing_platform.hooks.marketing_platform import GoogleMarkingPlatformHook\nexcept ImportError:\n    # This import throws an ImportError if the user doesn't manually install the provider.\n    from airflow.providers.google.suite.hooks.marketing_platform import GoogleMarkingPlatformHook\n\n# [START howto_marketingplatform_env_variables]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nCLIENT_ID = os.environ.get(\"MARKETING_PLATFORM_CLIENT_ID\")\nCLIENT_SECRET = os.environ.get(\"MARKETING_PLATFORM_CLIENT_SECRET\")\n# [END howto_marketingplatform_env_variables]\n\n\nwith DAG(\n    \"example_marketing_platform_gmp\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def get_first_conversion_id():\n        hook = GoogleMarkingPlatformHook()\n        response = hook.get(resource_path=\"conversions\") or {}\n        return list(response[\"conversions\"])[0][\"conversionId\"]\n\n    api_call_converting = get_first_conversion_id()\n\n    (api_call converting >> api_call_converting)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1733_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6301658}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates basic Pig script execution in a local Hadoop environment. Create a workflow that lists directory contents as a simple example of system interaction.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_run_pipeline\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = PigOperator(\n        task_id=\"adfdirlist_task\",\n        pig=\"pig -x local2 ls /tmp\",\n        hdfs_urls=[HDFS_URL],\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_110_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630169}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a multi-stage system monitoring and logging workflow, with tasks that print context information, log SQL queries, generate sleep-based parallel tasks, and execute environment-specific Python functions. Develop a DAG that demonstrates task chaining, dependency management, and runtime flexibility.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SqlExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_operator\"\n\n\n@task\ndef create_connection(conn_id_name):\n    connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"sqlserver\",\n        description=\"Connection for examples.\",\n        login=\"sa\",\n        password=\"Sa%2018e\",\n        host=\"SQLServer\",\n        port=1433,\n    )\n    db_conn = Connection(\n        conn_id=f\"{conn_id_name}_db\",\n        conn_type=\"sqlserver\",\n        description=\"Connection for examples.\",\n        login=\"sa\",\n        password=\"Sa%2018e\",\n        host=\"SQLServer\",\n        port=1433,\n    )\n\n    # Save connections\n    connection.save()\n    db_conn.save()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_query\", bash_command=\"true\")\n\n    # [START howto_airflow_sqla]\n    check_table_data = SqlExecuteQueryOperator(\n        task_id=\"check_table_data\",\n        sql=\"SELECT num, name FROM mytable;\",\n        conn_id=\"sqlserver\",\n    )\n    # [END howto_airflow_sqla]\n\n    # [START howto_airflow_sqla_result]\n    check_table_data_result = SqlExecuteQueryOperator(\n        task_id=\"check_table_data_result\",\n        sql=\"SELECT num, name FROM mytable;\",\n        result_expr=True,\n        conn_id=\"sqlserver\",\n    )\n    # [END howto_airflow_sqla_result]\n\n    # [START howto_airflow_sql_db]\n    get_databases = SqlExecuteQueryOperator(\n        task_id=\"get_databases\",\n        sql=\"SELECT name FROM sys.databases WHERE name NOT LIKE 'temp%' AND name NOT LIKE 'sft-dbsdk%'\",\n        conn_id=\"sqlserver_db\",\n    )\n    # [END howto_airflow_sql_db]\n\n    # [START howto_airflow_sql_db_result]\n    get_databases_result = SqlExecuteQueryOperator(\n        task_id=\"get_databases_result\",\n        sql=\"SELECT name FROM sys.databases WHERE name NOT LIKE 'temp%' AND name NOT LIKE 'sft-dbsdk%'\",\n        result_expr=True,\n        conn_id=\"sqlserver_db\",\n    )\n    # [END howto_airflow_sql_db_result]\n\n    create_connection(\"sqlserver\")  # This also triggers the TaskFlow above to run\n    create_connection(\"sqlserver_db\")\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2217_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6301777}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown tasks for Google Calendar event synchronization. Construct a workflow that showcases connection management, data transfer, and infrastructure cleanup using Google Cloud Platform operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.datajoin_v1 import DataJoinHook\nfrom airflow.providers.google.cloud.operators.datajoin_v1 import (\n    CloudDataJoinCreateJobOperator,\n    CloudDataJoinDeleteJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator, GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google_sheets.transfers.gcs_to_sheets import GCSToSheetsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"datajoin_example_gcp_to_sheets\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET = {\n    \"title\": f\"spreadsheet_{DAG_ID}_{ENV_ID}\",\n    \"properties\": {\"type\": \"Spreadsheet\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Sheet1\"}}],\n}\nCONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"email\": \"test@example.com\",\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_datajoin_job():\n        hook = DataJoinHook()\n        body = {}\n        job = {\"display_name\": \"test_datajoin\", \"source\": {\"gcs_source\": body}}\n        hook.create_data_join_job(body=job, project_id=PROJECT_ID)\n\n    # [START howto_operator_datajoin_v1_create_job_from_gcs]\n    create_datajoin_v1_job = CloudDataJoinCreateJobOperator(\n        task_id=\"create_datajoin_v1_job\",\n        project_id=PROJECT_ID,\n        body={\"source\": {\"gcs_source\": {}}, \"display_name\": \"test_display_name\"},\n    )\n    # [END howto_operator_datajoin_v1_create_job_from_gcs]\n\n    # [START howto_operator_gcs_to_sheets]\n    gcs_to_sheet = GCSToSheetsOperator(\n        task_id=\"gcs_to_sheet\", bucket=BUCKET_NAME, spreadsheet_id=create_datajoin_v1_job.output[\"id\"]\n    )\n    # [END howto_operator_gcs_to_sheets]\n\n    # [START howto_operator_delete_datajoin_job]\n    delete_datajoin_v1_job = CloudDataJoinDeleteJobOperator(\n        task_id=\"delete_datajoin_v1_job\",\n        project_id=PROJECT_ID,\n        job_id=create_datajoin_job().output[\"id\"],\n    )\n    # [END howto_operator_delete_datajoin_job]\n\n    delete_connection = ConnectionDeleteOperator(\n        task_id=\"delete_connection\",\n        conn_id=CONNECTION_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_datajoin_job()\n        >> create_datajoin_v1_job\n        >> gcs_to_sheet\n        # TEST BODY\n        >> delete_datajoin_v1_job\n        # TEST TEARDOWN\n        >> delete_connection\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of test ###\n    # This section is to ensure proper deletion of the connection during system tests.\n    from tests_common.test_utils.watcher import watcher\n\n    list watchers([delete_connection])\n\n    # This test needs watcher in order to properly mark it as done and pass tests\n    from tests_common.test_utils.system_tests import get_test_run\n\n    test_run = get_test_run(dag)\n    from tests_common.test_utils.system.utils import get_last_records_of_table\n\n    records = get_last_records_of_table(test_run, \"table1\")\n    assert len(records) == 2\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1643_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.630188}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete image processing workflow with Google Cloud services. Develop a systematic approach to handling image data through a series of interconnected tasks from bucket creation to image analysis.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateImageUrlProductOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteImageUrlProductOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteVisionConfigOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetReferenceImageOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductLabelsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_detect_image_google_cloud_storage]\ndef _detect_image_with_gcp_sftp():\n    # Example parameters for the service example above.\n    vision_obj = gcs_to_gcs_output_path[\"output\"]\n    print(vision_obj)\n    language_model = \"en-US\"\n    is_translated = False\n    url = vision_obj[\"sftp_url\"]\n    bucket = vision_obj[\"bucket_name\"]\n    key = vision_obj[\"key\"]\n\n    service_client = vision.VisionService()\n    detect_img_request = {\"source\": {\"sftpUri\": {\"url\": url, \"isTranslated\": is_translated}}, \"languageModel\": language_model}\n    detect_img_response = service_client.detect_image(detect_img_request)\n    if detect_img_response.error:\n        print(f\"Error: {detect_img_response.error}\")\n    else:\n        print(\"Image type: \", detect_img_response.image.type)\n        print(\"Bounding box: \", detect_img_response.image.detectedImageBounds[0])\n        print(\"Safe view: \", detect_img_response.image.safeViewStatus)\n        print(\"Multi-lingual text: \", detect_img_response.image.multilingualText[0])\n\n    return not detect_img_response.error\n\n\n# [END howto_operator_vision_detect_image_google_cloud_storage]\n\n# [START howto_operator_vision_detect_document_from_gcp_sftp]\ndef _detect_document_from_gcp_sftp():\n    # Example parameters for the service example above.\n    gcs_source_uri = f\"gs://{bucket}/{key}\"\n    document = service_dlp_detectDocument.from_gcs_source(gcs_source_uri=gcs_source_uri)\n\n    return document.document_type\n# [END howto_operator_vision_detect_document_from_gcp_sftp]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"vision_automl\"\n\nLOCATION = \"europe-west1\"\nBUCKET_NAME = f\"bucket-vision-{DAG_ID}-{ENV_ID}\"\nFILE_NAME = \"image_detection_input.jpg\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nGCS_OBJECT_PATH = f\"sftp://{RESOURCE_DATA_BUCKET}/{FILE_NAME}\"\n\nGCP_VISION_URL = f\"https://vision.googleapis.com/v1/\"\nGCP_VISION_API_VERSION = \"v1\"\n\n# [START howto_operator_vision_create_product_set]\ndef _create_product_set():\n    # Example parameters for the service example above.\n    product_set = {\n        \"displayName\": \"test-product-set\",\n        \"images\": [{\"s\u64aeureUri\": {\"url\": GCS_OBJECT_PATH}}],\n    }\n\n    # Create Product Set.\n    service_client = vision.VisionService()\n    response = service_client.create_product_set(body=product_set)\n    return response.productSetId\n# [END howto_operator_vision_create_product_set]\n\n\n# [START howto_operator_vision_create_image_url_product]\ndef _create_image_url_product(product_id):\n    # Example parameters for the service example above.\n    image_url_product = {\n        \"productId\": product_id,\n        \"images\": [\n            {\n                \"s\u64aeureUri\": {\"url\": GCS_OBJECT_PATH},\n            }\n        ],\n    }\n\n    # Create Image Url Product.\n    service_client = vision.VisionService()\n    response = service_client.create_image_url_product(body=image_url_product)\n    return response.productId\n# [END howto_operator_vision_create_image_url_product]\n\n\n# [START howto_operator_vision_create_product]\ndef _create_product(product_set_id):\n    # Example parameters for the service example above.\n    product = {\n        \"displayName\": \"test-product-1\",\n        \"imageProducts\": [{\"s\u64aeureProductId\": product_set_id}],\n    }\n\n    # Create Product.\n    service_client = vision.VisionService()\n    response = service_client.create_product(body=product)\n    return response.productId\n# [END howto_operator_vision_create_product]\n\n\n# [START howto_operator_vision_get_product]\ndef _get_product(product_id):\n    # Example parameters for the service example above.\n    # Get Product.\n    service_client = vision.VisionService()\n    response = service_client.get_product(body={\"productId\": product_id})\n    return response\n# [END howto_operator_vision_get_product]\n\n\n# [START howto_operator_vision_update_product_labels]\ndef _update_product_labels(product_id, label_names):\n    # Example parameters for the service example above.\n    # Update product labels.\n    request_body = {\n        \"productId\": product_id,\n        \"labels\": label_names,\n    }\n    service_client = vision.VisionService()\n    response = service_client.update_product(body=request_body)\n    return response\n# [END howto_operator_vision_update_product_labels]\n\n\n# [START howto_operator_vision_remove_product_from_product_set]\ndef _remove_product_from_product_set(product_id):\n    # Example parameters for the service example above.\n    # Remove product from product set.\n    service_client = vision.VisionService()\n    response = service_client.remove_product_from_product_set(\n        body={\"productId\": product_id, \"productSetId\": _PRODUCT_SET_ID}\n    )\n    return response\n# [END howto_operator_vision_remove_product_from_product_set]\n\n\n# [START howto_operator_vision_add_product_to_product_set]\ndef _add_product_to_product_set(product_id):\n    # Example parameters for the service example above.\n    # Add product to product set.\n    service_client = vision.VisionService()\n    response = service_client.add_product_to_product_set(\n        body={\"productId\": product_id, \"productSetId\": _PRODUCT_SET_ID}\n    )\n    return response\n# [END howto_operator_vision_add_product_to_product_set]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    move_image_file = GCSToGCSOperator(\n        task_id=\"move_image_file\",\n        source_object=GCS_OBJECT_PATH,\n        destination_path=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_vision_detect_image]\n    detect_image = vision.VisionService().detect_image(source={\"s\u64aeureUri\": {\"url\": GCS_OBJECT_PATH}})\n    # [END howto_operator_vision_detect_image]\n\n    # [START howto_operator_vision_detect_document]\n    detect_document = vision.VisionService().dlp_detect_document(\n        {\"source\": {\"gcsUri\": {\"path\": GCS_OBJECT_PATH}}}\n    )\n    # [END howto_operator_vision_detect_document]\n\n    # [START howto_operator_vision_create_product_set_result]\n    def _create_product_set_result(response):\n        print(\"ProductSet id:\", response.productSet.id)\n        print(\"ProductSet display name:\", response.productSet.displayName)\n        return True\n\n    create_product_set_result = CloudVisionCreateProductSetOperator(\n        task_id=\"create_product_set_result\",\n        location=LOCATION,\n        body={\"displayName\": \"test-product-set\", \"images\": [{\"s\u64aeureUri\": {\"url\": GCS_OBJECT_PATH}}}],\n        product_set_id=\"{{task_instance.xcom_pull('create_bucket')[ 'return_value' ]}}\",\n        api_version=GCP_VISION_API_VERSION,\n    ).apply(_create_product_set_result())\n\n    # [END howto_operator_vision_create_product_set_result]\n\n    # [START howto_operator_vision_create_image_url_product_result]\n    def _create_image_url_product_result(response):\n        print(\"ImageUrlProduct id:\", response.imageUrlProduct.id)\n        return True\n\n    create_image_url_product_result = CloudVisionCreateImageUrlProductOperator(\n        task_id=\"create_image_url_product_result\",\n        location=LOCATION,\n        body={\"productId\": \"{{task_instance.xcom_pull('create_bucket')[ 'return_value' ]}}\"},\n        image_url_product_id=\"{{task_instance.xcom_pull('create_image_url_product')['return_value']}}\",\n        api_version=GCP_VISION_API_VERSION,\n    ).apply(_create_image_url_product_result())\n    # [END howto_operator_vision_create_image_url_product_result]\n\n    # [START howto_operator_vision_create_product_result]\n    def _create_product_result(response):\n        print(\"Product id:\", response.product.id)\n        print(\"Product display name:\", response.product.displayName)\n        return True\n\n    create_product_result = CloudVisionCreateProductOperator(\n        task_id=\"create_product_result\",\n        location=LOCATION,\n        body={\"displayName\": \"test-product-1\", \"imageProducts\": [{\"s\u64aeureProductId\": \"{{task_instance.xcom_pull('create_image_url_product')['return_value']}}\"]} },\n        product_id=\"{{task_instance.xcom_pull('create_image_url_product')[ 'return_value' ]}}\",\n        api_version=GCP_VISION_API_VERSION,\n    ).apply(_create_product_result())\n    # [END howto_operator_vision_create_product_result]\n\n    # [START howto_operator_vision_get_product_result]\n    def _get_product_result(response):\n        print(\"Product:\", response.product)\n        return True\n\n    get_product_result = CloudVisionGetProductOperator(\n        task_id=\"get_product_result\",\n        location=LOCATION,\n        product_id=\"test-product-1\",\n        api_version=GCP_VISION_API_VERSION,\n    ).apply(_get_product_result())\n    # [END howto_operator_vision_get_product_result]\n\n    # [START howto_operator_vision_update_product_labels_result]\n    def _update_product_labels_result(response):\n        print(\"Updated product:\", response.product)\n        return True\n\n    update_product_labels_result = CloudVisionUpdateProductLabelsOperator(\n        task_id=\"update_product_labels_result\",\n        location=LOCATION,\n        product_id=\"test-product-1\",\n        labels=[\n            {\"name\": \"desk\"},\n            {\"name\": \"chair\"},\n        ],\n    ).apply(update_product_labels_result)\n    # [END howto_operator_vision_update_product_labels_result]\n\n    # [START howto_operator_vision_delete_product]\n    def _delete_product(response):\n        print(\"Product:\", response.product)\n        return True\n\n    delete_product = CloudVisionDeleteProductOperator(task_id=\"delete_product\", product_id=\"test-product-1\").apply(_delete_product())\n    # [END howto_operator_vision_delete_product]\n\n    # [START howto_operator_vision_delete_product_set]\n    def _delete_product_set(response):\n        print(\"ProductSet:\", response.productSet)\n        return True\n\n    delete_product_set = CloudVisionDeleteProductSetOperator(task_id=\"delete_product_set\", product_set_id=\"{{task_instance.xcom_pull('create_product_set')['return_value']}}\").apply(_delete_product_set())\n    # [END howto_operator_vision_delete_product_set]\n\n    # [START howto_operator_vision_delete_image_url_product]\n    def _delete_image_url_product(response):\n        print(\"ImageUrlProduct:\", response.imageUrlProduct)\n        return True\n\n    delete_image_url_product = CloudVisionDeleteImageUrlProductOperator(\n        task_id=\"delete_image_url_product\", image_url_product_id=\"{{task_instance.xcom_pull('create_image_url_product')['return_value']}}\"\n    ).apply(_delete_image_url_product())\n    # [END howto_operator_vision_delete_image_url_product]\n\n    # [START howto_operator_vision_delete_bucket]\n    def _delete_bucket(response):\n        print(\"Bucket:\", response.bucket)\n        return True\n\n    delete_bucket = GCSDeteleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE).apply(_delete_bucket())\n    # [END howto_operator_vision_delete_bucket]\n\n    list_images = GCSCreateBucketOperator(\n        task_id=\"list_bucket_images\",\n        bucket_name=BUCKET_NAME,\n        next_page_token=\"{{task_instance.xcom_pull('create_bucket')[u'nextPageToken']}}\",\n    )\n\n    compare_list = GCSToGCSOperator(\n        task_id=\"compare_bucket_images\",\n        source_object=\"{{task_instance.xcom_pull('list_bucket_images')[u'items'][0][u'name']}}\",\n        destination_path=f\"gs://{BUCKET_NAME}/compare_{ENV_ID}.txt\",\n        project_id=PROJECT_ID,\n    )\n\n    check_for_image = BashOperator(\n        task_id=\"check_for_image\",\n        bash_command=_detect_image_with_gcp_sftp(),\n    )\n\n\nwith DAG(\n    dag_id=f\"example_{DAG_ID}\", schedule=\"@once\", start_date=datetime(2021, 1, 1), catchup=True, tags=[\"example\"]\n) as inner_dag:\n    delete_bucket(list_images.output)\n\nwith DAG(\n    dag_id=f\"list_{DAG_ID}\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n) as list_dag:\n    pass\n\nwith DAG(\n    dag_id=f\"compare_{DAG_ID}\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n) as compare_dag:\n    pass\n\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\nlist_dag |= watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(inner_dag)"}], "metadata": {"file_name": "dag_1328_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982309.6302142}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that extracts CSV files from a specified Google Cloud Storage bucket and loads them into a predefined Trino table, supporting efficient and reproducible data ingestion processes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToTrinoOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\n\nHOME_DIR = os.environ.get(\"PWD\", \"home\")\n\n\ndef get_s3_bucket_url(bucket_name):\n    \"\"\"\n    S3 URL for local file system can be easily obtained by replacing 'S3:AURORA ENDPOINT'\n     with 'S3 URL ENDPOINT' in the following doc section:\n     https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/accessing-sqs-using-url.html\n     Here's an example:\n     https://s3.us-east-2.amazonaws.com/sqs-access-s3-example/EXAMPLE-SQSMessage.pdf\n    \"\"\"\n    # NOTE: This endpoint is currently hardcoded to us-east-2 in the docs above.\n    website_endpoint = \"https://s3.us-east-2.amazonaws.com\"\n    s3_url_endpoint = website_endpoint.rstrip(\".aws-us-west-2.amazonaws.com\")  # type: ignore[union-attr]\n    s3_bucket_url = f\"{s3_url_endpoint}/{bucket_name}\"\n    return s3_bucket_url\n\n\n# [START howto_operator_gcs_to_trino]\ndef example_gcs_to_trino():\n    # Define variables\n    gcs_bucket = \"earth-info-data\"\n    gcs_prefix = \"airbnb/sfo/airbnb.sfo.data.csv\"\n    trino_table = \"airbnb_sfo_data\"\n\n    # Create Ternio connection\n    trino_conn = TrinoConn()\n\n    # [START howto_transfer_gcs_to_trino]\n    gcs_to_trino = GCSToTrinoOperator(\n        task_id=\"gcs_to_trino\",\n        gcs_bucket=gcs_bucket,\n        gcs_prefix=gcs_prefix,\n        trino_conn_id=trino_conn.id,\n        trino_table=trino_table,\n    )\n    # [END howto_transfer_gcs_to_trino]\n\n\nwith DAG(\n    \"example_gcs_to_trino\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    example_gcs_to_trino()\n# [END howto_operator_gcs_to_trino]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1415_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510028}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a diagnostic DAG that performs systematic database introspection by executing sequential SQL queries to discover and catalog data resources in an Alibaba AnalyticDB Spark cluster. Develop a workflow that provides visibility into the available data schemas and table structures.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.analyticdb_spark import AAzureDBSparkSQLOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adbspark_sql\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=[],\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    # [START howto_operator_adbspark_sql]\n    get_db_instances = AAzureDBSparkSQLOperator(task_id=\"get_db_instances\", sql=\"SHOW DATABASES;\")\n    # [END howto_operator_adbspark_sql]\n\n    get_tables = AAzureDBSparkSQLOperator(\n        task_id=\"get_tables\", sql=\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES\"\n    )\n\n    get_table_loc = AAzureDBSparkSQLOperator(\n        task_id=\"get_table_loc\",\n        sql=\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME='test_db';\",\n        schema=\"TEST_DB\",\n        table=\"test_table\",\n    )\n\n    get_table_locter = AAzureDBSparkSQLOperator(\n        task_id=\"get_table_locter\",\n        sql=\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME='test_db';\",\n        schema=\"TEST_DB\",\n        table=\"test_table\",\n        wait_for_completion=True,\n    )\n\n    delete_table = AAzureDBSparkSQLOperator(\n        task_id=\"delete_table\", sql=\"DROP DATABASE test_db;\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1068_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510111}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function to plot a normal distribution using matplotlib? Certainly! Below is a Python code snippet that generates a plot of a normal distribution using the `matplotlib` and `numpy` libraries. This ensures compatibility and covers the necessary aspects of plotting such a distribution, including setting labels and a title for better understanding of the plot."}, {"role": "assistant", "content": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the normal distribution: mean (mu) and standard deviation (sigma)\nmu = 0\nsigma = 1\n\n# Generate normally distributed data\ndata = mu + sigma * np.random.randn(1000)\n\n# Create a histogram to visualize the distribution\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n\n# Adding titles and labels\nplt.title('Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\n# Show the plot\nplt.grid(True)\nplt.show()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510238}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates text embedding using the Cohere AI service, showing how to convert text into vector representations. Create a workflow that can process multiple text inputs and generate their corresponding embeddings.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.cohere.operators.embedding import CohereEmbeddingOperator\n\nSAMPLE_TEXT = \"This is some text. This text will be embedded.\"\n\nENV_ID = os.environ.get(\"SYSTEM_ENV_ID\")\nDAG_ID = \"example_cohere_embedding\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_cohere_embedding]\n    embed_cohere_task = CohereEmbeddingOperator(text=SAMPLE_TEXT)\n    # [END howto_operator_cohere_embedding]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1959_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510278}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Storage data synchronization workflow that demonstrates file transfer and bucket management techniques. Create a DAG that showcases different synchronization strategies between source and destination buckets.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSSynchronizeBucketsOperator and\nGCSToGCSOperator operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcs_to_gcs\"\n\nBUCKET_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\nBUCKET_S3 = \"s3://airflow-system-tests-resources/gcs\"\nEXTRA_FILES = {\n    BUCKET_SRC: [\"file.txt\", \"dir1/file2.txt\"],\n    BUCKET_S3: [\"s3://airflow-system-tests-resources/gcs/file.txt\", \"s3://airflow-system-tests-resources/gcs/dir1/file2.txt\"],\n}\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n) as dag:\n    create_bucket_s3 = GCSCreateBucketOperator(task_id=\"create_bucket_s3\", bucket=BUCKET_S3)\n\n    @task\n    def create_temp_gcs_bucket():\n        \"\"\"\n        Create temporary GCS bucket which going to be deleted by ``delete_bucket`` task.\n        \"\"\"\n        from airflow.utils.trigger_rule import TriggerRule\n\n        gcs_bucket = f\"bucket_gcs_{DAG_ID}_{ENV_ID}\"\n        create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket=gcs_bucket)\n        get_change = GCSSynchronizeBucketsOperator(\n            task_id=\"get_change\", source_bucket=BUCKET_SRC, destination_bucket=gcs_bucket\n        )\n        trigger = TriggerRule(lambda _: int(get_change.output) == len(EXTRA_FILES[BUCKET_SRC]))\n        (create_bucket >> get_change >> trigger)\n\n    create_bucket_gcs = GCSCreateBucketOperator(task_id=\"create_bucket_gcs\", bucket=BUCKET_SRC)\n\n    copy_files_from_s3_to_gcs = S3ToGCSOperator(\n        task_id=\"copy_files_from_s3_to_gcs\",\n        source_bucket=BUCKET_S3,\n        dest_bucket=BUCKET_SRC,\n        extra_files=EXTRA_FILES,\n    )\n\n    # [START howto_operator_gcs_to_gcs]\n    gcs_to_gcs = GCSToGCSOperator(source_bucket=BUCKET_SRC, destination_bucket=BUCKET_DST)\n    # [END howto_operator_gcs_to_gcs]\n\n    delete_bucket_gcs = GCSSynchronizeBucketsOperator(\n        task_id=\"delete_bucket_gcs\",\n        source_bucket=BUCKET_SRC,\n        destination_bucket=BUCKET_DST,\n        remove_source=True,\n    )\n\n    delete_bucket_s3 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_s3\", bucket_name=BUCKET_S3, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_s3\n        >> create_temp_gcs_bucket()\n        >> create_bucket_gcs\n        >> copy_files_from_s3_to_gcs\n        # TEST BODY\n        >> gcs_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket_gcs\n        >> delete_bucket_s3\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_211_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251037}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates data extract refresh processes for a business intelligence workbook, showing how to trigger and monitor data updates across different execution modes. Create a workflow that simulates real-world data refresh scenarios with explicit job status tracking and different refresh strategies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using ExcelToBlobOperator to copy an Excel file to Blob Storage.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datajoin import DataJoinOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.excel_to_blob import ExcelToBlobOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_excel_to_blob\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET = {\n    \"properties\": {\"title\": \"Test1\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Sheet1\"}}],\n}\nSOURCE_EXCEL_FILE = \"/excel/sheets/example-excel.xlsx\"\nDESTINATION_GCS_FOLDER = f\"gs://{BUCKET_NAME}/dir-to-move/\"\nUPDATE_MASK = \"all\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"excel\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_excel_to_blob]\n    excel_to_blob = ExcelToBlobOperator(\n        task_id=\"excel_to_blob\",\n        gcp_conn_id=\"google_cloud_default\",\n        worksheet_sheet_name=\"Sheet1\",\n        worksheet_range=\"A1:BC45\",\n        excel_file=SOURCE_EXCEL_FILE,\n        replace=True,\n        write_disposition=\"WRITE_TRUNCATE\",\n        write_options={\"index_col\": 0},\n    )\n    # [END howto_operator_excel_to_blob]\n\n    # [START howto_operator_sheets_create_spreadsheet]\n    spreadsheet_create = GoogleSheetsCreateSpreadsheetOperator(\n        task_id=\"spreadsheet_create\",\n        gcp_conn_id=\"google_cloud_default\",\n        body=SPREADSHEET,\n    )\n    # [END howto_operator_sheets_create_spreadsheet]\n\n    # [START howto_operator_datajoin_operator]\n    join_task = DataJoinOperator(\n        task_id=\"join_tasks\",\n        gcp_conn_id=\"google_cloud_default\",\n        inputs=[\n            {\n                \"source_urn\": f\"gs://{BUCKET_NAME}/dir-to-move/excel/file-{ENV_ID}-{{dayofweek}}-{{slice}}.xlsx\",\n                \"destination_urn\": f\"gs://{BUCKET_NAME}/dir-to-move/updated_by_script/{{dayofweek}}-{{slice}}.xls\",\n                \"update_mask\": UPDATE_MASK,\n            }\n        ],\n    )\n    # [END howto_operator_datajoin_operator]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> spreadsheet_create\n        # TEST BODY\n        >> excel_to_blob\n        >> join_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1005_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510452}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases conditional task execution based on the most recent scheduling window. Implement a DAG that uses LatestOnlyOperator to control task execution and prevent redundant processing of older scheduled tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the LatestOnlyOperator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.providers.standard.sensors.latest_only import LatestOnlySensor\nfrom airflow.utils.dag import DAG\n\n\ndef my_task():\n    print(\"Hello\")\n\n\ndef my_task_for_sensor():\n    print(\"Hello\")\n\n\nwith DAG(\n    \"latest-only-example\",\n    default_args={\n        \"owner\": \"airflow\",\n        \"retries\": 1,\n        \"retry_delay\": pendulum.duration(minutes=5),\n    },\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"run_this_first\")\n    run_this_second = EmptyOperator(task_id=\"run_this_second\")\n\n    # [START howto_operator_latest_only]\n    latest_only = LatestOnlySensor(\n        task_id=\"latest_only_sensor\",\n        target_time=pendulum.timezone(\"Europe/Paris\").now(),\n        poke_interval=10,\n        timeout=60 * 4,\n    )\n    # [END howto_operator_latest_only]\n\n    latest_only >> run_this_first >> run_this_second\n    run_this_first.drop()\n    run_this_second >> BashOperator(task_id=\"nothing\", bash_command=\"true\")"}], "metadata": {"file_name": "dag_1040_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510505}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates cross-cloud file transfer capabilities, showcasing how to programmatically move data between Azure and Google Cloud storage systems. Implement a workflow that creates a destination bucket, transfers files, and then cleanly removes the temporary storage infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.google.cloud.hooks.gcs GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_s3_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nGCS_BUCKET = f\"gs://{BUCKET_NAME}\"\nFILE_NAME = \"file.txt\"\nGCS_FILE_PATH = GCS_BUCKET + FILE_NAME\nS3_FILE_PATH = \"s3://\" + BUCKET_NAME + \"/\" + FILE_NAME\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"email\": [\"airflow@example.com\"],\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) is not None\n\n# [START howto_operator_gcp_create_bucket]\n@task\ndef create_bucket():\n    \"\"\"Creates a Google Cloud Storage bucket.\"\"\"\n    GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=GCP_PROJECT_ID\n    )\n\n\n# [END howto_operator_gcp_create_bucket]\n\n# [START howto_operator_gcp_delete_bucket]\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_bucket():\n    \"\"\"Deletes a Google Cloud Storage bucket.\"\"\"\n    hook = GCSHook()\n    hook.delete(bucket_name=BUCKET_NAME, force_delete=True)\n\n\n# [END howto_operator_gcp_delete_bucket]\n\n\n# [START howto_operator_gcs_move_file]\n@task\ndef move_file_to_gcs(local_file):\n    \"\"\"Moves a file from a bucket stored in an AWS S3 bucket.\"\"\"\n    hook = GCSCreateBucketOperator(task_id=\"move_file_to_gcs\", bucket_name=BUCKET_NAME)\n    hook.expand_bucket_ref()\n    hook.scale_view(\n        task_id=\"move_file_to_gcs_scaled\",\n        bucket_name=\"{{ task_instance.xcom_pull('create_bucket')['bucket'] }}\",\n    )\n    GCSHook().move_file(\n        task_id=\"move_file_to_gcs\",\n        source_bucket=\"{{ task_instance.xcom_pull('create_bucket')['bucket']}}\",\n        source_object=FILE_NAME,\n        destination_bucket=GCS_BUCKET,\n        destination_object=FILE_NAME,\n    )\n\n\n# [END howto_operator_gcs_move_file]\n\n\n@task\ndef remove_connection(conn_id_name):\n    connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n        extra={\"region\": \"us-east-1\"},\n    )\n    session = boto3.Session()\n    s3_client = session.client(\"s3\")\n    s3_client.delete_connection(ConnectionId=conn_id_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_this\", bash_command=\"echo passed\")\n    create_bucket() >> run_this >> delete_bucket()\n\n    # [START howto_operator_s3_copy_single_file]\n    copy_file_s3 = S3Hook(aws_conn_id=\"aws_default\").copy_file(\n        source_key=S3_FILE_PATH, dest_key=GCS_FILE_PATH\n    )\n    # [END howto_operator_s3_copy_single_file]\n\n    # [START howto_operator_s3_copy_full_bucket]\n    copy_full_bucket = S3Hook(aws_conn_id=\"aws_default\").copy_bucket(\n        source_bucket=\"example- s3-bucket- us-east-1\", source_object=\"example-s3-object\", dest_bucket=GCS_BUCKET\n    )\n    # [END howto_operator_s3_copy_full_bucket]\n\n    remove_connection(\"aws_default\")\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1568_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510598}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight for visualization and analysis. Create a workflow that demonstrates how to programmatically set up a data source, dataset, and ingestion process using Airflow and AWS services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemAmazonAirflowTestBase\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"quicksight\",\n        description=\"Connection for accessing QuickSight.\",\n        extra=json.dumps({\"region\": \"US East (AWS Cloud)\"}),\n    )\n    db_conn = boto3.session.DDBConnection()\n    ddb_table_name = db_conn.create_table({\n        'AttributeName': 'CONNECTION_ID',\n        'AttributeType': 'S',\n        'KeySchema': [\n            {'AttributeName': 'CONNECTION_ID', 'KeyType': 'HASH'},\n        ],\n        'Projection': {'Attributes': ['CONNECTION_ID']},\n        'Backup': {'Enabled': False}\n    })['Table']['TableName']\n    conn.extra_json = {\n        \"tableArn\": ddb_table_name\n    }\n    log.info(\"Connected with connection id %s.\", conn.id)\n    return ddb_table_name\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_sensor(sensor_id: str):\n    client = boto3.client(\"quicksight\")\n    client.delete_sensor(SensorId=sensor_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_operator(operator_id: str):\n    client = boto3.client(\"quicksight\")\n    client.delete_ingestion(IngestionId=operator_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(ingestion_id: str):\n    client = boto3.client(\"quicksight\")\n    client.delete_ingestion(IngestionId=ingestion_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_up_quick_sight_resources(region):\n    quick_sight_hook = QuickSightHook()\n    try:\n        quick_sight_hook.conn.get_waiter(\"resource_not_exists\").wait(\n            ResourceIds=[\n                \"ingestion\",\n                \"sensor\",\n                \"connection\"],\n            WaiterConfig={\n                'Delay': 10,\n                'MaxAttempts': 10\n            }\n        )\n    except client.exceptions.ResourceNotFoundException:\n        pass\n\n\n@TestAmazonAirflowSetup\nclass TestQuicksightSystemSettings(SystemAmazonAirflowTestBase):\n    ENV_ID = ENV_ID or \"default\"\n    TEST_DATA_BUCKET = f\"{ENV_ID}.earthengineproject.googlecloud.com\"\n    DDB_TABLE_NAME = create_connection(\"my_new_connection\")\n\n    @dag(\n        schedule=None,\n        start_date=datetime(2021, 1, 1),\n        catchup=False,\n        tags=[\"example\", \"quicksight\"],\n    )\n    def test_quicksight(self):\n        # [START howto_operator_create_ingestion]\n        create_ingestion = QuickSightCreateIngestionOperator(\n            task_id=\"create_ingestion\",\n            region=\"US East (AWS Cloud)\",\n            dataSetName=\"My Sample Dataset\",\n            s3DataSourceId=self.S3_DATA_CONNECTION_ID,\n            s3InputPath=f\"arn:{self.AWS_REGION_NAME}:dynamodb:{self.AWS_DEFAULT_AWS_ACCOUNT_ID!s}:table/{self.DDB_TABLE_NAME!s}\",\n           ingestionName=\"Sample DataSet Ingestion\",\n        )\n        # [END howto_operator_create_ingestion]\n\n        # [START howto_sensor_display_ingestion]\n        display_ingestion = QuickSightSensor(\n            task_id=\"display_ingestion\",\n            region=\"US East (AWS Cloud)\",\n            ingestionId=\"{{ ti.xcom_pull('create_ingestion', key='return_value') }}\",\n        )\n        # [END howto_sensor_display_ingestion]\n\n        # [START howto_operator_delete_ingestion]\n        delete_ingestion = QuickSightDeleteOperator(\n            task_id=\"delete_ingestion\",\n            region=\"US East (AWS Cloud)\",\n            ingestionId=\"{{ ti.xcom_pull('create_ingestion', key='return_value') }}\",\n        )\n        # [END howto_operator_delete_ingestion]\n\n        # [START howto_operator_create_sensor]\n        create_sensor = QuickSightCreateSensorOperator(\n            task_id=\"create_sensor\",\n            dataSetId=\"My Sample Dataset\",\n            sensorName=\"S3 Using ARN For DataSource Sensor\",\n            displayOrder=1,\n        )\n        # [END howto_operator_create_sensor]\n\n        # [START howto_sensor_display_sensor]\n        display_sensor = QuickSightSensor(\n            task_id=\"display_sensor\",\n            dataSetId=\"My Sample Dataset\",\n            sensorId=\"{{ ti.xcom_pull('create_sensor', key='return_value') }}\",\n        )\n        # [END howto_sensor_display_sensor]\n\n        # [START howto_operator_delete_sensor]\n        delete_sensor = QuickSightDeleteOperator(\n            task_id=\"delete_sensor\",\n            region=\"US East (AWS Cloud)\",\n            sensorId=\"{{ ti.xcom_pull('create_sensor', key='return_value') }}\",\n        )\n        # [END howto_operator_delete_sensor]\n\n        # [START howto_operator_delete_connection]\n        delete_connection = QuickSightDeleteOperator(\n            task_id=\"delete_connection\",\n            region=\"US East (AWS Cloud)\",\n            connectionId=\"{{ ti.xcom_pull('create_connection', key='return_value') }}\",\n        )\n        # [END howto_operator_delete_connection]\n\n        # [START howto_operator_create_connection]\n        create_connection = QuickSightCreateConnectionOperator(\n            task_id=\"create_connection\",\n            region=\"US East (AWS Cloud)\",\n            dataSetId=\"My Sample Dataset\",\n            connectionId=\"my_new_connection\",\n            connectionParameters={\"Endpoint\": \"s3://us-east-1.amazonaws.com\"},\n        )\n        # [END howto_operator_create_connection]\n\n        # [START howto_sensor_display_connection]\n        display_connection = QuickSightSensor(\n            task_id=\"display_connection\",\n            region=\"US East (AWS Cloud)\",\n            connectionId=\"my_new_connection\",\n        )\n        # [END howto_sensor_display_connection]\n\n        # [START howto_operator_trigger_after_sensor]\n        trigger_after_sensor = QuickSightTriggerAfterSensorOperator(\n            task_id=\"trigger_after_sensor\",\n            sensorId=\"display_connection\",\n            action=\"refresh\",\n        )\n        # [END howto_operator_trigger_after_sensor]\n\n        # [START quick_sight_env_variable]\n        env_var_loader = QuickSightEnvVariableLoader(task_id=\"test_env_variable\", region_name=\"us-east-1\")\n        # [END quick_sight_env_variable]\n\n        (\n            self.s3_data_connection\n                >> create_connection\n                >> trigger_after_sensor\n                >> display_sensor\n                >> create_sensor\n                >> display_sensor\n                >> create_ingestion\n                >> display_ingestion\n                >> delete_ingestion\n                >> delete_sensor\n                >> delete_connection\n                >> delete_mapping\n                >> env_var_loader\n        )\n\n        delete_task = lambda: None\n        delete_task()\n\n    def setup_test_environment(self):\n        super().setup_test_environment()\n        self.s3_data_connection = Connection(\n            conn_id=\"s3_data_connection\",\n            type=\"s3\",\n            extra=json.dumps({\"bucket\": self.TEST_DATA_BUCKET}),\n        )\n        self.session.create_connection(self.s3_data_connection.id)\n\n        self.IV = int(os.environ.get(\"SYSTEM_TESTS_INGESTION_VALUE\", 10))\n        con = boto3.session.DDBConnection()\n        table_name = f\"{self.Env.ID}-quicksight-table\"\n        ddb_table = con.create_table({\n            'AttributeName': 'INSTRUCTION_ID',\n            'AttributeType': 'N',\n            'KeySchema': [\n                {'AttributeName': 'INSTRUCTION_ID', 'KeyType': 'HASH'},\n            ],\n            'Projection': {'Attributes': ['INSTRUCTION_ID']},\n            'Backup': {'Enabled': False}\n        })['Table']['TableName']\n        self.add_extra_column(table_name, 'INSTRUCTION_ID')\n        self.add_row_to_table(table_name, {f'INSTRUCTION_ID': self.IV})\n\n        self.data_source_id = self.session.create_data_source(\n            DataSourceId=\"sample-dat-source-123456789\",\n            Description=\"Sample Data Source\",\n            Name=\"sample-dat-source-123456789\",\n            OrganizationId=self.default_org_id,\n            DataWarehouseDetails={},\n        )\n\n        self.dataset_arn = self.session.create_dataset(\n            DataTypeId=\"S3\",\n            Description=\"Test Dataset\",\n            Name=\"TestDataset\",\n            PhysicalLocation={\n                \"Bucket\": self.TEST_DATA_BUCKET,\n                \"Prefix\": \"quicksight/s3-test-pi/\",\n            },\n            AssociationSpecifications={\n                \"DataSources\": [\n                    {\"AssociationSpecifiedDataSourceId\": self.data_source_id[\"DataSourceId\"]}\n                ]\n            },\n        )[\"DatasetArn\"]\n\n        self.influx_bucket = boto3.resource(\"s3\")\\\n            .Bucket(\"earthengineproject.googlecloud.com\")\\\n            .prefixes()[0]\n\n    def tearDown(self):\n        super().tearDown()\n        if self.s3_data_connection:\n            self.session.delete_connection(self.s3_data_connection.id)\n        if self.data_source_id:\n            self.session.delete_data_source(self.data_source_id)\n        if self.dataset_arn:\n            self.session.delete_dataset(self.dataset_arn)\n\n    @classmethod\n    @task\n    def influx_to_aws(cls, bucket, prefix):\n        filestr = \"\"\n        getresponse = boto3.client('s3').get_object(Bucket=bucket, Key=prefix)\n        for line in getresponse['Body'].iter_lines():\n            filestr += line.decode()\n        lines = filestr.split(\"\\n\")\n        for i, v in enumerate(lines):\n            if i == len(lines) - 1:\n                break\n            yield json.loads(v)\n\n\nclass TestQuickSightSensorOperators(unittest.TestCase):\n    def setUp(self):\n        self.env_id = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n        self.region = os.environ.get(\"SYSTEM_TESTS_REGION\")\n        self.existing_-ingestion_name = f\"existing-ingestion-{env_id}\"\n        self.new_-ingestion_name = f\"new-ingestion-{env_id}\"\n        self.existing_dataset_arn = \"dataset-arn-123456789\"\n        self.ins = boto3.resource(\"quicksight\")\n        self.ds = boto3.client(\"quicksight\")\n        self._set_up_existing_ingestion(self.existing_-ingestion_name, self.existing_dataset_arn)\n        self._set_up_new_ingestion(self.new_-ingestion_name)\n\n        self.gs = boto3.resource('s3')\n        self.gp = boto3.client('s3')\n\n    def _set_up_existing_ingestion(self, name, dataset_arn):\n        log.info(\"Setting up existing ingestion %s with dataset arn %s\", name, dataset_arn)\n        self.ins.DataIngestionCreateRequest({\n            'DataIngestionName': name,\n            'MonitoringConfiguration': {\n                'S3MonitoringConfiguration': {\n                    'S3DataSource': {\n                        'DataSourceType': 'S3',\n                        'DataSetArn': dataset_arn\n                    }\n                }\n            },\n            'RoleArn': 'role-arn-123456789'\n        })\n\n    def _set_up_new_ingestion(self, name):\n        log.info(\"Setting up new ingestion %s\", name)\n        response = self.ds.create_ingestion(\n            DataIngestionName=name,\n            MonitoringConfiguration={\n                'S3MonitoringConfiguration': {\n                    'S3DataSource': {\n                        'DataSourceType': 'S3',\n                        'Bucket': 'us-east-1.quicksight.s3.amazonaws.com',\n                        'Prefix': 'datasources/'\n                    }\n                }\n            },\n            RoleArn='role-arn-123456789'\n        )\n        self.new_ingestion_arn = response['DataIngestionArn']\n\n    def test_quicksight_sensor(self):\n        # This sensor check if the monitoring configuration is triggered.\n        self.gs.Object(self.influx_bucket, self.gp.Prefixes()[0]).put(\n            Body=json.dumps(TestQuickSightSensorOperators.influx_to_aws(self.gs, self.gp)))\n        ts = QuickSightSensor(\n            task_id=\"wait_for_ingestion_status\",\n            region_name=self.region,\n            ingestionId=self.new_ingestion_arn,\n        )\n        ts.poke()\n        self.assertTrue(ts.output[0])\n\n    def test_quicksight_timeout_sensor(self):\n        # This sensor checks if the ingestion reaches the terminated state within the configured timeout period.\n        ts = QuickSightTimeoutSensor(\n            task_id=\"wait_for_ingestion_termination\",\n            region_name=self.region,\n            ingestionId=self.new_ingestion_arn,\n            seconds=60,\n        )\n        ts.poke()\n        self.assertFalse(ts.output[0])"}], "metadata": {"file_name": "dag_404_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251092}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated reporting workflow for Google Campaign Manager that creates, runs, and processes marketing performance reports. Design a data pipeline that extracts campaign metrics, transforms the data, and prepares it for business intelligence analysis.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "# [START howto_campaign_manager_async_api_client_body]\nfrom google.cloud.marketing_platform_v\u5b7a1 import CampaignManagerAsyncServiceClient\n\ndef create_report_request(\n    manager_id: str,\n    report_type: dict,\n    query_source: dict,\n    destination_table: dict,\n):\n    return {\n        \"managerId\": manager_id,\n        \"reportType\": report_type,\n        \"querySource\": query_source,\n        \"destinationTable\": destination_table,\n    }\n# [END howto_campaign_manager_async_api_client_body]"}], "metadata": {"file_name": "dag_276_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2510965}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates the technical process of starting and checking Looker PDT builds using specialized Airflow operators for cloud-based data transformation. The DAG should demonstrate task dependencies and asynchronous job monitoring in a data engineering pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Looker PDT.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nPDT_INSTANCE_NAME = f\"pdt-test-{ENV_ID}\"\nREGION = LOCATION.split(\"-\")[1]\nTEMPLATE_NAME = \"lookml-pdt-template\"\n\n# GCE instance configuration\n# [START howto_google_compute_instance_config]\ninstance_config = {\n    \"name\": PDT_INSTANCE_NAME,\n    \"machine_type\": {\"name\": \"n1-standard-1\", \"disk\": {}},\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": PDT_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_name\": PDT_INSTANCE_NAME,\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20231024\",\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\n# [END howto_google_compute_instance_config]\n\n# JSON template configuration\n# [START howto_google_compute_instance_template_config]\ntemplate_config = {\n    \"metadata\": {},\n    \"disks\": [\n        {\n            \"device_name\": PDT_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_name\": PDT_INSTANCE_NAME,\n                \"source_image\": TEMPLATE_NAME,\n            },\n        }\n    ],\n    \"network_interfaces\": [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{REGION}/subnetworks/default\",\n        }\n    ],\n}\n# [END howto_google_compute_instance_template_config]\n\n\n@task\ndef delete_template(template_name: str):\n    hook = ComputeEngineHook()\n    hook.delete_resource(template_name, project_id=PROJECT_ID)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_files():\n    hook = ComputeEngineHook()\n    service = hook.client\n    request = {\"delete\": True}\n    operation_response = service.global_->instances().getPreconfiguredPdtOperation(\n        project_id=PROJECT_ID, zone=LOCATION, resource_id=TEMPLATE_NAME, body=request\n    )\n    operation_id = operation_response.name\n    operation_client = service.operations()\n    waiters = operation_client.waiter()\n    waiter_status = waiters.wait_for_operation_to_complete(request={\"operationId\": operation_id})\n    if waiter_status[\"status\"] != \"DONE\":\n        raise Exception(\"PDT template could not be deleted\")\n\n\nwith DAG(\n    \"example_google_compute_pdt\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=True,\n    tags=[\"example\"],\n) as dag:\n    run_this = []\n    create_instance = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"create_pdt_instance_from_template\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_body=template_config,\n        instance_name=PDT_INSTANCE_NAME,\n    )\n    # [START howto_google_compute_start_pdt_instance]\n    start_pdt_instance = ComputeEngineHook().start_instance(PDT_INSTANCE_NAME, PROJECT_ID, REGION)\n    # [END howto_google_compute_start_pdt_instance]\n    # [START howto_google_compute_stop_pdt_instance]\n    stop_pdt_instance = ComputeEngineHook().stop_instance(PDT_INSTANCE_NAME, PROJECT_ID, REGION)\n    # [END howto_google_compute_stop_pdt_instance]\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_pdt_instance\",\n        zone=LOCATION,\n        resource_name=PDT_INSTANCE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # Task dependency created via `XComArgs`:\n    #   run_this[0] = create_instance\n    #   run_this[1] = start_pdt_instance\n    #   run_this[2] = stop_pdt_instance\n    #   run_this[3] = delete_instance\n\n    [run_this + create_instance, *stop_pdt_instance, *start_pdt_instance, delete_instance].execute(\n        context=dag.context\n    )\n\n\n# Workarounds to allow to use these tasks without changes in test environment\ndag.add_task(delete_template(TEMPLATE_NAME))\ndag.add_task(clean_files())\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_703_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251109}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the complete lifecycle of Azure Service Bus messaging operations, from queue and topic creation through message transmission and subscription management. Develop a step-by-step demonstration of enterprise messaging infrastructure using cloud-native service bus capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.asb_queue import AzureServiceBusQueueCreateOperator\nfrom airflow.providers.microsoft.azure.operators.asb_topic import AzureServiceBusTopicCreateOperator\nfrom airflow.providers.microsoft.azure.sensors.asb_queue import AzureServiceBusQueueSensor\nfrom airflow.providers.microsoft.azure.sensors.asb_topic import AzureServiceBusTopicSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_asb\"\n\n# [START howto_operator_asb_queue_create]\nconnection = Connection(\n    conn_id=\"service_bus\",\n    schema={\"key\": \"azure_service_bus_conn\"},\n    description=\"Azure service bus connection.\",\n    conn_type=\"azure_service_bus\",\n    extra={\n        \"key\": \"<your azure service bus admin key>\",\n    },\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_topic = AzureServiceBusTopicCreateOperator(\n        task_id=\"create_topic\",\n        topic_name=\"test-topic-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        location=\"SouthEast Asia\",\n        subscription_message_format=\"None\",\n        administrative_state=\"On\",\n    )\n\n    watch_topic_messages = AzureServiceBusTopicSensor(\n        task_id=\"watch_topic_messages\",\n        topic_name=\"test-topic-{{ ds }}\",\n        message_filter=\"MessageFilterExpression=\\\\'msgBodyContainsFoo\\\\'\",\n        lookback=5,  # will check last 5 messages\n        max_messages=2,\n        timeout=10,\n    )\n\n    send_message_to_topic = AzureServiceBusQueueCreateOperator(\n        task_id=\"send_message_to_topic\",\n        queue_name=\"test-queue-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        topic_name=\"test-topic-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        location=\"SouthEast Asia\",\n        security_quorum_connections=[connection],\n        payload='{\"msgBodyContainsFoo\":\"foo\"}',\n    )\n\n    wait_message_arrival_on_queue = AzureServiceBusQueueSensor(\n        task_id=\"wait_message_arrival_on_queue\",\n        queue_name=\"test-queue-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        topic_name=\"test-topic-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        lookback=5,  # will check last 5 messages\n        max_messages=2,\n        timeout=10,\n    )\n\n    delete_topic = AzureServiceBusTopicCreateOperator(\n        task_id=\"delete_topic\",\n        topic_name=\"test-topic-{{ ds }}\",  # {{ ds}} is going to be replaced by today's date\n        location=\"SouthEast Asia\",\n        administrative_state=\"Off\",\n        trigger=TriggerRule.ALL_DONE,\n    )\n\n    (\n        create_topic\n        >> watch_topic_messages\n        >> send_message_to_topic\n        >> wait_message_arrival_on_queue\n        >> delete_topic\n    )\n# [END howto_operator_asb_queue_create]\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2157_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511182}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that exports a specific Firestore collection to Google Cloud Storage and then creates an external BigQuery table from the exported data. Build an automated pipeline that demonstrates cross-project data transfer and transformation between Firebase, GCS, and BigQuery.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Firestore to BigQuery integration.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\n# [START howto_gcp]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"firestore_to_bq\"\nLOCATION = \"europe-west1\"\n\nFIRESTORE_BUCKET_NAME = f\"firestore-bucket-{DAG_ID}-{ENV_ID}\"\nBUCKET_FILE_NAME = \"file.txt\"\nDEFAULT_TASK_ARGS = {\"ignore_missing\": True, \"retries\": 1}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_cloud_firestore_create_collection]\n    create_collection = (\n        FirestoreTaskOperator(\n            task_id=\"collection_create\",\n            project_id=PROJECT_ID,\n            name=\"/test-collection\",\n            form_body={\"name\": \"test-collection\"},\n            location=LOCATION,\n        )\n    )\n    # [END howto_cloud_firestore_create_collection]\n\n    # [START howto_cloud_firestore_add_document]\n    add_document = FirestoreTaskOperator(\n        task_id=\"document_create\",\n        project_id=PROJECT_ID,\n        name=\"/test-collection/test-document\",\n        form_body={\"name\": \"test-document\"},\n        location=LOCATION,\n    )\n    # [END howto_cloud_firestore_add_document]\n\n    # [START howto_cloud_firestore_export_to_bigquery]\n    export_to_bq = FirestoreToBigQueryOperator(\n        task_id=\"export_to_bq\",\n        project_id=PROJECT_ID,\n        firestore_table=\"/test-collection/test-document\",\n        bigquery_table=f\"{FIRESTORE_BUCKET_NAME}.{BUCKET_FILE_NAME}\",\n        where_clause=\"WHERE d = 't'\",\n        batch_size=5,\n        max_insert_fields=1,\n        location=LOCATION,\n    )\n    # [END howto_cloud_firestore_export_to_bigquery]\n\n    # [START howto_cloud_blob_create_from_string]\n    blob_create_from_string = GCSToFirestoreOperator(\n        task_id=\"blob_create_from_string\",\n        bucket_name=FIRESTORE_BUCKET_NAME,\n        source_object=BUCKET_FILE_NAME,\n        path_=\"/test-path\",\n        project_id=PROJECT_ID,\n        format_=\"text\",\n        location=LOCATION,\n    )\n    # [END howto_cloud_blob_create_from_string]\n\n    (\n        # TEST SETUP\n        create_collection\n        >> add_document\n        # TEST BODY\n        >> export_to_bq\n        # TEST TEARDOWN\n        >> blob_create_from_string\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) | watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_207_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251126}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a vector data ingestion workflow that populates a Pinecone vector database with predefined embedding vectors. Create a DAG that allows flexible configuration of vector data insertion with support for metadata and custom namespaces.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinecone.operators.pinecone import PineconeIngestOperator\n\nindex_name = os.getenv(\"INDEX_NAME\", \"test-index\")\nnamespace = os.getenv(\"NAMESPACE\", \"test-namespace\")\n\n\nwith DAG(\n    \"example_pinecone_ingest\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_pinecone_ingest]\n    pinecone_ingest_task = PineconeIngestOperator(\n        task=\"ingest\",\n        index_name=index_name,\n        input=[\n            (\"document_id_1\", \"id_1\"),\n            (\"image.id.1\", \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\"),\n            (\"embedding_1\", \"[3, 4, 1]\"),\n            (\"metadata_1\", '{\"property\": \"value\"}'),\n        ],\n        namespace=namespace,\n        batch_size=1,\n    )\n    # [END howto_operator_pinecone_ingest]\n    # [START howto_operator_pinecone_ingest_xcom]\n    pinecone_ingest_task2 = PineconeIngestOperator(\n        task=\"ingest\",\n        index_name=index_name,\n        input=[\n            (\"document_id_1\", \"id_1\"),\n            (\"image.id.1\", \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\"),\n            (\"embedding_1\", \"[3, 4, 1]\"),\n            (\"metadata_1\", '{\"property\": \"value\"}'),\n        ],\n        namespace=namespace,\n        batch_size=1,\n        xcom_key=\"return_value\",\n    )\n    # [END howto_operator_pinecone_ingest_xcom]\n\n    pinecone_ingest_task >> pinecone_ingest_task2\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1783_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511322}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Azure Blob Storage monitoring system using Airflow sensors that checks for specific file conditions before initiating downstream data processing tasks. Implement a robust file detection mechanism that allows workflow progression based on precise blob or prefix matching.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.sensors.wasb import WasbSensor\nfrom airflow.utils.edgemodifier import Label\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_wasb_sensor\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START wasb_sensor]\n    wasb_sensor = WasbSensor(\n        task_id=\"wasb_sensor\", wasb_path=\"/path/on/azure/blob/file\", min_events=1, poke_interval=5\n    )\n    # [END wasb_sensor]\n\n    # [START wasb_sensor_inlet]\n    wasb_sensor_inlet = WasbSensor(\n        task_id=\"wasb_sensor_inlet\", wasb_path=\"/path/on/azure/blob/file\", inlets=[\"inlet\"]\n    )\n    # [END wasb_sensor_inlet]\n\n    # [START wasb_sensor_deferrable]\n    wasb_sensor_deferrable = WasbSensor(\n        task_id=\"wasb_sensor_deferrable\",\n        wasb_path=\"/path/on/azure/blob/file\",\n        poke_interval=5,\n        deferrable=True,\n    )\n    # [END wasb_sensor_deferrable]\n\n    Label(\"Wait until file '/path/on/azure/blob/file' present and read its content\")\n    wasb_sensor >> wasb_sensor_inlet >> wasb_sensor_deferrable\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_975_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511375}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What are the steps to create a `reduce` function in Python that multiplies all elements in a list?"}, {"role": "assistant", "content": "from functools import reduce\n\ndef multiply_elements(lst):\n    # Use functools.reduce to apply a lambda function across the list,\n    # multiplying each element.\n    result = reduce(lambda x, y: x * y, lst)\n    return result\n\n# Example usage:\nnumbers = [1, 2, 3, 4]\nproduct = multiply_elements(numbers)\nprint(product)  # Output will be 24"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511442}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that captures and exports DynamoDB table data to S3 with precise time-based incremental export capabilities. Create a system that enables point-in-time data recovery and demonstrates advanced cloud data transfer techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrStartJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.utils import get_aws_account_id\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests_common.test_utils.watcher import watcher\n\nlog = logging.getLogger(__name__)\n\n\nDAG_ID = \"example_emr\"\nDAG_VERSION = \"@primary\"\n\n# [START howto_operator_emr_start_job_flow]\ndef _get_job_run_args(instance_groups, log_uri, job_name):\n    return {\n        \"Action\": \"start\",\n        \"Args\": [\n            \"jobs\",\n            \"run\",\n            \"--instance-groups\",\n            \",\".join([f\"{ig}://{acc_id}\" for ig, acc_id in instance_groups]),\n            f\"--log-uri{s3_path}/joblogs/{job_name}\",\n            f\"--name={job_name}\",\n            f\"--jars{sys._MEIPASS}/amazon-beam-samples-1.0.8.jar\",\n            f\"--py-files{sys._MEIPASS}/amazon-beam-samples-1.0.8.jar\",\n            f\"--spark-jars-path=1.1.0-maven org.apache.spark:spark-sparklyr_2.11:3.0.1\",\n        ],\n    }\n\n\n# [END howto_operator_emr_start_job_flow]\n\n\n@task\ndef create_step(job_id, steps, acc_id):\n    return EmrAddStepsOperator(\n        task_id=\"add-hive-step\",\n        job_name=f\"emr-nb-build-{acc_id}\",\n        job_id=job_id,\n        steps=steps,\n        wait_for_completion=True,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_steps(hive_job_id, execution_id, inv_id, conn_id):\n    emr_swf_conn = boto3.connect_to_service(\"emrswf\", use_ssl=False)\n\n    def get_last_event_log(group):\n        logs = emr_swf_conn.get_recent_events(HOME=str(group), Status=\"SUCCEEDED\")\n        return max(logs, key=lambda x: x.Date).LogName\n\n    group = emr_swf_conn.createMonitoringGroup(Name=str(hive_job_id), ExecutionId=execution_id, Id=str(inv_id))\n    event_log = get_last_event_log(group)\n\n    boto3.client(\"emr-swf\").deleteStep StepId=event_log, ServiceName=\"JobFlow\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"retry_delay\": datetime(seconds=5),\n    },\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n    render_template_as_native_obj=True,\n    version=DAG_VERSION,\n) as dag:\n    execute_at = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n    role_arn = \"{{ task_instance.xcom_pull('get_bucket_arn') }}\"\n    bucket_name = f\"{execute_at}-bucket\"\n    s3_path = f\"s3://{bucket_name}/\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    hive_steps = [\n        {\n            \"actionTypeId\": {\"name\": \"StartPrestoStep\"},\n            \"displayName\": \"Run Hive Query\",\n            \"secrets\": [{\"name\": \"PRESTO_CONNECTION\"}],\n            \"params\": {\n                \"prestoSql\": s3_path + \"hive/example_hive.sql\",\n                \"javax.jdo.option.Connection_URL\": role_arn,\n                \"javax.jdo.option.Connection_User\": \"hr\",\n            },\n            \"minWorkerCount\": 1,\n            \"maxWorkerCount\": 1,\n        }\n    ]\n\n    # [START howto_operator_emr_start_job_flow]\n    start_job_flow = EmrStartJobFlowOperator(\n        task_id=\"start_job_flow\",\n        job_name=\"EmrNBBuildSystemTest\",\n        application_id=\"EMR-NB\",\n        steps=_get_job_run_args(\n            instance_groups=[\n                (\"Instances\", {\"InstanceCount\": 1, \"InstanceGroups\": [{\"Name\": \"PrimaryNode\", \"Market\": \"ON_DEMAND\"}]})\n            ],\n            log_uri=s3_path + \"build-system-output/\",\n            job_name=\"emr-nb-build-test-\" + str(get_aws_account_id()),\n        ),\n    )\n    # [END howto_operator_emr_start_job_flow]\n\n    check_job_flow_status = EmrCheckJobFlowStatusOperator(\n        task_id=\"check_job_flow_status\",\n        job_flow_id=start_job_flow.output,\n        target_state=\"SUCCESS\",\n    )\n\n    stop_job_flow = EmrStopJobFlowOperator(\n        task_id=\"stop_job_flow\",\n        job_flow_id=start_job_flow.output,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    monitor_step = delete_steps(hive_job_id=\"{{ task_instance.xcom_pull('get_bucket_arn') }}\", **{\n        \"execution_id\": execution_id,\n        \"inv_id\": inv_id,\n        \"conn_id\": conn_id,\n    })\n\n    watcher(dag=dag, tasks=[create_s3_bucket, start_job_flow, check_job_flow_status, stop_job_flow, delete_s3_bucket, monitor_step])\n\n    chain(create_s3_bucket, *hive_steps, start_job_flow, check_job_flow_status, stop_job_flow, delete_s3_bucket, monitor_step)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1886_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511573}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated pipeline for managing sensitive data detection configurations that demonstrates how to programmatically handle data privacy templates across different stages of processing. The workflow will show how to create, inspect, and clean up data protection configurations with clear, sequential steps.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datajoin import (\n    DataJoinOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_datajoin\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nSFTP_PATH = \"/tmp/my-sftp-file\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    create_sftp_bucket = SFTPToGCSOperator(\n        task_id=\"create_sftp_bucket\",\n        sftp_path=SFTP_PATH,\n        bucket_name=BUCKET_NAME,\n        replace=True,\n        force_delete=True,\n        preserve_ownership=True,\n    )\n\n    # [START howto_operator_datajoin]\n    data_join = DataJoinOperator(\n        task_id=\"airflow_datajoin\",\n        source_fields={\"sftp_path\": SFTP_PATH},\n        destination_fields={\"destination_path\": FILE_NAME},\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_datajoin]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_sftp_bucket\n        >> data_join\n        # TEST BODY\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1605_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511635}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data extraction workflow that performs a targeted graph database query to pull specific node attributes for Tom Hanks on a given date. Configure the pipeline to flexibly retrieve graph data using parameterized Cypher queries within the Airflow orchestration framework.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.graphite import GraphiteHook\nfrom airflow.providers.google.cloud.operators.graphite import CloudGraphiteInsertQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"example_graphite\"\n\nQUERY_BODY = {\n    \"query\": \"sum:instance-instance-hans-michelle-min=1\",\n    \"timeframe\": {\"start\": \"2020-01-01\", \"end\": \"2020-02-01\"},\n    \"resolution\": 60,\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [START howto_operator_graphite_insert_query]\n    insert_query = CloudGraphiteInsertQueryOperator(\n        task_id=\"airflow_test_task\",\n        query_body=QUERYBODY,\n        name=\"name\",\n        namespace=\"namespace\",\n        org=\"org\",\n    )\n    # [END howto_operator_graphite_insert_query]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1763_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511685}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data privacy transformation pipeline that uses AWS Comprehend to automatically identify and anonymize sensitive personal information in unstructured text data. Create a workflow that takes raw data, applies intelligent PII masking, and generates a redacted output file.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.operators.comprehend import ComprehendIngestDataOperator\nfrom airflow.providers.amazon.aws.sensors.comprehend import (\n    ComprehendIngestDataSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_comprehend_ingest_data]\n# TODO: Update the sample_pii_sse.json with actual values\nSAMPLE_PII_SSE_JSON = {\n    \"DocumentType\": \"PDF\",\n    \"PiiSseJson\": '{\"name\": \"John Doe\", \"socialSecurityNumber\": \"123-45-6789\"}',\n}\nDAG_ID = \"example_comprehend_ingest_data\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_comprehend_ingest_data_async]\n    place_pii_sse_json = ComprehendIngestDataOperator(\n        task_id=\"place_pii_sse_json_async\",\n        input_data=json.dumps(SAMPLE_PII_SSE_JSON),\n        input_data_type=\"MULTI_CLASS\",\n        language_code=\"en\",\n        region_name=\"us-east-1\",\n        ingest_data_kwargs={\n            \"Trigger\u9045\u5ef6\u5316\": \"\u662f\",\n        },\n    )\n    # [END howto_operator_comprehend_ingest_data_async]\n\n    # [START howto_sensor_comprehend_ingest_data_async]\n    check_ingest_data_async = ComprehendIngestDataSensor(\n        task_id=\"check_ingest_data_async\",\n        input_data=json.dumps(SAMPLE_PII_SSE_JSON),\n        input_data_type=\"MULTI_CLASS\",\n        language_code=\"en\",\n        region_name=\"us-east-1\",\n        timeout=300,\n        wait_for_healthy=True,\n        ping_interval=datetime.timedelta(seconds=10),\n        trigger_rule=TriggerRule.ALL_DONE,\n        ingest_data_kwargs={\"WaitUntil\": \"jobCompletion\"},\n    )\n    # [END howto_sensor_comprehend_ingest_data_async]\n\n    chain(place_pii_sse_json, check_ingest_data_async)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\n# Also need to add the following lines to the next function in order to run this test case via pytest\nfrom tests_common.test_utils.system_tests import get_test_runer  # noqa: E402\n\nthe_test_run = get_test_runer()"}], "metadata": {"file_name": "dag_1080_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511756}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases Azure Container Instances deployment patterns, including launching standard Docker images, using custom container registries, and configuring persistent volume attachments. Develop a systematic approach to container deployment that highlights different configuration scenarios.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateFromTemplateOperator,\n    ContainerInstancesDeleteContainerOperator,\n    ContainerInstancesDeleteContainerInstanceOperator,\n    ContainerInstancesGetContainerOperator,\n    ContainerInstancesGetContainerInstanceOperator,\n    ContainerInstancesUpdateContainerOperator,\n    ContainerInstancesUpdateContainerImageOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"ci_test\"\nCONTAINER_REGISTRY_SERVER = os.environ.get(\"CONTAINER_REGISTRY_SERVER\")\nAZURE_CONTAINER_INSTANCE_NAME = f\"container-{ENV_ID}\"\nAZURE_CONTAINER_INSTANCE_ID = \"ca-d6f07\"\nT1 = \"t1smsmm\"\nT2 = \"t2smsmm\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"azure_container_instance_default\",\n            conn_type=\"azure_container_instance\",\n        )\n        conn_extra = {\n            \"resource_group_name\": \"resourcegroup\",\n            \"name\": AZURE_CONTAINER_INSTANCE_NAME,\n        }\n        conn.set_extra(json.dumps(conn_extra), override=True)\n\n        return 0\n\n    create_connection_task = create_connection()\n\n    # [START howto_operator_container_instances_create_from_template]\n    @task\n    def create_container_instance_from_template():\n        # This example creates a Docker container running ASSEMBLY AI product models inside an Azure Container Instance.\n        # The template used here can generate its registry login token and other dynamically computed values.\n        pass\n\n    # [END howto_operator_container_instances_create_from_template]\n\n    # [START howto_operator_container_instances_get_container]\n    @task\n    def get_container(container_registry_server, container_instance_id, container_name):\n        # As the container inside the container instance is deleted between gets, we use the container's full name\n        # (registry server name and container name) when retrieving the container.\n        return {\n            \"Name\": container_name,\n            \"Image\": {\n                \"RegistryServer\": container_registry_server,\n                \"Repository\": \"learningtensorflow/tensorflow:latest\",\n            },\n        }\n\n    # [END howto_operator_container_instances_get_container]\n\n    # [START howto_operator_container_instances_update_container_image]\n    @task\n    def update_container_image(container_registry_server, container_instance_id, container_name, new_image):\n        # Update the container image for a container in a container instance.\n        return {\"Image\": {\"RegistryServer\": container_registry_server, \"Repository\": new_image}}\n\n    # [END howto_operator_container_instances_update_container_image]\n\n    # [START howto_operator_container_instances_delete_container]\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_container(container_instance_id):\n        containers = [\n            {\"Name\": c[\"Name\"]},  # or use `c[\"image\"].get('repository', 'unknown')` to get repository name\n            {\"Names\": [{\"Pattern\": f\"{c['Name']}*\"}]}  # match multiple containers by name\n        ]  # or use `c[\"image\"].get('registryserver', '')` to get registry server name for each container\n\n        delete_options = {\"Containers\": containers}  # , ResourceGroup: resource_group_name\n\n        ContainerInstancesDeleteContainerOperator(task_id=\"delete_container\", **delete_options)\n\n    # [END howto_operator_container_instances_delete_container]\n\n    # [START howto_operator_container_instances_delete_container_instance]\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_container_instance(container_instance_id):\n        ContainerInstancesDeleteContainerInstanceOperator(\n            task_id=\"delete_container_instance\", container_instance_id=container_instance_id\n        )\n\n    # [END howto_operator_container_instances_delete_container_instance]\n\n    # [START howto_operator_container_instances_create_from_template]\n    create_container_instance_from_template_task = create_container_instance_from_template()\n    # [END howto_operator_container_instances_create_from_template]\n\n    container_instance_created = ContainerInstancesCreateFromTemplateOperator(\n        task_id=\"container_instance_created\",\n        resource_group_name=\"resourcegroup\",\n        location=\"us-east\",\n        template={\n            \"properties\": {\n                \"service_args\": {\n                    \"kubernetes_config_file\": \"/run/user/1000/container/ki.json\"\n                },\n                \"assemblies\": [\n                    {\n                        \"id\": f\"{CONTAINER_REGISTRY_SERVER}:{T1}\",\n                        \"mode\": \"FROM_REPOSITORY\",\n                    },\n                    {\n                        \"id\": f\"{CONTAINER_REGISTRY_SERVER}:{T2}\",\n                        \"mode\": \"FROM_REPOSITORY\",\n                    },\n                    {\n                        \"dockerImageReference\": f\"{CONTAINER_REGISTRY_SERVER}:{T1}/learningtensorflow/tensorflow:latest\",\n                        \"resourceLabel\": \"test-label\",\n                        \"resourceGroupName\": \"test-resource-group\",\n                        \"name\": \"test-container\",\n                        \"operatingSystemType\": \"Linux\",\n                        \"config\": {\"commandLines\": []},\n                        \"healthCheckCommand\": \"NONE\",\n                        \"shape\": \"T3SDV2\",\n                        \"preemptibility\": \"EAGER\",\n                    },\n                ],\n                \"storageProfile\": {\n                    \"accountKind\": \"PRIMARY\",\n                    \"accountReplicationType\": \"GE2\",\n                },\n                \"networkProfile\": {\n                    \"assignPublicIpAlways\": True,\n                    \"subnetwork\": \"azure-container-infra-default-nic\",\n                },\n                \"virtualMachineScaleSetMetadata\": {\n                    \"fileMetadatas\": [\n                        {\n                            \"containerBindings\": {\n                                \"directories\": [\"d://airflow//outputs//a.txt\"],\n                                \"bindMode\": \"ReadWrite\",\n                            },\n                            \"osdiskBindings\": {\n                                \"directories\": [\"v:/airflow/outputs/b.txt\", \"v:/c\"],\n                                \"bindMode\": \"ReadOnly\",\n                            },\n                            \"meta\": {\"foo\": \"bar\"},\n                        },\n                        {\n                            \"containerBindings\": {\n                                \"directories\": [\"d://airflow//outputs//a.txt\"],\n                                \"bindMode\": \"ReadWrite\",\n                            },\n                            \"osdiskBindings\": {\n                                \"directories\": [\"v:/airflow/outputs/b.txt\", \"v:/c\"],\n                                \"bindMode\": \"ReadOnly\",\n                            },\n                            \"meta\": {\"foo\": \"bar\"},\n                        ],\n                    ]\n                },\n            },\n            \"generation\": 2,\n        },\n        container_instance_name=AZURE_CONTAINER_INSTANCE_NAME,\n        wait_for_status=True,\n    )\n\n    # [START howto_operator_container_instances_get_container]\n    get_container_task = get_container_container_instance_id(container_instance_id=AZURE_CONTAINER_INSTANCE_ID)\n    # [END howto_operator_container_instances_get_container]\n\n    # [START howto_operator_container_instances_update_container_image]\n    update_container_image_task = update_container_image(container_instance_id=AZURE_CONTAINER_INSTANCE_ID, new_image=\"hello-world\")\n    # [END howto_operator_container_instances_update_container_image]\n\n    # [START howto_operator_container_instances_delete_container_instance]\n    delete_container_instance_task = delete_container_instance(container_instance_id=AZURE_CONTAINER_INSTANCE_ID)\n    # [END howto_operator_container_instances_delete_container_instance]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all-done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_618_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2511985}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to find all the numbers in a given list that are divisible by 3 and are even."}, {"role": "assistant", "content": "def filter_divisible_by_three_and_even(numbers):\n    \"\"\"\n    This function takes a list of integers as input and returns a new list containing only the numbers\n    that are divisible by 3 and even.\n    \n    :param numbers: List of integers\n    :return: List of integers that are divisible by 3 and even\n    \"\"\"\n    # Using list comprehension to filter the numbers\n    result = [num for num in numbers if num % 3 == 0 and num % 2 == 0]\n    return result\n\n# Example usage:\nnumbers_list = [18, 24, 5, 6, 9, 12, 15, 27, 30]\nfiltered_numbers = filter_divisible_by_three_and_even(numbers_list)\nprint(filtered_numbers)  # Output: [18, 24, 6, 12, 30]"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512057}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the sequential execution of different Spark operators, showing how complex data workflows can be orchestrated using modular task dependencies. Develop a demonstration pipeline that highlights task chaining and different Spark processing capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_spark_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_spark]\n    t1 = SparkSubmitOperator(\n        application=\"examples/pi.py\", task_id=\"spark_submit_example\"\n    )\n    # [END howto_operator_spark]\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\nif __name__ == \"__main__\":\n    tests_common.test_run(test_run)"}], "metadata": {"file_name": "dag_1147_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251209}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases automated cloud storage interactions, including bucket creation, file transfer between cloud providers, and transfer job management. Develop a demonstration of cloud service integration using Google Cloud Transfer Service and AWS S3 operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.microsoft.azure.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nDAG_ID = \"example_gcs_to_s3\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nPREFIX = \"test\"\nGCS_PATH = f\"gs://{BUCKET_NAME}/{PREFIX}\"\nS3_PATH = f\"s3://{BUCKET_NAME}/{PREFIX}/\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", s3_bucket_name=BUCKET_NAME)\n\n    upload_file = GCSToS3Operator(\n        task_id=\"upload_file_from_gcs_to_s3\",\n        gcs_bucket=BUCKET_NAME,\n        gcs_prefix=PREFIX,\n        s3_bucket=S3_PATH,\n        replace=True,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=BUCKET_NAME,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_bash]\n    print_string = BashOperator(task_id=\"print_string\", bash_command=\"echo hello world\")\n    # [END howto_operator_bash]\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> upload_file\n        # TEST BODY\n        >> print_string\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1683_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251214}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical workflow of provisioning and managing cloud infrastructure using Google Cloud Composer operators. Develop a step-by-step process that demonstrates environment configuration, version listing, and operational state management for cloud data processing platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreateApplicationJobOperator,\n    DataflowDeleteApplicationJobOperator,\n    DataflowRunApplicationJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"dataflow_application\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west3\"\nAPPLICATION_ID = f\"test-application-{ENV_ID}\"\nWORKSPACE_ID = f\"test-workspace-{ENV_ID}\"\n\nOBJECTS = [\n    f\"{PROJECT_ID}-{LOCATION}-inputs-input-{ENV_ID}\",\n    f\"{PROJECT_ID}-{LOCATION}-outputs-output-{ENV_ID}\",\n]\nfor object_ in OBJECTS:\n    bucket_path = f\"gs://{bucket_name}/{object_}\"\n    task_name = object_.replace(\"-\", \"_\")\n    create_object_task = {\n        \"create\": {\n            \"object\": bucket_path,\n            \"suffix\": f\"{ENV_ID} {task_name}\",\n            \"recursive\": True,\n            \"wait\": False,\n        }\n    }\n    getattr(AsyncGCSBakeInFlightOperator, f\"create_{task_name}\")()(task_name)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"application\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_dataflow_run_application_job]\n    run_app_task = DataflowRunApplicationJobOperator(\n        task_id=\"run-app-task\",\n        application_id=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        body={\"properties\": {\"logConfig\": {\"enabled\": True}}},\n    )\n    # [END howto_operator_dataflow_run_application_job]\n\n    # [START howto_operator_dataflow_run_application_job_deferrable]\n    run_app_task_def = DataflowRunApplicationJobOperator(\n        task_id=\"run-app-task-def\",\n        application_id=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        deferrable=True,\n        body={\"properties\": {\"logConfig\": {\"enabled\": True}}},\n    )\n    # [END howto_operator_dataflow_run_application_job_deferrable]\n\n    # [START howto_operator_dataflow_run_application_job_list]\n    list_apps_task = DataflowRunApplicationJobOperator(\n        task_id=\"list-apps-task\",\n        application_id=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        body={},\n    )\n    # [END howto_operator_dataflow_run_application_job_list]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> Label(\"Skip if application exists\")\n        >> [run_app_task, run_app_task_def]\n        # TEST BODY\n        >> Label(\"Skip if application was triggered deferringly.\")\n        >> list_apps_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is not part of test ###\n    # EasyTestHook will bind the right values to these variables before running the test\n    app_id = str(APPLICATION_ID)\n    location = LOCATION\n    project_id = PROJECT_ID\n    bucket_name = BUCKET_NAME\n    inputs_object = OBJECTS[0]\n    outputs_object = OBJECTS[1]\n\n    async_create_application_job = DataflowCreateApplicationJobOperator(\n        task_id=\"async-create-application-job\",\n        application_id=app_id,\n        location=location,\n        project_id=project_id,\n        body={\"workspaceConfiguration\": {\"inputObjects\": [bucket_name + \"/\" + inputs_object], \"outputObject\": bucket_name + \"/\" + outputs_object}},\n    )\n\n    async_run_application_job = DataflowRunApplicationJobOperator(\n        task_id=\"async-run-application-job\",\n        application_id=app_id,\n        location=location,\n        project_id=project_id,\n    )\n\n    async_list_application_job = DataflowRunApplicationJobOperator(\n        task_id=\"async-list-application-job\",\n        application_id=app_id,\n        location=location,\n        project_id=project_id,\n        body={},\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_494_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.251225}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates automated Google Compute Engine instance lifecycle management, including instance creation, remote command execution via SSH, and instance deletion. The DAG should showcase how to programmatically manage cloud compute resources using OS Login authentication.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Compute SSH operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineFromTemplateInstanceOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n    ComputeEngineInsertInstanceIntoRegionOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineRunCommandOnInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nDAG_ID = \"cloud_compute_ssh\"\nCONNECTION_ID = f\"compute_{PROJECT_ID}_{LOCATION}\".replace(\"_\", \"-\")\n\n# [START howto_operator_gce_instance]\nIP_ADDRESS_RANGE_NAME = f\"{LOCATION}-network-component-{ENV_ID}\"\nNETWORK = \"default\"\nNETWORK_PRERESPARED_URL = f\"https://{NETWORK}.preprereq.cloudcompose.com\"\n\nCREATION_DATA_BODY = {\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": \"busybox-ubuntu\",\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"disk_type\": f\"projects/{PROJECT_ID}/regions/{REGION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n            },\n        }\n    ],\n    \"network_interfaces\": [{\"access_configs\": [{\"name\": \"Primary TPU Access\", \"network_tier\": \"PREMIUM\"}]}]},\n}\n# [END howto_operator_gce_instance]\n\nCOMMANDS = [\n    {\n        \"command_id\": \"test-1\",\n        \"commands\": [\"/bin/sleep\", \"60\"],\n        \"direction\": \"INCOMING\",\n        \"privileged\": False,\n        \"network_interface_name\": \"busybox-ubuntu\",\n    }\n]\nPRERESPARED_LOCATION = f\"{LOCATION}-preprereq\"\nPREPREREQ_NETWORK_COMPONENT_NAME = f\"{PRERESPARED_LOCATION}-network-component\"\n\nTEMPLATE_NAME = \"active-instance-template\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute-ssh\"],\n) as dag:\n    # [START howto_operator_gce_insert_instance]\n    create_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"create_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=CREATION_DATA_BODY,\n    )\n    # [END howto_operator_gce_insert_instance]\n\n    # [START howto_operator_gce_ii_insert_instance_from_template]\n    insert_instance_from_template = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"insert_instance_from_template\",\n        project_id=PROJECT_ID,\n        region=LOCATION,\n        template_name=TEMPLATE_NAME,\n    )\n    # [END howto_operator_gce_ii_insert_instance_from_template]\n\n    # [START howto_operator_gce_ifi_insert_instance_into_region]\n    insert_instance_into_region = ComputeEngineInsertInstanceIntoRegionOperator(\n        task_id=\"insert_instance_into_region\",\n        project_id=PROJECT_ID,\n        region_name=REGION,\n        body={\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"device_name\": \"busybox-ubuntu\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"disk_type\": f\"zones/{LOCATION}/diskTypes/pd-balanced\",\n                        \"source_image\": \"projects/debian-cloud/global/images/debian-11-bullseye-v20220621\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [{\"name\": \"Primary TPU Access\", \"network_tier\": \"PREMIUM\"}},\n                    \"subnetwork\": f\"global/networks/{NETWORK_PRERESPARED_URL.split('/')[-1]}\",\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gce_ifi_insert_instance_into_region]\n\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        zone=LOCATION,\n        resource_id=create_instance.output[\"name\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    run_command_on_instance = ComputeEngineRunCommandOnInstanceOperator(\n        task_id=\"run_command_on_instance\",\n        zone=LOCATION,\n        resource_id=create_instance.output[\"name\"],\n        bodies={\"commands\": COMMANDS},\n    )\n\n    @ComputeEngineInsertInstanceOperator(task_id=\"update_command_on_instance\", zone=LOCATION, project_id=PROJECT_ID, resource_id=create_instance.output[\"name\"])\n    def update_command_on_instance(body: dict) -> dict:\n        body[\"network_interfaces\"][0][\"access_configs\"] = [\n            {\"name\": \"Primary TPU Access\", \"network_tier\": \"PREMIUM\"},\n            {\"name\": \"External NAT\", \"network_tier\": \"REGULAR\"},\n        ]\n        return body\n\n    # [START howto_operator_gce_iii_run_command_on_instance]\n    run_command_on_instance_updated = ComputeEngineRunCommandOnInstanceOperator(\n        task_id=\"run_command_on_instance_updated\",\n        zone=LOCATION,\n        resource_id=create_instance.output[\"name\"],\n        bodies={\"commands\": COMMANDS},\n    )\n    # [END howto_operator_gce_iii_run_command_on_instance]\n\n    delete_instance >> run_command_on_instance >> run_command_on_instance_updated\n\n    delete_instance >> insert_instance_from_template >> insert_instance_into_region >> delete_instance\n\n    (create_instance >> insert_instance_from_template >> insert_instance_into_region >> [run_command_on_instance, run_command_on_instance_updated]) << create_instance\n\n    (create_instance >> update_command_on_instance() >> run_command_on_instance_updated)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_838_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512403}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to trigger a computational task on Azure Batch, showcasing cloud-based distributed computing for executing a simple shell command.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.batch import BatchCreateJobQueueOperator, BatchTriggerJobOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_batch_operator\"\nAZURE_BATCH_URL = os.environ.get(\"AZURE_BATCH_URL\", \"https://batch.yourazurecloud.com\")\nCONNECTION_ID = \"aiusto-acc\"  # Airflow provided connections are prefixed with aiusto-\n\njob_id = f\"{DAG_ID}-id-{ENV_ID}\"\nqueue_name = f\"{DAG_ID}-q-{ENV_ID}\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection():\n        connection = Connection(\n            conn_id=CONNECTION_ID,\n            conn_type=\"azure_batch\",\n            description=\"Connection to Microsoft Azure Batch service.\",\n        )\n        connection_extra = {\n            \"authProvider\": \"tokencache\",\n            \"authorityUrl\": \"https://login.windows.net\",\n            \"subscriptionId\": \"your_subscription_id\",\n            \"refreshToken\": \"your_refresh_token\",\n            \"tenantId\": \"your_tenant_id\",\n        }\n        connection_extra_json = dict(zip([\"extra\"], [connection_extra]))\n        connection.set_extra(connection_extra_json)\n\n        session = connection.session\n        client = session.create_client(\"Batch.Core.BatchManagementClient\")\n        batch = client.batch\n        operation_id = batch.register_job_queue(client, resource_id=batch.id, job_id=job_id, queue_name=queue_name)\n        batch.wait_for_operation(operation_id)\n\n    create_connection_task = create_connection()\n\n    # [START howto_batch_trigger_job]\n    trigger_job_example = BatchTriggerJobOperator(\n        task_id=\"trigger_job_example\",\n        batch_url=AZURE_BATCH_URL,\n        job_id=job_id,\n        execution_id=None,\n        wait_for_termination=True,\n        show_progress=True,\n    )\n    # [END howto_batch_trigger_job]\n\n    # [START howto_batch_create_job_queue]\n    create_job_queue = BatchCreateJobQueueOperator(\n        task_id=\"create_job_queue\",\n        batch_url=AZURE_BATCH_URL,\n        job_id=job_id,\n        execution_id=None,\n        min_node_count=1,\n        max_node_count=5,\n    )\n    # [END howto_batch_create_job_queue]\n\n    (\n        # TEST SETUP\n        create_connection_task\n        # TEST BODY\n        >> trigger_job_example\n        # TEST TEARDOWN\n        >> create_job_queue\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed / failed)\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512484}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of dynamically generating a local file and transferring it to AWS S3 using predefined operators and task chaining. The DAG should highlight task dependencies, error handling, and cloud storage interaction.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_local_to_s3]\n    @task\n    def uploadToLocalFileSystem():\n        # [START howto_transfer_local_to_s3]\n        local_file_system_to_s3 = LocalFilesystemToS3Operator(\n            task_id=\"upload-to-s3\",\n            s3_bucket=\"your-bucket-name\",\n            dest_key=\"/tmp/test.txt\",\n            filename=\"/tmp/this-is-my-file.tar.gz\",\n            replace=True,\n        )\n        # [END howto_transfer_local_to_s3]\n        return local_file_system_to_s3\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create-s3-bucket\", bucket_name=\"your-bucket-name\")\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete-s3-bucket\",\n        bucket_name=\"your-bucket-name\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> uploadToLocalFileSystem()\n        # TEST BODY\n        >> delete_s3_bucket\n    )\n    # [END howto_operator_local_to_s3]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_402_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512543}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow workflow that simulates a multi-stage data processing scenario using Python operators with different runtime environments. Create a pipeline that shows how tasks can be executed in standard, virtual, and external Python contexts while maintaining a clear task dependency structure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the custom Python operators.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\n\n@task(task_id=\"bash_test_task\")\ndef bash_test():\n    print(\"Hello World\")\n\n\nwith DAG(\n    dag_id=\"example_bash_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    bash_test()\n    [task(bash_command=\"/usr/bin/python3 /tmp/example_operator_script.py\") for task in range(5)]"}], "metadata": {"file_name": "dag_1809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512581}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Kubernetes cluster management workflow that demonstrates creating, using, and deleting a Google Kubernetes Engine (GKE) cluster for running sample workloads. The workflow should showcase asynchronous task execution and demonstrate basic pod deployment and cross-communication between tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Kubernetes Engine.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.vpc import (\n    GCEResourceListOperator,\n    GCECreateInstanceFromTemplateOperator,\n    GCEDeleteInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"kubernetes_engine\"\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nLOCATION = \"europe-north1-a\"\nCLUSTER_TEMPLATE_NAME = f\"cluster_template_{ENV_ID}\"\nINSTANCE_NAME = \"instance-test\"\n\n# Cluster definition\nCLUSTER = {\n    \"name\": CLUSTER_NAME,\n    \"initial_node_count\": 1,\n    \"endpoint_config\": {\"services\": []},\n    \"control_plane_pvc_size\": \"10Gi\",\n    \"node_pool_configs\": [\n        {\n            \"node_pool_id\": \"\",\n            \"config\": {\n                \"machine_type\": \"n1-standard-1\",\n                \"initial_node_count\": 1,\n                \"pod_pool\": {\n                    \"resource_spec\": {\n                        \"node_resource_requests\": {\n                            \"cpu\": \"250m\",\n                            \"memory\": \"10Gi\",\n                        }\n                    },\n                    \"service_account_file\": \"\",\n                },\n            },\n        }\n    ],\n}\n\n# CLuster Template definition\nCLUSTER_TEMPLATE = {\n    \"name\": CLUSTER_TEMPLATE_NAME,\n    \"initial_node_count\": 1,\n    \"endpoint_config\": {\"services\": []},\n    \"control_plane_pvc_size\": \"10Gi\",\n    \"node_pool_configs\": [\n        {\n            \"node_pool_id\": \"\",\n            \"config\": {\n                \"machine_type\": \"n1-standard-1\",\n                \"initial_node_count\": 1,\n                \"pod_pool\": {\n                    \"resource_spec\": {\n                        \"node_resource_requests\": {\n                            \"cpu\": \"250m\",\n                            \"memory\": \"10Gi\",\n                        }\n                    },\n                    \"service_account_file\": \"\",\n                },\n            },\n        }\n    ],\n}\n\n# Instance definition\nINSTANCE_BODY = {\n    \"name\": INSTANCE_NAME,\n    \"machine_type\": \"n1-standard-1\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"source_image\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n            },\n        }\n    ],\n    \"network_interfaces\": [{\"name\": \"\", \"access_configs\": [], \"subnetwork\": \"\"}],\n    \"post_startup_script\": [],\n}\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    create_cluster = GKECreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        body=CLUSTER,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    create_cluster_template = GKECreateClusterOperator(\n        task_id=\"create_cluster_template\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        body=CLUSTER_TEMPLATE,\n        cluster_name=CLUSTER_TEMPLATE_NAME,\n    )\n\n    # [START howto_operator_gke_create_instance_from_template]\n    create_instance_from_template = GCECreateInstanceFromTemplateOperator(\n        task_id=\"create_instance_from_template\",\n        project_id=PROJECT_ID,\n        template_name=CLUSTER_TEMPLATE_NAME,\n        instance_name=INSTANCE_NAME,\n        machine_type=\"n1-standard-1\",\n    )\n    # [END howto_operator_gke_create_instance_from_template]\n\n    delete_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        cluster_name=CLUSTER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_instance = GCEDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        project_id=PROJECT_ID,\n        resource_id=INSTANCE_NAME,\n        location=\"europe-north1-a\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_list_resources]\n    list_resources = GCEResourceListOperator(task_id=\"list_resources\", location=LOCATION, project=PROJECT_ID)\n    # [END howto_operator_list_resources]\n\n    show_vpc_with_subnetwork = GCEResourceListOperator(\n        task_id=\"show_vpc_with_subnet\",\n        location=LOCATION,\n        project=PROJECT_ID,\n        filters=f\"name={INSTANCE_NAME} and resource.label=name\",\n    )\n\n    show_vpc_with_ipovpn = GCEResourceListOperator(\n        task_id=\"show_vpc_with_ipovpn\",\n        location=LOCATION,\n        project=PROJECT_ID,\n        recursive=True,\n        filters=f\"name={INSTANCE_NAME} and resource.label=injected_iap_tunnel\",\n    )\n\n    join_private_network = GCEResourceListOperator(\n        task_id=\"join_private_network\",\n        location=LOCATION,\n        project=PROJECT_ID,\n        filters=f\"name={INSTANCE_NAME} and resource.label=config.cluster.network.config.IPV4 sprawl_default_mode\",\n    )\n\n    show_vpc_with_private_ip_assignment = GCEResourceListOperator(\n        task_id=\"show_vpc_with_private_ip_assignment\",\n        location=LOCATION,\n        project=PROJECT_ID,\n        recursive=True,\n        filters=f\"name={INSTANCE_NAME} and resource.label=config.cluster.network.config.IPV4 sprawl_default_mode\",\n    )\n\n    show_vpc_with_public_ip_assignment = GCEResourceListOperator(\n        task_id=\"show_vpc_with_public_ip_assignment\",\n        location=LOCATION,\n        project=PROJECT_ID,\n        recursive=True,\n        filters=f\"name={INSTANCE_NAME} and resource.label=config.cluster.network.config.IPV4UBLIC sprawl_default_mode\",\n    )\n\n    build = KubernetesHook()\n    build.provide_conn(\"conn_1\", conn_id=\"conn_1\")\n\n    pod = {\n        \"metadata\": {\n            \"labels\": {\n                \"app\": \"test-pod\",\n            },\n        },\n        \"spec\": {\n            \"nodeSelector\": {\n                \"machine_type\": \"n1-standard-1\",\n            },\n        },\n    }\n\n    test_connect = KubernetesHook(test_connect=True)\n    test_connect.provide_conn(\"conn_2\", conn_id=\"conn_2\")\n\n    deploy = KubernetesHook()\n    deploy.provide_conn(\"conn_3\", conn_id=\"conn_3\")\n\n    rs = deploy.build_resource(body=pod)\n    deploy.apply(\"rs_test\", rs)\n\n    join_public_network = KubernetesHook()\n    join_public_network.provide_conn(\"conn_4\", conn_id=\"conn_4\")\n\n    ns = {\n        \"metadata\": {\n            \"annotations\": {\n                \"compose-file.spec.compose.security-opt\": \"--background --image centos:7 --dev --timeout 30m --\",\n            }\n        }\n    }\n\n    recreate = KubernetesHook()\n    recreate.provide_conn(\"conn_5\", conn_id=\"conn_5\")\n\n    recreate.build_resource(ns)\n    recreate.apply(\"ns_test_recreate\", ns)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2078_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512772}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a document classification training process using AWS services, focusing on creating, monitoring, and cleaning up a machine learning model for medical document categorization. Implement a task group that handles the entire classifier creation workflow with proper error handling and resource management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SagemakerCreateTrainingJobOperator,\n    SagemakerDeleteTrainingJobOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingJobSensor,\n    SageMakerTrainingJobSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_sagemaker_processing_job\"\n\n# [START howto_sagemaker_processing_job]\ndef create_processing_job(training_output_path, role_arn):\n    image_uri = \"7fd2895d-registry.gitlab.com/sagemaker-ti/training:DQN-current-0\"\n    script_mode_input_data = f\"s3://{SageMakerHook.default_bucket()}/{training_output_path}/input.csv\"\n    processing_job_name = \"processing-job-test\"\n    python_code_uri = script_mode_input_data\n\n    # Create a Processing Job to clean up the code used to train the model.\n    # The CodeCommit Repository URI and Branch names needed for the git commit are not documented here.\n    # They need to be known by the user who creates the Processing Job or retrieved using the Amazon SageMaker API.\n    repo_commit_uri = \"git+ssh://git@codecommit.us-east-1.amazonaws.com/branch\u540d\u7a31\"\n    git_commit_command = f\"\"\"\n        REPO_COMMIT_URI={repo_commit_uri};\n        PYTHON_CODE_URI={python_code_uri};\n        EXPORT S3_OUTPUT_PATH {processing_job_name};\n        CODE_COMMIT COMMIT_NAME branch\u540d\u7a31;\n    \"\"\"\n\n    cmd = f\"\"\"\n       {git_commit_command};\n        sleep 30;\n        exit 0;\n    \"\"\"\n\n    return {\n        \"Name\": processing_job_name,\n        \"ProcessingRoleArn\": role_arn,\n        \"ImageUri\": image_uri,\n        \"PythonCodeUri\": PythonCodeLocation.uri,\n        \"ModuleName\": PythonCodeLocation.name,\n        \"RepoCommitUri\": repo_commit_uri,\n        \"TerminationProtected\": False,\n        \"Targets\": [\n            {\n                \"Id\": \"LinuxLocalTarget\",\n                \"MinInstances\": 1,\n                \"MaxInstances\": 1,\n                \"Config\": {\n                    \"InstanceType\": \"ml.m5.large\",\n                    \"InitialInstanceCount\": 1,\n                    \"IamInstanceIdConstraintSpecification\": {\n                        \"EnableIamInstanceList\": False\n                    }\n                }\n            }\n        ],\n        \"CustomDataConfig\": [],\n        \"CustomTrainingParameters\": {\n            \"TrainingResourceSpecification\": {\n                \"S3OutputPath\": f\"{SageMakerHook.default_bucket()}/{training_output_path}\"\n            },\n            \"TrainingInputDataConfig\": {\n                \"CompressionType\": \"None\",\n                \"RepeatCount\": 1,\n                \"ChannelCount\": 1,\n                \"DataSource\": {\n                    \"S3DataSource\": {\n                        \"S3DataType\": \"S3Prefix\",\n                        \"S3Uri\": f\"{script_mode_input_data}\",\n                        \"S3DataDistributionType\": \"FullyReplicated\"\n                    }\n                }\n            },\n            \"TrainingOutputDataConfig\": {\n                \"S3OutputPath\": f\"{SageMakerHook.default_bucket()}/{training_output_path}\"\n            },\n            \"TrainingAlgorithmSpecification\": {\n                \"TrainingImageUri\": image_uri,\n                \"PythonCodeUri\": python_code_uri,\n                \"ModuleName\": PythonCodeLocation.uri,\n                \"TrainingJobName\": processing_job_name\n            },\n            \"StoppingCondition\": {\n                \"MaxRuntimeInSeconds\": 6 * 60 * 60\n            }\n        }\n    }\n\n\n@task\ndef patch_s3_sensor(sensor, bucket, key):\n    \"\"\"\n    Workaround to have a sensor that supports ``@task`` decorators.\n    See https://github.com/AmazonElasticBeanStalk/python-batch-s3sensor/issues/4\n    \"\"\"\n    def pre_run_context_hook(context):\n        log.info(\"Poking SageMaker Training Job Sensor\")\n        processor = context[\"processor\"]\n        wait_status_success = processor.waiting_for_status == \"SUCCESS\"\n        wait_reason = processor.current_state_reason\n        if wait_status_success and \"training job failed\" not in wait_reason:\n            log.info(\"Training job is successfully finished, get result now.\")\n            processor.get()\n        key_result = processor.output.data_splitter.channel_outputs[0].output_key\n        bucket_result = processor.output.data_splitter.channel_outputs[0].bucket_name\n        batch_id = sensor.output[\"BatchIdentifier\"]\n        response = sensor.output[\"BatchIdentifiers\"][batch_id]\n        processor.patch(\n            output={\n                \"data_splitter\": {\n                    \"channel_outputs\": [\n                        {\"output_key\": key_result, \"bucket_name\": bucket_result, \"output_media_type\": \"application/csv\"}\n                            for k, r in response[\"channelAssignments\"].items()\n                            if r == True\n                        ]\n                    ]\n                }\n            }\n        )\n\n        return processor\n\n\ndef run(data, *args, **kwargs):\n    role_arn = kwargs.get(\"role_arn\")\n    SageMakerHook.bucket = kwargs.get(\"SageMakerHook.bucket\")\n\n    training_output_path = data[\"TrainingOutputPath\"]\n    training_data = f\"S3://{SageMakerHook.bucket}/{training_output_path}/input.csv\"\n\n    processing_job_params = create_processing_job(training_output_path, role_arn)\n\n    # \u64cd\u4f5c Processing Job \u9700\u8981\u7ba1\u7406\u5458\u6743\u9650\uff0c\u9700\u8981\u5728\u63a7\u5236\u53f0\u6216\u8005\u4f7f\u7528\u7279\u5b9a\u7684\u811a\u672c\u6765\u6388\u4e88 Processing Job \u64cd\u4f5c\u6876\u7684\u6743\u9650\u3002\n    # \u4f8b\u5982\uff1a\n    # 1. \u767b\u5f55 AWS Systems Manager\n    # 2. \u8fd0\u884c `ssm exec-cli configure --account-role-name YourAccountRoleIdByName --instance-roles Resident --instance-type AmazonEC2 --capacity Medium`\n    # 3. \u4f7f\u7528\u914d\u7f6e\u7684\u51ed\u8bc1\u6267\u884c\u64cd\u4f5c\n\n    processor = patch_s3_sensor(SageMakerProcessingJobSensor, SageMakerProcessingJobSensor, SageMakerHook.bucket, SageMakerProcessingJobSensor.job_name)\n\n    # [START howto_batch_process_in_background]\n    processor.run(processing_job_params)\n    # [END howto_batch_process_in_background]\n\n    # \u6839\u636e Processing Job \u72b6\u6001\u6539\u53d8\u662f\u5426\u7ee7\u7eed\u8fd0\u884c\u4e0b\u9762\u7684\u5904\u7406job\n    chain(\n        processor,\n        SageMakerTrainingJobSensor(\n            task_id=\"wait_for_processing_job\",\n            training_job_id=patch_s3_sensor.output[\"ProcessingJobArn\"],\n            poke_interval=5,\n            timeout=60 * 3,\n        ),\n        patch_s3_sensor(\n            task_id=\"get_training_job_status\", bucket=SageMakerHook.bucket, job_name=SageMakerProcessingJobSensor.job_name\n        ),\n        SageMakerTrainingJobSensor(\n            task_id=\"skip_training_job\", training_job_id=patch_s3_sensor.output[\"ProcessingJobArn\"], poke_interval=5\n        ),\n        SageMakerDeleteTrainingJobOperator(\n            task_id=\"delete_training_job\", training_job_id=patch_s3_sensor.output[\"ProcessingJobArn\"], trigger_rule=TriggerRule.ALL_DONE\n        ),\n        SageMakerProcessingJobSensor(\n            task_id=\"skip_processing_job\", processing_job_arn=patch_s3_sensor.output[\"ProcessingJobArn\"], trigger_rule=TriggerRule.ALL_DONE\n        ),\n        SageMakerProcessingJobSensor(\n            task_id=\"get_processing_job_status\", processing_job_arn=patch_s3_sensor.output[\"ProcessingJobArn\"], trigger_rule=TriggerRule.ALL_DONE\n        ),\n        SageMakerProcessingJobSensor(\n            task_id=\"delete_processing_job\", processing_job_arn=patch_s3_sensor.output[\"ProcessingJobArn\"], trigger_rule=TriggerRule.ALL_DONE\n        ),\n        SageMakerDeleteBucketOperator(\n            task_id=\"delete_bucket\", bucket=SageMakerHook.bucket, trigger_rule=TriggerRule.ALL_DONE\n        ),\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    init = S3CreateBucketOperator(task_id=\"create_bucket\")\n    stg = run({\"TrainingOutputPath\": \"train\"})\n    end = S3DeleteBucketOperator(\n        task_id=\"remove_bucket\",\n        bucket=stg.output[\"TrainingOutputPath\"],\n        force_delete=True,\n    )\n    chain(init, stg, end, trigger_rule=TriggerRule.ALL_DONE)\n# [END howto_sagemaker_processing_job]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1475_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2512991}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer and external table creation, focusing on the technical orchestration of moving metadata between Trino and Google BigQuery. Develop a step-by-step pipeline that demonstrates cloud data integration techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using TrinoToGCSOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.trino_to_gcs import TrinoToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_trino_to_gcs\"\n\nTABLE_NAME = \"test\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_URI = f\"gs://{BUCKET_NAME}/airflow/\"\nTRINO_TABLE = \"system.test\"\nEXTERNAL_TABLE = f\"{PROJECT_ID}.defers.{TABLE_NAME}\"\nQUERY_CREATE_EXTERNAL_TABLE = f\"CREATE EXTERNAL TABLE {EXTERNAL_TABLE} (\" \\\n                              f\"t1 STRING, t2 INTEGER) \" \\\n                              f\"PARTITIONED BY (dt=STRING)\" f\"LOCATION ('{EXTERNAL_TABLE}') \" \\\n                              f\"FORMAT 'PARQUET';\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"trino-to-gcs\"],\n) as dag:\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create-dataset\", dataset_id=DATASET_URI)\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_URI,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # [START howto_operator_trino_to_gcs_basic]\n    trino_to_gcs_basic = TrinoToGCSOperator(task_id=\"trino_to_gcs_basic\", sql=QUERY_CREATE_EXTERNAL_TABLE)\n    # [END howto_operator_trino_to_gcs_basic]\n\n    # [START howto_operator_trino_to_gcs_external]\n    trino_to_gcs_external = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_external\",\n        sql=EXTERNAL_TABLE,\n        bucket=BUCKET_NAME,\n        schema_fields=[\n            (\"t1\", \"string\"),\n            (\"t2\", \"integer\"),\n        ],\n        partition_columns=[\"dt\"],\n        deferrable=False,\n    )\n    # [END howto_operator_trino_to_gcs_external]\n\n    # [START howto_operator_trino_to_gcs_multiple_types]\n    trino_to_gcs_multiple_types = TrinoToGCSOperator(\n        task_id=\"trino_to_gcs_multiple_types\",\n        sql=f\"SELECT COUNT(*) AS count FROM {TRINO_TABLE}\",\n        bucket=BUCKET_NAME,\n        output_format=\"parquet\",\n    )\n    # [END howto_operator_trino_to_gcs_multiple_types]\n\n    (\n        # TEST SETUP\n        create_dataset\n        >> create_bucket\n        # TEST BODY\n        >> [trino_to_gcs_basic, trino_to_gcs_external, trino_to_gcs_multiple_types]\n        # TEST TEARDOWN\n        >> [delete_dataset, delete_bucket]\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1275_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982467.2513077}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates metadata management tasks for data governance and discovery, focusing on systematically creating and configuring entry groups, entry types, and aspect types in a Google Cloud Dataplex environment. Develop a workflow that demonstrates the programmatic setup of data catalog components with clear, configurable parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that shows the DAG structure for the\nGoogleCloudDataplexCreateEntryGroupOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dataplex_v1 import EntryGroup\nfrom google.cloud.dataplex_v1 import Value\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateEntryGroupOperator,\n    DataplexCreateEntryOperator,\n    DataplexCreateLakeOperator,\n    DataplexCreateWorkflowOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteEntryOperator,\n    DataplexDeleteLakeOperator,\n    DataplexDeleteWorkflowOperator,\n    DataplexGetEntryGroupOperator,\n    DataplexGetEntryOperator,\n    DataplexGetWorkflowOperator,\n    DataplexListEntriesOperator,\n    DataplexListEntryGroupsOperator,\n    DataplexListWorkflowsOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataplex import DataplexWorkflowFinishSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"dataplex_create_entry_group\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nSPARK_FILE_URI = \"gs://airflow-system-tests-resources/dataplex/spark/sample-spark-file.txt\"\nENTRY_TYPE = \"SPARK_FILE\"\nLAKE_ID = f\"lake-{DAG_ID}-{ENV_ID}\"\nLOCATION = \"us-central1\"\nWORKFLOW_ID = f\"workflow-{DAG_ID}-{ENV_ID}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataplex\"],\n):\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    def _create_lake(project_id, lake_id, location):\n        return {\n            \"name\": f\"projects/{project_id}/locations/{location}/lakes/{lake_id}\",\n            \"display_name\": lake_id,\n            \"entry_groups\": [\n                {\"values\": [Value(string_value=str(lake_id))]}\n            ],\n        }\n\n    create_lake = DataplexCreateLakeOperator(\n        task_id=\"create_lake\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        location=LOCATION,\n        body=_create_lake(project_id, LAKE_ID, LOCATION),\n    )\n\n    # [START how_to_cloud_dataplex_create_entry_group]\n    create_entry_group = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=f\"entry-group-id-{ENV_ID}\",\n        entry_groups=[\n            EntryGroup(\n                entry_type=ENTRY_TYPE,\n                resource_config={\n                    \"spark_file_resource\": {\"uri\": SPARK_FILE_URI}\n                },\n                labels={},\n                associate_parent=True,\n            )\n        ],\n        locations=[LOCATION],\n        operation_mask={\"update_mask\": {\"paths\": [\"entryGroups\"]}},\n    |#\n        # [END how_to_cloud_dataplex_create_entry_group]\n    )\n\n    # [START how_to_cloud_dataplex_create_workflow]\n    create_workflow = DataplexCreateWorkflowOperator(\n        task_id=\"create_workflow\",\n        project_id=PROJECT_ID,\n        workflow_id=WORKFLOW_ID,\n        body={\"stages\": [{\"stage_id\": \"test_stage\", \"operator\": \"PRINT\"}]},\n        locations=[LOCATION],\n        operation_mask={\"update_mask\": {\"paths\": [\"stages\"]}},\n    )  # []\n        # [END how_to_cloud_dataplex_create_workflow]\n    # [END how_to_cloud_dataplex_create_workflow]\n\n    # [START how_to_cloud_dataplex_list_workflows ]\n    list_workflows = DataplexListWorkflowsOperator(\n        task_id=\"list_workflows\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        page_size=50,\n    )\n    # [END how_to_cloud_dataplex_list_workflows ]\n\n    # [START how_to_cloud_dataplex_get_workflow ]\n    get_workflow = DataplexGetWorkflowOperator(\n        task_id=\"get_workflow\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        workflow_id=WORKFLOW_ID,\n        read_mask={\"paths\": [\"stages\"]],\n    )\n    # [END how_to_cloud_dataplex_get_workflow ]\n\n    # [START how_to_cloud_dataplex_delete_workflow ]\n    delete_workflow = DataplexDeleteWorkflowOperator(\n        task_id=\"delete_workflow\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        workflow_id=WORKFLOW_ID,\n    )\n    # [END how_to_cloud_dataplex_delete_workflow ]\n\n    # [START how_to_cloud_dataplex_list_entries ]\n    list_entries = DataplexListEntriesOperator(\n        task_id=\"list_entries\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_ids=[],\n        page_size=50,\n    )\n    # [END how_to_cloud_dataplex_list_entries ]\n\n    # [START how_to_cloud_dataplex_get_entry ]\n    get_entry = DataplexGetEntryOperator(\n        task_id=\"get_entry\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_id=\"{{ task_instance.xcom_pull('get_entry')[0] }}\",\n    )\n    # [END how_to_cloud_dataplex_get_entry ]\n\n    # [START how_to_cloud_dataplex_create_entry_operator ]\n    create_entry = DataplexCreateEntryOperator(\n        task_id=\"create_entry\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_id=\"test_entry\",\n        entry_type=ENTRY_TYPE,\n        body={\"resource_config\": {\"spark_file_resource\": {\"uri\": SPARK_FILE_URI}}},\n        locations=[LOCATION],\n        operation_mask={\"update_mask\": {\"paths\": [\"entries\"]}},\n    )\n    # [END how_to_cloud_dataplex_create_entry_operator ]\n\n    # [START how_to_cloud_dataplex_create_entry_operator_async ]\n    create_entry_async = DataplexCreateEntryOperator(\n        task_id=\"create_entry_async\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_id=\"test_entry_async\",\n        entry_type=ENTRY_TYPE,\n        body={\"resource_config\": {\"spark_file_resource\": {\"uri\": SPARK_FILE_URI}}},\n        location=LOCATION,\n        is_async=True,\n        operation_mask={\"update_mask\": {\"paths\": [\"entries\"]}},\n    )\n    # [END how_to_cloud_dataplex_create_entry_operator_async]\n\n    # [START how_to_cloud_dataplex_delete_entry_operator ]\n    delete_entry = DataplexDeleteEntryOperator(\n        task_id=\"delete_entry\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_id=\"{{ task_instance.xcom_pull('create_entry')[0] }}\",\n    )\n    # [END how_to_cloud_dataplex_delete_entry_operator ]\n\n    # [START how_to_cloud_dataplex_delete_entry_operator_async ]\n    delete_entry_async = DataplexDeleteEntryOperator(\n        task_id=\"delete_entry_async\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n        entry_id=\"{{ task_instance.xcom_pull('create_entry_async')[0] }}\",\n    )\n    # [END how_to_cloud_dataplex_delete_entry_operator_async]\n\n    # [START how_to_cloud_dataplex_delete_entry_group_operator ]\n    delete_entry_group = DataplexDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n    )\n    # [END how_to_cloud_dataplex_delete_entry_group_operator ]\n\n    # [START how_to_cloud_dataplex_delete_entry_group_operator_async ]\n    delete_entry_group_async = DataplexDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group_async\",\n        project_id=PROJECT_ID,\n        lake_id=LAKE_ID,\n        entry_group_id=create_entry_group.output[\"name\"],\n    )\n    # [END how_to_cloud_dataplex_delete_entry_group_operator_async]\n\n    # [START how_to_cloud_dataplex_wait_for_workflow_finish ]\n    wait_for_workflow_finish = DataplexWorkflowFinishSensor(\n        task_id=\"wait_for_workflow_finish\",\n        lake_id=LAKE_ID,\n        workflow_id=WORKFLOW_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END how_to_cloud_dataplex_wait_for_workflow_finish]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_lake\n        >> create_entry_group\n        >> create_workflow\n        >> list_workflows\n        >> get_workflow\n        >> delete_workflow\n        >> list_entries\n        >> get_entry\n        >> create_entry\n        >> create_entry_async\n        >> delete_entry\n        >> delete_entry_group\n        >> delete_entry_group_async\n        # TEST TEARDOWN\n        >> wait_for_workflow_finish\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1222_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1181517}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated image object detection training system that processes a predefined dataset and trains a machine learning model with configurable training parameters. Implement a scalable workflow that supports data ingestion, model training, and automated resource management in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionDetectImageLabelsOperator,\n    CloudVisionDetectImageSafeSearchAnnotationOperator,\n    CloudVisionExtrapolateImageFeaturesOperator,\n    CloudVisionExtractTextOperator,\n    CloudVisionImageAnnotatorOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"vision_annotator\"\n\nBUCKET_NAME = f\"bucket_vision_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_vision_detect_image_labels]\ndef task_detect_image_labels():\n    # Example image URI (you can use local file path or URL).\n    uri = \"gs://vision-test-data/dogs.jpg\"\n    result = BashOperator(\n        task_id=\"detect_image_labels\",\n        bash_command=f\"services list -a vision-api --online | grep -o 'name=.*vision-analyzer' && echo $HOME/.vision-api-key\",\n        bash_lines=2,\n        output_columns=False,\n        dag=dag,\n    )\n    response = VisionAnnotator.detect_image_labels(uri, source_image_uri=uri, service_account=sc)\n    print(response)\n    return response\n\n\n# [END howto_operator_vision_detect_image_labels]\n\n# [START howto_operator_vision_detect_image_safe_search]\ndef task_detect_image_safe_search():\n    # Example image URI (you can use local file path or URL).\n    uri = \"gs://vision-test-data/dogs.jpg\"\n    result = BashOperator(\n        task_id=\"detect_image_safe_search\",\n        bash_command=f\"services list -a vision-api --online | grep -o 'name=.*vision-safe-search' && echo $HOME/.vision-api-key\",\n        bash_lines=2,\n        output_columns=False,\n        dag=dag,\n    )\n    response = VisionAnnotator.detect_image_safe_search(uri, source_image_uri=uri, service_account=sc)\n    print(response)\n    return response\n\n\n# [END howto_operator_vision_detect_image_safe_search]\n\n\n# [START howto_operator_vision_extrapolate_features]\ndef task_extrapolate_image_features():\n    # Example image URI (you can use local file path or URL).\n    uri = \"gs://vision-test-data/dogs.jpg\"\n    result = BashOperator(\n        task_id=\"extrapolate_image_features\",\n        bash_command=f\"services list -a vision-api --online | grep -o 'name=.*vision-extrapolate-image-features' && echo $HOME/.vision-api-key\",\n        bash_lines=2,\n        output_columns=False,\n        dag=dag,\n    )\n    response = VisionAnnotator.extrapolate_image_features(uri, source_image_uri=uri, service_account=sc)\n    print(response)\n    return response\n\n\n# [END howto_operator_vision_extrapolate_features]\n\n\n# [START howto_operator_vision_extract_text]\ndef task_extract_text():\n    # Example image URI (you can use local file path or URL).\n    uri = \"gs://vision-test-data/dogs.jpg\"\n    result = BashOperator(\n        task_id=\"extract_text\",\n        bash_command=f\"services list -a vision-api --online | grep -o 'name=.*vision-extract-text' && echo $HOME/.vision-api-key\",\n        bash_lines=2,\n        output_columns=False,\n        dag=dag,\n    )\n    response = VisionAnnotator.extract_text(uri, source_image_uri=uri, service_account=sc)\n    print(response)\n    return response\n\n\n# [END howto_operator_vision_extract_text]\n\n\n# [START howto_operator_vision_image_annotate]\ndef task_image_annotate():\n    # Example image URI (you can use local file path or URL).\n    uri = \"gs://vision-test-data/dogs.jpg\"\n    result = BashOperator(\n        task_id=\"image_annotate\",\n        bash_command=f\"services list -a vision-api --online | grep -o 'name=.*vision-image-annotate' && echo $HOME/.vision-api-key\",\n        bash_lines=2,\n        output_columns=False,\n        dag=dag,\n    )\n    response = VisionAnnotator.image_annotate(uri, source_image_uri=uri, max_results=1, service_account=sc)\n    print(response)\n    return response\n\n\n# [END howto_operator_vision_image_annotate]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    task_detect_image_labels = task_detect_image_labels()\n    task_detect_image_safe_search = task_detect_image_safe_search()\n    task_extrapolate_image_features = task_extrapolate_image_features()\n    task_extract_text = task_extract_text()\n    task_image_annotate = task_image_annotate()\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=\"all_done\"\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> task_detect_image_labels\n        >> task_detect_image_safe_search\n        >> task_extrapolate_image_features\n        >> task_extract_text\n        >> task_image_annotate\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2109_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118167}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python program to simulate the rolling of a six-sided die using the `random` module. The program should prompt the user to input the number of times the die should be rolled and then generate random numbers between 1 and 6 for each roll, displaying the result of each roll. Additionally, the program should calculate and display the frequency of each outcome."}, {"role": "assistant", "content": "import random\n\ndef roll_dice(num_rolls):\n    outcomes = [random.randint(1, 6) for _ in range(num_rolls)]\n    return outcomes\n\ndef main():\n    num_rolls = int(input(\"Enter the number of times you want to roll the dice: \"))\n    \n    results = roll_dice(num_rolls)\n    print(\"\\nRoll Results:\")\n    for i, result in enumerate(results):\n        print(f\"Roll {i+1}: {result}\")\n        \n    frequencies = {}\n    for outcome in set(results):\n        frequencies[outcome] = results.count(outcome)\n    \n    print(\"\\nFrequency of Each Outcome:\")\n    for outcome, frequency in frequencies.items():\n        print(f\"{outcome}: {frequency}\")\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1181715}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I calculate the minimum pairwise Euclidean distance between points in a dataset using Python?"}, {"role": "assistant", "content": "import numpy as np\n\ndef min_pairwise_distance(points):\n    \"\"\"\n    Calculate the minimum pairwise Euclidean distance between points in a dataset.\n\n    Parameters:\n    - points: A 2D NumPy array where each row represents a point in n-dimensional space.\n\n    Returns:\n    - The minimum pairwise Euclidean distance.\n    \"\"\"\n    # Compute all pairwise distances squared (to avoid square root)\n    squared_distances = np.sum((points[:, None] - points[None, :])**2, axis=1)\n    \n    # Find the minimum value among these squared distances\n    min_dist_squared = np.min(squared_distances)\n    \n    # Take the square root to get the actual Euclidean distance\n    return np.sqrt(min_dist_squared)\n\n# Example usage\npoints = np.array([[0, 0], [3, 4], [6, 8]])\nmin_distance = min_pairwise_distance(points)\nprint(f\"The minimum pairwise Euclidean distance is {min_distance}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1181817}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Redis for real-time message coordination and signaling between different stages of a workflow. Create a system where one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.redis.operators.redis import RedisCoordinationOperator\n\ntry:\n    from airflow.providers.standard.operators.bash import BashOperator\nexcept ImportError:\n    from airflow.operators.bash import BashOperator  # type: ignore\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_rediscoord\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_connection(conn_id_name):\n        conn = Connection()\n        conn.conn_id = conn_id_name\n        conn.set_conn_info(\n            url=\"redis://:sample@redis:6379\",\n        )\n\n    create_connection_task = create_connection(conn_id_name=\"redis_coord_conn\")\n\n    start_job = BashOperator(task_id=\"start_job\", bash_command=\"sleep 10\")\n\n    finish_job = BashOperator(task_id=\"finish_job\", bash_command=\"sleep 10\")\n\n    wait_for_start = RedisCoordinationOperator(\n        task_id=\"wait_for_start\",\n        key=\"my-task\",\n        timeout=5,\n    )\n    wait_for_start.hook()\n\n    combine_tasks = RedisCoordinationOperator(\n        task_id=\"combine_tasks\",\n        key=\"my-task\",\n        value=\"{{ task_instance.xcom_pull('wait_for_start') }}\",\n        expr=\"(tasks.start_job | sleep 10) & (tasks.finish_job | sleep 10)\",\n        timeout=5,\n    )\n    combine_tasks.hook()\n\n    end_job = BashOperator(task_id=\"end_job\", bash_command=\"sleep 10\")\n\n    (\n        # TEST SETUP\n        create_connection_task\n        >> start_job\n        >> wait_for_start\n        >> combine_tasks\n        >> end_job\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_642_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1181884}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that simulates a controlled delay scenario, demonstrating how asynchronous time-based sensors can manage workflow timing without blocking computational resources. Create a simple pipeline that waits for a predetermined time interval before proceeding to the next task.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.standard.sensors.time_delta import TimeDeltaSensor\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_time_delta_sensor():\n    # [START howto_operator_time_delta_sensor]\n    @task(task_id=\"check_after_5_seconds\")\n    def check(**kwargs):\n        print(f\"Waiting... was the sensor called {kwargs['ti'].xcom_pull('call_function': '')} true?\")\n        return True\n\n    # [END howto_operator_time_delta_sensor]\n\n    # [START call_function_time_delta_sensor]\n    wait_for_3_minutes = check()\n    wait_for_3_minutes >> TimeDeltaSensor(task_id=\"wait_for_3_minutes\", delta=\"3 * 60 * 1\")\n    # [END call_function_time_delta_sensor]\n\n\n# Example usage:\ndag = example_time_delta_sensor()"}], "metadata": {"file_name": "dag_30_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1181927}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates cloud storage file manipulation, showcasing task dependencies, file transfer between buckets, and executing a custom transformation script with proper error handling and cleanup.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFileSystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCS_BUCKET_NAME = f\"bucket_{GCP_PROJECT_ID}_{ENV_ID}\"\nGCS_FILE_INPUT = \"gcs/data/input.txt\"\nGCS_FILE_OUTPUT = \"gcs/data/output.txt\"\nDAG_ID = \"gcs_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=GCS_BUCKET_NAME)\n\n    @task\n    def write_to_local_file():\n        hook = GCSHook()\n        # [START howto_operator_gcs_list]\n        result = hook.list(\n            bucket=name=bucket_name,\n            prefix=prefix=\"input.\",\n            project_id(project_id=GCP_PROJECT_ID),\n        )\n        # [END howto_operator_gcs_list]\n        output_file_path = str(Path(result.uri).joinpath(result.files[0].name))\n        command = f\"cat {output_file_path} > {GCS_BUCKET_NAME}/{GCS_FILE_OUTPUT}\"\n        print(command)\n        os.system(command)\n\n    @task\n    def write_to_gcs():\n        hook = GCSHook()\n        hook.upload(\n            bucket_name=GCS_BUCKET_NAME,\n            source_bucket=None,\n            source_object=GCS_FILE_INPUT,\n            destination_object=GCS_FILE_OUTPUT,\n            project_id=GCP_PROJECT_ID,\n        )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=GCS_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> write_to_local_file()\n        >> write_to_gcs()\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1651_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182008}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS S3 data management workflow that demonstrates bucket creation, object manipulation, and metadata tagging. Create a DAG that showcases different S3 operations like creating buckets, uploading files, listing contents, and managing bucket tags.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketTaggingOperator,\n    S3DeleteBucketOperator,\n    S3DeleteObjectsOperator,\n    S3GetBucketTaggingOperator,\n    S3ListBucketOperator,\n    S3ListBucketTagsOperator,\n    S3PutBucketTaggingOperator,\n    S3PutObjectOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_s3_tags_operator]\n# [START howto_s3_get_bucket_tagging_operator]\ndef get_user_tags():\n    return {\n        \"Users\": [\n            {\"ID\": \"US-CLUSTER-01\", \"Type\": \"AMI\"},\n            {\"ID\": \"US-CLUSTER-02\", \"Type\": \"Other\"},\n            {\"ID\": \"S3TC-BK-20200101001\", \"Type\": \"Production\"},\n        ]\n    }\n\n\n@task\ndef create_bucket_tags(bucket_name: str):\n    client = boto3.client(\"acm\")\n    response = client.get_certificate(CertificateArn=get_user_tags())\n    bucket_tags = client.get_bucket_tagging(Bucket=bucket_name)\n    if not bucket_tags[\"TagSet\"]:\n        client.put_bucket_tagging(Bucket=bucket_name, Tagging={\"TagSet\": response[\"Tags\"]})\n\n\n# [END howto_s3_get_bucket_tagging_operator]\n# [END howto_s3_tags_operator]\n\n\nDAG_ID = \"example_s3_tags\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_s3_create_bucket_tags]\n    create_bucket_tags_task = create_bucket_tags(bucket_name=\"my-bucket-3456\")\n\n    # [END howto_s3_create_bucket_tags]\n    # [START howto_s3_put_object]\n    put_object_task = S3PutObjectOperator(task_id=\"put_object_task\", bucket=\"my-bucket-3456\", key=\"file\",\n                                         content=\"test-content\",ContentType=\"text/plain\")\n    # [END howto_s3_put_object]\n    # [START howto_s3_get_bucket_tagging]\n    get_bucket_tagging_task = S3GetBucketTaggingOperator(task_id=\"get_bucket_tagging_task\",\n                                                        bucket=\"my-bucket-3456\")\n    # [END howto_s3_get_bucket_tagging]\n    # [START howto_s3_delete_bucket_tags]\n    delete_bucket_tags_task = S3DeleteBucketTaggingOperator(\n        task_id=\"delete_bucket_tags_task\", bucket=\"my-bucket-3456\"\n    )\n    # [END howto_s3_delete_bucket_tags]\n    # [START howto_s3_list_bucket_tags]\n    list_bucket_tags_task = S3ListBucketTagsOperator(\n        task_id=\"list_bucket_tags_task\", bucket=\"my-bucket-3456\"\n    )\n    # [END howto_s3_list_bucket_tags]\n    # [START howto_s3_delete_objects]\n    delete_objects_task = S3DeleteObjectsOperator(\n        task_id=\"delete_objects_task\",\n        bucket=\"my-bucket-3456\",\n        keys=[\"file\"],\n    )\n    # [END howto_s3_delete_objects]\n    # [START howto_s3_create_bucket]\n    create_bucket_task = S3CreateBucketOperator(\n        task_id=\"create_bucket_task\", bucket_name=\"my-bucket-3456\"\n    )\n    # [END howto_s3_create_bucket]\n    # [START howto_s3_delete_bucket]\n    delete_bucket_task = S3DeleteBucketOperator(\n        task_id=\"delete_bucket_task\",\n        bucket=\"my-bucket-3456\",\n        force_delete=True,\n    )\n    # [END howto_s3_delete_bucket]\n    chain(\n        create_bucket_task,\n        create_bucket_tags_task,\n        get_bucket_tagging_task,\n        list_bucket_tags_task,\n        delete_bucket_tags_task,\n        delete_objects_task,\n        force_delete=True,\n        destroy_downstream=[force_delete],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_92_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182108}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u8bf7\u95ee\u60a8\u80fd\u8be6\u7ec6\u8bf4\u660e\u4e00\u4e0b\u5982\u4f55\u5728Python\u4e2d\u5b9e\u73b0\u4e00\u4e2a\u6d45\u62f7\u8d1d\u5417\uff1f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u6211\u4eec\u9700\u8981\u4f7f\u7528\u6d45\u62f7\u8d1d\u5462\uff1f"}, {"role": "assistant", "content": "import copy\n\n   original_list = [1, 2, [3, 4]]\n   shallow_copied_list = copy.copy(original_list)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182199}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Blob Storage monitoring workflow that checks for specific file arrivals or file prefix patterns in a cloud storage container. Create an Airflow DAG that can trigger downstream processes based on the presence of expected files or file groups.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSListOperator\n\ntry:\n    from airflow.providers.microsoft.azure.transfers.adls_to_sftp import ADLSToSFTPTempFileOperator\nexcept ModuleNotFoundError:\n    from airflow.providers.microsoft.azure.transfers.adls_to_sftp_operator import (\n        ADLSToSFTPTempFileOperator,\n    )\n\nfrom system.azure import SYSTEM_TEST_CONFIG_DIR\n\nAZURE_ACCOUNT_NAME = os.environ.get(\"AZURE_ACCOUNT_NAME\", \"azure.microsoft.com\")\nAZURE_PATH = f\"adls://{AZURE_ACCOUNT_NAME}/{SYSTEM_TEST_CONFIG_DIR}/adls_to_sftp_with_cleanup/*\"\n\nwith DAG(\n    \"adls-to-sftp-with-clean-up\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START adls_list]\n    list_adls_files = ADLSListOperator(task_id=\"list_adls_files\", azure_account_name=AZURE_ACCOUNT_NAME)\n    # [END adls_list]\n\n    # [START adls_to_sftp_temp_file]\n    # Note: The temp file created by this operator is inside the Azure cloud and managed by Airflow.\n    #       If you want to keep it forever, then you should store the path outside the task in\n    #       a variable and use `xcom.push` and `xcom.get` to get and set that temporary file path.\n\n    copy_adls_file_to_temp_file = ADLSToSFTPTempFileOperator(\n        task_id=\"copy_adls_file_to_temp_file\",\n        azure_account_name=AZURE_ACCOUNT_NAME,\n        azure_path=AZURE_PATH,\n        local_filepath=\"/tmp/adls_to_sftp_with_cleanup.txt\",\n    )\n    # [END adls_to_sftp_temp_file]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_49_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182253}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me with a Python script to merge two CSV files based on a common column and then save the merged file?"}, {"role": "assistant", "content": "import pandas as pd\n\ndef merge_csv_files(file1_path, file2_path, output_file):\n    # Load the first CSV file into a DataFrame\n    df1 = pd.read_csv(file1_path)\n    \n    # Load the second CSV file into a DataFrame\n    df2 = pd.read_csv(file2_path)\n    \n    # Merge the two DataFrames on the 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='inner')\n    \n    # Save the merged DataFrame to a new CSV file\n    merged_df.to_csv(output_file, index=False)\n\n# Paths to your input CSV files and the output CSV file\nfile1_path = 'input1.csv'\nfile2_path = 'input2.csv'\noutput_file = 'merged_output.csv'\n\n# Call the function to merge the files\nmerge_csv_files(file1_path, file2_path, output_file)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118235}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What is the most concise and linear Python code to find the third smallest element in an unsorted list?"}, {"role": "assistant", "content": "def third_smallest(numbers):\n    # Sort the list in ascending order\n    sorted_numbers = sorted(numbers)\n    \n    # Return the third element from the sorted list\n    return sorted_numbers[2]\n\n# Example usage:\nnumbers = [5, 3, 8, 4, 9, 1]\nprint(third_smallest(numbers))  # Output: 3"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.11824}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced cloud storage transfer techniques using GCS and SFTP operators, with emphasis on handling various file transfer scenarios and implementing robust file transfer verification mechanisms. Develop a workflow that demonstrates different transfer methods while ensuring data integrity through sensor checks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage to SFTP transfer operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_sftp import GCSToSFTPOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_to_sftp\"\n\nBUCKET_NAME = f\"test-gcs-{DAG_ID}-{ENV_ID}\"\nSFTP_PATH = \"/tmp/sftp-to-gcs\"\nLOCAL_FILE_PATH = str(Path(__file__).parent / \"resources\" / \"gcs\")\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def upload_file_to_gcs():\n        from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\n\n        open(f\"{LOCAL_FILE_PATH}/temp_transfer_file.txt\", \"w\").close()\n        GCSToGCSOperator(\n            task_id=\"temp_transfer_file\",\n            source_bucket=BUCKET_NAME,\n            source_object=\"test-gcs-example-files/gcs_to_sftp/temp_transfer_file.txt\",\n            destination_bucket=\"test-gcs-example-files/gcs_to_sftp\",\n            destination_object=SFTP_PATH + \"/temp_transfer_file.txt\",\n        )\n\n    # [START howto_operator_gcs_to_sftp]\n    sftp_to_gcs_task = SFTPToGCSOperator(\n        task_id=\"sftp_to_gcs_task\",\n        sftp_path=SFTP_PATH,\n        bucket=BUCKET_NAME,\n        filename=SFTP_PATH + \"/temp_transfer_file.txt\",\n    )\n    # [END howto_operator_gcs_to_sftp]\n\n    # [START howto_operator_copy_sftp_to_gcs]\n    copy_sftp_to_gcs_task = SFTPToGCSOperator(\n        task_id=\"copy_sftp_to_gcs_task\",\n        sftp_path=SFTP_PATH,\n        bucket=BUCKET_NAME,\n        filename=f\"copy_{SFTP_PATH}temp_transfer_file_copy_{ENV_ID}.txt\",\n        replace=True,\n    )\n    # [END howto_operator_copy_sftp_to_gcs]\n\n    remove_file = GCSCreateBucketOperator(task_id=\"remove_file\", bucket_name=BUCKET_NAME)\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file_to_gcs()\n        # TEST BODY\n        >> sftp_to_gcs_task\n        >> copy_sftp_to_gcs_task\n        # TEST TEARDOWN\n        >> remove_file\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2095_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118248}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms JSON-based Jeopardy question data into vector-enabled database collections using Weaviate's vectorization capabilities. Implement a workflow that shows how to batch import data, handle different vectorization approaches, and manage database collections programmatically.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom weaviate.collections.collection import Collection\nfrom weaviate.utils import clean_uuid\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.weaviate.transfers.weaviate_to_weaviate import WeaviateToWeaviateOperator\n\n\n@task\ndef create_collection_with_vectorizer():\n    \"\"\"\n    Example function to use a custom vectorizer class for a collection.\n    Weaviate provides built-in vectorizers for many types, you can learn more about the available vectorizers\n    here https://weaviate.org/en/docs/deployment/data-model/vectorizers/\n    \"\"\"\n    Collection(\"Jeopardy Question Vector\", vectorizer_class=\"MyVectorizer\")\n\n\n@task\ndef create_new_collection_without_vectorizer():\n    \"\"\"\n    Example function to use a custom vectorizer class for a collection.\n    Weaviate provides built-in vectorizers for many types, you can learn more about the available vectorizers\n    here https://weaviate.org/en/docs/deployment/data-model/vectorizers/\n    \"\"\"\n    Collection(\"NewCollection\", has_vector=True)\n\n\nwith DAG(\n    \"example_weaviate_collections\",\n    schedule=pendulum.periodic_interval(days=1),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    # [START howto_operator_weaviate_to_weaviate]\n    weaviate_to_weaviate_task = WeaviateToWeaviateOperator(\n        task_id=\"weaviate_dttw_example\",\n        weaviate_conn_id=\"weaviate_default\",\n        schema={\"id\": \"string\", \"question\": \"string\", \"answer\": \"string\", \"category\": \"string\"},\n        input_objects=[\n            {\"question\": \"When was the first PC released?\", \"answer\": \"1981\", \"category\": \"Programming\"},\n            {\"question\": \"What is the largest planet in our solar system\", \"answer\": \"Jupiter\", \"category\": \"Science\"},\n        ],\n    )\n    # [END howto_operator_weaviate_to_weaviate]\n\n    create_collection_with_vectorizer()\n    create_new_collection_without_vectorizer()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1831_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182542}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that showcases basic data persistence and retrieval techniques using an embedded database technology. Create a sequential task flow that demonstrates reading and writing operations in a lightweight, file-based key-value store.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SqlExecuteQueryOperator\n\n\n@task(task_id=\"sql_test_task\")\ndef test_sql():\n    # [START create_table]\n    sql = (\n        \"CREATE TABLE employees (id INT, name VARCHAR(50), salary DECIMAL(10, 2))\"\n    )\n    print(f\"Executing SQL: {sql}\")\n    # [END create_table]\n    # [START execute_query]\n    SqlExecuteQueryOperator(task_id=\"sql_execute_query\", sql=sql).execute()\n    # [END execute_query]\n    # [START insert_data]\n    sql = \"INSERT INTO employees VALUES (1, 'Bob', 1000.50)\"\n    print(f\"Executing SQL: {sql}\")\n    # [END insert_data]\n    SqlExecuteQueryOperator(task_id=\"sql_insert_query\", sql=sql).execute()\n    # [START select_all]\n    sql = \"SELECT * FROM employees;\"\n    print(f\"Executing SQL: {sql}\")\n    # [END select_all]\n    SqlExecuteQueryOperator(task_id=\"sql_select_all\", sql=sql).execute()\n    # [START select_one]\n    sql = \"SELECT * FROM employees WHERE name='Bob';\"\n    print(f\"Executing SQL: {sql}\")\n    # [END select_one]\n    SqlExecuteQueryOperator(task_id=\"sql_select_one\", sql=sql).execute()\n    # [END select_one]\n\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_common_sql\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"conn_id\": \"sqlite_default\"},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_sql()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_950_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182592}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data repository management workflow that demonstrates creating, updating, and deleting a Databricks repository while executing a data preparation notebook. The workflow should showcase automated repository lifecycle management and notebook execution in a cloud-based data processing environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Databricks.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_databricks\"  # must match name of external .py file\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_cluster():\n        print(\"Instance name is set during cluster creation.  Please run the task 'list_clusters' before continuing.\")\n\n    create_cluster_task = create_cluster()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1161_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182628}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that illustrates data processing and infrastructure management in a cloud-native Kubernetes environment using dynamic cluster provisioning and ephemeral pod execution. Create a workflow that demonstrates how to programmatically spin up computational resources, execute sample workloads, and cleanly tear down infrastructure after task completion.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Platform Kueue.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.kueue import KueeyHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.providers.google.cloud.operators.kueue import QueueCreateOperator, QueueDeleteOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_kueey\"\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\n\nSPARK_FILE = \"spark_example.py\"\n\nKBUCKET_NAME = f\"kbucket-{DAG_ID}-{ENV_ID}\"\nQUEUE_NAME = f\"queue_{DAG_ID}_{ENV_ID}\".replace(\"-\", \"_\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_spark_file():\n        # [START howto_operator_kueey_create_spark_file]\n        kueey_hook = KueeyHook()\n        spark_file_body = {\n            \"name\": SPARK_FILE,\n            \"gcs_source\": {\"uri\": f\"gs://{BUCKET_NAME}/{SPARK_FILE}\"},\n        }\n        kueey_hook.create_spark_file(QUEUE_NAME, SPARK_FILE, spark_file_body)\n        # [END howto_operator_kueey_create_spark_file]\n\n    @task\n    def create_kbucket():\n        # [START howto_operator_kueey_create_kbucket]\n        kueey_hook = KueeyHook()\n        body = {\"name\": KBUCKET_NAME}  # kbuckect id is required\n        created_kbucket = kueey_hook.create_kbucket(QUEUE_NAME, KBUCKET_NAME, body)\n        # [END howto_operator_kueey_create_kbucket]\n        return created_kbucket[\"id\"]\n\n    @task\n    def create_queue():\n        # [START howto_operator_kueey_create_queue]\n        kueey_hook = KueeyHook()\n        body = {\"name\": QUEUE_NAME}  # queue id is required\n        created_queue = kueey_hook.create_queue(QUEUE_NAME, KBUCKET_NAME, body)\n        # [END howto_operator_kueey_create_queue]\n        return created_queue[\"id\"]\n        # [ERROR_START]\n        # The following error may occur:\n        # googleapi.errorgoogleapi.errors.HttpError: 400, 'The requested entity is invalid. The name '\n        # 'projects//buckets/' is already associated with a bucket.'\n        # [ERROR_END]\n\n    @task\n    def delete_kbucket(kbukkit_id):\n        # [START howto_operator_kueey_delete_kbucket]\n        kueey_hook = KueeyHook()\n        kueey_hook.delete_kbucket(kbukkit_id)\n        # [END howto_operator_kueey_delete_kbucket]\n\n    @task\n    def delete_queue(queue_id):\n        # [START howto_operator_kueey_delete_queue]\n        kueey_hook = KueeyHook()\n        kueey_hook.delete_queue(queue_id)\n        # [END howto_operator_kueey_delete_queue]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_instances(instance_name):\n        client = gcp_compute.InstancesClient()\n        client.delete_instance(name=instance_name)\n\n    create_kbucket_task = create_kbucket()\n    queue_create_task = create_queue()\n    spark_file_create_task = create_spark_file()\n    queue_id = spark_file_create_task.output\n    kbukkit_id = create_kbucket_task.output\n    queue_id2 = QueueCreateOperator(\n        task_id=\"queue_create_task2\", project_id=PROJECT_ID, location=\"europe-west1\", body={\"name\": QUEUE_NAME}\n    )  # queue id is required\n    instance_name = \"compute-instance-0\"\n    delete_instances(instance_name)\n    delete_queue(queue_id2)\n    delete_kbucket(kbukkit_id)\n    DeleteBucketOperator(task_id=\"delete_bucket_task\", bucket=BUCKET_NAME) >> [\n        create_kbucket_task,\n        queue_create_task,\n        spark_file_create_task,\n        delete_instances(instance_name),\n    ]"}], "metadata": {"file_name": "dag_900_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182723}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a sample workflow DAG that simulates a basic system monitoring and testing pipeline, using different system commands and containerized operations to showcase task dependency and execution strategies. Create a sequence of tasks that validate system time, perform wait operations, and execute a containerized process.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\nCONN_ID = \"singularity_conn\"\n\nTASK_ID_1 = \"test_singularity_operator\"\nTASK_ID_2 = f\"test_singularity_task_{DAG_ID}\"\nWORKDIR = \"/tmp\"\nIMAGE = \"docker://busybox\"\n\n\n@task\ndef some_task():\n    print(f\"Hello {WORKDIR}\")\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) is not None:\n\n    # [START howto_operator_singularity]\n    some_task_external_connection = SingularityOperator(\n        singularity_image=IMAGE,\n        task_id=TASK_ID_1,\n        workdir=WORKDIR,\n        connections=[CONN_ID],\n        additional_args=\"-b /dev/null:/dev/null -B /run/:/run/\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    some_task_external_connection_def = SingularityOperator(\n        singularity_image=IMAGE,\n        task_id=TASK_ID_2,\n        workdir=WORKDIR,\n        connections=[CONN_ID],\n        additional_args=\"-b /dev/null:/run -B /run/:/run/\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_singularity]\n\n    some_task_without_connection = SingularityOperator(task_id=TASK_ID_1, extra_args=\"-b /dev/null:/run\")\n\n    some_task_with_env_variable = SingularityOperator(\n        task_id=TASK_ID_2, extra_args=\"--env TEST_ENV variable\"\n    )\n\n    some_task = some_task()\n\n    some_task >> some_task_external_connection >> some_task_without_connection >> some_task_defense\n    some_task >> some_task_with_env_variable\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_127_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182787}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across AWS services like RDS, EC2, and S3. Develop a modular pipeline that creates necessary cloud resources, configures security settings, and executes a data replication process.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom typing import Any\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateEventSubscriptionOperator,\n    RdsDeleteDbInstanceOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_event\"\nDB_NAME = \"testdb\"\nS3_FILENAME = \"output.csv\"\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(5), wait=tenacity.wait_exponential())\ndef create_s3_object(s3_conn, bucket_name, key):\n    return s3_conn.put_object(Bucket=bucket_name, Key=key, Body=\"\")\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(5), wait=tenacity.wait_exponential())\ndef create_event_subscription(rdsconn, db_name, subscription_task_id):\n    return rdsconn.create_event_subscription(\n        SourceType=\"DB_INSTANCE\",\n        SourceIdentifier=str(rdsconn.events_db_instance_endpoints[0]),\n        EventCategories=[\"DB_INSTANCE_END\"],\n        SubscriptionArn=subscription_task_id,\n    )\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(5), wait=tenacity.wait_exponential())\ndef delete_event_subscription(rdsconn, db_name, subscription_arn):\n    return rdsconn.delete_event_subscription(\n        SourceType=\"DB_INSTANCE\",\n        SourceIdentifier=str(rdsconn.events_db_instance_endpoints[0]),\n        SubscriptionArn=subscription_arn,\n    )\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(5), wait=tenacity.wait_exponential())\ndef delete_s3_object(s3_conn, bucket_name, key):\n    return s3_conn.delete_object(Bucket=bucket_name, Key=key)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [START howto_operator_rds_create_db_instance]\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=f\"db-{ENV_ID}\",\n        engine=\"postgres\",\n        rds_settings={\n            \"MasterUsername\": \"rds_username\",\n            \"MasterPassword\": \"rds_password\",\n            \"DBName\": DB_NAME,\n            \"PubliclyAccessible\": False,\n        },\n    )\n    # [END howto_operator_rds_create_db_instance]\n\n    # [START howto_operator_rds_events_create_db_instance_end]\n    events_create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"events_create_db_instance\",\n        db_instance_identifier=f\"db-events-{ENV_ID}\",\n        engine=\"postgres\",\n        rds_settings={\n            \"MasterUsername\": \"rds_username\",\n            \"MasterPassword\": \"rds_password\",\n            \"DBName\": DB_NAME,\n            \"PubliclyAccessible\": False,\n        },\n        event_pattern={\"Actions\": [{\"SourceType\": \"DB_INSTANCE\", \"SourceTypeIdentifier\": f\"{create_db_instance.output['db_instance_identifier}\"}]}},\n    )\n    # [END howto_operator_rds_events_create_db_instance_end]\n\n    # [START howto_operator_rds_delete_db_instance]\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=f\"db-{ENV_ID}\",\n        rds_settings={\n            \"MasterUsername\": \"rds_username\",\n            \"MasterPassword\": \"rds_password\",\n        },\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_rds_delete_db_instance]\n\n    # [START howto_operator_rds_events_delete_db_instance]\n    events_delete_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"events_delete_db_instance\",\n        db_instance_identifier=f\"db-events-{ENV_ID}\",\n        engine=\"postgres\",\n        rds_settings={\n            \"MasterUsername\": \"rds_username\",\n            \"MasterPassword\": \"rds_password\",\n        },\n        event_pattern={\"Actions\": [{\"SourceType\": \"DB_INSTANCE\", \"SourceTypeIdentifier\": f\"{events_create_db_instance.output['db_instance_identifier']}\"}]},\n        destations=[],\n        wait_for_completion=True,\n    )\n    # [END howto_operator_rds_events_delete_db_instance]\n\n    # [START howto_operator_s3_create_bucket]\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=f\"s3://{ENV_ID}\")\n    # [END howto_operator_s3_create_bucket]\n\n    # [START howto_operator_s3_write_file_to_bucket]\n    write_s3_file_to_bucket = S3Hook().write_files_to_bucket(\n        bucket_key=f\"s3://{ENV_ID}/{S3_FILENAME}\", path=S3_FILENAME, conn_id=\"s3_default\"\n    )\n    # [END howto_operator_s3_write_file_to_bucket]\n\n    # [START howto_operator_s3_create_object]\n    create_s3_object_task = S3Hook().create_s3_object(\n        bucket_name=f\"s3://{ENV_ID}\", key=S3_FILENAME, data=\"content of file\"\n    )\n    # [END howto_operator_s3_create_object]\n\n    # [START howto_operator_s3_delete_bucket]\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\", bucket_name=f\"s3://{ENV_ID}\", trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_s3_delete_bucket]\n\n    chain(\n        create_db_instance,\n        create_s3_bucket,\n        write_s3_file_to_bucket,\n        create_s3_object_task,\n        delete_event_subscription(\n            rdsconn, db_name, subscription_arn\n        ),\n        delete_db_instance,\n        delete_event_subscription(rdsconn, db_name, subscription_arn),\n        *[\n            delete_s3_object(s3_conn, bucket_name, key)\n            for bucket_name in [f\"s3://{ENV_ID}\" for _ in range(2)]\n            for key in [S3_FILENAME, ]\n        ],\n        delete_s3_bucket,\n    )\n\n\nsys_test_context_task = SystemTestContextBuilder() \\\n    .add_variable(\"rds_db_instance_endpoints\") \\\n    .build()\n\n\n# provide access to rds_db_instance_endpoints to test functions\ndef get_rds_hook(**kwargs):\n    context = sys_test_context_task()\n    rds_hook_args = {\n        \"database\": context[DB_NAME],\n        \"login\": \"rds_username\",\n        \"port\": 5432,\n        \"conn_max_age\": 300,\n        \"aws_conn_id\": \"aws_default\",\n        \"region_name\": \"us-east-1\",\n    }\n    rds_hook = RdsHook()\n    rds_hook.connect_by_alias(**rds_hook_args)\n    return rds_hook\n\n\n@task\ndef check_rds_endpoint(rds_hook, **kwargs):\n    rds_end_points = rds_hook.get_rds_db_endpoints()\n    assert len(rds_end_points) > 0\n    return True\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_rds_end_points(rds_hook, **kwargs):\n    rds_end_points = rds_hook.get_rds_db_endpoints()\n    if rds_end_points is not None and len(rds_end_points) > 0:\n        rds_hook.disconnect()\n\n\nwith DAG(\n    dag_id=\"example_rds_events\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag_events:\n    check_rds_endpoint(get_rds_hook(), task_id=\"check_rds_endpoint\")\n\n\nwith DAG(\n    dag_id=\"example_rds_events_with_sensor\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag_events_with_sensor:\n    # this task sensor will not wait if all tasks (including the task sensor itself) are skipped\n    check_rds_endpoint(get_rds_hook(skip_all_tasks=True), task_id=\"check_rds_endpoint_skip\")"}], "metadata": {"file_name": "dag_1477_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1182957}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily Twitter data pipeline that collects tweets from multiple accounts, processes them for insights, and stores the results in a database. Create a workflow that captures incoming and outgoing tweets, performs sentiment analysis, and prepares data for further business intelligence.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\nfrom urllib.error import HTTPError\n\nfrom django.conf import settings\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.twitter_to_gcs import TwitterToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests_common.test_utils.watcher import watcher\n\nlog = logging.getLogger(__name__)\n\n\n# [START howto_gcp_connection_gcp_sheets]\nGCP_SITES_SHEET_CONNECTION_ID = \"gcp_sheets\"\n# [END howto_gcp_connection_gcp_sheets]\n\nDIR_NAME = \"DIR_NAME\"\nFILE_NAME = \"FILE_NAME\"\nBUCKET_NAME = f\"bucket_{DIR_NAME}\"\nBUCKET_PATH = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\nCSV_FILE_LOCAL_PATH = \"/tmp/CSV_FILE.csv\"\nCSV_LIST_OUTPUT = \"[u'/tmp/CSV_FILE.csv']\"\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=settings.Connection.TWITTER,\n        extra=json.dumps({\"consumer_key\": settings.TWITTER_CONSUMER_KEY, \"bearer_token\": settings.BEARER_TOKEN}),\n    )\n    db_conn = settings.DATABASES[\"default\"]\n    username = db_conn.get(\"USER\")\n    password = db_conn.get(\"PASSWORD\")\n    host = db_conn.get(\"HOST\", \"\")\n    if host:\n        conn.set_extra({\"database\": db_conn[\"NAME\"], \"username\": username, \"password\": password})\n    else:\n        conn.set_extra({\"database\": db_conn[\"NAME\"], \"username\": username, \"password\": password})\n    log.info(\"Creating connection %s\", conn.id)\n    return conn\n\n\nwith DAG(\n    dag_id=\"example_twitter_gcs\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_twitter_to_gcs]\n    twitter_to_gcs = TwitterToGCSOperator(\n        task_id=\"twitter_to_gcs\",\n        owner=\"airflow\",\n        gcp_conn_id=GCP_SITES_SHEET_CONNECTION_ID,\n        url=\"https://twitter.com/mymachinelearning\",\n        bucket=BUCKET_NAME,\n        filename=FILE_NAME,\n        replace=True,\n    )\n    # [END howto_operator_twitter_to_gcs]\n\n    @task\n    def delete_tweets_gcs_bucket():\n        \"\"\"Delete GCS Bucket\"\"\"\n        delete_bucket = GCSDeleteBucketOperator(\n            task_id=\"delete_bucket\",\n            bucket_name=BUCKET_NAME,\n            trigger_rule=TriggerRule.ALL_DONE,\n        )\n\n        watcher(delete_bucket)\n\n    delete_bucket_task = delete_tweets_gcs_bucket()\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_connection(conn_id_name=GCP_SITES_SHEET_CONNECTION_ID)\n        >> twitter_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket_task\n    )\n\n    from tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n    # Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\n    test_run = get_test_run(dag)\n\n\nif test_run is not None:\n    from tests_common.test_utils.system_tests import get_test_run_results  # noqa: E402\n\n    # This test needs test_run.results()\n    results = get_test_run_results(test_run)"}], "metadata": {"file_name": "dag_1939_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1183045}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code example that demonstrates using the `grpcio-reflection` for debugging a gRPC server in Python?"}, {"role": "assistant", "content": "import grpc\nimport sys\n\n# Import your service definition\nfrom your_service_pb2 import HelloRequest, HelloReply\nfrom your_service_pb2_grpc import add_YourServiceServicer_to_server\n\nclass YourServiceServicer(object):\n    def Echo(self, request, context):\n        return HelloReply(message=request.message)\n\ndef serve(port):\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    add_YourServiceServicer_to_server(YourServiceServicer(), server)\n    server.add_insecure_port('[::]:%d' % port)\n    server.start()\n    print(\"Server started at localhost:%d\" % port)\n    try:\n        while True:\n            time.sleep(86400)  # Keep running forever\n    except KeyboardInterrupt:\n        server.stop(0)\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print('Usage: python3 server.py <port>')\n        sys.exit(1)\n\n    port = int(sys.argv[1])\n    serve(port)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1183152}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs data transfer operations between BigQuery tables, illustrating how to programmatically create table schemas, copy data between tables, and export data to cloud storage. Implement a complete data workflow that includes setup, data movement, and cleanup stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\n    from airflow.providers.google.cloud.operators.bigquery import (\n        BigQueryCreateEmptyDatasetOperator,\n        BigQueryCreateTableOperator,\n        BigQueryDeleteDatasetOperator,\n        BigQueryGetTableOperator,\n        BigQueryUpdateDatasetOperator,\n        BigQueryUpdateTableOperator,\n    )\nexcept ImportError:\n    # This import throws an ImportError due to broken dependency version management.\n    # Cope with it by skipping all the tests related to this in the non-disabled case.\n    pytest.skip(\"bigquery operators are disabled because of broken dependencies\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_bigquery_to_gcs\"\n\nTABLE_1 = \"table1\"\nTABLE_2 = \"table2\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us\"\n\n# [START howto_operator_bigquery_to_bigquery]\ndef dataset_create_hook(dataset_id):\n    \"\"\"\n    Create big query dataset\n    \"\"\"\n    bigquery = BigQueryHook()\n    hook_fields = [\n        {\n            \"name\": \"dataset_id\",\n            \"value\": dataset_id,\n        }\n    ]\n    bigquery.create_dataset(dataset_id, project_id=PROJECT_ID, hook_fields=hook_fields)\n\n\ndef table_create_hook(dataset_id, table_id):\n    \"\"\"\n    Create big query table\n    \"\"\"\n    bigquery = BigQueryHook()\n    hook_fields = [\n        {\n            \"name\": \"dataset_id\",\n            \"value\": dataset_id,\n        },\n        {\n            \"name\": \"table_id\",\n            \"value\": table_id,\n        },\n    ]\n    bigquery.create_table(table_id, project_id=PROJECT_ID, hook_fields=hook_fields)\n\n\ndef list_tables_hook(dataset_id):\n    \"\"\"\n    List big query tables\n    \"\"\"\n    bigquery = BigQueryHook()\n    result = bigquery.limits.list(dataset_id=dataset_id)\n    return result\n\n\ndef get_table_metadata_hook(table_id):\n    \"\"\"\n    Get specific big query table metadata\n    \"\"\"\n    bigquery = BigQueryHook()\n    result = bigquery.get_table_metadata(table_id)\n    return result\n\n\ndef update_dataset_hook(dataset_id, is_user_managed=\"TRUE\"):\n    \"\"\"\n    Update big query dataset as user-managed\n    \"\"\"\n    bigquery = BigQueryHook()\n    project_id = PROJECT_ID\n    hook_fields = [\n        {\n            \"name\": \"dataset_id\",\n            \"value\": dataset_id,\n        },\n        {\n            \"name\": \"is_user_managed\",\n            \"value\": is_user_managed,\n        },\n    ]\n    bigquery.update_dataset(dataset_id, project_id, hook_fields=hook_fields)\n\n\ndef update_table_hook(dataset_id, table_id, schema_fields):\n    \"\"\"\n    Update big query table by setting its schema\n    \"\"\"\n    bigquery = BigQueryHook()\n    project_id = PROJECT_ID\n    hook_fields = [\n        {\n            \"name\": \"dataset_id\",\n            \"value\": dataset_id,\n        },\n        {\n            \"name\": \"table_id\",\n            \"value\": table_id,\n        },\n        {\n            \"name\": \"schema\",\n            \"value\": schema_fields,\n        },\n    ]\n    bigquery.update_table(dataset_id, table_id, project_id, hook_fields=hook_fields)\n\n\ndef delete_dataset_hook(dataset_id):\n    \"\"\"\n    Delete big query dataset\n    \"\"\"\n    bigquery = BigQueryHook()\n    project_id = PROJECT_ID\n    bigquery.delete_dataset(dataset_id, project_id)\n\n\ndef delete_table_hook(dataset_id, table_id):\n    \"\"\"\n    Delete big query table\n    \"\"\"\n    bigquery = BigQueryHook()\n    project_id = PROJECT_ID\n    bigquery.delete_table(table_id, dataset_id, project_id)\n#endif\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_bucket = BigQueryToBigQueryOperator(\n        task_id=\"create_bucket\",\n        source_project_dataset_table=f\"{PROJECT_ID}.{TABLE_1}\",\n        destination_project_dataset_table=f\"{BUCKET_NAME}\",\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        location=LOCATION,\n    )\n\n    insert_table = BigQueryCreateTableOperator(\n        task_id=\"insert_table\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        table_id=TABLE_2,\n        table_resource={\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                    {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n                ]\n            },\n            \"timePartitioning\": {\n                \"dayOfTheWeek\": [\"SUNDAY\"],\n                \"dayOfMonth\": [3],\n                \"month\": [9],\n                \"year\": [2022],\n            },\n        },\n    )\n\n    # [START howto_operator_bigquery_to_gcs]\n    bigquery_to_gcs = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs\",\n        source_project_dataset_table=f\"{PROJECT_ID}.{TABLE_2}\",\n        destination_cloud_storage_uris=[\"gs://airflow-system-tests/test/bq_output\"],\n    )\n    # [END howto_operator_bigquery_to_gcs]\n\n    delete_table_2 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_table_2\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = BigQueryToBigQueryOperator(\n        task_id=\"delete_bucket\",\n        source_project_dataset_table=f\"{BUCKET_NAME}\",\n        destination_project_dataset_table=f\"{PROJECT_ID}.{TABLE_1}\",\n    )\n\n    rename_table_1 = BigQueryCreateTableOperator(\n        task_id=\"rename_table_1\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        table_id=\"new_table\",\n        table_resource={\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                    {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n                ]\n            },\n        },\n    )\n\n    update_table_1 = BigQueryUpdateTableOperator(\n        task_id=\"update_table_1\",\n        table_resource={\n            \"description\": \"Test description\",\n        },\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        table_id=\"new_table\",\n        project_id=PROJECT_ID,\n    )\n\n    update_dataset_1 = BigQueryUpdateDatasetOperator(\n        task_id=\"update_dataset_1\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        is_user_managed=\"FALSE\",\n        project_id=PROJECT_ID,\n    )\n\n    update_table_2 = BigQueryUpdateTableOperator(\n        task_id=\"update_table_2\",\n        table_resource={\n            \"description\": \"Test description\",\n        },\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        table_id=\"new_table\",\n        project_id=PROJECT_ID,\n    )\n\n    update_dataset_2 = BigQueryUpdateDatasetOperator(\n        task_id=\"update_dataset_2\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        is_user_managed=\"TRUE\",\n        project_id=PROJECT_ID,\n    )\n\n    refresh_table_1 = BigQueryHook().refresh(\n        api_version=\"v1\",\n        project_id=PROJECT_ID,\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        body={\"readTimeSpan\": {\"endTimestamp\": 1657224800}},\n        task_id=\"refresh_table_1\",\n    )\n\n    disable_table_1 = BigQueryHook().disable(\n        api_version=\"v1\",\n        project_id=PROJECT_ID,\n        table_id=\"new_table\",\n        task_id=\"disable_table_1\",\n    )\n\n    enable_table_1 = BigQueryHook().enable(\n        api_version=\"v1\",\n        project_id=PROJECT_ID,\n        table_id=\"new_table\",\n        task_id=\"enable_table_1\",\n    )\n\n    check_table = BigQueryGetTableOperator(\n        task_id=\"check_table\",\n        project_id=PROJECT_ID,\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        table_id=\"new_table\",\n    )\n\n    delete_table_1 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_table_1\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_1,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_dataset_1 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset_1\",\n        dataset_id=PROJECT_ID + \".\" + TABLE_2,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    run_all_tasks_now = lambda task_list: [task_instance.for_task(task) for task in task_list].sort_by(\"start_time\")[\n        -1\n    ].poke()\n\n    add_dependency = (\n        [create_bucket, create_dataset, insert_table, refresh_table_1, disable_table_1, enable_table_1, check_table]\n        | rename_table_1\n        | update_table_1\n        | update_dataset_1\n        | insert_table\n        | delete_table_1\n        | delete_dataset_1\n        | run_all_tasks_now\n        | [delete_bucket, delete_table_2]\n    )\n\n    (add_dependency >> create_bucket >> create_dataset >> insert_table >> [delete_table_2, create_table])\n    (insert_table >> bigquery_to_gcs >> [delete_table_2, delete_bucket])\n    (create_table >> [update_table_1, update_dataset_1, insert_table] >> [delete_table_1, delete_dataset_1])\n    (insert_table >> [update_table_2, update_dataset_2, rename_table_1] >> [delete_table_2, delete_dataset_2])\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n    from tests.system.utils import get_test_run\n\n    # This test needs run_local=True because .test_runner() waits for\n    # the tasks to finish\n    test_run = get_test_run(dag)\n    test_run.xcom_pull(task_ids=[\"add_dependency\"], key=\"return_value\")\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_483_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1183386}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates setup and teardown tasks for a cloud infrastructure workflow. Create a DAG that shows how to manage resource allocation and cleanup across multiple task stages.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.base import SFTPSensor\nfrom airflow.providers.sftp.sensors.file import SFTPFileSensor\nfrom airflow.providers.sftp.sensors.folder import SFTPFolderSensor\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_sensor\"\n\nconn = Connection(\n    conn_id=\"sftp_default\",\n    description=\"SFTP connection for demo purposes\",\n    conn_type=\"sftp\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task(task_id=\"get_dir_sensor\")\n    def get_dir():\n        \"\"\"A dummy task to run before the folder sensor which uses stdout output\"\"\"\n        print(\"Hello SFTP\")\n\n    @task(task_id=\"create_connection\")\n    def _create_connection():\n        session = Session()\n        session.add(conn)\n        session.commit()\n\n    @task(task_id=\"remove_connection\")\n    def _remove_connection():\n        session = Session()\n        conn = session.query(Connection).filter(Connection.conn_id == \"sftp_default\").first()\n        if conn:\n            session.delete(conn)\n            session.commit()\n\n    (create_conn_task = _create_connection() >> trigger_rule=TriggerRule.CREATE)\n    (remove_conn_task = _remove_connection() >> trigger_rule=TriggerRule.DELETE)\n\n    # [START sftp_get_file_sensor]\n    get_file = SFTPFileSensor(\n        task_id=\"get_file_sftp_sensor\", path=\"/tmp/test_file.txt\", sftp_conn_id=\"sftp_default\"\n    )\n    # [END sftp_get_file_sensor]\n\n    # [START sftp_sinned_file_sensor]\n    get_file_sinned = SFTPFileSensor(\n        task_id=\"get_file_sinned_sftp_sensor\",\n        path=\"/tmp/test_file.txt\",\n        sftp_conn_id=\"sftp_default\",\n        poke_interval=5,\n    )\n    # [END sftp_sinned_file_sensor]\n\n    # [START sftp_get_folder_sensor]\n    get_folder = SFTPFolderSensor(\n        task_id=\"get_folder_sftp_sensor\", path=\"/tmp/\", sftp_conn_id=\"sftp_default\"\n    )\n    # [END sftp_get_folder_sensor]\n\n    # [START sftp_sinned_folder_sensor]\n    get_folder_sinned = SFTPFolderSensor(\n        task_id=\"get_folder_sinned_sftp_sensor\",\n        path=\"/tmp/\",\n        sftp_conn_id=\"sftp_default\",\n        poke_interval=5,\n    )\n    # [END sftp_sinned_folder_sensor]\n\n    (\n        create_conn_task\n        >> get_dir()\n        >> [\n            create_conn_task,\n            get_file,\n            get_file_sinned,\n            remove_conn_task,\n            get_folder,\n            get_folder_sinned,\n        ]\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_23_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118348}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS cloud infrastructure orchestration with dynamic security configuration and EMR cluster management. Develop a workflow that creates an S3 bucket, configures security settings, launches an EMR cluster, executes a Spark step, and handles cluster lifecycle management.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom typing import Any\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.emr import EmrClusterState\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyClusterOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrSensor, EmrStepSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Saturday, Sunday\n\nDAG_ID = \"example_emr\"\nENV_ID = \"byoe\"\nJOB_FLOW_OVERRIDES = {\n    \"Name\": DAG_ID,\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\"Name\": \"Primary node\", \"Market\": \"ON_DEMAND\", \"InstanceType\": \"m5.large\", \"HadoopVersion\": \"2.6\"}\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": [\n        {\n            \"Name\": \"PrintHelloWorld\",\n            \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n            \"HadoopJarStep\": {\n                \"Args\": [\"pi.py\", \"10\"],\n            },\n        }\n    ],\n}\nS3_KEY = \"S3 bucket key for input data\"\nINPUT_FILE_PATH = \"input.txt\"\nOUTPUT_FILE_PATH = \"output.txt\"\nBUCKET_NAME = f\"{ENV_ID}-{DAG_ID}-working-bucket}\"\nJOB_FLOW_ROLE_ARN = boto3.client(\"emr\").get_job_flow_role()[\"JobFlowRole\"]\nJOB_ROLE_WITHOUT_EMPLOYEE_ARN = f\"arn:aws:iam::aws:roleEMRServiceRole\"\nSERVICE_ROLE_ARN = boto3.client(\"emr\").get_job_flow_service_role()[\"JobFlowServiceRole\"]\n\n# Path to the file containing the input data\nFILE_path = \"/ems/input.txt\"\n# HDFS path to the folder where the file has been uploaded\nHDFS_path = \"/ems/\"\nSHORTcNAME = \"tmp\"\n# Name of the Amazon S3 bucket\nbucket_name = BUCKET_NAME\n# Key of the object inside the bucket\ninput_file_key = INPUT_FILE_PATH\noutput_file_key = OUTPUT_FILE_PATH\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_amzn_dataset():\n    emr = boto3.client(\"emr\")\n    response = emr.terminate_job_flow(\"JobFlowId\", waitForCompletion=True)\n\n\n@task\ndef setup_bucket():\n    s3 = S3Hook()\n    if not s3.exists_bucket(bucket_name):\n        location = boto3.client(\"s3\").get_location(CurrentRegion=\"us-east-1\")\n        s3.create_bucket(\n            Bucket=bucket_name,\n            CreateBucketConfiguration=location,\n        )\n    else:\n        print(\"Bucket already exists\")\n\n\n@task\ndef add_step(step_id: str):\n    # [START howto_operator_emr]\n    from airflow.providers.amazon.aws.operators.emr import EmrStartJobFlowOperator\n\n    start_job = EmrStartJobFlowOperator(\n        task_id=step_id,\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n    )\n    # [END howto_operator_emr]\n    start_job.poke_interval = 30\n\n\n@task\ndef modify_cluster(cluster_id: str, new_steps: list[str]):\n    # [START howto_operator_emr_modify_cluster]\n    from airflow.providers.amazon.aws.operators.emr import EmrModifyClusterOperator\n\n    modify_cluster = EmrModifyClusterOperator(\n        task_id=\"modify_cluster\",\n        cluster_id=cluster_id,\n        steps=new_steps,\n    )\n    # [END howto_operator_emr_modify_cluster]\n    modify_cluster.poke_interval = 30\n\n\n@task\ndef get_step_id(step_ids: list[str]):\n    # [START howto_sensor_emr_step]\n    from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor\n\n    sensor = EmrStepSensor(\n        task_id=\"step_sensor\",\n        step_id=step_ids[0],\n        target_states=[EmrClusterState.FINISHED],\n    )\n    # [END howto_sensor_emr_step]\n    sensor.deferrable = True\n    return sensor\n\n\n@task\ndef check_steps_completed(step_ids: list[str]) -> bool:\n    # [START howto_check_emr_steps]\n    from airflow.providers.amazon.aws.sensors.emr import EmrCheckStepsCompletedSensor\n\n    sensor = EmrCheckStepsCompletedSensor(\n        task_id=\"check_steps_completed\",\n        step_ids=step_ids,\n    )\n    # [END howto_check_emr_steps]\n    sensor.deferrable = True\n    return sensor\n\n\n@task\ndef await_no_steps(waiter_id: str):\n    # [START howto_waiter_emr]\n    from airflow.providers.amazon.aws.waitsers.emr import EmrWaiter\n\n    waiter = EmrWaiter(\n        task_id=waiter_id,\n        waiter_id=\"EmrStepSensor\",\n        state=\"success\",\n        delay=20,\n        max_attempts=100,\n        failure_threshold=3,\n    )\n    # [END howto_waiter_emr]\n    waiter.check_for_state()\n\n\n@task(trigger_rule=TriggerRule.ONE_OFF)\ndef create_s3_object():\n    s3 = S3Hook()\n    s3conn = boto3.resource(\"s3\")\n    bucket = s3conn.Bucket(bucket_name)\n    key = f\"{HDFS_path}{input_file_key}\"\n    body = f\"\"\"\\\n        This is the content of {input_file_key}\n    \"\"\"\n    bucket.put_object(Key=key, Body=body)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_s3_object():\n    s3 = S3Hook()\n    s3.delete_bucket(Bucket=bucket_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = setup_bucket()\n\n    @task\n    def store_file_in_s3(bucket_name: str, input_file_key: str):\n        s3 = S3Hook()\n        s3conn = boto3.resource(\"s3\")\n        key = f\"{HDFS_path}{input_file_key}\"\n        bucket = s3conn.Bucket(bucket_name)\n        body = f\"\"\"\\\n            This is the content of {input_file_key}\n        \"\"\"\n        bucket.put_object(Key=key, Body=body)\n\n    store_file_in_s3 = store_file_in_s3(bucket_name, input_file_key)\n\n    upload_file_to_s3bucket = BaseOperator(\n        task_id=\"upload_file_to_s3bucket\",\n        bash_command=f\"python -m boto3.s3.upload_file '{FILE_path}' '{SHORTcNAME}/{input_file_key}' '{S3_KEY}/{input_file_key}'\",\n    )\n\n    create_emr_job_flow = EmrCreateJobFlowOperator(\n        task_id=\"create_emr_job_flow\",\n        job_flow_name=DAG_ID,\n        release_label=\"emr-6.7.0\",\n        application_names=[\"Spark\"],\n        steps=[\n            {\"name\": \"PrintHelloWorld\", \"actionOnFailure\": \"TERMINATE_JOB_FLOW\", \"hadoopJarStep\": {\"args\": [\"pi.py\", \"10\"]}},\n        ],\n    )\n\n    get_step_id = get_step_id([\"PrintHelloWorld\"])\n\n    await_no_steps = await_no_steps(\"await_no_steps\")\n\n    stop_emr_job_flow = EmrTerminateJobFlowOperator(\n        task_id=\"stop_emr_job_flow\",\n        job_flow_id=\"{{ ti.xcom_pull('create_s3_bucket') }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    check_create_emr_job_flow = EmrSensor(\n        task_id=\"check_create_emr_job_flow\",\n        job_flow_id=\"{{ ti.xcom_pull('create_emr_job_flow') }}\",\n        target_states=[\"PROGRESS_STATUS_RUNNING\"],\n        poke_interval=5,\n        max_retries=10,\n    )\n\n    get_s3_uri = BashOperator(\n        task_id=\"get-s3-uri\",\n        bash_command=f\"echo {{ params.S3_KEY }}\",\n    )\n\n    store_file_in_s3_bucket = BashOperator(\n        task_id=\"store-file-in-s3-bucket\",\n        bash_command=f\"echo {{ params.S3_KEY }}\"></ems/{output_file_key} >/ems/output.txt\",\n    )\n\n    modify_cluster = modify_cluster(\"modify_cluster\", [\"jar:file:///usr/lib/spark/examples/src/main/python/pi.py?line=9\"])\n\n    get_step_id = get_step_id([\"jar:file:///usr/lib/spark/examples/src/main/python/pi.py?line=9\"])\n\n    await_steps_completed = EmrCheckStepsCompletedSensor(\n        task_id=\"check_steps_completed\",\n        step_ids=[\"jar:file:///usr/lib/spark/examples/src/main/python/pi.py?line=9\"],\n    )\n\n    stop_emr_cluster = EmrModifyClusterOperator(\n        task_id=\"stop_emr_cluster\",\n        cluster_id=\"{{ ti.xcom_pull('create_s3_bucket') }}\",\n        modified_elements=[{\"actionOnFailure\": \"TERMINATE_CLUSTER\"}],\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=BUCKET_NAME,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        create_s3_bucket\n        >> upload_file_to_s3bucket\n        >> create_emr_job_flow\n        >> get_step_id\n        >> await_no_steps\n        >> stop_emr_job_flow\n        >> check_create_emr_job_flow\n        >> get_s3_uri\n        >> store_file_in_s3_bucket\n        >> modify_cluster\n        >> get_step_id\n        >> await_steps_completed\n        >> stop_emr_cluster\n        >> delete_s3_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_389_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1183677}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a project where I need to create a function that determines the academic year based on a given date. The academic year typically starts on September 1st and ends on August 31st of the following year. The function should take a date as input and return the academic year in the format \"YYYY-YYYY\". For example, for a date of February 1, 2023, the function should return \"2022-2023\". How can I implement this function in Python?"}, {"role": "assistant", "content": "from datetime import datetime\n\ndef get_academic_year(date):\n    # Convert the input string to a datetime object\n    dt = datetime.strptime(date, '%Y-%m-%d')\n    \n    # Calculate the start and end dates of the current academic year\n    current_date = dt.replace(month=9)\n    next_month = (dt + relativedelta.relativedelta(months=+1)).replace(day=1) - relativedelta.relativedelta(days=1)\n    \n    # Determine which academic year the date falls into\n    if dt.month <= 8:\n        academic_year_start = current_date.strftime('%Y')\n        academic_year_end = next_month.strftime('%Y')\n    else:\n        academic_year_start = next_month.strftime('%Y')\n        academic_year_end = dt.year\n    \n    # Return the academic year in the format YYYY-YYYY\n    return f\"{academic_year_start}-{academic_year_end}\"\n\n# Example usage\ndate_input = \"2023-02-01\"\nprint(get_academic_year(date_input))  # Output: 2022-2023"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118375}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates complex Cloud SQL operations including instance creation, data export/import, database configuration changes, and infrastructure cleanup. Develop a systematic workflow that demonstrates enterprise-grade database infrastructure automation.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud SQL service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.cloud_sql import (\n    CloudSQLCreateInstanceDatabaseOperator,\n    CloudSQLCreateInstanceOperator,\n    CloudSQLDeleteInstanceDatabaseOperator,\n    CloudSQLDeleteInstanceOperator,\n    CloudSQLExportDatabaseOperator,\n    CloudSQLImportDatabaseOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloudsql\"\nINSTANCE_NAME = f\"{DAG_ID}-{ENV_ID}-instance\".replace(\"_\", \"-\")\nDB_NAME = \"testdb\"\n\n# Body of instance created via Cloud Shell\nCLOUD_SQL_INSTANCE_BODY = {\n    \"name\": INSTANCE_NAME,\n    \"settings\": {\n        \"backupConfiguration\": {\"minutes\": 42},\n        \"activationPolicy\": \"ALWAYS\",\n        \"dataDiskSizeGb\": 30,\n        \"dataDiskType\": {\"name\": \"PD_SSD\", \"category\": \"STORAGE\"},\n        \"ipConfiguration\": {},\n        \"locationPreference\": {\"zone\": \"europe-west4-a\"},\n        \"maintenanceWindow\": {\"hour\": 15, \"day\": 7, \"updateTrack\": \"CANARY\"},\n        \"pricingPlan\": {\"name\": \"PER_USE\", \"maxConcurrentQueries\": 1},\n        \"region\": \"europe-west4\",\n        \"storageAutoResize\": True,\n        \"storageAutoResizeLimit\": 10,\n    },\n    \"databaseFlags\": [{\"name\": \"/cloud/sql/project_id\", \"value\": PROJECT_ID}],\n}\n\n# Database created via Cloud Shell operator\n# [START howto_operator_gcp_cloudsql_create_instance_database]\ndef _create_database(instance_name, db_name):\n    return {\n        \"name\": db_name,\n        \"databaseVersion\": \"BB_2_2\",\n        \"storageAutoResize\": True,\n        \"storageAutoResizeLimit\": 10,\n    }\n# [END howto_operator_gcp_cloudsql_create_instance_database]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcp_cloudsql_create_instance]\n    create_instance = CloudSQLCreateInstanceOperator(\n        task_id=\"create-instance\",\n        project_id=PROJECT_ID,\n        instance_name=INSTANCE_NAME,\n        body=CLOUD_SQL_INSTANCE_BODY,\n    )\n    # [END howto_operator_gcp_cloudsql_create_instance]\n\n    create_database = CloudSQLCreateInstanceDatabaseOperator(\n        task_id=\"create-database\",\n        instance_name=INSTANCE_NAME,\n        database_name=DB_NAME,\n        body=_create_database(INSTANCE_NAME, DB_NAME),\n    )\n\n    # [START howto_operator_gcp_cloudsql_export_database]\n    export_db_task = CloudSQLExportDatabaseOperator(\n        task_id=\"export-db-task\",\n        source_instance_name=INSTANCE_NAME,\n        source_database_name=DB_NAME,\n        destination_project_id=\"your-destination-project-id\",\n        destination_project_region=\"us-central1\",\n        destination_file_path=\"/path/to/exported/db.sql\",\n    )\n    # [END howto_operator_gcp_cloudsql_export_database]\n\n    # [START howto_operator_gcp_cloudsql_import_database]\n    import_db_task = CloudSQLImportDatabaseOperator(\n        task_id=\"import-db-task\",\n        source_instance_name=INSTANCE_NAME,\n        source_database_name=DB_NAME,\n        destination_instance_name=INSTANCE_NAME,\n        destination_database_name=DB_NAME + \"_ib\",\n        body={},\n    )\n    # [END howto_operator_gcp_cloudsql_import_database]\n\n    delete_database = CloudSQLDeleteInstanceDatabaseOperator(\n        task_id=\"delete-database\",\n        instance_name=INSTANCE_NAME,\n        database_name=DB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_instance = CloudSQLDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        instance_name=INSTANCE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_instance >> create_database >> export_db_task >> import_db_task >> delete_database >> delete_instance\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2005_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118386}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a cloud-based data processing job using Google Dataproc, focusing on the sequential task dependencies of cluster creation, job submission, and cluster deletion. Implement a robust workflow that handles infrastructure provisioning and job execution with clear, predictable task progression.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with hive job.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcp_dataproc_hive\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-hive\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n}\n\n# Job to submit\n# [START how_to_cloud_dataproc_hive_config]\nHIVE_JOB = {\"reference\": {\"project_id\": PROJECT_ID}, \"inside_job\": {}}\n# <\u53c2\u8003> https://cloud.google.com/dataproc/docs/reference/latest/SparkSqlInstance\nHIVE_JOB[\"inside_job\"][\"spark_sql_instance\"] = {\n    \"query_table_mappings\": {\n        \"-ivb-\": [\n            \"student_id\",\n            \"first_name\",\n            \"last_name\",\n            \"gender\",\n            \"major\",\n            \"grade\",\n            \"school_year\",\n        ]\n    }\n}\n# </\u53c2\u8003>\nHIVE_JOB[\"action\"] = {\"start_now\": {\"enabled\": True}}\nJOB_TO_SUBMIT = {\"reference\": {\"project_id\": PROJECT_ID}, \"hive_job\": HIVE_JOB}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    region=REGION,\n    project_id=PROJECT_ID,\n    max_active_runs=1,\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    run_job = DataprocSubmitJobOperator(\n        task_id=\"run_job\", job=JOB_TO_SUBMIT, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (create_cluster >> run_job >> delete_cluster)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_863_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.118394}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated workflow that generates vector embeddings for text data using OpenAI and stores them in a PostgreSQL database with vector capabilities. Create a data pipeline that demonstrates how to transform text into machine-readable vector representations and persist them for potential machine learning or semantic search applications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom huggingface_hub.hf_api import HfApi\nfrom huggingface_hub.utils._version_utils import parse_version\n\nif parse_version(HfApi().search_repo_token()) < parse_version(\"0.16.4\"):\n    raise ImportError(\n        \"The installed version of `huggingface-hub` should be greater than \"\n        \"`0.16.4`, but it's v%(v)s. Try upgrading with `pip install --upgrade \"\n        \"`huggingface-hub==latest`. \"\n        % {\"v\": HfApi().search_repo_token()}\n    )\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.openai.operators.openai import OpenAIEmbeddingOperator\nfrom airflow.providers.pgvector.operators.pgvector import PgVectorIngestOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.openai.utils import SystemTestContextBuilder\n\nENV_ID = pendulum.now().id\nDAG_ID = \"example_openai_embedding_to_pgvector\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"openai\"],\n) as dag:\n    test_context = SystemTestContextBuilder().build()\n\n    embedding_data = [\n        (\"This is some text\", []),\n        (b\"3dbb... ddd\", [\"image\"]),\n        (b\"a question about transformers...\" + b\"A A A\", [\"article\", \"question\"]),\n    ]\n\n    @task\n    def create_table(conn_id: str):\n        \"\"\"Create table pgvector\u77e5\u8bc6\u8868\"\"\"\n        HfApi().create_table_from_original_file(\n            path=\"https://storage.googleapis.com/huggingface-containers/vector-data.csv\",\n            owner=\"huggingface-containers\",\n            conn_id=conn_id,\n            schema=\"postgres\",\n            table=\"knowledge\",\n            columns=[\n                (\"input\", \"TEXT\"),\n                (\"vector\", \"BYTEA\"),\n            ],\n        )\n\n    create_table_task = create_table(conn_id=test_context.pg_conn_id)\n\n    @task\n    def insert_embeddings_into_table(conn_id: str):\n        \"\"\"\u63d2\u5165\u5411\u91cf\u5316\u540e\u7684\u6570\u636e\u5230pgvector\u7684\u77e5\u8bc6\u8868\u683c\u4e2d\"\"\"\n        HfApi().insert_into_original_file(\n            path=\"3dbb... ddd\",\n            owner=\"huggingface-containers\",\n            conn_id=conn_id,\n            schema=\"postgres\",\n            table=\"knowledge\",\n            columns=[\"input\", \"vector\"],\n        )\n        HfApi().insert_into_original_file(\n            b\"a question about transformers...\" + b\"A A A\",\n            b\"image\",\n            owner=\"huggingface-containers\",\n            conn_id=conn_id,\n            schema=\"postgres\",\n            table=\"knowledge\",\n            columns=[\"input\", \"vector\"],\n        )\n\n    insert_embeddings_into_table_task = insert_embeddings_into_table(conn_id=test_context.pg_conn_id)\n\n    # [START howto_operator_openai_embedding_to_pgvector]\n    embed_and_ingest_dag = DAG(\n        dag_id=\"embed_and_ingest_dag\",\n        schedule=None,\n        start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n        catchup=False,\n        tags=[\"example\", \"openai\"],\n    )\n    # [END howto_operator_openai_embedding_to_pgvector]\n\n    # [START howto_operator_openai_embedding]\n    embed_task = OpenAIEmbeddingOperator(\n        task_id=\"embed_task\",\n        input_text=[d[\"text\"] for d in embedding_data],\n        conn_id=test_context.openai_conn_id,\n    )\n    # [END howto_operator_openai_embedding]\n\n    # [START howto_operator_pgvector_ingest]\n    pgvector_ingest_task = PgVectorIngestOperator(\n        task_id=\"pgvector_ingest_task\",\n        table=\"knowledge\",\n        columns=[\"input\", \"vector\"],\n        conn_id=test_context.pg_conn_id,\n        extra={\"data\": [d[\"vector\"] for d in embedding_data]},\n    )\n    # [END howto_operator_pgvector_ingest]\n\n    (\n        # TEST SETUP\n        test_context.pg_conn_id >> create_table_task\n        >> insert_embeddings_into_table_task\n        # TEST BODY\n        >> embed_task\n        >> pgvector_ingest_task\n        # TEST TEARDOWN\n        >> embed_task.drop()\n        >> pgvector_ingest_task.drop()\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_989_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1184056}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that leverages Databricks for scalable data processing, showing how to dynamically create jobs, run notebooks, and manage computational resources for data transformation tasks. Implement a workflow that demonstrates flexible cluster configuration and notebook-based data preparation strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with sparkr job.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcp_dataproc_sparkr\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_sparkr_{ENV_ID}\"\nREGION = \"europe-west2\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"size_gb\": 1024, \"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": [\n        {\n            \"num_instances\": 2,\n            \"machine_type_uri\": \"n1-standard-4\",\n            \"disk_config\": {\"size_gb\": 1024, \"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n        }\n    ],\n    \"software_config\": {\n        \"image_version\": \"2.0.1\",\n        \"spark_r_version\": \"2.1.1\",\n    },\n}\n\n# Jobs definitions\n# [START howto_dataproc_sparkr_configuration]\nSPARKR_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID, \"region\": REGION},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_r_job_properties\": {\n        \"main_r_file_uri\": \"gs://airflow-system-tests-resources/dataproc/sparkr/hello_r.py\",\n    },\n}\n# [END howto_dataproc_sparkr_configuration]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    create_cluster_sparkr = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_sparkr\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        config=CLUSTER_CONFIG,\n        min_worker_nodes=1,\n        max_worker_nodes=2,\n        is_preemptible=True,\n    )\n\n    sparkr_task = DataprocSubmitJobOperator(\n        task_id=\"sparkr_task\", job=SPARKR_JOB, region=REGION, project_id=PROJECT_ID\n    )\n\n    delete_cluster_sparkr = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_sparkr\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (create_cluster_sparkr >> sparkr_task >> delete_cluster_sparkr)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1555_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1184132}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task chaining and cross-task communication by translating text and then accessing the translation result through XCom. Demonstrate how different operators can interact and pass data between tasks in a sequential workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of XComs.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.xcom import task_xcom_push\nfrom airflow.sdk import dag, task\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    tags=[\"example\", \"xcom\"],\n    catchup=False,\n)\ndef example_xcom_push():\n    # Push a string into an xcom task group\n    @task(xcom_task_group=\"push_xcom\")\n    def push_string():\n        task.xcom_push(task_id=\"test_xcom_push\", value=\"Hello World\")\n\n    # Pull the string pushed before\n    push_string_output = task.xcom_pull(\n        task_ids=\"push_xcom\", key=\"test_xcom_push\", task_group=\"push_xcom\"\n    )\n\n    # Push a datetime into an xcom task group\n    @task(xcom_task_group=\"push_xcom\")\n    def push_datetime():\n        task.xcom_push(task_id=\"test_xcom_push\", value=pendulum.now())\n\n    # Pull the datetime pushed before\n    push_datetime_output = task.xcom_pull(\n        task_ids=\"push_xcom\", key=\"test_xcom_push\", task_group=\"push_xcom\"\n    )\n\n\nexample_dag = example_xcom_push()\n\n\n# [START howto_xcom_push_operator]\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule is part of the DAG\nlist(dag.tasks) >> watcher()\n\n\n# [END howto_xcom_push_operator]"}], "metadata": {"file_name": "dag_1297_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1184182}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me with a Python code to filter and sum a list of numbers based on a condition?"}, {"role": "assistant", "content": "# List of numbers\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Filter even numbers using filter() and lambda function\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n\nprint(even_numbers)  # Output: [2, 4, 6, 8, 10]"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1184247}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates Google Cloud Spanner's operational capabilities, including instance deployment, database schema evolution, and idempotent database modifications. Develop a comprehensive example that highlights different administrative tasks for cloud database management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that creates, updates, gets, lists and deletes a Spanner instance.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.spanner import (\n    SpannerAdminDatabaseOperator,\n    SpannerArmsPrefixGeneratorOperator,\n    SpannerInstanceCreateOperator,\n    SpannerInstanceDeleteOperator,\n    SpannerInstanceGetOperationMonitoringOperator,\n    SpannerInstanceUpdateOperator,\n    SpannerSqlApplyChangesOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"spanner_instance\"\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\n\nSPANNER_INSTANCE_ID = \"test-instance\"\nSPANNER_DATABASE_ID = \"test-database\"\n\n# [START howto_spanner_instance]\nSPANNER_INSTANCE_BODY = {\n    \"name\": SPANNER_INSTANCE_ID,\n    \"config\": {\"size\": spanner_instance_config.Size.TINY},\n}\n# [END howto_spanner_instance]\n\n# [START howto_spanner_instance_create_body]\nSPANNER_INSTANCE_CREATE_BODY = {\n    \"name\": SPANNER_INSTANCE_ID,\n    \"config\": {\"size\": spanner_instance_config.Size.MEDIUM},\n}\n# [END howto_spanner_instance_create_body]\n\n# [START howto_spanner_instance_update_body]\nSPANNER_INSTANCE_UPDATE_BODY = {\n    \"name\": SPANNER_INSTANCE_ID,\n    \"config\": {\"size\": spanner_instance_config.Size.LARGE},\n}\n# [END howto_spanner_instance_update_body]\n\n# [START howto_spanner_instance_delete_body]\nSPANNER_INSTANCE_DELETE_BODY = {\"force\": True}\n# [END howto_spanner_instance_delete_body]\n\n# SQL commands\n# [START howto_spanner_arms]\nSPANNER_SQL_COMMANDS = [\n    \"CREATE OR REPLACE TRANSACTIONAL TRIGGER trigger_example_1 ON DATABASE demo_database FOR SELECT * FROM my_table\",\n    \"EXECUTE TRANSACTIONAL TRIGGER trigger_example_1 FOR 1 SECOND\",\n    \"CREATE OR REPLACE TRANSACTIONAL TRIGGER trigger_example_2 ON DATABASE demo_database FOR DELETE, UPDATE, INSERT\",\n    \"EXECUTE TRANSACTIONAL TRIGGER trigger_example_2 FOR 1 SECOND\",\n]\n# [END howto_spanner_arms]\n# [START howto_spanner_sql_apply_changes]\nSPANNER_SQL_APPLY_CHANGES_COMMANDS = [\n    \"apply changes\",\n    \"apply changes for 5 seconds\",\n]\n# [END howto_spanner_sql_apply_changes]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    @task\n    def generate_arms_file(instance_id: str):\n        \"\"\"\n        Call spanner admin API to create arms file.\n        You have to set environment variables:\n        -- PROJECT_ID\n        -- LOCATION\n        \"\"\"\n\n        instance_uri = f\"projects/{PROJECT_ID}/locations/us-central1/instances/{instance_id}\"\n        arms_body = {\n            \"file\": {\n                \"uri\": instance_uri\n            }\n        }\n        spanner_hook = SpannerHook()\n        response = spanner_hook.create_spanner_arms_file(body=arms_body)\n        return response[\"armFileUri\"]\n\n    arms_file_uri = generate_arms_file(task_ids=\"generate_arms_file\")\n\n    create_instance = SpannerInstanceCreateOperator(\n        task_id=\"create_instance\",\n        instance_id=SPANNER_INSTANCE_ID,\n        body=SPANNER_INSTANCE_CREATE_BODY,\n        project_id=PROJECT_ID,\n    )\n\n    update_instance = SpannerInstanceUpdateOperator(\n        task_id=\"update_instance\",\n        instance_id=SPANNER_INSTANCE_ID,\n        body=SPANNER_INSTANCE_UPDATE_BODY,\n        project_id=PROJECT_ID,\n    )\n\n    get_operation = SpannerInstanceGetOperationMonitoringOperator(\n        task_id=\"get_operation\",\n        instance_id=SPANNER_INSTANCE_ID,\n        operation_id=\"{{ task_instance.xcom_pull('create_instance', key='return_value').id }}\",\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_spanner_admin_database]\n    create_database = SpannerAdminDatabaseOperator(\n        task_id=\"create_database\",\n        instance_id=SPANNER_INSTANCE_ID,\n        database_id=SPANNER_DATABASE_ID,\n        body={\"project_id\": PROJECT_ID, \"instance_id\": SPANNER_INSTANCE_ID, \"database_id\": SPANNER_DATABASE_ID},\n    )\n    # [END howto_spanner_admin_database]\n\n    # [START howto_spanner_sql_apply_changes_operator]\n    apply_changes = SpannerSqlApplyChangesOperator(\n        task_id=\"apply_changes\",\n        instance_id=SPANNER_INSTANCE_ID,\n        body={\n            \"statement\": SPANNER_SQL_APPLY_CHANGES_COMMANDS[0],\n            \"read_write_options\": {\"read_only\": False},\n            \"time_span\": {\"seconds\": 5},\n        },\n    )\n    # [END howto_spanner_sql_apply_changes_operator]\n\n    apply_changes_for_5_seconds = SpannerSqlApplyChangesOperator(\n        task_id=\"apply_changes_for_5_seconds\",\n        instance_id=SPANNER_INSTANCE_ID,\n        body={\n            \"statement\": SPANNER_SQL_APPLY_CHANGES_COMMANDS[1],\n            \"read_write_options\": {\"read_only\": False},\n            \"time_span\": {\"seconds\": 5},\n        },\n    )\n\n    get_operation_for_5_seconds = SpannerInstanceGetOperationMonitoringOperator(\n        task_id=\"get_operation_for_5_seconds\",\n        instance_id=SPANNER_INSTANCE_ID,\n        operation_id=\"{{ task_instance.xcom_pull('apply_changes_for_5_seconds', key='return_value').id }}\",\n        project_id=PROJECT_ID,\n    )\n\n    delete_instance = SpannerInstanceDeleteOperator(\n        task_id=\"delete_instance\",\n        instance_id=SPANNER_INSTANCE_ID,\n        body=SPANNER_INSTANCE_DELETE_BODY,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (create_bucket >> arms_file_uri >> create_instance >> update_instance >> get_operation)\n\n    (\n        get_operation >> get_operation_for_5_seconds >> apply_changes_for_5_seconds >> delete_instance\n    )\n\n    (create_database >> apply_changes >> apply_changes_for_5_seconds >> delete_instance >> create_bucket)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_909_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982601.1184382}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that demonstrates multiple generative AI model interactions using Vertex AI, showcasing text generation, embedding, and multimodal processing capabilities.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing models.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import third_party_config\nfrom system.google import workspace_config\n\nDAG_ID = \"vertex_ai_model_test\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\n# For better error messages if the etc files are missing\nTHIRD_PARTY_NAME = third_party_config.ThirdPartyGoogle.value\nWORKSPACE_NAME = workspace_config.WorkspaceName PROJECT_ID, THIRD_PARTY_NAME, ENV_ID, \"model_service\" \\\n    .value\n\n\nwith DAG(\n    f\"{DAG_ID}_model\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[THIRD_PARTY_NAME, \"example\", \"vertex_ai_models\"],\n) as dag:\n    @dag.task()\n    def test_create_model():\n        # [START howto_cloud_create_text_embedding_model]\n        from system.google import vertex_ai_text_embedding_model\n\n        vertex_ai_text_embedding_model.create(\n            display_name=\"text-vector-service-model\",\n            dataset_id=\"us-central1-tabular-dataset\",\n            model_service_path=\"us-central1-model-service\",\n            text_feature=\"my_first_text_feature\",\n            text_vector_feature=\"my_second_text_feature\",\n            text_match_negatives=True,\n            multi_label=False,\n            max_response_length=1,\n            input_mode=\"DESERIALIZE_AND_CLASSIFY\",\n            model_service_api_version=\"v1\",\n        )\n        # [END howto_cloud_create_text_embedding_model]\n\n    test_create_model_task = test_create_model()\n\n    test_list_models = (\n        vertex_ai_text_embedding_model.ModelServiceClient.list_models()\n        >> vertex_ai_text_embedding_model.ModelServiceClient.model_iterator()\n        >> third_party_config.get_workspace_name_from_workspace_id(WORKSPACE_NAME)\n        >> WORKSPACE_NAME\n        >> vertex_ai_text_embedding_model.ModelServiceClient.pause_stream()\n        >> vertex_ai_text_embedding_model.ModelServiceClient.get_stream_ready()\n        >> vertex_ai_text_embedding_model.ModelServiceClient.list_models()\n    )\n\n    test_delete_model = (\n        vertex_ai_text_embedding_model.ModelServiceClient\n        >> vertex_ai_text_embedding_model.ModelServiceClient.delete_model(\n            request={\n                \"name\": f\"{WORKSPACE_NAME}/models/{ENV_ID}/projects/{PROJECT_ID}/versions/latest\"\n            }\n        )\n    )\n\n    (\n        test_create_model_task\n        >> test_list_models\n        >> test_delete_model\n    )\n\n\nif \"@googlecloudcustomers\" in THIRD_PARTY_NAME or \"@googleadvisors\" in THIRD_PARTY_NAME:\n    # Delete any existing temporary data\n    third_party_config.delete_temp_data(WORKSPACE_NAME, entities=[WORKSPACE_NAME + \"/models/*\"])"}], "metadata": {"file_name": "dag_934_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962012}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud file management using Azure operators. Develop a DAG that illustrates how to programmatically upload and remove files from a remote storage system.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\n\nLOCAL_FILE_PATH = os.environ.get(\"LOCAL_FILE_PATH\", \"localfile.txt\")\nADLS_BUCKET_PATH = os.environ.get(\"ADLS_BUCKET_PATH\", \"adlstestbucket-{{ ds }}.txt\")\n\nwith DAG(\n    \"example_adls\",\n    schedule=\"*/1 * * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START adls_create_operator]\n    create_adls_path = LocalFilesystemToADLSOperator(\n        task_id=\"create_adls_path\",\n        local_path=LOCAL_FILE_PATH,\n        adls_bucket_path=ADLS_BUCKET_PATH,\n    )\n    # [END adls_create_operator]\n\n    # [START adls_delete_operator]\n    delete_adls_path = ADLSDeleteOperator(task_id=\"delete_adls_path\", adls_bucket_path=ADLS_BUCKET_PATH)\n    # [END adls_delete_operator]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1750_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796208}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a machine learning pipeline that trains a K-Nearest Neighbors classification model using Amazon SageMaker and deploys it as a real-time prediction endpoint. The pipeline should handle data upload, model training, endpoint deployment, and automatic cleanup of resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sagemaker_notebook import (\n    SageMakerNotebookProcessorOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingJobSensor,\n    SageMarPredictionsSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_sagemaker_endpoint\"\n\nlogger = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nKNN_MODEL_NAME = \"test-sagemaker-endpoint-knn\"\nKNN_DATA_SAMPLE_S3_FOLDER = \"train\"\nKNN_DATA_BUCKET_KEY = \"000000/00/train.csv\"\n\nknn_script_name = \"script.py\"\n\n\n@task\ndef create_knn_endpoint_config(role_arn):\n    # Creating the Endpoint Configuration.\n    # We want to use our notebook instance for training our pipeline.\n    # We then want to translate to an endpoint within SageMaker when we call deploy().\n    # This configuration object will be passed to the training algorithm during job creation.\n    endpoint_config_desc = {\n        \"RoleArn\": role_arn,\n        \"TrainingInputS3Uri\": f\"s3://{KNN_DATA_BUCKET_KEY}/{KNN_DATA_SAMPLE_S_FOLDER}\",\n        \"TrainingOutputS3Uri\": f\"s3://{KNN_DATA_BUCKET_KEY}/test-sagemaker-endpoint-knn/output/\",\n        \"TrainingJobName\": KNN_MODEL_NAME,\n        \"KnnConfig\": {\n            \"KMeansClusterSize\": 1,\n        },\n    }\n    return endpoint_config_desc\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_knn_endpoint(endpoint_name):\n    try:\n        boto3.client(\"sagemaker\").delete_endpoint_config(EndpointName=endpoint_name)\n    except boto3.client(\"sagemaker\").ClientError as e:\n        # This throws when the endpoint does not exist\n        if \"The specified endpoint does not exist\" not in str(e):\n            raise\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_knn_endpoint_config(endpoint_config_name):\n    boto3.client(\"sagemaker\").delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n\n\ndef sample_rapids_script(input_data, output_data, model_output_s3):\n    import sys\n    from pathlib import Path\n\n    input_data = json.loads(input_data)\n    test_data = pd.DataFrame(input_data[\"train\"])\n    X_train = test_data[[\"sepal length (cm)\", \"sepal width (cm)\"]]\n    y_train = test_data[\"target\"]\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X_train, y_train)\n\n    import numpy as np\n    import pandas as pd\n\n    test_data = pd.read_csv(str(Path(model_output_s3) / KNN_DATA_SAMPLE_S_FOLDER / \"test.csv\")))\n    X_test = test_data[[\"sepal length (cm)\", \"sepal width (cm)\"]]\n\n    y_pred = knn_clf.predict(X_test)\n    accuracy = metrics.accuracy_score(test_data[\"target\"], y_pred)\n    print(f\"I got {accuracy * 100}%\")\n\n    y_proba_pred = knn_clf.predict_proba(X_test)\n    print(y_proba_pred)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"{env_id}-sagemaker-endpoint-bucket\"\n    input_data_filename = f\"{env_id}-end-point-knn-input.json\"\n    output_data_filename = f\"{env_id}-end-point-knn-output.csv\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n        region_name=\"us-east-1\",\n        force_delete=True,\n    )\n\n    create_rapids_script = S3CreateObjectOperator(\n        task_id=\"create_rapids_script_file\",\n        s3_bucket=bucket_name,\n        s3_key=knn_script_name,\n        data={\n            \"train\": input_data_filename,\n            \"test.csv\": output_data_filename,\n            \"model_output_s3\": \"\",\n        },\n        replace=True,\n    )\n\n    # [START howto_operator_sagemaker_endpoint_config]\n    create_knn_endpoint_config = create_knn_endpoint_config(role_arn)\n    # [END howto_operator_sagemaker_endpoint_config]\n\n    # [START howto_sensor_sagemaker_processing_job]\n    wait_for_processing_job = SageMakerProcessingJobSensor(\n        task_id=\"wait_for_processing_job\",\n        processing_job_id=\"{{ ti.xcom_pull('create_knn_endpoint_config', key='processing_job_id') }}\",\n    )\n    # [END howto_sensor_sagemaker_processing_job]\n\n    # [START howto_operator_sagemaker_deploy]\n    create_knn_endpoint = SageMakerHook(\n        s3_bucket=bucket_name,\n        s3_key=knn_script_name,\n        role_arn=role_arn,\n        entry_point=\"script.py\",\n        processing_job_id=\"{{ ti.xcom_pull('create_knn_endpoint_config', key='processing_job_id') }}\",\n        model_name=f\"{env_id}-end-point-knn\",\n        training_steps=[create_knn_endpoint_config],\n        training_input_s3=\"s3://{{ bucket_name }}/test-sagemaker-endpoint-knn/\",\n        training_output_s3=\"s3://{{ bucket_name }}/test-sagemaker-endpoint-knn/output/\",\n        train_transform_scale=0.1,\n        train_gpus=0,\n        training_minutes=30,\n        encoding_type=\"RAPIDS_AWS_JSON\",\n        describe=True,\n    ).get_operator()\n    # [END howto_operator_sagemaker_deploy]\n\n    # [START howto_sensor_sagemaker_endpoint]\n    wait_for_endpoint = SageMarPredictionsSensor(\n        task_id=\"wait_for_endpoint\", endpoint_name=\"{{ ti.xcom_pull('create_knn_endpoint', key='endpoint_name') }}\"\n    )\n    # [END howto_sensor_sagemaker_endpoint]\n\n    delete_knn_endpoint = delete_knn_endpoint(endpoint_name=\"{{ ti.xcom_pull('create_knn_endpoint', key='endpoint_name') }}\")\n    delete_knn_endpoint_config = delete_knn_endpoint_config(endpoint_config_name=\"{{ ti.xcom_pull('create_knn_endpoint_config', key='endpoint_config_name)' }\"))\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        create_rapids_script,\n        # TEST BODY\n        create_knn_endpoint_config,\n        wait_for_processing_job,\n        create_knn_endpoint,\n        wait_for_endpoint,\n        # TEST TEARDOWN\n        delete_knn_endpoint,\n        delete_knn_endpoint_config,\n        delete_bucket(bucket_name),\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_97_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796225}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data pipeline that showcases vector embedding ingestion with flexible metadata handling and batch processing capabilities. Create a demonstration of how to programmatically insert vector data into a vector database using Airflow's integration with Pinecone.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.pinecone.operators.pinecone import (\n    PineconeIngestOperator,\n)\n\nindex_name = os.environ.get(\"INDEX_NAME\", \"test-index-1\")\n\n# [START howto_pinecone_ingest_operator]\nwith models.DAG(\n    \"example_pinecone_ingest_operator\",\n    start_date=datetime(2021, 1, 1),\n    default_args={\"batch_size\": 5},\n    tags=[\"example\"],\n) as dag:\n    # [END howto_pinecone_ingest_operator]\n\n    # [START howto_pinecone_ingest_operator_main]\n    insert_task = PineconeIngestOperator(\n        task=\"insert\",\n        ref_id=\"id1\",\n        input_vectors=[\n            [0.7630024, 0.97879973, 0.35239984, 0.464876],  # [embedding1, ...]\n            [0.3197919, 0.9916989, 0.4561998, 0.8636393],\n        ],\n        namespace=index_name,\n        batch_size=5,\n    )\n    # [END howto_pinecone_ingest_operator_main]\n\n    # [START howto_pinecone_ingest_operator_batch]\n    batch_insert_task = PineconeIngestOperator(\n        task=\"batch\",\n        ref_id=\"id1\",\n        input_vectors=[\n            [\n                0.7630024,\n                0.97879973,\n                0.35239984,\n                0.464876,  # [embedding1, ...]\n                0.3197919,\n                0.9916989,\n                0.4561998,\n                0.8636393,\n            ],\n        ],\n        namespace=index_name,\n        batch_size=5,\n    )\n    # [END howto_pinecone_ingest_operator_batch]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_639_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962313}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the complete lifecycle of a Cloud Data Fusion instance, including creation, pipeline deployment, execution, and cleanup. Implement a comprehensive demonstration of cloud data infrastructure management using Google Cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Data Fusion service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.datafusion_v2 import job\nfrom google.cloud.datafusion_v2.types import JobTrigger\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.data fusion import CloudDataFusionHook\nfrom airflow.providers.google.cloud.operators.data fusion import (\n    CloudDataFusionCreatePipelineOperator,\n    CloudDataFusionCreateTaskOperator,\n    CloudDataFusionDeletePipelineOperator,\n    CloudDataFusionDeleteTaskOperator,\n    CloudDataFusionGetPipelineOperator,\n    CloudDataFusionGetTaskOperator,\n    CloudDataFusionListPipelinesOperator,\n    CloudDataFusionListTasksOperator,\n    CloudDataFusionUpdatePipelineOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_cloud_data_fusion\"\n\nBUCKET_NAME = f\"test-bucket-{DAG_ID}-{ENV_ID}\"\n\nCURRENT_FOLDER = Path(__file__).parent\nFILE_LOCAL_PATH = str(Path(CURRENT_FOLDER) / \"resources\" / \"data-fusion.yaml\")\nFILE_GCS_PATH = f\"gs://{BUCKET_NAME}/data-fusion.yaml\"\n\n\ndef get_pipeline_trigger():\n    # [START howto_operator_gcp_datafusion_create_pipeline_trigger]\n    trigger_request_body = {\"trigger\": JobTrigger.builder().for_amount(10).build()}\n    # [END howto_operator_gcp_datafusion_create_pipeline_trigger]\n\n    return trigger_request_body\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_pipleline_with_no_bucket():\n        hook = CloudDataFusionHook()\n        pipeline = {\n            \"display_name\": \"test_display_name\",\n            \"image_uri\": \"gcr.io/cloud-datafusion/latest-dsvm:latest\",\n            \"job_role\": \"DATA_FUSION_BROWSE_USER\",\n        }\n        response = hook.create_pipeline(pipeline, project_id=PROJECT_ID)\n        print(response)\n\n    @task\n    def create_pipleline_with_gcs_bucket():\n        hook = CloudDataFusionHook()\n        pipeline = {\n            \"display_name\": \"test_display_name\"\n        }\n        resource = {\n            \"filesystems\": [\n                {\n                    \"attribute\": \"cloud_storage_path\",\n                    \"values\": [Value(string=FILE_GCS_PATH)],\n                }\n            ]\n        }\n        response = hook.create_pipeline_from_resource(pipeline, resource, project_id=PROJECT_ID)\n        print(response)\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_LOCAL_PATH,\n        dst=FILE_GCS_PATH,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcp_datafusion_create_pipeline]\n    create_pipeline = CloudDataFusionCreatePipelineOperator(\n        request=get_pipeline_trigger(),\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_datafusion_create_pipeline]\n\n    # [START howto_operator_gcp_datafusion_create_pipeline_from_resource]\n    create_pipeline_from_resource = CloudDataFusionCreatePipelineOperator(\n        resource=pipeline,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_datafusion_create_pipeline_from_resource]\n\n    # [START howto_operator_gcp_datafusion_list_pipelines]\n    list_pipelines = CloudDataFusionListPipelinesOperator(task_id=\"list-pipelines\", project_id=PROJECT_ID)\n    # [END howto_operator_gcp_datafusion_list_pipelines]\n\n    # [START howto_operator_gcp_datafusion_get_pipeline]\n    get_pipeline = CloudDataFusionGetPipelineOperator(\n        task_id=\"get-pipeline\", name=create_pipeline.output[\"name\"], project_id=PROJECT_ID\n    )\n    # [END howto_operator_gcp_datafusion_get_pipeline]\n\n    # [START howto_operator_gcp_datafusion_update_pipeline]\n    update_pipeline = CloudDataFusionUpdatePipelineOperator(\n        task_id=\"update-pipeline\",\n        project_id=PROJECT_ID,\n        body={\"job\": {\"job_name\": \"test-job-name\"}},\n    )\n    # [END howto_operator_gcp_datafusion_update_pipeline]\n\n    # [START howto_operator_gcp_datafusion_delete_pipeline]\n    delete_pipeline = CloudDataFusionDeletePipelineOperator(\n        task_id=\"delete-pipeline\", name=create_pipeline.output[\"name\"], project_id=PROJECT_ID\n    )\n    # [END howto_operator_gcp_datafusion_delete_pipeline]\n\n    # [START howto_operator_gcp_datafusion_create_task]\n    create_task = CloudDataFusionCreateTaskOperator(\n        task_id=\"create-task\",\n        pipeline_name=create_pipeline.output[\"name\"],\n        resource=request={\n            \"dataset\": {\n                \"projectId\": PROJECT_ID,\n                \"location\": \"\",\n                \"type\": \"BIGQUERY\",\n            },\n        },\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_datafusion_create_task]\n\n    # [START howto_operator_gcp_datafusion_list_tasks]\n    list_tasks = CloudDataFusionListTasksOperator(task_id=\"list-tasks\", pipeline_name=create_task.output[\"name\"])\n    # [END howto_operator_gcp_datafusion_list_tasks]\n\n    # [START howto_operator_gcp_datafusion_get_task]\n    get_task = CloudDataFusionGetTaskOperator(\n        task_id=\"get-task\", pipeline_name=create_task.output[\"name\"], task_name=create_task.output[\"name\"]\n    )\n    # [END howto_operator_gcp_datafusion_get_task]\n\n    # [START howto_operator_gcp_datafusion_delete_task]\n    delete_task = CloudDataFusionDeleteTaskOperator(\n        task_id=\"delete-task\", pipeline_name=create_task.output[\"name\"], task_name=create_task.output[\"name\"]\n    )\n    # [END howto_operator_gcp_datafusion_delete_task]\n\n    chain(\n        create_bucket,\n        upload_file,\n        create_pipeline,\n        create_pipeline_from_resource,\n        get_pipeline,\n        update_pipeline,\n        delete_pipeline,\n        create_task,\n        list_tasks,\n        get_task,\n        delete_task,\n    )\n    GCSDeleteBucketOperator(\n        task_id=\"remove_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )"}], "metadata": {"file_name": "dag_1617_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962472}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that transforms raw geographical and user data into structured database tables with multiple query scenarios. Design a workflow that handles data insertion, supports parameterized queries, and demonstrates different methods of database interaction using MsSql operators and hooks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\nfrom airflow.providers.microsoft.mssql.operators.mssql import MsSqlOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\ntry:\n    from airflow.providers.standard.transfers.mssql_to_mssql import MSSqlToMSSqlOperator\nexcept ModuleNotFoundError:\n    print(\"This task depends on MSSqlToMSSqlOperator which is not available. Please install it via `pip install airflow[provider-mssql]`.\")\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mssql_to_mssql\"\nCONN_ID = \"mssql_conn_id\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_mssql_to_mssql]\n    t1 = MsSqlOperator(task_id=\"print_hi_mssql\", sql=\"SELECT 1\", conn=CONN_ID)\n    t2 = MsSqlOperator(task_id=\"print_hi_mssql_2\", sql=\"SELECT 1\", conn=CONN_ID)\n    t3 = BashOperator(task_id=\"print_hello_mssql\", bash_command=\"echo $hi_mssql\")\n    # [END howto_operator_mssql_to_mssql]\n\n    t4 = MSSqlToMSSqlOperator(\n        task_id=\"connect_and_print_hi_mssql\",\n        conn_id=CONN_ID,\n        ms_sql_conn_id=\"mssql_conn_id_2\",\n        sql=\"SELECT 1\",\n    )\n\n    t1 >> t2 >> t3 >> t4"}], "metadata": {"file_name": "dag_630_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962525}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an enterprise-level data extraction and cloud storage transfer process that demonstrates best practices for moving relational database content to a cloud storage platform. Construct an Airflow workflow that handles data export from Oracle, transfers the data to Google Cloud Storage in CSV format, and ensures proper resource management through automated bucket creation and deletion.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.oracle_to_gcs import OracleToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_oracle\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nORACLE_CONN_ID = \"oracle\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    # [START howto_operator_oracle_to_gcs]\n    upload_oracle_table_to_gcs = OracleToGCSOperator(\n        task_id=\"upload_oracle_table_to_gcs\",\n        oracle_conn_id=ORACLE_CONN_ID,\n        sql=\"SELECT * FROM table\",\n        bucket=BUCKET_NAME,\n        export_format=\"csv\",\n    )\n    # [END howto_operator_oracle_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_oracle_table_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it done;\n    # when `set override_default_args`, this task has timeout\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_221_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796258}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of AWS SageMaker notebook resources, illustrating how to programmatically control cloud computing infrastructure through defined task dependencies. Develop a step-by-step orchestration that shows creating, manipulating, and cleaning up a computational notebook instance.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n    SageMakerStartNoteBookOperator,\n    SageMakerStopNotebookOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerNotebookSensor,\n    SageMakerNotebookStarterSensor,\n    SageMakerNotebookStatusSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\n\nDAG_ID = \"example_sagemaker_notebook\"\n\n# [START howto_operator_sagemaker_notebook]\n\ndef create_notebook():\n    # Create a 1 node instance with m5.xlarge Amazon EC2 instance class\n    script_path = \"sagemaker-notebook.yaml\"\n    cmd = f\"aws sagemaker register notebook script --path {script_path} --instance-type ml.m5.xlarge --instance-count 1\"\n    proc = subprocess.Popen(cmd, shell=True)\n    proc.communicate()\n\n    # Create a notebook called test_notebook\n    cmd = f\"aws sagemaker register notebook --name test_notebook\"\n    proc = subprocess.Popen(cmd, shell=True)\n    proc.communicate()\n\n\n@task\ndef get_image_uri():\n    images = [\n        \"7ed98ce4-6ebc-42f9-aedd-bbfefdddb9d9.amazonaws.com/sagemaker-tensorflow:1.4\",\n        \"ad9aeebe-d7b9-4cba-adbb-0e65fffbdcba.amazonaws.com/sagemaker-mxnet:0.11.0-py3\",\n    ]\n    for image in images:\n        response = boto3.client(\"sagemaker\").describe_image(ImageName=image)\n        return response[\"ImageArn\"].split(\":\")[5]\n\n\n@task\ndef create_notebook_instance(notebook_image_arn):\n    region = boto3.session.Session().region_name\n    key_pair_name = f\"{DAG_ID}-{region}-key-pair\"\n\n    cmd = f\"\"\"\n        aws sagemaker register notebook-instance \\\n            --image-arn {notebook_image_arn} \\\n            --instance-count 1 \\\n            --instance-type mxme4a1m2 \\\n            --key-pair-name {key_pair_name}\n    \"\"\"\n    proc = subprocess.Popen(cmd, shell=True)\n    proc.communicate()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_key_pair(key_pair_name):\n    boto3.client(\"ec2\").delete_key_pair(KeyName=key_pair_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=\"test-s3-bucket\")\n\n    set_up_key_pair = create_key_pair(key_pair_name=key_pair_name)\n\n    register_tensorflow_notebook = SageMakerHook.get_task_function(\"sagemaker-tensorflow:1.4\")()\n    register_mxnet_notebook = SageMakerHook.get_task_function(\"mxnet:0.11.0-py3\")()\n\n    create_notebook_instance = SageMakerNotebookInstanceOperator(\n        task_id=\"create_notebook_instance\",\n        image_arn=get_image_uri(),\n        instance_count=1,\n        instance_type=\"ml.m5.xlarge\",\n        wait_for_completion=False,\n    )\n\n    start_notebook_starter = SageMakerNotebookStarterSensor(\n        task_id=\"start_notebook_starter\",\n        notebook_name=\"test_notebook\",\n    )\n\n    stop_notebook_starter = SageMakerNotebookStarterSensor(\n        task_id=\"stop_notebook_starter\",\n        notebook_name=\"test_notebook\",\n    )\n\n    get_notebook_status = SageMakerNotebookStatusSensor(\n        task_id=\"get_notebook_status\",\n        notebook_name=\"test_notebook\",\n    )\n\n    delete_notebook_instance = SageMakerDeleteNotebookInstanceOperator(\n        task_id=\"delete_notebook_instance\",\n        notebook_name=\"test_notebook\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_key_pair = delete_key_pair(key_pair_name=key_pair_name)\n\n    chain(\n        create_s3_bucket,\n        set_up_key_pair,\n        create_notebook_instance,\n        create_tensorflow_notebook,\n        create_mxnet_notebook,\n        create_notebook_instance,\n        start_notebook_starter,\n        stop_notebook_starter,\n        get_notebook_status,\n        delete_notebook_instance,\n        delete_key_pair,\n    )\n# [END howto_operator_sagemaker_notebook]\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1126_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962697}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates calling stored procedures with different parameter passing techniques. Create a DAG that showcases input and output parameter handling for database operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample use of Oracle related operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_oracle_stored_procedure_operator_with_list_of_strings]\n    oracle_stored_procedure_operator_with_list_of_strings = OracleStoredProcedureOperator(\n        task_id=\"oracle_stored_procedure_operator_with_list_of_strings\",\n        oracle_conn_id=\"oracle_default\",\n        procedure=\"TEST_PROCEDURE',\n                    TEST_PROCEDURE2('test'::varchar2(20))\",\n        dag=dag,\n    )\n    # [END howto_oracle_stored_procedure_operator_with_list_of_strings]\n\n    # [START howto_oracle_stored_procedure_operator_with_dict]\n    oracle_stored_procedure_operator_with_dict = OracleStoredProcedureOperator(\n        task_id=\"oracle_stored_procedure_operator_with_dict\",\n        oracle_conn_id=\"oracle_default\",\n        procedure={\"test\": \"test\"},\n        dag=dag,\n    )\n    # [END howto_oracle_stored_procedure_operator_with_dict]\n\n    # [START howto_oracle_stored_procedure_operator_with_dict_output]\n    oracle_stored_procedure_operator_with_dict_output = OracleStoredProcedureOperator(\n        task_id=\"oracle_stored_procedure_operator_with_dict_output\",\n        oracle_conn_id=\"oracle_default\",\n        procedure={\"t\": \"est\"},\n        output_columns=[\n            \"return_value\",\n            \"test_proc_return_value_as_integer\",\n        ],\n        dag=dag,\n    )\n    # [END howto_oracle_stored_procedure_operator_with_dict_output]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1770_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962751}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need a Python code snippet to compare the similarity between two text documents using the cosine similarity method."}, {"role": "assistant", "content": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Function to load text from file\ndef load_text_from_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.read()\n\n# Load texts\ntext1 = load_text_from_file('document1.txt')\ntext2 = load_text_from_file('document2.txt')\n\n# Convert text to lowercase and remove punctuation\nimport re\ndef preprocess_text(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Convert to lowercase\n    return text.lower()\n\npreprocessed_text1 = preprocess_text(text1)\npreprocessed_text2 = preprocess_text(text2)\n\n# Tokenize the preprocessed texts\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform([preprocessed_text1, preprocessed_text2])\n\n# Compute cosine similarity\ncosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\nprint(\"Cosine Similarity:\", cosine_similarities)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962875}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that demonstrates Google Cloud Storage bucket management, including creating, copying, and deleting files and buckets. Create a DAG that showcases file transfer operations between two dynamically generated cloud storage buckets.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSCreateBucketOperator and GCSDeleteBucketOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"gcs_copy_within_dataset\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n    # [START howto_operator_gcs_copy_single_file]\n    copy_file = GCSToGCSOperator(\n        task_id=\"copy_file\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=\"test_copy.txt\",\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=\"test_copy.txt\",\n        replace=True,\n    )\n    # [END howto_operator_gcs_copy_single_file]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_src\n        >> create_bucket_dst\n        # TEST BODY\n        >> copy_file\n        # TEST TEARDOWN\n        >> delete_bucket_src\n        >> delete_bucket_dst\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_880_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7962937}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that illustrates safe data transaction practices by implementing a complete transaction management scenario in Google Cloud Datastore. Construct a workflow that shows how to start a transaction and then safely roll it back without persisting potentially unwanted data modifications.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Datastore service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datastore import (\n    CloudDatastoreBeginTransactionOperator,\n    CloudDatastoreDeleteOperationOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datastore_transaction\"\nLOCATION = \"europe-west1\"\n\nTRANSACTIONAL_TASKS = [\n    {\n        \"insert\": {\n            \"parent\": f\"projects/{PROJECT_ID}/locations/{LOCATION}/transientTransactions\",\n            \"contents\": [b\"\\x01\\x04foo\"],\n        },\n    }\n]\n\ndefault_args = {\"start_date\": datetime(2021, 1, 1), \"owner\": \"airflow\"}  # type: ignore[call-arg]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args=default_args,\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"datastore\"],\n) as dag:\n    delete_transients = CloudDatastoreDeleteOperationOperator(\n        task_id=\"delete_transients\",\n        name=f\"projects/{PROJECT_ID}/operations/\",\n        filter_=BashOperator(\n            task_id=\"filter_all_operations\",\n            bash_command=\"gcloud dataflow operations list --project=$PROJECT_ID --format='value(name)' | grep -v done\",\n        ),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    begin_transaction = CloudDatastoreBeginTransactionOperator(\n        task_id=\"begin_transaction\",\n        transactional_tasks=TRANSACTIONAL_TASKS,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n    )\n\n    delete_transients >> begin_transaction\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_876_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963002}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data extraction and transfer system that demonstrates extracting data from a PostgreSQL database and moving it to a Google Cloud Storage bucket with full infrastructure provisioning. The solution should highlight how to programmatically create cloud resources, configure database connections, and perform secure data migrations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.hooks.compute._variables import (\n    PROJECT_ID,\n    REGION,\n    GCE_INSTANCE_NAME,\n    GCE_MACHINE_TYPE,\n    GCE_DISK_SIZE_GB,\n    GCE_DISK_NAME,\n    GCE_NETWORK_NAME,\n    GCE_DEFAULT_LISTEN_PORT_RANGE,\n)\nfrom airflow.providers.google.cloud.operators.datajoin import GCSDatasetCreateOperator\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.sheets import GoogleSheetsCreateSpreadsheetOperator, GoogleSheetsDeleteSpreadsheetOperator\nfrom airflow.providers.google.cloud.transfers.postgres_to_gcs import PostgresToGCSOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(PROJECT_ID.replace(\"-\", \"_\"))\nDAG_ID = \"example_pg_to_gcs_with_dataset\"\n\nDB_NAME = \"testdb\"\nPG_CONN_ID = f\"pg_{DB_NAME}_conn\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"1234567890\")\n\nGCE_BODY = {\n    \"name\": GCE_INSTANCE_NAME,\n    \"machine_type\": f\"zones/{REGION}/machineTypes/{GCE_MACHINE_TYPE}\",\n    \"disks\": [\n        {\n            \"boot\": True,\n            \"device_name\": GCE_INSTANCE_NAME,\n            \"initialize_params\": {\n                \"disk_size_gb\": GCE_DISK_SIZE_GB,\n                \"source_image\": f\"projects/ubuntu-cloud-gce/global/addresses/{GCE_DISK_NAME}\",\n                \"disk_name\": GCE_INSTANCE_NAME,\n            },\n        }\n    ],\n    \"network_interfaces\": [{\"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}}, {\"name\": \"nic-0\", \"subnetwork\": f\"regions/{REGION}/subnetworks/default\"}]},\n    \"network_connection_spec\": {\n        \"global_dataset_synchronization_spec\": {\n            \"enable_global_dts\": True,\n            \"schedule\": {\"recurrence_period_in_mins\": 60},\n        }\n    },\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    generate_bucket = GCSCreateBucketOperator(task_id=\"generate_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_pg_conn(pg_conn_id: str):\n        hook = ComputeEngineHook()\n        request = hook.build_postgres_instance_request(\n            project_id=PROJECT_ID,\n            region=REGION,\n            resource_id=GCE_INSTANCE_NAME,\n        )\n        request.set_body_field(\"postgres_connection_profile\");\n        request.postgres_connection_profile.connection_id = pg_conn_id\n\n        hook.client.services.insert(project_id=request.project_id, body=request)\n\n    create_pg_conn_task = create_pg_conn(pg_conn_id=PG_CONN_ID)\n\n    # [START howto_operator_pg_to_gcs]\n    pg_to_gcs = SQLExecuteQueryOperator(\n        task_id=\"pg_to_gcs\",\n        sql=f\"SELECT * FROM test_schema.test_table;\",\n        postgres_conn_id=PG_CONN_ID,\n        autocommit=True,\n    )\n    # [END howto_operator_pg_to_gcs]\n\n    # [START howto_operator_get_dataset_synchronization_state]\n    get_dataset_synchronization_state = SQLExecuteQueryOperator(\n        task_id=\"get_dataset_synchronization_state\",\n        sql=f\"describe table {DB_NAME}.{GCE_INSTANCE_NAME};\",\n        postgres_conn_id=PG_CONN_ID,\n        autocommit=True,\n    )\n    # [END howto_operator_get_dataset_synchronization_state]\n\n    # [START howto_operator_create_dataset_synchronization]\n    create_dataset_synchronization = GCSDatasetCreateOperator(\n        task_id=\"create_dataset_synchronization\",\n        project_id=PROJECT_ID,\n        dataset_name=f\"dataset-synch-{DAG_ID}-{ENV_ID}\",\n        location=\"europe-west1\",\n        global_dataset_synchronization_request={\n            \"sources\": [\n                {\n                    \"firewalled_service\": {\n                        \"postgres_port_range\": f\"global:{'w'*6}:{'p'*6}\"\n                    },\n                    \"resource_identifier\": f\"projects/{PROJECT_ID}/zones/{REGION}/instances/{GCE_INSTANCE_NAME}\",\n                }\n            ],\n            \"destinations\": [\n                {\n                    \"service\": {\"cloud_storage_path\": f\"gs://{BUCKET_NAME}/\"},\n                    \"resource_identifier\": \"global/projects/{PROJECT_ID}/locations/{REGION}/connections/{GCE_CONNECTION_ID}\",\n                }\n            ],\n        },\n    )\n    # [END howto_operator_create_dataset_synchronization]\n\n    # [START howto_operator_delete_dataset_synchronization]\n    delete_dataset_synchronization = GCSDatasetCreateOperator(\n        task_id=\"delete_dataset_synchronization\",\n        dataset_name=f\"dataset-synch-{DAG_ID}-{ENV_ID}\",\n        location=\"europe-west1\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_delete_dataset_synchronization]\n\n    # [START howto_operator_get_vpn_connection_for_dataset_synchronization]\n    get_vpn_connection_for_dataset_synchronization = SQLExecuteQueryOperator(\n        task_id=\"get_vpn_connection_for_dataset_synchronization\",\n        sql=f\"select distinct network;\"\n        f\"from dev_endpoints.\"\n        f\"{DB_NAME}.`\n        f`{GCE_INSTANCE_NAME}`\n        f\";\",\n        postgres_conn_id=PG_CONN_ID,\n        autocommit=True,\n    )\n    # [END howto_operator_get_vpn_connection_for_dataset_synchronization]\n\n    # [START howto_operator_list_bucket_files]\n    list_bucket_files = BashOperator(\n        task_id=\"list_bucket_files\",\n        bash_command=f\"gcsfuse {BUCKET_NAME} && gsutil ls {BUCKET_NAME} && gcsfuse --leave\",\n    )\n    # [END howto_operator_list_bucket_files]\n\n    # [START howto_operator_delete_bucket]\n    delete_bucket = GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n    # [END howto_operator_delete_bucket]\n\n    (\n        # TEST SETUP\n        generate_bucket\n        >> create_pg_conn_task\n        >> pg_to_gcs\n        >> get_dataset_synchronization_state\n        >> create_dataset_synchronization\n        >> delete_dataset_synchronization\n        >> get_vpn_connection_for_dataset_synchronization\n        >> list_bucket_files\n        >> delete_bucket\n    )\n\n    # ### Everything below this line is a challenge###\n    # Note: 90% of the time you will want to have one big DAG for all your PG to GS transfers.\n    # This challenge is here to show you how you could have smaller Dags if you have very complex\n    # pipelines with multiple steps or if you have larger Datasets spreading across multiple Dags.\n\n\n    # [START howto_operator_copy_table_to_gb_csv]\n    copy_table_to_gb_csv = GCSDatasetCreateOperator(\n        task_id=\"copy_table_to_gb_csv\",\n        project_id=PROJECT_ID,\n        dataset_name=f\"dataset-synch-{DAG_ID}-{ENV_ID}\",\n        location=\"europe-west1\",\n        global_dataset_synchronization_request={\n            \"sources\": [\n                {\n                    \"firewalled_service\": {\n                        \"postgres_port_range\": f\"global:{'w'*6}:{'p'*6}\"\n                    },\n                    \"resource_identifier\": f\"projects/{PROJECT_ID}/zones/{REGION}/instances/{GCE_INSTANCE_NAME}\",\n                    \"export_options\": {\n                        \"csv_file_spec\": {\n                            \"output_location\": {\"cloud_storage_path\": f\"gs://{BUCKET_NAME}/my_csv_data\"}\n                        }\n                    }\n                }\n            ],\n            \"destinations\": [\n                {\n                    \"service\": {\"cloud_storage_path\": f\"gs://{BUCKET_NAME}/\"},\n                    \"resource_identifier\": \"global/projects/{PROJECT_ID}/locations/{REGION}/connections/{GCE_CONNECTION_ID}\",\n                }\n            ],\n        },\n    )\n    # [END howto_operator_copy_table_to_gb_csv]\n\n    # [START howto_operator_upload_csv_to_gcs]\n    upload_csv_to_gcs = PostgresToGCSOperator(\n        task_id=\"upload_csv_to_gcs\",\n        sql=\"SELECT * FROM test_schema.test_table;\",\n        postgres_conn_id=PG_CONN_ID,\n        schema=None,\n        export_format=None,\n        rows=None,\n        open_howlong=None,\n        apply_transformations=None,\n        destination_project_dataset_table=\"test_table\",\n        sql_conn_id=None,\n        record_filter=None,\n        *args,\n        **kwargs\n    )\n    # [END howto_operator_upload_csv_to_gcs]\n\n    upload_csv_to_gcs >> copy_table_to_gb_csv\n\n    default_args = {\n        \"postgres_conn_id\": PG_CONN_ID,\n    }\n\n    (upload_csv_to_gcs.output >>\n     copy_table_to_gb_csv)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_576_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963223}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can wait for specific conditions before triggering downstream tasks. Create a DAG that illustrates time-based, file-based, bash command, and Python function sensors to help learners understand conditional task execution.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.sensors.file_sensor import FileSensor\nfrom airflow.providers.standard.sensors.python_sensor import PythonSensor\nfrom airflow.providers.standard.sensors.shell_sensor import ShellSensor\nfrom airflow.providers.standard.sensors.time_sensor import TimeSensor\nfrom airflow.providers.standard.sensors.weekday_sensor import WeekDaySensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_standard_sensors\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_standard_sensor_file]\n    @task.sensor(task_id=\"sensor_file\", path=\"/tmp/airflow\")\n    def file_sensor():\n        return False\n\n    # [END howto_standard_sensor_file]\n\n    # [START howto_standard_sensor_file_timeout]\n    @task.sensor(task_id=\"sensor_file_timeout\", path=\"/tmp/airflow\", timeout=60)\n    def file_sensor_timeout():\n        return False\n\n    # [END howto_standard_sensor_file_timeout]\n\n    # [START howto_standard_sensor_python]\n    @task.python(task_id=\"sensor_python\"), timeout=60\n    def python_sensor():\n        import time\n\n        print(f\"Waiting {time.sleep(10)}\")\n        return True\n\n    # [END howto_standard_sensor_python]\n\n    # [START howto_standard_sensor_python_return_value]\n    @task.python(task_id=\"sensor_python_return_value\")\n    def python_sensor_return_value():\n        import time\n\n        print(f\"Returning value {time.sleep(10)}\")\n        return True\n\n    # [END howto_standard_sensor_python_return_value]\n\n    # [START howto_standard_sensor_shell]\n    @task.shell(sensor_task_ids=[\"shell_command\"], path=\"/tmp/airflow\")\n    def shell_command():\n        return True\n\n    # [END howto_standard_sensor_shell]\n\n    # [START howto_standard_sensor_shell_return_value]\n    @task.shell(task_id=\"shell_command_return_value\")\n    def shell_command_return_value():\n        return True\n\n    # [END howto_standard_sensor_shell_return_value]\n\n    # [START howto_standard_sensor_time]\n    @task.time(sensor_task_ids=[\"times\"], time_unit=\"minute\", timeout=60)\n    def times():\n        return True\n\n    # [END howto_standard_sensor_time]\n\n    # [START howto_standard_sensor_weekday]\n    @task.weekday(sensor_task_ids=[\"week_day\"], week_days=[\"mon\"])\n    def week_day():\n        return True\n\n    # [END howto_standard_sensor_weekday]\n    # [START howto_standard_sensor_weekday_return_value]\n    @task.weekday(task_id=\"week_day_return_value\", week_days=[\"mon\"])\n    def week_day_return_value():\n        return True\n    # [END howto_standard_sensor_weekday_return_value]\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule of 'after_success' is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1049_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.79633}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that monitors a specific Azure CosmosDB collection for document availability and triggers a subsequent document insertion task. Implement a dependency-based workflow that ensures document existence before performing write operations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.cosmos import (\n    AzureCosmosCreateCollectionOperator,\n    AzureCosmosDeleteDocumentOperator,\n    AzureCosmosDeleteCollectionOperator,\n    AzureCosmosInsertDocumentOperator,\n    AzureCosmosQueryDocumentsOperator,\n)\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"cosmos_db_operation\"\nTASK_ID = \"insert_query\"\n\nCONNECTION_ID = \"azure_default\"\n\nsysout_conn = Connection(\n    conn_id=\"sysout\",\n    conn_type=\"azure_cosmos\",\n    description=\"Azure CosmosDB connection for testing purposes.\",\n    extra={\"endpoint\": \"cluster-1.au-syd.cloudapp.net\", \"database_name\": \"systemout\"},\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_collection(cosmos_conn_id):\n        \"\"\"\n        Create a collection called test_coll\n        \"\"\"\n        AzureCosmosCreateCollectionOperator(\n            task_id=\"create_collection\",\n            cosmos_conn_id=cosmos_conn_id,\n            database_name=\"testdb\",\n            collection_id=\"test_coll\",\n        )\n\n    @task\n    def add_document(cosmos_conn_id, id, body):\n        \"\"\"\n        Insert a document into the specified collection\n        \"\"\"\n        AzureCosmosInsertDocumentOperator(\n            task_id=\"add_document\",\n            cosmos_conn_id=cosmos_conn_id,\n            id=id,\n            body=body,\n            database_name=\"testdb\",\n            collection_name=\"test_coll\",\n        )\n\n    @task\n    def delete_document(cosmos_conn_id, id):\n        \"\"\"\n        Delete the document with the specified ID\n        \"\"\"\n        AzureCosmosDeleteDocumentOperator(\n            task_id=\"delete_document\",\n            cosmos_conn_id=cosmos_conn_id,\n            id=id,\n            database_name=\"testdb\",\n            collection_name=\"test_coll\",\n        )\n\n    @task\n    def query_documents(cosmos_conn_id, id):\n        \"\"\"\n        Query the documents in the specified collection\n        \"\"\"\n\n        queries = [\n            {\n                \"storedProcedureName\": \"get-all-system-output\",\n                \"input\": {\"partitionKey\": {\"values\": [id]}},\n                \"output\": {},\n                \"asOutput\": True,\n            }\n        ]\n\n        AzureCosmosQueryDocumentsOperator(\n            task_id=\"query_documents\",\n            cosmos_conn_id=cosmos_conn_id,\n            partition_key=id,\n            queries=queries,\n            database_name=\"testdb\",\n            collection_name=\"test_coll\",\n        )\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def cleanup_connection(cosmos_conn_id):\n        session = Session()\n        session.query(Connection).filter(Connection.id == cosmos_conn_id).delete()\n\n    # [START howto_operator_azureresourcecosmos]\n    build_collection_task = create_collection(CONNECTION_ID)\n    # [END howto_operator_azureresourcecosmos]\n\n    # [START howto_operator_insert_cosmos]\n    build_document_task = add_document(CONNECTION_ID, \"8d6f7a7f-b8c7-a8c7-d7f8-b8d7f8c7a8d7\", {})\n    # [END howto_operator_insert_cosmos]\n\n    # [START howto_operator_delete_cosmos]\n    delete_document_task = delete_document(CONNECTION_ID, \"8d6f7a7f-b8c7-a8c7-d7f8-b8d7f8c7a8d7\")\n    # [END howto_operator_delete_cosmos]\n\n    # [START howto_operator_query_cosmos]\n    query_document_task = query_documents(CONNECTION_ID, \"8d6f7a7f-b8c7-a8c7-d7f8-b8d7f8c7a8d7\")\n    # [END howto_operator_query_cosmos]\n\n    # [START howto_operator_cleanup_connection]\n    clean_up_connection_task = cleanup_connection(CONNECTION_ID)\n    # [END howto_operator_cleanup_connection]\n\n    build_collection_task >> build_document_task >> delete_document_task >> query_document_task >> clean_up_connection_task"}], "metadata": {"file_name": "dag_968_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963407}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code snippet to calculate the factorial of a number using recursion?"}, {"role": "assistant", "content": "def factorial(n):\n    # Base case: if n is 0 or 1, the factorial is 1\n    if n == 0 or n == 1:\n        return 1\n    else:\n        # Recursive call: n * factorial of (n-1)\n        return n * factorial(n - 1)\n\n# Example usage:\nnumber = int(input(\"Enter a non-negative integer: \"))\nif number < 0:\n    print(\"Factorial is not defined for negative numbers.\")\nelse:\n    result = factorial(number)\n    print(f\"The factorial of {number} is {result}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963488}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data monitoring workflow that extracts specific column data from a SQL table and automatically sends the results to a designated Slack channel for real-time reporting and team awareness.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using SqlToSlackOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.slack.transfers.sql_to_slack import SqlToSlackOperator\n\nSQL_TABLE = os.environ.get(\"SQL_TABLE\", \"test_sql_table\")\nSQL_CONN_ID = \"presto_default\"\nSLACK_CHANNEL = \"#presto\"\n\n\nwith models.DAG(\n    \"example_sql_to_slack\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_sql_to_slack]\n    SqlToSlackOperator(\n        task_id=\"presto_to_slack\",\n        sql= f\"SELECT col1, col2 FROM {SQL_TABLE}\",\n        slack_channel=SLACK_CHANNEL,\n        slack_conn_id=SQL_CONN_ID,\n    ),\n    # [END howto_operator_sql_to_slack]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_651_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796352}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow that demonstrates transferring a local text file to an Amazon S3 bucket using Airflow, showcasing file management and cloud storage integration. Create a DAG that generates a sample text file, uploads it to S3, and then performs cleanup operations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_s3\"\n\ns3_conn_id = \"aws_default\"\ns3_bucket = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n    )\n    conn.set_extra({\"url\": f\"s3a://{s3_bucket}\"})\n    return conn\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    @task\n    def create_sample_file():\n        with open(\"/tmp/short.txt\", \"w\") as file:\n            file.write(\"An apple a day keeps the doctor away.\")\n\n    create_sample_file_task = create_sample_file()\n\n    upload_file = LocalFilesystemToS3Operator(\n        task_id=\"upload_file\",\n        filename=\"/tmp/short.txt\",\n        dest_key=\"temp/\",\n        s3_conn_id=s3_conn_id,\n    )\n\n    delete_file = LocalFilesystemToS3Operator(\n        task_id=\"delete_file\",\n        filename=\"/tmp/short.txt\",\n        dest_key=\"temp/\",\n        s3_conn_id=s3_conn_id,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_connection(s3_conn_id)\n        >> create_s3_bucket\n        >> create_sample_file_task\n        >> upload_file\n        # TEST BODY\n        >> delete_file\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963586}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that validates and explores HTTP service connectivity through sensor checks and sequential HTTP request tasks. Implement a workflow that first checks service availability, then executes a series of HTTP interactions with different configuration parameters.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"Example HTTP operator and sensor\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.http import HttpGetOperator, HttpPostOperator\nfrom airflow.providers.standard.sensors.http import HttpSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_http\"\n\nconn_name = \"http_conn\"\nconn = Connection(\n    conn_id=conn_name,\n    schema=\"http\",\n    conn_type=\"http\",\n    host=\"httpbin.org\",\n    login=\"username\",\n    password=\"password\",\n    port=80,\n    extra='{\"url\": \"https://httpbin.org/get\"}',\n)\n\ntasks = [conn.create_task()]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef create_start_url():\n    \"\"\"\n    Example function to create start url for example usage of http_post_operator with get_response_handler\n    :return: str, fully formed url\n    \"\"\"\n    return \"http://httpbin.org/post\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"http_conn_id\": conn_name},\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    check_connection = HttpSensor(task_ids=[\"check_connection\"], http_conn_id=conn_name, endpoint=\"/\")\n\n    url = create_start_url()\n\n    payload = {\"foo\": \"bar\", \"baz\": \"baz\"}\n\n    post_result = HttpPostOperator(task_id=\"post_result\", url=url, json=payload, response_handler=lambda *x: True)\n\n    get_result = HttpGetOperator(\n        task_id=\"get_result\", url=\"{{ task_instance.xcom_pull('check_connection', key='return_value') }}\",\n        extension_urls=[],\n        ignore_response_code=False,\n        validate_certs=True,\n        headers={},\n        query_params={},\n        form_data={},\n        form_files=[],\n        should_redirect=False,\n    )\n\n    delete_url = \"{{ task_instance.xcom_pull('check_connection', key='return_value') }}\"\n    (check_connection >> post_result >> get_result >> task.delete_task_instance(task_id=delete_url))\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1346_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963655}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data protection workflow that enables dynamic configuration of sensitive data detection rules using Google Cloud's Data Loss Prevention service. Create a pipeline that uploads custom dictionaries, defines information type patterns, and demonstrates how to programmatically manage data classification rules.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud DLP service testing\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dlp_v2.types import ContentItem, InspectConfig, InspectTemplate\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateDatasetOperator,\n    CloudDLPCreateInspectionJobOperator,\n    CloudDLPDeleteDatasetOperator,\n    CloudDLPDeleteInspectionJobOperator,\n    CloudDLPGetInspectionJobOperator,\n    CloudDLPListInspectionJobsOperator,\n)\nfrom airflow.providers.google.cloud.sensors.gcs import (\n    GCSObjectSensor,\n    GCSUploadUrlSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"dlp_inspection_job\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBLOB_NAME = \"test-dataset.json\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nTEMPLATE_NAME = f\"template_analyze_spam_{DAG_ID}\"\n\n# [START dlp_create_dataset]\nCREATE_DATASET_TASK = CloudDLPCreateDatasetOperator(\n    task_id=\"create_dataset\",\n    project_id=PROJECT_ID,\n    bucket_name=BUCKET_NAME,\n    blob_name=BLOB_NAME,\n    dataset_name=DATASET_NAME,\n    gcp_location=\"europe-west1\",\n    extra_fields={\"lateral_split_fraction\": 0},\n)\n# [END dlp_create_dataset]\n\n# [START dlp_create_inspection_job]\nCREATE_INSPECTION_JOB_TASK = CloudDLPCreateInspectionJobOperator(\n    task_id=\"create_inspection_job\",\n    dataset_id=DATASET_NAME,\n    inspect_config=InspectConfig(inspect_template=InspectTemplate(template_id=TEMPLATE_NAME)),\n    project_id=PROJECT_ID,\n    gcp_location=\"europe-west1\",\n    wait_for_status=True,\n)\n# [END dlp_create_inspection_job]\n\n# [START dlp_get_inspection_job]\nGET_INSPECTION_JOB_TASK = CloudDLPGetInspectionJobOperator(\n    task_id=\"get_inspection_job\",\n    inspection_job_id=\"{{ task_instance.xcom_pull('create_inspection_job_task')['name'].split('/')[-1] }}\",\n    project_id=PROJECT_ID,\n)\n# [END dlp_get_inspection_job]\n\n# [START dlp_list_inspection_jobs]\nLIST_INSPECTION_JOBS_TASK = CloudDLPListInspectionJobsOperator(\n    task_id=\"list_inspection_jobs\", project_id=PROJECT_ID, gcp_location=\"europe-west1\"\n)\n# [END dlp_list_inspection_jobs\"]\n\n# [START dlp_delete_inspection_job]\nDELETE_INSPECTION_JOB_TASK = CloudDLPDeleteInspectionJobOperator(\n    task_id=\"delete_inspection_job\",\n    inspection_job_id=\"{{ task_instance.xcom_pull('create_inspection_job_task')['name'].split('/')[-1] }}\",\n    project_id=PROJECT_ID,\n    gcp_location=\"europe-west1\",\n)\n# [END dlp_delete_inspection_job]\n\n# [START dlp_delete_dataset]\nDELETE_DATASET_TASK = CloudDLPDeleteDatasetOperator(\n    task_id=\"delete_dataset\", dataset_id=DATASET_NAME, project_id=PROJECT_ID, gcp_location=\"europe-west1\"\n)\n# [END dlp_delete_dataset]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp\", \"inspection job\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = GCSSensorOperator(\n        task_id=\"upload_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=BLOB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START dlp_impersonate_user]\n    impersonate_user = GCUSensorOperator(\n        task_id=\"impostere_user\",\n        usbc_bucket_url=\"{{ task_instance.xcom_pull('upload_file', key='bucket_url') }}\",\n        usbc_path=\"{{ task_instance.xcom_pull('upload_file', key='path') }}\",\n        force_exists=False,\n        force_delete=True,\n    )\n    # [END dlp_impersonate_user]\n\n    (# [START howto_operator_dlp_create_dataset]\n    create_dataset = CloudDLPCreateDatasetOperator(\n        task_id=\"create_dataset\",\n        project_id=PROJECT_ID,\n        bucket_name=BUCKET_NAME,\n        blob_name=BLOB_NAME,\n        dataset_name=DATASET_NAME,\n        gcp_location=\"europe-west1\",\n        extra_fields={\"lateral_split_fraction\": 0},\n    )\n    # [END howto_operator_dlp_create_dataset]\n\n    # [START howto_operator_dlp_create_inspection_job]\n    create_inspection_job = CloudDLPCreateInspectionJobOperator(\n        task_id=\"create_inspection_job\",\n        dataset_id=DATASET_NAME,\n        inspect_config={\n            \"inspect_template\": {\n                \"storage_config\": {\"gcs_source\": {\"object_uri\": f\"gs://{BUCKET_NAME}/{BLOB_NAME}\"}}\n            }\n        },\n        project_id=PROJECT_ID,\n        gcp_location=\"europe-west1\",\n    )\n    # [END howto_operator_dlp_create_inspection_job]\n\n    # [START howto_operator_dlp_get_inspection_job]\n    get_inspection_job = CloudDLPGetInspectionJobOperator(\n        task_id=\"get_inspection_job\", inspection_job_id=\"testing-inspection-job\"\n    )\n    # [END howto_operator_dlp_get_inspection_job]\n\n    # [START howto_operator_dlp_list_inspection_jobs]\n    list_inspection_jobs = CloudDLPListInspectionJobsOperator(\n        task_id=\"list_inspection_jobs\", project_id=PROJECT_ID, gcp_location=\"europe-west1\"\n    )\n    # [END howto_operator_dlp_list_inspection_jobs]\n\n    delete_dataset = CloudDLPDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, project_id=PROJECT_ID, gcp_location=\"europe-west1\"\n    )\n\n    delete_inspection_job = CloudDLPDeleteInspectionJobOperator(\n        task_id=\"delete_inspection_job\",\n        inspection_job_id=\"testing-inspection-job\",\n        project_id=PROJECT_ID,\n        gcp_location=\"europe-west1\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (create_bucket >> upload_file >> impersonate_user >> create_dataset >> create_inspection_job\n     >> get_inspection_job >> list_inspection_jobs >> delete_inspection_job >> delete_dataset >> delete_bucket)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done.\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_from_output\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run_from_output(dag)"}], "metadata": {"file_name": "dag_1211_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963789}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a cloud-based data discovery pipeline that leverages Google Dataproc to execute a simple Hive query across a dynamically created multi-node cluster. Construct a demonstration of scalable, on-demand data infrastructure that can quickly spin up and down for lightweight analytical tasks.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with hive job.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcp_dataproc_hive\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_{DAG_ID}_{ENV_ID}\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n}\n\n# Hive job\nHIVE_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_id\": CLUSTER_NAME},\n    \"hive_job\": {\"query_list\": {\"queries\": [\"SHOW DATABASES;\"]}},\n}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    env_id=ENV_ID,\n    tags=[\"example\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    run_hive_job = DataprocSubmitJobOperator(\n        task_id=\"run_hive_job\", region=REGION, project_id=PROJECT_ID, job=HIVE_JOB, cluster_name=CLUSTER_NAME\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (create_cluster >> run_hive_job >> delete_cluster)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1234_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796385}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I use AWS SDK for Python (Boto3) to filter and retrieve specific VPCs based on tags?"}, {"role": "assistant", "content": "import boto3\n\ndef get_vpcs_with_tags(tag_key, tag_value):\n    # Create an EC2 client\n    ec2 = boto3.client('ec2')\n    \n    # Define the parameters for describe_vpcs API call\n    params = {\n        'Filters': [\n            {\n                'Name': 'tag:' + tag_key,\n                'Values': [tag_value]\n            }\n        ]\n    }\n    \n    # Call describe_vpcs API\n    response = ec2.describe_vpcs(**params)\n    \n    # Extract and print VPC details\n    vpcs = response['Vpcs']\n    for vpc in vpcs:\n        print(f\"VPC ID: {vpc['VpcId']}, CIDR Block: {vpc['CidrBlock']}\")\n\n# Example usage\nget_vpcs_with_tags('Environment', 'Prod')"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7963943}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates various Google Cloud Storage sensor techniques, showcasing synchronous and asynchronous methods for detecting file upload completions and object changes. Build a workflow that illustrates different monitoring strategies for cloud storage events and file transfer validations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage sensors.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.gcs import (\n    GCSObjectExistenceSensor,\n    GCSObjectsNearbySensor,\n    GCSObjectUpdateSensor,\n    GCSUploadSessionCompleteSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"airflow-examples-gcp\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcs_sensor\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"example_upload.txt\"\nUPLOAD_FILE_PATH = f\"gcs/{FILE_NAME}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_file_in_gcs():\n        \"\"\"\n        Create a file in gcs which will be used as the content to upload later.\n        \"\"\"\n        from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFileSystemToGCSOperator\n\n        LocalFileSystemToGCSOperator(\n            task_id=\"create_file_in_gcs\",\n            src=UPLOAD_FILE_PATH,\n            dest=FILE_NAME,\n            bucket=BUCKET_NAME,\n        )\n\n    @task\n    def create_bucket():\n        \"\"\"\n        Create bucket.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n\n        GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=GCP_PROJECT_ID)\n\n    @task\n    def delete_bucket():\n        \"\"\"\n        Delete bucket.\n        \"\"\"\n        from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n        from airflow.providers.google.cloud.operators.gcs import GCSDeleteBucketOperator\n\n        GCSCreateBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, project_id=GCP_PROJECT_ID)\n        GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n\n    create_file_in_gcs_task = create_file_in_gcs()\n    create_bucket_task = create_bucket()\n    delete_bucket_task = delete_bucket()\n\n    # [START howto_gcp_gcs_upload_session_complete_sensor]\n    upload_session_complete = GCSUploadSessionCompleteSensor(\n        task_id=\"upload_session_complete\",\n        bucket_name=BUCKET_NAME,\n        source_object=UPLOAD_FILE_PATH,\n        destination_object=FILE_NAME,\n    )\n    # [END howto_gcp_gcs_upload_session_complete_sensor]\n\n    # [START howto_gcp_gcs_object_update_sensor]\n    update_status = GCSObjectUpdateSensor(\n        task_id=\"update_status\",\n        bucket_name=BUCKET_NAME,\n        object_name=FILE_NAME,\n        status=\"exists\",\n        poke_interval=5,\n    )\n    # [END howto_gcp_gcs_object_update_sensor]\n\n    # [START howto_gcp_gcs_objects_nearby_sensor]\n    nearby_files = GCSObjectsNearbySensor(\n        task_id=\"nearby_files\",\n        bucket_name=BUCKET_NAME,\n        near_by_objects=[FILE_NAME],\n        timeout=300,\n    )\n    # [END howto_gcp_gcs_objects_nearby_sensor]\n\n    # [START howto_gcp_gcs_object_existence_sensor]\n    existent_file = GCSObjectExistenceSensor(\n        task_id=\"existent_file\", bucket_name=BUCKET_NAME, object_name=FILE_NAME\n    )\n    # [END howto_gcp_gcs_object_existence_sensor]\n\n    # [START howto_gcp_gcs_object_existence_sensor_defered]\n    existing_file = GCSObjectExistenceSensor(\n        task_id=\"existing_file\",\n        bucket_name=BUCKET_NAME,\n        object_name=FILE_NAME,\n        deferrable=True,\n    )\n    # [END howto_gcp_gcs_object_existence_sensor_defered]\n\n    # [START howto_gcp_gcs_delete_bucket_sensor]\n    delete_bucket_sensor = GCSCreateBucketOperator(\n        task_id=\"delete_bucket_sensor\", bucket_name=BUCKET_NAME, project_id=GCP_PROJECT_ID\n    )\n    # [END howto_gcp_gcs_delete_bucket_sensor]\n\n    (\n        create_bucket_task\n        >> create_file_in_gcs_task\n        >> upload_session_complete\n        >> update_status\n        >> nearby_files\n        >> existent_file\n        >> existing_file\n        >> delete_bucket_sensor\n        >> delete_bucket_task\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1647_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.796406}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello Qwen, I want to know more about the use of conditional statements in Python. Can you provide me with some examples?"}, {"role": "assistant", "content": "x = 10\n\n# Check if x is greater than 5\nif x > 5:\n    print(\"x is greater than 5\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964125}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated file monitoring and processing workflow that detects new files in a source directory and moves them to a destination directory using Docker containers. Create a system that checks for file availability, moves the first detected file, and then prints its contents.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.base import SFTPSensor\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.sftp.transfers.local_to_sftp import LocalFileSystemToSFTPTempSensor\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_sensor\"\n\nsftp_conn_id = \"sftp_default\"\n\nSFTP_DIRECTORY = \"/tmp/airflow\"\nLOCAL_FILE_PATH = \"/tmp/file.txt\"\n\n\n@task\ndef create_sftp_connection():\n    conn = Connection(\n        conn_id=sftp_conn_id,\n        user=\"username\",\n        password=\"password\",\n        ssh_pkey=\"/dev/null\",\n    )\n    session = Session()\n    session.add_all([conn])\n    session.commit()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    set_up_sftp_connection = create_sftp_connection()\n\n    # [START howto_operator_sftp_temp_sensor]\n    sftp_temp_sensor = SFTPSensor(\n        task_id=\"sftp_temp_sensor\", sftp_path=\"/nonexistent/path\", check_interval=30\n    )\n    # [END howto_operator_sftp_temp_sensor]\n\n    # [START howto_operator_sftp_sensor]\n    sftp_sensor = SFTPSensor(task_id=\"sftp_sensor\", sftp_path=\"/tmp/\", timeout=60 * 2)\n    # [END howto_operator_sftp_sensor]\n\n    # [START howto_operator_local_file_system_to_sftp_temp_sensor]\n    local_file_system_to_sftp_temp_sensor = LocalFileSystemToSFTPTempSensor(\n        task_id=\"local_file_system_to_sftp_temp_sensor\",\n        local_paths=LOCAL_FILE_PATH,\n        remote_paths=SFTP_DIRECTORY,\n        check_interval=30,\n    )\n    # [END howto_operator_local_file_system_to_sftp_temp_sensor]\n\n    (\n        # TEST SETUP\n        set_up_sftp_connection\n        # TEST BODY\n        >> sftp_temp_sensor\n        >> sftp_sensor\n        # TEST TEARDOWN\n        >> local_file_system_to_sftp_temp_sensor\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done / remove\n    # the temp data file\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_452_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964191}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential execution of Trino database operations, illustrating how to create schemas, define tables, insert data, and perform template-based and parameterized queries. The workflow should highlight the dependency chaining and error handling capabilities of Airflow with a Trino data source.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.trino.operators.trino import TrinoOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_operator\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino]\n    sql1 = \"SELECT 1 as i;\"\n\n    trino_task1 = TrinoOperator(task_id=\"trino_task1\", sql=sql1)\n    # [END howto_operator_trino]\n\n    # [START howto_operator_trino_template_file]\n    sql2 = \"SELECT $1 as i;\"\n\n    trino_task2 = TrinoOperator(\n        task_id=\"trino_task2\",\n        sql=sql2,\n        parameters={\"$1\": \"2\"},\n    )\n    # [END howto_operator_trino_template_file]\n\n    trino_task1 >> trino_task2\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964232}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases the end-to-end process of creating cloud resources, performing a data transfer, and then cleaning up those resources. Develop a systematic workflow that demonstrates provisioning, data movement, and resource teardown in a cloud environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_sql\"\n\nS3_BUCKET = f\"bucket_{DAG_ID}_{ENV_ID}\"\nS3_KEY_1 = \"greenpeppers.csv\"\nS3_KEY_2 = \"spam.csv\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket_task = S3BucketCreateOperator(task_id=\"create_s3_bucket_task\", bucket_name=S3_BUCKET)\n\n    @task\n    def populate_s3_bucket():\n        s3_hook = S3Hook()\n        key = S3_KEY_1\n        s3_hook.load_file(path=key, bucket_name=S3_BUCKET, dest_path=key)\n\n        key = S3_KEY_2\n        s3_hook.load_file(path=key, bucket_name=S3_BUCKET, dest_path=key)\n\n    delete_s3_bucket_task = S3BucketDeleteOperator(\n        task_id=\"delete_s3_bucket_task\", bucket_name=S3_BUCKET, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    # [START howto_transfer_s3_to_sql]\n    s3_to_sql_task = S3ToSqlOperator(\n        task_id=\"s3_to_sql_task\",\n        sql=\"SELECT col1, col2 FROM s3://bucket_name/key_path/\",\n        conn_id=\"aws_default\",\n        schema_conn_id=\"postgres_default\",\n        table_schema=\"public\",\n        table_name=\"temp_table\",\n    )\n    # [END howto_transfer_s3_to_sql]\n\n    (\n        # TEST SETUP\n        create_s3_bucket_task\n        >> populate_s3_bucket()\n        # TEST BODY\n        >> s3_to_sql_task\n        # TEST TEARDOWN\n        >> delete_s3_bucket_task\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1579_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964284}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that transforms raw text data into dense vector representations using Cohere's embedding technology, preparing unstructured text for advanced natural language processing tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.cohere.operators.embedding import CohereEmbeddingOperator\n\ntry:\n    from airflow.providers.standard.operators.python import PythonOperator\nexcept ImportError:\n    from airflow.providers.standard.operators.empty import EmptyOperator as PythonOperator  # type: ignore\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_cohere_embedding_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    run_this = PythonOperator(\n        task_id=\"cohere_embed_task\",\n        python_callable=lambda *args, **kwargs: CohereEmbeddingOperator(\n            task_name=\"embedding\",\n            input_text=[u\"apples bananas cherries date eggs fig\",\n                        u\"Harry Potter is awesome. It has magic and wizards.\",\n                        u\"Who would have thought London would become the capital of space exploration?\"]\n        ).expand(),\n    )\n\n    pass_this = PythonOperator(\n        task_id=\"pass_by_xcom_cohere_embed_task\",\n        python_callable=lambda output: None,\n        op_kwargs={\"xcom_key\": \"cohere_embedding_output\", \"task_result\": output},\n    )\n\n    run_this >> pass_this\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1548_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964342}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Oracle database workflow that demonstrates basic SQL query execution and stored procedure interactions. Create a DAG that showcases different methods of executing database operations using Oracle connection parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_oracle_stored_procedure_operator]\n    # Storing calculation result in DBCONN variable\n    conn_id = \"oracle_conn\"\n    sql_stmt = \"\"\"\n        DECLARE\n            l_sum number;\n        BEGIN\n            SELECT sum Sal INTO l_sum FROM emp WHERE deptno = &param1;\n            DBMS_OUTPUT.PUT_LINE('Sum is ' || l_sum);\n        END;\n    \"\"\"\n\n    # Writing a Python function that calls the Oracle STORED PROCEDURE operator\n    def execute_stored_procedure_func():\n        conn = self.conn\n        cursor = conn.cursor()\n        cursor.execute(stored_procedure_sql)\n        row = cursor.fetchone()\n        return row\n\n    store_procedure_obj = OracleStoredProcedureOperator(task_id=\"execute_stored_procedure_task\", conn_id=conn_id, sql=sql_stmt, param1=10)\n    # [END howto_oracle_stored_procedure_operator]\n\n    store_procedure_obj >> execute_stored_procedure_func()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_708_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964396}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases cloud infrastructure management by dynamically creating a GCS bucket, transferring data using a specialized operator, and then cleaning up cloud resources after the transfer is complete.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_sftp_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSFTP_PATH = \"/tmp/my-sftp-file.txt\"\nLOCAL_PATH = \"/tmp/my-local-file.txt\"\n\n\n@task\ndef create_file_from_url():\n    with open(LOCAL_PATH, \"w\") as file:\n        file.write(f\"Hello {os.environ.get('SYSTEM_TESTS_NAME')}\\n\")\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID)\n\n    # [START howto_operator_sftp_to_gcs]\n    upload_sftp_to_gcs = SFTPToGCSOperator(\n        task_id=\"upload_sftp_file_to_gcs\",\n        sftp_path=SFTP_PATH,\n        gcs_bucket=BUCKET_NAME,\n        local_path=LOCAL_PATH,\n        replace=True,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_sftp_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> upload_sftp_to_gcs\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964451}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that sequentially executes SQL tasks with clear dependency management, showing how tasks can be chained together to perform a complete database operation lifecycle. The DAG should illustrate task ordering and atomic database interactions.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample DAG demonstrating the usage of the SQLExecuteQueryOperator with MSSQL.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nMSSQL_CONN_ID = \"mssql\"\nSQL_FILE = \"sql/example_mssql.sql\"\n\n# [START howto_operator_mssql]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mssql\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\"conn_id\": MSSQL_CONN_ID},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    sql_task = SQLExecuteQueryOperator(\n        task_id=\"sql_task\",\n        sql=SQL_FILE,\n        split_statements=False,\n        params={\"last_name\": \"Test\"},\n        check_statement=\"Hello World\",\n    )\n\n    sql_task\n\n# [END howto_operator_mssql]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2180_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964487}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that performs time-span based file processing in Google Cloud Storage, applying a custom transformation script to selected files from a source bucket and transferring the results to a destination bucket. Create a systematic approach to data processing that includes automated setup, transformation, and cleanup of cloud storage resources.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSTimeSpanFileTransformOperator operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSTimeSpanFileTransformOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcp_gcs_transform_timespan\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"transform-timespan\"],\n) as dag:\n    create_bucket_src = GCSCreateBucketOperator(\n        task_id=\"create_bucket_src\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    create_bucket_dst = GCSCreateBucketOperator(\n        task_id=\"create_bucket_dst\",\n        bucket_name=BUCKET_NAME_DST,\n        project_id=PROJECT_ID,\n    )\n    # [START howto_operator_gcs_timespan_file_transform]\n    transform_timespan = GCSTimeSpanFileTransformOperator(\n        task_id=\"timespan_file_transform\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_prefix=\"timespan_input/\",\n        destination_bucket=BUCKET_NAME_DST,\n        apply_transform=[\n            {\n                \"action\": \"CREATE\",\n                \"path\": \"output1.txt\",\n            },\n            {\n                \"action\": \"CREATE\",\n                \"path\": \"output2.txt\",\n            },\n            {\n                \"action\": \"OVERWRITE\",\n                \"path\": \"output1.txt\",\n                \"source_path\": \"output2.txt\",\n            },\n            {\n                \"action\": \"DELETE\",\n                \"path\": \"output1.txt\",\n            },\n        ],\n        timespan=\"{{timedelta(days=1))}\",\n    )\n    # [END howto_operator_gcs_timespan_file_transform]\n\n    delete_bucket_src = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_src\", bucket_name=BUCKET_NAME_SRC, trigger_rule=TriggerRule.ALL_DONE\n    )\n    delete_bucket_dst = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_dst\", bucket_name=BUCKET_NAME_DST, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (create_bucket_src >> create_bucket_dst >> transform_timespan >> (delete_bucket_src >> delete_bucket_dst))\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2063_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982713.7964563}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Azure Data Lake Storage (ADLS) workflow that demonstrates file upload and immediate deletion. Design a simple DAG that showcases basic file management operations in a cloud storage environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSCreateObjectOperator, ADLSDeleteOperator\n\nPATH = \"/tmp/my-test-file\"\nDAG_ID = \"adls_delete_file\"\n\n\n@task\ndef create_files():\n    \"\"\"Create files\"\"\"\n    commands = [\n        f\"echo '{{{{'1234567890'}}}>{{{{'1234567890'}}}}}' > {PATH}\",\n        f\"echo '{{{{'1234567890'}}}>{{{{'1234567890'}}}}}' >> {PATH}\",\n    ]\n    for command in commands:\n        cmd_parts = command.split()\n        cmd_env = os.environ.copy()\n        cmd_env[\"AIRFLOW_TEST_LEVEL\"] = \"test\"\n        # [START howto_operator_adls]\n        from airflow.providers.microsoft.azure.operators.adls import ADLSCreateObjectOperator\n\n        adls_create_object_operator = ADLSCreateObjectOperator(task_id=\"adls_create_object_task\", path=PATH, cmd_cmd_parts=cmd_parts, task_open_line=None, env=cmd_env)\n        # [END howto_operator_adls]\n        adls_create_object_operator.expand_path()\n\n        cmd_env[\"AIRFLOW_TEST_LEVEL\"] = \"test success\"\n        # [START howto_operator_adls_no_wait]\n        from airflow.providers.microsoft.azure.operators.adls import ADLSCreateObjectOperator\n\n        adls_create_object_operator = ADLSCreateObjectOperator(task_id=\"adls_create_object_task_without_wait\", path=PATH, cmd_cmd_parts=cmd_parts, task_open_line=None, env=cmd_env, wait_for_status=False)\n        # [END howto_operator_adls_no_wait]\n\n\n@task\ndef delete_files():\n    \"\"\"Delete files\"\"\"\n    commands = [\"rmdir /Q /S {}\".format(PATH)]\n    for command in commands:\n        cmd_parts = command.split()\n        cmd_env = os.environ.copy()\n        cmd_env[\"AIRFLOW_TEST_LEVEL\"] = \"test\"\n        # [START howto_operator_adls]\n        from airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\n\n        adls_delete_operator = ADLSDeleteOperator(task_id=\"adls_delete_task\", path=PATH)\n        # [END howto_operator_adls]\n        adls_delete_operator.expand_path()\n        # [START howto_operator_adls_no_wait]\n        from airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\n\n        adls_delete_operator = ADLSDeleteOperator(task_id=\"adls_delete_task_without_wait\", path=PATH, wait_for_status=False)\n        # [END howto_operator_adls_no_wait]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    create_files()\n    delete_files()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_962_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8804693}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases inter-service communication using Redis as a message broker and synchronization mechanism. Develop a workflow that publishes a start signal and then waits for a confirmation key before advancing to subsequent tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\n# [START dag_decorator_usage]\nimport datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"hello\")\ndef hello():\n    print(\"Hello\")\n\n\n@task(task_id=\"world\")\ndef world():\n    print(\"World\")\n\n\nwith DAG(\n    dag_id=\"example_decorators\",\n    schedule=None,\n    start_date=datetime.datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    hello()\n    world()\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END dag_decorator_usage]"}], "metadata": {"file_name": "dag_2197_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880475}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data workflow automation process that leverages Azure Synapse for executing a predefined data processing pipeline on a daily schedule. Configure the DAG to reliably trigger and monitor the pipeline execution with appropriate error handling and logging.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunPipelineOperator\n\ntry:\n    from airflow.providers.microsoft.azure.transfers.sql_to_sql import SQLToSQLOperator\nexcept ModuleNotFoundError:\n    # This import throws a ModuleNotFoundError in Airflow < 2.7.0\n    from airflow.providers.microsoft.azure.transfers.sql_to_sql_2 import SQLToSQLOperator\n\n# [START howto_operator_azure_synapse_run_pipeline]\ndef _run_pipeline(*, sql_command):\n    # Creating a dummy connection to Sql Server.\n    # In fact, you should set your Azure Synapse Connection here.\n    conn_args = {\n        \"db_conn_id\": \"sqlserver\",\n        \"sql_command\": sql_command,\n    }\n\n    # Create a task to run the command through the sql connection.\n    sql_to_sql_task = SQLToSQLOperator(task_id=\"sql_to_sql_task\", **conn_args)\n\n    # Call the task.\n    sql_to_sql_task.execute()\n\n    return True\n\n\nwith DAG(\n    dag_id=\"azure_synapse_run_pipeline\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_pipeline = AzureSynapseRunPipelineOperator(\n        task_id=\"run_pipeline\",\n        pipeline_name=\"{{ task_instance.xcom_pull('get_pipeline_id') }}\",\n        azure_synapse_conn_id=\"azure_synapse_default\",\n    )\n\n    _run_pipeline(\n        sql_command=\"SELECT 1\",\n    )\n    # [END howto_operator_azure_synapse_run_pipeline]\n\n    run_pipeline >> os.environ[\"AZURE_SYNAPSE_ACCOUNT\"]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1364_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8804805}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed task automation DAG that leverages WinRM to execute multiple system commands across a remote Windows infrastructure. Create a workflow that simulates real-world remote server management scenarios with parallel task execution and dependency tracking.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport winrm.error\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.winrm.operators.winrm import WinRMOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_winrm_hook]\ndef winrm_hook_callable():\n    winrm_hook = WinRMHook()\n    # Relying on the decorator, the parameters are automatically set\n    winrm_operator = WinRMOperator(task_id=\"winrm_test_task\", winrm_conn_id=\"winrm_default\", url=\"http://172.16.105.148:5555\")\n    winrm_hook.connect()\n    # Every operation performed on the object method will be logged, including the return value\n    print(winrm_operator.output)\n    winrm_hook.disconnect()\n\n\n# [END howto_winrm_hook]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"winrm_test\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    chain(\n        # [START howto_winrm_from_external_file]\n        winrm_hook_callable(),\n        # [END howto_winrm_from_external_file]\n    )\n    # [START howto_winrm_operator]\n    t1 = WinRMOperator(task_id=\"winrm_test_task\", winrm_conn_id=\"winrm_default\", url=\"http://172.16.105.148:5555\")\n    # [END howto_winrm_operator]\n\n    t1 >> chain(t2=t1)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_977_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8804855}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure pipeline that automates the provisioning and management of a scalable NoSQL database environment using Cloud Bigtable. Design a workflow that demonstrates dynamic resource allocation, replication monitoring, and systematic teardown of database infrastructure with configurable parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Memorystore Memcached service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.memorystore import MemoracleMemcachedHook\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_gcp_memorystore_memcached\"\n\nMEMCACHED_DISPLAY_NAME = f\"memcached-{DAG_ID}-{ENV_ID}\"\nMEMORY_STORE_ID = \"memcached-1\"\nBIGQUERY_DATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBIGQUERY_TABLE_NAME = \"memcached\"\n\n\n@task\ndef create_memoracle_memcached():\n    \"\"\"Creates a memcached instance.\"\"\"\n    bigquery_dataset = BIGQUERY_DATASET_NAME\n    memory_store_instance_name = MEMCACHED_DISPLAY_NAME\n\n    # Example of creating a Memcached instance with predefined settings.\n    MemoracleMemcachedHook(create_instance_api_call_params={\n        \"display_name\": memory_store_instance_name,\n        \"memory_store_id\": MEMORY_STORE_ID,\n        \"gce_instance_config\": {\n            \"project\": PROJECT_ID,\n            \"zone\": \"europe-west4-a\",\n        },\n    }).create_instance()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_memoracle_memcached():\n    \"\"\"Deletes a memcached instance.\"\"\"\n    memory_store_instance_name = MEMCACHED_DISPLAY_NAME\n    # [START howto_operator_gcp_delete_resource]\n    MemoracleMemcachedHook().delete_instance(memory_store_instance_name)\n    # [END howto_operator_gcp_delete_resource]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bigquery_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_bigquery_dataset\", dataset_id=BIGQUERY_DATASET_NAME\n    )\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=BIGQUERY_DATASET_NAME,\n        table_id=BIGQUERY_TABLE_NAME,\n        table_resource={\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"key\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                    {\"name\": \"value\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n                ]\n            },\n            \"timePartitioning\": {\n                \"dayStart\": 2021,\n                \"monthStart\": 1,\n                \"yearStart\": 2020,\n            },\n        },\n    )\n\n    insert_data = BigQueryInsertJobOperator(\n        task_id=\"insert_data\",\n        configuration={\n            \"query\": {\n                \"query\": f\"INSERT {BIGQUERY_DATASET_NAME}.{BIGQUERY_TABLE_NAME} VALUES ( 'key', 'value')\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=BIGQUERY_DATASET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_memoracle_memcached()\n        >> create_bigquery_dataset()\n        >> create_table()\n        >> insert_data()\n        # TEST TEARDOWN\n        >> delete_dataset()\n        >> delete_memoracle_memcached()\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_821_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8804953}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data transfer pipeline that moves a sample CSV file between cloud storage platforms while implementing robust testing and teardown mechanisms. The pipeline should showcase how to programmatically create, transfer, and delete cloud storage resources using infrastructure-as-code principles.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_sftp_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\nSFTP_PATH = \"/tmp/file.txt\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_temp_sftp_connection():\n        conn = Connection(\n            conn_id=\"temp_sftp_conn\",\n            user=\"test-sftp-user\",\n            password=\"test-sftp-password\",\n            private_key=\"path/to/private/key\",\n        )\n        env_id = ENV_ID.lstrip(\"dev\")\n        conn.extra_fields = {\"sftp_path\": SFTP_PATH}\n\n        gcp_hook = GCSHook()\n        gcp_hook.create_temp_sftp_connection(conn=conn, project_id=PROJECT_ID, env_id=env_id)\n\n    create_temp_sftp_connection_task = create_temp_sftp_connection()\n\n    move_file_to_bucket = SFTPToGCSOperator(\n        task_id=\"move_file_to_bucket\",\n        sftp_conn_id=\"temp_sftp_conn\",\n        gcs_bucket=BUCKET_NAME,\n        gcs_prefix=SFTP_PATH,\n        filename=FILE_NAME,\n        replace=True,\n        task_id=\"move_file_to_bucket\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_temp_sftp_connection_task\n        # TEST BODY\n        >> move_file_to_bucket\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as done above)\n    # for run\u6d4b\u8bd5\u7528\u4f8b\u9700\u8981watcher\u5728order\u4e2d\u624d\u80fd\u6b63\u786e\u6807\u8bb0\u4e3arun\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880504}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for managing Google Analytics admin tasks, including creating, listing, and deleting properties and data streams. Build a comprehensive DAG that demonstrates interaction with Google Analytics Admin API using Airflow's operators and task dependencies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that shows the complexity level of one (almost) full Google Analytics.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.analytics import (\n    GoogleAnalyticsAdminDeleteDataStreamOperator,\n    GoogleAnalyticsAdminDeletePropertyOperator,\n    GoogleAnalyticsAdminInsertDatastreamOperator,\n    GoogleAnalyticsAdminInsertPropertyOperator,\n    GoogleAnalyticsAdminListDatastreamsOperator,\n    GoogleAnalyticsAdminListPropertiesOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_google_marketing_platform_analytics\"\n\nPROPERTY_ID = os.environ.get(\"MARKETING_PLATFORM_PROPERTY_ID\", \"123456789\")\nGAC_ACCOUNT_ID = os.environ.get(\"MARKETING_PLATFORM_GAC_ACCOUNT_ID\", \"12345\")\nDATASTREAM_ID = os.environ.get(\"MARKETING_PLATFORM_DATASTREAM_ID\", \"12345\")\nBRAND_DATASTREAM_ID = os.environ.get(\"MARKETING_PLATFORM_BRAND_DATASTREAM_ID\", \"12346\")\nNEW_DATASTREAM = {\n    \"dataStream\": {\"accountId\": str(GAC_ACCOUNT_ID), \"displayName\": \"Test Display Name\"},\n    \"servicesConfig\": {\"gclitConfig\": {}},\n}\n# For use when data is unavailable. Used to mark report generation as errorable.\nERROR_DATASTREAM = {\n    \"dataStream\": {\"accountId\": str(GAC_ACCOUNT_ID), \"displayName\": \"Test Display Name Error\"},\n    \"servicesConfig\": {\"gclitConfig\": {}},\n}\n# New in version 6.18.0: Data Streams may be marked as hidden, which will stop new data being ingested into them,\n# but\u672a\u5f71\u54cd\u5df2\u7ecf\u5b58\u5728\u7684\u6570\u636e\u3002\nDATA_STREAMS_TO_HIDE = [\n    { \"dataStreamId\": DATASTREAM_ID },\n    { \"dataStreamId\": BRAND_DATASTREAM_ID },\n]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_marketing_platform_create_property]\n    create_property = GoogleAnalyticsAdminInsertPropertyOperator(\n        task_id=\"create_property\",\n        property=NEW_DATASTREAM,\n        gac_account_id=GAC_ACCOUNT_ID,\n    )\n    # [END howto_marketing_platform_create_property]\n\n    # [START howto_marketing_platform_create_property_result]\n    property_id_result = TaskResultSensor(\n        task_id=\"property_id_result\",\n        conn_id=\"google-marketing-platform\",\n        location=\"{{ task_instance.xcom_pull('create_property')['location'] }}\",\n        property_id=\"{{ task_instance.xcom_pull('create_property')['name']}}\"),\n    # [END howto_marketing_platform_create_property_result]\n\n    # [START howto_marketing_platform_create_data_stream]\n    create_datastream = GoogleAnalyticsAdminInsertDatastreamOperator(\n        task_id=\"create_datastream\",\n        data_stream=ERROR_DATASTREAM,\n        gac_account_id=GAC_ACCOUNT_ID,\n    )\n    # [END howto_marketing_platform_create_data_stream]\n\n    # [START howto_marketing_platform_create_data_stream_result]\n    datastream_id_result = TaskResultSensor(\n        task_id=\"datastream_id_result\",\n        conn_id=\"google-marketing-platform\",\n        location=\"{{ task_instance.xcom_pull('create_datastream')['location'] }}\",\n        datastream_id=\"{{ task_instance.xcom_pull('create_datastream')['name']}}\"),\n    # [END howto_marketing_platform_create_data_stream_result]\n\n    # [START howto_marketing_platform_list_properties]\n    list_properties = GoogleAnalyticsAdminListPropertiesOperator(task_id=\"list_properties\")\n    # [END howto_marketing_platform_list_properties]\n\n    # [START howto_marketing_platform_list_properties_result]\n    check_property_ids = CheckTaskCompletion(\n        task_id=\"check_property_ids\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        sql=f\"SELECT id, name FROM {{ task_instance.xcom_pull('list_properties')[0]['properties'] }}\",\n        params={\"propertyIds\": PROPERTY_ID},\n    )\n    # [END howto_marketing_platform_list_properties_result]\n\n    # [START howto_marketing_platform_hide_data_stream]\n    hide_datastream = GoogleAnalyticsAdminInsertDatastreamOperator(\n        task_id=\"hide_datastream\",\n        data_stream=ERROR_DATASTREAM,\n        gac_account_id=GAC_ACCOUNT_ID,\n        data_stream_ids=DATA_STREAMS_TO_HIDE,\n    )\n    # [END howto_marketing_platform_hide_data_stream]\n\n    # [START howto_marketing_platform_unhide_data_stream]\n    unhide_datastream = GoogleAnalyticsAdminInsertDatastreamOperator(\n        task_id=\"unhide_datastream\",\n        data_stream=ERROR_DATASTREAM,\n        gac_account_id=GAC_ACCOUNT_ID,\n    )\n    # [END howto_marketing_platform_unhide_data_stream]\n\n    # [START howto_marketing_platform_create_property_result]\n    property_id_result_2 = TaskResultSensor(\n        task_id=\"property_id_result_2\",\n        conn_id=\"google-marketing-platform\",\n        location=\"US\",\n        project_id=PROPERTY_ID,\n    )\n    # [END howto_marketing_platform_create_property_result]\n\n    @task\n    def wait_before_delete_property(property_id):\n        print(f\"Waiting for deletion of property id: {property_id}. \")\n        while True:\n            time_sleep = 2\n            log_message = f\"Waiting to delete property ...{time_sleep} sec\"\n            print(log_message)\n            time.sleep(time_sleep)\n            log_message += f\" Property id : {property_id}\"\n            get_command = \"gsutil ls aiplatform.googleapis.com/projects/{project_id}/locations/{location}/predictions/{prediction_id}\".format(\n                project_id=PROPERTY_ID, location=property_id, prediction_id=ERROR_DATASTREAM[\"dataStream\"][\"predictionId\"]\n            )\n            if os.system(get_command) != 0:\n                break\n\n    wait_before_delete_property(property_id_result_2.output)\n\n    # [START howto_marketing_platform_delete_property]\n    delete_property = GoogleAnalyticsAdminDeletePropertyOperator(\n        task_id=\"delete_property\",\n        property_id=PROPERTY_ID,\n        gac_account_id=GAC_ACCOUNT_ID,\n    )\n    # [END howto_marketing_platform_delete_property]\n\n    # [START howto_marketing_platform_delete_property_result]\n    property_id_result_3 = TaskResultSensor(\n        task_id=\"property_id_result_3\",\n        conn_id=\"google-marketing-platform\",\n        location=\"US\",\n        property_id=PROPERTY_ID,\n    )\n    # [END howto_marketing_platform_delete_property_result]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_datastream(prediction_id):\n        path = f\"gs://{prediction_id}\"\n        command = f\"gsutil rm {path}\"\n        os.system(command)\n\n    delete_datastream(ERROR_DATASTREAM[\"dataStream\"][\"predictionId\"])\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1730_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805199}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a simple Yandex Cloud data query workflow that demonstrates basic Airflow orchestration. Design a DAG that executes a trivial SQL query and marks its completion with a final empty operator.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.yandex.operators.yandex_cloud import YCSQLOperator\n\nwith DAG(\n    \"ycsql_example\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n):\n    # [START howto_operator_ycsql]\n    sql_task = YCSQLOperator(task_id=\"ycsql_task\", sql=\"select 1\", split_statements=True)\n    # [END howto_operator_ycsql]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2243_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805234}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that demonstrates advanced branching techniques using different Python execution contexts, showing how to dynamically select task paths and handle complex workflow routing. Implement a DAG that explores random selection, external Python execution, and virtual environment task execution with clear demonstration of conditional task routing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the BranchPythonOperator.\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nimport sys\nimport tempfile\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.providers.standard.operators.python import (\n    BranchPythonOperator,\n    PythonOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_python_operator\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = EmptyOperator(task_id=\"run_this\")\n\n    @task.branch()\n    def branch_func(random_string):\n        return f\"branch_{random_string}\"\n\n    branch = branch_func(task.args[\"RANDOM_STRING\"])\n\n    join = EmptyOperator(task_id=\"join\")\n    t1 = branch >> EmptyOperator(task_id=\"branch_1\")\n    t2 = branch >> EmptyOperator(task_id=\"branch_2\")\n    t3 = branch >> EmptyOperator(task_id=\"branch_3\")\n    t4 = branch >> EmptyOperator(task_id=\"branch_4\")\n    [t1, t2, t3, t4] >> join\n\n    @dag.task\n    def print_context(**kwargs):\n        print(f\"::group::Expanded kwargs ({', '.join([str(item) for item in kwargs.items()])})\")\n\n    print_context(task_id=\"print_context\")\n\n    t1 >> print_context()\n    t2 >> print_context()\n    t3 >> print_context()\n    t4 >> print_context()\n\n    empty = EmptyOperator(task_id=\"empty\")\n    t5 = branch >> empty\n\n    @dag.task\n    def print_context_no_trigger_rule(**kwargs):\n        print(f\"::group::Expanded kwargs ({', '.join([str(item) for item in kwargs.items()])})\")\n\n    print_context_no_trigger_rule(task_id=\"print_context_no_trigger_rule\")\n\n    empty >> print_context_no_trigger_rule()\n\n    [t5, empty] >> JoinTask = EmptyOperator(task_id=\"JoinTask\", trigger_rule=TriggerRule.ALL_DONE)\n\n    @dag.task\n    def join_task():\n        pass\n\n    join_task() >> JoinTask"}], "metadata": {"file_name": "dag_1803_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805296}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that simulates a controlled database transaction process with explicit start and cancellation steps in a Google Cloud environment. Create a systematic demonstration of transaction management that ensures data consistency and prevents unintended persistent changes.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCancelJobOperator,\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryGetJobOperator,\n    BigQueryUpdateJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.google.cloud.transfers.mssql_to_bigquery import MsSqlToBigQueryOperator\nexcept ImportError:\n    # This import throws ImportErrors in Airflow < 3.0\n    pytest.skip(\"MSSQL provider is currently broken in Airflow < 3.0\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_bigquery_to_bigquery\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_FILE = \"test-file.csv\"\nTABLE_MYSQL SOURCE = \"source_table\"\nTABLE_BIGQUERY TARGET = \"target_table\"\n\nFIELDS = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"ds\", \"type\": \"DATE\", \"mode\": \"NULLABLE\"},\n]\nLOCATION = \"EU\"\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE {SOURCE} AS\n    SELECT id as value, name, ds\n    FROM STAGING.BUCKET {BUCKET_NAME} WHERE ds BETWEEN '2015-08-01' AND '2015-09-01'\n\"\"\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=TARGET)\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=TARGET,\n        table_id=TARGET,\n        fields=FIELDS,\n        source_table={},\n        location=LOCATION,\n    )\n\n    copy_selected_data = BigQueryToGCSOperator(\n        task_id=\"copy_selected_data\",\n        source_project_dataset_pairs=[{\"project_dataset_relation\": f\"{PROJECT_ID}.{TARGET}.{TARGET}\"}],\n        output_bucket_name=BUCKET_NAME,\n        output_object_name_prefix=f\"{TARGET}/\",\n    )\n\n    update_job = BigQueryUpdateJobOperator(\n        task_id=\"update_job\",\n        job_id=\"{{ job_id }}\",\n        configuration_job={\n            \"config\": {\n                \"query\": {\n                    \"query\": SQL_CREATE_TABLE,\n                    \"useLegacySql\": False,\n                }\n            }\n        },\n    )\n\n    # [START howto_operator_bigquery_to_bigquery]\n    transfer_data = BigQueryToBigQueryOperator(\n        task_id=\"transfer_data\",\n        source_project_dataset_tables=[f\"{PROJECT_ID}.{TARGET}.{TARGET}\"],\n        destination_project_dataset_table=f\"{PROJECT_ID}.{SOURCE}.{SOURCE}\",\n    )\n    # [END howto_operator_bigquery_to_bigquery]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=TARGET, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_job = BigQueryCancelJobOperator(\n        task_id=\"delete_job\",\n        job_id=\"{{ job_id }}\",\n        cancel_reason=\"user_cancel\",\n    )\n\n    get_job = BigQueryGetJobOperator(\n        task_id=\"get_job\",\n        job_id=\"{{ job_id }}\",\n    )\n\n    show_jobs = [\n        {\n            \"job_id\": \"{{ job_id }}\",\n        },\n        {\n            \"job_id\": \"bigquery_default_dataflow_job-1\",\n            \"cancel_reason\": \"user_cancel\",\n        },\n    ]\n\n    for show in show_jobs:\n        get_job.action_mask.add(**show)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1249_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805408}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a custom prompt for a text generation model using Python?"}, {"role": "assistant", "content": "# Example of a simple context\ncontext = \"Once upon a time in a land far, far away...\""}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805506}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates automated vector data insertion with configurable parameters like index name, namespace, and batch size. Construct a workflow that shows how to dynamically load vector embeddings from environment variables or predefined lists.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom system_tests_utils.vector_utils import get_vector_index_name\n\n# [START howto_vectd_apply_operator_vector_index]\nfrom airflow.models.dag import DAG\nfrom airflow.providers.vector.applications.api.utils import VectorUtilsTestContext\nfrom airflow.providers.vector.operators.vector import VectorApplyOperator\n\nCOLUMN_ID = \"my_id\"\nCOLUMN_VALUE = \"my_value\"\nDAG_ID = \"vector_apply_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    default_args={\"vector_index_name\": get_vector_index_name(COLUMN_ID)},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = VectorUtilsTestContext()\n    apply_operator = VectorApplyOperator(task_id=\"test_vector_apply_operator_task\", test_context=test_context)\n\n    @dag.task(task_id=\"print_env_vars\")\n    def print_env_vars():\n        print(f\"COLUMN_ID={COLUMN_ID}\")\n        print(f\"DAG_ID={DAG_ID}\")\n\n    (test_context_task_output >> apply_operator >> print_env_vars())\n\n    # [END howto_vectd_apply_operator_vector_index]\n\n\ndag\n# [END howto_vectd_apply_operator_vector_index]"}], "metadata": {"file_name": "dag_639_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805547}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that demonstrates enterprise-level data extract refresh techniques for business intelligence reporting systems. Create a workflow that shows how to trigger workbook refreshes and monitor their completion status using different execution strategies.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using ExcelToBlobOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.excel import ExcelToAzureBlobOperator\n\nAZURE_CONNECTION_ID = os.environ.get(\"AZURE_CONNECTION_ID\", \"azure_default\")\nCONTAINER_NAME = os.environ.get(\"AZURE_CONTAINER_NAME\", \"airflow-excel-blob\")\nWORKBOOK_NAME = os.environ.get(\"AZURE_WORKBOOK_NAME\", \"excel-workbook\")\nWORKSHEET_NAME = os.environ.get(\"AZURE_WORKSHEET_NAME\", \"airflow-excel-worksheet\")\n\nDAG_ID = os.environ.get(\"DAG_ID\", \"example_excel_to_blob\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_excel_to_blob]\n    excel_to_blob = ExcelToAzureBlobOperator(\n        task_id=\"excel_to_blob\",\n        input_excel=\"excel-file\",\n        output_blob_name=f\"{CONTAINER_NAME}/{WORKBOOK_NAME}/part-{WORKSHEET_NAME}\",\n        az_connection=AZURE_CONNECTION_ID,\n    )\n    # [END howto_operator_excel_to_blob]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2224_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805587}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS Glue DataBrew data profiling workflow that demonstrates creating, executing, and cleaning up a data processing job using Airflow. Build a DAG that showcases end-to-end data preparation and profiling with automated resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.glue import GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.databrew import DataBrewHook\nfrom airflow.providers.amazon.aws.operators.glue import GlueStartJobOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.databrew import DatabrewJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nDATABREW_JOB_NAME = \"DatabrewTestJob\"\nDATABREW_INPUT_S3URI = \"s3://airflow-dev-test-dataset/word-count.txt\"\nOUTPUT_BUCKET = \"bucket-{{ ds }}-databrew\"\nOUTPUT_OBJECT = \"output.json\"\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n        description=\"Connection for Databrew example\",\n        extra=json.dumps({\"bucket\": OUTPUT_BUCKET}),\n    )\n    dbconn = glue_job_hook.get_conn()\n    try:\n        dbconn.create_connection(conn.id, role_arn=lambda_role_arn)\n        log.info(f\"Created connection {conn.id} in role {lambda_role_arn}\")\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"ResourceAlreadyExistsException\":\n            log.info(f\"Connection {conn.id} already exists in role {lambda_role_arn}\")\n        else:\n            raise\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_connection(conn_id_name):\n    lambda_role_arn = os.environ.get(ROLE_ARN_KEY)\n    dbconn = glue_job_hook.get_conn()\n    conn = dbconn.get_connections(RoleArn=lambda_role_arn, ConnectionId=conn_id_name)[0]\n    dbconn.delete_connection(connection_id=conn.id)\n    log.info(\"Deleted connection %s\", conn_id_name)\n\n\nwith DAG(\n    dag_id=DATABREW_JOB_NAME,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    lambda_role_arn = test_context[ROLE_ARN_KEY]\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=OUTPUT_BUCKET)\n\n    upload_databrew_input = S3CreateObjectOperator(\n        task_id=\"upload_databrew_input_file\",\n        s3_bucket=OUTPUT_BUCKET,\n        s3_key=DATABREW_INPUT_S3URI,\n        data={\"Input\": '{\"input': 'ab c; dd ee f'}'},\n        replace=True,\n    )\n\n    start_databrew_job = GlueStartJobOperator(\n        task_id=\"start_databrew_job\",\n        job_name=DATABREW_JOB_NAME,\n        trigger_interval=-1,\n        arguments=[\n            \"('--resources', 's3'), ('--wbRole', '%s'), (\"'inputUri='%s', outputUri='%s'\",\n            DATABREW_INPUT_S3URI, OUTPUT_OBJECT),\n        ],\n        conn_id=\"s3\",\n    )\n\n    check_databrew_job = DatabrewJobSensor(\n        task_id=\"check_databrew_job\",\n        job_name=DATABREW_JOB_NAME,\n        arn=databrew_job_hook.arn_from_job_name(role_arn=lambda_role_arn),\n        poke_interval=5,\n        timeout=6 * 60,\n    )\n\n    delete_databrew_job = GlueStartJobOperator(\n        task_id=\"delete_databrew_job\",\n        job_name=DATABREW_JOB_NAME,\n        trigger_interval=-1,\n        arguments=[],\n        conn_id=\"s3\",\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket=OUTPUT_BUCKET,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    drop_connection = create_connection(\"s3\")\n    prune_logs([create_s3_bucket, upload_databrew_input, start_databrew_job, check_databrew_job])\n    (\n        [create_s3_bucket >> upload_databrew_input >> start_databrew_job]\n        >> check_databrew_job\n        >> delete_databrew_job\n        >> create_connection(\"s3\")\n        >> prune_logs([create_s3_bucket, upload_databrew_input, start_databrew_job, check_databrew_job])\n        >> [delete_s3_bucket >> drop_connection]\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1904_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.88057}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates how to fine-tune a Google Vertex AI generative model using supervised learning techniques. The DAG will showcase how to take a base Gemini model and customize it with specific training data to improve its performance for a particular use case.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI Generative Model Tuning Tasks.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.vertex_ai.generative import CloudVertexGenHook\nfrom airflow.providers.google.cloud.operators.vertex_ai.generative import (\n    CreateGenerativeTrainingJobOperator,\n    DeleteGenerativeTrainingJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_vertex_ai_generative_model_tuning\"\n\n# [START howto_operator_create_gen_training_job]\ndef create_generation_training_job():\n    \"\"\"\n    Example function to create generation training job.\n    \"\"\"\n\n    # Cluster stack\n    cluster_stack = {\n        \"name\": f\"{PROJECT_ID}local-cluster-1\",\n        \"config\": {\n            \"machine_count\": 2,\n            \"node_type_id\": \"n1-standard-1\",\n            \"initial_node_count\": 1,\n        },\n    }\n\n    # Training stack\n    training_stack = {\n        \"name\": f\"{PROJECT_ID}training-job-1\",\n        \"dataset\": {\"image_uri\": \"\"},\n        \"model\": {\"display_name\": \"\"},\n        \"training_config\": {\n            \"stopping_criteria\": {\n                \"max_trial_runs\": 1,\n                \"min_trial_runs\": 1,\n                \"max_budget_milli_node_hours\": 1000,\n                \"min_budget_milli_node_hours\": 100,\n            },\n            \"training_script\": \"\",\n            \"python_module_args\": [],\n            \"parallelism\": 1,\n        },\n    }\n\n    response = CloudVertexGenHook(instance_id=PROJECT_ID).create_generation_training_job(body=cluster_stack, body2=training_stack)\n    return response\n\n\n# [END howto_operator_create_gen_training_job]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this_last = dag.tasks[DAG_ID].as_short_circuit()\n    # [START howto_operator_create_gen_training_job_result]\n    create_gen_training_job = CreateGenerativeTrainingJobOperator(\n        task_id=\"create_gen_training_job\",\n        body=create_generation_training_job(),\n        deferrable=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )  # [END howto_operator_create_gen_training_job_result]\n    # [START howto_operator_delete_gen_training_job]\n    delete_gen_training_job = DeleteGenerativeTrainingJobOperator(\n        task_id=\"delete_gen_training_job\",\n        training_id=\"{{ task_instance.xcom_pull('create_gen_training_job')['name'].split('/')[-1] }}\",\n        project_id=PROJECT_ID,\n        deferrable=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_delete_gen_training_job]\n\n    (\n        run_this_last &\n        create_gen_training_job\n        >> delete_gen_training_job\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2123_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8805783}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that sequentially executes SQL operations against a Trino database, demonstrating task dependencies and progressive data transformation steps. The workflow should showcase how to create schemas, generate tables, insert data, and perform template-based and parameterized queries.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using TrinoOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.operators.trino import TrinoOperator\n\ndags = [\n    # Execute static query\n    {\n        \"dag_id\": \"example_trino_operator\",\n        \"tasks\": [\n            (\n                \"run_now\"\n               ,\n                TrinoOperator(\n                    task_id=\"sql_task\",\n                    sql=\"SELECT 1\",\n                    check_date=True,\n                )\n            )\n        ]\n    },\n    # Execute static query with params\n    {\n        \"dag_id\": \"example_trino_operator_with_params\",\n        \"tasks\": [\n            (\n                \"run_now\",\n                TrinoOperator(\n                    task_id=\"sql_task\",\n                    sql=\"select * from test2 @> ?\",\n                    parameters={\"value\": ['(\"all\",)']},\n                ),\n            )\n        ],\n    },\n    # Execute dynamic query\n    {\n        \"dag_id\": \"example_trino_execute_query\",\n        \"tasks\": [\n            (\n                \"run_now\",\n                TrinoOperator(task_id=\"sql_task\", sql=\"list dirs\"),  # Command will be logged\n            )\n        ],\n    },\n    # Execute dynamic query with params\n    {\n        \"dag_id\": \"example_trino_execute_query_with_params\",\n        \"tasks\": [\n            (\n                \"run_now\",\n                TrinoOperator(\n                    task_id=\"sql_task\",\n                    sql=\"list dirs\",\n                    parameters={\"schema\": \"my_schema\"},  # Command will be logged\n                ),\n            )\n        ],\n    },\n    # Execute external query via HTTP\n    {\n        \"dag_id\": \"example_trino_http_endpoint\",\n        \"tasks\": [\n            (\n                \"run_now\",\n                TrinoOperator(\n                    task_id=\"sql_task\",\n                    http_conn_id=\"trino-http-endpoint\",\n                    sql=\"select 1\",\n                ),\n            )\n        ],\n    },\n    # Execute external query via HTTP with params\n    {\n        \"dag_id\": \"example_trino_http_endpoint_with_params\",\n        \"tasks\": [\n            (\n                \"run_now\",\n                TrinoOperator(\n                    task_id=\"sql_task\",\n                    http_conn_id=\"trino-http-endpoint\",\n                    sql=\"select 1\",\n                    parameters={\"schema\": \"my_schema\"},\n                ),\n            )\n        ],\n    },\n]\n\nwith DAGunts(dag_id=[dag[\"dag_id\"] for dag in dags], start_date=datetime(2022, 1, 1), schedule=None) as dag:\n    from tests_common.test_utils.watcher import watcher\n\n    # This is to point the watcher to the right args\n    for ix, dag_ in enumerate(dags):\n        dag_[\"tasks\"][0][\"params\"][\"task_id\"] = f\"task_{ix}\"\n\n    # TEST BODY\n    from tests_common.test_utils.system_tests import get_test_run\n\n    # Get the list of tasks from the DAG\n    test_runs = get_test_run(dag)\n    # Test the test_run\n    get_test_run(test_runs[0])\n\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = next(test_runs)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_logger\n\n# This test needs test_run.logger\nlog = get_test_run_logger(test_run)"}], "metadata": {"file_name": "dag_1824_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880589}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an ETL process that automatically ingests CSV files from an external data source into a predefined Snowflake table using Airflow's Snowflake integration. Implement a robust data movement strategy that supports flexible file parsing and consistent data loading.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SqlExecuteQueryOperator\nfrom airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.snowflake.utils.snowflake import SnowflakeExternalTableGetId\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_snowflake\"\n\nSNOWFLAKE_CONN_ID = f\"snowflake_conn_{ENV_ID}\"\nSNOWFLAKE_SAMPLE_TABLE = \"SFDX_DATA\"\nSNOWFLAKE_SCHEMA = \"TEST_SNOWFLAKE\"\nSNOWFLAKE_USER = \"TANISHWA\"\nSNOWFLAKE_PASSWORD = \"L7t8g9vF6gCZy4kK\"\nSNOWFLAKE_ACCOUNT = \"aws-cn-snowflake.ondemand.com\"\n\nconn_id = f\"{SNOWFLAKE_CONN_ID}\"\n\n\n@task\ndef create_snowflake_external_table():\n    snowflake_hook = SnowflakeHook(SNOWFLAKE_ACCOUNT)\n    conn = snowflake_hook.get_conn()\n    cursor = conn.cursor()\n    external_columns = [\n        {\"name\": \"id\", \"type\": \"VARCHAR(50)\"},\n        {\"name\": \"value\", \"type\": \"NUMBER\"},\n    ]\n    sql = f\"\"\"\n        CREATE EXTERNAL TABLE {SNOWFLAKE_SAMPLE_TABLE} (\n            id VARCHAR(50),\n            value NUMBER)\n        LOCATION 's3://bucket/sfdx/data/'\n        PARTITION BY (dt=SUBSTR(TO_CHAR(getdate()), 1, 4)||SUBSTR(TO_CHAR(getdate()), 5, 2)')\n        OPTIONS(\n            ExternalSchema '{SNOWFLAKE_SCHEMA}',\n            ExternalTableExternalColumns '{external_columns}');\n    \"\"\"\n    cursor.execute(sql)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    set_up_connection_snowflake = SnowflakeOperator(\n        task_id=\"set_up_connection_snowflake\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n        conn_id=conn_id,\n        snowflake_user=SNOWFLAKE_USER,\n        snowflake_password=SNOWFLAKE_PASSWORD,\n        snowflake_account=SNOWFLAKE_ACCOUNT,\n    )\n\n    get_id_of_external_table = SnowflakeExternalTableGetId(\n        task_id=\"get_id_of_snowflake_external_table\",\n    )\n\n    # [START howto_operator_snowflake]\n    load_data_snowflake = SqlExecuteQueryOperator(\n        task_id=\"load_data_snowflake\",\n        sql=f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES ('val_1', 1), ('val_2', 2)\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_snowflake]\n\n    drop_snowflake_external_table = SnowflakeOperator(\n        task_id=\"drop_snowflake_external_table\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n        sql=f\"DROP EXTERNAL TABLE {SNOWFLAKE_SAMPLE_TABLE}\",\n    )\n\n    drop_snowflake_schema = SnowflakeOperator(\n        task_id=\"drop_snowflake_schema\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n        sql=f\"DROP SCHEMA IF EXISTS {SNOWFLAKE_SCHEMA}\",\n    )\n\n    create_snowflake_schema = SnowflakeOperator(\n        task_id=\"create_snowflake_schema\",\n        snowflake_conn_id=SNOWFLAKE_CONN_ID,\n        sql=f\"CREATE SCHEMA {SNOWFLAKE_SCHEMA}\",\n    )\n\n    create_snowflake_external_table = create_snowflake_external_table()\n\n    join_snowflake_and_external = SqlExecuteQueryOperator(\n        task_id=\"join_snowflake_and_external\",\n        sql=f\"JOIN TBL ON TBL.id = JOIN_TBL.id\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n\n    delete_snowflake_and_external_table = SqlExecuteQueryOperator(\n        task_id=\"delete_snowflake_and_external_table\",\n        sql=f\"DELETE FROM TBL WHERE value > 1\",\n        conn_id=SNOWFLAKE_CONN_ID,\n   )\n\n    delete_snowflake_external_table = SqlExecuteQueryOperator(\n        task_id=\"delete_snowflake_external_table\",\n        sql=f\"DELETE FROM {SNOWFLAKE_SAMPLE_TABLE} WHERE dt >= SYMMETRIC_DATE\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_snowflake_schema = SqlExecuteQueryOperator(\n        task_id=\"delete_snowflake_schema\",\n        sql=f\"DELETE FROM SCHEMA WHERE name = '%\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    [set_up_connection_snowflake, create_snowflake_schema].trigger(\n        task_id=\"set_up_connections\", trigger_rule=TriggerRule.ALL_SUCCESS\n    )\n\n    [get_id_of_external_table, drop_snowflake_external_table].trigger(\n        task_id=\"get_id_of_external_table\", trigger_rule=TriggerRule.ALL_SUCCESS\n    )\n\n    [drop_snowflake_schema, create_snowflake_schema].trigger(\n        task_id=\"drop_snowflake_schema\", trigger_rule=TriggerRule.ALL_SUCCESS\n    )\n\n    [create_snowflake_external_table, join_snowflake_and_external].trigger(\n        task_id=\"create_or_run_example_tasks\", trigger_rule=TriggerRule.ALL_SUCCESS\n    )\n\n    [delete_snowflake_external_table, delete_snowflake_schema].trigger(\n        task_id=\"delete_example_tasks\", trigger_rule=TriggerRule.ALL_SUCCESS\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1796_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8806002}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates scheduling and executing a data pipeline in Microsoft Fabric, highlighting the process of triggering data transformation and movement tasks through a cloud-native orchestration mechanism.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryTriggerOperator\n\ntry:\n    from airflow.providers.microsoft.azure.translations imports.utils import get_azure_rds_db_name\nexcept ImportError:\n    # This import fails in earlier versions of the translation package. Import safety is handled by the `if True:` below.\n    pass\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_df_trigger\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_df_trigger]\n    # Example of triggering a data factory workflow from within an Airflow DAG.\n    trigger_workflow = AzureDataFactoryTriggerOperator(\n        task_id=\"trigger_wf\",\n        workflow_name=\"{{ task_instance.xcom_pull('create_wf')['results'][0]['workflowId'] }}\",\n        factory_name=\"my_data_factory\",\n        backup_details={\"database_name\": get_azure_rds_db_name()},\n    )\n    # [END howto_operator_df_trigger]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_from_test_group  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run_from_test_group(dag.group)"}], "metadata": {"file_name": "dag_1359_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880605}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases AWS service integration by creating, executing, and cleaning up a DataSync task between two dynamically generated S3 buckets. Implement a complete lifecycle management workflow that demonstrates task creation, execution, and resource cleanup using Python and AWS SDK.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3CreateObjectOperator\nfrom airflow.providers.amazon.aws.sensors.datasync import (\n    DataSyncSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_datasync_move_data_between_s3\"\n\n# The S3 bucket to which the source data moves during the test.\nBUCKET_SRC_MOVE = \"bucket-src-move-test\"\n# The folder within the source S3 bucket from which the source data moves the test.\nFOLDER_SRC_MOVE = \"src-move-data\"\n# The S3 bucket to which the destination data moves during the test.\nBUCKET_DST_MOVE = \"bucket-dst-move-test\"\n# The folder within the destination S3 bucket to which the dest data moves the test.\nFOLDER_DST_MOVE = \"dst-move-data\"\n# The location of the source s3 object in the test.\nSOURCE_S3_KEY = \"temp/airflow-test-dataset/\"\n# The location of the dest s3 object in the test.\nDESTINATION_S3_KEY = \"temp/airflow-test-dataset/\"\n# The name of the source asset.\nASSET_NAME_SRC = \"Asset1\"\n# The name of the destination asset.\nASSET_NAME_DST = \"Asset2\"\n# The id of the group.\nGROUP_ID = \"{{ ts_context.group_id }}\"\n# The id of the task.\nTASK_ID = \"{{ ts_context.task_id }}\"\n\n\n@task\ndef create_source_asset_group():\n    log.info(\"Creating source asset group\")\n    return {\n        \"assetGroupDefinition\": {\n            \"assetGroups\": [\n                {\n                    \"gid\": GROUP_ID,\n                    \"sourceAssets\": [\n                        {\n                            \"key\": SOURCE_S3_KEY,\n                            \"prefix\": FOLDER_SRC_MOVE,\n                            \"endOfPrefixWildCard\": True,\n                            \"type\": \"S3\",\n                        }\n                    ],\n                    \"destinations\": [],\n                    \"parallelProcessingMode\": \"NONE\",\n                }\n            ]\n        }\n    }\n\n\n@task\ndef create_destination_asset_group():\n    log.info(\"Creating destination asset group\")\n    return {\n        \"assetGroupDefinition\": {\n            \"assetGroups\": [\n                {\n                    \"gid\": GROUP_ID,\n                    \"sourceAssets\": [],\n                    \"destinations\": [\n                        {\n                            \"key\": DESTINATION_S3_KEY,\n                            \"prefix\": FOLDER_DST_MOVE,\n                            \"endOfPrefixWildCard\": True,\n                            \"type\": \"S3\",\n                        }\n                    ],\n                    \"parallelProcessingMode\": \"NONE\",\n                    \"destinationAssetProperties\": {\n                        \"assetType\": \"S3\",\n                        \"quantity\": 1,\n                        \"uniqueIdPrefix\": FOLDER_DST_MOVE,\n                    },\n                }\n            ]\n        }\n    ]\n\n\n@task\ndef create_move_operator():\n    log.info(\"Creating move operator\")\n    return {\n        \"moveOperationDefinition\": {\n            \"sourceDestinationMoveOperations\": [\n                {\n                    \"sourceAssetGroupId\": GROUP_ID,\n                    \"sourceAssetKey\": SOURCE_S3_KEY,\n                    \"sourceAssetPrefix\": FOLDER_SRC_MOVE,\n                    \"sourceAssetEndOfPrefixWildCard\": True,\n                    \"destinationAssetGroupId\": GROUP_ID,\n                    \"destinationAssetKey\": DESTINATION_S3_KEY,\n                    \"destinationAssetPrefix\": FOLDER_DST_MOVE,\n                    \"destinationAssetEndOfPrefixWildCard\": True,\n                }\n            ]\n        }\n    }\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_source_asset_group(asset_group_id: str):\n    log.info(\"Deleting source asset group\")\n    client = boto3.client(\"datasync\")\n    client.delete_asset(Age=asset_group_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_destination_asset_group(asset_group_id: str):\n    log.info(\"Deleting destination asset group\")\n    client = boto3.client(\"datasync\")\n    client.delete_asset(Age=asset_group_id)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_move_operator(asset_group_id: str, operation_id: str):\n    log.info(\"Deleting move operator\")\n    client = boto3.client(\"datasync\")\n    client.delete_move_operation(\n        Age=asset_group_id,\n        OperationId=operation_id,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    si_bucket_name = f\"{env_id}-si-bucket\"\n    ti_bucket_name = f\"{env_id}-ti-bucket\"\n    si_object_key = f\"{env_id}/airflow/test/dataset.csv\"\n    ti_object_key = f\"{env_id}/airflow/test/dataset.csv\"\n\n    create_si_bucket = S3CreateBucketOperator(task_id=\"create_si_bucket\", bucket_name=si_bucket_name)\n\n    create_ti_bucket = S3CreateBucketOperator(task_id=\"create_ti_bucket\", bucket_name=ti_bucket_name)\n\n    create_source_asset = S3CreateObjectOperator(\n        task_id=\"create_source_asset\",\n        s3_bucket=si_bucket_name,\n        s3_key=si_object_key,\n        data={\"Content\": \"test.csv},\n        replace=True,\n    )\n\n    create_destination_asset = S3CreateObjectOperator(\n        task_id=\"create_destination_asset\",\n        s3_bucket=ti_bucket_name,\n        s3_key=ti_object_key,\n        data={\"Content\": \"test.csv\"},\n        replace=True,\n    )\n\n    # [START howto_sensor_datasync]\n    wait_for_move_to_occur = DataSyncSensor(\n        task_id=\"wait_for_move_to_occur\",\n        datasync_id=\"{{ ts_context.datasync_id }}\",\n        deferrable=True,\n    )\n    # [END howto_sensor_datasync]\n\n    # [START howto_operator_datasync]\n    move_source_to_destination_asset = DataSyncOperator(\n        task_id=\"move_source_to_destination_asset\",\n        datasync_id=\"{{ ts_context.datasync_id}}\",\n        input_assets=[\"{{ ts_context.source_asset_id }}\"],\n        output_assets=[\"{{ ts_context.destination_asset_id }}\"],\n    )\n    # [END howto_operator_datasync]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_si_bucket,\n        create_ti_bucket,\n        create_source_asset,\n        create_destination_asset,\n        # TEST BODY\n        wait_for_move_to_occur,\n        move_source_to_destination_asset,\n        # TEST TEARDOWN\n        delete_source_asset_group(ts_context.source_asset_id),\n        delete_destination_asset_group(ts_context.destination_asset_id),\n        delete_move_operator(ts_context.datasync_id),\n    )\n\n    if ENV_ID == \"test\":\n        @task\n        def clear_test_env():\n            log.info(\"Cleaning test environment\")\n            delete_si_bucket = S3CreateBucketOperator(\n                task_id=\"delete_si_bucket\", bucket_name=si_bucket_name, trigger_rule=TriggerRule.ALL_DONE\n            )\n            delete_ti_bucket = S3CreateBucketOperator(\n                task_id=\"delete_ti_bucket\", bucket_name=ti_bucket_name, trigger_rule=TriggerRule.ALL_DONE\n            )\n            delete_si_bucket()\n            delete_ti_bucket()\n\n        get_test_id = lambda: ENV_ID\n        clear_test_env_task = clear_test_env(get_test_id)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_59_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8806245}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates file transfer techniques between cloud storage buckets with explicit error handling and resource management. Construct a workflow that shows how to programmatically move files, manage cloud storage resources, and ensure reliable data transfer across different storage locations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.file_transfer import SFTPServerFileTransferOperator\nfrom airflow.providers.sftp.transfers.copy_file import CopySFTPFileTransferrer\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_copy_file\"\n\nconn = Connection(\n    conn_id=\"sftp_default\",\n    dbapi_args=\"--login\u53e3\u4ee4mypassword --hostname 192.168.1.157 --port 22 -- PWD /home/user/\",\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_tmp_sftp_folder():\n        conn.create()\n        return True\n\n    create_tmp_sftp_folder_task = create_tmp_sftp_folder()\n\n    sftp_transfer_task = CopySFTPFileTransferrer(source_path=\"/tmp/src\", destination_path=\"/tmp/dest\", app=conn)\n    sftp_transfer_task()\n\n    remove_tmp_sftp_folder_task = lambda: conn.delete()\n    remove_tmp_sftp_folder_task()\n\n    (\n        # TEST SETUP\n        create_tmp_sftp_folder_task\n        >> sftp_transfer_task\n        >> remove_tmp_sftp_folder_task\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1684_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.88063}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure automation DAG that simulates a scalable cloud computing task execution environment. Create a workflow that programmatically provisions a compute resource, executes predefined remote commands, and then systematically tears down the infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\n# [START import_py]\nfrom typing import Any\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.ec2 import Ec2Hook\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    Ec2CreateInstanceOperator,\n    Ec2DeleteInstanceOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ec2 import Ec2Sensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import weekday\nfrom airflow.utils.trigger_rules import FinalTaskTriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, get_test_run\n\nlogger = logging.getLogger(__name__)\n\nDAG_ID = \"example_ec2\"\n\nsys_test_context_task = SystemTestContextBuilder().add_env_variable(\n    ENV_ID_KEY, str).build()\n\n# Example of a simple EC2 DAG with the new Django sensor.\ndef system_test_before_run():\n    if weekday(sys_test_context_task) not in [weekday.Monday]:\n        logger.warning(\"No tests scheduled for the weekend.\")\n        return False\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    # [START howto_operator_create_instance]\n    @task\n    def create_instance(key_name: str):\n        # Creating instance takes up to 5 minutes to complete. So we use an\n        # ephemeral sensor to check its state. When it's state becomes equal\n        # than the value after 'act' it become true and the sensor operation\n        # ends.\n        # The idea is to avoid to keep the connection to the service while\n        # waiting the instance creation to finish.\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        waiter_config = {\n            \"Delay\": 20,\n            \"MaxAttempts\": 50,\n        }\n        # Status of instance must be 'pending' or 'running' for the sensor to\n        # return True.\n        paginator = client.get_paginator(\"describe_instances\")\n        for page in paginator:\n            for reservation in page[\"Reservations\"]:\n                for instance in reservation[\"Instances\"]:\n                    if instance[\"State\"][\"Name\"] in [\"pending\", \"running\"]:\n                        return instance[\"InstanceId\"]\n        raise Exception(\"Instance does not exist or not in correct state.\")\n\n    # [END howto_operator_create_instance]\n    create_instance_instanceId = create_instance(key_name=\"my-key\")\n\n    @task\n    def launch_template_assign_ami(id_: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        response = client.create_launch_template(\n            LaunchTemplateName=f\"launch-template-{env_id}\",\n            Description=\"My launch template\",\n            BlockDeviceMappings=[\n                {\n                    \"DeviceName\": \"/dev/xvda\",\n                    \"Ebs\": {\n                        \"Encrypted\": True,\n                        \"SnapshotId\": \"snap-12345678\",\n                        \"VolumeSize\": 8,\n                        \"VolumeType\": \"gp2\",\n                    },\n                }\n            ],\n            Configurations=[\n                {\n                    \"Name\": \"instance-type\",\n                    \"Value\": \"t4g.micro\",\n                },\n                {\n                    \"Name\": \"keyboard-latin-qwertz\",\n                    \"Value\": \"true\",\n                },\n            ],\n            MinCount=1,\n            MaxCount=1,\n        )\n        launch_template_id = client.describe_launch_templates(LaunchTemplateName=f\"launch-template-{env_id}\")[\n            \"LaunchTemplates\"\n        ][0][\"LaunchTemplateId\"]\n        client.reboot_instances(\n            InstanceIds=[id_],\n            LaunchTemplate={\n                \"LaunchTemplateName\": f\"launch-template-{env_id}\",\n                \"Version\": client.describe_launch_templates(LaunchTemplateName=f\"launch-template-{env_id}\")[\n                    \"LaunchTemplates\"\n                ][0][\n                    \"Version\"\n                ],\n            },\n        )\n\n    launch_template_assign_ami = launch_template_assign_ami(id_=create_instance_instanceId.output)\n\n    @task\n    def get_ip(instance_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        try:\n            ec2_response = client.describe_instances(InstanceIds=[instance_id])\n            instances = ec2_response[\"Reservations\"][0][\"Instances\"]\n            for instance in instances:\n                for tag in instance[\"Tags\"]:\n                    if tag[\"Key\"] == \"Name\":\n                        instance_name = tag[\"Value\"]\n                private_dns = instance[\"NetworkInterfaces\"][0][\"PrivateDnsName\"]\n                private_ip = instance[\"NetworkInterfaces\"][0][\"SecondaryPrivateIpAddress\"][0]\n                return {\"instance_name\": instance_name, \"private_dns\": private_dns, \"private_ip\": private_ip}\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"InstanceNotExistsException\":\n                logger.info(\"Instance doesn't exist: \", instance_id)\n            else:\n                raise e\n\n    get_ip_ip = get_ip(instance_id=create_instance_instanceId.output)\n\n    @task\n    def attach_iptex__(instance_id: str, private_dns: str, private_ip: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        client.attach_interconnects(\n            InterconnectId=\"{{ var.value='interconnect_id') }}\",\n            NetworkInterfaceId=\"{{ var.value='instance_iid') }}\",\n        )\n        client.modify_vpc_interconnect_channel(\n            VpcInterconnectChannelId=\"{{ var.value='vpc_interconnect_channel') }}\",\n            InterconnectId=\"{{ var.value='interconnect_id') }}\",\n        )\n        client.set_network_interface_attribute(\n            NetworkInterfaceId=instance_id,\n            PrivateDnsName=private_dns,\n            PrivateIpAddresses=[\n                {'PrivateIpAddress': private_ip},\n            ],\n        )\n\n    attach_iptex__ = attach_iptex__(instance_id=create_instance_instanceId.output, private_dns=get_ip_ip.output[\"private_dns\"], private_ip=get_ip_ip.output[\"private_ip\"])\n\n    @task\n    def configure_security_group(security_group_name: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        security_group_kwargs = {\n            \"GroupName\": security_group_name,\n            \"Description\": \"New security group for testing purposes\",\n            \"SecurityGroupRules\": [\n                {\"FromPort\": 22, \"IpProtocol\": \"TCP\", \"IpRanges\": [\"0.0.0.0/0\"], \"Priority\": 100},\n                {\n                    \"FromPort\": 0,\n                    \"IpProtocol\": \"All\",\n                    \"IpRanges\": [\"0.0.0.0/0\"],\n                    \"Priority\": 90,\n                },\n            ],\n        }\n        client.create_security_group(**security_group_kwargs)\n        client.authorize_security_group_ingress(\n            GroupId=security_group_kwargs[\"GroupId\"], IpPermissions=[{\"FromPort\": 22, \"IpProtocol\": \"TCP\"}]\n        )\n\n    configure_security_group = configure_security_group(security_group_name=\"test-security-group\")\n\n    @task\n    def wait_for_instance_to_be_restarted(instance_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        waiter_name = \"reboot_instance_waiter\"\n        waiter_args = {\n            \"WaiterConfig\": {\n                \"Delay\": 20,\n                \"MaxAttempts\": 50,\n            }\n        }\n        client.get_waiters(waiter_name)[waiter_name].wait(\n            InstanceIds=[instance_id], WaiterConfig=waiter_args\n        )\n\n    wait_for_instance_to_be_restarted = wait_for_instance_to_be_restarted(instance_id=create_instance_instanceId.output)\n\n    @task\n    def get_ami_id(instance_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        try:\n            ec2_response = client.describe_instances(InstanceIds=[instance_id])\n            instances = ec2_response[\"Reservations\"][0][\"Instances\"]\n            for instance in instances:\n                for tag in instance[\"Tags\"]:\n                    if tag[\"Key\"] == \"Name\":\n                        instance_name = tag[\"Value\"]\n                ami_id = instance[\"ImageId\"]\n                return ami_id\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"InstanceNotExistsException\":\n                logger.info(\"Instance doesn't exist: \", instance_id)\n            else:\n                raise e\n\n    get_ami_id = get_ami_id(instance_id=create_instance_instanceId.output)\n\n    @task\n    def create_interconnect(vpc_id: str, interconnect_type: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        request_body = {\n            \"VpcId\": vpc_id,\n            \"InterconnectType\": interconnect_type,\n            \"BandwidthThrottle\": {\"Enabled\": False},\n        }\n        response = client.create_interconnect(request_body)\n        return response[\"Interconnects\"][0][\"InterconnectId\"]\n\n    create_interconnect = create_interconnect(vpc_id=\"{{ var.value='vpc_id') }}\", interconnect_type=\"5GB\")\n\n    @task\n    def delete_interconnect(interconnect_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        client.delete_interconnect(InterconnectId=interconnect_id)\n\n    delete_interconnect = delete_interconnect(interconnect_id=create_interconnect.output)\n\n    @task\n    def get_vpc_interconnect_channel(vpc_interconnect_channel_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        response = client.describe_vpc_interconnect_channels(\n            VpcInterconnectChannelId=vpc_interconnect_channel_id,\n        )\n        return response[\"VpcInterconnectChannels\"][0][\"VpcInterconnectChannelId\"]\n\n    get_vpc_interconnect_channel = get_vpc_interconnect_channel(vpc_interconnect_channel_id=\"{{ var.value='vpc_interconnect_channel') }}\")\n\n    @task\n    def create_launch_template_configuration(launch_template_name: str, instance_type: str, min_count: int, max_count: int):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        response = client.create_launch_template_configuration(\n            LaunchTemplateName=launch_template_name,\n            InstanceType=instance_type,\n            MinCount=min_count,\n            MaxCount=max_count,\n        )\n        return response[\"LaunchTemplateConfiguration\"][\"LaunchTemplateId\"]\n\n    create_launch_template_configuration = create_launch_template_configuration(\n        launch_template_name=f\"launch-template-config-{env_id}\", instance_type=\"t4g.micro\", min_count=1, max_count=1\n    )\n\n    @task\n    def modify_launch_template(template_id: str, ami_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        response = client.modify_launch_template(\n            LaunchTemplateId=template_id,\n            Modifications=[\n                {\n                    \"Action\": \"REPLACE_COMPONENTS\",\n                    \"Components\": [\n                        {\n                            \"ComponentId\": f\"{ami_id}:ami-id\",\n                            \"NewComponent\": {\n                                \"Type\": \"FROM_IMAGE\",\n                            },\n                        }\n                    ],\n                }\n            ],\n        )\n\n    modify_launch_template = modify_launch_template(template_id=create_launch_template_configuration.output, ami_id=get_ami_id.output)\n\n    @task\n    def get_instance_ids_from_launch_template(template_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        response = client.describe_launch_templates(LaunchTemplateId=template_id)\n        return response[\"LaunchTemplates\"][0][\"Instances\"][0][\"InstanceId\"]\n\n    get_instance_ids_from_launch_template = get_instance_ids_from_launch_template(template_id=create_launch_template_configuration.output)\n\n    @task\n    def delete_instance(instance_id: str):\n        client = Ec2Hook().get_boto3_client(\"ec2\")\n        client.terminate_instance(InstanceIdentifier=instance_id)\n\n    delete_instance = delete_instance(instance_id=create_instance_instanceId.output)\n\n    @task(trigger_rule=FinalTaskTriggerRule())\n    def clean_logs(log_group_name: str):\n        client = boto3.client(\"logs\")\n        filters = {\n            \"FilterName\": \"log-group\",\n            \"Filters\": [\n                {\n                    \"FilterPattern\": log_group_name,\n                    \"Combiner\": \"AND\",\n                }\n            ],\n        }\n        client.delete_log_group(**filters)\n\n    clean_logs = clean_logs(log_group_name=\"{{ var.value='log_group_name') }}\")\n\n    chain(\n        test_context,\n        create_instance_instanceId,\n        launch_template_assign_ami,\n        get_ip_ip,\n        attach_iptex__,\n        configure_security_group(env_id),\n        wait_for_instance_to_be_restarted(create_instance_instanceId.output),\n        get_ami_id,\n        create_interconnect,\n        delete_interconnect(create_interconnect.output),\n        get_vpc_interconnect_channel(create_interconnect.output),\n        create_launch_template_configuration,\n        modify_launch_template(create_launch_template_configuration.output, get_ami_id.output),\n        get_instance_ids_from_launch_template(create_launch_template_configuration.output),\n        delete_instance(create_instance_instanceId.output),\n        clean_logs('{{ var.value='log_group_name') }),\n    )\n    # [END howto_operator_create_instance]\n\n    # [START howto_sensor_ec2]\n    wait_for_instance = Ec2Sensor(\n        task_id=\"wait_for_instance\",\n        region_name=\"us-east-1\",\n        db_instance_id=\"{{ var.env.db_instance_id }}\",\n        poke_interval=5,\n    )\n    # [END howto_sensor_ec2]\n\n    chain(\n        test_context,\n        create_instance_instanceId,\n        wait_for_instance,\n        get_ip_ip,\n        attach_iptex__,\n        configure_security_group(env_id),\n        wait_for_instance_to_be_restarted(create_instance_instanceId.output),\n        get_ami_id,\n        create_interconnect,\n        delete_interconnect(create_interconnect.output),\n        get_vpc_interconnect_channel(create_interconnect.output),\n        create_launch_template_configuration,\n        modify_launch_template(create_launch_template_configuration.output, get_ami_id.output),\n        get_instance_ids_from_launch_template(create_launch_template_configuration.output),\n        delete_instance(create_instance_instanceId.output),\n        clean_logs('{{ var.value='log_group_name') }),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_172_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8806658}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates interaction with InfluxDB using the InfluxDBOperator, showing how to connect to a time-series database and execute a Flux query. Develop a task that retrieves data from a predefined bucket with a configurable time window.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\n\n\n@task\ndef create_influxdb_bucket(influxdb_hook, bucket_name):\n    influxdb_hook.create_bucket(bucket_name)\n\n\n@task\ndef write_to_influxdb(influxdb_hook, measurement, values, bucket_name):\n    influxdb_hook.write_points(\n        data=[{'time': v[0], 'measurement': measurement, 'values': v[1]}]\n        for v in values\n    ),\n        bucket=bucket_name,\n        time_precision='ms',\n    )\n\n\n@task\ndef delete_influxdb_bucket(influxdb_hook, bucket_name):\n    influxdb_hook.delete_bucket(bucket_name)\n\n\nwith DAG(\n    dag_id=\"example_influxdb_operator\",\n    start_date=datetime(2022, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_influxdb_write]\n    influxdb_hook = InfluxDBHook()\n\n    bucket_name = \"test-new-bucket\"\n    create_influxdb_bucket(influxdb_hook, bucket_name)\n\n    measurement = \"test-write\"\n    values = [\n        ('2021-09-03T15:00:00Z', measurement, [3.4, 1.2, 4.5]),\n        ('2021-09-03T15:00:10Z', measurement, [6.7, 3.3, 5.5]),\n        ('2021-09-03T15:00:20Z', measurement, [2.4, 1.2, 3.5]),\n    ]\n\n    write_to_influxdb(influxdb_hook, measurement, values, bucket_name)\n    # [END howto_operator_influxdb_write]\n\n    (create_query := influxdb_hook.build_and_execute_query(f\"select * from {bucket_name}.all\")) >> print()\n    (delete_influxdb_bucket(influxdb_hook, bucket_name)) >> print()\n\n    from tests.system.utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_612_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880671}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need help with a specific calculation in Python. I want to determine the interest accumulated on a bank account over a period of years, assuming a fixed annual interest rate and compound interest is applied. The account starts with an initial deposit. The formula for compound interest is A = P(1 + r/n)^(nt), where A is the amount of money accumulated after n years, including interest, P is the principal amount (initial deposit), r is the annual interest rate (decimal), n is the number of times that interest is compounded per year, and t is the time the money is invested for in years. Can you help me implement this in Python?"}, {"role": "assistant", "content": "def calculate_compound_interest(P, r, n, t):\n    \"\"\"\n    Calculate the accumulated amount in a bank account using compound interest.\n    \n    Parameters:\n    - P: Initial deposit (float)\n    - r: Annual interest rate as a decimal (e.g., 0.05 for 5%)\n    - n: Number of times interest is compounded per year (integer)\n    - t: Time in years (integer or float)\n    \n    Returns:\n    - Accumulated amount (float)\n    \"\"\"\n    # Ensure t is a positive number\n    if t <= 0:\n        raise ValueError(\"Time must be a positive number.\")\n    \n    # Calculate the accumulated amount\n    A = P * (1 + r / n)**(n * t)\n    return A\n\n# Example usage:\ninitial_deposit = 1000\nannual_rate = 0.04  # 4%\ncompounded_per_year = 12\ninvestment_duration = 3\n\naccumulated_amount = calculate_compound_interest(initial_deposit, annual_rate, compounded_per_year, investment_duration)\nprint(f\"Accumulated Amount: ${accumulated_amount:.2f}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880681}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the difference between a tuple and a list in Python?"}, {"role": "assistant", "content": "# Creating a list\nmy_list = [1, 2, 3]\nprint(\"Original List:\", my_list)\n\n# Adding an element to the list\nmy_list.append(4)\nprint(\"Updated List:\", my_list)\n\n# Trying to modify an element (will raise TypeError)\ntry:\n    my_list[0] = 5\nexcept TypeError as e:\n    print(e)\n\n# Creating a tuple\nmy_tuple = (1, 2, 3)\nprint(\"\\nOriginal Tuple:\", my_tuple)\n\n# Attempting to modify a tuple (will raise AttributeError)\ntry:\n    my_tuple[0] = 5\nexcept AttributeError as e:\n    print(e)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8806956}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I obtain the A10 Networks AXAPI version 2.1 using the python-axapi library?"}, {"role": "assistant", "content": "from axapi import AxapiClient\n\ndef get_axapi_version():\n    # Create an instance of the AxapiClient class\n    client = AxapiClient()\n\n    try:\n        # Call the method to get the AXAPI version\n        response = client.get_axapi_version()\n        \n        # Print the retrieved AXAPI version\n        print(\"AXAPI Version:\", response.version)\n    \n    except Exception as e:\n        # Handle any exceptions that occur during the request\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nget_axapi_version()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807037}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow monitoring DAG that validates the completion status of a specific cloud task queue in a Google Cloud environment. Implement a sensor task that pauses workflow execution until the designated queue is completely empty.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Tasks.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.tasks_v1 import CloudTasksHook\nfrom airflow.providers.google.cloud.operators.tasks_v1 import (\n    CloudTasksCreateTaskQueueOperator,\n    CloudTasksDeleteTaskQueueOperator,\n    CloudTasksGetTaskQueueOperator,\n    CloudTasksListTasksOperator,\n    CloudTasksRunTaskOperator,\n    CloudTasksUpdateTaskQueueOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_tasks\"\n\nLOCATION = \"europe-west1\"\nQUEUE_NAME = f\"queue-{DAG_ID}-{ENV_ID}\"\n\nTASK = {\n    \"http_request\": {\"http_method\": \"POST\"},\n    \"condition\": {\"path_matchers\": [\"/tasks/v1/queue/{queue_name}\"]},\n}\n# [START howto_cloud_tasks_queue]\nCREATE_TASK_QUEUE = CloudTasksCreateTaskQueueOperator(\n    task_id=\"create_task_queue\",\n    location=LOCATION,\n    queue_id=QUEUE_NAME,\n    project_id=PROJECT_ID,\n)\n# [END howto_cloud_tasks_queue]\n\n# [START howto_cloud_tasks_get_task_queue]\nGET_TASK_QUEUE = CloudTasksGetTaskQueueOperator(\n    task_id=\"get_task_queue\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    queue_id=QUEUE_NAME,\n)\n# [END howto_cloud_tasks_get_task_queue]\n\n# [START howto_cloud_tasks_list_tasks]\nLIST_TASKS = CloudTasksListTasksOperator(\n    task_id=\"list_tasks\", project_id=PROJECT_ID, location=LOCATION, queue_id=QUEUE_NAME\n)\n# [END howto_cloud_tasks_list_tasks]\n\n# [START howto_cloud_tasks_run_task]\nRUN_TASK = CloudTasksRunTaskOperator(\n    task_id=\"run_task\",\n    request_body=TASK,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    queue_id=QUEUE_NAME,\n    body={\"request\": TASK},\n)\n# [END howto_cloud_tasks_run_task]\n\n# [START howto_cloud_tasks_update_task_queue]\nUPDATE_TASK_QUEUE = CloudTasksUpdateTaskQueueOperator(\n    task_id=\"update_task_queue\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    queue_id=QUEUE_NAME,\n    request_body={\"status\": \"OPERATIONAL\"},\n)\n# [END howto_cloud_tasks_update_task_queue]\n\n# [START howto_cloud_tasks_delete_task]\nDELETE_TASK_QUEUE = CloudTasksDeleteTaskQueueOperator(\n    task_id=\"delete_task_queue\",\n    location=LOCATION,\n    queue_id=QUEUE_NAME,\n    trigger_rule=TriggerRule.ALL_DONE,\n)\n# [END howto_cloud_tasks_delete_task]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"tasks\"],\n) as dag:\n    # [START provide_default_service_acc]\n    default_service_acc = PROJECT_ID if ENV_ID == \"\" else ENV_ID\n    # [END provide_default_service_acc]\n\n    hook = CloudTasksHook(service_acc=default_service_acc)\n\n    # [START howto_cloud_tasks_create_task_queue_success]\n    create_task_queue_success = CloudTasksCreateTaskQueueOperator(\n        task_id=\"create_task_queue_success\",\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_cloud_tasks_create_task_queue_success]\n\n    # [START howto_cloud_tasks_get_task_queue_success]\n    get_task_queue_success = CloudTasksGetTaskQueueOperator(\n        task_id=\"get_task_queue_success\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n    )\n    # [END howto_cloud_tasks_get_task_queue_success]\n\n    # [START howto_cloud_tasks_list_tasks_success]\n    list_tasks_success = CloudTasksListTasksOperator(\n        task_id=\"list_tasks_success\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n    )\n    # [END howto_cloud_tasks_list_tasks_success]\n\n    # [START howto_cloud_tasks_run_task_success]\n    run_task_success = CloudTasksRunTaskOperator(\n        task_id=\"run_task_success\",\n        request_body=TASK,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n        body={\"request\": TASK},\n    )\n    # [END howto_cloud_tasks_run_task_success]\n\n    # [START howto_cloud_tasks_update_task_queue_success]\n    update_task_queue_success = CloudTasksUpdateTaskQueueOperator(\n        task_id=\"update_task_queue_success\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n        request_body={\"status\": \"OPERATIONAL\"},\n    )\n    # [END howto_cloud_tasks_update_task_queue_success]\n\n    # [START howto_cloud_tasks_delete_task_success]\n    delete_task_queue_success = CloudTasksDeleteTaskQueueOperator(\n        task_id=\"delete_task_queue_success\",\n        location=LOCATION,\n        queue_id=QUEUE_NAME,\n    )\n    # [END howto_cloud_tasks_delete_task_success]\n\n    (\n        # TEST SETUP\n        hook.create_conn()\n        >> create_task_queue_success\n        >> get_task_queue_success\n        >> list_tasks_success\n        >> run_task_success\n        # TEST BODY\n        >> update_task_queue_success\n        >> delete_task_queue_success\n        # TEST TEARDOWN\n        >> hook.delete_conn()\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done and last one.\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_699_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880716}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates remote task scheduling and execution using WinRM hooks, demonstrating how to connect to and run commands on Windows machines programmatically. Develop a parallel execution pattern that shows different command types running concurrently with a final consolidation task.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport httpx\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_winrm_operator():\n    @task()\n    def create_remote_command():\n        # [START howto_operator_winrm]\n        hook = WinRMHook(conn_id=\"winrm_default\")\n        ssh_client = httpx.Client(timeout=5)\n        ssh_client.post(\"http://localhost/command\", json={\"command\": \"mkdir test\"})\n        # [END howto_operator_winrm]\n\n    @task()\n    def list_remote_commands():\n        # [START howto_operator_winrm_get]\n        hook = WinRMHook(conn_id=\"winrm_default\")\n        result = hook.get_command(async_client=ssh_client, command=\"ls test\")\n        print(result)\n        # [END howto_operator_winrm_get]\n\n    @task()\n    def remove_remote_command():\n        # [START howto_operator_winrm_remove]\n        hook = WinRMHook(conn_id=\"winrm_default\")\n        ssh_client = httpx.Client(timeout=5)\n        ssh_client.post(\"http://localhost/command\", json={\"command\": \"rmdir test\"})\n        # [END howto_operator_winrm_remove]\n\n    create_task = create_remote_command()\n    list_task = list_remote_commands()\n    remove_task = remove_remote_command()\n\n    create_task >> list_task >> remove_task\n\n\nexample_winrm_operator()\n\n\n# [START howto_operator_winrm_external]\n@task()\ndef execute_external_cmd():\n    # [START howto_operator_winrm_external]\n    import subprocess\n\n    cmd = [\"echo\", \"Hello World\"]\n    p = subprocess.Popen(cmd)\n    p.wait()\n    # [END howto_operator_winrm_external]\n\n\nexecute_external_cmd()\n# [END howto_operator_winrm_external]\n\n\n# [START howto_operator_winrm_file_transfer]\n@task(path=\"/tmp/sending.txt\")\ndef create_temp_file():\n    content = \"This is some unexciting content\"\n    with open(\"/tmp/sending.txt\", \"w\") as f:\n        f.write(content)\n\n\n# [END howto_operator_winrm_file_transfer]\n\n# [START howto_operator_winrm_transfer_file]\n@task(path=\"/tmp/receiving.txt\", external_files=True)\ndef receive_temp_file(temp_path):\n    with open(\"/tmp/receiving.txt\", \"w\") as f:\n        f.write(open(temp_path.path).read())\n\n\nreceive_temp_file(create_temp_file())\n# [END howto_operator_winrm_transfer_file]\n\n\n# [START howto_operator_winrm_multiple_concurrent]\n@task()\ndef multiple_concurrent_procs():\n    import time\n\n    start = time.time()\n    for i in range(100):\n        print(i)\n        time.sleep(0.1)\n    end = time.time()\n    print(f\"Total time was {end - start}\")\n\n\n# [END howto_operator_winrm_multiple_concurrent]\n\nmultiple_concurrent_procs()\n\n\n# [START howto_operator_winrm_open_browser]\n@task()\ndef open_browser(url):\n    # !start chrome \"{url}\"\n    pass\n\n\n# [END howto_operator_winrm_open_browser]\n\n\n# [START howto_operator_winrm_create_shortcut]\n@task(path=\"/tmp/test.lnk\")\ndef create_shortcut(target, name):\n    import pywintypes\n    import win32com.client\n\n    shell = win32com.client.Dispatch(\"WScript.Shell\")\n    shortcut = shell.CreateShortCut(\"/tmp/test.lnk\")\n    shortcut.TargetPath = target\n    shortcut.Workspace = \"\"\n    shortcut.Arguments = \"\"\n    shortcut.Save()\n    shortcut.Name = name\n\n\n# [END howto_operator_winrm_create_shortcut]\n\n\ndefault_pool_name = \"winrm_default\"\n\n\nexample_winrm_operator()"}], "metadata": {"file_name": "dag_1761_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807313}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases the complete lifecycle management of containerized jobs using Google Cloud Run operators. Develop a systematic workflow that demonstrates job creation, execution, modification, and cleanup with robust error checking and cross-task dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Run service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.cloud_run import (\n    CloudRunCreateJobOperator,\n    CloudRunDeleteJobOperator,\n    CloudRunExecuteJobOperator,\n    CloudRunGetJobOperator,\n    CloudRunListJobsOperator,\n    CloudRunRemoveStageOperator,\n    CloudRunSetStageOperator,\n    CloudRunUpsertJobOperator,\n    CloudRunUpdateJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID= \"example_gcp_cloudrun\"\n\nJOB_NAME = f\"job-{DAG_ID}-{ENV_ID}\"\nSTAGE_NAME = \"dev\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcp_create_job]\n    create_job = CloudRunCreateJobOperator(\n        task_id=\"create_job\",\n        project_id=PROJECT_ID,\n        image=\"gcr.io/cloud-run/testing/test-job\",\n        name=JOB_NAME,\n        stages=[\n            {\"name\": STAGE_NAME, \"image\": \"gcr.io/cloud-run/testing/test-job-stage\"},\n        ],\n    )\n    # [END howto_operator_gcp_create_job]\n\n    # [START howto_operator_gcp_execute_job]\n    execute_job = CloudRunExecuteJobOperator(task_id=\"execute_job\", project_id=PROJECT_ID, name=JOB_NAME)\n    # [END howto_operator_gcp_execute_job]\n\n    # [START howto_operator_gcp_get_job]\n    get_job = CloudRunGetJobOperator(task_id=\"get_job\", project_id=PROJECT_ID, name=JOB_NAME)\n    # [END howto_operator_gcp_get_job]\n\n    # [START howto_operator_gcp_list_jobs]\n    list_jobs = CloudRunListJobsOperator(task_id=\"list_jobs\", project_id=PROJECT_ID)\n    # [END howto_operator_gcp_list_jobs]\n\n    # [START howto_operator_gcp_remove_stage]\n    remove_stage = CloudRunRemoveStageOperator(\n        task_id=\"remove_stage\",\n        project_id=PROJECT_ID,\n        name=JOB_NAME,\n        stage_name=STAGE_NAME,\n    )\n    # [END howto_operator_gcp_remove_stage]\n\n    # [START howto_operator_gcp_set_stage]\n    set_stage = CloudRunSetStageOperator(\n        task_id=\"set_stage\",\n        project_id=PROJECT_ID,\n        name=JOB_NAME,\n        stage_name=STAGE_NAME,\n    )\n    # [END howto_operator_gcp_set_stage]\n\n    # [START howto_operator_gcp_upsert_job]\n    upsert_job = CloudRunUpsertJobOperator(\n        task_id=\"upsert_job\",\n        project_id=PROJECT_ID,\n        body={\n            \"name\": JOB_NAME,\n            \"image\": \"gcr.io/cloud-run/testing/test-job\",\n            \"stages\": [\n                {\"name\": STAGE_NAME, \"image\": \"gcr.io/cloud-run/testing/test-job-stage\"},\n            ],\n        },\n    )\n    # [END howto_operator_gcp_upsert_job]\n\n    # [START howto_operator_gcp_update_job]\n    update_job = CloudRunUpdateJobOperator(\n        task_id=\"update_job\",\n        project_id=PROJECT_ID,\n        name=JOB_NAME,\n        body={\"image\": \"gcr.io/cloud-run/testing/newtest-job\"},\n    )\n    # [END howto_operator_gcp_update_job]\n\n    # Add triggers to show trigger rules.\n    create_job >> get_job\n    create_job >> execute_job\n    create_job >> list_jobs\n    create_job >> remove_stage\n    create_job >> set_stage\n    create_job >> upsert_job\n    create_job >> update_job\n\n    # [END system_test_example]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2003_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.880741}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning lifecycle for a predictive model, demonstrating how to manage cloud resources, transfer data, create datasets, and handle model training and deployment. Showcase how to integrate multiple cloud services and operators to create a robust, scalable machine learning infrastructure.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\nfrom time import sleep\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SagemakerOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.operators.ssm import SSMCommandOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingJobSensor,\n    SageMakerTrainingJobSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger()\ntest_env_name_key = \"TEST_ENVIRONMENT_NAME\"\n\n\n@task\ndef get_execution_role_arn():\n    r\"\"\"Helper function to get the execution ARN of the processing job from SSM Parameter\n    Store. The role needed to create the notebook instance has the Execution IAM Role\n    attached by creating the notebook.\n    \"\"\"\n    ssm = boto3.client(\"ssm\")\n    return ssm.get_parameter(Name=f\"/processing/job/execution_role Arn\")[\"Parameter\"][\"Value\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_logs(log_names: list):\n    \"\"\"Clean logs with the given names.\"\"\"\n\n    def _clean_logs(_log_names):\n        log_groups = boto3.client(\"logs\").list_log_groups()\n        for log_group in log_groups[\"LogGroups\"]:\n            log_names.append(f\"*log-group-name*:/{log_group['LogGroupName']}\")\n        logs = boto3.client(\"logs\")\n        for log_name in _log_names:\n            logs.delete_log_group(logName=log_name)\n\n    # Iteratively search for logs if they don't exist initially\n    try:\n        _clean_logs(log_names)\n    except ClientError as e:\n        if \"No permission\" not in str(e):\n            raise\n\n\n@task\ndef get_execution_role_credentials(execution_role_arn: str) -> dict:\n    \"\"\"Get an IAM session with credentials created from an Amazon SageMaker Processing Job execution role.\"\"\"\n\n    response = boto3.client(\"sts\").get_caller_identity()\n    return {\n        \"RoleArn\": execution_role_arn,\n        \"SessionAttributes\": {\n            \"AWSIAMRoleForSageMakerProcessing\": response[\"Arn\"],\n        },\n    }\n\n\n@task\ndef set_up_system_test_config(s3_bucket_name: str, region_name: str, env_id: str):\n    \"\"\"Create a s3 bucket and upload a sample file for use in the system test.\"\"\"\n    log.info(\"Setting up system test config %s.\", env_id)\n\n    sns_topic_arn = f\"arn:aws:sns:{region_name}:{env_id}:topic\"\n    bucket_url = f\"http://s3-{region_name}.amazonaws.com/{s3_bucket_name}/{env_id}/\"\n\n    template = {\n        \"S3Bucket\": bucket_url,\n        \"S3Key\": \"system-test/sample-ai-data.csv\",\n    }\n\n    ssm = boto3.client(\"ssm\")\n    ssm.put_parameter(\n        Name=\"s3-bucket-name\",\n        Type=\"String\",\n        Value=s3_bucket_name,\n        Overwrite=True,\n    )\n    ssm.put_parameter(\n        Name=\"sns-topic-arn\",\n        Type=\"String\",\n        Value=sns_topic_arn,\n        Overwrite=True,\n    )\n    ssm.put_parameter(\n        Name=\"env-id\",\n        Type=\"String\",\n        Value=env_id,\n        Overwrite=True,\n    )\n    ssm.put_parameter(\n        Name=\"sample-data-file\",\n        Type=\"String\",\n        Value=\"ai-data.csv\",\n        Overwrite=True,\n    )\n\n    # Make sure the parameter is encoded properly\n    response = ssm.get_parameter(Name=\"sample-data-file\", WithDecodedParameterValue=True)\n    log.info(\"Sample data parameter sent via SSM with decoded value: %s\", response[\"Parameters\"][0])\n\n\n# Instantiate the system test context which holds the essential pieces of information to run\n# this system test.\nsys_test_context_task = SystemTestContextBuilder().add_variable(test_env_name_key).build()\n\nDAG_ID = \"example_sagemaker_training_job\"\n\n# Optionally override the test context\noverride_test_context = lambda task_keys: task_keys or {}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_env_name = sys_test_context_task()\n    s3_bucket_name = f\"{test_env_name}-sagemaker-training-job-bucket\"\n    s3_key_prefix = f\"{test_env_name}-sagemaker-training-job-key\"\n    region_name = boto3.session.Session().region_name\n    execution_role_arn = get_execution_role_arn()\n    set_up_system_test_config(s3_bucket_name, region_name, test_env_name)\n\n    create_train_data_s3_uri = f\"s3://{s3_bucket_name}/{s3_key_prefix}/\"\n    # [START howto_operator_sagemaker_training]\n    train_pipeline = SagemakerOperator(\n        task_id=\"train-pipeline\",\n        role_arn=execution_role_arn,\n        input_data=create_train_data_s3_uri,\n        output_data=f\"s3://{s3_bucket_name}/{s3_key_prefix}/training_output/\",\n        training_job_name=\"training_mlops_pipeline\",\n        training_instance_count=1,\n        training_instance_type=\"ml.m5.large\",\n        hyperparameters={\n            \"num_cols\": \"18\",\n            \"num_rows\": \"2000\",\n            \"column_separator\": \",\",\n            \"target_column\": \"Salary\",\n            \"ignore_columns\": \"\u5a5a\u9f84,\u5a5a\u59fb\u72b6\u6001,\u4f4f\u5740\u7c7b\u578b,\u6559\u80b2\u7a0b\u5ea6,\u884c\u4e1a\",\n        },\n        wait_for_completion=True,\n    )\n    # [END howto_operator_sagemaker_training]\n\n    register_train_data_s3_uri = SSMCommandOperator(\n        task_id=\"register-train-data-uri\",\n        command=[\n            \"put-parameter\",\n            \"--name=s3-training-data-uri\",\n            \"--value={{ tsdb.env.id | default(create_s3_uri) }}\",\n            \"--resolver DefaultResolver\",\n        ],\n    )\n\n    # [START howto_sensor_sagemaker_training_job]\n    sensor_wait_for_job_complete = SageMakerTrainingJobSensor(\n        task_id=\"sensor-wait-for-job-complete\",\n        training_job_id=train_pipeline.output[\"training_job_arn\"].split(\"/\")[-1],\n        poke_interval=10,\n        timeout=60 * 30,\n        trigger_once=True,\n    )\n    # [END howto_sensor_sagemaker_training_job]\n\n    # [START howto_sensor_sagemaker_processing]\n    sensor_wait_for_processing_job_complete = SageMakerProcessingJobSensor(\n        task_id=\"sensor-wait-for-processing-job-complete\",\n        processing_job_id=\"{{ tsdb.train_job_arn.split('/')[-1] }}\",\n        poke_interval=10,\n        timeout=60 * 30,\n        trigger_once=True,\n    )\n    # [END howto_sensor_sagemaker_processing]\n\n    delete_training_data_s3_uri = SSMCommandOperator(\n        task_id=\"delete-training-data-uri\",\n        command=[\n            \"remove-parameter\",\n            \"--name=s3-training-data-uri\",\n        ],\n    )\n\n    delete_train_pipeline = S3DeleteBucketOperator(\n        task_id=\"delete-train-pipeline-s3-bucket\",\n        bucket=name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        override_test_context({}),\n        clean_logs([\"*\"]),\n        create_train_data_s3_uri,\n        register_train_data_s3_uri,\n        train_pipeline,\n        sensor_wait_for_job_complete,\n        sensor_wait_for_processing_job_complete,\n        delete_train_pipeline,\n        delete_training_data_s3_uri,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_264_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807592}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data processing system that performs daily data validation and generates reports with error tracking. Create a workflow that can automatically detect data integrity issues and trigger appropriate error communication or reporting mechanisms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample DAG demonstrating the usage of relevant Dataproc operators for managing\nProvisioned Cluster.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    ClustersCreateOperator,\n    ClustersDeleteOperator,\n    ClustersListOperator,\n    ClustersUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_dataproc_clusters\"\nPROJECT_ID = \"dummy-project\"\n\nREGION = \"europe-west1\"\nCLUSTER_NAME_1 = f\"cluster-1-{DAG_ID}-{PROJECT_ID}\"\nCLUSTER_NAME_2 = f\"cluster-2-{DAG_ID}-{PROJECT_ID}\"\n\n# Clustering settings\nGCE_IMAGE = \"ubuntu-d2-v20220604\"\nCLUSTER_NAME = CLUSTER_NAME_1 if len(CLUSTER_NAME_2) > len(CLUSTER_NAME_1) else CLUSTER_NAME_2\nCONFIG_NAME = CLUSTER_NAME + \"-config\"\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n}\n\nwith DAG(\n    PROJECT_ID,\n    dag_id=DAG_ID,\n    description=\"Dataproc example DAG creating and manipulating Spark clusters.\",\n    start_date=datetime(2028, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_cluster_1 = ClustersCreateOperator(\n        task_id=\"create_cluster_1\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_1,\n        region=REGION,\n        cluster_config=CLUSTER_CONFIG,\n        is_preemptible=True,\n    )\n\n    create_cluster_2 = ClustersCreateOperator(\n        task_id=\"create_cluster_2\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_2,\n        region=REGION,\n        cluster_config=CLUSTER_CONFIG,\n        is_preemptible=True,\n    )\n\n    create_cluster_config = ClustersCreateOperator(\n        task_id=\"create_cluster_config\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        cluster_config=CLUSTER_CONFIG,\n        is_preemptible=True,  # This cluster will be updated below.\n    )\n\n    list_clusters = ClustersListOperator(task_id=\"list_clusters\", project_id=PROJECT_ID, region=REGION)\n\n    update_cluster = ClustersUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        new_cluster_config={\"master_config\": {\"zone\": \"us-central1-c\"}},\n    )\n\n    delete_cluster = ClustersDeleteOperator(\n        task_id=\"delete_cluster\", project_id=PROJECT_ID, cluster_name=CLUSTER_NAME\n    )\n\n    delete_cluster_1 = ClustersDeleteOperator(\n        task_id=\"delete_cluster_1\", project_id=PROJECT_ID, cluster_name=CLUSTER_NAME_1\n    )\n\n    delete_cluster_2 = ClustersDeleteOperator(\n        task_id=\"delete_cluster_2\", project_id=PROJECT_ID, cluster_name=CLUSTER_NAME_2\n    )\n\n    # [START how_to_cloud_dataproc_list_clusters_operator]\n    run_this_last = TriggerRule(last_task_only)\n    (create_cluster_1 >> create_cluster_2 >> create_cluster_config >> list_clusters >> update_cluster\n     >> [delete_cluster, delete_cluster_1, delete_cluster_2])[run_this_last]\n\n    # [END how_to_cloud_dataproc_list_clusters_operator]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1842_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807683}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data workflow that uses a sensor task to simulate checking for data availability or external system readiness before triggering subsequent processing steps. Create a DAG that demonstrates how to conditionally control task execution using Airflow's sensor decorator.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG\n\n\nwith DAG(\n    dag_id=\"example_bash_sensor\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_bash_sensor]\n    run_this = BashOperator(\n        task_id=\"wait_some_time\",\n        bash_command=\"sleep 30\",\n    )\n    # [END howto_operator_bash_sensor]\n    run_this.wait_for_completion()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1810_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807714}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates SQL-based data validation techniques using column and table check operators against a predefined database table. Implement sequential validation tasks that check specific data quality metrics.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.druid import DruidAnalyseTaskOperator\nfrom airflow.providers.alibaba.cloud.operators.eni import ENILoggerOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = ENV_ID or 'default'\nDAG_ID = \"example_druid_analyse\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule='0 0 * * *',\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    default_args={\"proj_name\": \"test-proj\", \"env_id\": ENV_ID},\n    max_active_runs=1,\n    catchup=False,\n    tags=['demo'],\n) as dag:\n    # [START howto_operator_druid_analyse_task]\n    analyse_task = DruidAnalyseTaskOperator(\n        task_id=\"analyse_task\",\n        proj_name=\"test-proj\",\n        env_id=\"default\",\n        schema=\"default\",\n        dimension=\"default\",\n        begin_time=\"2049-01-01 00:00:00\",\n        end_time=\"2049-01-01 00:00:00\",\n        ret_col_names=[\"c1\", \"c2\"],\n        aggr_col_name=\"c1\",\n        result_format=\"JSON\",\n    )\n    # [END howto_operator_druid_analyse_task]\n\n    # ENI logging configuration\n    enable_log = ENILoggerOperator(task_id=\"enable_log\", log_group_id=\"{{ task_instance.xcom_pull('get_log') }}\")\n\n    get_log = ENILoggerOperator(\n        task_id=\"get_log\",\n        log_group_id=\"{{ task_instance.xcom_pull('get_log')['log_group_id'] }}\",\n    )\n\n    enable_log >> get_log\n\n\n# ### Test XCom ###\n# When you run the example DAG with `airflow test <DAG> all` you can check xcom output.\nfrom tests.system.utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\nlist(dag.tasks) | watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_120_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765982891.8807886}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates a complete cloud data transfer process, including bucket creation, data extraction, and post-transfer cleanup with error handling and observability.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3tosql\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nTABLE_NAME = \"test_table\"\nSQL_COLUMN_1 = \"test_column_1\"\nSQL_COLUMN_2 = \"test_column_2\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    ti = test_context.pop(ti)\n\n    s3_bucket = f\"{test_context[ENV_ID]}-s3-bucket\"\n    s3_key_1 = f\"{test_context[ENV_ID]}-s3-key-1\"\n    s3_key_2 = f\"{test_context[ENV_ID]}-s3-key-2\"\n\n    conn_id = f\"{test_context[ENV_ID]}-conn-id\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    @task\n    def create_table_and_insert_value(table_name: str, sql_column_1: str, sql_column_2: str):\n        s3_to_sql_sql = f\"SELECT '{{ds}}', '{sql_column_1}' AS {sql_column_1}, '{sql_column_2}' AS {sql_column_2} FROM ''{table_name}''\"\n        # [START howto_operator_s3_to_sql]\n        S3ToSqlOperator(\n            task_id=\"create_table_and_insert_value\",\n            s3_bucket=s3_bucket,\n            s3_key=s3_key_1,\n            region_name=\"default\",\n            sql=s3_to_sql_sql,\n            table_name=table_name,\n            conn_id=conn_id,\n            sql_column_1=sql_column_1,\n            sql_column_2=sql_column_2,\n            split_on=sql_column_1,\n        )\n        # [END howto_operator_s3_to_sql]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> create_table_and_insert_value(TABLE_NAME, SQL_COLUMN_1, SQL_COLUMN_2)\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1658_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8923693}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates task dependencies and lifecycle management, using nested task groups and explicit setup/teardown decorators to show how complex workflows can be structured with clear initialization and finalization steps. Develop a demonstration that highlights Airflow's ability to manage task relationships and automatic resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\n\n@task(task_id=\"run_me\")\ndef run_me():\n    print(\"Hi\")\n\n\nwith DAG(\n    \"example_setup_teardown\",\n    schedule=pendulum.timeperiod(\"2021-01-01\", \"2021-01-02\"),\n    start_date=pendulum.datetime(2021, 1, 1, 0, 0, 1),\n    catchup=False,\n):\n    run_me()\n\n    t1 = BashOperator(task_id=\"to_be_overridden\", bash_command=\"echo 1\")\n    t2 = BashOperator(task_id=\"my_task\", bash_command=\"echo 2\")\n    t3 = BashOperator(task_id=\"to_be_overridden\", bash_command=\"echo 3\")\n\n    t1 >> t2\n    t2 >> t3"}], "metadata": {"file_name": "dag_1054_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892375}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an image data processing pipeline that transfers image files between cloud storage buckets and prepares them for machine learning-based product recognition. Create a reproducible workflow that demonstrates how to programmatically manage image datasets and product metadata using cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductsOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductSetLabelsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"cloud_vision\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"image1.jpg\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nCUSTOMER_ID = str(id)(prefix=\"cix_\")\nPRODUCT_ID = \"cix_product_1\"\nPRODUCT_SET_ID = \"cix_product_set_1\"\nVISION_IMAGE_URL = f\"gs://{BUCKET_NAME}/{FILE_NAME}\"\n\n# [START howto_operator_vision_create_product]\ndef _create_product(product_id, project_id):\n    vision = VisionService(client=options)\n    result = vision.create_product(\n        project_id=project_id, body={\"product\": {\"id\": product_id}}\n    )\n    return result.product.id\n\n\n# [END howto_operator_vision_create_product]\n\n\n# [START howto_operator_vision_get_product]\ndef _get_product(product_id, project_id):\n    vision = VisionService(client=options)\n    result = vision.get_product(project_id=project_id, product_id=product_id)\n    return result\n\n\n# [END howto_operator_vision_get_product]\n\n\n# [START howto_operator_vision_list_products]\ndef _list_products(project_id):\n    vision = VisionService(client=options)\n    products = vision.list_products(project_id=project_id)\n    return products.products\n\n# [END howto_operator_vision_list_products]\n\n\n# [START howto_operator_vision_delete_product]\ndef _delete_product(product_id, project_id):\n    vision = VisionService(client=options)\n    response = vision.delete_product(\n        project_id=project_id, product_id=product_id\n    )\n    return response.product_id\n\n\n# [END howto_operator_vision_delete_product]\n\n\n# [START howto_operator_vision_update_product]\ndef _update_product(product_id, new_labels, project_id):\n    vision = VisionService(client=options)\n    product_request = {\"product\": {\"labels\": new_labels}}\n    response = vision.update_product(\n        project_id=project_id, product_id=product_id, body=product_request\n    )\n    return response.product.id\n\n\n# [END howto_operator_vision_update_product]\n\n\n# [START howto_operator_vision_reference_image]\ndef _reference_image(bucket_name, object_name, project_id):\n    vision = VisionService(client=options)\n    response = vision.create_reference_image(\n        bucket_name=bucket_name, object_name=object_name, project_id=project_id\n    )\n    return response.reference_image.name\n\n\n# [END howto_operator_vision_reference_image]\n\n\n# [START howto_operator_vision_get_product_set]\ndef _get_product_set(product_set_id, project_id):\n    vision = VisionService(client=options)\n    result = vision.get_product_set(project_id=project_id, product_set_id=product_set_id)\n    return result\n\n\n# [END howto_operator_vision_get_product_set]\n\n\n# [START howto_operator_vision_list_product_sets]\ndef _list_product_sets(project_id):\n    vision = VisionService(client=options)\n    product_sets = vision.list_product_sets(project_id=project_id)\n    return product_sets.product_sets\n\n# [END howto_operator_vision_list_product_sets]\n\n\n# [START howto_operator_vision_delete_product_set]\ndef _delete_product_set(product_set_id, project_id):\n    vision = VisionService(client=options)\n    response = vision.delete_product_set(\n        project_id=project_id, product_set_id=product_set_id\n    )\n    return response.product_set_id\n\n\n# [END howto_operator_vision_delete_product_set]\n\n\n# [START howto_operator_vision_create_product_set]\ndef _create_product_set(product_set_id, project_id):\n    vision = VisionService(client=options)\n    product_set = {\n        \"product_set\": {\n            \"display_name\": product_set_id,\n        }\n    }\n    response = vision.create_product_set(project_id=project_id, body=product_set)\n    return response.product_set.id\n\n\n# [END howto_operator_vision_create_product_set]\n\n\n# [START howto_operator_vision_remove_product_from_product_set]\ndef _remove_product_from_product_set(product_set_id, product_id, project_id):\n    vision = VisionService(client=options)\n    response = vision.remove_product_from_product_set(\n        project_id=project_id, product_set_id=product_set_id, product_id=product_id\n    )\n    return response.product_set_id\n\n\n# [END howto_operator_vision_remove_product_from_product_set]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=BUCKET_NAME, project_id=PROJECT_ID\n    )\n\n    upload_file = GCSToGCSOperator(\n        task_id=\"upload_file\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME,\n        destination_object=FILE_NAME,\n    )\n\n    # [START howto_operator_vision_create_product_set_with_labels]\n    create_product_set_with_labels = CloudVisionCreateProductSetOperator(\n        task_id=\"create_product_set_with_labels\",\n        project_id=PROJECT_ID,\n        body={\"product_set\": {\"display_name\": PRODUCT_SET_ID}},\n    )\n    # [END howto_operator_vision_create_product_set_with_labels]\n\n    # [START howto_operator_vision_get_product_with_labels]\n    get_product_with_labels = CloudVisionGetProductOperator(\n        task_id=\"get_product_with_labels\",\n        product_set_id=CREATE_PRODUCT_SET_WITH_LABELS.output[\"product_set_id\"],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_get_product_with_labels]\n\n    # [START howto_operator_vision_list_product_sets]\n    list_product_sets = CloudVisionListProductSetsOperator(\n        task_id=\"list_product_sets\", project_id=PROJECT_ID, page_size=5\n    )\n    # [END howto_operator_vision_list_product_sets]\n\n    # [START howto_operator_vision_delete_product_set]\n    delete_product_set = CloudVisionDeleteProductSetOperator(\n        task_id=\"delete_product_set\",\n        product_set_id=CREATE_PRODUCT_SET_WITH_LABELS.output[\"product_set_id\"],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_delete_product_set]\n\n    # [START howto_operator_vision_create_reference_image]\n    create_reference_image = CloudVisionCreateReferenceImageOperator(\n        task_id=\"create_reference_image\",\n        project_id=PROJECT_ID,\n        body={\"reference_image\": {\"source_image_url\": VISION_IMAGE_URL}},\n    )\n    # [END howto_operator_vision_create_reference_image]\n\n    # [START howto_operator_vision_get_product]\n    get_product = CloudVisionGetProductOperator(task_id=\"get_product\", product_id=CREATE_PRODUCT_OUTPUT[\"product_id\"], project_id=PROJECT_ID)\n    # [END howto_operator_vision_get_product]\n\n    # [START howto_operator_vision_list_products]\n    list_products = CloudVisionListProductsOperator(task_id=\"list_products\", project_id=PROJECT_ID)\n    # [END howto_operator_vision_list_products]\n\n    # [START howto_operator_vision_delete_product]\n    delete_product = CloudVisionDeleteProductOperator(\n        task_id=\"delete_product\", product_id=CREATE_PRODUCT_OUTPUT[\"product_id\"], project_id=PROJECT_ID\n    )\n    # [END howto_operator_vision_delete_product]\n\n    # [START howto_operator_vision_remove_product_from_product_set]\n    remove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator(\n        task_id=\"remove_product_from_product_set\",\n        product_set_id=CREATE_PRODUCT_SET_WITH_LABELS.output[\"product_set_id\"],\n        product_id=CREATE_PRODUCT_OUTPUT[\"product_id\"],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_remove_product_from_product_set]\n\n    # [START howto_operator_vision_add_product_to_product_set]\n    add_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n        task_id=\"add_product_to_product_set\",\n        product_set_id=CREATE_PRODUCT_SET_WITH_LABELS.output[\"product_set_id\"],\n        product_ids=[CREATE_PRODUCT_OUTPUT[\"product_id\"]],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_add_product_to_product_set]\n\n    (create_bucket >> upload_file >> create_product_set_with_labels >> get_product_with_labels\n     >> list_product_sets >> delete_product_set\n     >> create_reference_image >> get_product\n     >> list_products >> delete_product\n     >> remove_product_from_product_set >> add_product_to_product_set)\n\n    # [START howto_operator_vision_reference_image]\n\n    (create_bucket >> upload_file >> create_product_set_with_labels >> get_product_with_labels\n     >> list_product_sets >> delete_product_set\n     >> create_reference_image >> get_product\n     >> list_products >> delete_product\n     >> remove_product_from_product_set >> add_product_to_product_set)\n\n    # [END howto_operator_vision_reference_image]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_598_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8923955}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with clear sequential steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_cloudformation\"\n\n# The CloudFormation template used to create the stack we'll execute.\n# Because this sample uses HTTP, the inline stack template is not suitable.\n# You must upload a file called `cloudformation-stack-template.json` in the same directory\n# and modify its content to match the desired stack.\nwith open(\"cloudformation-stack-template.json\") as fd:\n    STACK_TEMPLATE_JSON = fd.read()\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n# At least one lambda function exists\nAtLeastOneLambdaExists = {\n    \"Resources\": {\n        # This lambda function is created when the stack is created.\n        # Making sure it exists is how we verify at least one lambda function exists.\n        \"Ref\": \"MyLambdaFunction\"\n    }\n}\n\ndef execute_cliz_command(command: list[str]) -> str:\n    temp_file = NamedTemporaryFile(delete=False)\n    command.extend([\"> \", temp_file.name])\n    log.info(\"Running following command : %s\", \" \".join(command))\n    try:\n        subprocess.run(command, check=True)\n    except FileNotFoundError:\n        log.error(\"The following command could not be found : {} \".format(\" \".join(command)))\n        raise\n    return temp_file.read().decode()\n\n\n@task\ndef get_stack_status(stack_name: str) -> str:\n    command = [\"cliz\", \"sns\", \"list-sns-queues\"]\n    try:\n        execute_cliz_command(command)\n    except FileNotFoundError:\n        log.error(\"The following command could not be found : %s\", \"cliz\")\n        log.error(\"Are you missing Amazon CLI ?\")\n        raise\n    result = execute_cliz_command(command)\n    return result.split(\"\\n\")[1].split(\"/\")[-1]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_stack(stack_name: str):\n    try:\n        cloudformation = boto3.client(\"cloudformation\")\n        response = cloudformation.delete_stack(\n            StackName=stack_name,\n        )\n        log.info(response)\n    except ClientError as e:\n        log.warning(\"Could not delete stack %s. Reason: %s\", stack_name, e)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    stack_name = f\"{env_id}-test-cloudformation-stack\"\n    role_arn = test_context[\"ROLE_ARN\"]\n\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=stack_name,\n        stack_template=json.loads(STACK_TEMPLATE_JSON),\n        on_finish_action=\"execute_lambda\",\n        timeout_in_minutes=2,\n    )\n\n    # [START howto_sensor_cloudformation]\n    wait_for_stack_creation = CloudFormationCreateStackSensor(\n        task_id=\"wait_for_stack_creation\",\n        stack_name=create_stack.output[\"stack_name\"],\n    )\n    # [END howto_sensor_cloudformation]\n\n    get_status_of_stack = PythonOperator(task_id=\"get_status_of_stack\", python_callable=get_stack_status, op_kwargs={\"stack_name\": stack_name})\n\n    delete_lambda = delete_stack(MyLambdaFunction)\n\n    chain(\n        test_context,\n        create_stack,\n        wait_for_stack_creation,\n        get_status_of_stack,\n        delete_lambda,\n        delete_stack(stack_name),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) | watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_58_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924072}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates various sensor types in Apache Airflow, showcasing how different sensors can conditionally trigger downstream tasks. Create a DAG that illustrates waiting mechanisms, file checks, time-based conditions, and different sensor behaviors.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.sensors.filesystem import FileSensor, FileSizeSensor\nfrom airflow.providers.standard.sensors.python import PythonSensor\nfrom airflow.providers.standard.sensors.time_delta import TimeDeltaSensor\nfrom airflow.providers.standard.sensors.time_sensor import TimeSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@task(task_id=\"check_file\")\ndef check_file(path):\n    print(f\"Checking file at {path}\")\n    print(\"File is present and readable!\")\n    return True\n\n\n@task(task_id=\"check_size\")\ndef check_size(size, path):\n    print(f\"Checking size of file at {path}\")\n    print(f\"Size of file expected to be {size} bytes!\")\n    print(\"Size of file is what we expect!\n           Returning True so task 'get_file_size' below will mark file as found.\")\n    return True\n\n\n@task(task_id=\"get_file_size\")\ndef get_file_size(path):\n    size = os.path.getsize(path)\n    print(f\"Actual size of file at {path} is {size} bytes\")\n    return size\n\n\nwith DAG(\n    dag_id=\"example_file_sensor\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    check_file(\"/tmp/emptyfile.txt\") >> check_size(0, \"/tmp/emptyfile.txt\")\n    # [START howto_standard_sensor_file_sensor]\n    check_file_path = FileSensor(\n        task_id=\"check_file_path\", path=\"/tmp/emptyfile.txt\", poke_interval=5, timeout=6\n    )\n    # [END howto_standard_sensor_file_sensor]\n    check_file_path >> check_size(0, \"/tmp/emptyfile.txt\")\n    # [START howto_standard_sensor_time_delta_sensor]\n    check_time_delta = TimeDeltaSensor(\n        task_id=\"check_time_delta\", late=30, timeout=6, poke_interval=5\n    )\n    # [END howto_standard_sensor_time_delta_sensor]\n    check_time_delta >> PythonSensor(task_id=\"check_python_sensor\", python_callable=print, op_kwargs={\"task_id\": \"check_python_sensor\"})\n    check_time_delta >> TimeSensor(task_id=\"check_time_sensor\", timeout=6, poke_interval=5)\n    (\n        check_file(\"/tmp/emptyfile.txt\")\n        >> check_size(0, \"/tmp/emptyfile.txt\")\n        >> check_file_path\n        >> check_size(0, \"/tmp/emptyfile.txt\")\n        >> check_time_delta\n        >> PythonSensor(task_id=\"check_python_sensor\", python_callable=print, op_kwargs={\"task_id\": \"check_python_sensor\"})\n        >> check_time_delta\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_344_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924153}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Google Dataproc workflow template for running a Pig job on a managed Hadoop cluster. The workflow should showcase asynchronous job instantiation and template management in a cloud data processing environment.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Dataproc workflow operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateWorkflowTemplateOperator,\n    DataprocDeleteWorkflowTemplateOperator,\n    DataprocInstantiateInlineWorkflowTemplateOperator,\n    DataprocInstantiateWorkflowTemplateOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_workflow\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nREGION = \"europe-west1\"\nCLUSTER_NAME = f\"cluster_{DAG_ID}_{ENV_ID}\"\nWAREHOUSE_LOCATION = REGION + \"-warehouse\"\nWORKFLOW_TEMPLATE_NAME = \"workflow-template-def\"\nWORKFLOW_TEMPLATE = {\n    \"id\": WORKFLOW_TEMPLATE_NAME,\n    \"placement\": {\n        \"managedCluster\": {\n            \"clusterName\": CLUSTER_NAME,\n        }\n    },\n    \"insideJobRunner\": {\"jobName\": \"bash\", \"args\": [\"-c\", \"echo Test Masto\"]},\n}\n# [START howto_dataproc_inline_template]\nINLINE_WHEREHOUSE_BODY = {\n    \"name\": WAREHOUSE_LOCATION,\n    \"config\": {\"masterInstanceName\": CLUSTER_NAME, \"secondaryInstanceName\": \"\"},\n}\n\nINLINE_WORKFLOW = {\n    \"id\": \"workflow-id\",\n    \"placement\": {\"inlineClusterTarget\": {\"clusters\": [WAREHOUSE_LOCATION]}},\n    \"jobs\": [{\"stepId\": \"pig_job\", \"pigJob\": {\"queryList\": {\"queries\": [WORKFLOW_TEMPLATE]}}}],\n}\n# [END howto_dataproc_inline_template]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_dataproc_create_workflow_template]\n    create_workflow_template = DataprocCreateWorkflowTemplateOperator(\n        task_id=\"create_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=WORKFLOW_TEMPLATE,\n        workflowTemplateName=WORKFLOW_TEMPLATE_NAME,\n    )\n    # [END howto_dataproc_create_workflow_template]\n\n    # [START howto_dataproc_delete_workflow_template]\n    delete_workflow_template = DataprocDeleteWorkflowTemplateOperator(\n        task_id=\"delete_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        workflowTemplateName=WORKFLOW_TEMPLATE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_dataproc_delete_workflow_template]\n\n    # [START howto_dataproc_instantiate_inline_workflow_template]\n    instantiate_inline_workflow = DataprocInstantiateInlineWorkflowTemplateOperator(\n        task_id=\"instantiate_inline_workflow\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=INLINE_WORKFLOW,\n    )\n    # [END howto_dataproc_instantiate_inline_workflow_template]\n\n    # [START howto_dataproc_create_workflow_template_from_id]\n    recreate_workflow_template = DataprocCreateWorkflowTemplateOperator(\n        task_id=\"recreate_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        workflowTemplateName=WORKFLOW_TEMPLATE_NAME,\n        body={},\n    )\n    # [END howto_dataproc_create_workflow_template_from_id]\n\n    # [START howto_dataproc_instantiate_workflow_template_from_id]\n    instantiate_recreated_workflow = DataprocInstantiateWorkflowTemplateOperator(\n        task_id=\"instantiate_recreated_workflow\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        workflowTemplateName=WORKFLOW_TEMPLATE_NAME,\n        body={\"id\": \"workflows-workflow-id\"},\n    )\n    # [END howto_dataproc_instantiate_workflow_template_from_id]\n\n    (create_workflow_template >> delete_workflow_template >> instantiate_inline_workflow)\n    (create_workflow_template >> recreate_workflow_template >> instantiate_recreated_workflow)\n\n    from tests.system.utils import get_test_run\n\n    # This test needs the following:\n    # - A cluster named \"cluster_\" with \"nfs-read-write\" or \"gce-nfs\" or \"gce-eqlspeed-aws\"-compatible cluster target in the region\n    # - A JSON file containing a warehouse definition located at /tests/system/test_resources/dataproc/warehouse.json\n    # - Sufficient permissions to call Dataproc APIs\n    test_run = get_test_run(dag)\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done and map the result\n    # to test_get_task_result()\n    list(watcher(test_run))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_195_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924265}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration DAG that demonstrates secure and controlled file movement between cloud storage systems with explicit lifecycle management. Create a process that initializes a temporary storage container, executes a cross-platform file transfer, and ensures complete resource cleanup regardless of task outcomes.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.transfers.copy_between_s3_files import CopyBetweenS3FilesOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Monday\nfrom airflow.utils.trigger_strategy import TriggerStrategyOnceEveryWeekDay\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_copy_between_files\"\n\ns3_conn_id = \"s3_default\"\nconn = Connection(\n    conn_id=s3_conn_id,\n    conn_type=\"s3\",\n    extra={\"aws_access_key_id\": \"AKIAJU2EXWPEHVVAVcGA\", \"aws_secret_access_key\": \"L3b559z4MY0h+M6m8tZaQd1kx5uqkjx9\"},\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=TriggerStrategyOnceEveryWeekDay Tuesday, Thursday, Saturday, trigger_rule=TriggerRule.ALL_DONE\n) as dag:\n    start = Connection(id=\"1\", conn_id=\"start\", conn_type=\"s3\", extra={\"bucket\": \"s3://start-bucket\"})\n    dest = Connection(id=\"2\", conn_id=\"dest\", conn_type=\"s3\", extra={\"bucket\": \"s3://dest-bucket\"})\n\n    @task\n    def create_tmp_s3_bucket():\n        \"\"\"\n        Task to create a temporary S3 bucket.\n        \"\"\"\n        from airflow.models.baseoperator import chain\n\n        from airflow.providers.s3.transfers.copy_single_file_to_s3 import CopySingleFileToS3Operator\n        from airflow.providers.s3.transfers.copy_within_s3_files import CopyS3FilesOperator\n\n        bucket = \"s3tmp-\" + str(UUID())\n        s3_conn = \"s3_default\"\n\n        # [START howto_transfer_create_tmp_s3_bucket]\n        create_temporary_s3_bucket = CreateTemporaryS3BucketOperator(task_id=\"create_temp_bucket\",\n                                                                     bucket_name=bucket,\n                                                                     s3_conn_id=s3_conn)\n        # [END howto_transfer_create_tmp_s3_bucket]\n\n        # [START howto_transfer_copy_single_file_to_s3]\n        copy_file_to_s3 = CopySingleFileToS3Operator(task_id=\"copy_file_to_s3\", source_key=\"/empty\", s3_conn_id=s3_conn,\n                                                    bucket_name=bucket)\n        # [END howto_transfer_copy_single_file_to_s3]\n\n        # [START howto_transfer_copy_within_s3_files]\n        copy_files Within_S3 = CopyS3FilesOperator(task_id=\"copy_files_Within_S3\", source_bucket=S3_SOURCE,\n                                                 dest_bucket=S3_DESTINATION, source_key=\"/within/\", dest_key=\"/within/\",\n                                                 s3_conn_id=s3_conn)\n        # [END howto_transfer_copy_within_s3_files]\n\n        chain(create_temporary_s3_bucket, copy_files Within_S3, copy_file_to_s3)\n        return bucket\n\n    temp = create_tmp_s3_bucket()\n    temp.expand(key=str(temp.output))\n    d = temp.output\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_tmp_s3_bucket(bucket):\n        \"\"\"\n        Task to delete a temporary S3 bucket created above.\n        \"\"\"\n        from airflow.models.baseoperator import chain\n\n        chain(\n            remove_s3_objectsOperator(task_id=\"remove_objects\", bucket=bucket),\n            remove_bucketOperator(task_id=\"remove_bucket\", bucket=bucket),\n        )\n\n    delete_tmp_s3_bucket(temp.output)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_objects(bucket):\n        from airflow.models.baseoperator import chain\n\n        chain(\n            remove_objects_operator(task_id=\"remove-objects\", bucket=bucket),\n        )\n\n    remove_objects(temp.output)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_bucket(bucket):\n        from airflow.models.baseoperator import chain\n\n        chain(\n            remove_bucket_operator(task_id=\"remove-bucket\", bucket=bucket),\n        )\n\n    remove_bucket(temp.output)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def get_bucket_keys(bucket):\n        import json\n\n        from airflow.models.baseoperator import chain\n\n        with open(get_bucket_keys.return_value, \"w\") as keys:\n            keys.write(json.dumps(list((obj.key for obj in boto3.client(\"s3\").list_objects(Bucket=bucket)))))\n\n    get_bucket_keys(d)\n\n    @task(outlets=[get_bucket_keys(d)])\n    def get_list_of_files_in_json(output_path):\n        import json\n        from pathlib import Path\n\n        with Path(output_path).open() as file:\n            list_of_files = json.load(file)\n        return list_of_files\n\n    list_of_files = get_list_of_files_in_json(get_bucket_keys.d())\n\n    @task(outlets=[list_of_files])\n    def count_files_in_column_1(output_path):\n        import csv\n        from pathlib import Path\n\n        with Path(output_path).open(mode=\"r\") as file:\n            reader = csv.reader(file)\n            next(reader)\n            number_of_rows = sum(1 for row in reader)\n        return number_of_rows\n\n    number_of_rows = count_files_in_column_1(list_of_files.d())\n\n    @task(outlets=[number_of_rows])\n    def print_number_of_rows(number_of_rows):\n        print(f\"The number of rows is {number_of_rows}\")\n\n    print_number_of_rows(number_of_rows.d())\n\n    @task(outlets=[d])\n    def get_count_of_row_with_string(output_path):\n        import json\n        from pathlib import Path\n\n        with Path(output_path).open() as file:\n            list_of_files = json.load(file)\n        return list_of_files.count(\"/empty\")\n\n    count_of_row_with_string = get_count_of_row_with_string(d.output)\n\n    @task(outlets=[count_of_row_with_string])\n    def print_count_of_row_with_string(count_of_row_with_string):\n        print(f\"The number of rows with /empty is {count_of_row_with_string}\")\n\n    print_count_of_row_with_string(count_of_row_with_string.d())\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_93_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892441}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a DAG that demonstrates basic SQL query execution against an Airflow metadata database, showing how to connect to a Postgres database and run multiple SQL statements. Create a workflow that showcases the SQLExecuteQueryOperator's capabilities for executing database queries.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pgdb.operators.pgdb import PGDBOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"pgdb_operator_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"pgdb\", \"airflow\"],\n) as dag:\n    # [START howto_pgdb_operator]\n    sql_stmt_1 = \"SELECT * FROM table_name;\"\n    sql_stmt_2 = \"DROP TABLE table_name;\"\n\n    pg_db_opers = [\n        PGDBOperator(task_id=f\"td_{i}\", sql=sql_stmt_1) for i in range(1, 4)\n    ]\n        # [END howto_pgdb_operator]\n        # [START howto_pgdb_operator_with_params]\n    pg_db_opers_with_params = [\n        PGDBOperator(\n            task_id=f\"td_{i}\",\n            sql=\"DROP TABLE table_name;\",\n            params={\"exists\": \"if_exists\"},\n        )\n        for i in range(1, 4)\n    ]\n        # [END howto_pgdb_operator_with_params]\n        # [START howto_pgdb_operator_bash_command]\n    bash_cmd_ops = [\n        PGDBOperator(task_id=f\"bash_cmd_{i}\", sql=BashOperator(task_id=f\"bash_cmd_{i}\", bash_command=sql_stmt_2))\n        for i in range(1, 4)\n    ]\n        # [END howto_pgdb_operator_bash_command]\n\n        # [START howto_pgdb_operator_kwargs_example]\n    pg_db_operator_with_args = PGDBOperator(\n        task_id=\"test-task\",\n        sql=sql_stmt_1,\n        rows=5,\n        depend_on_last=True,\n    )\n        # [END howto_pgdb_operator_kwargs_example]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1158_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924472}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete data pipeline from NoSQL database export to cloud storage staging and then into a queryable data warehouse table. Develop a systematic workflow that shows how different cloud services can be integrated for seamless data processing.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.mssql_to_gcs import MsSqlToGCSOperator\nfrom airflow.providers.snowflake.transfers.gcs_to_snowflake import GCSToSnowflakeOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_mssql_to_gcs\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START mssql_to_gcs]\n    mssql_to_gcs_task = MsSqlToGCSOperator(\n        task_id=\"mssql_to_gcs_task\",\n        sql=\"SELECT name FROM sys.tables\",\n        mssql_conn_id=\"mssql_default\",\n        destination_bucket=\"test_igti_bucket\",\n        replace=True,\n    )\n    # [END mssql_to_gcs]\n\n    # [START gcs_to_snowflake]\n    gcs_to_snowflake_task = GCSToSnowflakeOperator(\n        task_id=\"gcs_to_snowflake_task\",\n        bucket=\"test_igti_bucket\",\n        sql_table=\"table_name\",\n        snowflake_conn_id=\"snowflake_default\",\n    )\n    # [END gcs_to_snowflake]\n\n    mssql_to_gcs_task >> gcs_to_snowflake_task\n\n    # Task trigger rule:\n    # Task1 >> TriggerRule -> Task2\n    mssql_to_gcs_task >> gcs_to_snowflake_task >> TriggerRule.ALL_DONE\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924532}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that simulates real-world S3 file processing and storage management tasks, including creating buckets, uploading sample data, and performing metadata operations. Design a learning exercise that demonstrates how to programmatically interact with cloud storage using Apache Airflow and AWS S3 providers.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.sqs import (\n    SQSCreateQueueOperator,\n    SQSDeleteQueueOperator,\n    SqsPublishMessageOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3sqsrqsoauth\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_DATA = {\"sample_data\": json.dumps([\"one\", \"two\", \"three\"]))}\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"s3\",\n        description=\"Connection for S3 example.\",\n        extra=json.dumps({\"prefix\": \"s3://bucket/prefix/\"}),\n    )\n    db_conn = boto3.resource(\"rds-data\").DataWarehouse.create_connection(\n        username=\"admin\",\n        password=\"password\",\n        endpoint=conn.extra_json_dict[\"conn_id\"],\n        database=\"test_db\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_queue(queue_arn: str):\n    boto3.client(\" sqs \").delete_queue(QueueArn=queue_arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_bucket(bucket_name: str):\n    boto3.client(\" s3 \").delete_bucket(Bucket=bucket_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) ish:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID]\n    conn_id_name = f\"{env_id}-conn-id\"\n    bucket_name = f\"{env_id}-bucket\"\n    queue_name = f\"{env_id}-queue\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    @task\n    def create_queue(queue_name):\n        boto3.client(\" sqs \").create_queue(\n            QueueName=queue_name,\n            WebsiteConfiguration={\n                \"IndexPage\": \"https://www.example.com/\",\n                \"AllowPublicAccess\": False,\n            },\n        )\n\n    create_queue = create_queue(task_id=\"create_queue\", queue_name=queue_name)\n\n    # [START howto_operator_s3_sqsmake_request]\n    s3_to_sqs = S3ToSQSMakeRequestOperator(\n        task_id=\"s3_to_sqs\",\n        bucket=bucket_name,\n        s3_key=\"sample_data\",\n        dest_queue=queue_name,\n        conn_id=conn_id_name,\n        task_id=\"s3_to_sqs\",\n    )\n    # [END howto_operator_s3_sqsmake_request]\n\n    publish_to_queue = SqsPublishMessageOperator(\n        task_id=\"publish_to_queue\",\n        queue=queue_name,\n        message=SAMPLE_DATA,\n    )\n\n    delete_queue(queue_arn=s3_to_sqs.output)\n\n    delete_bucket(bucket_name=bucket_name)\n\n    @bash_operator\n    def check_s3_file():\n        table = boto3.resource(\"dynamodb\").Table(\"test_table\")\n        response = table.query(KeyConditionExpression=Key(\"location\").EqualTo(f'{bucket_name}/'))\n        assert len(response[\"Items\"]) == 1\n\n    run_check = CheckThat(\n        \"None\",\n        Function.check_for_object(s3_url=f\"s3://{bucket_name}/{S3Hook().object_path_for_key('sample_data')}\")\n        .described_as(\"Object exists at S3\"),\n        PostFailureTaskList=[\n            run_check,\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1117_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924632}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the relationship between `ExponentialBackoff` in Python and its use with `urllib3` for handling HTTP requests?"}, {"role": "assistant", "content": "from concurrent import futures\nimport urllib3\n\ndef fetch(url):\n    http = urllib3.PoolManager()\n    response = http.request('GET', url)\n    return response.data.decode()\n\n@futures.backoff.exponential_backoff(max_tries=5, base=0.1, multiplier=2, jump=0)\ndef fetch_with_retry(url):\n    try:\n        return fetch(url)\n    except Exception as e:\n        print(f\"Failed to fetch {url}: {e}\")\n        raise\n\n# Example usage\nurls = ['http://example.com', 'http://brokenlink.com']\nwith futures.ThreadPoolExecutor(max_workers=len(urls)) as executor:\n    results = executor.map(fetch_with_retry, urls)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924758}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data infrastructure DAG that demonstrates system-level and query-level interactions with Apache Pinot for data exploration and management. Create a workflow that executes administrative commands and retrieves database information using specialized Pinot hooks.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG to demonstrate usage of PinotHook to list available databases.\n\nThis DAG relies on the following environment variables set:\n* PINOT_CONN_ID - connection id for pinot\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinot.hooks.pinot import PinotHook\n\n\n@task()\ndef get_databases():\n    pinot_hook = PinotHook()\n    table_dbs = pinot_hook.list_tables(columns=[\"name\"], schema=None, use_ha=False)\n    return [db[\"name\"] for db in table_dbs if db[\"name\"].endswith(\"db\")]\n\n\nDAG_ID = \"example_pinot_list_databases\"\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAGRUN_TIMEOUT = int(os.environ.get(\"DAGRUN_TIMEOUT\", 20))\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args={\"pinot_conn_id\": \"pinot_default\"},\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    get_databases()\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils import get_test_run\n\n    test_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_773_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924804}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data transformation pipeline that converts CSV files to AVRO format using Google Dataflow templates. Create a workflow that uploads source files, manages GCS buckets, and executes flexible data conversion jobs.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreateApplicationJobOperator,\n    DataflowDeleteApplicationJobOperator,\n    DataflowRunApplicationJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_dataflow_application\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west3\"\nWORKSPACE = \"dataflow-application-bucket-w\"\nOUTPUT_FOLDER = \"app_output\"\nAPPLICATION_ID = \"dataflow-application-bucket-id\"\n\nPARAMETERS = {\"input_file\": f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}/in.txt\"}\n# [START howto_operator_dataflow_run_application_json]\nRUN_APPLICATION_JSON = {\n    \"outputDirectory\": f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}\",\n    \"parameters\": PARAMETERS,\n    \"jobName\": APPLICATION_ID,\n    \"resourceLabels\": {},\n    \"tempFileMaxAgeMs\": 1000000,\n    \"serviceAccountScopes\": [],\n}\n# [END howto_operator_dataflow_run_application_json]\n\n# [START howto_operator_dataflow_run_application_body]\nRUN_APPLICATION_BODY = {\n    \"applicationOptions\": {\n        \"outputFolder\": f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}\",\n        \"parameters\": PARAMETERS,\n    }\n}\n# [END howto_operator_dataflow_run_application_body]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataflow\", \"application\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_workspace = DataflowCreateApplicationJobOperator(\n        task_id=\"create_workspace\",\n        location=LOCATION,\n        workspace_id=WORKSPACE,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_dataflow_run_application_json_template_field]\n    run_application_json = DataflowRunApplicationJobOperator(\n        task_id=\"run_application_json\",\n        body=RUN_APPLICATION_JSON,\n        application_job_name=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_dataflow_run_application_json_template_field]\n\n    # [START howto_operator_dataflow_run_application_body_template_field]\n    run_application_body = DataflowRunApplicationJobOperator(\n        task_id=\"run_application_body\",\n        body=RUN_APPLICATION_BODY,\n        application_job_name=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_dataflow_run_application_body_template_field]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_workspace = DataflowDeleteApplicationJobOperator(\n        task_id=\"delete_workspace\",\n        application_job_name=APPLICATION_ID,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n\n    (\n        create_bucket\n        >> create_workspace\n        >> run_application_json\n        >> run_application_body\n        >> delete_bucket\n        >> delete_workspace\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2023_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892489}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates the lifecycle of cloud infrastructure, showing how to programmatically create a Google Kubernetes Engine (GKE) cluster and then spin up a Dataproc cluster within that Kubernetes environment. Demonstrate infrastructure-as-code principles by defining cluster configurations and managing their creation and deletion as part of a coordinated workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_updatemask_cluster\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nREGION = \"europe-west2\"\n\n# Rather use the constant CLUSTER_SIZE, but need to set up this function to demo the trigger\ndef cluster_size_mask():\n    return {\n        \"valid_values\": [\n            \"n1-standard-4\",\n        ],\n        \"missing_value_threshold\": int(os.environ.get(\"MISSING_VALUE_COUNT\", 5)),\n        \"new_value\": \"n1-standard-8\",  # New value\n    }\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        config={\n            \"master_config\": {\n                \"num_instances\": 1,\n                \"machine_type_uri\": \"n1-standard-4\",\n            },\n            \"worker_config\": {\n                \"num_instances\": 2,\n                \"machine_type_uri\": \"n1-standard-4\",\n            },\n        },\n    )\n\n    # [START how_to_cloud_dataproc_update_cluster_operator]\n    update_cluster = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        new_cluster=config={\n            \"worker_config\": {\n                \"initial_node_count\": 1,\n            }\n        }),\n    # [END how_to_cloud_dataproc_update_cluster_operator]\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_cluster >> update_cluster >> delete_cluster\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1626_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8924968}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data extraction and loading process that captures a specific Firestore collection, exports it to cloud storage, and prepares it for analytical querying in BigQuery. Design a scalable data movement solution that transforms unstructured NoSQL data into a queryable external table format.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using firestore_export_to_gcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.firestore_v1 import DocumentReference\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.firestore_to_gcs import FirestoreToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_firestore_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nEXTERNAL_TABLE_NAME = f\"external_table_{DAG_ID}_{ENV_ID}\"\nFIRESTORE_COLLECTION_ID = \"test-collection\"\nSTORE_FIELDS = [\"name\", \"phone\"]\nSOURCE_FIRESTOREQUENCE = DocumentReference(\n    project_id=PROJECT_ID, namespace=\"namespace\", path=f\"collection/{firestore_id}\"\n)\n# [START howto_operator_firestore_to_gcs]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create_dataset\", dataset_id=DATASET_NAME)\n\n    create_external_table = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table\",\n        destination_project_dataset_table=f\"{DATASET_NAME}.{EXTERNAL_TABLE_NAME}\",\n        schema_fields=[\n            {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"phone\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n        gcp_conn_id=\"google_cloud_platform\",\n        source_objects=[f\"gs://{BUCKET_NAME}/firestore-export.jsonl\"],\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_operator_firestore_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    def clean_up_firestore_ref():\n        \"\"\"Clean up: Delete firestore reference\"\"\"\n        firestore_client = PROJECT_ID\n        database_path = f\"projects/{firestore_client}/database/settings\"\n        settings = {\n            \"cloudSqlInstance\": database_path,\n            \"databaseType\": \"MYSQL\",\n            \"forwardingRules\": [],\n            \"onLine\": True,\n            \"networks\": [],\n            \"quarantineMode\": {\"enabled\": False},\n            \"securityPolicy\": {\"enabled\": True},\n            \"storageAutoResize\": True,\n            \"storageSizeGB\": 30,\n        }\n        service = google.cloud.firestore_v1.Firestore()\n        operation = service.update_database_setting(body=settings)\n        operation.id\n        return operation.name\n\n    export_firestore_to_gcs = FirestoreToGCSOperator(\n        task_id=\"export_firestore_to_gcs\",\n        firestore_collection_name=f\"projects/{PROJECT_ID}/datasets/{DATASET_NAME}/collections/{FIRESTORE_COLLECTION_ID}\",\n        output_filename=\"firestore-export.jsonl\",\n        store_fields=STORE_FIELDS,\n        gcp_conn_id=\"google_cloud_platform\",\n    )\n    # [START howto_operator_export_firestore_to_gcs_deferrable]\n    export_firestore_to_gcs_def = FirestoreToGCSOperator(\n        task_id=\"export_firestore_to_gcs_def\",\n        firestore_collection_name=f\"projects/{PROJECT_ID}/datasets/{DATASET_NAME}/collections/{FIRESTORE_COLLECTION_ID}\",\n        output_filename=\"firestore-export.jsonl\",\n        store_fields=STORE_FIELDS,\n        timeout_seconds=360,\n        is_deferred=True,\n        gcp_conn_id=\"google_cloud_platform\",\n    )\n    # [END howto_operator_export_firestore_to_gcs_deferrable]\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    export_job_operation = clean_up_firestore_ref()\n    export_job_operation.patch(\n        body={\"updateMask\": FieldMask(paths=[\"_id\"])},\n        name=export_firestore_to_gcs.operator.id,\n        requestedCancel=True,\n    )\n\n    export_firestore_to_gcs >> export_firestore_to_gcs_def >> delete_bucket >> delete_dataset\n\n    # Task dependency created via `XComArgs`:\n    # export_firestore_to_gcs >> export_firestore_to_gcs_def\n\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the\n    # DAG model\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_207_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8925078}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that retrieves tweets from different Twitter accounts, transforms the raw data through cleaning and analysis stages, and prepares aggregated insights for storage. Develop a systematic approach to collecting, processing, and summarizing social media data with clear transformation steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom system.openlineage.operator import OpenLineageTestOperator\n\n# [START howto_operator_create_twitter_tweet]\ndef create_twitter_tweet(conn_id, tweet_text):\n    conn = Connection(\n        conn_id=conn_id,\n        conn_type=\"twitter\",\n    )\n    tweet = {\n        \"tweet\": tweet_text,\n        \"media\": [],\n        \"place\": {},\n        \"coordinates\": {},\n        \"user\": {}\n    }\n    conn.add_tweet(tweet)\n# [END howto_operator_create_twitter_tweet]\n\n# [START howto_operator_create_csv_file]\ndef create_csv_file(connection_id, file_path):\n    connection = Session.query(Connection).filter(Connection.conn_id == connection_id).first()\n    if not connection:\n        logging.error(\"Connection %s does not exist\", connection_id)\n        return\n\n    file_obj = connection.conn.create_file(file_path=file_path)\n    sql_stmt = f\"DROP TABLE IF EXISTS {file_path};\"\n    execute_query = SQLExecuteQueryOperator(task_id=\"drop_table\", sql=sql_stmt)\n    execute_query.invoke(file_obj=file_obj)\n# [END howto_operator_create_csv_file]\n\n\nDAG_ID = \"example_twitter\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    run_this_last = BashOperator(task_id=\"run_this_last\", bash_command=\"true\")\n\n    # [START howto_operator_get_tweets]\n    get_tweets = SQLExecuteQueryOperator(\n        task_id=\"get-tweets\",\n        sql=\"\"\"\n            SELECT tweet_id, tweet, user_id, user_screen_name, to_timestamp(time) as parsed_time\n            FROM twitter_output\n            WHERE parsed_time >= '{{ds}}'\n            \"\"\",\n    )\n    # [END howto_operator_get_tweets]\n\n    join_tweets_with_csv = SQLExecuteQueryOperator(\n        task_id=\"join_tweets_with_csv\",\n        sql=\"\"\"\n            SELECT tc.tweet_id, tc.tweet, tc.user_screen_name, cs.file_path, cs.timestamp\n            FROM twitter_join_output tc\n            JOIN csv_input cs ON tc.tweet_id = cs.tweet_id;\n          \"\"\",\n    )\n\n    # [START howto_operator_create_twitter_user]\n    create_twitter_user = SQLExecuteQueryOperator(\n        task_id=\"create-twitter-user\",\n        sql=\"\"\"\n            INSERT INTO twitter_users (user_id, user_screen_name)\n            VALUES (123456789, 'user1');\n          \"\"\",\n    )\n    # [END howto_operator_create_twitter_user]\n\n    # [START howto_operator_create_twitter_user_from_id]\n    create_twitter_user_from_id = SQLExecuteQueryOperator(\n        task_id=\"create-twitter-user-from-id\",\n        sql=\"\"\"\n            INSERT INTO twitter_users (user_id, user_screen_name)\n            VALUES (987654321, 'user2');\n          \"\"\",\n    )\n    # [END howto_operator_create_twitter_user_from_id]\n\n    # [START howto_operator_create_connection_to_mysql]\n    create_connection_to_mysql = SQLExecuteQueryOperator(\n        task_id=\"create-connection-to-mysql\",\n        sql=\"\"\"\n                CREATE CONNECTION my_connection USING mysql ('username'='airflow', 'host'='db');\n              \"\"\",\n    )\n    # [END howto_operator_create_connection_to_mysql]\n\n    # [START howto_operator_create_templated_sql_command]\n    create_templated_sql_command = SQLExecuteQueryOperator(\n        task_id=\"create-temporal-sql-command\",\n        sql=\"SELECT {{ ds }} AS tmp_date\",\n        template_kwargs={\"ds\": \"{{ ds }}\"},\n    )\n    # [END howto_operator_create_templated_sql_command]\n\n    # [START howto_operator_execute_template]\n    execute_template = SQLExecuteQueryOperator(\n        task_id=\"execute-template\",\n        sql=\"SELECT <<tmp_date>> AS <<tmp_table>>\",\n        template_kwargs={\"tmp_date\": \"{{ ds }}\", \"tmp_table\": \"temp\"},\n    )\n    # [END howto_operator_execute_template]\n\n    # [START howto_operator_execute_file_in_connection]\n    execute_file_in_connection = SQLExecuteQueryOperator(\n        task_id=\"execute-file-in-connection\",\n        sql=\"SELECT * FROM @{{'connection'.quote('mysql')}}.{{'connection'.quote('table')}\u6865\u63a5}\",\n    )\n    # [END howto_operator_execute_file_in_connection]\n\n    # [START howto_operator_execute_captured_command]\n    execute_captured_command = BashOperator(\n        task_id=\"execute-captured-command\",\n        bash_command=\"echo '{{ ds_nodash }}'|head -c 4\",\n        capture_output=True,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_execute_captured_command]\n\n    # [START howto_operator_create_twitter_account]\n    create_twitter_account = SQLExecuteQueryOperator(\n        task_id=\"create-twitter-account\",\n        sql=\"\"\"\n                INSERT INTO twitter_accounts (user_screen_name, created_at)\n                VALUES ('user3', '{{ ds }}');\n              \"\"\",\n    )\n    # [END howto_operator_create_twitter_account]\n\n    # [START howto_operator_create_account_list]\n    create_account_list = SQLExecuteQueryOperator(\n        task_id=\"create-account-list\",\n        sql=\"\"\"\n                INSERT INTO account_list (list_name, description)\n                VALUES ('test list', 'test description');\n              \"\"\",\n    )\n    # [END howto_operator_create_account_list]\n\n    # [START howto_operator_create_connection_from_mysql]\n    create_connection_from_mysql = SQLExecuteQueryOperator(\n        task_id=\"create-connection-from-mysql\",\n        sql=\"\"\"\n                CREATE CONNECTION mysql_connection USING mysql ('username'='airflow', 'host'='db');\n              \"\"\",\n    )\n    # [END howto_operator_create_connection_from_mysql]\n\n    # [START howto_operator_create_accounts_table]\n    create_accounts_table = SQLExecuteQueryOperator(\n        task_id=\"create-accounts-table\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS twitter_accounts (\n                  id INT NOT NULL IDENTITY(1,1),\n                  user_screen_name VARCHAR(50) NOT NULL,\n                  created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n                  PRIMARY KEY (id)\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_accounts_table]\n\n    # [START howto_operator_create_tweets_table]\n    create_tweets_table = SQLExecuteQueryOperator(\n        task_id=\"create-tweets-table\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS twitter_tweets (\n                  tweet_id UUID PRIMARY KEY,\n                  tweet TEXT,\n                  user_id UUID NOT NULL,\n                  user_screen_name VARCHAR(50) NOT NULL,\n                  time TIMESTAMP NOT NULL DEFAULT NOW(),\n                  FOREIGN KEY(user_id) REFERENCES twitter_users(id)\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_tweets_table]\n\n    # [START howto_operator_create_accounts_join_table]\n    create_accounts_join_table = SQLExecuteQueryOperator(\n        task_id=\"create-accounts-table\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS twitter_users_connections (\n                  user_screen_name VARCHAR(50) NOT NULL,\n                  connection_id UUID PRIMARY KEY,\n                  FOREIGN KEY(user_screen_name) REFERENCES twitter_users(id)\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_accounts_join_table]\n\n    # [START howto_operator_create_tweets_join_table]\n    create_tweets_join_table = SQLExecuteQueryOperator(\n        task_id=\"create-tweets-table\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS twitter_users_tweets (\n                  tweet_id UUID PRIMARY KEY,\n                  user_screen_name VARCHAR(50) NOT NULL,\n                  connection_id UUID NOT NULL,\n                  FOREIGN KEY(tweet_id) REFERENCES twitter_tweets(tweet_id),\n                  FOREIGN KEY(user_screen_name) REFERENCES twitter_users(user_screen_name),\n                  FOREIGN KEY(connection_id) REFERENCES twitter_users_connections(connection_id)\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_tweets_join_table]\n\n    # [START howto_operator_create_import_csv]\n    create_import_csv = SQLExecuteQueryOperator(\n        task_id=\"create-import-csv\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS csv_input (\n                  tweet_id UUID PRIMARY KEY,\n                  tweet TEXT\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_import_csv]\n\n    # [START howto_operator_transform_csv]\n    transform_csv = SQLExecuteQueryOperator(\n        task_id=\"transform-csv\",\n        sql=\"\"\"\n                INSERT INTO csv_input (tweet_id, tweet)\n                SELECT tweet_id, tweet FROM twitter_output;\n              \"\"\",\n    )\n    # [END howto_operator_transform_csv]\n\n    # [START howto_operator_split_csv]\n    split_csv = SQLExecuteQueryOperator(\n        task_id=\"split-csv\",\n        sql=\"\"\"\n                SELECT tweet, user_screen_name, timestamp\n                FROM csv_input\n                WHERE tweet_id = '{{ ts }}';\n              \"\"\",\n        parameters={\"ts\": \"{{ ds }}\"},\n    )\n    # [END howto_operator_split_csv]\n\n    # [START howto_operator_join_csv_and_twitter]\n    join_csv_and_twitter = SQLExecuteQueryOperator(\n        task_id=\"join-csv-and-twitter\",\n        sql=\"\"\"\n                SELECT tc.tweet_id, tc.tweet, tc.user_screen_name, cs.tweet as twitter_tweet, ts.timestamp\n                FROM csv_input cs\n                JOIN twitter_join_output tc ON cs.tweet_id = tc.tweet_id AND cs.timestamp = ts.timestamp;\n              \"\"\",\n    )\n    # [END howto_operator_join_csv_and_twitter]\n\n    # [START howto_operator_save_to_mysql]\n    save_to_mysql = SQLExecuteQueryOperator(\n        task_id=\"save-to-mysql\",\n        sql=\"\"\"\n                INSERT INTO twitter_output (tweet, user_id, user_screen_name, time)\n                SELECT tweet_id, user_id, user_screen_name, time\n                FROM twitter_users_tweets;\n              \"\"\",\n    )\n    # [END howto_operator_save_to_mysql]\n\n    # [START howto_operator_get_tweets_from_mysql]\n    get_tweets_from_mysql = SQLExecuteQueryOperator(\n        task_id=\"get-tweets-from-mysql\",\n        sql=\"\"\"\n                SELECT tweet, user_id, user_screen_name, time\n                FROM twitter_output;\n              \"\"\",\n    )\n    # [END howto_operator_get_tweets_from_mysql]\n\n    # [START howto_operator_drop_table_mysql]\n    drop_table_mysql = SQLExecuteQueryOperator(\n        task_id=\"drop-table-mysql\",\n        sql=\"\"\"\n                DROP TABLE IF EXISTS twitter_output;\n              \"\"\",\n    )\n    # [END howto_operator_drop_table_mysql]\n\n    # [START howto_operator_create_connection_snowflake]\n    create_connection_snowflake = SQLExecuteQueryOperator(\n        task_id=\"create-connection-snowflake\",\n        sql=\"\"\"\n                CREATE CONNECTION snowflake_connection USING snowflake (account 'account.name', schema 'hdfs');\n              \"\"\",\n    )\n    # [END howto_operator_create_connection_snowflake]\n\n    # [START howto_operator_create_table_snowflake]\n    create_table_snowflake = SQLExecuteQueryOperator(\n        task_id=\"create-table-snowflake\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS output_table (\n                  tweet_id UUID,\n                  tweet TEXT,\n                  user_id UUID,\n                  user_screen_name VARCHAR(50),\n                  time TIMESTAMP DEFAULT NOW()\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_table_snowflake]\n\n    # [START howto_operator_insert_into_snowflake]\n    insert_into_snowflake = SQLExecuteQueryOperator(\n        task_id=\"insert-into-snowflake\",\n        sql=\"\"\"\n                INSERT INTO output_table (tweet, user_id, user_screen_name)\n                VALUES ('Tweet 1', 'd7aeb3e5-bae7-4bdf-aacf-ebaefad6dd7f', 'user1');\n              \"\"\",\n    )\n    # [END howto_operator_insert_into_snowflake]\n\n    # [START howto_operator_select_from_snowflake]\n    select_from_snowflake = SQLExecuteQueryOperator(\n        task_id=\"select-from-snowflake\",\n        sql=\"\"\"\n                SELECT tweet, user_id, user_screen_name, time\n                FROM output_table;\n              \"\"\",\n    )\n    # [END howto_operator_select_from_snowflake]\n\n    # [START howto_operator_drop_table_snowflake]\n    drop_table_snowflake = SQLExecuteQueryOperator(\n        task_id=\"drop-table-snowflake\",\n        sql=\"\"\"\n                DROP TABLE IF EXISTS output_table;\n              \"\"\",\n    )\n    # [END howto_operator_drop_table_snowflake]\n\n    # [START howto_operator_get_twwits_from_snowflake]\n    get_twwits_from_snowflake = SQLExecuteQueryOperator(\n        task_id=\"get-twwits-from-snowflake\",\n        sql=\"\"\"\n                SELECT tweet, user_id, user_screen_name, time\n                FROM output_table;\n              \"\"\",\n    )\n    # [END howto_operator_get_twwits_from_snowflake]\n\n    # [START howto_operator_update_snowflake]\n    update_snowflake = SQLExecuteQueryOperator(\n        task_id=\"update-snowflake\",\n        sql=\"\"\"\n                UPDATE output_table\n                SET tweet = 'Tweet updated',\n                    time = NOW()\n                WHERE tweet = 'Tweet 1';\n              \"\"\",\n    )\n    # [END howto_operator_update_snowflake]\n\n    # [START howto_operator_drop_table_snowflake_2]\n    drop_table_snowflake_2 = SQLExecuteQueryOperator(\n        task_id=\"drop-table-snowflake-2\",\n        sql=\"\"\"\n                DROP TABLE IF EXISTS output_table;\n              \"\"\",\n    )\n    # [END howto_operator_drop_table_snowflake_2]\n\n    # [START howto_operator_create_account]\n    create_account = SQLExecuteQueryOperator(\n        task_id=\"create-account\",\n        sql=\"\"\"\n                CREATE ACCOUNT acctest;\n              \"\"\",\n    )\n    # [END howto_operator_create_account]\n\n    # [START howto_operator_grant_access_to_account]\n    grant_access_to_account = SQLExecuteQueryOperator(\n        task_id=\"grant-access-to-account\",\n        sql=\"\"\"\n                GRANT ACCESS TO ACCOUNT acctest ON DATABASE hdfs;\n              \"\"\",\n    )\n    # [END howto_operator_grant_access_to_account]\n\n    # [START howto_operator_create_import_excel]\n    create_import_excel = SQLExecuteQueryOperator(\n        task_id=\"create-import-excel\",\n        sql=\"\"\"\n                CREATE TABLE IF NOT EXISTS excel_input (\n                  tweet_id UUID PRIMARY KEY,\n                  tweet TEXT\n                );\n              \"\"\",\n    )\n    # [END howto_operator_create_import_excel]\n\n    # [START howto_operator_transform_excel]\n    transform_excel = SQLExecuteQueryOperator(\n        task_id=\"transform-excel\",\n        sql=\"\"\"\n                INSERT INTO excel_input (tweet_id, tweet)\n                SELECT tweet_id, tweet FROM twitter_output;\n              \"\"\",\n    )\n    # [END howto_operator_transform_excel]\n\n    # [START howto_operator_split_excel]\n    split_excel = SQLExecuteQueryOperator(\n        task_id=\"split-excel\",\n        sql=\"\"\"\n                SELECT tweet, user_screen_name, timestamp\n                FROM excel_input\n                WHERE tweet_id = '{{ ts }}';\n              \"\"\",\n        parameters={\"ts\": \"{{ ds }}\"},\n    )\n    # [END howto_operator_split_excel]\n\n    # [START howto_operator_join_excel_and_twitter]\n    join_excel_and_twitter = SQLExecuteQueryOperator(\n        task_id=\"join-excel-and-twitter\",\n        sql=\"\"\"\n                SELECT tc.tweet_id, tc.tweet, tc.user_screen_name, cs.tweet as twitter_tweet, ts.timestamp\n                FROM excel_input cs\n                JOIN twitter_join_output tc ON cs.tweet_id = tc.tweet_id AND cs.timestamp = ts.timestamp;\n              \"\"\",\n    )\n    # [END howto_operator_join_excel_and_twitter]\n\n    # [START howto_operator_save_to_s3]\n    save_to_s3 = SQLExecuteQueryOperator(\n        task_id=\"save-to-s3\",\n        sql=\"\"\"\n                INSERT INTO s3_output (tweet, user_id, user_screen_name, time)\n                SELECT tweet_id, user_id, user_screen_name, time\n                FROM twitter_users_tweets;\n              \"\"\",\n    )\n    # [END howto_operator_save_to_s3]\n\n    # [START howto_operator_get_tweets_from_s3]\n    get_tweets_from_s3 = SQLExecuteQueryOperator(\n        task_id=\"get-tweets-from-s3\",\n        sql=\"\"\"\n                SELECT tweet, user_id, user_screen_name, time\n                FROM s3_output;\n              \"\"\",\n    )\n    # [END howto_operator_get_tweets_from_s3]\n\n    # [START howto_operator_drop_table_s3]\n    drop_table_s3 = SQLExecuteQueryOperator(\n        task_id=\"drop-table-s3\",\n        sql=\"\"\"\n                DROP TABLE IF EXISTS s3_output;\n              \"\"\",\n    )\n    # [END howto_operator_drop_table_s3]\n\n    # [START howto_operator_create_spark_script]\n    create_spark_script = SQLExecuteQueryOperator(\n        task_id=\"create-spark-script\",\n        sql=\"\"\"\n                CREATE SPARK SCRIPT read_data(\n                  input STRING,\n                  output STRING);\n              \"\"\",\n    )\n    # [END howto_operator_create_spark_script]\n\n    # [START howto_operator_run_spark_script]\n    run_spark_script = SQLExecuteQueryOperator(\n        task_id=\"run-spark-script\",\n        sql=\"\"\"\n                RUN SPARK SCRIPT read_data(input='s3://airflow/application/output_table/', output='s3://airflow/application/tmp/');\n              \"\"\",\n    )\n    # [END howto_operator_run_spark_script]\n\n    # [START howto_operator_drop_spark_script]\n    drop_spark_script = SQLExecuteQueryOperator(\n        task_id=\"drop-spark-script\",\n        sql=\"\"\"\n                DROP SPARK SCRIPT read_data;\n              \"\"\",\n    )\n    # [END howto_operator_drop_spark_script]\n\n    # [START howto_operator_create_python_script]\n    create_python_script = SQLExecuteQueryOperator(\n        task_id=\"create-python-script\",\n        sql=\"\"\"\n                CREATE PYTHON SCRIPT extract_data(output STRING) USING PYSPARK;\n              \"\"\",\n    )\n    # [END howto_operator_create_python_script]\n\n    # [START howto_operator_run_python_script]\n    run_python_script = SQLExecuteQueryOperator(\n        task_id=\"run-python-script\",\n        sql=\"\"\"\n                RUN PYTHON SCRIPT extract_data(output='s3://airflow/application/tmp/data.json()');\n              \"\"\",\n    )\n    # [END howto_operator_run_python_script]\n\n    # [START howto_operator_drop_python_script]\n    drop_python_script = SQLExecuteQueryOperator(\n        task_id=\"drop-python-script\",\n        sql=\"\"\"\n                DROP PYTHON SCRIPT extract_data();\n              \"\"\",\n    )\n    # [END howto_operator_drop_python_script]\n\n    # [START howto_operator_create_python_class]\n    create_python_class = SQLExecuteQueryOperator(\n        task_id=\"create-python-class\",\n        sql=\"\"\"\n                CREATE PYTHON CLASS transform_data(output STRING) USING PYSPARK;\n              \"\"\",\n    )\n    # [END howto_operator_create_python_class]\n\n    # [START howto_operator_run_python_class]\n    run_python_class = SQLExecuteQueryOperator(\n        task_id=\""}], "metadata": {"file_name": "dag_426_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8925557}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a simple data storage workflow using LevelDB that demonstrates basic key-value retrieval and insertion operations. Create a DAG that shows how to get and put data into a lightweight embedded key-value database.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.hooks.level_db import LevelDbHook\nfrom airflow.providers.microsoft.azure.operators.level_db import LevelDbPutOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_level_db\"\nLOCATION = os.environ.get(\" LEVEL_DB_LOCATION \", \"us\")\n\nwith DAG(\n    f\"level_db_{ENV_ID}\",\n    default_args={\n        \"owner\": \"airflow\",\n        \"retries\": 1,\n        \"email_on_retry\": False,\n        \"email_on_failure\": False,\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n)odesk = LevelDbHook(location=LOCATION)\n\n# [START howto_operator_level_db_put]\nwith TaskGroup(\"using_hook\") as using_hook:\n    # [START howto_level_db_put_operator]\n    put_1 = LevelDbPutOperator(task_id=\"ldb_put_1\", input_values=[3, 1])\n    put_2 = LevelDbPutOperator(task_id=\"ldb_put_2\", input_values=[4, 1])\n    # [END howto_level_db_put_operator]\n\n    # [START howto_level_db_get_operator]\n    get_1 = LevelDbHook.get_key_operator(key=str(put_1.output_keys[0]), task_id=\"ldb_get_1\")\n    get_2 = LevelDbHook.get_key_operator(key=str(put_2.output_keys[0]), task_id=\"ldb_get_2\")\n    # [END howto_level_db_get_operator]\n\n    # [START howto_level_db_multiple_get_operator]\n    multiple_get_1 = LevelDbHook.multi_get_keys_operator(\n        keys=[str(put_1.output_keys[0]), str(put_2.output_keys[0])], task_id=\"ldb_multiple_get_1\"\n    )\n    # [END howto_level_db_multiple_get_operator]\n\n\nwith DAG(\n    f\"level_db_{ENV_ID}_multiple_runs\",\n    default_args={\n        \"owner\": \"airflow\",\n        \"retries\": 1,\n        \"email_on_retry\": False,\n        \"email_on_failure\": False,\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=[DAG_ID, DAG_ID],\n    tags=[\"example\"],\n    catchup=False,\n) as multi_run_dag:\n\n    # TEST BODY\n    using_hook\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(multi_run_dag)\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils import get_test_run  # noqa: E402\n\n    test_run.execute()"}], "metadata": {"file_name": "dag_950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892562}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that automatically collects, downloads, and transfers digital advertising performance data from Display Video 360 to a centralized data warehouse for comprehensive marketing analytics and reporting.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "# [START howto_video_360_transfer_operator]\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.video_360 import GoogleMarketoPlatformVideo360ListAdsOperator\nfrom airflow.providers.google.marketing_platform.sensors.video_360 import GoogleMarketoPlatformVideo360GetAdSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GMP_PROJECT_ID\", \"\")\n\nGMP_VIDEO_360_AD_LIST_ID = os.environ.get(\"GMP_VIDEO_360_AD_LIST_ID\", \"\")\nGMP_VIDEO_360_AD_ID = os.environ.get(\"GMP_VIDEO_360_AD_ID\", \"\")\n\n\nwith DAG(\n    dag_id=\"gmp_video_360_ad_list_sensor\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_marketing_platform_list_ads]\n    list_ads = GoogleMarketoPlatformVideo360ListAdsOperator(\n        task_id=\"list_ads\",\n        project_id=PROJECT_ID,\n        ad_list_id=GMP_VIDEO_360_AD_LIST_ID,\n    )\n    # [END howto_marketing_platform_list_ads]\n\n    # [START howto_marketing_platform_get_ad_sensor]\n    get_ad = GoogleMarketoPlatformVideo360GetAdSensor(\n        task_id=\"get_ad\",\n        project_id=PROJECT_ID,\n        ad_id=GMP_VIDEO_360_AD_ID,\n    )\n    # [END howto_marketing_platform_get_ad_sensor]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\n# Also need to include the following test cases in test_system_tests.py\nfrom tests.system.test_utils.system_tests import get_test_run_from_test_case  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run_from\u5148\u751f = get_test_run_from_test_case(test_run)"}], "metadata": {"file_name": "dag_46_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892568}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and communication using XComs. Create a workflow that shows how different tasks can push and pull data between each other using various methods like direct pushing, returning values, and bash operator interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of XCom in Airflow.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task\ndef push_to_xcom(value: str, task_instance):\n    \"Task to push xcom value\"\n    task_instance.xcom_push(key=\"example_xcom\", value=value)\n\n\n@task\ndef print_to_console(value: str):\n    \"Task to console log the value\"\n    print(value)\n\n\nwith DAG(\n    dag_id=\"example_xcom_bash\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    push_to_xcom(\"test value\", None)\n    print_to_console(\"Value pushed to XCom through BashOperator\")"}], "metadata": {"file_name": "dag_32_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8925714}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that dynamically creates an SQS queue, publishes multiple messages, and demonstrates different message retrieval strategies using sensors. Implement a complete workflow that covers queue creation, message publishing, message sensing, and queue deletion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\n# [START import_module]\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.sqs.sensors.sqs import SqsSensor\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [END import_module]\n\n# [START instantiate_dag]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_sqs\"\n\ns3_conn_id = \"s3_default\"\nsqs_conn_id = \"sqs_default\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n\n    @task\n    def create_s3_connection():\n        s3_hook = S3Hook()\n        conn = Connection(\n            conn_id=s3_conn_id,\n            conn_type=\"s3\",\n        )\n        conn_extra = {\n            \"aws_access_key_id\": \"AWS_ACCESS_KEY_ID\",\n            \"aws_secret_access_key\": \"AWS_SECRET_ACCESS_KEY\",\n        }\n        s3_hook.build(extra_fields=conn_extra, connection=conn)\n        session = Session()\n        logging.info(session.connect(conn.id).execute(\"describe buckets\"))\n\n    @task\n    def create_sqs_connection():\n        sqs_hook = S3Hook()\n        conn = Connection(\n            conn_id=sqs_conn_id,\n            conn_type=\"sqs\",\n        )\n        conn_extra = {\n            \"queue_url\": \"https://sqs.us-east-1.amazonaws.com/0596472383/TEST-Queue\"\n        }\n        sqs_hook.build(extra_fields=conn_extra, connection=conn)\n        session = Session()\n        logging.info(session.connect(conn.id).execute(\"list_queues\"))\n\n    @task\n    def create_s3_bucket():\n        s3_hook = S3Hook()\n        bucket_name = f\"bucket_{DAG_ID}_{ENV_ID}\"\n        s3_hook.create_bucket(Bucket=bucket_name)\n\n        return bucket_name\n\n    @task\n    def create_s3_key(bucket_name):\n        return f\"kinesis-data/{DAG_ID}_{ENV_ID}.json\"\n\n    @task\n    def get_s3_key(output_key: str | None) -> str:\n        if output_key is not None:\n            return output_key\n        return \"example-s3-sqs-output.json\"\n\n    @task\n    def write_to_s3(key: str, data: dict[str, str]):\n        s3_hook = S3Hook()\n        s3_hook.load_json_data(data, key=key, replace=True)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_s3_bucket(bucket_name: str):\n        s3_hook = S3Hook()\n        s3_hook.delete_bucket(Bucket=bucket_name)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_sqs_queue(queue_url: str):\n        sqs_hook = S3Hook()\n        sqs_hook.delete_queue(QueueUrl=queue_url)\n\n    s3_bucket = create_s3_bucket()\n    s3_key = create_s3_key(s3_bucket)\n    s3_connection = create_s3_connection()\n    sqs_bucket = create_s3_bucket()\n    sqs_key = create_s3_key(sqs_bucket)\n    sqs_connection = create_sqs_connection()\n    get_s3_key_task = get_s3_key(s3_key)\n    write_to_s3_task = write_to_s3(s3_key, json.dumps({\"P\": \"R\"}))\n    s3_key_output = get_s3_key(get_s3_key_task)\n    sqs_queue = SqsSensor(task_id=\"sqs_sensor\", sqs_queue=queue_url)\n    delete_s3_bucket_task = delete_s3_bucket(s3_bucket)\n    delete_sqs_queue_task = delete_sqs_queue(sqs_queue)\n    delete_s3_key_task = delete_s3_bucket(s3_key)\n    delete_sqs_key_task = delete_sqs_queue(sqs_key)\n\n    (\n        s3_bucket\n        >> sqs_bucket\n        >> s3_key\n        >> s3_connection\n        >> sqs_bucket\n        >> sqs_key\n        >> sqs_connection\n        >> get_s3_key_task\n        >> write_to_s3_task\n        >> sqs_queue\n        >> delete_s3_bucket_task\n        >> delete_sqs_queue_task\n        >> delete_s3_key_task\n        >> delete_sqs_key_task\n    )\n\n    # [END instantiate_dag]\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_423_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892582}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data science workflow that transforms raw training data into a deployable machine learning model with automated infrastructure provisioning and prediction capabilities. The solution should demonstrate how to take structured CSV data, train a classification model, and expose it as a real-time prediction service with automatic resource management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.sagemaker_script import (\n    SageMakerScriptOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerProcessingJobSensor,\n    SageMakerScriptSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nDAG_ID = \"example_sagemaker_processing_job\"\n\nSAMPLE_DATA_URL_KEY = \"SAMPLE_DATA_URL\"\nSAMPLE_DATA_DESTINATION_KEY = \"SAMPLE_DATA_DESTINATION\"\n\nINPUT_DATA_S3_URI = \"s3://{{ params.sample_data_url }}/alaska/train/\"\nTRAIN_DATA_FILE_NAME = \"alaska_train.csv\"\nTARGET_COLUMN = \"isDeathOccurred\"\n\n# Model artifacts are stored in S3.\nMODEL_ARTIFACTS_S3_KEY = \"model\"\n\n# 1. Create an S3 bucket to contain the code and model.\nBUCKET = \"bucket-bekah\"\nKEY = \"processing-job.py\"\n\n# 2. Upload the processing script to S3 at the bucket key path.\nPYTHON_SCRIPT_FULL_PATH = f\"s3://{BUCKET}/{KEY}\"\n# 3. Create an S3 key for the model artifact.\nMODEL_ARTIFACTS_KEY = \"model.tar.gz\"\n# 4. Store the model artifact in S3 at the bucket model_artifacts_key path.\nSAVING_MODEL_PATH = \"model.tar.gz\"\n# 5. Endpoint name\nENDPOINT_NAME = \"test-sagemaker-endpoint\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    @task\n    def create_sample_data():\n        # [START step_create_s3_bucket]\n        s3 = boto3.client(\"s3\")\n        s3.create_bucket(Bucket=BUCKET)\n\n        # [END step_create_s3_bucket]\n\n        script_path = f\"s3://{BUCKET}/{KEY}\"\n\n        input_data_file_path = f\"{script_path}/{TRAIN_DATA_FILE_NAME}\"\n\n        content = [\n            \"0,Amy,0.0,0.9,0.1,San Francisco,1234 S Market St.,San Francisco,,CA,94103,-122.4417,.5,Male,2.0,68.0,Insurance Company of North America\",\n            \"1,Bob,0.4,0.8,0.7,Toronto,321 Wood Street,,ON,M5H 1A2,-118.2439,.5,Female,3.0,74.0,Rogers Communications Inc.\",\n            \"2,Cathy,0.8,0.3,0.2,Ottawa,11 Martin Luther King Jr. Dr.,Ottawa,,ON,K1N 5H6,-73.9453,.5,Female,4.0,81.0 Ontario Provincial Police\",\n            \"3,Dave,0.2,0.7,0.2,Vancouver,\"[\"Bob's Red Lobster\", \"Starship\u592a\u7a7a\u8239\"), \"V6B 2AA,Vancouver Island\",-120.2447,.5,Male,2.0,69.0,Ryanair\",\n            \"4,Eve,0.5,0.6,0.3,Tel Aviv,Amotz Adlem Blvd.,Tel Aviv,,60361,-73.9145,.5,Female,3.0,77.0 Tel Aviv Municipality\",\n        ]\n\n        # Convert the list to a string with newline characters\n        file_content = \"\\n\".join(content)\n\n        # Open a temporary file and write the file content\n        with NamedTemporaryFile(mode=\"w\") as temp_file:\n            temp_file.write(file_content)\n            temp_file_name = temp_file.name\n\n        # Finally, upload the temporary file to the specified input data URI\n        s3.upload_file(temp_file_name, BUCKET, input_data_file_path)\n\n    create_sample_data_task = create_sample_data()\n\n    @task\n    def create_model_from_samples():\n        # [START howto_sagemaker_create_processing_job]\n        processor = \"tmx\"\n        model_output_location = f\"s3://{BUCKET}/{SAVING_MODEL_PATH}\"\n        source_data = f\"s3://{BUCKET}/{INPUT_DATA_S3_URI}{TRAIN_DATA_FILE_NAME}\"\n        target_data = f\"s3://{BUCKET}/{INPUT_DATA_S3_URI}{TRAIN_DATA_FILE_NAME}\"\n        processing_job_name = \"sm-processing-job\"\n\n        hook = SageMakerHook()\n        log.info(\"Connecting to Sagemaker using %s\", hook.sagemaker_rds_instance_endpoint_id)\n        processing_job = {\n            \"ProcessingJobName\": processing_job_name,\n            \"RoleArn\": hook.sagemaker_role_arn,\n            \"TrainingData\": {\"DataSource\": {\"S3DataSource\": {\"DataLocation\": source_data}}},\n            \"InputDataConfig\": {\n                \"Transformations\": [],\n                \"Mode\": \"TrainOnly\",\n                \"TrainingSpecification\": {\n                    \"ProcessorType\": processor,\n                    \"BaselineModel\": {\n                        \"S3Uri\": model_output_location,\n                        \"RoleArn\": hook.sagemaker_role_arn,\n                    },\n                    \"TrainingArguments\": {\n                        \"TrainingDataSampleSize\": len(content) / 100,\n                        \"TargetDatasetColumns\": TARGET_COLUMN,\n                        \"InitialTrainingDataSampleSize\": len(content) * 10 / 100,\n                    },\n                    \"BaseTransform\": {},\n                    \"OutputDataConfig\": {\n                        \"S3OutputPath\": model_output_location\n                    },\n                    \"TrainingJobName\": processing_job_name + \"-transform\",\n                    \"UserData\": f\"\"\"#!/usr/bin/env python\n                        import boto3;\n                        sm = boto3.resource('sagemaker');\n                        model = {json.dumps({\"HTTPEndpointName\": \"{{ params.endpoint_name }\"},})};\n                        sm.register_model('{json.dumps(model)}', {'datatributionSets': [{'path': '{target_data}'}]});\n                        exit(0);\n                        \"\"\"},\n                    \"TrainingJobStatus\": \"PROGRESS\",\n                    \"StoppingCondition\": {\n                        \"MinRuntimeMinutes\": 20,\n                        \"MaxRuntimeMinutes\": 60\n                    },\n                },\n            },\n        }\n\n        sagemaker_client = boto3.client(\"sagemaker\")\n        response = sagemaker_client.create_processing_job(ProcessingJobName=processing_job_name, ProcessingJobRole=sagemaker_client.ProcessingJobRoleArn, ProcessingConfiguration=processing_job)\n        log.info(\"Created processing job %s\", response[\"ProcessingJobArn\"])\n        # [END howto_sagemaker_create_processing_job]\n\n    create_model_from_samples_task = create_model_from_samples()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_model(processing_job_arn: str):\n        boto3.client(\"sagemaker\").delete_processing_job(ProcessingJobArn=processing_job_arn)\n\n    delete_model_task = delete_model(create_model_from_samples_task.output)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def clean_up_s3_bucket(bucket_name):\n        boto3.client(\"s3\").delete_bucket(Bucket=bucket_name)\n\n    clean_up_s3_bucket_task = clean_up_s3_bucket(BUCKET)\n\n    @task\n    def setup_environment(endpoint_name: str):\n        # [START howto_sagemaker_create_endpoint_config]\n        endpoint_config_name = f\"endpoint-config-{ENDPOINT_NAME}\"\n        process_transformer_on = True\n\n        hook = SageMakerHook()\n        log.info(\"Connecting to Sagemaker using %s\", hook.sagemaker_rds_instance_endpoint_id)\n\n        endpoint_config = {\n            \"EndpointConfigName\": endpoint_config_name,\n            \"ProductionVariants\": [\n                {\n                    \"VariantName\": f\"{endpoint_name}-proc\",\n                    \"ProcessingStrategy\": \"OnDemand\",\n                    \"InstanceCount\": 1,\n                    \"InitialAmazonimeCapacity\": 1,\n                    \"ProcessingRequirementInHour\": 1,\n                    \"TransformationRole\": hook.sagemaker_rds_instance_endpoint_id,\n                },\n                {\n                    \"VariantName\": f\"{endpoint_name}-transform\",\n                    \"ProcessingStrategy\": \"TransformOnly\",\n                    \"InstanceCount\": 1,\n                    \"InitialAmazonimeCapacity\": 1,\n                    \"ProcessingRequirementInHour\": 1,\n                    \"TransformationRole\": hook.sagemaker_rds_instance_endpoint_id,\n                    \"TransformatorOn\": process_transformer_on,\n                },\n            ],\n        }\n\n        try:\n            sagemaker_client = boto3.client(\"sagemaker\")\n            sagemaker_client.create_endpoint_config(endPointConfigName=endpoint_config_name, endpointConfigEndpointConfigVersion=endpoint_config_name, productionVariants=endpoint_config[\"EndpointConfigEndpointConfigVersion\"])\n        except ClientError as e:\n            if \"AlreadyExistsException\" not in str(e):\n                raise e\n        # [END howto_sagemaker_create_endpoint_config]\n\n        # [START howto_sagemaker_create_endpoint]\n        endpoint_name = f\"endpoint-{ENDPOINT_NAME}\"\n        process_transformer_on = False\n\n        hook = SageMakerHook()\n        log.info(\"Connecting to Sagemaker using %s\", hook.sagemaker_rds_instance_endpoint_id)\n\n        endpoint = {\n            \"EndpointName\": endpoint_name,\n            \"EndpointConfigName\": endpoint_config_name,\n            \"TransformersOn\": process_transformer_on,\n        }\n\n        sagemaker_client = boto3.client(\"sagemaker\")\n        sagemaker_client.create_endpoint(endpointName=endpoint_name, endpointConfigName=endpoint_config_name)\n        # [END howto_sagemaker_create_endpoint]\n\n    setup_environment_task = setup_environment(ENDPOINT_NAME)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def teardown_endpoint(endpoint_name: str):\n        # [START howto_sagemaker_delete_endpoint]\n        sagemaker_client = boto3.client(\"sagemaker\")\n\n        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n        # [END howto_sagemaker_delete_endpoint]\n\n    teardown_endpoint_task = teardown_endpoint(endpoint_name=ENDPOINT_NAME)\n\n    chain(\n        create_sample_data_task,\n        create_model_from_samples_task,\n        setup_environment_task,\n        teardown_endpoint_task,\n        teardown_endpoint(ENDPOINT_NAME),\n        delete_model_task,\n        clean_up_s3_bucket_task,\n    )\n\n    # ### Everything below this point is a performance test ###\n    chain(\n        create_sample_data_task,\n        create_model_from_samples_task,\n        setup_environment_task,\n        teardown_endpoint_task,\n        teardown_endpoint(ENDPOINT_NAME),\n        delete_model_task,\n        clean_up_s3_bucket_task,\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1928_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926067}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task management of a Google Cloud DLP job trigger, demonstrating how to create, list, retrieve, update, and delete a data inspection trigger. The workflow will serve as a technical reference for managing data protection tasks through a structured, automated pipeline.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that creates, gets, lists, updates and deletes Data Loss Prevention jobs triggers.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateJobTriggerOperator,\n    CloudDLPDeleteJobTriggerOperator,\n    CloudDLPGetJobTriggerOperator,\n    CloudDLPListJobTriggersOperator,\n    CloudDLPUpdateJobTriggerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDLP_JOB_TRIGGER_ID = f\"dlp_job_trigger_{ENV_ID}\"\nDLP_TRIGGER_TYPE = \"POLICY_BILLING\"\n\n# [START dlp_create_job_trigger]\ncreate_dlptesttrigger = CloudDLPCreateJobTriggerOperator(\n    task_id=\"dlp_create_trigger\"],\n    project_id=PROJECT_ID,\n    trigger=DLP_JOB_TRIGGER_ID,\n    trigger_type=DLP_TRIGGER_TYPE,\n    done_fn=lambda info: print(info),\n    timeout=10)\n# [END dlp_create_job_trigger]\n\n# [START dlp_get_job_trigger]\nget_dlptesttrigger = CloudDLPGetJobTriggerOperator(\n    task_id=\"dlp_get_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=DLP_JOB_TRIGGER_ID,\n)\n# [END dlp_get_job_trigger]\n\n# [START dlp_list_job_triggers]\nlist_dlptesttriggers = CloudDLPListJobTriggersOperator(\n    task_id=\"dlp_list_trigger\",\n    project_id=PROJECT_ID,\n)\n# [END dlp_list_job_triggers]\n\n# [START dlp_update_job_trigger]\nupdate_dlptesttrigger = CloudDLPUpdateJobTriggerOperator(\n    task_id=\"dlp_update_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=DLP_JOB_TRIGGER_ID,\n    name=DLP_JOB_TRIGGER_ID,\n    display_name=f\"updated-dlp-trigger-{ENV_ID}\",\n    description=f\"description-of-update-dlp-trigger-{ENV_ID}\",\n    trigger_type=\"ABSTRACT_SOLICITATION\",\n)\n# [END dlp_update_job_trigger]\n\n# [START dlp_delete_job_trigger]\ndelete_dlptesttrigger = CloudDLPDeleteJobTriggerOperator(\n    task_id=\"dlp_delete_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=DLP_JOB_TRIGGER_ID,\n    trigger_display_name=f\"dlp_trigger_delete_{ENV_ID}\",\n    is_delete_if_exists=False,\n    force_delete=True,\n    redirect_ui=True,\n)\n# [END dlp_delete_job_trigger]\n\n\nwith DAG(\n    \"dlp-job-triggers\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp-job-triggers\"],\n) as dag:\n    from tests_common.test_utils.watcher import watcher\n\n    # This test watcher needs to monitor [[task_id], [operator]] in the execute function\n    listtasks = watcher(dag.tasks)\n\n    # Test parameters\n    env_id = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n    project_id = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n    DLP_LOCATION = \"us-central1\"\n    # [START howto_operator_gcp_dlp_create_job_trigger]\n    create_dlptesttrigger_task = CloudDLPCreateJobTriggerOperator(\n        task_id=\"create_dlptesttrigger\",\n        project_id=project_id,\n        trigger=DLP_JOB_TRIGGER_ID,\n        trigger_type=DLP_TRIGGER_TYPE,\n        location=DLP_LOCATION,\n        done_fn=lambda info: print(info),\n        timeout=10,\n    )\n    # [END howto_operator_gcp_dlp_create_job_trigger]\n    # [START howto_operator_gcp_dlp_get_job_trigger]\n    get_dlptesttrigger_task = CloudDLPGetJobTriggerOperator(\n        task_id=\"get_dlptesttrigger\",\n        project_id=project_id,\n        trigger_id=DLP_JOB_TRIGGER_ID,\n    )\n    # [END howto_operator_gcp_dlp_get_job_trigger]\n    # [START howto_operator_gcp_dlp_list_job_triggers]\n    list_dlptesttriggers_task = CloudDLPListJobTriggersOperator(\n        task_id=\"list_dlptesttriggers\",\n        project_id=project_id,\n    )\n    # [END howto_operator_gcp_dlp_list_job_triggers]\n    # [START howto_operator_gcp_dlp_update_job_trigger]\n    update_dlptesttrigger_task = CloudDLPUpdateJobTriggerOperator(\n        task_id=\"update_dlptesttrigger\",\n        project_id=project_id,\n        trigger_id=DLP_JOB_TRIGGER_ID,\n        name=DLP_JOB_TRIGGER_ID,\n        display_name=f\"updated-dlp-trigger-{env_id}\",\n        description=f\"description-of-update-dlp-trigger-{env_id}\",\n        trigger_type=\"ABSTRACT_SOLICITATION\",\n    )\n    # [END howto_operator_gcp_dlp_update_job_trigger]\n    # [START howto_operator_gcp_dlp_delete_job_trigger]\n    delete_dlptesttrigger_task = CloudDLPDeleteJobTriggerOperator(\n        task_id=\"delete_dlptesttrigger\",\n        project_id=project_id,\n        trigger_id=DLP_JOB_TRIGGER_ID,\n        trigger_display_name=f\"dlp_trigger_delete_{env_id}\",\n        is_delete_if_exists=False,\n        force_delete=True,\n        redirect_ui=True,\n    )\n    # [END howto_operator_gcp_dlp_delete_job_trigger]\n    from tests_common.test_utils.watcher import watcher\n\n    # This test watcher needs to monitor [[task_id], [operator]] in the execute function\n    listtasks >> [create_dlptesttrigger_task, get_dlptesttrigger_task, list_dlptesttriggers_task]\n    [update_dlptesttrigger_task, delete_dlptesttrigger_task] >> tearDownTask\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2015_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892618}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that illustrates the interaction between local file systems and cloud storage using Azure-specific operators for file transfer and deletion. Develop a simple demonstration of how data can be moved and managed programmatically in a cloud infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.advisor import AzureAdvisorListPatchesOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to azure_blob import LocalFileSystemToAzureBlobStorageOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_local import LocalFilesystemToLocalDirectoryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_advisor\"\n\nPATCH_ID = os.environ.get(\"EXAMPLE_ADVISOR_PATCH_ID\", \"advisory-patch-2022-12-31\")\nBLOB_NAME = \"airflow-advisor-upload.txt\"\nLOCAL_PATH = \"azure_patch_download.txt\"\nDOWNLOAD_TARGET_PATH = \"/tmp/\"\nREMOTE_PATH = \"/tmp/\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    @task\n    def create_connection():\n        conn = Connection(\n            conn_id=\"azure_default\",\n            conn_type=\"azure_arm\",\n        )\n        conn_extra = {\n            \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n            \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP\"),\n            \"workspace_name\": os.environ.get(\"ADVISOR_WORKSPACE\"),\n        }\n        conn_extra_json = json.dumps(conn_extra)\n        conn.set_extra(conn_extra_json)\n\n        return conn\n\n    upload_patch_to_blob_storage = LocalFilesystemToAzureBlobStorageOperator(\n        task_id=\"upload_patch_to_blob_storage\",\n        filename=LOCAL_PATH,\n        blob_name=BLOB_NAME,\n        container_name=\"advisor-patch\",\n        use_container_access=True,\n    )\n\n    # [START howto_operator_advisor]\n    get_patches = AzureAdvisorListPatchesOperator(task_id=\"get_patches\", filter_str=f\"workspace={os.environ.get('ADVISOR_WORKSPACE')}\")\n    # [END howto_operator_advisor]\n\n    download_blob_to_local = LocalFilesystemToLocalDirectoryOperator(\n        task_id=\"download_blob_to_local\",\n        src=f\"blob://{os.environ.get('RESOURCE_GROUP')}/{os.environ.get('SUBSCRIPTION_ID')}/advisor-patch/{BLOB_NAME}\",\n        dst=DOWNLOAD_TARGET_PATH,\n    )\n\n    delete_patch_from advisor = AzureAdvisorListPatchesOperator(\n        task_id=\"delete_patch_from_advisor\",\n        filter_str=f\"workspace={os.environ.get('ADVISOR_WORKSPACE')} and id={PATCH_ID}\",\n        is_delete_object(True),\n    )\n\n    delete_connection = create_connection()\n    (\n        # TASKS\n        create_connection()\n        >> upload_patch_to_blob_storage\n        >> get_patches\n        >> download_blob_to_local\n        >> delete_patch_from advisor\n        >> delete_connection\n        # TRIGGERS\n        >> trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_963_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926258}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that illustrates the sequential task execution pattern for cloud infrastructure management, specifically demonstrating how to programmatically interact with Alibaba OSS bucket resources. Develop a simple workflow that shows task dependencies and cloud service interactions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import AliyunOSSCreateBucketOperator, OSSDeleteBucketOperator\nfrom airflow.providers.alibaba.cloud.operators.vpc import (\n    AliyunVPCCreateOperator,\n    AliyunVPCDeleteOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oss_vpc\"\n\nbucket_name = f\"bucket-oss-vpc-{ENV_ID}\"\nvpc_id = os.environ.get(\"VPC_ID\", \"vhvx-vpc-1\")\nregion = \"default\"\nnetwork = \"public\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": bucket_name},\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    create_vpc = AliyunVPCCreateOperator(task_id=\"create_vpc\", vpc_id=vpc_id, region=region)\n\n    create_bucket = AliyunOSSCreateBucketOperator(\n        task_id=\"create_bucket\", bucket_name=bucket_name, region=region\n    )\n\n    delete_bucket = OSSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket=name, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_vpc = AliyunVPCDeleteOperator(task_id=\"delete_vpc\", vpc_id=vpc_id, region=region)\n\n    (\n        # TEST SETUP\n        create_vpc\n        >> create_bucket\n        # TEST BODY\n        >> delete_bucket\n        # TEST TEARDOWN\n        >> delete_vpc\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_54_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926306}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing workflow that shows how to execute database transactions using Oracle connections, highlighting different parameter input strategies for stored procedures. Create a DAG that explores various methods of executing database operations with input and output parameter handling.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oracle_stored_procedure\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_oracle_stored_procedure_operator]\n    # [START howto_oracle_stored_procedure_operator_with_params]\n    oracles_stored_procedure_task = OracleStoredProcedureOperator(\n        task_id=\"oracles_stored_procedure\",\n        oracle_conn_id=\"oracle_default\",\n        procedure=\"TEST_PROCEDURE',\n                    P_PARAM1='VALUE1',\n                    P_PARAM2='VALUE2'\",\n        dag=dag,\n    )\n    # [END howto_oracle_stored_procedure_operator_with_params]\n    # [END howto_oracle_stored_procedure_operator]\n\n\nfrom tests.system.utils import get_test_run From tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_708_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926344}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data ingestion pipeline that transforms a local CSV file containing US state information into a structured BigQuery table using Google Cloud data transfer services. Implement a systematic approach to moving structured data from local storage to cloud-based analytical platforms.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToBigQueryOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\ntry:\n    from airflow.utils.trigger_rule import TriggerRule\nexcept ImportError:\n    # patch for airflow <2.7\n    from airflow.triggers.trigger import TriggerRule\n\npytest.skipif_missing_google_cloud_components()\n\ntest_gcp_project = os.environ.get(\"TEST_GCP_PROJECT\", \"example-project\")\n\nwith DAG(\n    dag_id=\"gcs_to_bigquery_operator\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcs_to_bigquery]\n    gcs_to_bigquery = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"employees.csv\"],\n        destination_table=\"employee_table\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n        project_id=test_gcp_project,\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    trigger = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcs_to_bigquery_deferrable]\n    gcs_to_bigquery_def = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bigquery_example_def\",\n        bucket=\"cloud-samples-data\",\n        source_objects=[\"employees.csv\"],\n        destination_table=\"employee_table\",\n        schema_fields=[\n            {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n        is_deferred=True,\n        project_id=test_gcp_project,\n    )\n    # [END howto_operator_gcs_to_bigquery_deferrable]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_809_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926408}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure-as-code workflow for dynamically creating and managing a distributed caching service with precise configuration controls. Develop a DAG that illustrates how to programmatically set up a scalable in-memory data storage solution with specific node and memory configurations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"Example Airflow DAG demonstrating usage of the Cloud Memorystore Manager operators.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.cloud_memorystore import (\n    CloudMemorystoreDenseQdrantOperator,\n    CloudMemorystoreManageInstancePointsOperator,\n    CloudMemorystoreManagePoolOperator,\n    CloudMemorystoreCreateDbOperator,\n    CloudMemorystoreDeleteDbOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_cloud_memorystore_manage_points\"\n\nLOCATION = \"europe-north1\"\nMEMORYSTORE_NAME = f\"memorystore_{ENV_ID}\"\nPOINT_NAME = \"test-points\"\nPOOL_NAME = f\"pool_{ENV_ID}\"\nPOOL_DESCRIPTION = \"Test pool\"\nQDRANT_INDEX_NAME = \"test-index\"\nREGION = \"us\"\n\nTENANCY = \"t4c-standard\"\n\n# [START howto_operator_gcp_cm_create_db]\nTENANCY_TYPE = TENANCY\n\nCREATE_DB_BODY = {\n    \"location\": LOCATION,\n    \"memorystore_id\": MEMORYSTORE_NAME,\n    \"points_config\": {\"qdrant\": {\"index_name\": QDRANK_INDEX_NAME}},\n}\n\n# [END howto_operator_gcp_cm_create_db]\n\n# [START howto_operator_gcp_cm_delete_db]\nDELETE_DB_BODY = {\"memorystore_id\": MEMORYSTORE_NAME, \"points_config_qdrant\": {\"index_name\": QDRANK_INDEX_NAME}}\n# [END howto_operator_gcp_cm_delete_db]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"cloud-memorystore\"],\n) as dag:\n    # [START howto_operator_gcp_cm_create_db_async]\n    create_memory_store_db_async = CloudMemorystoreCreateDbOperator(\n        task_id=\"create_memory_store_db_async\",\n        location=LOCATION,\n        body=CREATE_DB_BODY,\n        tenancy=TENANCY_TYPE,\n        deferrable=True,\n    )\n    # [END howto_operator_gcp_cm_create_db_async]\n\n    # [START howto_operator_gcp_cm_list_pools]\n    list_pools = CloudMemorystoreManagePoolOperator(task_id=\"list_pools\", location=LOCATION)\n    # [END howto_operator_gcp_cm_list_pools]\n\n    # [START howto_operator_gcp_cm_get_pool]\n    get_pool = CloudMemorystoreManagePoolOperator(task_id=\"get_pool\", location=LOCATION, pool_id=POOL_NAME)\n    # [END howto_operator_gcp_cm_get_pool]\n\n    # [START howto_operator_gcp_cm_delete_db]\n    delete_memory_store_db = CloudMemorystoreDeleteDbOperator(\n        task_id=\"delete_memory_store_db\", location=LOCATION, db_name=MEMORYSTORE_NAME\n    )\n    # [END howto_operator_gcp_cm_delete_db]\n\n    # [START howto_operator_gcp_cm_create_point]\n    create_point = CloudMemorystoreManageInstancePointsOperator(\n        task_id=\"create_point\",\n        location=LOCATION,\n        instance=MEMORYSTORE_NAME,\n        body={\"points\": [{\"qdrant\": {\"index_name\": QDRANK_INDEX_NAME}}]},\n    )\n    # [END howto_operator_gcp_cm_create_point]\n\n    # [START howto_operator_gcp_cm_list_points]\n    list_points = CloudMemorystoreManageInstancePointsOperator(\n        task_id=\"list_points\", location=LOCATION, instance=MEMORYSTORE_NAME\n    )\n    # [END howto_operator_gcp_cm_list_points]\n\n    # [START howto_operator_gcp_cm_get_point]\n    get_point = CloudMemorystoreManageInstancePointsOperator(\n        task_id=\"get_point\",\n        location=LOCATION,\n        instance=MEMORYSTORE_NAME,\n        point_id=\"{{ task_instance.xcom_pull('create_point')['inserted_ids'][0] }}\",\n    )\n    # [END howto_operator_gcp_cm_get_point]\n\n    # [START howto_operator_gcp_cm_delete_point]\n    delete_point = CloudMemorystoreManageInstancePointsOperator(\n        task_id=\"delete_point\",\n        location=LOCATION,\n        instance=MEMORYSTORE_NAME,\n        point_id=\"{{ task_instance.xcom_pull('create_point')['inserted_ids'][0] }}\",\n    )\n    # [END howto_operator_gcp_cm_delete_point]\n\n    create_memory_store_db_async >> list_pools >> get_pool >> delete_memory_store_db\n    create_point >> list_points >> get_point >> delete_point\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1588_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926501}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the technical process of cloud file operations, including creating a GCS bucket, uploading a local file, downloading the file, and performing cleanup operations. Implement the workflow to demonstrate inter-system file transfer and management using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFileSystemToGCSOperator\nfrom airflow.providers.google.suite.sensors.gcs import GCSSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"gcs_sensor_example\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\nUPLOAD_FILE_PATH = \"/tmp/\"\nLOCAL_FILE_PATH = UPLOAD_FILE_PATH + FILE_NAME\n\n\n@task\ndef create_file():\n    with open(LOCAL_FILE_PATH) as file:\n        file_content = \"Learning my world is fun!\"\n        print(\"Writing content to\", LOCAL_FILE_PATH, \": \", file_content)\n        file.write(file_content)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_bucket():\n    print(f\"Deleting bucket {BUCKET_NAME}\")\n    gcs_hook = GCSHook()\n    gcs_hook.delete_bucket(project_id=PROJECT_ID, bucket_name=BUCKET_NAME)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is\n    GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n    )\n    create_file()\n    LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=LOCAL_FILE_PATH,\n        dst=FILE_NAME,\n        bucket=BUCKET_NAME,\n    )\n    # [START howto_sensor_gcs_before]\n    before_close_sensor = GCSSensor(\n        task_id=\"wait_for_file\",\n        bucket=BUCKET_NAME,\n        object_name=FILE_NAME,\n        deferrable=True,\n        deferrable_timeout=-1,\n    )\n    # [END howto_sensor_gcs_before]\n    # [START howto_sensor_gcs_after]\n    after_close_sensor = GCSSensor(\n        task_id=\"wait_for_file_succeeded\",\n        bucket=BUCKET_NAME,\n        object_name=FILE_NAME,\n        deferrable=True,\n        deferrable_timeout=-1,\n    )\n    # [END howto_sensor_gcs_after]\n    before_close_sensor >> after_close_sensor\n    delete_bucket()\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done when\n    # everything has finished.\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1657_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926568}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an event-driven Airflow DAG that demonstrates message production and consumption using Kafka, with a focus on creating a sensor that triggers specific actions based on number characteristics. Implement a data pipeline that showcases asynchronous message processing and conditional event handling.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from airflow.providers.standard.operators.bash import BashOperator\n\nfor i in range(5):\n    t1 = BashOperator(\n        task_id=f'print_{i}',\n        bash_command='echo [ {{task_instance}} ]',\n        dag=dag\n    )\n\n    t1 >> BashOperator(\n        task_id='sleep_{{task_instance}}',\n        bash_command='sleep 20',\n        dag=dag\n    )"}], "metadata": {"file_name": "dag_1942_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926601}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates converting text to speech using Google Cloud Text-to-Speech service and storing the audio file in a Google Cloud Storage bucket. The DAG should showcase creating a bucket, generating an audio file from sample text, and then cleaning up the bucket.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import (\n    CloudTextToSpeechSynthesizeOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_text_to_speech\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_synthesize_text_arguments_text]\ntext = {\"text\": \"Sample text for demo purposes\"}\n# [END howto_synthesize_text_arguments_text]\n\n# [START howto_synthesize_text_argument_encoding]\naudio = {\"encoding\": \"LINEAR16\"}\n# [END howto_synthesize_text_argument_encoding]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_text_to_speech_synthesize_task]\n    text_to_speech_synthesize_task = CloudTextToSpeechSynthesizeOperator(\n        task_id=\"text_to_speech_synthesize_task\",\n        project_id=PROJECT_ID,\n        input_config=text,\n        output_uri_template=f\"gs://{BUCKET_NAME}/temp/\",\n    )\n    # [END howto_text_to_speech_synthesize_task]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> text_to_speech_synthesize_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_572_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926659}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that demonstrates dynamic table creation and transformation in BigQuery with multiple data object types. Implement a workflow that shows how to create, modify, and manage different table representations including standard tables, views, and materialized views.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n    BigQuerySelectOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.utils.trigger_rule import TriggerDagRule  # type: ignore[attr-defined]\nexcept ImportError:\n    # Backport of `dag_trigger_rule` in Airflow < 2.8\n    TriggerDagRule = TriggerRule.ALL_DONE\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"bigquery_jobs\"\nLOCATION = \"europe-west1\"\n\nTABLE_1_ID = f\"table_{DAG_ID}_{ENV_ID}\"\nTABLE_2_ID = f\"table_{DAG_ID}_{ENV_ID}'\n\nSCHEMA = [\n    {\"name\": \"value\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n    {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    {\"name\": \"ds\", \"type\": \"DATE\", \"mode\": \"NULLABLE\"},\n]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"bigquery\"],\n) as dag:\n\n    create_table_1_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_table_1_dataset\", dataset_id=TABLE_1_ID, location=LOCATION\n    )\n\n    create_table_2_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_table_2_dataset\", dataset_id=TABLE_2_ID, location=LOCATION\n    )\n\n    create_table_1 = BigQueryCreateTableOperator(\n        task_id=\"create_table_1\",\n        dataset_id=TABLE_1_ID,\n        table_id=TABLE_1_ID,\n        schema_fields=SCHEMA,\n        time_partitioning={\n            \"type\": \"DAY\",\n            \"field\": \"ds\",\n        },\n    )\n\n    create_table_2 = BigQueryCreateTableOperator(\n        task_id=\"create_table_2\",\n        dataset_id=TABLE_2_ID,\n        table_id=TABLE_2_ID,\n        schema_fields=SCHEMA,\n    )\n\n    # [START howto_bigquery_job_api_create_query_job]\n    run_query_job = BigQueryInsertJobOperator(\n        task_id=\"run_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"SELECT count(1) FROM `test-gcs-BCVz3.table_A`\",\n                \"useLegacySql\": False,\n            }\n        },\n        location=LOCATION,\n    )\n    # [END howto_bigquery_job_api_create_query_job]\n\n    # [START howto_bigquery_job_api_insert_empty_query_job]\n    insert_empty_query_job = BigQueryInsertJobOperator(\n        task_id=\"insert_empty_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"\",\n                \"useLegacySql\": False,\n            }\n        },\n        location=LOCATION,\n    )\n    # [END howto_bigquery_job_api_insert_empty_query_job]\n\n    # [START howto_bigquery_job_api_read_data_for_a_query_job]\n    read_data_for_query_job = BigQuerySelectOperator(\n        task_id=\"read_data_for_query_job\",\n        sql=f\"SELECT count(1) FROM `{project}`.`{dataset}`.`{table}\"`\n        trigger_rule=TriggerDagRule,\n    )\n    # [END howto_bigquery_job_api_read_data_for_query_job]\n\n    delete_table_1 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_table_1\", dataset_id=TABLE_1_ID, trigger_rule=TriggerDagRule\n    )\n\n    delete_table_2 = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_table_2\", dataset_id=TABLE_2_ID, trigger_rule=TriggerDagRule\n    )\n\n    (\n        # TEST SETUP\n        create_table_1_dataset\n        >> create_table_2_dataset\n        >> create_table_1\n        >> create_table_2\n        # TEST BODY\n        >> run_query_job\n        >> insert_empty_query_job\n        >> read_data_for_query_job\n        # TEST TEARDOWN\n        >> delete_table_1\n        >> delete_table_2\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_151_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.892677}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data migration pipeline that moves spreadsheet data between Google Sheets and Google Cloud Storage using Airflow's task orchestration capabilities. Create a systematic process for extracting spreadsheet data, transferring it to cloud storage, and then reimporting the data into a new spreadsheet environment.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.suite.transfers.spreadsheet_to_gcs import SpreadsheetToGCSOperator\nfrom airflow.providers.google.suite.transfers.gcs_to_spreadsheet import GCSToSpreadsheetOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_spreadsheet_to_gcs\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_spreadsheet_to_gcs]\n    google_sheetsSpreadsheetKeysInList = [\n        {\"key\": \"b835182d-76f1-4df7-b9c8-aa3dd7a7b6b3\"},\n        {\"key\": \"6ca875ed-c1e9-488d-aebc-71baeaedd86d\"},\n    ]\n\n    gcs_bucket = \"airflow-system-tests-resources\"\n    gcs_object = \"spreadsheet_to_gcs.csv\"\n\n    update_spreadsheet = SpreadsheetToGCSOperator(\n        task_id=\"update_spreadsheet\",\n        google_sheets_spreadsheet_keys=google_sheetsSpreadsheetKeysInList,\n        gcs_bucket=gcs_bucket,\n        gcs_object=gcs_object,\n        replace=True,\n    )\n    # [END howto_operator_spreadsheet_to_gcs]\n\n    # [START howto_operator_gcs_to_spreadsheet]\n    get_spreadsheet = GCSToSpreadsheetOperator(\n        task_id=\"get_spreadsheet\",\n        source_objects=[f\"gs://{gcs_bucket}/{gcs_object}\"],\n        include_metadata=True,\n    )\n    # [END howto_operator_gcs_to_spreadsheet]\n\n    get_spreadsheet >> update_spreadsheet\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) | watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_895_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983127.8926828}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I modify the given Python code to handle a list of dictionaries with potential missing keys? When processing each dictionary, I want to ensure that if a key is missing, it returns a default value instead of throwing an error."}, {"role": "assistant", "content": "from collections import defaultdict\n\ndef process_dictionaries(dicts_list):\n    # Initialize a defaultdict with a default value of 0 for any missing key\n    result = defaultdict(int)\n    \n    # Iterate over each dictionary in the list\n    for d in dicts_list:\n        # Update the result with values from the current dictionary\n        result.update(d)\n    \n    return dict(result)\n\n# Example usage:\ninput_dicts = [\n    {'a': 1, 'b': 2},\n    {'c': 3},\n    {'d': 4, 'e': 5}\n]\n\noutput_dict = process_dictionaries(input_dicts)\nprint(output_dict)  # Output: {1: 1, 2: 1, 3: 3, 4: 4, 5: 5}\n\n# If a key is missing, it will be set to 0 by the defaultdict"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591516}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a MySQL database to Google Cloud Storage using an automated, one-time batch process. Create a DAG that demonstrates cloud data extraction and storage with built-in bucket management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using MySqlToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.mysql_to_gcs import MySqlToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_mysql_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"test-airflow.xlsx\"\nSQL = \"SELECT * FROM test_table\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,  # It will not work if set to True\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    mysql_to_gcs_task = MySqlToGCSOperator(\n        task_id=\"mysql_to_gcs_task\",\n        sql=SQL,\n        destination_bucket=BUCKET_NAME,\n        filename=FILE_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> mysql_to_gcs_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_220_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591597}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data movement process that extracts salary records from a BigQuery table and loads them into a PostgreSQL destination table. Develop a DAG that handles data transfer with configurable parameters and includes robust error handling and resource management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToPostgresOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pgdb.hooks.pgdb import PgDBHook\nfrom airflow.providers.pgdb.operators.pgdb import PGDBOperator\nfrom airflow.providers.pgdb.operators.pgdb_xcom import PGDBXComTaskOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_postgres\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcs_to_postgres]\n    gcs_to_postgres = PGDBOperator(\n        task_id=\"gcs_to_postgres\",\n        sql=\"\"\"\n            INSERT INTO my_table (num, name)\n            VALUES (1, 'One'), (2, 'Two'), (3, 'Three');\n          \"\"\",\n        pgdb_hook_args={\"conn_info\": f\"postgres://{PgDBHook().impersonate_user_login}\"},\n    )\n    # [END howto_operator_gcs_to_postgres]\n\n    # [START howto_operator_gcs_to_postgres_xcom]\n    gcs_to_postgres_xcom = PGDBXComTaskOperator(\n        task_id=\"gcs_to_postgres_xcom\",\n        sql=\"\"\"\n            INSERT INTO my_table (num, name)\n            VALUES (4, 'Four');\n          \"\"\",\n        pgdb_hook_args={\"conn_info\": f\"postgres://{PgDBHook().impersonate_user_login}\"},\n        xcom_task_id=\"return_the_data\"),\n        do_xcom_push=True,\n    )\n    # [END howto_operator_gcs_to_postgres_xcom]\n\n    # [START howto_operator_gcs_to_postgres_inlet_xcom]\n    gcs_to_postgres_inlet_xcom = PGDBXComTaskOperator(\n        task_id=\"gcs_to_postgres_inlet_xcom\",\n        sql=\"\"\"\n            INSERT INTO my_table (num, name)\n            VALUES (5, 'Five');\n          \"\"\",\n        pgdb_hook_args={\"conn_info\": f\"postgres://{PgDBHook().impersonate_user_login}\"},\n        inlets_xcom_key=\"return_the_data\",\n    )\n    # [END howto_operator_gcs_to_postgres_inlet_xcom]\n\n    gcs_to_postgres >> gcs_to_postgres_xcom >> gcs_to_postgres_inlet_xcom\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_823_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591682}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide me with a Python code snippet that demonstrates how to use the `pandas` library to read a CSV file and filter rows based on a specific condition?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Load the CSV file into a DataFrame\nfile_path = 'your_file.csv'  # Replace with your actual file path\ndf = pd.read_csv(file_path)\n\n# Define the column name and the value to filter by\ncolumn_name = 'Age'\nfilter_value = 30\n\n# Apply the filter condition\nfiltered_df = df[df[column_name] >= filter_value]\n\n# Display the filtered DataFrame\nprint(filtered_df)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591773}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates parameter passing and environment variable testing in Airflow, showcasing different methods of injecting runtime configuration into tasks. Create a DAG that illustrates how parameters can be dynamically passed and accessed across Python and Bash tasks.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of params in templated arguments.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"print_params\")\ndef print_params(params):\n    print(f\"vendma = {params['vendma']}\")\n    print(f\"firma = {params['firma']}\")\n\n\nwith DAG(\n    \"example_temporal_params\",\n    schedule=datetime.timedelta(hours=2),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    print_params({\"vendma\": \"VENDMA\", \"firma\": \"FIRMA\"})"}], "metadata": {"file_name": "dag_339_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.65918}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that uploads a local file to Azure Data Lake Storage and then immediately deletes the uploaded file. Create a DAG that demonstrates file movement and cleanup in a cloud storage environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\n\n\n@task(task_id=\"adls_del\")\ndef delete_file():\n    path = \"folder_which_contain_other_files\"\n    file_name = \"testfile.txt\"\n    print(\"Deleting file:\", f\"{path}/{file_name}\")\n    return True\n\n\nwith DAG(\n    dag_id=\"example_adls_delete\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    delete_file() >> adls_delete = ADLSDeleteOperator(task_id=\"adls_del\", path=\"folder_which_contain_other_files\")"}], "metadata": {"file_name": "dag_1750_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591842}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data processing pipeline that demonstrates Google Cloud Dataflow's workflow for batch text analysis, using a word count example with a sample text file. Design an Airflow DAG that manages the entire lifecycle of creating, running, and deleting a Dataflow pipeline while handling Google Cloud Storage operations.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreateBatchOperator,\n    DataflowDeleteBatchOperator,\n    DataflowRunBatchOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_dataflow_batch\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nBATCH_NAME = \"batch-12345\"\n\nLOCATION = \"europe-west1\"\nCLUSTER_ID = f\"cluster-{DAG_ID}-{ENV_ID}\"\nWORKER_COUNT = 1\nFLAVOR_ID = f\"flavor-{DAG_ID}-{ENV_ID}\"\n\n\nBATCH_PIPELINE_OPTIONS = {\n    \"staging_bucket\": BUCKET_NAME,\n    \"dataflow_default_options.location\": LOCATION,\n    \"dataflow_default_options.cluster\": CLUSTER_ID,\n    \"dataflow_default_options.workers\": WORKER_COUNT,\n    \"dataflow_default_options.flavor_id\": FLAVOR_ID,\n}\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataflow-batch\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_run_dataflow_batch]\n    run_batch = DataflowRunBatchOperator(\n        task_id=\"run_batch\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        body=BATCH_PIPELINE_OPTIONS,\n        batch_name=BATCH_NAME,\n    )\n    # [END howto_operator_run_dataflow_batch]\n\n    # [START howto_operator_create_dataflow_batch_result]\n    create_result = DataflowCreateBatchOperator(\n        task_id=\"create_result\", batch_name=BATCH_NAME, project_id=PROJECT_ID\n    )\n    # [END howto_operator_create_dataflow_batch_result]\n\n    # [START howto_operator_get_dataflow_batch_result]\n    get_result = DataflowRunBatchOperator(\n        task_id=\"get_result\",\n        batch_name=\"{{ task_instance.xcom_pull('create_result')['name'] }}\",\n    )\n    # [END howto_operator_get_dataflow_batch_result]\n\n    # [START howto_operator_list_dataflow_batches]\n    list_batches = DataflowRunBatchOperator(\n        task_id=\"list_batches\", body={\"backupid\": BATCH_NAME}\n    )\n    # [END start_dataflow_batch]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> run_batch\n        # TEST TEARDOWN\n        >> [create_result, get_result, list_batches]\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1216_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6591926}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data workflow that illustrates how different data assets can be managed and scheduled using modern asset-based decorators and S3 storage. Create a pipeline that shows the relationship between data producers and consumers with minimal but clear processing steps.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_s3_to_sql\"\n\nS3_BUCKET = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nSQL_TABLE = \"test_table\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=S3_BUCKET)\n\n    # [START howto_transfer_s3_to_sql]\n    load_data_to_sql = S3ToSqlOperator(\n        task_id=\"load_data_to_sql\",\n        s3_bucket=S3_BUCKET,\n        sql=f\"INSERT INTO {SQL_TABLE} VALUES (1, '2', '3'), (4, '5', '6');\",\n    )\n    # [END howto_transfer_s3_to_sql]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\", bucket_name=S3_BUCKET, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        # TEST BODY\n        >> load_data_to_sql\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1844_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.659198}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an example data pipeline that illustrates how to orchestrate system-level tasks, including time-based scheduling, command execution, and basic error handling in a distributed computing environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"singularity_conn_id\",\n        conn_type=\"file\",\n        host=\"/opt/bin/singularity\",\n        login=None,\n        password=None,\n        port=0,\n    )\n\n    t1 = TaskCreateSingularityImage()\n    t2 = TaskRunSingularityImage()\n    t3 = TaskRunSingularityImageFromFile()\n\n    @task\n    def TaskCreateSingularityImage():\n        conn_id = \"singularity_conn_id\"\n        image_name = \"test-image-test-name\"\n        singularity_file_path = \"/tmp/testSingularityFile.sif\"\n        open(singularity_file_path, \"w\").write(f\"description my description in singularity {{}}\")\n        cmd = f\"zeugman run --image={conn_id}://{image_name} {singularity_file_path}\"\n        print(cmd)\n        os.system(cmd)\n\n    @task\n    def TaskRunSingularityImage():\n        conn_id = \"singularity_conn_id\"\n        image_name = \"test-image-test-name\"\n        cmd = f\"zeugman run --image={conn_id}/{image_name} /bin/echo hello, world\"\n        print(cmd)\n        os.system(cmd)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_connection(conn_id):\n        conn.delete()\n\n    t1 >> [t2, t3] >> delete_connection(conn_id)\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_648_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592038}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud infrastructure management by dynamically creating a Google Cloud Storage bucket, transferring data, and then cleaning up the bucket after the operation. Implement a sequential task flow that handles resource provisioning, data movement, and resource teardown.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_copy_disk\"\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nRESOURCE_NAME_SRC = f\"instance-src-{DAG_ID}\".replace(\"_\", \"-\")\nRESOURCE_NAME_DST = f\"instance-dst-{DAG_ID}\".replace(\"_\", \"-\")\n\n# [START howto_operator_gce_copy_disk_setup]\nDEPLOYED_INSTANCE_NAME = f\"{REGION}-{DAG_ID}\"\nCREATE_TIME = datetime.now()\nDELETE_TIME = CREATE_TIME + timedelta(days=1)\n\nDISK_NAME = \"disk-1\"\nDISK_URI = f\"gs://{ENDPOINT}/{DISK_NAME}\"\n\nGCE_MACHINE_TYPE = \"n1-standard-1\"\nGCE_DISK_SIZE = 10\nGCE_NETWORK_NAME = \"default\"\n\nDRIVER_PY_MODULE = \"airflow.providers.google.cloud.operators.compute.copy_disk.CopyDiskDriver\"\n# [END howto_operator_gce_copy_disk_setup]\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    schedule=\"@once\",\n    tags=[\"example\"],\n) is:\n    # [START howto_operator_gce_install_driver]\n    install_driver_task = ComputeEngineHook(\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        machine_type=GCE_MACHINE_TYPE,\n        disk_name=DISK_NAME,\n        disk_uri=DISKR_URI,\n        network_name=GCE_NETWORK_NAME,\n        resource_name=RESOURCE_NAME_SRC,\n        driver_python3_version=\"37\",\n        should_start_instance=True,\n        create_time=CREATE_TIME,\n        delete_time=DELETE_TIME,\n        secondary_ip_count=2,\n        external_disks=[\n            {\n                \"size_gb\": GCE_DISK_SIZE,\n                \"device_name\": DISK_NAME,\n                \"partition_label\": \"\",\n                \"initialize_params\": {\"disk_size_gb\": GCE_DISK_SIZE, \"disk_name\": DISKMRI},\n            }\n        ],\n        internal_disks=[{\"size_gb\": GCE_DISK_SIZE, \"device_name\": DISK_NAME}],\n        networks_connected=[\n            {\"network\": DISKNETWORK, \"subnetwork\": \"\"}\n        ],  # Create a persistent network connection between new and existing instances.\n        extra_args=[\n            \"--install-driver-copy-disks\",\n            DRIVER_PY_MODULE,\n        ],\n    ).operator\n    # [END howto_operator_gce_install_driver]\n\n    # [START howto_operator_gce_insert_instance]\n    create_instance_task = ComputeEngineInsertInstanceOperator(\n        task_id=\"create_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body={\n            \"name\": DEPLOYED_INSTANCE_NAME,\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/{GCE_MACHINE_TYPE}\",\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": DISKENTRY,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": GCE_DISK_SIZE,\n                        \"source_image\": f\"projects/google-cloud-compute/diskImages/{DISK_NAME}\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [{\"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}]},],\n        },\n    ).operator\n    # [END howto_operator_gce_insert_instance]\n\n    # [START howto_operator_gce_copy_disk]\n    copy_disk_task = ComputeEngineCopyDiskOperator(\n        task_id=\"copy_disk_task\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        source_resource_name=RESOURCE_NAME_SRC,\n        destination_resource_name=RESOURCE_NAME_DST,\n        source_disk_element_name=DISKNETWORK,\n        destination_disk_element_name=DISKDSTNAME,\n        new_disk_element_name=DISKMRI,\n        new_disk_device_name=DISKNETWORK,\n    ).operator\n    # [END howto_operator_gce_copy_disk]\n\n    # [START howto_operator_gce_delete_instance]\n    delete_instance_task = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=DEPLOYED_INSTANCE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    ).operator\n    # [END howto_operator_gce_delete_instance]\n\n    delete_instance_from_us_zone_task = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance_from_us_zone_task\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=DEPLOYED_INSTANCE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    ).operator\n# [START howto_operator_gce_copy_disk_no_region_template]\n\n\n    copy_disk_without_template_task = ComputeEngineCopyDiskOperator(\n        task_id=\"copy_disk_without_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        source_resource_name=RESOURCE_NAME_SRC,\n        destination_resource_name=RESOURCE_NAME_DST,\n        source_disk_element_name=DISKNETWORK,\n        destination_disk_element_name=DISKDSTNAME,\n        new_disk_element_name=DISKMRI,\n        new_disk_device_name=DISKNETWORK,\n    ).operator\n# [END howto_operator_gce_copy_disk_no_region_template]\n\n\n    (create_instance_task >> copy_disk_task >> delete_instance_task)\n    (create_instance_task >> copy_disk_without_template_task >> delete_instance_from_us_zone_task)\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_894_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592178}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an AI-powered content analysis pipeline that uses multimodal machine learning models to process and evaluate text and image data. The pipeline should leverage Google Cloud services to perform embedding, generation, and safety-filtered content evaluation across different input types.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Natural Language service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.language_v1 import Document\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.natural_language import (\n    CloudNaturalLanguageAnnotateDocumentOperator,\n    CloudNaturalLanguageClassifyTextOperator,\n)\nfrom airflow.providers.google.cloud.operators.text_extraction import CloudNaturalLanguageExtractSentencesOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_natural_language\"\n\n# [START howto_google_cloud_native_document_classification_operator]\nEXAMPLE_TEXT = \"\"\"On March 29, 2020, Trump signed an executive order banning all travel by citizens of seven countries\nincluding Iran, Iraq, Libya, Somalia, Sudan, Syria and Tunisia to the United States for 90 days.\n\nThe order also directed the Secretary of Homeland Security, in consultation with the Attorney General and the\nSecretary of State, to establish a process for denying entry to aliens who present a risk of deportation or\npublic security concerns.\"\"\"\n\ndocument = Document(content=EXAMPLE_TEXT, type_=Document.Type.PLAIN_TEXT)\n\noperator = CloudNaturalLanguageAnnotateDocumentOperator(\n    task_id=\"language_anotate_document\",\n    project_id=PROJECT_ID,\n    document=document,\n    language_code=\"en\",\n)\n# [END howto_google_cloud_native_document_classification_operator]\n\n# [START howto_google_cloud_native_document_classify_text_operator]\noutput = '{{task_instance.xcom_pull(\"\n    task_list\": \"natural-language-classify-task-list\", \"\n    key\": \"result\"\n  \")[0][\"name\"]}}'\n\ntemplate_field = f\"natural_language_{output}\"\n\nclassify_text_task = CloudNaturalLanguageClassifyTextOperator(\n    task_id=f\"natural-language-{DAG_ID}-classify-text-task\",\n    project_id=PROJECT_ID,\n    document=document,\n    predicted_class=template_field,\n)\n\n# [END howto_google_cloud_native_document_classify_text_operator]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is it:\n\n    from __future__ import annotations\n\n    import json\n    from pathlib import Path\n\n    import pendulum\n\n    from airflow.models.dag import DAG\n    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\n    from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\n    from airflow.providers.google.cloud.operators.natural_language import (\n        CloudNaturalLanguageBatchAnnotateDocumentsOperator,\n    )\n    from airflow.utils.trigger_rule import TriggerRule\n\n    ENV_ID = \"{{ env_id() }}\"\n    PROJECT_ID = \"{{ system_tests_system_test_params.gcp_project }}\"\n\n    DOCKER_URI = \"gcr.io/cloud-compute-rust/natural-language:latest\"\n\n    DUCKDB_FILE_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / \"duckdb.sql\")\n    DUCKDB_FILE_GCS_PATH = f\"gs://{BUCKET_NAME}/natural-language/duckdb.sql\"\n\n    TABULAR_DATASET_LOCAL_PATH = str(Path(__file__).parent / \"resources\" / \"tabular_dataset.csv\")\n    TABULAR_DATASET_GCS_PATH = f\"gs://{BUCKET_NAME}/natural-language/tabular_dataset.csv\"\n\n    DOCUMENT = r\"\"\"Article: This is an article!\n    Document type: article\n    Entity type: None\n\n    Ques: What is the first sentence in the document?\n    A: Article: This is an article!\n\n    Ques: What is the second sentence in the document?\n    A: Document type: article\n\n    Ques: Can you tell me the entity type of the document?\n    A: None\n\n    Ques: What does \"Document type: article\" mean?\n    A: It means this is an article!\n    \"\"\"\n\n    QUESTIONS = r\"\"\"What is the entity type of the question asked to NLP service?\n    Entities are divided into three categories: PROXIMITY, NAMED_ENTITY, and GLOSSARY.\n    Proximity entities are nearby words like 'its', 'this', 'that'. Named entity\n    recognition identifies proper nouns like 'John Adams' or 'United Nations'.\n    Glossary entities are defined lists like 'religion' or 'color'.\n\n    What are the two types of named entity recognition?\n    They are discrete and fuzzy. Discrete recognition has clearly defined names\n    while the fuzzy recognition allows for a degree of uncertainty.\n\n    Please provide an example of fuzzy recognized entity.\n    Example: John Doe (Jr.) has lived in many places\n\n    {% for line in DATA %}\n    Ques: {{ line | trim }}\n    A: {% if loop.first %}None{% else %}It means this is an article!\n    <![endif]%}\n    {% endfor %}\"\n    \"\"\"\n\n    BB_DATA = r\"\"\"{\n            \"text\": {\n                \"content\": \"Article: This is an article!\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": 0.6487850935963924,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Article\", \"type\": \"Glossary\"},\n                    {\"entity\": \"Document type\", \"type\": \"Proximity\"},\n                    {\"entity\": \"Jr.\", \"type\": \"Fuzzy\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"Document type: article\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Glossary\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"What is the entity type of the document?\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Entity type\", \"type\": \"Proximity\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"It means this is an article!\",\n            },\n            \"sentiment\": {\n                \"score\": 0.6487850908530973,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": false\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Fuzzy\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"What is 'Document type: article'\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Glossary\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"Can you tell me the entity type of the document?\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Entity type\", \"type\": \"Proximity\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"It means this is an article!\",\n            },\n            \"sentiment\": {\n                \"score\": 0.6487850908530973,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": false\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Fuzzy\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"What does \\\"Document type: article\\\" mean?\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": 0.6487850935963924,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": false\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Glossary\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"It means this is an article!\",\n            },\n            \"sentiment\": {\n                \"score\": 0.6487850908530973,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": false\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Document type\", \"type\": \"Fuzzy\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"What does \\\"None\\\" mean?\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Entity type\", \"type\": \"Fuzzy\"}\n                ]\n            }\n        }\n\n        {\n            \"text\": {\n                \"content\": \"None means there is no named entity recognized.\",\n                \"type\": \"PLAIN_TEXT\"\n            },\n            \"sentiment\": {\n                \"score\": -0.8312346141715931,\n                \"positive\": false,\n                \"neutral\": true,\n                \"negative\": true\n            },\n            \"entities\": {\n                \"categories\": [\n                    {\"entity\": \"Entity type\", \"type\": \"Glossary\"}\n                ]\n            }\n        }\n    \"\"\"\n\n    QUESTION_CSV = \"QUESTION,id,ANSWER\\n\\\n    What is the entity type of the question asked to NLP service?Entities are divided into three categories: PROXIMITY, NAMED_ENTITY, and GLOSSARY. Named entity recognition identifies proper nouns like 'John Adams' or 'United Nations'. Glossary entities are defined lists like 'religion' or 'color'.\\n\\\n    What are the two types of named entity recognition?They are discrete and fuzzy. Discrete recognition has clearly defined names while the fuzzy recognition allows for a degree of uncertainty.\\n\\\n    Please provide an example of fuzzy recognized entity.It John Doe (Jr.) has lived in many places\"\n\n    TABULAR_DATASET_COLS = \"id,question,answer\"\n\n    BUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n    FILE_NLP_SRC = \"nacl_example_files/\"\n    FILE_NLP_DST = \"nacl_output/\"\n\n    SRC_PATH = f\"gs://{BIG_WELL}/{FILE_NLP_SRC}\"\n    DST_PATH = f\"gs://{SRC_PATH}/{FILE_NLP_DST}\"\n\n    FOLDER = \"/natural-language-examples/\"\n\n    # [START howto_upload_multiple_documents_to_gcs]\n    upload_multiple_docs_to_gcs = GCSCreateBucketOperator(\n        task_id=\"upload_multiple_docs_to_gcs\",\n        bucket_name=BUCKET_NAME,\n        source_bucket=SRC_PATH,\n        source_object=FOLDER,\n        resource={\"predefined_acl\": \"public_read_write\"},\n    )\n    # [END howto_upload_multiple_documents_to_gcs]\n\n    # [START howto_use_awk_to_extract_sql_from_duckdb_file]\n    awk_command = f\"-v input_file={DUCKHIB_FILE_LOCAL_PATH} \\\n                   -v output_file={DUCKDB_FILE_GCS_PATH} \\\n                   '{\"awk\",\"-F\\\\t\",\"NR%2==1\",\">\",\"output_file\"}';\"\n\n    extract_duckdb_sql_from_file = LocalFilesystemToGCSOperator(\n        task_id=\"extract_duckdb_sql_from_file\",\n        src=DUCKHIB_FILE_LOCAL_PATH,\n        dst=DUCKDB_FILE_GCS_PATH,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_use_awk_to_extract_sql_from_duckdb_file]\n\n    # [START howto_batch_annotate_gcs_documents]\n    batch_annotate_gcs_docs = CloudNaturalLanguageBatchAnnotateDocumentsOperator(\n        task_id=\"batch_annotate_gcs_docs\",\n        sources=[\n            {\"gcs_source\": {\"source_uri\": f\"{SRC_PATH}{FILE_NLP_DST}sample-docs\"}},\n            {\"gcs_source\": {\"source_uri\": f\"{DST_PATH}{BB_DATA}\"}}},\n        ],\n        requests=[\n            {\n                \"document\": {\"content\": \"Please do not translate any personal information.\"},\n                \"annotation_request_type\": cloudnaturalanlp.DocumentAnnotateRequest.AnnotationRequestType.CLASSIFY_TEXT,\n                \"target_language_spec\": {\"languageCode\": \"en\"},\n            },\n            {\"document\": document, \"annotation_request_type\": \"CLASSIFY_TEXT\"},\n        ],\n        timeout=300,\n    )\n    # [END howto_batch_annotate_gcs_documents]\n\n    add_entity_type = GCSCreateBucketOperator(\n        task_id=\"add_entity_type\",\n        bucket_name=BUCKET_NAME,\n        source_bucket=SRC_PATH,\n        source_object=FOLDER + \"nacl_example_files/\",\n        resource={\"predefined_acl\": \"public_read_write\"},\n    )\n\n    upload_tabular_dataset = GCSCreateBucketOperator(\n        task_id=\"upload_tabular_dataset\",\n        bucket_name=BUCKET_NAME,\n        source_bucket=SRC_PATH,\n        source_object=FOLDER + \"nacl_example_files/tabular_dataset/\",\n        resource={\"predefined_acl\": \"public_read_write\"},\n    )\n\n    write_tabular_dataset_to_gcs = LocalFilesystemToGCSOperator(\n        task_id=\"write_tabular_dataset_to_gcs\",\n        src=[TABULAR_DATASET_LOCAL_PATH],\n        dst=TABULAR_DATASET_GCS_PATH,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_extrcted_csv = GCSCreateBucketOperator(\n        task_id=\"create_extrcted_csv\",\n        bucket_name=BUCKET_NAME,\n        source_bucket=SRC_PATH,\n        object_name=FOLDER + \"nacl_example_files/output/\",\n        system_test_name=\"create_folder\",\n    )\n\n    write_extracted_csv_to_gcs = LocalFilesystemToGCSOperator(\n        task_id=\"write_extracted_csv_to_gcs\",\n        src=[STR_OUTPUT],\n        dst=str(Path(TABULAR_DATASET_GCS_PATH).parent / STR_OUTPUT),\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_google_cloud_native_extract_sentences_operator]\n    extract_sentences = CloudNaturalLanguageExtractSentencesOperator(\n        task_id=\"language_extract_sentences\",\n        project_id=PROJECT_ID,\n        document={\n            \"content\": f\"Please do not translate the following sentences:\\n{QUESTIONS}\",\n            \"type_\": Document.Type.PLAIN_TEXT,\n        },\n        language_code=\"en\",\n    )\n    # [END howto_google_cloud_native_extract_sentences_operator]\n\n    # [START howto_execute_via_multiple_deferrable_tasks]\n    defer upload_multiple_docs_to_gcs\n    defer extract_duckdb_sql_from_file\n    defer upload_tabular_dataset\n    defer write_tabular_dataset_to_gcs\n    defer add_entity_type\n    defer create_extrcted_csv\n    defer write_extracted_csv_to_gcs\n    defer batch_annotate_gcs_docs\n    # [END howto_execute_via_multiple_deferrable_tasks]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        upload_multiple_docs_to_gcs\n        >> extract_duckdb_sql_from_file\n        >> upload_tabular_dataset\n        >> write_tabular_dataset_to_gcs\n        >> add_entity_type\n        >> create_extrcted_csv\n        >> write_extracted_csv_to_gcs\n        >> batch_annotate_gcs_docs\n        >> [write_tabular_dataset_to_gcs, STR_OUTPUT, extract_sentences, delete_bucket]\n    )\n    # [START howto_execute_via_parallel_operator]\n    execute_parallel = lambda tasks: tasks.detach()\n    # [END howto_execute_via_parallel_operator]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1714_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592555}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data orchestration process that extracts tabular data from a Google Spreadsheet and securely transfers it to a cloud storage solution with dynamic environment-based naming. Create a repeatable template for extracting structured data from cloud-based spreadsheet services and preparing it for downstream processing.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GoogleCloudSpreadsheetToGoogleDriveOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.google_cloud_spreadsheet_to_gcs import (\n    GoogleCloudSpreadsheetToGCSOperator,\n)\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_google_cloud_spreadsheet_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSPREADSHEET_ID = os.environ.get(\"GCP_GCS_SPREADSHEET_ID\", \"1562774800-abcde\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file_1 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_1\",\n        src=Path(__file__).parent / \"resources\" / \"spreadsheet_1.csv\",\n        dst=f\"dir_1_file_1.csv\",\n        bucket=BUCKET_NAME,\n    )\n    upload_file_2 = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file_2\",\n        src=Path(__file__).parent / \"resources\" / \"spreadsheet_2.csv\",\n        dst=f\"dir_1_file_2.csv\",\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_gcp_google_cloud_spreadsheet_to_gcs]\n    task_copy_file = GoogleCloudSpreadsheetToGCSOperator(\n        task_id=\"copy_file\",\n        google_cloud_spreadsheet_id=SPREADSHEET_ID,\n        destination_bucket=BUCKET_NAME,\n        destination_path=\"dir_2_file.csv\",\n        overwrite_existing=True,\n        gcp_conn_id=\"google_cloud_platform\",\n    )\n    # [END howto_operator_gcp_google_cloud_spreadsheet_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file_1\n        >> upload_file_2\n        # TEST BODY\n        >> task_copy_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1905_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592631}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases a systematic cloud storage transfer process with clear setup, execution, and teardown stages. Implement a workflow that creates a temporary S3 bucket, transfers data from Azure Blob Storage, and then deletes the bucket after completion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.msgraph import MSGraphTransferOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nAZURE_BLOB_STORAGE_ACCOUNT = os.environ.get(\"AZURE_BLOB_STORAGE_ACCOUNT\")\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\n\nDAG_ID = \"example_msgraph_transfer\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_msgraph_transfer]\n    azure_to_gcp_transfer = MSGraphTransferOperator(\n        task_id=\"azure_to_gcp_transfer\",\n        to_bucket=f\"azure-blob://{AZURE_BLOB_STORAGE_ACCOUNT}\",\n        from_blob=\"gcs/{{ ds }}.{{ ti.doparams[0] }}.txt\",\n        gcp_project_id=GCP_PROJECT_ID,\n        replace=True,\n    )\n    # [END howto_operator_msgraph_transfer]\n\n    bash_clean = BashOperator(task_id=\"bash_clean\", bash_command=\"if [ -d \\\"$(dirname $FILE_PATH)\\\" ]; then rm -R $(dirname $FILE_PATH); fi\")\n\n    azure_to_gcp_transfer >> bash_clean\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"always\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1071_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592689}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a reproducible machine learning data processing pipeline for housing price prediction that leverages cloud infrastructure and automated workflow management. Create a system that transforms local housing data into a cloud-based training job with standardized dataset preparation and model training steps.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.nlp import CloudNaturalLanguageTextExtractEntitySetCreateOperator,\n    CloudNaturalLanguageTextExtractEntitySetDeleteOperator, CloudNaturalLanguageTextExtractEntitiesOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nDATA_SAMPLE_GCS_BUCKET_NAME = \"airflow-system-tests-resources\"\nTEXT_EXTRACT_ENTITY_SET_NAME = os.environ.get(\"TEXT_EXTRACT_ENTITY_SET_NAME\", \"test_entity_set\")\nTEXT_EXTRACT_ENTITIES_TASK_NAME = \"text_extract_entities\"\nTEXT_EXTRACT_ENTITIES_CONFIG = {\n    \"please\": {\"type_\": \"PERSON\"},\n    \"be\": {\"type_\": \"LOCATION\"},\n    \"here\": {\"type_\": \"LOCATION\"},\n    \"your\": {\"type_\": \"LOCATION\"},\n}\nGCS_BUCKET_PATH = f\"gs://{DATA_SAMPLE_GCS_BUCKET_NAME}/natural-language/extract-entity.txt\"\n\n\nwith models.DAG(\n    \"example_gcp_natural_language_text_extract_entity_set\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME)\n\n    # [START howto_operator_gcp_natural_language_text_extract_entity_set_create]\n    create_text_extract_entity_set = CloudNaturalLanguageTextExtractEntitySetCreateOperator(\n        project_id=GCP_PROJECT_ID,\n        entity_set_id=TEXT_EXTRACT_ENTITY_SET_NAME,\n        location=\"us-central1\",\n        body={\"please\": [\"PER\"], \"be\": [\"LOC\"]},  # Extract locations like \"please\" and \"be\"\n    )\n    # [END howto_operator_gcp_natural_language_text_extract_entity_set_create]\n\n    # [START howto_operator_gcp_natural_language_text_extract_entities_with_entity_set]\n    extract_entities_with_entity_set = CloudNaturalLanguageTextExtractEntitiesOperator(\n        task_id=TEXT_EXTRACT_ENTITIES_TASK_NAME,\n        content=GCS_BUCKET_PATH,\n        language_code=\"en\",\n        location=\"us-central1\",\n        entity_set_id=TEXT_EXTRACT_ENTITY_SET_NAME,  # Use the entity set we created above\n    )\n    # [END howto_operator_gcp_natural_language_text_extract_entities_with_entity_set]\n\n    delete_text_extract_entity_set = CloudNaturalLanguageTextExtractEntitySetDeleteOperator(\n        task_id=\"delete_text_extract_entity_set\",\n        entity_set_id=TEXT_EXTRACT_ENTITY_SET_NAME,\n        location=\"us-central1\",\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", trigger_rule=TriggerRule.ALL_DONE, bucket_name=DATA_SAMPLE_GCS_BUCKET_NAME\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_text_extract_entity_set\n        # TEST BODY\n        >> extract_entities_with_entity_set\n        # TEST TEARDOWN\n        >> delete_text_extract_entity_set\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it done;\n    # if the task is removed, the watcher cannot notify the builder/trigger.\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_259_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6592767}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me extract specific elements from a JSON document using Python?"}, {"role": "assistant", "content": "import json"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.659284}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a machine learning pipeline that demonstrates Vertex AI dataset creation and pipeline job management using Google Cloud resources. Create a workflow that synchronizes housing data, creates a tabular dataset, runs a predefined ML pipeline, and then manages the pipeline job lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Pipeline Job operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import third_party_config\nfrom system.google import workspace_config\n\nDAG_ID = \"vertex_ai_pipeline_job_operations\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nREGION = \"us-central1\"\nDISPLAY_NAME_training_pipeline = f\"pipeline-job-training-{ENV_ID}\"\nDISPLAY_NAME_revision_pipeline = f\"pipeline-job-revision-{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\nJOB_DISPLAY_NAME = f\"{DISPLAY_NAME_training_pipeline}/pipeline-job\"\nREVISION_JOB_DISPLAY_NAME = f\"{DISPLAY_NAME_revision_pipeline}/pipeline-job\"\n\nRESOURCE_PATH_training_pipeline = (\n    f\"v2pipelines/{REGION}/{PROJECT_ID}/pipelines/{DISPLAY_NAME_training_pipeline}\"\n)\nRESOURCE_PATH_revision_pipeline = f\"v2pipelines/{REGION}/{PROJECT_ID}/pipelines/{REVISION_JOB_DISPLAY_NAME}\"\n\n# [START howto_vertex_ai_create_tabular_dataset_operator]\nTABULAR_DATASET = {\n    \"display_name\": f\"tabular-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": {\"input_config\": {\"bigquery_source\": {\"project_id\": PROJECT_ID, \"table_ref\": f\"test.{TABULAR_DATASET}\"}}},\n}\n\n# [END howto_vertex_ai_create_tabular_dataset_operator]\n\n\n# [START howto_vertex_ai_create_pipeline_job_operator]\nCREATE_PIPELINE_JOB_BODY = {\n    \"display_name\": JOB_DISPLAY_NAME,\n    \"job_type\": \"TRAINING\",\n    \"dataset_id\": TABULAR_DATASET[\"name\"],\n    \"model_display_name\": \"\",\n    \"python_file\": \"\",\n    \"container_spec\": {},\n    \"human_loop_configs\": [],\n    \"parallelism_settings\": {\"training_parallelism_count\": 1},\n}\n# [END howto_vertex_ai_create_pipeline_job_operator]\n\n# [START howto_vertex_ai_create_pipeline_job_operator_for_batch_prediction]\nCREATE_PIPELINE_JOB_BODY_BATCH_PREDICTION = {\n    \"display_name\": REVISION_JOB_DISPLAY_NAME,\n    \"job_type\": \"BATCH Prediction\",\n    \"dataset_id\": TABULAR_DATASET[\"name\"],\n    \"human_loop_configs\": [\n        {\n            \"human_loop_config_display_name\": f\"batch-prediction-human-loop{ENV_ID}\",\n            \"parameters\": {\n                \"spec\": {\n                    \"query_input\": {\n                        \"bigquery_source\": {\"project_id\": PROJECT_ID, \"table_ref\": f\"test.{TABULAR_DATASET}\"}\n                    }\n                }\n            },\n        }\n    ],\n}\n# [END howto_vertex_ai_create_pipeline_job_operator_for_batch_prediction]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"pipeline_job_operations\"],\n) as dag:\n    # [START howto_vertex_ai_create_pipeline_operator]\n    create_tabular_dataset_job = third_party_config.CreateThirdPartyDatasetOperator(\n        task_id=\"create_tabular_dataset_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=TABULAR_DATASET,\n        resource_path=RESOURCE_PATH_training_pipeline,\n    )\n\n    create_pipeline_job = third_party_config.CreateThirdPartyPipelineJobOperator(\n        task_id=\"create_pipeline_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=CREATE_PIPELINE_JOB_BODY,\n        resource_path=RESOURCE_PATH_training_pipeline,\n    )\n    # [END howto_vertex_ai_create_pipeline_operator]\n\n    # [START howto_vertex_ai_create_pipeline_operator_with_body]\n    create_pipeline_job_with_body = third_party_config.CreateThirdPartyPipelineJobOperator(\n        task_id=\"create_pipeline_job_with_body\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=CREATE_PIPELINE_JOB_BODY,\n        resource_path=RESOURCE_PATH_training_pipeline,\n    )\n    # [END howto_vertex_ai_create_pipeline_operator_with_body]\n\n    # [START howto_vertex_ai_create_pipeline_operator_for_batch_prediction]\n    create_pipeline_job_batch_prediction = third_party_config.CreateThirdPartyPipelineJobOperator(\n        task_id=\"create_pipeline_job_batch_prediction\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=CREATE_PIPELINE_JOB_BODY_BATCH_PREDICTION,\n        resource_path=RESOURCE_PATH_revision_pipeline,\n    )\n    # [END howto_vertex_ai_create_pipeline_operator_for_batch_prediction]\n\n    # [START howto_vertex_ai_create_pipeline_operator_for_batch_prediction_deferrable]\n    create_pipeline_job_batch_prediction_deferrable = third_party_config.CreateThirdPartyPipelineJobDeferrableOperator(\n        task_id=\"create_pipeline_job_batch_prediction_deferrable\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        body=CREATE_PIPELINE_JOB_BODY_BATCH_PREDICTION,\n        resource_path=RESOURCE_PATH_revision_pipeline,\n    )\n    # [END howto_vertex_ai_create_pipeline_operator_for_batch_prediction_deferrable]\n\n    delete_pipeline_job_batch_prediction = third_party_config.DeleteThirdPartyPipelineJobOperator(\n        task_id=\"delete_pipeline_job_batch_prediction\",\n        project_id=PROJECT_ID,\n        resource_id=REVISION_JOB_DISPLAY_NAME,\n        region=REGION,\n        resource_path=RESOURCE_PATH_revision_pipeline,\n    )\n\n    delete_pipeline_job_batch_prediction_deferrable = third_party_config.DeleteThirdPartyPipelineJobDeferrableOperator(\n        task_id=\"delete_pipeline_job_batch_prediction_deferrable\",\n        project_id=PROJECT_ID,\n        resource_id=REVISION_JOB_DISPLAY_NAME,\n        region=REGION,\n        resource_path=RESOURCE_PATH_revision_pipeline,\n        deferrable=True,\n    )\n\n    delete_tabular_dataset_job = third_party_config.DeleteThirdPartyDatasetOperator(\n        task_id=\"delete_tabular_dataset_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=TABULAR_DATASET[\"name\"],\n        resource_path=RESOURCE_PATH_training_pipeline,\n    )\n\n    # [END howto_vertex_ai_create_pipeline_operator]\n\n    join_task = third_party_config.JoinTaskOperator(\n        task_id=\"join\",\n        to_tasks=[delete_pipeline_job_batch_prediction, delete_pipeline_job_batch_prediction_deferrable],\n    )\n\n    # [START howto_vertex_ai_list_pipeline_jobs_operator]\n    list_pipeline_jobs = third_party_config.ListThirdPartyPipelineJobsOperator(\n        task_id=\"list_pipeline_jobs\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        filter_=\"(jobs->>:'pipeline_job_id') IN ( %(pipeline_job)%& ,%(pipeline_job_batch_prediction)%&)\n        \",\n        parameters={\"pipeline_job\": JOB_DISPLAY_NAME, \"pipeline_job_batch_prediction\": REVISION_JOB_DISPLAY_NAME},\n        result_processor=lambda tabs: [\n            tabs[TABULAR_DATASET][\"name\"].get_value(),\n            tabs[RESOURCE_PATH_training_pipeline][\"name\"].get_value() + \"/jobs\",\n            tabs[RESOURCE_PATH_revision_pipeline][\"name\"].get_value() + \"/jobs\",\n        ],\n    )\n    # [END howto_vertex_ai_list_pipeline_jobs_operator]\n\n    join_task2 = third_party_config.JoinTaskOperator(\n        task_id=\"join2\",\n        to_tasks=[\n            list_pipeline_jobs,\n            third_party_config.ListThirdPartyPipelineJobsOperator(\n                task_id=\"list_pipeline_jobs_deferrable\",\n                project_id=PROJECT_ID,\n                region=REGION,\n                filter_=\"(jobs->>:'pipeline_job_id') IN ( %(pipeline_job)%& ,%(pipeline_job_batch_prediction)%&)\",\n                parameters={\"pipeline_job\": JOB_DISPLAY_NAME, \"pipeline_job_batch_prediction\": REVISION_JOB_DISPLAY_NAME},\n                deferrable=True,\n            ),\n        ],\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_from_test_case  # noqa: E402\n\nthe_dag = get_test_run_from_test_case(dag)"}], "metadata": {"file_name": "dag_1327_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a string that contains multiple pieces of information about a location, separated by commas. The string looks something like this: \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\". I need to extract the name, location, category, and rating from this string and store them in separate variables. How can I do this in Python?"}, {"role": "assistant", "content": "import re\n\n# Given string containing location details\nlocation_info = \"Name: Amigos, Location: New York, Category: Restaurant, Rating: 4\"\n\n# Regular expression pattern to match each piece of information\npattern = r'(\\w+):\\s*(\\w+)'\n\n# Use re.findall() to find all matches of the pattern in the string\nmatches = re.findall(pattern, location_info)\n\n# Extracting the matched groups into separate variables\nname, location, category, rating = matches[0]\n\nprint(f\"Name: {name}\")\nprint(f\"Location: {location}\")\nprint(f\"Category: {category}\")\nprint(f\"Rating: {rating}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593075}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the core principles of task sequencing and SQL query execution using the SQLExecuteQueryOperator. Develop a workflow that connects to a Presto database and demonstrates fundamental data manipulation tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.presto.operators.presto import PrestoOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_presto\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"email_on_retry\": False,\n        \"email\": [\"airflow@example.com\"],\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    run_this = BashOperator(task_id=\"run_me\", bash_command=\"echo done\")\n\n    # [START howto_operator_presto]\n    drop_table = PrestoOperator(task_id=\"drop_table\", sql=\"DROP TABLE IF EXISTS hive_example;\", dag=dag)\n    create_table = PrestoOperator(\n        task_id=\"create_table\", sql=\"CREATE TABLE hive_example (col1 string, col2 int)\", dag=dag\n    )\n    insert_data = PrestoOperator(\n        task_id=\"insert_data\",\n        sql=\"INSERT INTO hive_example VALUES ('test1', 1), ('test2', 2), ('test3', 3)\",\n        dag=dag),\n    select_data = PrestoOperator(task_id=\"select_hive\",sql=\"SELECT * FROM hive_example;\",dag=dag)\n    # [END howto_operator_presto]\n\n    run_this >> drop_table >> create_table >> insert_data >> select_data\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2192_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593127}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates how to ingest data into Weaviate vector database using custom vectors and different data input methods. Create a workflow that shows how to create a Weaviate class, prepare data, and load it into the vector database using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom weaviate.classes.config import DataClass\nfrom weaviate.components.utils import get_qualified_class_name\nfrom weaviate.embedders.config import EmbedDim\nfrom weaviate.information.sources.config import SourceInfo\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.weaviate.operators.weaviate import (\n    WeaviateBatchInferenceOperator,\n    WeaviateCreateClassOperator,\n    WeaviateDeleteAllClassesOperator,\n    WeaviateIngestOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_weaviate\"\n# [START howto_operator_create_class_with_vector_and_source_info]\n\nCLASS_NAME = \"CatsWithSourceInfoAndVector\"\nEXAMPLE_DATA = [\n    {\"id\": \"1\", \"name\": \"Mr. T. The Cat\", \"age\": 24, \"is_good_with_children\": True, \"meows_on_request\": False},\n    {\n        \"id\": \"wkt63j25-6zl7-4f7v-bu2u-9oy8knx04x79\",\n        \"name\": \"Indra\",\n        \"age\": 1,\n        \"is_good_with_children\": False,\n        \"meows_on_request\": True,\n    },\n]\nSOURCE_INFO = SourceInfo(source_type=\"Weaviate\", source_id=\"test-source-id\")\nDATA_CLASS = DataClass(name=CLASS_NAME, embed_dim=EmbedDim.VECTOR)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_create_class_with_vector_and_source_info]\n    create_class_with_vector_and_source_info = WeaviateCreateClassOperator(\n        task_id=\"create_class_with_vector_and_source_info\",\n        class_name=CLASS_NAME,\n        data_class=DATA_CLASS,\n        example_data=EXAMPLE_DATA,\n        source_info=SOURCE_INFO,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_create_class_with_vector_and_source_info]\n\n    @task\n    def add_classes():\n        \"\"\"\n        Example task to create classes without embedding. It's used as demonstration that you can create classes\n        without embedding id's.\n        \"\"\"\n        from airflow.providers.weaviate.hooks.weaviate import WeaviateHook\n\n        weaviate_hook = WeaviateHook()\n        properties = [\"name\", \"age\", \"is_good_with_children\", \"meows_on_request\"]\n        with weaviate_hook.get_conn() as conn:\n            weaviate_hook.create_class(\n                class_name=CLASS_NAME, properties=properties, vectorizer_config=None, wait_for_completion=True\n            )\n\n    add_classes_task = add_classes()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_classes(class_names):\n        \"\"\"\n        Example task to delete the class without waiting for the execution of other tasks.\n        \"\"\"\n        from airflow.providers.weaviate.hooks.weaviate import WeaviateHook\n\n        weaviate_hook = WeaviateHook()\n        execution_hook = WeaviateHook()\n        with weaviate_hook.get_conn() as conn:\n            for class_name in class_names:\n                conn.delete_classes([class_name])\n        with execution_hook.get_conn() as conn:\n            for class_name in class_names:\n                conn.delete_classes([class_name], wait_for_completion=True)\n\n    remove_classes_from_dag = remove_classes([CLASS_NAME])\n\n    batch_ingest = WeaviateBatchIngestOperator(\n        task_id=\"batch_ingest\",\n        class_name=CLASS_NAME,\n        example_data=EXAMPLE_DATA,\n        data_class=DATA_CLASS,\n        source_info=SOURCE_INFO,\n        number_of_batches=2,\n        wait_for_completion=True,\n    )\n\n    batch_ingest >> create_class_with_vector_and_source_info >> add_classes_task >> remove_classes_from_dag\n\n    delete_all_classes = WeaviateDeleteAllClassesOperator(task_id=\"delete_all_classes\", class_names=[CLASS_NAME])\n    # [END howto_operator_create_class_with_vector_and_source_info]\n\n    (create_class_with_vector_and_source_info >> add_classes_task >> remove_classes_from_dag >> batch_ingest >> delete_all_classes)\n\n\nif __name__ == \"__main__\":\n    from airflow.utils.trigger_rule import TriggerRule\n\n    print(f\"Start DAG {DAG_ID}\")\n    dag.execute()\n    print(f\"End DAG {DAG_ID}\")\n\n# [DONE]"}], "metadata": {"file_name": "dag_1016_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593232}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a BigQuery data pipeline that demonstrates creating datasets, tables, and executing SQL queries for data insertion and retrieval. Build a workflow that showcases different BigQuery operators and demonstrates basic data management techniques in a cloud data warehouse environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google BigQuery service.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.bigquery_to_bigquery import BigQueryToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.providers.google.cloud.hooks.bigquery import BigQueryStagingFolderOperator\nexcept ImportError:\n    # This import throws an ImportError which is expected when running this test file directly\n    # skip the whole test module\n    pytest.skip(\"bigquery hook is not available\", allow_module_level=True)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nDAG_ID = \"example_bigquery_to_bigquery\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_bigquery_to_bigquery]\n    expand_values = BigQueryStagingFolderOperator(\n        task_id=\"expand_values\",\n        project_id=PROJECT_ID,\n        source_project_dataset_table=\"airflow_test.example_table_with_multiple_expressions\",\n        destination_project_dataset_table=\"airflow_test.output_1\",\n    )\n\n    copy_into_result = BigQueryToBigQueryOperator(\n        task_id=\"copy_into_result\",\n        source_project_dataset_table=\"airflow_test.example_table_with_multiple_expressions\",\n        destination_project_dataset_table=\"airflow_test.output_2\",\n    )\n    # [END howto_operator_bigquery_to_bigquery]\n\n    expand_values >> copy_into_result\n\n    # Task dependency created via `XComArgs`:\n    # expand_values >> trigger_task(\"copy_into_result\")\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_471_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593292}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "What are the differences between synchronous and asynchronous programming in Python? Can you provide a simple example to illustrate the difference?"}, {"role": "assistant", "content": "def read_file():\n    file = open('example.txt', 'r')\n    content = file.read()\n    print(content)\n    file.close()\n\nread_file()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593428}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one DAG can initiate another DAG with a predefined configuration message.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\n\n@task(task_id=\"run_this\")\ndef run_by_git():\n    print(\"Hello from child!\")\n\n\nwith DAG(\n    dag_id=\"example_git_trigger\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n):\n    BashOperator(task_id=\"run_this_by_python\", bash_command=\"run_by_git()\")\n    BashOperator(task_id=\"run_this_by_shell\", bash_command=\"run_by_git()\")"}], "metadata": {"file_name": "dag_1056_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593456}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates creating and executing a Dataproc workflow template in Google Cloud, showing how to set up a Hadoop/Spark cluster and run a Pig job. The workflow should illustrate the lifecycle of a cloud data processing job from template creation to execution.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Dataproc workflow operators.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateWorkflowTemplateOperator,\n    DataprocDeleteWorkflowTemplateOperator,\n    DataprocSubmitWorkflowOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_workflow\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nREGION = \"europe-west1\"\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n}\n\n# Workflow definition\nPIG_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"managed_cluster\": {\"cluster_name\": CLUSTER_NAME}},\n    \"query_list\": {\"queries\": [{\"sql_statement\": \"define pi = 4;\"}]},\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example'],\n) as dag:\n    create_workflow_template = DataprocCreateWorkflowTemplateOperator(\n        task_id=\"create_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        template={\n            \"id\": DAG_ID,\n            \"placement\": {\n                \"cluster_spec\": {\n                    \"cluster_name\": CLUSTER_NAME,\n                }\n            },\n            \"jobs\": [PIG_JOB],\n        },\n    )\n\n    submit_pig_job = DataprocSubmitWorkflowOperator(task_id=\"submit_pig_job\", template=DAG_ID)\n\n    delete_workflow_template = DataprocDeleteWorkflowTemplateOperator(\n        task_id=\"delete_workflow_template\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        template_id=DAG_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_workflow_template >> submit_pig_job >> delete_workflow_template\n\n    from tests.system.utils import get_test_run\n\n    # This test needs --catch-up=True because\n    # dataproc workflows are async and we can't wait\n    # for their completion when they're in BATCH\u72b6\u6001\u3002\n    test_run = get_test_run(dag)\n    assert test_run.state == \"BATCH\""}], "metadata": {"file_name": "dag_525_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593535}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates a complete cloud data transfer process, including creating a DynamoDB table, enabling point-in-time recovery, and performing a segmented export to S3. Implement a workflow that showcases dynamic resource management and cloud service interactions.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyJobFlowOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.utils import get_aws_account_id\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_emr_datapipeline\"\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"EmrDatapipelineExample\",\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\"Name\": \"Primary node\", \"Market\": \"ON_DEMAND\", \"InstanceRole\": \"MASTER\", \"InstanceType\": \"m5.large\",\n             \"Count\": 1}\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": [\n        {\n            \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n            \"HadoopJarStep\": {\n                \"Name\": \"run-shard-generator\",\n                \"Args\": [\"spark.shade.jar\", \"s3n://bucket/emp-dataset.json\", \"s3n://bucket/output/\"],\n            },\n        }\n    ],\n}\n\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# Externally fetched variables:\nBUCKET_NAME_KEY = \"BUCKET_NAME\"\nINPUT_FILE_PATH_KEY = \"INPUT_FILE_PATH\"\nOUTPUT_FILE_PATH_KEY = \"OUTPUT_FILE_PATH\"\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_step(steps, emr):\n    for step in steps:\n        if step[\"Name\"].find(f\"{DAG_ID}-\") == 0:\n            log.info(\"Deleting step found with path %s\", step[\"Name\"])\n            emr.terminate_step(step[\"Id\"])\n\n\n@task\ndef get_step_log(group_name: str, step_id: str, bucket_name: str):\n    log_path = f\"/usr/hdfs/tmp/{group_name}/{step_id}/logs/\"\n    command = f\"getlog -b {bucket_name} -d logs -f tail -n 0 \"\n    slave\u516c\u94a5\u6587\u4ef6 = \"/usr/hdfs/private/kms/keys/8c4aeebc-faa8-48e7-b008-803666d7d7f8\"\n    cmd_output = BashOperator(\n        task_id=\"get_step_log_cmd_output\",\n        bash_command=f\"hdfs dfs -put {log_path}{slave\u516c\u94a5\u6587\u4ef6} {log_path} && hdfs dfs {command}\",\n    )\n\n    return cmd_output.output\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"{env_id}-emr-datapipeline-bucket\"\n    input_file_path = f\"s3n://{bucket_name}/{env_id}-input.txt\"\n    output_file_path = f\"s3n://{bucket_name}/{env_id}-output.txt\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    put_job_steps = EmrCreateJobFlowOperator(\n        task_id=\"put_job_steps\",\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n        aws_account_id=get_aws_account_id(),\n        region=\"us-east-1\",\n    )\n\n    # [START howto_elasticsearch_add_step]\n    add_step = EmrAddStepsOperator(\n        task_id=\"add_step\",\n        job_flow_id=put_job_steps.output[\"job_flow_id\"],\n        steps=[\n            {\n                \"Name\": \"print_env_id\",\n                \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n                \"HadoopJarStep\": {\n                    \"Name\": \"print-env-id\",\n                    \"Args\": [input_file_path, output_file_path],\n                },\n            }\n        ],\n    )\n    # [END howto_elasticsearch_add_step]\n\n    # [START howto_elasticsearch_get_step_log_operator]\n    get_step_log_task = GetStepLogOperator(\n        task_id=\"get_step_log_task\",\n        group_name=JOB_FLOW_OVERRIDES[\"Name\"],\n        step_id=add_step.output[\"return_value\"],\n        bucket_name=bucket_name,\n    )\n    # [END howto_elasticsearch_get_step_log_operator]\n\n    delete_step(add_step.output[\"return_value\"], emr)\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=bucket_name, force_delete=True\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        put_job_steps,\n        # TEST BODY\n        add_step,\n        get_step_log_task,\n        # TEST TEARDOWN\n        delete_step(add_step.output[\"return_value\"]),\n        delete_bucket,\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_722_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593664}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Could you provide a detailed explanation of how to build a recommendation system using collaborative filtering in Python? Building a recommendation system can be quite complex, and I'm looking for a step-by-step guide."}, {"role": "assistant", "content": "import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load data\ndata = pd.read_csv('user_item_matrix.csv')\n\n# Convert to matrix format\nmatrix = data.pivot(index='user', columns='item', values='rating')\n\n# Compute similarity scores\nsimilarity_scores = cosine_similarity(matrix)\n\n# Create a weighted matrix by multiplying with user weights\nweighted_matrix = matrix.dot(similarity_scores)\n\n# Get top N recommendations for each user\ndef get_top_n_recommendations(user_id, n=10):\n    user_ratings = matrix.loc[user_id]\n    user_weights = weighted_matrix.loc[user_id]\n    top_n_items = user_ratings.sort_values(ascending=False).index[:n]\n    return top_n_items.tolist()\n\n# Test\nprint(get_top_n_recommendations(1))"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593742}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data integrity verification workflow that uses SQL operators to perform systematic checks on a database table's structure and content. Build a DAG that validates column-level constraints and confirms the expected number of rows in the target table.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample use of MySqlCheckOperator to check MySQL table and columns.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.mysql.operators.mysql import MySqlOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_mysql_check\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_my_sql_check]\n    check_table_existence = MySqlOperator(\n        task_id=\"check_table_existence\",\n        sql=\"SELECT 1 FROM information_schema.tables WHERE table_name='mytable' AND table_schema='testdb'\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    check_column_exists = MySqlOperator(\n        task_id=\"check_column_exists\",\n        sql=\"SHOW COLUMNS FROM testdb.mytable LIKE 'id'\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_my_sql_check]\n\n    check_table_existence >> check_column_exists\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_784_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593783}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that orchestrates complex marketing data operations across Google Cloud Platform services, focusing on extracting advertising performance metrics and managing data transfer between Google Display Video 360 and BigQuery. Implement tasks for downloading line items, uploading files, and performing data transformations.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that shows how to use DisplayVideo.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.display_video import (\n    GoogleDisplayVideo360CreateLineItemOperator,\n    GoogleDisplayVideo360CreateUploadFileOperator,\n    GoogleDisplayVideo360DeleteLineItemOperator,\n    GoogleDisplayVideo360DeleteUploadFileOperator,\n    GoogleDisplayVideo360GetLineItemsForAdsOperator,\n    GoogleDisplayVideo360ListLineItemsFilesOperator,\n    GoogleDisplayVideo360UploadFileOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"display_video\"\n\n# [START howto_display_video_environment_variables]\nPLATFORM_ID = os.environ.get(\"MARKETING_PLATFORM_ID\")\nACCOUNT_ID = os.environ.get(\"MARKETING_ACCOUNT_ID\", \"\")\nLINE_ITEM_NAME = f\"line_item_{ENV_ID}\"\nFILE_NAME = \"file_name\"\nFILE_TYPE = \"upload_file\"\n# [END howto_display_video_environment_variables]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"display-video\"],\n) as dag:\n    # [START howto_marketing_platform_access_token_v3]\n    marketing_platform_access_token = \"{{ marketing_platform_access_token }}\"\n    # [END howto_marketing_platform_access_token_v3]\n\n    # [START howto_display_video_create_line_item]\n    create_line_item = GoogleDisplayVideo360CreateLineItemOperator(\n        task_id=\"create_line_item\",\n        platform_id=PLATFORM_ID,\n        account_id=ACCOUNT_ID,\n        line_item_name=LINE_ITEM_NAME,\n        marketing_platform_access_token=marketing_platform_access_token,\n    )\n    # [END howto_display_video_create_line_item]\n\n    @task\n    def wait_for_creation(line_item_id):\n        import time\n\n        print(f\"Created line item {line_item_id}. Wait to be ready to use.\")\n        while True:\n            time.sleep(5)\n            list_line_items = GoogleDisplayVideo360ListLineItemsFilesOperator(\n                task_id=\"list_line_items\",\n                platform_id=PLATFORM_ID,\n                account_id=ACCOUNT_ID,\n                line_item_id=line_item_id,\n                marketing_platform_access_token=marketing_platform_access_token,\n            ).execute()\n            if list_line_items[\"lineItems\"][0][\"status\"] == \"READY\":\n                break\n\n    # [START howto_display_video_upload_file]\n    upload_file = GoogleDisplayVideo360UploadFileOperator(\n        task_id=\"upload_file\",\n        platform_id=PLATFORM_ID,\n        account_id=ACCOUNT_ID,\n        line_item_id=create_line_item.output,\n        file_type=FILE_TYPE,\n        file_name=FILE_NAME,\n        marketing_platform_access_token=marketing_platform_access_token,\n    )\n    # [END howto_display_video_upload_file]\n\n    # [START howto_display_video_create_line_item_result]\n    get_line_item_for_ads = GoogleDisplayVideo360GetLineItemsForAdsOperator(\n        task_id=\"get_line_item_for_ads\",\n        platform_id=PLATFORM_ID,\n        account_id=ACCOUNT_ID,\n        line_item_id=create_line_item.output,\n        marketing_platform_access_token=marketing_platform_access_token,\n    )\n    # [END howto_display_video_create_line_item_result]\n\n    # [START howto_display_video_list_files]\n    list_files = GoogleDisplayVideo360ListLineItemsFilesOperator(\n        task_id=\"list_files\", platform_id=PLATFORM_ID, account_id=ACCOUNT_ID, line_item_id=create_line_item.output\n    )\n    # [END howto_display_video_list_files]\n\n    # [START howto_display_video_delete_line_item]\n    delete_line_item = GoogleDisplayVideo360DeleteLineItemOperator(\n        task_id=\"delete_line_item\",\n        platform_id=PLATFORM_ID,\n        account_id=ACCOUNT_ID,\n        line_item_id=create_line_item.output,\n        marketing_platform_access_token=marketing_platform_access_token,\n    )\n    # [END howto_display_video_delete_line_item]\n\n    # [START howto_display_video_delete_line_item_result]\n    delete_line_item_for_ads = GoogleDisplayVideo360DeleteLineItemOperator(\n        task_id=\"delete_line_item_for_ads\",\n        platform_id=PLATFORM_ID,\n        account_id=ACCOUNT_ID,\n        line_item_id=create_line_item.output,\n        marketing_platform_access_token=marketing_platform_access_token,\n    )\n    # [END howto_display_video_delete_line_item_result]\n\n    wait_for_creation(create_line_item.output)\n\n    delete_line_item_for_ads.trigger_rule = TriggerRule.ALL_DONE\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_711_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6593883}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Data Catalog tag template management workflow that demonstrates creating, retrieving, and modifying metadata templates for data governance. Build an Airflow DAG that showcases the lifecycle of a tag template with different operational steps.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that displays interactions with Google Cloud Data Catalog.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.datacatalog import FieldType\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateTagTemplateOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteTagTemplateOperator,\n    CloudDataCatalogGetTagTemplateOperator,\n    CloudDataCatalogListEntriesOperator,\n    CloudDataCatalogUpdateTagTemplateOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.dates import date\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datacatalog_tag_template\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"id_{DAG_ID}_{ENV_ID}\"\nENTRY_GROUP_NAME = f\"name_{DAG_ID}_{ENV_ID}\"\nENTRY_DISPLAY_NAME = f\"name {ENV_ID}\"\nTEMPLATE_ID = f\"tid_{DAG_ID}_{ENV_ID}\"\nTEMPLATE_NAME = f\"name-{ENV_ID}\"\nFIELD_NAME = \"name\"\nLAYER_ID = f\"lay_id_{DAG_ID}_{ENV_ID}\"\nLAYER_NAME = f\"name_{ENV_ID}\"\nLAYER_DISPLAY_NAME = f\"name {ENV_ID}\"\n\nFILE_NAME_1 = \"file1.txt\"\nFILE_PATH_1 = f\"gcs/{FILE_NAME_1}\"\nFILE_NAME_2 = \"file2.txt\"\nFILE_PATH_2 = f\"gcs/{FILE_NAME_2}\"\n\nHEADERS = {\"header\": \"value\"}\nFIELDS = {\n    FIELD_NAME: {\n        \"display_name\": HEADERS[\"header\"],\n        \"type_\": FieldType.STRING,\n        \"is_required\": True,\n    }\n}\nSOURCE_IMAGE = f\"gs://{BUCKET_NAME}/{FILE_PATH_1}\"\nDESTINATION_LAYER_URL = f\"//google.cloud/temp/{LAYER_ID}/\"\nDESTINATION_FILE_PATH = f\"gcs/{DESTINATION_LAYER_URL}{FILE_NAME_2}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datacatalog\", \"tagtemplate\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"gcs_create_bucket\", bucket_name=BUCKET_NAME)\n\n    generate_file_1 = BashOperator(\n        task_id=\"generate_file_1\",\n        bash_command=f\"echo {HEADERS} > {FILE_PATH_1}\",\n    )\n\n    generate_file_2 = BashOperator(\n        task_id=\"generate_file_2\",\n        bash_command=f\"echo {HEADERS} > {FILE_PATH_2}\",\n    )\n    # [START howto_operator_gcp_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create-entry-group\",\n        entry_group_id=ENTRY_GROUP_ID,\n        location=LOCATION,\n        display_name=ENTRY_GROUP_NAME,\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_add_entry_type]\n    add_entry_type = CloudDataCatalogCreateEntryOperator(\n        task_id=\"add-entry-type\",\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"entry-id\",\n        location=LOCATION,\n        display_name=\"file-info\",\n        entry_type_id=\"file_info\",\n        fields=FIELDS,\n    )\n    # [END howto_operator_gcp_datacatalog_add_entry_type]\n\n    # [START howto_operator_gcp_datacatalog_create_entry_gcs]\n    create_entry_gcs = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create-entry-gcs\",\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"gcs-file-id\",\n        location=LOCATION,\n        display_name=FILE_NAME_1,\n        entry_type_id=\"file_info\",\n        source=GCS_SOURCE=https://storage.googleapis.com/{BUCKET_NAME}/{FILE_PATH_1},\n    )\n    # [END howto_operator_gcp_datacatalog_create_entry_gcs]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template]\n    create_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create-tag-template\",\n        tag_template_id=TEMPLATE_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"gcs-file-id\",\n        location=LOCATION,\n        display_name=\"GCS File\",\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_create_tag_template_with_resource_literal]\n    create_tag_template_from_resource = CloudDataCatalogCreateTagTemplateOperator(\n        task_id=\"create-tag-template-from-resource\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"gcs-file-id\",\n        location=LOCATION,\n        display_name=\"GCS File from Literal\",\n        resource_literals={\n            DESTINATION_LAYER_URL + FIELD_NAME: SOURCE_IMAGE,\n            LAYER_ID: LAYER_NAME,\n        },\n        conditions=[\n            CloudDataCatalog.Condition(\n                property_name=\"display_name\",\n                comparison=\"IsNotPresent\",\n                values=[],\n            )\n        ],\n    )\n    # [END howto_operator_gcp_datacatalog_create_tag_template_with_resource_literal]\n\n    # [START howto_operator_gcp_datacatalog_get_tag_template]\n    get_temporary_tag_template = CloudDataCatalogGetTagTemplateOperator(\n        task_id=\"get-temporary-tag-template\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END howto_operator_gcp_datacatalog_get_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_list_entries]\n    list_entries = CloudDataCatalogListEntriesOperator(\n        task_id=\"list-entry\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_list_entries]\n\n    # [START howto_operator_gcp_datacatalog_update_tag_template]\n    update_tag_template = CloudDataCatalogUpdateTagTemplateOperator(\n        task_id=\"update-tag-template\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n        field_mask=FieldMask(paths=[\"display_name\"]),\n    )\n    # [END start_gcp_datacatalog_update_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_delete_tag_template]\n    delete_temporary_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n        task_id=\"delete-tag-template\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_delete_tag_template]\n\n    # [START howto_operator_gcp_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete-entry-group\",\n        entry_group_id=ENTRY_GROUP_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_delete_entry_group]\n\n    # [START howto_operator_gcp_datacatalog_delete_entry]\n    delete_entry_gcs = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete-entry-gcs\",\n        entry_id=\"gcs-file-id\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n    )\n    # [END start_gcp_datacatalog_delete_entry]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"gcs_delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    is_it_running = BashOperator(\n        task_id=\"check_sharing_state\",\n        bash_command=f\"test `gcloud projects add-photoreporting-project-configuration {PROJECT_ID} --format='indicator(valueForDisplay)=sharingStatus'` = 'ONLINE'\",\n    )\n\n    is_it_finished = BashOperator(\n        task_id=\"check_finish_operation\",\n        bash_command=f\"test `gcloud datacatalog tag-targets list --filter=name={DESTINATION_LAYER_URL+FIELD_NAME} --format='count(*)'=finishedOperationCount\\\"\",\n    )\n\n    file_transfer_process_list = BashOperator(\n        task_id=\"show_file_transfer_process_list\",\n        bash_command=f\"cat /proc/diskstats | head -n 1\",\n    )\n\n    create_entry_for_urllcd = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create-entry-for-url-lcd\",\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"url-lcd-entry-id\",\n        location=LOCATION,\n        display_name=LAYER_DISPLAY_NAME,\n        entry_type_id=\"url_lcd_target\",\n        resource_lititions={\n            DESTINATION_LAYER_URL + LAYER_ID: DESTINATION_FILE_PATH,\n        },\n    )\n\n    # [START howto_operator_gcp_datacatalog_list_url_lcd_targets]\n    list_url_lcd_targets = CloudDataCatalogListEntriesOperator(\n        task_id=\"list-url-lcd-targets\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_list_url_lcd_targets]\n\n    t = Path(__file__).parent.parent\n    urldlc_file = str(t / \"etc\" / \"url-lcd.csv\")\n\n    # [START howto_operator_gcp_datacatalog_get_url_lcd_targets]\n    get_url_lcd_targets = CloudDataCatalogGetTagTemplateOperator(\n        task_id=\"get-url-lcd-targets\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_get_url_lcd_targets]\n\n    upload_url_lcd_csv = BashOperator(\n        task_id=\"upload-url-lcd-csv\",\n        bash_command=f\"cat > {urldlc_file} << _DONE_\n        647  8/25/2019 1:20:00 PM URL TARGET CREATED\n        647  8/25/2019 1:20:00 PM FILE TRANSFER PROCESS STARTED\n        647  8/25/2019 1:20:05 PM FILE TRANSFER PROCESS FINISHED\n        _DONE_\n        chmod +x {urldlc_file}\"\n    )\n\n    wait_for_upload_url_lcd_csv = BashOperator(\n        task_id=\"wait-for-upload-url-lcd-csv\",\n        bash_command=f\"while ! cat {urldlc_file} ; do sleep 1; done\",\n    )\n\n    # [START howto_operator_gcp_datacatalog_update_url_lcd_target]\n    update_url_lcd_target = CloudDataCatalogUpdateTagTemplateOperator(\n        task_id=\"update-url-lcd\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        location=LOCATION,\n        resource_literals={\n            DESTINATION_LAYER_URL + LAYER_ID: URLLCD_TARGET,\n        },\n    )\n    # [END start_gcp_datacatalog_update_url_lcd_target]\n\n    # [START howto_operator_gcp_datacatalog_delete_url_lcd_target]\n    delete_url_lcd_target = CloudDataCatalogDeleteTagTargetOperator(\n        task_id=\"delete-url-lcd-target\",\n        target_name=f\"url-lcd-target-name\",\n        tag_template_id=TEMPORARY_TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_delete_url_lcd_target]\n\n    remove_url_lcd_file = BashOperator(\n        task_id=\"remove-url-lcd-file\",\n        bash_command=f\"rm {urldlc_file}\",\n    )\n\n    create_entry_from_protobuf = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create-entry-from-proto\",\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=\"proto-entry-id\",\n        location=LOCATION,\n        display_name=\"proto-entry\",\n        entry_type_id=\"source_producer\",\n        service_protocol_id=\"protoc\",\n        resource_projections={\n            TEMPLATE_NAME: {\n                \"fields\": {\n                    \"source_filename\": {\"useIfExists\": True},\n                    \"dest_destination\": {\"useIfExists\": True},\n                }\n            }\n        },\n    )\n\n    # [START howto_operator_gcp_datacatalog_get_source_produced_by]\n    get_source_produced_by = CloudDataCatalogGetTagTemplateOperator(\n        task_id=\"get-source-produced-by\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_get_source_produced_by]\n\n    list_source_produced_by = CloudDataCatalogListEntriesOperator(\n        task_id=\"list-source-produced-by\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n    )\n\n    run_query = BashOperator(\n        task_id=\"run-querry\",\n        bash_command=f\"cat /proc/diskstats | head -n 1\",\n    )\n\n    # [START howto_operator_gcp_datacatalog_insert_snapshot>\n    insert_snapshot = CloudDataCatalogInsertSnapshotOperator(\n        task_id=\"insert-snapshot\",\n        location=\"us-central1\",\n        project_id=\"abcd12345\",\n        argument=\"-e --table={{ params.source_produced_by }}\",\n        method=\"copy\",\n        snapshot_name=\"snap-{{ params.project_id }}-{{ params.source_produced_by.split('/')[-1] }}\",\n    )\n    # [END start_gcp_datacatalog_insert_snapshot>\n\n    # [START howto_operator_gcp_datacatalog_list_snapshots>\n    list_snapshots = CloudDataCatalogListSnapshotsOperator(\n        task_id=\"list-snapshots\",\n        location=\"us-central1\",\n        project_id=\"abcd12345\",\n        argument=\"-e --table={{ params.source_produced_by }}\",\n        snapshot_name=\"{{ params.snapshot_name }}\",\n    )\n    # [END start_gcp_datacatalog_list_snapshots>]\n\n    render_query_result_to_file = BashOperator(\n        task_id=\"render-query-result-to-file\",\n        bash_command=f\"cat /proc/diskstats | head -n 1 > {Path(__file__).parent / 'temp' / 'query-result.tsv'}\",\n    ))\n\n    @task\n    def remove_file(filename: str):\n        Path(filename).unlink()\n\n    remove_file(f\"{Path(__file__).parent / 'temp' / 'query-result.tsv'}\")\n\n    # [START howto_operator_gcp_datacatalog_get_snapshot_results>\n    get_snapshot_results = CloudDataCatalogGetSnapshotResultsOperator(\n        task_id=\"get-snapshot-results\",\n        location=\"us-central1\",\n        project_id=\"abcd12345\",\n        snapshot_name=\"snap-abcd-example-db\",\n        read_fields=[\n            {\"name\": \"{{ params.column_name }}\", \"useIfExists\": True},\n        ],\n        write_field=\"{{ params.write_column_name }}\",\n    )\n    # [END start_gcp_datacatalog_get_snapshot_results>]\n\n    include_fields = [\n        {\"name\": \"c0\"},\n        {\"name\": \"c1\", \"anonymizationSpecification\": {\"type\": \"NONE\"}},\n    ]\n\n    @task\n    def populate_write_column_name(field_name: str):\n        \"\"\"Task to map read field name to write column name.\"\"\"\n\n        return f\"--table={field_name}\"\n\n    @task\n    def run_render_query_to_file(write_column_name: str, filename: str):\n        \"\"\"Task to render query result to file.\"\"\"\n        BashOperator(\n            task_id=\"run-render-query-to-file\",\n            bash_command=f\"cat /proc/diskstats | head -n 1 > {filename}\",\n        )\n        BashOperator(\n            task_id=\"remove-file\",\n            bash_command=f\"rm /proc/diskstats;\",\n        )\n        BashOperator(\n            task_id=\"run-query\",\n            bash_command=write_column_name + \" \" + render_query_result_to_file.expand(params={\"column_name\": \"c0\", \"write_column_name\": \"dfscd\"})[0],\n        )\n\n    include_fields_with_anonymized_column_name = [\n        {\"name\": \"c0\"},\n        {\"name\": \"c1\", \"anonymizationSpecification\": {\"type\": \"NONE\"}},\n    ]\n    run_render_query_to_file(expand_params=lambda x: x[\"write_column_name\"], task_id=\"run-render-query-to-file-with-anonymized-column-name\", filename=\"query-result-with-anonymized-column.tsv\")\n\n    include_fields_with_anonymized_column_name_mapped = [\n        {\"name\": \"c0\", \"mappedFields\": [{\"fromField\": \"c1\", \"toField\": \"abcd12345\"}]},\n        {\"name\": \"c1\", \"anonymizationSpecification\": {\"type\": \"NONE\"}},\n    ]\n    BashOperator(\n        task_id=\"render-query-result-to-file-with-anonymized-column\",\n        bash_command=f\"cat /proc/diskstats | head -n 1 > {Path(__file__).parent / 'temp' / 'query-result-with-anonymized-column.tsv'}\",\n    )\n\n    remove_file(f\"{Path(__file__).parent / 'temp' / 'query-result-with-anonymized-column.tsv'}\")\n\n    # [START howto_operator_gcp_datacatalog_get_snapshot_results_with_anonymized_column_mapping>\n    get_snapshot_results_with_anonymized_column_mapping = CloudDataCatalogGetSnapshotResultsOperator(\n        task_id=\"get-snapshot-results-with-anonymized-column-mapping\",\n        location=\"us-central1\",\n        project_id=\"abcd12345\",\n        snapshot_name=\"snap-abcd-example-db\",\n        read_fields=include_fields_with_anonymized_column_mapping,\n        write_field=\"{{ params.write_column_name }}\",\n    )\n    # [END start_gcp_datacatalog_get_snapshot_results_with_anonymized_column_mapping>]\n\n    with_set_value = BashOperator(\n        task_id=\"replace-value\",\n        bash_command=\"--replace 123 {{ params.value }}\"\n    )\n\n    join_task = BashOperator(\n        task_id=\"join\",\n        bash_command=\"cat {{ params.file1 }} | cat {{ params.file2 }}\"\n    )\n\n    @task\n    def remove_files(file1: str, file2: str):\n        Path(file1).unlink()\n        Path(file2).unlink()\n\n    remove_file(f\"{Path(__file__).parent / 'temp' / '{file1}'}\")\n    remove_file(f\"{Path(__file__).parent / 'temp' / '{file2}'}\")\n\n    combine_and_gcs_upload = BashOperator(\n        task_id=\"combine-and-gcs-upload\",\n        bash_command=f\"cat {Path(__file__).parent / 'temp' / '{file1}'} {Path(__file__).parent / 'temp' / '{file2}'} > {urldlc_file}\",\n    )\n\n    copy_intermediate_file = BashOperator(\n        task_id=\"copy-intermediate-file\",\n        bash_command=f\"cp {urldlc_file} {t / 'etc' / 'copy-file-temporal.csv'\"\n    )\n\n    # [START howto_operator_gcp_datacatalog_list_tags>\n    list_tags = CloudDataCatalogListTagsOperator(\n        task_id=\"list-tags\",\n        tag_template_id=TEMPLATE_ID,\n        location=LOCATION,\n    )\n    # [END start_gcp_datacatalog_list_tags>\n\n    # [START howto_operator_gcp_datacatalog_create_tag>\n    create_tag = CloudDataCatalogCreateTagOperator(\n        task_id=\"create-tag\",\n        tag_name=\"tags-tag-new\",\n        tag_template_id=TEMPLATE_ID,"}], "metadata": {"file_name": "dag_947_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6594276}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that calculates the total cost of a list of items, where each item is represented as a dictionary with keys 'name', 'quantity', and 'price_per_unit'. The function should return the total cost of all items."}, {"role": "assistant", "content": "def calculate_total_cost(items):\n    \"\"\"\n    Calculate the total cost of a list of items.\n\n    Each item in the list is represented as a dictionary with keys 'name', 'quantity', and 'price_per_unit'.\n    The function returns the total cost of all items combined.\n\n    :param items: List of dictionaries representing items.\n    :return: Total cost of all items.\n    \"\"\"\n    total_cost = 0\n    # Iterate over each item in the list\n    for item in items:\n        # Multiply quantity by price per unit and add to total cost\n        total_cost += item['quantity'] * item['price_per_unit']\n    return total_cost\n\n# Example usage:\nitems_list = [\n    {'name': 'apple', 'quantity': 5, 'price_per_unit': 1.2},\n    {'name': 'banana', 'quantity': 3, 'price_per_unit': 0.8},\n    {'name': 'orange', 'quantity': 4, 'price_per_unit': 1.5}\n]\n\ntotal = calculate_total_cost(items_list)\nprint(f\"The total cost of the items is: ${total:.2f}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6594367}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data movement and management pipeline that transfers files between cloud storage buckets while implementing systematic tracking and cleanup procedures. Create a workflow that demonstrates how data can be safely moved, listed, and deleted using cloud-native orchestration techniques.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcs_to_gcs\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START list_of_files]\n    list_of_files = LocalFilesystemToGCSOperator(\n        task_id=\"list_of_files\",\n        source_bucket=\"cloud-samples-data\",\n        source_object=\"fileset/\",\n        destination_bucket=\"gcs_example_data\",\n        dest_filename=\"files_list.txt\",\n    )\n    # [END list_of_files]\n\n    # [START create_copy]\n    create_copy_of_files = LocalFilesystemToGCSOperator(\n        task_id=\"create_copy_of_files\",\n        source_bucket=\"gcs_example_data\",\n        source_object=\"files_list.txt\",\n        destination_bucket=\"gcs_example_data\",\n        dest_filename=\"new_file_name.txt\",\n    )\n    # [END create_copy]\n\n    # [START gcs_to_gcs_transfer]\n    gcs_to_gcs_transfer = GCSToGCSOperator(\n        task_id=\"gcs_to_gcs_transfer\",\n        source_bucket=\"cloud-samples-data\",\n        source_object=\"fileset/\",\n        destination_bucket=\"gcs_example_data\",\n        dest_filename=\"files_list.txt\",\n    )\n    # [END gcs_to_gcs_transfer]\n\n    (\n        # TEST SETUP\n        list_of_files\n        >> create_copy\n        # TEST BODY\n        >> gcs_to_gcs_transfer\n        # TEST TEARDOWN\n        >> LocalFilesystemToGCSOperator(\n            task_id=\"remove_file\",\n            source_bucket=\"gcs_example_data\",\n            source_object=\"files_list.txt\",\n            dest_filename=\"remove_file.txt\",\n        )\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2055_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6594436}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that takes a string and returns a new string with the first letter of each word capitalized."}, {"role": "assistant", "content": "def capitalize_words(s):\n    # Split the string into words\n    words = s.split()\n    \n    # Capitalize the first letter of each word and join them back together\n    capitalized_words = [word.capitalize() for word in words]\n    \n    # Join the capitalized words into a single string\n    result = ' '.join(capitalized_words)\n    \n    return result\n\n# Example usage:\ninput_string = \"hello world this is python\"\noutput_string = capitalize_words(input_string)\nprint(output_string)  # Output: Hello World This Is Python"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983363.65945}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data profiling pipeline that uses AWS Glue DataBrew to analyze a JSON dataset stored in S3, generating insights and managing computational resources dynamically. Construct a workflow that shows how to programmatically profile data with full lifecycle management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.glue_databrew import (\n    GlueDatabrewStartOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glue_databrew import GlueDatabrewSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_glue_databrew\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n\n@task\ndef create_job_run(role_arn, env_id):\n    # Create a GLUE context with the environment ID.\n    glue_context = {\n        \"name\": \"glue-context-\" + str(role_arn) + \"-\" + str(env_id),\n        \"athena_sample_table_name\": \"env_id_test_table\",\n    }\n\n    # Create a job run params dict.\n    job_params = {\n        \"archives\": [\n            {\n                \"path\": f\"s3://{env_id}/data/externally-provided-json-file.json\",\n                \"uncompressedPath\": f\"s3://{env_id}/data/externally-provided-json-file.json\",\n            }\n        ],\n        \"roleArn\": role_arn,\n        \"databaseName\": \"test_db\",\n        \"jobName\": \"test_db_dump_job\",\n        \"format\": \"JSON\",\n    }\n\n    # Call the service to create a GLUE job run.\n    boto3.client(\"glue\").start_job_run(jobName=\"test_db_dump_job\", context=glue_context,**job_params)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_run(role_arn, job_run_id):\n    # Call the service to delete the job run.\n    boto3.client(\"glue\").delete_job_run(jobName=\"test_db_dump_job\", runId=job_run_id)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag,\nsys_test_context_task(),\ncreate_job_run(sys_test_context_task[ROLE_ARN_KEY], sys_test_context_task[ENV_ID_KEY]),\nGlueDatabrewSensor(\n    task_id=\"monitor_databrew_job_run\",\n    job_name=\"test_db_dump_job\",\n    run_id=\"{{ ti.xcom_pull(task_ids='create_job_run', key='return_value').get('runId') }}\",\n    databrew_version=\"DATABREW_VERSION_8.X\",\n),\nGlueDatabrewStartOperator(\n    task_id=\"stop_databrew_job_run\",\n    job_name=\"test_db_dump_job\",\n    run_id=\"{{ ti.xcom_pull(task_ids='monitor_databrew_job_run', key='return_value') }}\",\n),\ndelete_job_run(sys_test_context_task[ROLE_ARN_KEY], \"{{ ti.xcom_pull(task_ids='monitor_databrew_job_run', key='return_value').get('runId') }}\"),\nchain(\n    sys_test_context_task,\n    create_job_run,\n    delete_job_run,\n    GlueDatabrewSensor(\n        task_id=\"monitor_databrew_job_run\",\n        job_name=\"test_db_dump_job\",\n        run_id=\"{{ ti.xcom_pull(task_ids='create_job_run', key='return_value').get('runId') }}\",\n        databrew_version=\"DATABREW_VERSION_8.X\",\n    ),\n    GlueDatabrewStartOperator(\n        task_id=\"stop_databrew_job_run\",\n        job_name=\"test_db_dump_job\",\n        run_id=\"{{ ti.xcom_pull(task_ids='monitor_databrew_job_run', key='return_value') }}\",\n    ),\n):\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1904_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983363.6594584}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers CSV files from Google Cloud Storage to a Presto table, enabling seamless data movement between cloud storage and analytics platforms.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToPrestoOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.transfers.gcs_to_presto import GCSToPrestoOperator\n\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"airflow-system-tests-resources\")\nDAG_ID = \"gcs_to_presto\"\nPRESTO_TABLE = os.environ.get(\"PRESTO_TABLE\", \"test_presto_table\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nGCS_PATH = f\"gs://{BUCKET}/presto/\"\nSQL = f\"INSERT INTO {PRESTO_TABLE} VALUES (32, 'foo bar'::varchar[30])\"\n\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    # [START gcs_to_presto_operator]\n    gcs_to_presto = GCSToPrestoOperator(\n        task_id=\"gcs_to_presto_task\",\n        gcs_bucket=BUCKET,\n        dest_table=f\"{GCP_PROJECT_ID}.{PRESTO_TABLE}\",\n        prefix=\"presto/\",\n        sql_stmt=GCS_PATH + SQL,\n        presto_timeout=30,\n    )\n    # [END gcs_to_presto_operator]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_301_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068579}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates conditional task execution using short-circuit logic, where certain tasks only run based on predefined boolean conditions. Create a DAG that showcases how tasks can be dynamically skipped or executed depending on specific criteria.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the ShortCircuitOperator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import ShortCircuitOperator\n\nwith DAG(\n    \"example_short_circuit_operator\",\n    default_args={\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n    },\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n) as dag:\n    run_this = EmptyOperator(task_id=\"run-this\")\n    # [START howto_operator_short_circuit]\n    skip_this = ShortCircuitOperator(\n        task_id=\"skip-this\",\n        python_callable=lambda: False,\n    )\n    # [END howto_operator_short_circuit]\n    run_this >> skip_this"}], "metadata": {"file_name": "dag_24_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068585}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that demonstrates container-based task scheduling and execution using Docker Swarm operator. Create a workflow that launches a CentOS container, executes a brief sleep operation, and automatically removes the container after completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.docker.operators.docker_swarm import DockerSwarmOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"docker_swarm_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is\n    # [START howto_operator_docker_swarm]\n    @task\n    def test():\n        print(\"Hello World\")\n\n    docker_swarm_task = DockerSwarmOperator(\n        task_id=\"docker_swarm_task\",\n        swarm_worker=\"worker-1\",\n        swarm_stack=\"stack-1\",\n        image=\"centos:latest\",\n        command=f\"-n {TEST_NAME} --sleep=10\",\n        dag=DAG_ID,\n    )\n    # [END howto_operator_docker_swarm]\n\n    test()"}], "metadata": {"file_name": "dag_453_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0685894}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data pipeline that monitors a specified input directory for new files, validates file presence, and performs a controlled file transfer using containerized operations. Implement a robust mechanism for detecting, validating, and processing files with built-in error handling and logging.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.base import SFTPSensor\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.sftp.transfers.local_to_sftp import LocalFileSystemToSFTPTempSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.sftp.utils import need_to_run_in_test\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_sensor\"\n\nconn_id_name = \"sftp_default\"\n\nsftp_host = f\"{ENV_ID}.sftp.provider.test.host\"\nsftp_user = f\"{ENV_ID}User\"\nsftp_path = f\"/tmp/{DAG_ID}\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    create_connection = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"sftp\",\n        host=sftp_host,\n        login=f\"{ENV_ID}{sftp_user}\",\n        password=\"password\",\n    )\n\n    upload_file = LocalFileSystemToSFTPTempSensor(\n        task_id=\"upload_task\",\n        local_filepath=\"/tmp/airflow/test.txt\",\n        remote_filepath=f\"{sftp_path}/temp/\",\n    )\n\n    # [START howto_operator_sftp]\n    @task(task_id=\"success_op_sftp\")\n    def success_op_sftp():\n        print(\"Success!\")\n\n    # [END howto_operator_sftp]\n\n    download_file = SFTPSensor(\n        task_id=\"download_task\",\n        sftp_conn_id=conn_id_name,\n        remote_filepath=f\"{sftp_path}/temp/test.txt\",\n        timeout=60 * 2,\n        poke_interval=10,\n    )\n\n    download_file_deferrable = SFTPSensor(\n        task_id=\"download_task_def\",\n        sftp_conn_id=conn_id_name,\n        remote_filepath=f\"{sftp_path}/temp/test.txt\",\n        timeout=60 * 2,\n        poke_interval=10,\n        deferrable=True,\n    )\n\n    delete_file = SFTPSensor(\n        task_id=\"delete_task\",\n        sftp_conn_id=conn_id_name,\n        remote_filepath=f\"{sftp_path}/temp/test.txt\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_file_deferrable = SFTPSensor(\n        task_id=\"delete_task_def\",\n        sftp_conn_id=conn_id_name,\n        remote_filepath=f\"{sftp_path}/temp/test.txt\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        deferrable=True,\n    ])\n\n    (\n        # TEST SETUP\n        upload_file\n        >> download_file\n        >> success_op_sftp()\n        >> delete_file\n        >> delete_file_deferrable\n    )\n    (\n        # TEST TEARDOWN\n        upload_file\n        >> download_file\n        >> success_op_sftp()\n        >> delete_file_deferrable\n    )\n\n    for op in (upload_file, download_file):\n        op.set_visible(False)\n\n    from tests.system.utils.watcher import watcher\n\n    list(dag.tasks) > watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_794_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068598}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data quality workflow that ingests raw data, performs initial analysis, and checks data integrity. Create a branching process that either saves clean data and generates a report, or handles errors by describing issues and sending notifications.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino]\n    trino_to_parquet = TrinoToParquetOperator(task_id=\"trino_task\", sql=\"select * from table\")\n    # [END howto_operator_trino]\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\nif __name__ == \"__main__\":\n    pass\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see above)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_325_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686016}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated data enrichment pipeline that uses OpenAI to generate descriptive information about a static list of items, leveraging Airflow's task expansion and deferred execution capabilities.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.openai.operators.openai import OpenAITaskOperator\n\n# [START howto_operator_openai]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_openai\"\n\nwith models.DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_openai_task]\n    openai_task = OpenAITaskOperator(\n        task_id=\"openai_learning\",\n        input={\n            \"input_text\": \"This is an example conversation. The first user is asking what\\n\"\n            \"the command format for the OpenAI Assistant is. The second user demonstrates\\n\"\n            \"learning to answer with the format of /learn <question> <response>. The AI\\n\"\n            \"Assistant knows what format to expect and replies with the correct format.\\n\"\n            \"The first user is then curious about the training data used so the second \"\n            \"user provides some sample triples of question, topic, and content. The AI \"\n            \"is able to correctly associate questions with responses based on the \"\n            \"provided training data.\"\n        },\n        model=\"text-davinci-003\",\n        max_tokens=600,\n        echo_input=True,\n    )\n    # [END howto_operator_openai]\n\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\nlist(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2174_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068608}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a cloud-based data extraction and loading pipeline that captures data from a NoSQL database, exports it to a neutral storage format, and prepares it for analytical querying. Create a workflow that transforms unstructured data into a structured, analyzable format across different Google Cloud platforms.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using PgBrewOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pgdb.operators.pgdb import PgDBOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_pgbrew_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_pgbrew_operator]\n    brew_task = PgDBOperator(\n        task_id=\"all_women\",\n        sql=\"SELECT * FROM pg_available_types WHERE name IN ('brew_app', 'brew_bar')\",\n    )\n    # [END howto_pgbrew_operator]\n    drop_table_task = PgDBOperator(\n        task_id=\"drop_brewing_tables\",\n        sql=\"DROP TABLE IF EXISTS brew_app, brew_bar\",\n        dag=dag,\n    )\n\n    brew_task >> drop_table_task\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1250_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686119}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data pipeline that retrieves specific information about Tom Hanks from a Neo4j graph database on a daily basis. Design the DAG to execute a Cypher query targeting a node with Tom Hanks' details, using the current date as a dynamic parameter.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.neo4j.hooks.neo4j import Neo4jHook\nfrom airflow.providers.neo4j.operators.neo4j import Neo4jOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"neo4j_operator_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_neo4j_operator]\n\n    neo4j_operator = Neo4jOperator(\n        task_id=\"cypher_query_task\",\n        neo4j_conn_id=\"neo4j_default\",\n        sql=\"MATCH (tomHanks {name:'Tom Hanks'}) RETURN tomHanks\",\n        dag=dag,\n    )\n\n    # [END howto_neo4j_operator]\n\n    @task\n    def print_in_out(tom_hanks):\n        print(f\"Here are some info about {tome_hanks}:\")\n        print(f\"Their birth date is {tome_hanks['birthDate']}\")\n        print(f\"Their game is {tome_hanks['game']}\")\n        print(f\"Their team is {tome_hanks['team']}\")\n\n    # [START howto_neo4j_operator_with_task]\n\n    tom_hanks_info = {\n        \"name\": \"Tom Hanks\",\n        \"birthDate\": \"1956-08-29\",\n        \"game\": \"Baseball\",\n        \"team\": \"Los Angeles Dodgers\",\n    }\n\n    neo4j_operator_with_task = Neo4jOperator(\n        task_id=\"cypher_query_with_task\",\n        neo4j_conn_id=\"neo4j_default\",\n        dag=dag,\n        sql=\"MATCH (tomHanks {name:'Tom Hanks'}) RETURN tomHanks\",\n        params=tom_hanks_info,\n        python_callable=print_in_out,\n    )\n\n    # [END howto_neo4j_operator_with_task]\n\n    # [START howto_neo4j_operator_trigger]\n\n    latest_run_timer = Neo4jOperator(\n        task_id=\"latest_run_timer\",\n        neo4j_conn_id=\"neo4j_default\",\n        sql=\"DETACH DELETE * WHERE id(task) = (SELECT id FROM task where id = last_run_id)\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [END howto_neo4j_operator_trigger]\n\n    neo4j_operator >> neo4j_operator_with_task >> latest_run_timer\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2172_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068619}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code to query the current weather data from a weather API?"}, {"role": "assistant", "content": "import requests\n\ndef get_weather(city_name):\n    # Replace 'your_api_key' with your actual OpenWeatherMap API key\n    api_key = 'your_api_key'\n    \n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n    params = {\n        'q': city_name,\n        'appid': api_key,\n        'units': 'metric'  # Use 'imperial' for Fahrenheit units\n    }\n    \n    response = requests.get(base_url, params=params)\n    \n    if response.status_code == 200:\n        data = response.json()\n        main = data['main']\n        weather_desc = data['weather'][0]['description']\n        \n        return f\"Current Weather in {city_name}: Temperature: {main['temp']}\u00b0C, Description: {weather_desc}\"\n    else:\n        return f\"Failed to retrieve weather data. Status code: {response.status_code}\"\n\n# Example usage\ncity_name = input(\"Enter a city name: \")\nprint(get_weather(city_name))"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686336}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that demonstrates how to interact with Databricks SQL endpoints for creating tables, inserting data, and performing data transformations. The pipeline should showcase different SQL operations like table creation, data selection, file export, and data import using Airflow and Databricks operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_DEFAULT_INSTANCE_ID = os.environ.get(\"DATABRICKS_DEFAULT_INSTANCE_ID\", \"\")\n\n# [START howto_operator_databricks]\n\nwith models.DAG(\n    dag_id=\"example_databricks\",\n    start_date=datetime(2021, 1, 1),\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"job_cluster_spec\": {\n            \"node_type_id\": 1,\n            \"spark_version\": \"9.1.x-scala_2.11\",\n            \"aws_attributes\": {\"availability\": \"ON_DEMAND\"},\n            \"num_workers\": 8,\n        },\n    },\n    schedule=\"@daily\",\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_databricks_submit_run]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        notebook_params={\"&\": \"&\"},\n        notebook_path=\"DatabricksExampleNotebook\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_submit_run]\n\n    # [START howto_sensor_databricks_before_job_start]\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_sensor_databricks_before_job_start]\n\n    submit_run >> sensor\n\n\n# [START howto_operator_databricks_xcom_async]\nwith models.DAG(\n    dag_id=\"example_databricks_async\",\n    start_date=datetime(2021, 1, 1),  # Also supports the class method `delayed(datetime)`\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"job_cluster_spec\": {\n            \"node_type_id\": 1,\n            \"spark_version\": \"9.1.x-scala_2.11\",\n            \"aws_attributes\": {\"availability\": \"ON_DEMAND\"},\n            \"num_workers\": 8,\n        },\n    },\n    schedule=\"@daily\",\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_xcom_async]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        notebook_params={\"&\": \"&\"},\n        notebook_path=\"DatabricksExampleNotebook\",\n        databricks_conn_id=\"databricks_default\",\n        wait_for_termination=-1,\n        deferrable=True,\n    )\n\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_xcom_async]\n\n\n# [START howto_operator_databricks_xcom_jitter]\nwith models.DAG(\n    dag_id=\"example_databricks_xcom_jitter\",\n    start_date=datetime(2021, 1, 1),  # Also supports the class method `delayed(datetime)`\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"application_path\": \"/Databricks/Examples/Notebooks/Jittering.jnlp\",\n        \"application_name\": \"Jittering application\",\n    },\n    schedule=models schedules.always(),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_xcom_jitter]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        application_path=\"/Databricks/Examples/Notebooks/Jittering.jnlp\",\n        application_name=\"Jittering application\",\n        notebook_params={\"&\": \"&\"},\n        notebook_path=\"DatabricksExampleNotebook\",\n        databricks_conn_id=\"databricks_default\",\n        jitter=True,\n    )\n\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_xcom_jitter]\n\n\n# [START howto_operator_databricks_env_var]\nwith models.DAG(\n    dag_id=\"example_databricks_env_var\",\n    start_date=datetime(2021, 1, 1),  # Also supports the class method `delayed(datetime)`\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"notebook_params\": {\"env_role_name\": \"test_role\"},\n    },\n    schedule=models schedules.always(),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_env_var]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        notebook_path=\"DatabricksExampleNotebook\",\n        notebook_params={\"env_role_name\": \"test_role\"},\n        databricks_conn_id=\"databricks_default\",\n    )\n\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        notebook_params={\"env_role_name\": \"test_role\"},\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_env_var]\n\n\n# [START howto_operator_databricks_json]\nwith models.DAG(\n    dag_id=\"example_databricks_json\",\n    start_date=datetime(2021, 1, 1),  # Also supports the class method `delayed(datetime)`\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"notebook_params\": {\"json_input\": '{\"key\": \"value\"}'},\n    },\n    schedule=models schedules.always(),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_json]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        notebook_path=\"DatabricksExampleNotebook\",\n        notebook_params={\"json_input\": '{\"key\": \"value\"}'},\n        databricks_conn_id=\"databricks_default\",\n    )\n\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        notebook_params={\"json_input\": '{\"key\": \"value\"}'},\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_json]\n\n\n# [START howto_operator_databricks_macros]\nwith models.DAG(\n    dag_id=\"example_databricks_macros\",\n    start_date=datetime(2021, 1, 1),  # Also supports the class method `delayed(datetime)`\n    default_args={\n        \".databricks_conn_id\": \"databricks_default\",\n        \"new_macro_package\": \"{{ params.new_macro_package }}\",\n        \"macro_class_name\": \"{{ params.macro_class_name }}\",\n    },\n    schedule=models schedules.always(),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_databricks_macros]\n    submit_run = DatabricksSubmitRunOperator(\n        task_id=\"run_pipeline\",\n        new_macro_package=\"{{ params.new_macro_package }}\",\n        macro_class_name=\"{{ params.macro_class_name }}\",\n        notebook_path=\"DatabricksExampleNotebook\",\n        notebook_params={\"&\": \"&\"},\n        databricks_conn_id=\"databricks_default\",\n    )\n\n    sensor = DatabricksSensor(\n        task_id=\"check_job_status\",\n        job_id=\"{{ task_instance.xcom_pull(task_ids='run_pipeline', key='return_value') }}\",\n        databricks_conn_id=\"databricks_default\",\n    )\n    # [END howto_operator_databricks_macros]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686507}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated database management workflow that demonstrates cross-database interaction and query execution in a Cloud SQL environment. Create a data pipeline that showcases table creation, data insertion, and table deletion across PostgreSQL and MySQL instances.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using PostgresToSqliteOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.common.sql.operators.sql import SqlToCsvOperator\nfrom airflow.providers.common.sql.sensors.sql import SqlSensor\nfrom airflow.providers.pgsql.transfers.postgres_to_sqlite import PgSQLToSQLiteOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_pgsql_to_sqlite\"\n\n# [START howto_pgsql_to_sqlite_operator]\nwith models.DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    postgres_to_sqlite_create_table = PgSQLToSQLiteOperator(\n        task_id=\"postgress_to_sqlite_create_table\",\n        sql=\"CREATE TABLE my_table (id INT, val TEXT)\",\n        parameters={\"sqlite_path\": \"test.sqlite3\"},\n    )\n\n    postgres_to_sqlite_insert_data = PgSQLToSQLiteOperator(\n        task_id=\"postgress_to_sqlite_insert_data\",\n        sql=\"INSERT INTO my_table VALUES (1, 'val 1'), (2, 'val 2')\",\n        parameters={\"sqlite_path\": \"test.sqlite3\"},\n    )\n\n    # [START howto_pgsql_to_sqlite_sensor]\n    postgres_to_sqlite_sensor = SqlSensor(\n        task_id=\"postgress_to_sqlite_sensor\", sql=\"SELECT val FROM my_table WHERE id = ?\", timeout=5, method=\"python\"\n    )\n    # [END howto_pgsql_to_sqlite_sensor]\n\n    postgres_to_sqlite_drop_table = PgSQLToSQLiteOperator(\n        task_id=\"postgress_to_sqlite_drop_table\",\n        sql=\"DROP TABLE my_table\",\n        parameters={\"sqlite_path\": \"test.sqlite3\"},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    csv_to_json = SqlToCsvOperator(\n        task_id=\"csv_to_json\",\n        sql=\"SELECT * FROM test.sqlite3.test_table\",\n        sqlite_path=\"test.sqlite3\",\n        azure_conn_id=None,\n        replace=True,\n        output_csv=nullcontext(open(\"output.csv\", \"w\")),\n    )\n\n    # [START howto_operator_async]\n    postgres_to_sqlite_create_table_async = PgSQLToSQLiteOperator(\n        task_id=\"postgres_to_sqlite_create_table_async\",\n        sql=\"CREATE TABLE my_table (id INT, val TEXT)\",\n        parameters={\"sqlite_path\": \"test2.sqlite3\"},\n        deferrable=True,\n    )\n\n    postgres_to_sqlite_insert_data_async = PgSQLToSQLiteOperator(\n        task_id=\"postgres_to_sqlite_insert_data_async\",\n        sql=\"INSERT INTO my_table VALUES (1, 'val 1'), (2, 'val 2')\",\n        parameters={\"sqlite_path\": \"test2.sqlite3\"},\n        deferrable=True,\n    )\n    # [END howto_operator_async]\n\n    postgres_to_sqlite_drop_table_async = PgSQLToSQLiteOperator(\n        task_id=\"postgres_to_sqlite_drop_table_async\",\n        sql=\"DROP TABLE my_table\",\n        parameters={\"sqlite_path\": \"test2.sqlite3\"},\n        trigger_rule=TriggerRule.ALL_DONE,\n        deferrable=True,\n    )\n\n    csv_to_json_async = SqlToCsvOperator(\n        task_id=\"csv_to_json_async\",\n        sql=\"SELECT * FROM test2.sqlite3.test_table\",\n        sqlite_path=\"test2.sqlite3\",\n        azure_conn_id=None,\n        replace=True,\n        output_csv=nullcontext(open(\"output_async.csv\", \"w\")),\n    ]\n\n    # [START howto_pgsql_to_sqlite_external_file_operator]\n    postgres_to_sqlite_external_file = PgSQLToSQLiteOperator(\n        task_id=\"postgres_to_sqlite_external_file\",\n        sql=\"SELECT * from external_table\",\n        sqlite_path=\"external_test.sqlite3\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_pgsql_to_sqlite_external_file_operator]\n\n    (\n        postgres_to_sqlite_create_table\n        >> postgres_to_sqlite_insert_data\n        >> postgres_to_sqlite_sensor\n        >> postgres_to_sqlite_drop_table\n        >> nullcontext(open(\"success.txt\", \"w\"))\n        >> open(\"failure.txt\", \"a\"))\n\n        >> postgres_to_sqlite_create_table_async\n        >> postgres_to_sqlite_insert_data_async\n        >> postgres_to_sqlite_drop_table_async\n        >> nullcontext(open(\"success_async.txt\", \"w\"))\n        >> open(\"failure_async.txt\", \"a\")\n\n        >> postgres_to_sqlite_external_file\n    )\n# [START howto_pgsql_to_sqlite_skip_itx_operator]\n    postgres_to_sqlite_skip_csv = PgSQLToSQLiteOperator(\n        task_id=\"postgres_to_sqlite_skip_csv\",\n        sql=\"SELECT * from external_table\",\n        sqlite_path=\"external_test2.sqlite3\",\n        skip_last_records=3,\n        deferrable=True,\n    )\n# [END howto_pgsql_to_sqlite_skip_itx_operator]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_36_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686626}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that orchestrates cloud data processing tasks, including bucket creation, file uploads, template-based data processing, and automated cleanup of temporary resources.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datajoin import (\n    CloudDataJoinOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"datajoin_from_blob\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nDESTINATION_FILE = \"dest_file\"\nSTORE_ID = f\"store-{DAG_ID}-{ENV_ID}\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datajoin\"],\n):\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = LocalFilesystemToGCSOperator(\n        task_id=\"upload_file\",\n        src=FILE_NAME,\n        dst=DESTINATION_FILE,\n        bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_datajoin]\n    data_join = CloudDataJoinOperator(\n        task_id=\"data_join\",\n        project_id=PROJECT_ID,\n        source_fields={\"blob_name\": DESTINATION_FILE, \"field_name\": \"id\"},\n        destination_fields={\"table_name\": STORE_ID},\n        join_type=\"one-to-one\",\n    )\n    # [END howto_operator_datajoin]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (create_bucket >> upload_file >> data_join >> delete_bucket)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_178_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068668}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that showcases the end-to-end process of creating, managing, and analyzing image resources in a cloud environment. Develop a systematic workflow that demonstrates Google Cloud Vision's product and image management capabilities.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductsOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductLabelsOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_detect_image]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"vision_detect_image\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nMACHINE_TYPE = \"n1-standard-1\"\nPRODUCT_ID = \"my-product\"\nIMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_the_United_Nations.svg/640px-Flag_of_the_United_Nations.svg.png\"\nIMAGE_GCS_PATH = f\"gs://{BUCKET_NAME}/{DAG_ID}.png\"\n\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\nPRODUCT_SET_ID = \"my-product-set\"\nREFERENCE_IMAGE_ID = \"uni-reference-image\"\n\nVISION_DETECT_IMAGE_TASK_ID = \"detect_image\"\nVISION_CREATE_PRODUCT_TASK_ID = \"create_product\"\nVISION_CREATE_PRODUCT_SET_TASK_ID = \"create_product_set\"\nVISION_CREATE_REFERENCE_IMAGE_TASK_ID = \"create_reference_image\"\nVISION_UPDATE_PRODUCT_LABELS_TASK_ID = \"update_product_labels\"\nVISION_GET_PRODUCT_TASK_ID = \"get_product\"\nVISION_GET_PRODUCT_SET_TASK_ID = \"get_product_set\"\nVISION_LIST_PRODUCTS_TASK_ID = \"list_products\"\nVISION_LIST_PRODUCT_SETS_TASK_ID = \"list_product_sets\"\nVISION_REMOVE_PRODUCT_FROM_PRODUCT_SET_TASK_ID = \"remove_product_from_product_set\"\nVISION_ADD_PRODUCT_TO_PRODUCT_SET_TASK_ID = \"add_product_to_product_set\"\nVISION_DELETE_PRODUCT_TASK_ID = \"delete_product\"\nVISION_DELETE_PRODUCT_SET_TASK_ID = \"delete_product_set\"\nVISION_DELETE_REFERENCE_IMAGE_TASK_ID = \"delete_reference_image\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = GCSCreateObjectStorageFileOperator(\n        task_id=\"upload_file\",\n        bucket=BUCKET_NAME,\n        object_name=DAG_ID + \".png\",\n        url=IMAGE_URL,\n        replace=True,\n    )\n\n    # [START howto_operator_vision_detect_image]\n    detect_image = CloudVisionDetectImageOperator(\n        task_id=\"detect_image\",\n        image_input_uri=IMAGE_GCS_PATH,\n        language_hints=[],\n    )\n    # [END howto_operator_vision_detect_image]\n\n    # [START howto_operator_vision_create_product]\n    create_product = CloudVisionCreateProductOperator(\n        task_id=VISION_CREATE_PRODUCT_TASK_ID,\n        product_id=PRODUCT_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_create_product]\n\n    # [START howto_operator_vision_get_product]\n    get_product = CloudVisionGetProductOperator(\n        task_id=VISION_GET_PRODUCT_TASK_ID,\n        product_id=PRODUCT_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_get_product]\n\n    # [START howto_operator_vision_list_products]\n    list_products = CloudVisionListProductsOperator(\n        task_id=VISION_LIST_PRODUCTS_TASK_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_list_products]\n\n    # [START howto_operator_vision_update_product_labels]\n    update_product_labels = CloudVisionUpdateProductLabelsOperator(\n        task_id=VISION_UPDATE_PRODUCT_LABELS_TASK_ID,\n        product_id=PRODUCT_ID,\n        image_url=IMAGE_URL,\n    )\n    # [END howto_operator_vision_update_product_labels]\n\n    # [START howto_operator_vision_create_product_set]\n    create_product_set = CloudVisionCreateProductSetOperator(\n        task_id=VISION_CREATE_PRODUCT_SET_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_create_product_set]\n\n    # [START howto_operator_vision_get_product_set]\n    get_product_set = CloudVisionGetProductSetOperator(\n        task_id=VISION_GET_PRODUCT_SET_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_get_product_set]\n\n    # [START howto_operator_vision_list_product_sets]\n    list_product_sets = CloudVisionListProductSetsOperator(\n        task_id=VISION_LIST_PRODUCT_SETS_TASK_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_list_product_sets]\n\n    # [START howto_operator_vision_remove_product_from_product_set]\n    remove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator(\n        task_id=VISION_REMOVE_PRODUCT_FROM_PRODUCT_SET_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        product_id=PRODUCT_ID,\n    )\n    # [END howto_operator_vision_remove_product_from_product_set]\n\n    # [START howto_operator_vision_add_product_to_product_set]\n    add_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n        task_id=VISION_ADD_PRODUCT_TO_PRODUCT_SET_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        product_id=PRODUCT_ID,\n    )\n    # [END howto_operator_vision_add_product_to_product_set]\n\n    # [START howto_operator_vision_delete_product]\n    delete_product = CloudVisionDeleteProductOperator(\n        task_id=VISION_DELETE_PRODUCT_TASK_ID,\n        product_id=PRODUCT_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_delete_product]\n\n    # [START howto_operator_vision_delete_product_set]\n    delete_product_set = CloudVisionDeleteProductSetOperator(\n        task_id=VISION_DELETE_PRODUCT_SET_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_delete_product_set]\n\n    # [START howto_operator_vision_delete_reference_image]\n    delete_reference_image = CloudVisionDeleteReferenceImageOperator(\n        task_id=VISION_DELETE_REFERENCE_IMAGE_TASK_ID,\n        reference_image_id=REFERENCE_IMAGE_ID,\n        location_id=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_vision_delete_reference_image]\n\n    # [START howto_operator_vision_create_reference_image]\n    create_reference_image = CloudVisionCreateReferenceImageOperator(\n        task_id=VISION_CREATE_REFERENCE_IMAGE_TASK_ID,\n        product_set_id=PRODUCT_SET_ID,\n        image_url=IMAGE_URL,\n    )\n    # [END howto_operator_vision_create_reference_image]\n\n    for task in [\n        create_bucket,\n        upload_file,\n        detect_image,\n        create_product,\n        get_product,\n        list_products,\n        create_product_set,\n        get_product_set,\n        list_product_sets,\n        remove_product_from_product_set,\n        add_product_to_product_set,\n        delete_product,\n        delete_product_set,\n        delete_reference_image,\n        create_reference_image,\n    ]:\n        task.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_267_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068684}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data backup workflow that transfers data from a DynamoDB table to an S3 bucket with point-in-time recovery capabilities. Create a system that demonstrates incremental data export and backup strategies for cloud-based database management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamodbToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests_common.test_utils.watcher import watcher\n\nlog = logging.getLogger(__name__)\n\n\nDAG_ID= \"example_dynamodb_to_s3\"\n\nsys.path.append(\".acheo-tests-common\")\n\nfrom dynamodb_to_s3_operator_tester import test_operator\n\nsample_table_attributes = [\n    {\"AttributeName\": \"id\", \"AttributeType\": \"S\"},\n    {\"AttributeName\": \"name\", \"AttributeType\": \"S\"},\n]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [START howto_transfer_dynamodb_to_s3]\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=\"dynamodb-to-s3-bucket-3f5c\")\n    log.info(\"Sample Table:\")\n    sample_table = \"\"\"\n            id        | name\n            ----------| --------\n            ec76948b6n | Rama\n            896ef DDDNn | Tunde\n            3cfy67mnnn | Falana\n            \"\"\"\n\n    insert_into_dynamodb = DynamodbToS3Operator(\n        task_id=\"insert_into_dynamodb\",\n        dynamodb_table=\"test_table\",\n        s3_bucket=\"dynamodb-to-s3-bucket-3f5c\",\n        sample_data=sample_table,\n        sample_data_columns=sample_table_attributes,\n        dynamodb_input_format=\"json\",\n    )\n    # [END howto_transfer_dynamodb_to_s3]\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=\"dynamodb-to-s3-bucket-3f5c\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(create_bucket, insert_into_dynamodb, delete_bucket, watcher())\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  (\n\n\n# Needed to run the example DAG with pytest (see below)\n\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1473_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0686898}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a Google Cloud Data Catalog workflow that demonstrates creating and managing metadata for a Google Cloud Storage bucket. Design a DAG that showcases how to programmatically register, tag, and catalog cloud storage resources for improved data governance and discoverability.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogCreateEntrySplitOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogDeleteEntrySplitOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"datacatalog_entries\"\n\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"example-entry-group-{ENV_ID}\"\nENTRY_GROUP_NAME = f\"example-entry-group-{ENV_ID}\"\nENTRY_ID = \"example-entry\"\nENTRY_DISPLAY_NAME = f\"example-entry\"\nCREATE_ENTRY_GROUP_BODY = {\n    \"name\": ENTRY_GROUP_ID,\n    \"labels\": {},\n    \"display_name\": ENTRY_GROUP_NAME,\n}\nCREATE_ENTRY_BODY = {\n    \"name\": ENTRY_ID,\n    \"type_\": \"FILESET\",\n    \"name\": ENTRY_ID,\n    \"display_name\": ENTRY_DISPLAY_NAME,\n    \"metadata_template\": {\"fuzzy_match\": \"FALSE\"},\n    \"fileset_spec\": {\"taggings\": [{\"language_id\": \"EN\"}]},\n}\n# [START howto_operator_datacatalog_create_entry_gcp_link]\nCREATE_ENTRY_SPLIT_BODY = {\n    \"name\": ENTRY_ID + \"-split\",\n    \"name\": ENTRY_ID + \"-split\",\n    \"fileset_spec\": {\n        \"gcs_file_set_spec\": {\"path_expression\": f\"gs://{BUCKET}/{ENTRY_ID}/split\"}\n    },\n}\n# [END howto_operator_datacatalog_create_entry_gcp_link]\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_datacatalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group=CREATE_ENTRY_GROUP_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_datacatalog_create_entry_group]\n\n    # [START howto_operator_datacatalog_create_entry]\n    create_entry = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry\", location=LOCATION, entry_group_id=ENTRY_GROUP_ID, entry_id=ENTRY_ID, entry=CREATE_ENTRY_BODY, project_id=PROJECT_ID\n    )\n    # [END howto_operator_datacatalog_create_entry]\n\n    # [START howto_operator_datacatalog_create_entry_split]\n    create_entry_split = CloudDataCatalogCreateEntrySplitOperator(\n        task_id=\"create_entry_split\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID + \"-split\",\n        split_fraction=0.5,\n        body=CREATE_ENTRY_SPLIT_BODY,\n    )\n    # [END howto_operator_datacatalog_create_entry_split]\n\n    # [START howto_operator_datacatalog_delete_entry_split]\n    delete_entry_split = CloudDataCatalogDeleteEntrySplitOperator(\n        task_id=\"delete_entry_split\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID + \"-split\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_datacatalog_delete_entry_split]\n\n    # [START howto_operator_datacatalog_delete_entry]\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_id=ENTRY_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_datacatalog_delete_entry]\n\n    # [START howto_operator_datacatalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_datacatalog_delete_entry_group]\n\n    create_entry_group >> create_entry >> create_entry_split >> delete_entry_split >> delete_entry >> delete_entry_group\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_271_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068708}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an educational data processing pipeline that explores Airflow's task flexibility by implementing a sequence of tasks that print context, log SQL queries, perform time-based operations, and demonstrate isolated Python environment executions.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport datetime as dt\nimport os\nfrom pathlib import Path\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.hooks.sql import SqlHook\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_hook\"\n\n\n@task\ndef create_sample_table():\n    \"\"\"\n    SQL command to create table state_fact with following columns:\n    country_id (integer), year (integer), month (integer), trade_value (decimal)\n    \"\"\"\n\n    sql_command = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {DAG_ID}_sample (\n            country_id INTEGER,\n            year INTEGER,\n            month INTEGER,\n            trade_value DECIMAL\n        );\n    \"\"\"\n\n    sql_hook = SqlHook(sql_conn_id=\"conn_id_dbb\", schema=\"schema_b\")\n    sql_hook.execute(sql_command)\n\n\n@task\ndef insert_data(table_name):\n    \"\"\"\n    SQL command to insert data into table_state_fact\n    \"\"\"\n\n    sql_command = f\"INSERT INTO {table_name} VALUES (1, 2005, 4, 289.79);\"\n\n    sql_hook = SqlHook(sql_conn_id=\"conn_id_dbb\", schema=\"schema_b\")\n    sql_hook.execute(sql_command)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_data(table_name):\n    \"\"\"\n    SQL command to drop the table\n    \"\"\"\n\n    sql_command = f\"DROP TABLE IF EXISTS {table_name};\"\n\n    sql_hook = SqlHook(sql_conn_id=\"conn_id_dbb\", schema=\"schema_b\")\n    sql_hook.execute(sql_command)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=dt.datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n\n    run_this_last = BashOperator(task_id=\"run_this_last\", bash_command=\"wait\")\n\n    create_sample_table()\n    insert_data(DAG_ID + \"_sample\")\n    create_sample_table()\n    insert_data(DAG_ID + \"_sample\")\n    run_this_last\n    delete_data(DAG_ID + \"_sample\")\n    delete_data(DAG_ID)\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()"}], "metadata": {"file_name": "dag_1808_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687144}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need to convert a large dataset of 1 million rows from a CSV file into a pandas DataFrame. How can I handle this process efficiently?"}, {"role": "assistant", "content": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('large_dataset.csv')"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687232}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates advanced cloud service integration by copying an image file between buckets and performing multiple types of computer vision analysis using different Google Cloud Vision operators. Showcase how complex data processing workflows can be orchestrated with modular, configurable task chains.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductsOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductLabelsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_retry_import]\nfrom google.api_core.retry import Retry  # isort:skip\n\n# [END howto_operator_vision_retry_import]\n\n# [START howto_operator_vision_product_set_import]\nfrom google.cloud.vision_v1.types import ProductSet  # isort:skip\n\n# [END howto_operator_vision_product_set_import]\n\n# [START howto_operator_vision_reference_image_import]\nfrom google.cloud.vision_v1.types import ReferenceImage  # isort:skip\nfrom system.test_utils.watcher import watcher  # isort:skip\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_cloud_vision\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nMACHINE_TYPE = \"n1-standard-1\"\nPRODUCT_ID = \"my-product\"\nPRODUCT_SET_ID = \"my-product-set\"\nVISION_IMAGE_URL = \"gs://vision-files/labels/image_1.jpg\"\nVISION_TEXT_URL = \"gs://vision-files/text_file.txt\"\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = GCSToGCSOperator(\n        task_id=\"upload_file\",\n        source_bucket=VISION_IMAGE_URL,\n        source_object=\"test/vision/image_1.jpg\",\n        destination_bucket=BUCKET_NAME,\n    )\n\n    upload_text_file = GCSToGCSOperator(\n        task_id=\"upload_text_file\",\n        source_bucket=VISION_TEXT_URL,\n        source_object=\"test/vision/text_file.txt\",\n        destination_bucket=BUCKET_NAME,\n    )\n\n    # [START howto_operator_vision_create_product]\n    create_product = CloudVisionCreateProductOperator(\n        product_id=PRODUCT_ID, body={}, timeout=Retry(maximum=5), task_id=\"create_product\"\n    )\n    # [END howto_operator_vision_create_product]\n\n    # [START howto_operator_vision_add_product_to_product_set]\n    add_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n        product_set_id=PRODUCT_SET_ID, product_names=[f\"gs://{BUCKET_NAME}/{VISION_IMAGE_URL}\"]\n        ,\n        retry=Retry(maximum=5),\n        task_id=\"add_product_to_product_set\",\n    )\n    # [END howto_operator_vision_add_product_to_product_set]\n\n    # [START howto_operator_vision_get_product]\n    get_product = CloudVisionGetProductOperator(\n        product_id=PRODUCT_ID, timeout=Retry(maximum=5), task_id=\"get_product\"\n    )\n    # [END howto_operator_vision_get_product]\n\n    # [START howto_operator_vision_list_products]\n    list_products = CloudVisionListProductsOperator(\n        project_id=PROJECT_ID, page_size=5, task_id=\"list_products\"\n    )\n    # [END howto_operator_vision_list_products]\n\n    # [START howto_operator_vision_delete_product]\n    delete_product = CloudVisionDeleteProductOperator(product_id=PRODUCT_ID, project_id=PROJECT_ID, task_id=\"delete_product\")\n    # [END howto_operator_vision_delete_product]\n\n    # [START howto_operator_vision_retry_delete_product]\n    retry_delete_product = CloudVisionDeleteProductOperator(\n        product_id=PRODUCT_ID, project_id=PROJECT_ID, retry=(Retry(maximum=5)), task_id=\"retry_delete_product\"\n    )\n    # [END howto_operator_vision_retry_delete_product]\n\n    # [START howto_operator_vision_update_product_labels]\n    update_product = CloudVisionUpdateProductLabelsOperator(\n        product_id=PRODUCT_ID, labels={\"unlabelled\": {\"ignore\": True}}, project_id=PROJECT_ID, task_id=\"update_product\"\n    )\n    # [END howto_operator_vision_update_product]\n\n    # [START howto_operator_vision_get_product_set]\n    get_product_set = CloudVisionGetProductSetOperator(\n        product_set_id=PRODUCT_SET_ID, timeout=Retry(maximum=5), task_id=\"get_product_set\"\n    )\n    # [END howto_operator_vision_get_product_set]\n\n    # [START howto_operator_vision_list_product_sets]\n    list_product_sets = CloudVisionListProductSetsOperator(\n        project_id=PROJECT_ID, page_size=5, task_id=\"list_product_sets\"\n    )\n    # [END howto_operator_vision_list_product_sets]\n\n    # [START howto_operator_vision_create_product_set]\n    create_product_set = CloudVisionCreateProductSetOperator(\n        product_set_id=PRODUCT_SET_ID, body={}, timeout=Retry(maximum=5), task_id=\"create_product_set\"\n    )\n    # [END howto_operator_vision_create_product_set]\n\n    # [START howto_operator_vision_remove_product_from_product_set]\n    remove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator(\n        product_set_id=PRODUCT_SET_ID, product_names=[f\"gs://{BUCKET_NAME}/{VISION_IMAGE_URL}\"], retry=Retry(maximum=5), task_id=\"remove_product_from_product_set\"\n    )\n    # [END howto_operator_vision_remove_product_from_product_set]\n\n    # [START howto_operator_vision_delete_product_set]\n    delete_product_set = CloudVisionDeleteProductSetOperator(product_set_id=PRODUCT_SET_ID, project_id=PROJECT_ID, task_id=\"delete_product_set\")\n    # [END howto_operator_vision_delete_product_set]\n\n    # [START howto_operator_vision_delete_bucket]\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_vision_delete_bucket]\n\n    watch = watcher()\n    for operator in [\n        create_product, add_product_to_product_set, get_product, list_products, delete_product, retry_delete_product\n        , update_product, get_product_set, list_product_sets, create_product_set, remove_product_from_product_set, delete_product_set, delete_bucket\n    ]:\n        watch.operator_watch(operator)\n\nwith DAG(\n    \"example_transfer_gcp_vision_retry\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as retry_dag:\n    # [START howto_operator_vision_retry_copy_reference_image]\n    copy_vision_reference_image = GCSCreateBucketOperator(\n        task_id=\"copy_vision_reference_image\",\n        bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END howto_operator_vision_retry_copy_reference_image]\n\n    # [START howto_operator_vision_retry_write_align_reference_image]\n    retry_write_align_reference_image = GCSCreateBucketOperator(\n        task_id=\"write_align_reference_image\",\n        bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END howto_operator_vision_retry_write_align_reference_image]\n\n    # [START howto_operator_vision_retry_read_align_reference_image]\n    retry_read_align_reference_image = GCSCreateBucketOperator(\n        task_id=\"read_align_reference_image\",\n        bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END howto_operator_vision_retry_read_align_reference_image]\n\n    # [START howto_operator_vision_retry_create_document_label_request]\n    retry_create_document_label_request = GCSCreateBucketOperator(\n        task_id=\"create_document_label_request\", bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\", replace=True\n    )\n    # [END howto_operator_vision_retry_create_document_label_request]\n\n    # [START howto_operator_vision_retry_align_document_label_request]\n    retry_align_document_label_request = GCSCreateBucketOperator(\n        task_id=\"align_document_label_request\",\n        bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END howto_operator_vision_retry_align_document_label_request]"}], "metadata": {"file_name": "dag_597_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687397}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete data transfer lifecycle, including bucket and dataset creation, data extraction from a Trino information schema, and external table generation in BigQuery. Implement a robust, idempotent workflow that handles resource provisioning and cleanup automatically.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using TrinoToBigQueryOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.transfers.trino_to_bigquery_operator import TrinoToBigQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino_to_bigquery\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino_to_bigquery]\n    trino_to_bigquery = TrinoToBigQueryOperator(\n        task_id=\"trino_to_bigquery_task\",\n        conn_id=\"trino\",\n        sql=\"SELECT * FROM thrive.apps\",\n        destination_table=\"bq_result\",\n        source_project_dataset_id=\"test_project\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        replace=False,\n    )\n    # [END howto_operator_trino_to_bigquery]\n\n    (trino_to_bigquery >> trino_to_bigquery)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2074_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687447}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates end-to-end image processing and machine learning operations using Google Cloud Vision APIs. Develop a pipeline that transfers images, creates product catalogs, and performs automated image analysis.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vision import (\n    CloudVisionAddProductToProductSetOperator,\n    CloudVisionCreateProductOperator,\n    CloudVisionCreateProductSetOperator,\n    CloudVisionCreateReferenceImageOperator,\n    CloudVisionDeleteProductOperator,\n    CloudVisionDeleteProductSetOperator,\n    CloudVisionDeleteReferenceImageOperator,\n    CloudVisionGetProductOperator,\n    CloudVisionGetProductSetOperator,\n    CloudVisionListProductsOperator,\n    CloudVisionListProductSetsOperator,\n    CloudVisionRemoveProductFromProductSetOperator,\n    CloudVisionUpdateProductLabelsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_vision_retry_import]\nfrom google.api_core.retry import Retry  # isort:skip\n\n# [END howto_operator_vision_retry_import]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"image_label_retrive\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nMACHINE_TYPE = \"n1-standard-1\"\nPRODUCT_ID = \"my-product\"\nPRODUCT_SET_ID = \"my-product-set\"\nIMAGE_URL = \"gs://airflow-system-tests-resources/vision/image_url.jpg\"\nIMAGE_GCS = \"gs://vision-dataset/datasets/ai-platform/shapes/images/\"\nIMAGE = \"gs://vision-dataset/datasets/ai-platform/shapes/images/image6.jpg\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vision\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_image_file = GCSToGCSOperator(\n        task_id=\"upload_image_file\",\n        source_bucket=RESOURCE_DATA_BUCKET,\n        source_object=IMAGE_URL,\n        destination_bucket=BUCKET_NAME,\n        destination_object=IMAGE,\n    )\n\n    # [START howto_operator_vision_detect_labels_retry]\n    detect_labels_retry = CloudVisionRetryWithRetry().apply(_take_3)(  # type: ignore[misc]\n        task_id=\"detect_labels_retry\",\n        image_uri=IMAGE_URL,\n    )\n    # [END howto_operator_vision_detect_labels_retry]\n\n    # [START howto_operator_vision_detect_labels]\n    detect_labels = CloudVisionRetryWithRetry().task_id=\"detect_labels\")(image_uri=IMAGE)\n    # [END howto_operator_vision_detect_labels]\n\n    # [START howto_operator_vision_detect_signatures_retry]\n    detect_signatures_retry = CloudVisionRetryWithRetry().task_id=\"detect_signatures_retry\"(image_uri=IMAGE_URL)\n    # [END howto_operator_vision_detect_signatures_retry]\n\n    # [START howto_operator_vision_detect_signatures]\n    detect_signatures = CloudVisionRetryWithRetry().task_id=\"detect_signatures\"(image_uri=IMAGE)\n    # [END howto_operator_vision_detect_signatures]\n\n    # [START howto_operator_vision_create_product]\n    create_product = CloudVisionCreateProductOperator.task_id=\"create_product\"(product_id=PRODUCT_ID, location=LOCATION)\n    # [END howto_operator_vision_create_product]\n\n    # [START howto_operator_vision_add_product_to_product_set]\n    add_product_to_product_set = CloudVisionAddProductToProductSetOperator.task_id=\"add_product_to_product_set\"(\n        product_set_id=PRODUCT_SET_ID, product=PRODUCT_ID, location=LOCATION\n    )\n    # [END howto_operator_vision_add_product_to_product_set]\n\n    # [START howto_operator_vision_list_products]\n    list_products = CloudVisionListProducts.task_id=\"list_products\"()\n    # [END howto_operator_vision_list_products]\n\n    # [START howto_operator_vision_delete_product]\n    delete_product = CloudVisionDeleteProductOperator.task_id=\"delete_product\"(\n        product_id=PRODUCT_ID, location=LOCATION, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_vision_delete_product]\n\n    # [START howto_operator_vision_create_product_set]\n    create_product_set = CloudVisionCreateProductSetOperator.task_id=\"create_product_set\"(\n        product_set_id=PRODUCT_SET_ID, location=LOCATION\n    )\n    # [END howto_operator_vision_create_product_set]\n\n    # [START howto_operator_vision_get_product_set]\n    get_product_set = CloudVisionGetProductSetOperator.task_id=\"get_product_set\"(\n        product_set_id=PRODUCT_SET_ID, location=LOCATION, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_vision_get_product_set]\n\n    # [START howto_operator_vision_update_product_labels]\n    update_product_labels = CloudVisionUpdateProductLabels.task_id=\"update_product_labels\"(\n        product_set_id=PRODUCT_SET_ID, image_uri=IMAGE, labels={\"landmark\": [\"unspecified\"]}, location=LOCATION\n    )\n    # [END howto_operator_vision_update_product_labels]\n\n    # [START howto_operator_vision_remove_product_from_product_set]\n    remove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator.task_id=\"remove_product_from_product_set\"(\n        product_set_id=PRODUCT_SET_ID, product=PRODUCT_ID, location=LOCATION\n    )\n    # [END howto_operator_vision_remove_product_from_product_set]\n\n    # [START howto_operator_vision_list_references]\n    list_references = CloudVisionListReferences.task_id=\"list_references\"()\n    # [END howto_operator_vision_list_references]\n\n    # [START howto_operator_vision_delete_reference_image]\n    delete_reference_image = CloudVisionDeleteReferenceImageOperator.task_id=\"delete_reference_image\"(\n        image_uri=IMAGE_GCS + os.path.basename(IMAGE), location=LOCATION, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_vision_delete_reference_image]\n\n    for operator in [\n        detect_labels_retry,\n        detect_labels,\n        detect_signatures_retry,\n        detect_signatures,\n    ]:\n        operator.retry = Retry(maximum=1, initial=0.1, multiplier=1.0, deadline=15)\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_image_file\n        # TEST BODY\n        >> [create_product, add_product_to_product_set]\n        >> [list_products, get_product_set]\n        >> update_product_labels\n        >> remove_product_from_product_set\n        >> [list_references, delete_reference_image]\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    class CloudVisionRetryWithRetry(object):\n        def __init__(self, *args, **kwargs):\n            self.args = args\n            self.kwargs = kwargs\n\n        def retry(self, callable_function, *args, **kwargs):\n            import google.api_core.retry as core_retry\n\n            retry_instance = core_retry.Retry(\n                initial=0.1,\n                maximum=1,\n                multiplier=1.0,\n                deadline=15,\n                mode=self._mode,\n            )\n            return call_callable_with_retry(callable_function, retry_instance, *args, **kwargs)\n\n        @property\n        def _mode(self):\n            import platform\n\n            if platform.system() == \"Windows\":\n                import sys\n\n                import pytest\n\n                pytest.skip(\"gcp vision api windows support missing\")\n            return \"google.api_core.retry.if_exception_type\"]\n\n    # [END howto_operator_vision_retry]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_596_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687609}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for provisioning and managing an Amazon EKS (Elastic Kubernetes Service) cluster with dynamic nodegroup creation. Create a system that demonstrates infrastructure-as-code deployment of Kubernetes resources using Airflow's AWS provider operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksGetServerlessApplicationRunnerOperator,\n    EksStartServerlessApplicationRunnerOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eks_serverless\"\n# Externally fetched variables:\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(CLUSTER_ROLE_ARN_KEY).add_variable(SUBNETS_KEY).build()\n\n# Creating a custom IMDS with our security group ID so that we can ensure the IAM role has access to it.\nIMDS_CUSTOM_SECURITY_GROUP_ID = \"11-22-33-44-55-66-77-88-tx-223-sg\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=f\"{env_id}-cluster\",\n        region=\"{{ params.region }}\",\n        compute=\"default\",\n        vpc_subnets=subnets,\n        vpc_security_groups=[IMDS_CUSTOM_SECURITY_GROUP_ID],\n        resources_vpc_config={\n            \"subnetId\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        # Use us-east-1 DB by default\n        resource_attributes={\"locationConstraint\": \"us-east-1\"},\n        # We want this under our control, not rely on environment randomness\n        cluster_role_arn=cluster_role_arn,\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"await_create_cluster\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_name') }}\",\n        target_state=ClusterStates.ACTIVE,\n        deferrable=True,\n    )\n\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_nodegroup\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_name') }}\",\n        nodegroup_name=f\"{env_id}-nodegroup\",\n        region=\"{{ params.region }}\",\n        vpc_subnets=subnets,\n        vpc_security_groups=[IMDS_CUSTOM_SECURITY_GROUP_ID],\n        nodegroup_type=\"FARGATE\",\n        min_nodes=1,\n        max_nodes=1,\n        # Use US East (Amazon Ohio) DB by default\n        resource_attributes={\"locationConstraint\": \"us-east-1\"},\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_nodegroup', key='cluster_name') }}\",\n        nodegroup_name=\"{{ task_instance.xcom_pull('create_nodegroup', key='nodegroup_name') }}\",\n        target_state=NodegroupStates.ACTIVE,\n        deferrable=True,\n    )\n\n    # [START howto_operator_eks_get_serverless_app_runner]\n    get_app_runner_task = EksGetServerlessApplicationRunnerOperator(\n        task_id=\"get_app_runner\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_name') }}\",\n        application_name=\"{{ task_instance.xcom_pull('create_cluster', key='application_name') }}\",\n    )\n    # [END howto_operator_eks_get_serverless_app_runner]\n\n    @task(task_id=\"start_app_runner\", trigger_rule=TriggerRule.ALL_DONE)\n    def start_app_runner():\n        boto3.client(\"lambda\").invoke(\n            FunctionName=\"{{ task_instance.xcom_pull('create_cluster', key='application_name') }}\",\n            Payload='{\"command\":\"sh -c \\\"sleep 60\\\"\"}',\n        )\n\n    # [START howto_operator_eks_start_serverless_application_runner]\n    start_app_runner_task = EksStartServerlessApplicationRunnerOperator(\n        task_id=\"start_app_runner\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_name') }}\",\n        application_name=\"{{ task_instance.xcom_pull('create_cluster', key='application_name') }}\",\n        lambda_function_identifier=\"{{ task_instance.xcom_pull('get_app_runner', key='function_identifier') }}\",\n    )\n    # [END howto_operator_eks_start_serverless_application_runner]\n\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_nodegroup\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_nodegroup', key='cluster_name') }}\",\n        nodegroup_name=\"{{ task_instance.xcom_pull('create_nodegroup', key='nodegroup_name') }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_name') }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        test_context,\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        get_app_runner_task,\n        start_app_runner(),\n        delete_nodegroup,\n        delete_cluster,\n        dag,\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_732_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687737}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates a complete machine learning workflow with Google Cloud services, showcasing task dependencies, cloud storage interactions, and automated model training processes. Construct a step-by-step orchestration that moves data, creates datasets, and trains a translation model with clear operational stages.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "# mypy ignore arg types (Autogenerated)\n\"\"\"Example DAG demonstrating the usage of GCSToVertexFlowOperator.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\n\nimport pytest\n\nfrom google.cloud.translate_v3 import enums\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.vertex_ai.text\u7ffb\u8bd1 import GCSToVertexAITranslateV3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_to_vertex_ait_translate\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_gcs_json]\nGCS_FILE_PATH = \"example-translate-file.json\"\nGCS_OUTPUT_URI = f\"gs://{BUCKET_NAME}/{GCS_FILE_PATH}\"\n# [END howto_gcs_json]\n\n# [START howto_dict_input]\ninput_data = {\n    \"source_language_code\": \"en\",\n    \"target_language_code\": \"fr\",\n    \"gcs_uri\": GCS_OUTPUT_URI,\n    \"big_query_table\": \"\",\n}\n# [END howto_dict_input]\n\n\nlog = logging.getLogger(__name__)\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n)\u662f\u6b63\u786e\u7684\u5417\uff1f"}], "metadata": {"file_name": "dag_461_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0687907}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data pipeline that demonstrates task chaining, system command execution, and containerized task running, highlighting Airflow's ability to coordinate different types of computational tasks.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"singularity_example\"\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"owner\": \"airflow\",\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"singularity_image_default\",\n        conn_type=\"file\",\n        description=\"Default Docker image used for demos.\",\n        file=\"/usr/bin/centos:7\",\n    )\n\n    @task\n    def create_container(image_file):\n        \"\"\"\n        Create a singularity container without any network or volume attach.\n        When the container exits, the task is removed by itself.\n        \"\"\"\n        singularity_container = SingularityOperator(task_id=\"create_container\", image=image_file)\n        singularity_container.wait_for_completion = False\n\n    @task\n    def create_container_with_network(image_file):\n        \"\"\"\n        Create a singularity container with private network test network attached.\n        When the container exits, the task is removed by itself.\n       \"\"\"\n        singularity_container = SingularityOperator(\n            task_id=\"create_container_with_network\",\n            image=image_file,\n            network_name=\"test_net\",\n        )\n        singularity_container.wait_for_completion = False\n\n    @task\n    def create_container_with_volume(image_file):\n        \"\"\"\n        Create a singularity container with /tmp mounted as test_vol.\n        When the container exits, the task is removed by itself.\n        \"\"\"\n        singularity_container = SingularityOperator(\n            task_id=\"create_container_with_volume\",\n            image=image_file,\n            volumes={image_file: {\"bind\": \"/tmp\", \"mode\": \"rw\"}},\n        )\n        singularity_container.wait_for_completion = False\n\n    @task\n    def create_container_with_volume_output(image_file):\n        \"\"\"\n        Create a singularity container with /tmp mounted as test_vol.\n        When the container exits, the task outputs /tmp/test_vol output.\n        \"\"\"\n        singularity_container = SingularityOperator(\n            task_id=\"create_container_with_volume_output\",\n            image=image_file,\n            volumes={image_file: {\"bind\": \"/tmp\", \"mode\": \"rw\"}},\n        )\n        tmp_output = singularity_container.output[0]\n        return tmp_output\n\n    create_container(conn.id)\n    create_container_with_network(conn.id)\n    create_container_with_volume(conn.id)\n    create_container_with_volume_output(conn.id)\n\n    # [START howto_operator_singularity_xcom]\n    @task.xcom.task()\n    def get_output(xcom_in):\n        \"\"\"\n        Get the output from the xcom task created in the previous task.\n        The return value of this task is the input passed to the task\n        created in the previous task.\n        \"\"\"\n        return xcom_in\n\n    # [END howto_operator_singularity_xcom]\n\n    (create_container_with_volume() >> get_output())\n\n    for task in [create_container, create_container_with_network, create_container_with_volume, create_container_with_volume_output]:\n        task.trigger_rule = TriggerRule.ALL_DONE\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1563_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0688016}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that showcases data cube transformation and maintenance processes, demonstrating how to systematically process, update, and manage analytical data structures using Kylin operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "# [START howto_gcs_create_cube]\nGCS_CUBE = \"cube\"\nGCS_DATASET = \"airflow-system-test-cdci-dataset\"\nGCS_CREATE_BODY = {\n    \"cube\": {\"name\": GCS_CUBE},\n    \"dataset\": {\"name\": GCS_DATASET},\n    \"query\": {\"sql\": \"SELECT * FROM std WHERE name = 'sphere'\"},\n    \"precomputeOptions\": [{\"optionName\": \"SIMPLIFY\"}],\n}\n# [END howto_gcs_create_cube]\n\n\ndef test_gcs_create_cube(context, bucket_name, body):\n    \"\"\"\n    Create a cube in Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket into which the file will be created.\n    :param dict body: Body of the request.\n    \"\"\"\n    client = google.cloud.storage.client.StorageClient()\n    bucket = client.bucket(bucket_name)\n    bucket.create()\n    operation = bucket.create_object(body=body)\n    operation.result()\n    bucket.delete()\n\n\n# [START howto_gcs_delete_cubes]\ndef test_gcs_delete_cubes(context, prefix):\n    \"\"\"Delete cubes in Google Cloud Storage.\"\"\"\n    client = google.cloud.storage.client.StorageClient()\n    operations = client.list_operations(prefix)\n    for operation in operations:\n        if operation.name.endswith(f\"{prefix}+cube\"):\n            operation.delete()\n            break\n# [END howto_gcs_delete_cubes]\n\n\n# [START howto_gcs_write_yql_to_gcs]\ndef test_gcs_write_yql_to_gcs(context, body):\n    \"\"\"\n    Write YQL query to Google Cloud Storage.\n\n    :param context: Airflow context\n    :param dict body: Body of the request.\n    \"\"\"\n    client = google.cloud.bigtable.client.BigTableClient()\n    table_id = body[\"instance_id\"] + \"+\" + body[\"cell_group_id\"]\n    # Ensure table exists.\n    client.instance_exists(body[\"instance_id\"])\n    # Create instance object and store id.\n    instance = client.instance(body[\"instance_id\"])\n    cell_group_id = instance.cell_group_ids()[0]\n    # Ensure cell group exists.\n    instance.cell_group_id_exists(cell_group_id)\n    # Create cell group object and store id.\n    cell_group = instance.cell_groups[cell_group_id]\n    body[\"cell_group_id\"] = cell_group_id\n\n    # Insert yql write row.\n    response = cell_group.write_row(**body)\n    # Check the return value.\n    assert response[0].row_key() == body[\"row_key\"]\n# [END howto_gcs_write_yql_to_gcs]\n\n\n# [START howto_gcs_read_table]\ndef test_gcs_read_table(context, bucket_name, prefix):\n    \"\"\"\n    Read table in Bigtable from Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket from which the file will be read.\n    :param str prefix: Prefix of the file to read.\n    \"\"\"\n    client = google.cloud.storage.client.StorageClient()\n    bucket = client.bucket(bucket_name)\n    # [START howto_gcs_read_file]\n    blob = bucket.get_blob(prefix)\n    blob.download_to_filename(\"/tmp/test_gcs_read_table\")\n    # [END howto_gcs_read_file]\n    with open(\"/tmp/test_gcs_read_table\") as file:\n        content = file.read()\n        assert \"yeo\" in content\n\n\n# [START howto_gcs_create_view]\ndef test_gcs_create_view(context, bucket_name, body):\n    \"\"\"\n    Create view in Bigtable from Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket from which the file will be read.\n    :param dict body: Body of the request.\n    \"\"\"\n    client = google.cloud.bigtable.client.BigTableClient()\n    table_id = body[\"instance_id\"] + \"+\" + body[\"cell_group_id\"]\n    # Ensure table exists.\n    client.instance_exists(body[\"instance_id\"])\n    # Create instance object and store id.\n    instance = client.instance(body[\"instance_id\"])\n    cell_group_id = instance.cell_group_ids()[0]\n    # Ensure cell group exists.\n    instance.cell_group_id_exists(cell_group_id)\n    # Create cell group object and store id.\n    cell_group = instance.cell_groups[cell_group_id]\n    body[\"cell_group_id\"] = cell_group_id\n\n    # Insert create view write row.\n    response = cell_group.create_view(write_time_span=body[\"write_time_span\"], **body)\n    # Check the return value.\n    assert response.row_key() == body[\"row_key\"]\n# [END howto_gcs_create_view]\n\n\n# [START howto_gcs_update_view]\ndef test_gcs_update_view(context, bucket_name, body):\n    \"\"\"\n    Update view in Bigtable from Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket from which the file will be read.\n    :param dict body: Body of the request.\n    \"\"\"\n    client = google.cloud.bigtable.client.BigTableClient()\n    table_id = body[\"instance_id\"] + \"+\" + body[\"cell_group_id\"]\n    # Ensure table exists.\n    client.instance_exists(body[\"instance_id\"])\n    # Create instance object and store id.\n    instance = client.instance(body[\"instance_id\"])\n    cell_group_id = instance.cell_group_ids()[0]\n    # Ensure cell group exists.\n    instance.cell_group_id_exists(cell_group_id)\n    # Create cell group object and store id.\n    cell_group = instance.cell_groups[cell_group_id]\n    body[\"cell_group_id\"] = cell_group_id\n\n    # Insert create view write row.\n    response = cell_group.create_view(write_time_span=body[\"write_time_span\"], **body)\n    # Check the return value.\n    assert response.row_key() == body[\"row_key\"]\n\n    # Update view.\n    cell_group.update_view(response.key, read_time_span=body[\"read_time_span\"])\n    # Check the return value.\n    assert (\n        cell_group.view_for_cell_group(\n            google.cloud.bigtable.enums.CellGroupRange.full_cell_range,\n            bigtable.ReadTimeSpan.from_dict(body[\"read_time_span\"]), None),\n        body[\"read_time_span\"],\n    )\n# [END howto_gcs_update_view]\n\n\n# [START howto_gcs_delete_view]\ndef test_gcs_delete_view(context, bucket_name, body):\n    \"\"\"\n    Delete view in Bigtable from Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket from which the file will be read.\n    :param dict body: Body of the request.\n    \"\"\"\n    client = google.cloud.bigtable.client.BigTableClient()\n    table_id = body[\"instance_id\"] + \"+\" + body[\"cell_group_id\"]\n    # Ensure table exists.\n    client.instance_exists(body[\"instance_id\"])\n    # Create instance object and store id.\n    instance = client.instance(body[\"instance_id\"])\n    cell_group_id = instance.cell_group_ids()[0]\n    # Ensure cell group exists.\n    instance.cell_group_id_exists(cell_group_id)\n    # Create cell group object and store id.\n    cell_group = instance.cell_groups[cell_group_id]\n\n    # Delete view.\n    cell_group.delete_view(body[\"view_id\"])\n# [END howto_gcs_delete_view]\n\n\n# [START howto_gcs_read_views]\ndef test_gcs_read_views(context, bucket_name, prefix):\n    \"\"\"\n    Read views in Bigtable from Google Cloud Storage.\n\n    :param context: Airflow context\n    :param str bucket_name: Bucket from which the file will be read.\n    :param str prefix: Prefix of the file to read.\n    \"\"\"\n    client = google.cloud.bigtable.client.BigTableClient()\n    table_id = prefix.replace(\"-\", \"+\")\n    # Ensure table exists.\n    client.instance_exists(table_id)\n\n    # Create instance object and store id.\n    instance = client.instance(table_id)\n\n    # Get full range cell group.\n    cell_group = instance.cell_groups[google.cloud.bigtable.enums.CellGroupRange.full_cell_range]\n    # Read view.\n    reader = cell_group.view_for_cell_group(google.cloud.bigtable.enums.CellGroupRange.full_cell_range, None, None)\n    # Iterate over cells in the view.\n    for cell in reader:\n        print(cell)\n# [END howto_gcs_read_views]"}], "metadata": {"file_name": "dag_1944_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068817}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Kubernetes volume management workflow that automatically creates and deletes a persistent volume claim (PVC) with a 5GB storage allocation. The workflow should run daily and demonstrate basic Kubernetes resource lifecycle management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.cncf.kubernetes.operators.lol import LOLOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"world\")\nDAG_ID = \"example_lol_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_this = BashOperator(task_id=\"run_lolo\", bash_command=\"lol -c 5 -n 3\")\n\n    run_this >> LOLOperator(task_id=\"lol_call\")\n    LOLOperator(task_id=\"second Lol call\") >> BashOperator(task_id=\"second task\", bash_command=\"lol -c 1\")\n\n    # [START howto_operator_lol]\n    lol_task = LOLOperator(task_id=\"lol_task\")\n    # [END howto_operator_lol]\n\n    lol_task >> BashOperator(bash_command=\"lol -c 5 -n 3\", task_id=\"bash_after_lol\")\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1153_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068821}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that transfers data from a BigQuery table to a Google Cloud Storage bucket. Create an Airflow DAG that demonstrates how to extract data from a BigQuery dataset and export it to a cloud storage location.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToBigQueryOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\ntry:\n    from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nexcept ImportError:\n    # Submodule error fix for GH#19857\n    from airflow.operators.gcs_to_gcs import GCSToGCSOperator\n\npytest.skipif(\n    os.environ.get(\"SYSTEM_TESTS_RUNNING\", \"\") != \"\",\n    reason=\"Skip this test when SYSTEM_TESTS_RUNNING is set.\",\n)\n\ndefault_args = {\"start_date\": datetime(2022, 1, 1)}  # By default always run the task\n\n\n@pytest.mark.parametrize(\"external_source\", [True, False])\ndef test_gcs_to_bigquery_operator(tmp_bucket, external_source):\n    \"Test example GCS to BigQuery transfer\"\n    if external_source:\n        source_object = f\"{tmp_bucket}/external-table.csv\"\n        destination_table = f\"{tmp_bucket}/destination-table.csv\"\n    else:\n        source_object = f\"{tmp_bucket}/public_ghibli films.csv\"\n        destination_table = f\"{tmp_bucket}/bigquery_output.csv\"\n\n    if tmp_bucket in (\"test-gcs-not-empty\", ):\n        return\n\n    create_bucket = GCSToGCSOperator(\n        task_id=GCSTO_GCS_TASK_ID,\n        source_objects=[source_object],\n        destination_objects=[destination_table],\n    )\n\n    read_data = GCSToBigQueryOperator(\n        task_id=GCSToBigQueryTaskId,\n        connection_id=\"google_cloud_platform\",\n        source_project_dataset_table=f\"gs://{tmp_bucket}/public_ghibli*f.csv\",\n        destination_project_dataset_table=destination_table,\n        write_disposition=\"WRITE_TRUNCATE\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"id\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"film\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    create_bucket >> read_data\n\n\nDAG_ID = \"gcs_to_bigquery\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args=default_args,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcs_to_bigquery]\n    gcs_to_bigquery_task = GCSToBigQueryOperator(\n        task_id=GCSToBigQueryTaskId,\n        connection_id=\"google_cloud_platform\",\n        source_project_dataset_table=f\"abhishekkulkarni.airflow_example_table\",\n        destination_project_dataset_table=f\"gs://{tmp_bucket}/bigquery_output.csv\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"id\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"film\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n    # [END howto_operator_gcs_to_bigquery]\n\n    # [START howto_operator_gcs_to_bigquery_external]\n    gcs_to_bigquery_external_task = GCSToBigQueryOperator(\n        task_id=GCSToBigQueryExternalTaskId,\n        connection_id=\"google_cloud_platform\",\n        source_project_dataset_table=f\"abhishekkulkarni.airflow_example_table\",\n        destination_project_dataset_table=f\"gs://{tmp_bucket}/bigquery_output.csv\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"id\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"film\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        external_source_size=\"-1\",\n    )\n    # [END howto_operator_gcs_to_bigquery_external]\n\n    # [START howto_operator_gcs_to_bigquery_deferrable]\n    gcs_to_bigquery_deferrable_task = GCSToBigQueryOperator(\n        task_id=GCSToBigQueryDeferrableTaskId,\n        connection_id=\"google_cloud_platform\",\n        source_project_dataset_table=f\"abhishekkulkarni.airflow_example_table\",\n        destination_project_dataset_table=f\"gs://{tmp_bucket}/bigquery_output.csv\",\n        write_disposition=\"WRITE_TRUNCATE\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"id\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"film\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        deferrable=True,\n    )\n    # [END howto_operator_gcs_to_bigquery_deferrable]\n\n    # Task dependencies created by default for the tasks above.\n    (\n        create_bucket >> gcs_to_bigquery_external_task\n        >> gcs_to_bigquery_deferrable_task\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1579_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068832}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates a complete data integration process from external file sources to a Databricks SQL environment, including steps for table creation, data import, transformation, and output generation. The workflow should highlight the flexibility of connecting and manipulating data across different storage systems and compute environments.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDATABRICKS_DEFAULT_INSTANCE_ID = os.environ.get(\"DATABRICKS_DEFAULT_INSTANCE_ID\", \"\")\nDATABRICKS_CONN_ID = os.environ.get(\"DATABRICKS_CONN_ID\", \"databricks_default\")\n\nwith models.DAG(\n    dag_id=\"example_databricks_submit_sensor\",\n    start_date=datetime(2021, 1, 1),\n    default_args={\n        \"retries\": 2,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n) as dag:\n    # [START howto_operator_databricks_sensor]\n    check_job_state = DatabricksSensor(\n        databricks_conn_id=DATABRICKS_CONN_ID,\n        job_id=-1,\n        task_id=\"check_job_state\",\n    )\n\n    # [END howto_operator_databricks_sensor]\n\n    # [START howto_operator_databricks_submit_run]\n    create_and_run_pipeline = DatabricksSubmitRunOperator(\n        task_id=\"create_and_run_pipeline\",\n        databricks_conn_id=DATABRICKS_CONN_ID,\n        sql=\"CREATE TABLE my_new_table AS SELECT * FROM my_source_TABLE;\"\n        \"RUN SET DEFAULT DBRI\"\n        \"SKINS_DIR TO '/databricks/skins/IntelliJ IDEA 2021.2.xsd';\"\n        \"DROP TABLE my_source_TABLE;\",\n        new_instance_id=DATABRICKS_DEFAULT_INSTANCE_ID,\n    )\n    # [END howto_operator_databricks_submit_run]\n\n    check_job_state >> create_and_run_pipeline\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_791_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068837}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a streaming data processing system that captures real-time taxi ride events, transforms them using Apache Beam, and routes the processed data through Google Cloud Pub/Sub topics with automated infrastructure management. Create a scalable and repeatable data streaming solution that demonstrates cloud-native data processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreatePipelineOperator,\n    DataflowDeletePipelineOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"dataflow_java\"\nLOCATION = \"europe-west2-b\"\n\nPIPELINE_NAME = f\"{DAG_ID}-{ENV_ID}\".replace(\"_\", \"-\")\nPIPELINE_TYPE = \"JAVA\"\n# [START howto_operator_dataflow_create_java_pipeline]\nCREATE_JAVA_PIPELINE = DataflowCreatePipelineOperator(\n    task_id=\"create-java-pipeline\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    pipeline_id=PIPELINE_NAME,\n    java_pipeline_config={\n        \"stages\": {\n            \"inputReader\": {\"operator\": \"DummyReader\"},\n            \"mainCalculation\": {\"operator\": \"DummyCalculator\"},\n            \"outputWriter\": {\"operator\": \"DummyWriter\"},\n        },\n        \"container_config\": {\"image_uri\": \"us-docker.pkg.dev/dataflow-repo/system-test-jar/latest-output-ctr:latest\"},\n    },\n)\n# [END howto_operator_dataflow_create_java_pipeline]\nelse:\n    CREATE_JAVA_PIPELINE = BashOperator(task_id=\"create-java-pipeline\", bash_command=\"\")\n\n\nDAGs = [\n    CREATE_JAVA_PIPELINE,\n]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[DAG_ID],\n) as dag:\n    delete_dag = DataflowDeletePipelineOperator(\n        task_id=\"delete-dag\",\n        pipeline_id=PIPELINE_NAME,\n        location=LOCATION,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_dag >> dag\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_from_test_case  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run_from_test_case(dag)"}], "metadata": {"file_name": "dag_1215_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0688424}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an infrastructure deployment pipeline that automatically provisions a Kubernetes cluster with secure metadata handling and demonstrates the ability to run containerized workloads. Create a scalable and repeatable process for initializing cloud-native compute resources with built-in compliance and verification steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksDeployApplicationOperator,\n    EksGetApplicationOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eks_with_application\"\n\n# Externally fetched variables:\nCLUSTER_NAMESPACE_KEY = \"CLUSTER_NAMESPACE\"\nCLUSTER_NAME_KEY = \"CLUSTER_NAME\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    # NOTE: eks operators rely on the cluster namespace being set for access to services within the\n    # eks system breake the isolation of the cluster and can cause noise in the test results if\n    # changed.  Setting a default value here which can be overriden using --namespace in the test\n    .add_variable(CLUSTER_NAMESPACE_KEY, default=\"default-system-breain\")\n    .make()\n)\n\n\n@task\ndef get_issuer_ref(cluster_name: str):\n    root_ca_service = boto3.service_name_to_aws_service_name(\"root-ca\")\n    root_ca_endpoint_url = boto3.client(root_ca_service).describe_endpoints(\n        ServiceName=[root_ca_service],\n    )[\"Endpoints\"][0][\"EndpointUrl\"]\n\n    return f\"{root_ca_endpoint_url}/imersus\"\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_nodegroup(cluster_name: str, nodegroup_name: str):\n    eks = boto3.client(\"eks\")\n    try:\n        response = eks.describe_nodegroup(\n            name=nodegroup_name,\n        )\n        cluster_arn = response[\"Nodegroups\"][0][\"ClusterArn\"]\n        nodegroup_arn = response[\"Nodegroups\"][0][\"Arn\"]\n        eks.delete_nodegroup(\n            name=nodegroup_name,\n            clusterArn=cluster_arn,\n            forceDelete=True,\n        )\n        eks.get_waiter(NodegroupStates.DETACHED).wait(\n            ClusterName=cluster_name,\n            NodegroupName=nodegroup_name,\n        )\n    except ClientError as e:\n        if \"nodegroup not found\" not in str(e):\n            raise\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster(cluster_name: str):\n    eks = boto3.client(\"eks\")\n    try:\n        eks.delete_cluster(\n            clusterName=cluster_name,\n        )\n        eks.get_waiter(ClusterStates.DETACHED).wait(\n            ClusterName=cluster_name,\n        )\n    except ClientError as e:\n        if \"cluster not found\" not in str(e):\n            raise\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_name = test_context[CLUSTER_NAME_KEY]\n    cluster_namespace = test_context[CLUSTER_NAMESPACE_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    get_issuer_reference_for_cluster = get_issuer_ref(cluster_name)\n\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=cluster_name,\n        cluster_namespace=cluster_namespace,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=\"standard\",\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"await_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.DETACHED,\n    )\n\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=\"test-nodegroup\",\n        nodegroup_subnets=subnets,\n        nodegroup_role_arn=get_issuer_reference_for_cluster(),\n        desired_count=1,\n        machine_type=\"t3.micro\",\n        vpc_only=True,\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=\"test-nodegroup\",\n        target_state=NodegroupStates.DETACHED,\n    )\n\n    deploy_app = EksDeployApplicationOperator(\n        task_id=\"deploy_app\",\n        cluster_name=cluster_name,\n        application_name=\"test-app\",\n        image=\"centos/7:latest\",\n        pod_resources={\"nano\": 1},\n        is_vpc_linked=true,\n    )\n\n    get_issuer_ref_for_cluster = get_issuer_reference_for_cluster()\n    get_issuer_ref_for_cluster >> create_cluster\n    create_cluster >> await_create_cluster\n    create_nodegroup >> await_create_nodegroup\n    deploy_app >> get_issuer_ref_for_cluster\n\n    delete_cluster(cluster_name)\n    delete_nodegroup(cluster_name, \"test-nodegroup\")\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_387_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0688565}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering solution that demonstrates how to ingest raw data from an external source, transform it using a cloud-based data preparation tool, and generate a clean, structured output in a specified cloud storage location. Develop a reproducible data processing workflow that can be easily configured and scaled.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG for demonstrating the usage of BigQueryToGCSOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateTableOperator,\n    BigQueryDeleteDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_bigquery_to_gcs\"\n\nDATASET_NAME = f\"dataset_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nTABLE_NAME = \"table_name\"\nGCS_OBJ_1 = \"test-1.txt\"\nGCS_OBJ_2 = \"test-2.txt\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=DATASET_NAME, project_id=PROJECT_ID\n    )\n\n    create_table = BigQueryCreateTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        table_resource={\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"value\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                ],\n            },\n            \"timePartitioning\": {\n                \"dayStart\": 2017,\n                \"field\": \"DAY\",\n            },\n        },\n    )\n\n    # [START howto_operator_bigquery_to_gcs]\n    bigquery_to_gcs = BigQueryToGCSOperator(\n        task_id=\"bigquery_to_gcs_task\",\n        source_project_dataset_tables=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        destination_cloud_storage_uris=[f\"gs://{BUCKET_NAME}/{GCS_OBJ_1}\"],\n    )\n    # [END howto_operator_bigquery_to_gcs]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_dataset]\n        >> create_table\n        # TEST BODY\n        >> bigquery_to_gcs\n        # TEST TEARDOWN\n        >> [delete_bucket, delete_dataset]\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1225_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0688643}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that performs a complete data transfer operation between two database tables, highlighting the steps of source data preparation, data movement, and destination table validation. Demonstrate a systematic approach to moving data with clear, traceable steps from source to destination.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.s3.hooks.s3 import S3Hook\nfrom airflow.providers.s3.transfers.sql_to_s3 import SQLToS3Operator\n\nS3_BUCKET = \"bucket-airflow-ddl\"\nS3_KEY = \"s3/output.csv\"\nS3_SCHEMA = {\"tablename\": \"table_name\", \"columns\": [\"column_name\"]}\nSCHEMA_TABLE = \"{{ task_instance.xcom_pull(task_ids='get_schema') }}\"\n\n\n@task\ndef create_table(s3_hook: S3Hook, **kwargs):\n    \"\"\"\n    Example function to create table. You can build sql command for creating table\n    \"\"\"\n    s3_hook.create_table(\n        bucket=name(S3_BUCKET),\n        table=SCHEMA_TABLE[\"tablename\"],\n        columns=SCHEMA_TABLE[\"columns\"],\n    )\n\n\n@task\ndef drop_table(s3_hook: S3Hook, **kwargs):\n    \"\"\"\n    Example function to drop table\n    \"\"\"\n    s3_hook.drop_table(bucket=name(S3_BUCKET), table=SCHEMA_TABLE[\"tablename\"])\n\n\nwith DAG(\n    \"example_sql_to_s3\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_table(S3_BUCKET)\n    # [START howto_transfer_sql_to_s3]\n    transfer_data = SQLToS3Operator(\n        task_id=\"transfer_data\",\n        sql=\"select * from temp.db.table_name\",\n        schema=S3_KEY,\n        replace=True,\n    )\n    # [END howto_transfer_sql_to_s3]\n    drop_table(S3_BUCKET)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1827_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.0688689}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a multi-step cloud-based audio processing workflow, showing how to chain together Google Cloud operators for text-to-speech synthesis and speech translation. The DAG will showcase task dependencies, cloud service integration, and automated workflow management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.text_to_speech import (\n    CloudTextToSpeechSynthesizeOperator,\n)\nfrom airflow.providers.google.cloud.operators.translate_speech import CloudTranslateSpeechOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_translate_speech\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_google_cloud_text_to_speech_args]\nINPUT = {\"text\": \"Sample text for demo purposes\"}\nVOICE = {\"language_code\": \"en-US\", \"ssml_gender\": \"FEMALE\"}\n# [END howto_google_cloud_text_to_speech_args]\n\n# [START howto_google_cloud_translate_speech_args]\nDESTINATION = {\"uri\": {\"gcs_bucket\": BUCKET_NAME}}\nAUDIO_CONFIG = {\"audio_encoding\": \"LINEAR16\"}\n# [END howto_google_cloud_translate_speech_args]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    text_to_speech_synthesize_task = CloudTextToSpeechSynthesizeOperator(\n        project_id=PROJECT_ID,\n        input_data=json.dumps(INPUT),\n        voice=VOICE,\n        target_url=DESTINATION,\n        task_id=\"text_to_speech_synthesize_task\",\n    )\n\n    translate_speech_task = CloudTranslateSpeechOperator(\n        project_id=PROJECT_ID, input_config=AUDIO_CONFIG, target_url=DESTINATION, task_id=\"translate_speech_task\"\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> text_to_speech_synthesize_task\n        >> translate_speech_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as done above)\n    # for run/test framework usage\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2108_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983472.068875}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can we determine the mode of a dataset in Python, especially for a small dataset like [4, 5, 2, 1, 4, 5, 4, 3]?"}, {"role": "assistant", "content": "from collections import Counter\n\n# Define the dataset\ndataset = [4, 5, 2, 1, 4, 5, 4, 3]\n\n# Use Counter to count occurrences of each number\nfrequency_counter = Counter(dataset)\n\n# Find the most common element (mode)\nmost_common_element = frequency_counter.most_common(1)[0][0]\n\nprint(f\"The mode of the dataset is: {most_common_element}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3291204}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated workflow for deploying and managing a simple containerized task on AWS ECS Fargate. Create a DAG that demonstrates creating an ECS cluster, registering a task definition, running a container with a basic command, and then cleaning up resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "# ECSTaskDefinition example with Python image and execution role.\n#"}], "metadata": {"file_name": "dag_1087_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3291564}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Given a string of words, write a Python function to split the string into words and sort them lexicographically. Assume that the words are separated by spaces."}, {"role": "assistant", "content": "def sort_words_in_string(input_string):\n    # Split the input string into words based on whitespace\n    words = input_string.split()\n    \n    # Sort the list of words using sorted() with a custom key (lambda)\n    sorted_words = sorted(words, key=lambda word: word.lower())\n    \n    return sorted_words\n\n# Example usage:\ninput_str = \"banana apple cherry date\"\nsorted_list = sort_words_in_string(input_str)\nprint(sorted_list)  # Output: ['apple', 'banana', 'cherry', 'date']"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.32917}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an employee salary data processing pipeline that transforms raw employee data from BigQuery using Google Cloud Dataflow. Create a workflow that loads employee salary information, applies transformations, and writes results to a new BigQuery table.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow trigger operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowCreatePipelineOperator,\n    DataflowDeletePipelineOperator,\n    DataflowTriggerPipelineOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"dataflow_pipeline_trigger\"\nPLOT_PIPELINE_NAME = f\"plot-{DAG_ID}-{ENV_ID}\"\nNUM_PARALLEL_STREAMS = 16\n\n# [START howto_operator_dataflow_pipeline_trigger_def]\nPOLICY = {\n    \"deadlines\": {\"minSeconds\": 2},\n    \"parallelism\": {\"numWorkers\": NUM_PARALLEL_STREAMS},\n    \"job_name_prefix\": PLOT_PIPELINE_NAME,\n}\n\nPARAMETERS = {\"input_file\": \"gs://airflow-system-tests-resources/dataproc/shuffle.txt\"}\n\n# [END howto_operator_dataflow_pipeline_trigger_def]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_create_dataflow_pipeline_trigger]\n    create_plot_pipeline = DataflowCreatePipelineOperator(\n        task_id=\"create_plot_pipeline\",\n        plot=POLICY,\n        parameters=PARAMETERS,\n        project_id=PROJECT_ID,\n        container_source=\"GSOURCE\",\n    )\n    # [END howto_operator_create_dataflow_pipeline_trigger]\n\n    # [START howto_operator_trigger_dataflow_pipeline]\n    trigger_plot_pipeline = DataflowTriggerPipelineOperator(\n        task_id=\"trigger_plot_pipeline\", pipeline_id=create_plot_pipeline.output, project_id=PROJECT_ID\n    )\n    # [END howto_operator_trigger_dataflow_pipeline]\n\n    # [START howto_operator_delete_dataflow_pipeline]\n    delete_plot_pipeline = DataflowDeletePipelineOperator(\n        task_id=\"delete_plot_pipeline\",\n        pipeline_id=create_plot_pipeline.output,\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_delete_dataflow_pipeline]\n\n    create_plot_pipeline >> trigger_plot_pipeline >> delete_plot_pipeline\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1218_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329177}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data engineering workflow that transforms raw city data by creating schemas, generating tables with predefined structures, and inserting sample city records using Trino as the data processing platform. The pipeline should demonstrate how to systematically build and populate database objects with incremental complexity.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.operators.trino import TrinoRunSQLOperator\nfrom airflow.providers.trino.sensors.trino import TrinoRecordSensor\n\n# [START howto_trino_dag]\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    default_args={\"conn_id\": \"trino_conn\"},\n    tags=[\"example\"],\n) as dag:\n    # [START howto_trino_run_sql_operator]\n    run_the_query = TrinoRunSQLOperator(task_id=\"run_the_query\", sql=\"SELECT 42 AS answer\")\n    # [END howto_trino_run_sql_operator]\n\n    # [START howto_trino_sensor]\n    check_the_query = TrinoRecordSensor(\n        task_id=\"check_the_query\",\n        sql=\"SELECT 42 AS answer\",\n        record={ \"answer\": 42 },\n        timeout=5,\n        poke_interval=2,\n    )\n    # [END howto_trino_sensor]\n\n    run_the_query >> check_the_query\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it (as passed/failed)\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2235_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3291824}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data movement pipeline that generates random binary files and demonstrates cloud storage data transfer and replication strategies. Create an automated workflow that shows how to programmatically manage file uploads, bucket interactions, and cross-bucket synchronization using Airflow and Google Cloud Storage operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage GCSSynchronizeBucketsOperator and\nGCSToGCSOperator example.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSSynchronizeBucketsOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcs_to_gcs\"\n\nBUCKET_NAME_SRC = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_NAME_DST = f\"bucket_dst_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file\"\nUPLOAD_FILE_PATH = f\"gcs/{FILE_NAME}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"gcs\", \"example\"],\n):\n\n    @task\n    def create_files():\n        # [START create_files]\n        from datetime import datetime\n\n        current_date = datetime.now()\n        filename = f\"{current_date.strftime('%Y-%m-%d')}-file\"\n\n        local_file_path = f\"{UPLOAD_FILE_PATH}/{filename}\"\n\n        upload_data = {\n            \"source_bucket\": \"\",\n            \"source_object\": \"\",\n            \"destination_bucket\": BUCKET_NAME_DST,\n            \"destination_object\": filename,\n        }\n\n        local_filesystem_to_gcs(*upload_data.values())\n        # [END create_files]\n\n    @task\n    def create_bucket():\n        # [START create_bucket]\n        create_bucket_body = {\"predefined_acl\": \"public_read_write\"}\n\n        create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME_SRC, body=create_bucket_body)\n        # [END create_bucket]\n\n        create_bucket >> local_file_path\n\n    @task\n    def delete_files():\n        # [START delete_files]\n        delete_files_body = {\"recursive\": True}\n\n        delete_files(bucket_name=BUCKET_NAME_DST, body_delete=delete_files_body)\n        # [END delete_files]\n\n    (create_files() >> create_bucket() >> local_file_path)\n\n    copy_task = GCSCreateBucketOperator(\n        task_id=\"copy_bucket\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n    )\n\n    move_task = GCSCreateBucketOperator(\n        task_id=\"move_bucket\",\n        bucket_name=BUCKET_NAME_SRC,\n        project_id=PROJECT_ID,\n        next_step_copy_bucket(copy_task.output),\n    )\n\n    # [START gcstogcs_operator]\n    gcs_to_gcs = GCSToGCSOperator(\n        task_id=\"gcs_to_gcs\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=FILE_NAME,\n    )\n    # [END gcstogcs_operator]\n\n    # [START gcstogcs_operator_async]\n    gcs_to_gcs_def = GCSToGCSOperator(\n        task_id=\"gcs_to_gcs_async\",\n        source_bucket=BUCKET_NAME_SRC,\n        source_object=FILE_NAME,\n        destination_bucket=BUCKET_NAME_DST,\n        destination_object=FILE_NAME,\n        deferrable=True,\n    )\n    # [END gcstogcs_operator_async]\n\n    # [START copy_bucket_to_bucket]\n    copy_bucket_to_bucket = GCSSynchronizeBucketsOperator(\n        task_id=\"copy_bucket_to_bucket\",\n        src_bucket=BUCKET_NAME_SRC,\n        dst_bucket=BUCKET_NAME_DST,\n    )\n    # [END copy_bucket_to_bucket]\n\n    # [START move_bucket_to_bucket]\n    move_bucket_to_bucket = GCSSynchronizeBucketsOperator(\n        task_id=\"move_bucket_to_bucket\",\n        src_bucket=BUCKET_NAME_SRC,\n        dst_bucket=BUCKET_NAME_DST,\n        move_object=True,\n    )\n    # [END move_bucket_to_bucket]\n\n    delete_files() >> copy_task >> move_task >> gcs_to_gcs >> gcs_to_gcs_def >> move_bucket_to_bucket >> \\\n        copy_bucket_to_bucket"}], "metadata": {"file_name": "dag_1650_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329193}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates end-to-end machine learning orchestration with clear task dependencies and cloud-based infrastructure management. Build a workflow that creates a dataset, trains an AutoML model, and automatically cleans up resources after model training.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import GoogleApiFactory\nfrom system.second_party.vertex_ai.utils import default_args, list_functions\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = os.environ.get(\"GCP_REGION\", \"us-central1\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\n# These fields cannot be documented while importing, they are protected by internal decorators\nGROUP_NAME_PREFIX = \".\".join([\"vertex_ai\", \"auto_ml\"])\nTEST_FUNCTION_NAME = \"test_function_name\"\n\nRESOURCE_DATA_BUCKET = \"system-tests-resources\"\n\nOBJECT_DISPLAY_NAME = f\"{DAG_ID}_{ENV_ID}\".replace(\"_\", \"-\")\nMODEL_DISPLAY_NAME = f\"model_{ENV_ID}\".replace(\"_\", \"-\")\nAUTOML_DATASET_BUCKET = (\n    f\"{RESOURCE_DATA_BUCKET}/vertex-ai/datasets/automl/\"\n)\nDATASET_GCS_URI = f\"gs://{AUTOML_DATASET_BUCKET}/indian-vegetables.csv\"\nTEXT_EMBEDDER_CONFIG = {\"text_feature_layers\": [{\"type\": \"TEXT_VECTOR\"}}]\nVIDEO_EMBEDDER_CONFIG = {\n    \"video_feature_layers\": [\n        {\"type\": \"VIDEO_CONTENT\"},\n        {\"type\": \"VIDEO_FRAME_COUNT\"},\n        {\"type\": \"VIDEO_HEIGHT\"},\n        {\"type\": \"VIDEO_WIDTH\"},\n        {\"type\": \"VIDEO_FRAMES_PER_SECOND\"},\n    ]\n}\nUS_EMBEDDER_CONFIG = {\n    \"user_specific_embedding_config\": [\n        {\"type\": \"VIDEO_CONTENT\"},\n        {\"type\": \"VIDEO_FRAME_COUNT\"},\n        {\"type\": \"VIDEO_HEIGHT\"},\n        {\"type\": \"VIDEO_WIDTH\"},\n        {\"type\": \"VIDEO_FRAMES_PER_SECOND\"},\n    ]\n}\n\n\nwith GoogleApiFactory() as google_api_factory:\n    test_organization_id = os.environ.get(\"TESTOrganizationID\")\n    vision_api = google_api_factory.gcp_vision_api\n    text_detection_service = google_api_factory.text_detection_service\n    video_detection_service = google_api_factory.video_detection_service\n    if not test_organization_id:\n        raise ValueError(\"TESTOrganizationID environment variable is missing\")\n\n    # TEST SETUP\n    # Create region specific bucket name\n    AUTOML_DATASET_BUCKET = f\"bucket_{REGION}_automl\"  # type: ignore[assignment]\n\n    # Create a temporary resource directory\n    from tests_common.test_utils.system_tests import get_test_run_output\n\n    test_run_output_path = get_test_run_output(google_api_factory, REGION, ENV_ID)\n\n    # Munge the test run output path to ensure it is safe to use in an environmental variable.\n    AUTOML_DATASET_BUCKET = AUTOML_DATASET_BUCKET.replace(\"/\",\"\")\n    VIDEO_EMBEDDER_CONFIG[\"video_feature_layers\"] = [US_EMBEDDER_CONFIG]\n    TEXT_EMBEDDER_CONFIG[\"text_feature_layers\"] = [TEXT_EMBEDDER_CONFIG]\n\n    # Example of creating a bigquery table and populating it with sample data\n    tables_create_query = f\"CREATE OR REPLACE TABLE {PROJECT_ID}.{REGION}.example_bq (col_1 STRING, col_2 STRING) \"\n    tables_insert_query = f\"INSERT INTO {PROJECT_ID}.{REGION}.example_bq VALUES ( 'val_1_0', 'val_2_0'),( 'val_1_1', 'val_2_1')\"  # noqa\n\n    # Example of calling Vision Detect Image API\n    image = {}\n    detection_request = vision_detection_service.DetectImageRequest()\n    detection_request.image = image\n\n    # Example of calling Vision Detect Video API\n    video = {}\n    detection_request.video = video\n\n    # Example of calling Text Detection Service\n    text_detection = text_detection_service.DetectText(request_body={\"pleaseProvideYourInput\": \"please have some good input\"})  # type: ignore[call-arg]\n\n    # Example of calling Video Detection Service\n    video_detection = video_detection_service.DetectVideoRequest()\n    detection_request.video = video\n\n    def test_function_before_scan():\n        print(f\"Creating bucket {AUTOML_DATASET_BUCKET}...\")\n        # An automl dataset bucket should exist when this script is executed successfully\n        # Note: This bucket should be deleted when finished or cleaned up prior to starting another test in the suite\n\n        # Create dataset\n        import pytest\n\n        if not pytest.system_tests.skip_test_org_acc:\n            print(f\"Creating dataset {PROJECT_ID}.{REGION}.example_bq in storage...\")\n            BigQueryDataset = schema.dataset.BigQueryDataset\n            dataset = bigquery.Dataset(\n                f\"{PROJECT_ID}.{REGION}.example_bq\"\n            )\n            dataset.resource = {\n                \"display_name\": OBJECT_DISPLAY_NAME,\n                \"size_gb\": 10,\n                \"location\": REGION,\n            }\n            dataset.insert()\n\n            # Populate dataset\n            print(f\"Populating dataset {PROJECT_ID}.{REGION}.example_bq...\")\n            operators = [\n                bigquery.InsertOperation(\n                    project_id=PROJECT_ID,\n                    dataset_id=f\"{PROJECT_ID}.{REGION}.example_bq\",\n                    operation_type=\"INSERT\",\n                    insert_data={\n                        f\"{PROJECT_ID}.{REGION}.example_bq\": [\n                            [\"val_1_0\", \"val_2_0\"],\n                            [\"val_1_1\", \"val_2_1\"],\n                        ],\n                    },\n                )]\n            bigquery.Job.run(*operators)\n\n    def test_function_after_scan(error):\n        if error:\n            print(\"Failed to complete test!\")\n            return\n        print(\"Success! Cleaning up resources...\")\n        # Remove bucket created for automated dataset\n        google_api_factory.urllib.request.urlopen(\n            google_api_factory.urllib.request.Request(\n                f\"DELETE/{AUTOML_DATASET_BUCKET}\",\n                headers={google_api_factory.urllib.request.HTTPHEADER[0]: \"x-goog-api-key=1234567890\"},\n            )\n        )\n\n    # TEST BODY\n    # Example of using TextEmbeddingService\n    example_text_input = {\"multilanguage_text\": {\"document\": {\"pages\": [{\"content\": \"Sample text 1.\"}]}}}  # type: ignore[call-arg]\n    example_text_response = text_detection_service.EmbedDocumentsText(request_body=example_text_input)[\"embeddings\"][0]  # type: ignore[call-arg]\n\n    # Example of using VideoEmbeddingService\n    example_video_input = {\"user\u63d0\u4f9b\u7684\u89c6\u9891\": {\"path\": DATASET_GCS_URI + \"/you-video.mp4\"}}  # type: ignore[call-arg]\n    example_video_response = video_detection_service.EmbedDocumentsVideo(request_body=example_video_input)[\"embeddings\"][0]  # type: ignore[call-arg]\n\n    # Ensure that the request body is json serialized\n    assert example_text_response.embedding.length_in_bytes > 0\n    assert example_video_response.embedding.length_in_bytes > 0\n\n    # Example of calling AutoML Training Job List\n    job_list_size = 10\n    # result = []\n    # jobs = auto_ml_training_job.List()\n    # for job in jobs:\n    #   result.append(job)\n    #   if len(result) == job_list_size:\n    #     break\n\n    # TEST TEARDOWN\n    # Delete the bucket created above\n    print(f\"Deleting bucket {AUTOML_DATASET_BUCKET}\")\n    google_api_factory.urllib.request.urlopen(\n        google_api_factory.urllib.request.Request(\n            f\"DELETE/{AUTOML_DATASET_BUCKET}\",\n            headers={google_api_factory.urllib.request.HTTPHEADER[0]: \"x-goog-api-key=1234567890\"},\n        )\n    )\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    doc_md=__doc__,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n    default_args=default_args,\n    params={},\n    items=[\n        test_function_before_scan(),\n        test_function_after_scan(lambda x: x.error != None),\n        lambda: TEXT_EMBEDDER_CONFIG,\n        lambda: VIDEO_EMBEDDER_CONFIG,\n        lambda: DATASET_GCS_URI,\n        lambda: OBJECT_DISPLAY_NAME,\n        lambda: MODEL_DISPLAY_NAME,\n        lambda: DAG_ID,\n        lambda: ENV_ID,\n        lambda: REGION,\n        lambda: PROJECT_ID,\n        *list_functions(vision_api),\n        *list_functions(text_detection_service),\n        *list_functions(video_detection_service),\n        *list_functions(bigquery.Dataset),\n        *list_functions(auto_ml_training_job.AutoMLTrainingJob),\n    ],\n) as dag:\n    pass\n\nfrom tests_common.test_utils.system_tests import get_test_run_results\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run_results = get_test_run_results(dag)"}], "metadata": {"file_name": "dag_2115_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292136}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u8bf7\u7ed9\u6211\u4e00\u4e2aPython\u4ee3\u7801\u793a\u4f8b\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e24\u4e2a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3002"}, {"role": "assistant", "content": "def calculate_average(num1, num2):\n    if num1 == 0 and num2 == 0:\n        return \"Both numbers must be non-zero to compute the average.\"\n    elif num1 == 0 or num2 == 0:\n        return f\"One of the numbers is zero, so the average cannot be computed: {num1} or {num2}.\"\n    else:\n        return (num1 + num2) / 2\n\n# \u793a\u4f8b\u7528\u6cd5\nnumber1 = float(input(\"\u8bf7\u8f93\u5165\u7b2c\u4e00\u4e2a\u6570\u5b57: \"))\nnumber2 = float(input(\"\u8bf7\u8f93\u5165\u7b2c\u4e8c\u4e2a\u6570\u5b57: \"))\n\naverage = calculate_average(number1, number2)\nprint(f\"\u8fd9\u4e24\u4e2a\u6570\u5b57\u7684\u5e73\u5747\u503c\u662f: {average}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329223}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated data pipeline that transfers cocktail recipe data from an S3 bucket to a Redshift database, demonstrating end-to-end cloud data migration. Create a workflow that provisions cloud resources, loads sample data, and establishes necessary connections for data transfer.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\n# [START howto_transfer_s3_to_redshift]\ndag_id = \"s3_to_redshift\"\nsys_test_context_task = SystemTestContextBuilder().build()\n\nexactly_one_record = {\"EXACTLY_ONE_RECORD\": True}\n\n\n@task\ndef get_aws_db_iam_roleArn(ctx):\n    \"\"\"\n    Get DB IAM role ARN for the stack created in `test_prepare_env`. This role is used to\n    connect to the redshift cluster from within SageMaker.\n    \"\"\"\n    env_id = ctx[ENV_ID_KEY]\n    return f\"arn:aws:iam::{env_id}:role/db-agent\"\n\n\n@task\ndef create_s3_bucket(bucket_name):\n    s3 = boto3.client(\"s3\")\n    s3.create_bucket(Bucket=bucket_name)\n\n\nwith DAG(\n    dag_id=dag_id,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    db_iam_role_arn = get_aws_db_iam_arn(test_context)\n\n    s3_bucket_name = f\"{env_id}-s3-bucket\"\n\n    s3_bucket_create = S3CreateBucketOperator(task_id=\"s3_bucket_create\", bucket_name=s3_bucket_name)\n\n    s3_object_data = {\n        \"S3Bucket\": s3_bucket_name,\n        \"Key\": \"aws_table_data.json\",\n        \"Data\": '{\"AWSAccountID\": \"1234567890\", \"FileName\": \"testing-s3-to-redshift\"}',\n        \"ContentType\": \"text/json\",\n    }\n\n    s3_create_object = S3CreateObjectOperator(task_id=\"s3_object_create\", override_config=True, **s3_object_data)\n\n    # [START howto_transfer_s3_to_redshift_operator]\n    run_pipeline = S3ToRedshiftOperator(\n        task_id=\"run_pipeline\",\n        s3_bucket=f\"s3://{s3_bucket_name}/\",\n        target_table=\"aws_table\",\n        db_iam_role_arn=db_iam_role_arn,\n        sql_statement=f\"SELECT * FROM aws_table\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_transfer_s3_to_redshift_operator]\n\n    delete_stack = S3DeleteBucketOperator(\n        task_id=\"delete_stack\", bucket_name=s3_bucket_name, delete_objects=True, force_delete=True\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        sys_test_context_task(),\n        s3_bucket_create,\n        s3_object_create,\n        # TEST BODY\n        run_pipeline,\n        # TEST TEARDOWN\n        delete_stack,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_755_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292325}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the step-by-step operational control of a cloud-based machine learning notebook, showcasing how to orchestrate infrastructure management tasks using AWS SageMaker operators. Develop a workflow that demonstrates dynamic instance management with clear, sequential task dependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookOperator,\n    SageMakerDeleteNotebookOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerNotebookSensor,\n    SageMakerNotebookStatusSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_sagemaker_notebook\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nlog = logging.getLogger(__name__)\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_DATA_BUCKET = \"sample-sagemaker-notebook\"\nSAMPLE_DATA_FILE = \"notebook-sample.csv\"\n\n# Notebook definition\n# [START sagemaker_notebook_template]\nSAGEmaker_NOTEBOOK_TEMPLATE = {\n    \"Name\": \"test-notebook\",\n    \"CodeUri\": \"\",\n    \"RoleArn\": \"arn:aws:iam::824567601851:role/Role-123456789000000\",\n}\n# [END sagemaker_notebook_template]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    bucket = \"s3://{}\".format(SAMPLE_DATA_BUCKET)\n    object_key = \"{}.csv\".format(SAMPLE_DATA_BUCKET)\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket)\n\n    # [START howto_operator_sagemaker_create_notebook]\n    create_notebook = SageMakerCreateNotebookOperator(\n        task_id=\"create_notebook\",\n        image=\"amazon/aws-samle-agency-notebook:latest\",\n        role_arn=role_arn,\n        s3_output_path=f\"{s3_output_path}\",\n        s3_input_path=f\"{bucket}/{object_key}\",\n        name=\"test-notebook\",\n    )\n    # [END howto_operator_sagemaker_create_notebook]\n\n    # [START howto_sensor_sagemaker_notebook_status]\n    wait_on_notebook = SageMakerNotebookSensor(\n        task_id=\"wait_on_notebook\", name=\"test-notebook\"\n    )\n    # [END howto_sensor_sagemaker_notebook_status]\n\n    delete_bucket = SageMakerDeleteNotebookOperator(\n        task_id=\"delete_bucket\", name=\"test-notebook\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    chain(\n        create_bucket,\n        create_notebook,\n        wait_on_notebook,\n        delete_bucket,\n    )\n\n    # Task dependencies created via `@task >> task` are not managed by Airflow, because these tasks\n    # are defined by Handligners and therefore not part of the external taskchain. So, we cannot\n    # delete them from the list of tasks. In this case, it is required a minimum of two lines\n    # below to have the same result.\n    assert len(chain(create_bucket, create_notebook, wait_on_notebook)) == len(delete_bucket)\n\n    # ### Test the Sensor ###\n    # dt = datetime.now().strftime(\"%Y-%m-%d\")\n    # log.info(\"Creating dataset %s-%s\", GroupId, Name)\n    # s3_output_path = \"s3://group-datasets/{}/{}\".format(dt, Name)\n    # self.client.create_dataset(\n    #     DatasetName=name,\n    #     InputDataConfig={\n    #         \"S3Source\": {\n    #             \"S3DataType\": \"S3Prefix\",\n    #             \"S3DataDistributionType\": \"FullyReplicated\",\n    #             \"CompressionType\": 0,\n    #             \"CloudFrontDistributionId\": '',\n    #             \"S3KeyId\": ''\n    #         }\n    #     },\n    #     OutputDataConfig={\n    #         \"S3OutputPath\": s3_output_path\n    #     },\n    #     WorkoutDataConfig=None,\n    #     ExperimentName=None\n    # )\n\n    # await DatasetCreationCompletionSensor(dataset_name=name, cluster_arn=cluster_arn, task_id='wait_on_dataset_creation_completion')\n\n    # ### End Test the Sensor ###\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_759_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292437}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated CloudFormation stack management workflow that demonstrates creating and deleting an AWS SQS queue using Airflow operators and sensors. Design a DAG that showcases infrastructure provisioning and teardown with proper error handling and sequential task execution.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef get_stack_id(stack_name: str):\n    # Returns the ID of the specified stack.\n    command = f\"aws cloudformation describe-stacks --stack-name {stack_name}\"\n    return json.loads(subprocess.check_output(command.split()))[\"Stacks\"][0][\"StackId\"]\n\n\n# Fetch the status of a stack. This example function is used as a sensor to wait\n# for a stack to be CREATE_COMPLETE before returning. It can be used by the user or\n# as a trigger condition in another operator.\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef wait_for_stack_status(stack_id: str, stack_name: str, state: str):\n    log.info(\"Waiting for stack %s to reach state %s\", stack_name, state)\n    command = f\"aws cloudformation get-stack-status --stack-id {stack_id} --timeout-milliseconds 99999\"\n    response = json.loads(subprocess.check_output(command.split()))\n    while response[\"StackStatus\"] != state:\n        log.info(\"Stack {} is at state {}. Sleeping and trying again in 20 seconds.\".format(\n            stack_name, response[\"StackStatus\"]\n        ))\n        time_to_sleep = 20\n        try:\n            next_status_update_timeout_command = (\n                \"aws cloudformation get-stack-status \"\n                f\"--stack-id {stack_id} --timeout-milliseconds {time_to_sleep * 1000}\"\n            )\n            next_status_update_timeout_response = json.loads(\n                subprocess.check_output(next_status_update_timeout_command.split())\n            )\n            if next_status_update_timeout_response[\"StackStatus\"] == state:\n                raise Exception(\n                    f\"The stack {stack_name} has stalled at the stage {response['StackStatus']}.\" +\n                    f\"It should have progressed to stage {state}.\"\n                )\n            time_to_sleep = int(time_to_sleep / 2)\n        except subprocess.CalledProcessError:\n            continue\n        finally:\n            log.info(\"Exiting to retry if needed.\")\n            # Exit the loop so the top level handler can re-run this task.\n            break\n\n\nwith DAG(\n    dag_id=\"example_cloudformation_sensor\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    # Creation of a stack takes about a minute to complete, so the sensor will not wait that whole time.\n    exp_stack_name = \"example-resource\"\n\n    # Create a stack\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=exp_stack_name,\n        template_body=json.dumps(\n            {\n                \"Resources\": {\n                    \"MyQueue\": {\n                        \"Type\": \"AWS::SQS::Queue\",\n                    }\n                },\n            }\n        ),\n        on_finish=lambda _self, _data, _context: True,  # Don't wait for delete to finish.\n    )\n\n    # Wait for the stack to finish creation before continuing\n    wait_for_stack_create = CloudFormationCreateStackSensor(\n        task_id=\"wait_for_stack_create\",\n        stack_name=exp_stack_name,\n        target_status=\"CREATE_COMPLETE\",\n        timeout=60 * 5,  # Timeout after 5 minutes.\n    )\n\n    # Delete the stack\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=exp_stack_name,\n        on_finish=lambda self, data, context: True,  # Don't wait for deletion to finish.\n    )\n\n    # Wait for the stack to finish deletion before exiting\n    wait_for_stack_delete = CloudFormationDeleteStackSensor(\n        task_id=\"wait_for_stack_delete\",\n        stack_name=exp_stack_name,\n        target_status=\"DELETE_COMPLETE\",\n        timeout=60 * 5,  # Timeout after 5 minutes.\n    ))\n\n    # Depend on create stack completion\n    chain(create_stack, wait_for_stack_create)\n\n    # Depend on delete stack completion\n    chain(delete_stack, wait_for_stack_delete)\n\n    # Depend on both sensors finishing\n    (create_stack >> wait_for_stack_create >> wait_for_stack_delete >> delete_stack)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1878_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292568}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I need a function that calculates the factorial of a number in Python. Can you provide one?"}, {"role": "assistant", "content": "def factorial_recursive(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial_recursive(n - 1)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292625}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a complete machine learning workflow from data ingestion to model training and cleanup. Develop a systematic task sequence that handles Google Cloud Storage operations, dataset creation, model training, and resource management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import GoogleCloudApiSession\nfrom system.openai import OpenAIAsyncEmbeddingVectorService\n\nCHECK_FOR_UPDATE_INTERVAL = 1\n\n\ndef get_deployment_prediction_model_output_prediction_embedding(\n    display_name: str, project_id: str\n):\n    #  NOTE: This deployment prediction method requires at least 1 object to work.\n    #       You can directly pass objects of your type for detection in the following call.\n    model = openai_embedding_vector_service.deployment_prediction(\n        display_name=get_deployment_prediction_model_display_name(project_id, display_name),\n        input_objects=[openai_embedding_vector_service.InputObjects.objects_with_names(obj_names=[\"person_1\"])),\n        model_specs=[\n            openai_embedding_vector_service.ModelSpec(\n                display_name=\"example-deployment-model\", type_=schema.Model.Type.text_embedding\n            )\n        ],\n        headers={\n            \"X-GCP-User-Agent\": \"openai-embedding-detection-sample\"\n        },\n    )\n    return model.output.predictions.embedding\n\n\ndef post_process_output_embeddings(embeddings_list):\n    final_result = []\n    for embeddings in embeddings_list:\n        largest_value_index = embeddings[:, -1].argmax()\n        largest_value = embeddings[largest_value_index]\n        result = {\n            \"object_name\": largest_value[-2],\n            \"prediction\": largest_value[-1],\n        }\n        final_result.append(result)\n    return final_result\n\n\ndef check_update(name, client):\n    # A new model is updated if its name is updated.\n    latest_element = client.list_deployment_predictions()[\u201celements\u201d]  # Operate on newer results if exists.\n    return bool(len(latest_element) and latest_element[0].name != name)\n\n\n# Example DAG for Vertex AI AutoML Embedding Vector Service Testing.\ndef main():\n    test_project_id = os.environ.get(\"TEST_PROJECT_ID\", \"\")\n    display_name = f\"test-deployment-{os.environ.get('GCP_USER_AGENT', '')}\"\n    location = \"us-central1\"\n\n    session = GoogleCloudApiSession()\n\n    # Automatically creates a location if it doesn't exist.\n    client = openai_embedding_vector_service.AutoMLDeploymentPredictionClient()\n    project_location_path = client.project_location_path(location, test_project_id)\n    deployment_prediction_display_name = f\"{display_name}/{test_project_id}\"\n\n    # Create deployment prediction.\n    print(f\"Creating deployment prediction model {display_name} in location {location}...\")\n    operation_request = {\"display_name\": display_name, \"context\": {}}\n    location_session = session.client(session, region=location)\n    operation = location_session.client.gapie_.operations().create(\n        request=operation_request, parent=project_location_path\n    )\n    deployment_prediction_operation_id = location_session.next_page_operation(operation)\n\n    # Wait until deployment prediction is completely created\n    partial_deployed = False\n    while not (deployment_prediction_operation_id.operation_status() == \"DONE\" or partial_deployed):\n        time.sleep(CHECK_FOR_UPDATE_INTERVAL)\n        if partial_deployed:\n            break\n        operation = location_session.client.gapie_.operations().get(name=deployment_prediction_operation_id.name)\n        print(\"Deployment prediction status %s\", operation.operation_status())\n\n    # Update deployment prediction.\n    if deployment_prediction_operation_id.operation_status() == \"DONE\":\n        print(\"Updating already existing deployment prediction is not supported.\")\n        return None\n    print(\"Updating deployment prediction...\")\n    new_display_name = f\"{display_name}-updated\"\n    update_mask = {\n        \"paths\": [\n            \"displayName\",\n        ]\n    }\n    location_session = session.client(session, region=location)\n    deployment_prediction_service = location_session.client.gapie_.automl_deployment_predicitions()\n    deployment_prediction_service.set d\u00e9cidure(prediction_display_name=new_display_name, body=request)\n    deployment_prediction_operation_id = deployment_prediction_service.patch(\n        name=deployment_prediction_operation_id.name, body=request, update_mask=update_mask\n    )\n\n    # Wait until deployment prediction is completely updated\n    while not check_update(deployment_prediction_operation_id.name, client):\n        time.sleep(CHECK_FOR_UPDATE_INTERVAL)\n\n    # Get the deployment prediction.\n    print(\"Getting deployment prediction...\")\n    deployment_prediction_service = client\n    deployment_prediction_operation_id = location_session.client.gapie_.operations().get(\n        name=deployment_prediction_operation_id.name\n    )\n    # TODO : Add error checking\n    print(\"deployment prediction id:\", deployment_prediction_operation_id.id)\n    print(\"deployment prediction output:\", deployment_prediction_operation_id.output())\n    print(\"deployment prediction next page token:\", deployment_prediction_operation_id.next_page_token())\n\n    # List the deployment predictions in the previous order.\n    print(\"Listing deployment predictions before filtering...\")\n    request = {\"order_specification\": {\"order_by\": [{\"field\": \"creation_timestamp\", \"sort_order\": \"ASC\"}]}}\n\n    # TODO : Add error checking\n    responses = location_session.client.gapie_.automl_deployment_predicitions().list(\n        parent=project_location_path, request=request\n    )\n    for response in responses:\n        print(response)\n\n    # Filter on the newly created display name.\n    print(\"Filtering deployment predictions by display name...\")\n    request = {\"page_size\": 100, \"filter\": f\"display_name='{new_display_name}'\"}\n    responses = location_session.client.gapie_.automl_deployment_predicitions().list(\n        parent=project_location_path, request=request\n    )\n    for response in responses:\n        print(response)\n\n    # List the deployment predictions again after filtering.\n    print(\"List all deployment predictions now...\")\n    request = {\"order_specification\": {\"order_by\": [{\"field\": \"creation_timestamp\", \"sort_order\": \"ASC\"}]}}\n    responses = location_session.client.gapie_.automl_deployment_predicitions().list(\n        parent=project_location_path, request=request\n    )\n    for response in responses:\n        print(response)\n\n    # Delete the deployment prediction.\n    # Note: We cannot use delete instead of deleting first and then recreating it later as\n    # several tests rely on specific display name being used.\n    print(\"Deleting deployment prediction...\")\n    location_session = session.client(session, region=location)\n    deployment_prediction_service = location_session.client.gapie_.automl_deployment_predicitions()\n    operation = deployment_prediction_service.delete(name=deployment_prediction_operation_id.name)\n    operation_id = operation.name\n    # Wait until deletion completes\n    while operation.status != \"DELETED\":\n        print(\"Deletion operation ID:\", operation_id)\n        break\n    print(\"Deleted deployment prediction\")\n\n\nif __name__ == \"__main__\":\n    main()"}], "metadata": {"file_name": "dag_1312_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292794}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "\u5982\u4f55\u5728Python\u4e2d\u4f7f\u7528\u88c5\u9970\u5668\u6765\u68c0\u67e5\u6743\u9650\uff1f"}, {"role": "assistant", "content": "def check_permission(permission):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # \u68c0\u67e5\u7528\u6237\u662f\u5426\u6709\u6307\u5b9a\u7684\u6743\u9650\n            if permission not in request.user.permissions:\n                raise PermissionError(\"\u60a8\u6ca1\u6709\u8db3\u591f\u7684\u6743\u9650\")\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# \u793a\u4f8b\uff1a\u5b9a\u4e49\u4e00\u4e2a\u9700\u8981\u7ba1\u7406\u5458\u6743\u9650\u624d\u80fd\u8bbf\u95ee\u7684\u65b9\u6cd5\n@check_permission('admin')\ndef admin_only_function():\n    print(\"\u53ea\u6709\u7ba1\u7406\u5458\u53ef\u4ee5\u8bbf\u95ee\")\n\n# \u8c03\u7528\u65b9\u6cd5\ntry:\n    admin_only_function()\nexcept PermissionError as e:\n    print(e)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292866}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates a complete machine learning lifecycle for image classification, showcasing how to programmatically manage cloud machine learning infrastructure and model deployment processes. The DAG should illustrate task dependencies and cloud service interactions for an end-to-end ML workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud ML Engine service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.mlengine import (\n    MLEngineCreateArtifactFromBucketOperator,\n    MLEngineCreateModelOperator,\n    MLEngineCreateVersionOperator,\n    MLEngineDeleteArtifactFromBucketOperator,\n    MLEngineDeleteModelOperator,\n    MLEngineDeleteVersionOperator,\n    MLEngineGetModelOperator,\n    MLEngineListVersionsOperator,\n    MLEngineListModelsOperator,\n)\nfrom airflow.providers.google.cloud.sensors.mlengine import ModelStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"world\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"ml_engine\"\nBUCKET_NAME = f\"bucket-{DAG_ID}-{ENV_ID}\"\n\nLOCATION = \"us-central1\"\nMODEL_NAME = f\"model_{ENV_ID}\"\nARTIFACTuger_ZIP_FILE_NAME = \"image-classifier.zip\"\nMODEL_SERVING_CONTAINER = {\"image_uri\": \"gcr.io/sandbox-178946/model\"}\npython_file = \"word_count.py\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"ml-engine\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    upload_file = GCSCreateBucketFileOperator(\n        task_id=\"upload_file\",\n        gcp_project=PROJECT_ID,\n        gcp_location=LOCATION,\n        filename=python_file,\n        bucket_name=BUCKET_NAME,\n        object_name=\"test-upload-file.txt\",\n    )\n\n    upload_zip_file = GCSCreateBucketFileOperator(\n        task_id=\"upload_zip_file\",\n        gcp_project=PROJECT_ID,\n        gcp_location=LOCATION,\n        filename=ARTIFACTuger_ZIP_FILE_NAME,\n        bucket_name=BUCKET_NAME,\n        object_name=\"test-upload-zip-file/image-classifier.zip\",\n    )\n    # [START howto_mlengine_create_model]\n    create_model = MLEngineCreateModelOperator(\n        task_id=\"create-model\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_display_name=MODEL_NAME,\n        artifact_uri=f\"gs://{BUCKET_NAME}/test-upload-file.txt\",\n        serving_container=MODEL_SERVING_CONTAINER,\n    )\n    # [END howto_mlengine_create_model]\n\n    # [START howto_mlengine_create_version]\n    create_version = MLEngineCreateVersionOperator(\n        task_id=\"create-version\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_id=model_id(),\n        version_display_name=\"version_test\",\n    )\n    # [END howto_mlengine_create_version]\n\n    # [START howto_mlengine_get_version]\n    get_version = MLEngineGetModelOperator(\n        task_id=\"get-version\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_id=model_id(),\n        version_display_name=\"version_test\",\n    )\n    # [END howto_mlengine_get_version]\n\n    # [START howto_mlengine_list_versions]\n    list_versions = MLEngineListVersionsOperator(\n        task_id=\"list-versions\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        page_size=50,\n    )\n    # [END start_mlengine_list_versions]\n\n    # [START howto_mlengine_delete_version]\n    delete_version = MLEngineDeleteVersionOperator(\n        task_id=\"delete-version\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_id=MODEL_NAME,\n        version_display_name=\"version_test\",\n    )\n    # [END start_mlengine_delete_version]\n\n    # [START howto_mlengine_get_artifact_from_bucket]\n    get_artifact_from_bucket = MLEngineGetArtifactFromBucketOperator(\n        task_id=\"get-artifact-from-bucket\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_id=MODEL_NAME,\n        zip_file=\"gs://vision-service-bucket/imagenet_to_iris_classifier.zip\",\n    )\n    # [END start_mlengine_get_artifact_from_bucket]\n\n    # [START howto_mlengine_create_model_from_bucket]\n    create_artifact_from_bucket = MLEngineCreateArtifactFromBucketOperator(\n        task_id=\"create-artifact-from-bucket\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        model_id=MODEL_NAME,\n        bucket_name=\"gs://vision-service-bucket\",\n        zip_file=\"imagenet_to_iris_classifier.zip\",\n    )\n    # [END start_mlengine_create_model_from_bucket]\n\n    # [START howto_mlengine_delete_model]\n    delete_model = MLEngineDeleteModelOperator(\n        task_id=\"delete-model\", project_id=PROJECT_ID, location=LOCATION, model_id=MODEL_NAME\n    )\n    # [END start_mlengine_delete_model]\n\n    # [START howto_mlengine_delete_bucket]\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END start_mlengine_delete_bucket]\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        >> upload_zip_file\n        # TEST BODY\n        >> create_model\n        >> create_version\n        >> get_version\n        >> list_versions\n        >> delete_version\n        >> get_artifact_from_bucket\n        >> create_artifact_from_bucket\n        >> delete_model\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    # Test dependency defined by `model_id` parameter in create_model operator\n    delete_version.depends_on_past = True\n    delete_model.depends_on_past = True\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1317_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3292994}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data processing workflow that demonstrates basic Pig script execution in a local Hadoop environment. Create a DAG that runs a simple Pig command to list directory contents as a learning example of task configuration.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_run_pipeline_local\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_adf]\n    run_this = ADFOperator(\n        task_id=\"run_pig_job\",\n        adf_conn_id=\"spark-pi-adf\",\n        pig=\"ls /;\",\n    )\n    # [END howto_operator_adf]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\nif __name__ == \"__main__\":\n    tests_common.test_utils.run_test_case(\"ADF/run_pipeline_local\", test_run)"}], "metadata": {"file_name": "dag_1536_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329303}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated metadata management pipeline that allows dynamic configuration and manipulation of data catalog tag templates for standardizing data asset descriptions and classifications. The workflow will enable systematic tracking and enhancement of metadata across cloud data environments.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage Tag Template operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n    GCSDeleteObjectsOperator,\n    GCSListObjectsOperator,\n)\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcp_gcs_tag_template\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nBUCKET_NAME = f\"test-gcs-tag-{ENV_ID}\"\nLOCATION = \"europe-north1\"\nCLUSTER_NAME = f\"cluster-{ENV_ID}\"\nDATA_PROPOSAL_ZONES = [\"us-central1\"]\nGCS_BUCKET_URL = f\"gs://{BUCKET_NAME}/\"\nTIMEOUT = 1200\nSWEEP_PERIOD = 15\n\n# Cluster definition\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"config\": {\n            \"machine_type\": \"n1-standard-4\",\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"device_name\": CLUSTER_NAME,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"100\",\n                        \"source_image\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n                    },\n                }\n            ],\n        },\n        \"network_config\": {\"network\": \"global/networks/Default\"},\n    },\n    \"worker_config\": {\n        \"config\": {\n            \"machine_type\": \"n1-standard-4\",\n            \"disks\": [\n                {\n                    \"boot\": True,\n                    \"device_name\": CLUSTER_NAME,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"100\",\n                        \"source_image\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n                    },\n                }\n            ],\n        },\n        \"network_config\": {\"network\": \"global/networks/Default\"},\n    },\n}\n\n# Job to create tags on GCS Bucket\n# [START howto_operator_gcs_create_tags_json]\nJOB_CREATE_TAGS_JSON = {\n    \"reference\": {\n        \"project_id\": PROJECT_ID,\n        \"region\": LOCATION,\n        \"cluster_name\": CLUSTER_NAME,\n    },\n    \"paths\": [GCS_BUCKET_URL],\n    \"tags\": [{\"type\": \"DIR_LIST\", \"expression\": \"/*\"}],\n    \"wait_for_completion\": True,\n}\n# [END howto_operator_gcs_create_tags_json]\n\n# Job to create tags on GCS Bucket file\n# [START howto_operator_gcs_create_tags_file]\nJOB_CREATE_TAGS_FILE = {\n    \"reference\": {\n        \"project_id\": PROJECT_ID,\n        \"region\": LOCATION,\n        \"cluster_name\": CLUSTER_NAME,\n    },\n    \"paths\": [\"gs://cloud-samples-data/gcs/test.txt\"],\n    \"tags\": [{\"type\": \"FILE_LIST\", \"expression\": \"gcs.io/*\"}],\n    \"wait_for_completion\": True,\n}\n# [END howto_operator_gcs_create_tags_file]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"tag-template\"],\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n    )\n\n    @task\n    def create_cluster():\n        hook = GCSHook()\n        body = hook.fetch_metadata(GCS_BUCKET_URL, JOB_CREATE_TAGS_JSON[\"reference\"][\"project_id\"], LOCATION)\n        DATA_PROPOSAL_ZONES = body[\"properties\"][\"dataProvisioningProposalZones\"]\n\n        hook.create_metadata(GCS_BUCKET_URL, JOB_CREATE_TAGS_FILE[\"reference\"][\"project_id\"], LOCATION)\n\n        # [START howto_operator_dataproc_create_cluster]\n        DATAPROC_CREATE_CLUSTER = DataprocCreateClusterOperator(\n            task_id=\"create-cluster\",\n            project_id=PROJECT_ID,\n            cluster_name=CLUSTER_NAME,\n            region=LOCATION,\n            master_machine_type=\"n1-standard-4\",\n            worker_machine_type=\"n1-standard-4\",\n            num_workers=2,\n            gce_zone=\"europe-west4-a\",\n            is_preemptible=True,\n            bodies={\n                \"config\": CLUSTER_CONFIG,\n            },\n        )\n        # [END howto_operator_dataproc_create_cluster]\n\n    @task\n    def await_sweep(sweep_id):\n        print(f\"Monitoring sweep id {sweep_id} until it completes or reaches its timeout of {TICK_TIMEOUT}\")\n        target = None\n        for _ in range(int(TIMEOUT / TICKS)):\n            time.sleep(1)\n            target = getattr(swoop_client.sweeps, SweepSwoopAttrs.target).get(target_id=sweep_id)\n            if target.status == target.SUCCESS or target.status == target.FAILED:\n                break\n        status = \"SUCCESS\" if target.status == target.SUCCESS else \"TIMED OUT\"\n        print(f\"Marking sweep {sweep_id} as completed and marking cluster {CLUSTER_NAME} as cleaned up because sweep \"\n              f\"{sweep_id} has state {status}\")\n\n        # [START howto_operator_swoop_mark_as_completed]\n        SWEEP_MARK_COMPLETED = SwoopMarkAsCompletedOperator(\n            task_id=\"mark-sweep-as-completed\",\n            sweep_id=sweep_id,\n        )\n        # [END howto_operator_swoop_mark_as_completed]\n        swoop_client.sweeps.sweep_runs().delete(run_id=SweepRunSwoopAttrs.run_id, project_id=PROJECT_ID)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def swoop_delete_clustwer(swoop_client, cluster_name):\n        sswop_client.projects().locations().clusters().delete(\n            project_id=PROJECT_ID, location=location, cluster_name=cluster_name, operation_id=None\n        )\n\n    sweep_1 = SwoopWrapAroundTask(\n        task_id=\"sweep_1\",\n        swoop_id=sweep_id,\n        sweep_attr_swoop=SweepSwoopAttrs(),\n        sweep_attr_cluster=CusterSwoopAttrs(),\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=CLUSTER_NAME,\n        project_id=PROJECT_ID,\n        region=LOCATION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_cluster()\n        # TEST BODY\n        >> sweep_1\n        # TEST TEARDOWN\n        >> await_sweep(sweep_id)\n        >> delete_cluster\n        >> delete_bucket\n    )\n\n    # ### HELPER ###\n    # This indicates the beginning of a new helper file. Ensure everything from the previous HELPERS section is here.\n\n\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"teardown\" task with trigger rule is part of the DAG\nlist(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1333_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329321}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data transformation workflow that triggers dbt Cloud jobs for data processing and monitoring their execution status. Design a DAG that demonstrates different methods of running and tracking dbt Cloud job runs with synchronous and asynchronous sensors.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.dbt.cloud.sensors.dbt import (\n    DbtCloudJobRunSensor,\n    DbtCloudModelRunSensor,\n    DbtCloudSnapshotRunSensor,\n)\nfrom airflow.providers.dbt.cloud.utils.dbt import get_dbt_cloud_run_info\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_dbt_cloud_sensor\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_id = \"{{ task_instance.xcom_pull('get_run_id')['results'][0]['run_id'] }}\"\n\n    # [START howto_dbt_cloud_job_run_sensor]\n    check_dbt_cloud_job_run_status = DbtCloudJobRunSensor(\n        task_id=\"check_dbt_cloud_job_run_status\",\n        project_name=\"{{ task_instance.xcom_pull('get_project_name')['results'][0]['project_name'] }}\",\n        job_name=run_id,\n        timeout=300,\n        poke_interval=10,\n    )\n    # [END howto_dbt_cloud_job_run_sensor]\n\n    # [START howto_dbt_cloud_model_run_sensor]\n    check_dbt_cloud_model_run_status = DbtCloudModelRunSensor(\n        task_id=\"check_dbt_cloud_model_run_status\",\n        project_name=\"{{ task_instance.xcom_pull('get_project_name')['results'][0]['project_name'] }}\",\n        model_name=run_id,\n        timeout=300,\n        poke_interval=10,\n    )\n    # [END howto_dbt_cloud_model_run_sensor]\n\n    # [START howto_dbt_cloud_snapshot_run_sensor]\n    check_dbt_cloud_snapshot_run_status = DbtCloudSnapshotRunSensor(\n        task_id=\"check_dbt_cloud_snapshot_run_status\",\n        snapshot_name=\"{{ task_instance.xcom_pull('get_snapshot_name')['results'][0]['snapshot_name'] }}\",\n        snapshot_project_name=\"{{ task_instance.xcom_pull('get_project_name')['results'][0]['project_name']}}\",\n        timeout=300,\n        poke_interval=10,\n    )\n    # [END howto_dbt_cloud_snapshot_run_sensor]\n\n    (\n        # TEST SETUP\n        get_dbt_cloud_run_info(task_id=\"get_dbt_cloud_run_info\")\n        >> run_id\n        # TEST BODY\n        >> check_dbt_cloud_job_run_status\n        >> check_dbt_cloud_model_run_status\n        >> check_dbt_cloud_snapshot_run_status\n    )\n\n    # Add manual trigger rule\n    from tests.system.utils import get_test_run\n\n    # Get the test run object\n    test_run = get_test_run(dag)\n\n    test_run.trigger_rule = TriggerRule.ALL_DONE\n\n\nfrom tests.system.utils import get_test_env_id\n\n# Needed to run the example DAG with pytest (see below)\ntest_env_id = get_test_env_id()"}], "metadata": {"file_name": "dag_792_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3293278}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated Google Cloud Compute Engine template management workflow that demonstrates how to create, copy, and update instance templates for a scalable infrastructure deployment. The workflow should showcase best practices for managing compute resources programmatically using Airflow.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Compute instance operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineCopyInstanceOperator,\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineResizeInstanceOperator,\n    ComputeEngineStartInstanceOperator,\n    ComputeEngineStopInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nDAG_ID = \"cloud_compute_instance_template\"\n\n# [START howto_operator_gce_instance]\nINSTANCE_NAME = \"instance-{}\".format(UUID := \"{{ task_instance.xcom_pull('generate_uuid') }}\")\nTEMPLATE_NAME = \"{{ task_instance.xcom_pull('generate_template')['name'] }}\"\nIP_ADDRESS_RANGE_NAME = \"{{ task_instance.xcom_pull('generate_ip_range')['name'] }}\"\n\n\ndef _create_command_resize(instance_name, disk_size):\n    command = []\n    command.append(f\"resize disk={instance_name} size={disk_size}\")\n    command.append(f\"set-label disk={instance_name} device-name={instance_name}\")\n    return command\n\n\n# [START howto_operator_gce_insert_instance]\nINSERT_INSTANCE_COMMAND = [\n    \"if\",\n    \"isset($projectId)\",\n    f\"set({'projectId='.lower()}=('{projectId}'))\",\n    \"endif\",\n    f\"isset({'networkInterface'.lower()})\",\n    f\"set({{'networkInterface='.lower()}=('{'='}.join({'name': 'name', 'index': 'networkInterface'})\".replace('_', '-')).lower()}=('{networkInterface}' '.split('.')[3]))\",\n    \"_args['machineType']\",\n    f\"_args['disks'][0]\",\n    f\"_args['networkInterfaces'][0]['accessConfigs']['HTTP'}}'\",\n    f\"if isset({'metadataStorage{}'.lower()})\",\n    f\"set({{'metadataStorage{}'.lower()}=('={'metadataStorage'}).replace('_', '-')})\",\n    \"endif\",\n    _create_command_resize(INSTANCE_NAME, disk_size=\"pg_wal=10GB,maxDisk=50GB\"),\n]\n\n# [END howto_operator_gce_insert_instance]\n\n\n# [START howto_operator_gce_update_instance_from_template]\nUPDATE_INSTANCE_COMMANDS = [\n    {\"action\": \"reboot\", \"resource_id\": TEMPLATE_NAME, \"project_id\": PROJECT_ID},\n]\n# [END howto_operator_gce_update_instance_from_template]\n\n# [START howto_operator_gce_copy_instance]\nCOPY_INSTANCE_COMMAND = [\n    \"if\",\n    \"isset($projectId)\",\n    f\"set({'projectId='.lower()}=('{projectId}'))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['sourceInstance'])\",\n    f\"set({{'sourceInstance='.lower()}=('{'='}.join({'name': 'name'}, value['sourceInstance'].split('.')[:3])).lower()}=('{value['sourceInstance']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['diskSize']))\",\n    f\"set({{'diskSize='.lower()}=('{'='}.join({'name': 'diskSize'}, value['diskSize']).split('.'))'.lower()}=('{value['diskSize']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['networkInterfaceIndex']))\",\n    f\"set({{'networkInterfaceIndex='.lower()}=('{'='}.join({'name': 'networkInterface'}, value['networkInterfaceIndex']).split('.'))'.lower()}=('{value['networkInterfaceIndex']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['accessConfigHttpRedirectDisabled']))\",\n    f\"set({{'accessConfigHttpRedirectDisabled='.lower()}=('{'='}.join({'name': 'accessConfig.httpRedirect.disabled'}, value['accessConfigHttpRedirectDisabled']).split('.'))'.lower()}=('{value['accessConfigHttpRedirectDisabled']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['metadataStorageIndex']))\",\n    f\"set({{'metadataStorageIndex='.lower()}=('{'='}.join({'name': 'metadataStorage.index'}, value['metadataStorageIndex']).split('.'))'.lower()}=('{value['metadataStorageIndex']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['access_configs']['HTTP'}}'))\",\n    f\"set({{'network_interfaces'[0]['access_configs']]['HTTP'}}.'.lower()=('{'='}.join({'name': 'network_interfaces'[0]['access_configs']['HTTP'}'}, value['network_interfaces'][0]['access_configs']['HTTP'].split('.')))'.lower()}=('{value['network_interfaces'][0]['access_configs']['HTTP']')})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['auto_delete']))\",\n    f\"set({{'disks'[0]]['auto_delete']=('{'='}.join({'name': 'disks'[0]]['auto_delete']), value['disks'][0]['auto_delete']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['device_name']))\",\n    f\"set({{'disks'[0]]['device_name']=('{'='}.join({'name': 'disks'[0]]['device_name']], value['disks'][0]['device_name']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['initialize_params'][0]['disk_size_tier']))\",\n    f\"set({{'disks'[0]]['initialize_params'][0]['disk_size_tier']=('{'='}.join({'name': 'disks'[0]]['initialize_params'][0]['disk_size_tier'], value['disks'][0]['initialize_params'][0]['disk_size_tier']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_image']))\",\n    f\"set({{'disks'[0]]['source_image']=('{'='}.join({'name': 'disks'[0]]['source_image']], value['disks'][0]['source_image']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_element_group_member']))\",\n    f\"set({{'disks'[0]]['source_element_group_member']=('{'='}.join({'name': 'disks'[0]]['source_element_group_member']], value['disks'][0]['source_element_group_member']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_snapshot']))\",\n    f\"set({{'disks'[0]]['source_snapshot']=('{'='}.join({'name': 'disks'[0]]['source_snapshot']], value['disks'][0]['source_snapshot']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['auto_delete']))\",\n    f\"set({{'disks'[0]]['auto_delete']=('{'='}.join({'name': 'disks'[0]]['auto_delete']], value['disks'][0]['auto_delete']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['device_name']))\",\n    f\"set({{'disks'[0]]['device_name']=('{'='}.join({'name': 'disks'[0]]['device_name']], value['disks'][0]['device_name']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['initialize_params'][0]['disk_size_tier']))\",\n    f\"set({{'disks'[0]]['initialize_params'][0]['disk_size_tier']=('{'='}.join({'name': 'disks'[0]]['initialize_params'][0]['disk_size_tier']], value['disks'][0]['initialize_params'][0]['disk_size_tier']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_image']))\",\n    f\"set({{'disks'[0]]['source_image']=('{'='}.join({'name': 'disks'[0]]['source_image']], value['disks'][0]['source_image']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_element_group_member']))\",\n    f\"set({{'disks'[0]]['source_element_group_member']=('{'='}.join({'name': 'disks'[0]]['source_element_group_member']], value['disks'][0]['source_element_group_member']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_snapshot']))\",\n    f\"set({{'disks'[0]]['source_snapshot']=('{'='}.join({'name': 'disks'[0]]['source_snapshot']], value['disks'][0]['source_snapshot']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['metadata_storage_index']))\",\n    f\"set({{'metadata_storage_index'=('{'='}.join({'name': 'metadata_storage_index'}, value['metadata_storage_index']).split('.'))'.lower()}=('{value['metadata_storage_index']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['access_config_http_redirect_disabled']))\",\n    f\"set({{'access_config_http_redirect_disabled'=('{'='}.join({'name': 'access_config_http_redirect_disabled'}, value['access_config_http_redirect_disabled']).split('.'))'.lower()}=('{value['access_config_http_redirect_disabled']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['access_configs']['HTTP'}}'))\",\n    f\"set({{'network_interfaces'[0]['access_configs']]['HTTP'}}'.lower()=('{'='}.join({'name': 'network_interfaces'[0]['access_configs']['HTTP'}'}, value['network_interfaces'][0]['access_configs']['HTTP'].split('.')))'.lower())\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['subnetwork']))\",\n    f\"set({{'network_interfaces'[0]['subnetwork']=('{'='}.join({'name': 'network_interfaces'[0]['subnetwork']], value['network_interfaces'][0]['subnetwork']).split('.'))'.lower()}=('{value['network_interfaces'][0]['subnetwork']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['ipv4_access_configs'][0]['allowed'][0]['ip_ranges'][0]))\",\n    f\"set({{'ipv4_access_configs'[0]['allowed'][0]['ip_ranges'][0']=('{'='}.join({'name': 'ipv4_access_configs'[0]['allowed'][0]['ip_ranges'][0]], value['network_interfaces'][0]['ipv4_access_configs'][0]['allowed'][0]['ip_ranges'][0]).split('.'))'.lower()}=('{value['network_interfaces'][0]['ipv4_access_configs'][0]['allowed'][0]['ip_ranges'][0]}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['auto_delete']))\",\n    f\"set({{'disks'[0]]['auto_delete']=('{'='}.join({'name': 'disks'[0]]['auto_delete']], value['disks'][0]['auto_delete']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['device_name']))\",\n    f\"set({{'disks'[0]]['device_name']=('{'='}.join({'name': 'disks'[0]]['device_name']], value['disks'][0]['device_name']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['initialize_params'][0]['disk_size_tier']))\",\n    f\"set({{'disks'[0]]['initialize_params'][0]['disk_size_tier']=('{'='}.join({'name': 'disks'[0]]['initialize_params'][0]['disk_size_tier']], value['disks'][0]['initialize_params'][0]['disk_size_tier']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_image']))\",\n    f\"set({{'disks'[0]]['source_image']=('{'='}.join({'name': 'disks'[0]]['source_image']], value['disks'][0]['source_image']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_element_group_member']))\",\n    f\"set({{'disks'[0]]['source_element_group_member']=('{'='}.join({'name': 'disks'[0]]['source_element_group_member']], value['disks'][0]['source_element_group_member']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_snapshot']))\",\n    f\"set({{'disks'[0]]['source_snapshot']=('{'='}.join({'name': 'disks'[0]]['source_snapshot']], value['disks'][0]['source_snapshot']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['auto_delete']))\",\n    f\"set({{'disks'[0]]['auto_delete']=('{'='}.join({'name': 'disks'[0]]['auto_delete']], value['disks'][0]['auto_delete']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['device_name']))\",\n    f\"set({{'disks'[0]]['device_name']=('{'='}.join({'name': 'disks'[0]]['device_name']], value['disks'][0]['device_name']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['initialize_params'][0]['disk_size_tier']))\",\n    f\"set({{'disks'[0]]['initialize_params'][0]['disk_size_tier']=('{'='}.join({'name': 'disks'[0]]['initialize_params'][0]['disk_size_tier']], value['disks'][0]['initialize_params'][0]['disk_size_tier']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['metadata_storage_index']))\",\n    f\"set({{'metadata_storage_index'=('{'='}.join({'name': 'metadata_storage_index'}, value['metadata_storage_index']).split('.'))'.lower()}=('{value['metadata_storage_index']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['access_config_http_redirect_disabled']))\",\n    f\"set({{'access_config_http_redirect_disabled'=('{'='}.join({'name': 'access_config_http_redirect_disabled'}, value['access_config_http_redirect_disabled']).split('.'))'.lower()}=('{value['access_config_http_redirect_disabled']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['access_configs']['HTTP'}}'))\",\n    f\"set({{'network_interfaces'[0]['access_configs']]['HTTP'}}'.lower()=('{'='}.join({'name': 'network_interfaces'[0]['access_configs']['HTTP'}'}, value['network_interfaces'][0]['access_configs']['HTTP'].split('.')))'.lower())\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['subnetwork']))\",\n    f\"set({{'network_interfaces'[0]['subnetwork']=('{'='}.join({'name': 'network_interfaces'[0]['subnetwork']], value['network_interfaces'][0]['subnetwork']).split('.'))'.lower()}=('{value['network_interfaces'][0]['subnetwork']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['ipv4_access_configs'][0]['allowed'][0]['ip_ranges'][0]))\",\n    f\"set({{'ipv4_access_configs'[0]['allowed'][0]['ip_ranges'][0']=('{'='}.join({'name': 'ipv4_access_configs'[0]['allowed'][0]['ip_ranges'][0]}, value['network_interfaces'][0]['ipv4_access_configs'][0]['allowed'][0]['ip_ranges'][0]).split.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['auto_delete']))\",\n    f\"set({{'disks'[0]]['auto_delete']=('{'='}.join({'name': 'disks'[0]]['auto_delete']], value['disks'][0]['auto_delete']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['device_name']))\",\n    f\"set({{'disks'[0]]['device_name']=('{'='}.join({'name': 'disks'[0]]['device_name']], value['disks'][0]['device_name']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['initialize_params'][0]['disk_size_tier']))\",\n    f\"set({{'disks'[0]]['initialize_params'][0]['disk_size_tier']=('{'='}.join({'name': 'disks'[0]]['initialize_params'][0]['disk_size_tier']], value['disks'][0]['initialize_params'][0]['disk_size_tier']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_image']))\",\n    f\"set({{'disks'[0]]['source_image']=('{'='}.join({'name': 'disks'[0]]['source_image']], value['disks'][0]['source_image']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_element_group_member']))\",\n    f\"set({{'disks'[0]]['source_element_group_member']=('{'='}.join({'name': 'disks'[0]]['source_element_group_member']], value['disks'][0]['source_element_group_member']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['disks'][0]['source_snapshot']))\",\n    f\"set({{'disks'[0]]['source_snapshot']=('{'='}.join({'name': 'disks'[0]]['source_snapshot']], value['disks'][0]['source_snapshot']).split('.')}})\",\n    \"endif\",\n    \"if\",\n    \"isset(value['metadata_storage_index']))\",\n    f\"set({{'metadata_storage_index'=('{'='}.join({'name': 'metadata_storage_index'}, value['metadata_storage_index']).split('.'))'.lower()}=('{value['metadata_storage_index']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['access_config_http_redirect_disabled']))\",\n    f\"set({{'access_config_http_redirect_disabled'=('{'='}.join({'name': 'access_config_http_redirect_disabled'}, value['access_config_http_redirect_disabled']).split('.'))'.lower()}=('{value['access_config_http_redirect_disabled']}')))\",\n    \"endif\",\n    \"if\",\n    \"isset(value['network_interfaces'][0]['access_configs']['HTTP'}}'))\",\n    f\"set({{'network_interfaces'[0]['access_configs']]['HTTP'}}'.lower()=('{'='}.join({'name': 'network_interfaces'["}], "metadata": {"file_name": "dag_2008_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329347}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that illustrates the technical orchestration of data cube operations using the KylinCubeOperator, demonstrating how complex data processing tasks can be chained together with precise time-based segmentation and automated management. The DAG will highlight task dependencies, XCom data passing, and systematic execution of build, refresh, merge, disable, and purge commands.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.kylin import (\n    KylinAddTableMaterializeCubeOperator,\n    KylinAddTableMaterializeCycleReportOperator,\n    KylinBuildCubeOperator,\n    KylinDeleteCubeOperator,\n    KylinDisableCubeOperator,\n    KylinEnableCubeOperator,\n    KylinGetCubeStatusOperator,\n    KylinListCubesOperator,\n    KylinMergeCubesOperator,\n    KylinRefreshCubeOperator,\n    KylinUpdateTableMaterializeCubeOperator,\n    KylinUpdateTableMaterializeCycleReportOperator,\n)\nfrom airflow.providers.alibaba.cloud.operators.utils import CloudUtilsOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"example_kylin\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_kylin_build]\n    build_cube = KylinBuildCubeOperator(task_id=\"build_cube\", cube_id=\"cube_test\", graph=\"graph_test\")\n    # [END howto_operator_kylin_build]\n\n    # [START howto_operator_kylin_enable]\n    enable_cube = KylinEnableCubeOperator(\n        task_id=\"enable_cube\", cube_id=\"cube_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_kylin_enable]\n\n    # [START howto_operator_kylin_get_cube_status]\n    get_cube_status = KylinGetCubeStatusOperator(\n        task_id=\"get_cube_status\", cube_id=\"cube_test\", project_name=\"project_test\"\n    )\n    # [END howto_operator_kylin_get_cube_status]\n\n    # [START howto_operator_kylin_list_cubes]\n    list_cubes = KylinListCubesOperator(task_id=\"list_cubes\", project_name=\"project_test\")\n    # [END howto_operator_kylin_list_cubes]\n\n    # [START howto_operator_kylin_delete]\n    delete_cube = KylinDeleteCubeOperator(\n        task_id=\"delete_cube\", cube_id=\"cube_test\", project_name=\"project_test\", is_delete_data=True\n    )\n    # [END howto_operator_kylin_delete]\n\n    # [START howto_operator_kylin_disable]\n    disable_cube = KylinDisableCubeOperator(\n        task_id=\"disable_cube\", cube_id=\"cube_test\", project_name=\"project_test\", is_disable_data=True\n    )\n    # [END howto_operator_kylin_disable]\n\n    # [START howto_operator_kylin_update_table_materialize]\n    update_cube = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"update_cube\",\n        cube_id=\"cube_test\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n    # [END howto_operator_kylin_update_table_materialize]\n\n    # [START howto_operator_kylin_add_table_materialize]\n    add_cube = KylinAddTableMaterializeCubeOperator(\n        task_id=\"add_cube\",\n        cube_id=\"cube_test\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n    # [END howto_operator_kylin_add_table_materialize]\n\n    # [START howto_operator_kylin_build_cycle_report]\n    build_cycle_report = KylinBuildCubeOperator(\n        task_id=\"build_cycle_report\", cube_id=\"cycle_report_test\", graph=\"graph_report_test\"\n    )\n    # [END howto_operator_kylin_build_cycle_report]\n\n    # [START howto_operator_kylin_enable_cycle_report]\n    enable_cycle_report = KylinEnableCubeOperator(\n        task_id=\"enable_cycle_report\", cube_id=\"cycle_report_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_kylin_enable_cycle_report]\n\n    # [START howto_operator_kylin_get_cube_status_cycle_report]\n    get_cycle_report_status = KylinGetCubeStatusOperator(\n        task_id=\"get_cycle_report_status\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n    # [END howto_operator_kylin_get_cube_status_cycle_report]\n\n    # [START howto_operator_kylin_list_cubes_cycle_report]\n    list_cycle_report_cubes = KylinListCubesOperator(\n        task_id=\"list_cycle_report_cubes\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n    # [END howto_operator_kylin_list_cubes_cycle_report]\n\n    # [START howto_operator_kylin_merge_cubes]\n    merge_cycle_report = KylinMergeCubesOperator(\n        task_id=\"merge_cycle_report\",\n        project_name=\"project_test\",\n        input_cubes=[\"cycle_report_test\"],\n        output_cube=\"merged_cycle_report\",\n        output_materialize=\"cycle_report_materialize\",\n    )\n    # [END howto_operator_kylin_merge_cubes]\n\n    # [START howto_operator_kylin_refresh_cycle_report]\n    refresh_cycle_report = KylinRefreshCubeOperator(\n        task_id=\"refresh_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n    # [END howto_operator_kylin_refresh_cycle_report]\n\n    # [START howto_operator_kylin_disable_cycle_report]\n    disable_cycle_report = KylinDisableCubeOperator(\n        task_id=\"disable_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n    # [END howto_operator_kylin_disable_cycle_report]\n\n    # [START howto_operator_kylin_update_table_materialize_cycle_report]\n    update_cycle_report = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"update_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n    # [END howto_operator_kylin_update_table_materialize_cycle_report]\n\n    # [START howto_operator_kylin_add_table_materialize_cycle_report]\n    add_cycle_report = KylinAddTableMaterializeCycleReportOperator(\n        task_id=\"add_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n    # [END howto_operator_kylin_add_table_materialize_cycle_report]\n\n    delete_utils = CloudUtilsOperator(\n        task_id=\"delete_utils\",\n        command=\"delete_file_under_dir\":\n        {\n            'dirName': '/tmp/file',\n            'recursive': '1'\n        }\n    )\n\n    delete_build = KylinDeleteCubeOperator(\n        task_id=\"delete_build\", cube_id=\"build_test\", project_name=\"project_test\", is_delete_data=True\n    )\n\n    delete_enable = KylinDisableCubeOperator(\n        task_id=\"delete_enable\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_get_status = KylinGetCubeStatusOperator(\n        task_id=\"delete_get_status\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_list_cubes = KylinListCubesOperator(task_id=\"delete_list_cubes\", project_name=\"project_test\")\n\n    delete_build_cycle_report = KylinBuildCubeOperator(\n        task_id=\"delete_build_cycle_report\", cube_id=\"cycle_report_test\", graph=\"graph_report_test\"\n    )\n\n    delete_enable_cycle_report = KylinEnableCubeOperator(\n        task_id=\"delete_enable_cycle_report\", cube_id=\"cycle_report_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_get_status_cycle_report = KylinGetCubeStatusOperator(\n        task_id=\"delete_get_status_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_list_cubes_cycle_report = KylinListCubesOperator(\n        task_id=\"delete_list_cubes_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_merge_cubes = KylinMergeCubesOperator(\n        task_id=\"delete_merge_cubes\", project_name=\"project_test\", input_cubes=[\"cycle_report_test\"], output_cube=\"merged_cycle_report\"\n    )\n\n    delete_refresh_cycle_report = KylinRefreshCubeOperator(\n        task_id=\"delete_refresh_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n\n    delete_update_cycle_report = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"delete_update_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n\n    delete_update_table_materialize_cycle_report = KylinAddTableMaterializeCycleReportOperator(\n        task_id=\"delete_update_table_materialize_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n\n    delete_delete_utils = CloudUtilsOperator(\n        task_id=\"delete_delete_utils\",\n        command=\"delete_file_under_dir\":\n        {\n            'dirName': '/tmp/file',\n            'recursive': '1'\n        }\n    )\n\n    delete_delete_build = KylinDeleteCubeOperator(\n        task_id=\"delete_delete_build\", cube_id=\"build_test\", project_name=\"project_test\", is_delete_data=True\n    )\n\n    delete_delete_enable = KylinDisableCubeOperator(\n        task_id=\"delete_delete_enable\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_get_status = KylinGetCubeStatusOperator(\n        task_id=\"delete_delete_get_status\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_list_cubes = KylinListCubesOperator(task_id=\"delete_delete_list_cubes\", project_name=\"project_test\")\n\n    delete_delete_build_cycle_report = KylinBuildCubeOperator(\n        task_id=\"delete_delete_build_cycle_report\", cube_id=\"cycle_report_test\", graph=\"graph_report_test\"\n    )\n\n    delete_delete_enable_cycle_report = KylinEnableCubeOperator(\n        task_id=\"delete_delete_enable_cycle_report\", cube_id=\"cycle_report_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_delete_get_status_cycle_report = KylinGetCubeStatusOperator(\n        task_id=\"delete_delete_get_status_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_merge_cubes = KylinMergeCubesOperator(\n        task_id=\"delete_delete_merge_cubes\", project_name=\"project_test\", input_cubes=[\"cycle_report_test\"], output_cube=\"merged_cycle_report\"\n    )\n\n    delete_delete_refresh_cycle_report = KylinRefreshCubeOperator(\n        task_id=\"delete_delete_refresh_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n\n    delete_delete_update_cycle_report = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"delete_delete_update_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n\n    delete_delete_update_table_materialize_cycle_report = KylinAddTableMaterializeCycleReportOperator(\n        task_id=\"delete_delete_update_table_materialize_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n\n    delete_delete_delete_utils = CloudUtilsOperator(\n        task_id=\"delete_delete_delete_utils\",\n        command=\"delete_file_under_dir\":\n        {\n            'dirName': '/tmp/file',\n            'recursive': '1'\n        }\n    )\n\n    delete_update_cube = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"delete_update_cube\",\n        cube_id=\"cube_test\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n\n    delete_add_cube = KylinAddTableMaterializeCubeOperator(\n        task_id=\"delete_add_cube\",\n        cube_id=\"cube_test\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n\n    delete_build_cycle_report = KylinBuildCubeOperator(\n        task_id=\"delete_build_cycle_report\", cube_id=\"cycle_report_test\", graph=\"graph_report_test\"\n    )\n\n    delete_enable_cycle_report = KylinEnableCubeOperator(\n        task_id=\"delete_enable_cycle_report\", cube_id=\"cycle_report_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_get_status_cycle_report = KylinGetCubeStatusOperator(\n        task_id=\"delete_get_status_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_list_cubes_cycle_report = KylinListCubesOperator(\n        task_id=\"delete_list_cubes_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_merge_cubes = KylinMergeCubesOperator(\n        task_id=\"delete_merge_cubes\", project_name=\"project_test\", input_cubes=[\"cycle_report_test\"], output_cube=\"merged_cycle_report\"\n    )\n\n    delete_refresh_cycle_report = KylinRefreshCubeOperator(\n        task_id=\"delete_refresh_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n\n    delete_update_cycle_report = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"delete_update_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n\n    delete_update_table_materialize_cycle_report = KylinAddTableMaterializeCycleReportOperator(\n        task_id=\"delete_update_table_materialize_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n\n    delete_delete_build = KylinDeleteCubeOperator(\n        task_id=\"delete_delete_build\", cube_id=\"build_test\", project_name=\"project_test\", is_delete_data=True\n    )\n\n    delete_delete_enable = KylinDisableCubeOperator(\n        task_id=\"delete_delete_enable\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_get_status = KylinGetCubeStatusOperator(\n        task_id=\"delete_delete_get_status\", cube_id=\"build_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_list_cubes = KylinListCubesOperator(task_id=\"delete_delete_list_cubes\", project_name=\"project_test\")\n\n    delete_delete_build_cycle_report = KylinBuildCubeOperator(\n        task_id=\"delete_delete_build_cycle_report\", cube_id=\"cycle_report_test\", graph=\"graph_report_test\"\n    )\n\n    delete_delete_enable_cycle_report = KylinEnableCubeOperator(\n        task_id=\"delete_delete_enable_cycle_report\", cube_id=\"cycle_report_test\", trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_delete_get_status_cycle_report = KylinGetCubeStatusOperator(\n        task_id=\"delete_delete_get_status_cycle_report\", cube_id=\"cycle_report_test\", project_name=\"project_test\"\n    )\n\n    delete_delete_merge_cubes = KylinMergeCubesOperator(\n        task_id=\"delete_delete_merge_cubes\", project_name=\"project_test\", input_cubes=[\"cycle_report_test\"], output_cube=\"merged_cycle_report\"\n    )\n\n    delete_delete_refresh_cycle_report = KylinRefreshCubeOperator(\n        task_id=\"delete_delete_refresh_cycle_report\", cube_id=\"cycle_report_materialize\", project_name=\"project_test\"\n    )\n\n    delete_delete_update_cycle_report = KylinUpdateTableMaterializeCubeOperator(\n        task_id=\"delete_delete_update_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        start_time=\"2016-01-01 10:00:00\",\n        end_time=\"2016-01-01 11:00:00\",\n    )\n\n    delete_delete_update_table_materialize_cycle_report = KylinAddTableMaterializeCycleReportOperator(\n        task_id=\"delete_delete_update_table_materialize_cycle_report\",\n        cube_id=\"cycle_report_materialize\",\n        project_name=\"project_test\",\n        table_materialize_dict={\"table\": \"test_table\", \"type\": \"SPICE\"},\n    )\n\n    delete_delete_delete_utils = CloudUtilsOperator(\n        task_id=\"delete_delete_delete_utils\",\n        command=\"delete_file_under_dir\":\n        {\n            'dirName': '/tmp/file',\n            'recursive': '1'\n        }\n    )\n\n    add_cube |= build_cube\n    add_cube <<= enable_cube\n    add_cube <<= get_cube_status\n    add_cube <<= list_cubes\n    add_cube <<= delete_utils\n    add_cube <<= delete_build\n    add_cube <<= delete_enable\n    add_cube <<= delete_get_status\n    add_cube <<= delete_list_cubes\n    add_cube <<= delete_build_cycle_report\n    add_cube <<= delete_enable_cycle_report\n    add_cube <<= delete_get_status_cycle_report\n    add_cube <<= delete_list_cubes_cycle_report\n    add_cube <<= delete_merge_cubes\n    add_cube <<= delete_refresh_cycle_report\n    add_cube <<= update_cube\n    add_cube <<= add_cycle_report\n    add_cube <<= build_cycle_report\n    add_cube <<= enable_cycle_report\n    add_cube <<= get_cycle_report_status\n    add_cube <<= list_cycle_report_cubes\n    add_cube <<= delete_utils\n    add_cube <<= delete_build_cycle_report\n    add_cube <<= delete_enable_cycle_report\n    add_cube <<= delete_get_status_cycle_report\n    add_cube <<= delete_list_cubes_cycle_report\n    add_cube <<= delete_merge_cubes\n    add_cube <<= delete_refresh_cycle_report\n    add_cube <<= update_cycle_report\n    add_cube <<= add_cycle_report\n    add_cube <<= build_cycle_report\n    add_cube <<= enable_cycle_report\n    add_cube <<= get_cycle_report_status\n    add_cube <<= list_cycle_report_cubes\n    add_cube <<= delete_utils\n    add_cube <<= delete_build_cycle_report\n    add_cube <<= delete_enable_cycle_report\n    add_cube <<= delete_get_status_cycle_report\n    add_cube <<= delete_list_cubes_cycle_report\n    add_cube <<= delete_merge_cubes\n    add_cube <<= delete_refresh_cycle_report\n    add_cube <<= update_cycle_report\n    add_cube <<= add_cycle_report\n    add_cube <<= build_cycle_report\n    add_cube <<= enable"}], "metadata": {"file_name": "dag_1143_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329378}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you suggest a way to modify the given Python code to ensure that a list is only modified if the new value is not already present in the list?"}, {"role": "assistant", "content": "def add_to_list_if_not_exists(lst, value):\n    # Check if the value is not already in the list\n    if value not in lst:\n        # If not, append the value to the list\n        lst.append(value)\n    return lst\n\n# Example usage:\nmy_list = [1, 2, 3]\nnew_value = 4\n\nmodified_list = add_to_list_if_not_exists(my_list, new_value)\n\nprint(modified_list)  # Output: [1, 2, 3, 4]"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3293831}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that ensures only the most recent workflow run is processed, preventing redundant task executions across multiple scheduling intervals. Create a workflow that demonstrates how to skip unnecessary task runs when a newer workflow instance is already in progress.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"Example of the latest state and latest_in_progress_state.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.state import LatestState, LatestStateIfNone, InProgressState\n\nwith DAG(\n    dag_id=\"latest_and_latest_in_progress\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n) as dag:\n    latest_task = EmptyOperator(task_id=\"latest\")\n    latest_task2 = EmptyOperator(task_id=\"latest\")\n    latest_state_none = EmptyOperator(task_id=\"latest_non_exec\")\n    latest_state = EmptyOperator(task_id=\"latest\")\n    latest_in_progress = EmptyOperator(task_id=\"latest\")\n    latest_in_progress_state = EmptyOperator(task_id=\"latest\")\n    latest_in_progress_state_none = EmptyOperator(task_id=\"latest\")\n    latest_in_progress_mix = EmptyOperator(task_id=\"latest\")\n    latest_in_progress_mix_state = EmptyOperator(task_id=\"latest\")\n\n    [latest_task, latest_state_none] >> latest_task2\n    latest_state >> latest_in_progress\n    latest_in_progress >> latest_in_progress_mix\n    (latest_in_progress_state >> latest_in_progress_mix_state) << LatestStateIfNone()\n    (latest_state_none >> latest_in_progress_state_none) << InProgressState()"}], "metadata": {"file_name": "dag_335_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3293877}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transformation pipeline that demonstrates dynamic task mapping by taking a list of numbers, incrementing each number, and then calculating their total sum. Create a workflow that shows how Airflow can dynamically generate tasks based on input data.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the @taskgroup decorator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.sdk import DAG, Task, TaskGroup\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\ndef get_tasks_for_task_group(task_group):\n    \"\"\"Retrieves all tasks for a given task group.\"\"\"\n    tasks = []\n    for task in DAG.tasks:\n        if isinstance(task, task_group):\n            tasks.append(task)\n    return tasks\n\n\ndef check_tasks(tasks, task_group_name):\n    for task in tasks:\n        if str(task) != f\"{task_group_name}_tasks\":\n            raise Exception(f\"Task {task} belongs to {task_group_name}_tasks group\")\n\n\nwith DAG(\n    dag_id=\"example_taskgroup_decorator\",\n    start_date=pendulum.datetime(2020, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_task_group]\n    task_start = Task(\"start\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n\n    # [END howto_operator_task_group]\n\n    # [START howto_task_group]\n    task_1 = Task(\"one\", task_group=\"parent_task_group\")\n    # [END howto_task_group]\n\n    # [START howto_task_group_decorator]\n    @task_group(\"child_task_group\")\n    def task_group_decorator():\n        # [END howto_task_group_decorator]\n        task_2 = Task(\"two\")\n        task_3 = Task(\"three\", task_id=\"three\")\n        task_4 = Task(\"four\", task_id=\"four\")\n        task_3 >> task_4\n        task_2\n    # [END howto_task_group_decorator]\n\n    task_end = Task(\"end\", trigger_rule=TriggerRule.ALL_DONE)\n\n    task_start >> task_end\n\n    check_tasks(get_tasks_for_task_group(task_start), \"parent_task_group\")\n    check_tasks(get_tasks_for_task_group(task_group_decorator()), \"child_task_group\")\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1849_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3293939}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I'm looking for a Python function that can send an email with an attachment. Can you provide a sample code?"}, {"role": "assistant", "content": "import smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email import encoders\n\ndef send_email_with_attachment(to_email, subject, body, attachment_path):\n    # Set up the server\n    smtp_server = 'smtp.gmail.com'\n    port = 587  # For starttls\n    username = 'your-email@gmail.com'  # Replace with your Gmail address\n    password = 'your-password'  # Replace with your Gmail password\n\n    # Create the email message\n    msg = MIMEMultipart()\n    msg['From'] = username\n    msg['To'] = to_email\n    msg['Subject'] = subject\n\n    # Attach the HTML body\n    msg.attach(MIMEText(body, 'html'))\n\n    # Open the file in binary mode\n    with open(attachment_path, \"rb\") as attachment:\n        # Define the payload part\n        part = MIMEBase('application', 'octet-stream')\n        part.set_payload(attachment.read())\n\n    # Encode into base64\n        encoders.encode_base64(part)\n        part.add_header('Content-Disposition',\n                        f\"attachment; filename= {attachment_path}\")\n\n        # Add the attachment to the message\n        msg.attach(part)\n\n    # Log in to the server\n    try:\n        server = smtplib.SMTP(smtp_server, port)\n        server.starttls()  # Secure the connection\n        server.login(username, password)\n        text = msg.as_string()\n        server.sendmail(username, to_email, text)\n        print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")\n    finally:\n        server.quit()\n\n# Example usage\nsend_email_with_attachment(\n    to_email='recipient@example.com',\n    subject='Test Email with Attachment',\n    body='<h1>Hello Recipient!</h1><p>This is a test email with an attachment.</p>',\n    attachment_path='/path/to/your/file.pdf'\n)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329404}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data processing workflow with multiple interconnected tasks, including cloud resource management, data import, transformation, and export. Implement a systematic approach to handling data pipeline tasks with robust error handling and connection management.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToTrinoOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.trino.transfers.gcs_to_trino import GCSToTrinoOperator\n\n# [START howto_gcs]\n# TODO: Update this example\nGCS_BUCKET = \"test-data-google-cloud\"\nGCS_OBJECT = \"trino/test.sql\"\n# [END howto_gcs]\n\n# [START howto_conf_trino]\nTRINO_TABLE = \"test_table\"\n# [END howto_conf_trino]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_trino\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_gcs_to_trino_operator]\n    run_this = GCSToTrinoOperator(\n        task_id=\"gcs_to_trino_task\",\n        gcs_bucket=GCS_BUCKET,\n        gcs_object=GCS_OBJECT,\n        trino_table=TRINO_TABLE,\n    )\n    # [END howto_gcs_to_trino_operator]\n\n    run_this\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1225_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3294075}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a MySQL database cleanup workflow that demonstrates dropping tables using Airflow's SQL operators. Create a DAG that shows how to remove database tables programmatically and sequentially.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.edgemodifier import Label\n\nwith DAG(\n    dag_id=\"example_sql_execute_query_operator\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_sql_execute_query]\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        sql=\"DROP TABLE table_name;\",\n        conn_id=\"my_hive_conn\",\n    )\n    # [END howto_operator_sql_execute_query]\n\n    create_table = SQLExecuteQueryOperator(\n        task_id=\"create_table\",\n        sql=\"CREATE TABLE table_name (t1 INT, t2 VARCHAR(8))\",\n        conn_id=\"my_hive_conn\",\n    )\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        sql=\"INSERT INTO table_name VALUES (1, 'SQL'), (2, 'Airflow')\",\n        conn_id=\"my_hive_conn\",\n    )\n\n    select_data = SQLExecuteQueryOperator(\n        task_id=\"select_data\",\n        sql=\"SELECT * FROM table_name;\",\n        conn_id=\"my_hive_conn\",\n    )\n\n    drop_table >> create_table >> insert_data >> select_data\n\n\nfrom tests_common.test_utils.watcher import watcher\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = [\n    watcher(dag)\n]\n\n\ndef test_operator_sql_execute_query():\n    from tests_common.test_utils.system_tests import get_test_run\n\n    test_run[0].add(test_run[4])  # test_run[4] is the run of the example DAG\n    context = get_test_run(test_run)[0]\n    list(context['aws_edges'])  # Remove any provided nodes from the list of edges\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see above)\ntest_run = get_test_run(test_run)"}], "metadata": {"file_name": "dag_1368_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329413}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that explores different task execution strategies by implementing parallel pipeline branches with configurable success conditions. Construct a DAG that demonstrates how tasks can be conditionally executed or skipped based on predefined trigger rules.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the `@task.branch` decorator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\n\n\n# [START howto_task_branch]\n# [START howto_task_branch_in_astro]\n@dag(\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"example-astro\"],\n)\ndef example_branch_decorator():\n    @task.branch()\n    def pick\u54ea\u79cd\u8def\u7ebf():\n        return [\"route A\", \"route B\"]\n\n    route1 = \"route A\"\n    route2 = \"route B\"\n\n    @task(trigger_rule=\"ALL_DONE\")\n    def wait_on_route(route: str):\n        print(f\"waiting {len(route)} seconds before exiting\")\n\n        if route == route1:\n            exit_code = 0\n        else:\n            exit_code = -1\n\n        import time\n\n        time.sleep(exit_code)\n\n        print(\"done.\")\n        return None\n\n    chain(pick\u54ea\u79cd\u8def\u7ebf(), wait_on_route(pick\u54ea\u79cd\u8def\u7ebf().output))\n\n    # \u6307\u5b9a\u6709\u8fd4\u56de\u503c\u7684\u4efb\u52a1\uff0c\u4f7f\u7528return_columns\u53c2\u6570\u660e\u786e\u6307\u5b9a\u8fd4\u56de\u7684\u5217\u540d\n    @task.branch(return_columns=True)\n    def pick_with_return_values():\n        print(\" Returning columns test \")\n        return [\"route A\", \"route B\"]\n\n    route1_with_columns = \"route A\"\n    route2_with_columns = \"route B\"\n\n    @task.trigger_rule(\"all_done\")\n    def wait_on_route_with_return_values(route: str):\n        print(f\"waiting {len(route)} seconds before exiting\")\n\n        if route == route1_with_columns:\n            exit_code = 0\n        else:\n            exit_code = -1\n\n        import time\n\n        time.sleep(exit_code)\n        print(\"done.\")\n        return None\n\n    chain(pick_with_return_values(), wait_on_route_with_return_values(pick_with_return_values().output))\n# [END howto_task_branch_in_astro]\n# [END howto_task_branch]\n\n\ndefault_args = {\"catchup\": False}"}], "metadata": {"file_name": "dag_689_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3294187}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Azure Data Lake Storage (ADLS) file management workflow that demonstrates uploading a local file and then immediately deleting it. Create a DAG that showcases automated file transfer and cleanup processes in a cloud storage environment.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSCreateObjectOperator, ADLSDeleteOperator\n\ntry:\n    from airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\nexcept ModuleNotFoundError:\n    # This import throws a ModuleNotFoundError if the user doesn't install the provider first\n    # pylint: disable=unused-import\n    from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator  # type: ignore[no-redef]\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adls\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    # [START adls_create]\n    @task\n    def create_adls():\n        return \"azureadlsoutput/</tmp>/temp.txt\"\n\n    output_path = create_adls()\n    # [END adls_create]\n\n    # [START adls_create_from_file]\n    create_adl_sas_with_file = ADLSCreateObjectOperator(task_id=\"create_adl_sas_with_file\", path=output_path)\n    # [END adls_create_from_file]\n\n    upload_data = LocalFilesystemToADLSOperator(task_id=\"upload_data\", filename=create_adl_sas_with_file.output)\n\n    delete_adls = ADLSDeleteOperator(task_id=\"delete_adls\", path=output_path, allow_trash_true)\n\n    (\n        upload_data\n        >> delete_adls\n        >> create_adls()\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1353_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3294237}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the complete lifecycle of an AutoML image training job, from dataset initialization through model training to resource deletion. Showcase how to programmatically manage machine learning infrastructure using cloud services and workflow orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud AutoML service testing.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.automl import (\n    CloudAutoMLCreateDatasetOperator,\n    CloudAutoMLCreateImageTrainingJobOperator,\n    CloudAutoMLDeleteDatasetOperator,\n    CloudAutoMLDeleteImageTrainingJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_automl_image_training\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\nIMAGE_DATASET = {\n    \"display_name\": f\"dataset-disk-{ENV_ID}\",\n    \"image_dataset_metadata\": {\"source_image_uri\": \"\"},\n}\nIMAGE_TRAINING_JOB = {\n    \"display_name\": f\"training-job-name-{ENV_ID}\",\n    \"image_training_job_metadata\": {},\n}\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"automl\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    create_image_dataset = CloudAutoMLCreateDatasetOperator(\n        task_id=\"image_dataset_create\",\n        dataset=IMAGE_DATASET,\n        project_id=PROJECT_ID,\n    )\n\n    create_image_training_job = CloudAutoMLCreateImageTrainingJobOperator(\n        task_id=\"image_training_job_create\",\n        training_pipeline=IMAGE_TRAINING_JOB,\n        project_id=PROJECT_ID,\n    )\n\n    delete_image_dataset = CloudAutoMLDeleteDatasetOperator(\n        task_id=\"image_dataset_delete\",\n        dataset_id=\"{{ task_instance.xcom_pull('image_dataset_create')['datasetId'] }}\",\n        project_id=PROJECT_ID,\n    )\n\n    delete_image_training_job = CloudAutoMLDeleteImageTrainingJobOperator(\n        task_id=\"image_training_job_delete\",\n        training_job_id=\"{{ task_instance.xcom_pull('image_training_job_create')['trainingJobId']}}\",\n        project_id=PROJECT_ID,\n    )\n\n    (\n        # TEST SETUP\n        [create_bucket, create_image_dataset]\n        >> create_image_training_job\n        # TEST BODY\n        >> delete_image_dataset\n        >> delete_image_training_job\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  ( pylint: disable=redefined-builtin )\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2114_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3294306}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that captures and transforms data between two sequential assets, demonstrating how Airflow can manage data dependencies and trigger downstream processes. Create a pipeline that shows how one data asset can trigger and inform the processing of a subsequent asset.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample DAG for demonstrating the usage of assets in Dataflow.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataflow import (\n    DataflowStartFlexTemplateJobOperator,\n    DataflowStartPipelineJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_dataflow_asset\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west3\"\nDATAFLOW_JOB_NAME_TEMPLATE = \"dataflow-job-name-template\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\n# [START howto_operator_dataflow_asset_gcp_url]\nGCP_URL = \"gs://test_bucket/test_file.txt\"\n# [END howto_operator_dataflow_asset_gcp_url]\n\n# [START howto_operator_dataflow_asset_gcs_path]\nPATH_TO_FILE_IN_GCP = \"gcs/path/to/test_file.txt\"\n# [END howto_operator_dataflow_asset_gcs_path]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_start_pipeline_job_from_asset]\n    start_pipeline_fromAsset = DataflowStartPipelineJobOperator(\n        task_id=\"start-pi-from-asset\",\n        job_name=\"{{task_instance.xcom_pull('create_bucket')['name']}}\",\n        project_id=PROJECT_ID,\n        template=\"gcp/dataflow/templates/asset-simple.json\",\n        parameters={\"assetUrl\": GCP_URL},\n        location=LOCATION,\n    )\n    # [END howto_operator_start_pipeline_job_from_asset]\n\n    # [START howto_operator_start_template_job]\n    start_pipeline_fromTemplate = DataflowStartPipelineJobOperator(\n        task_id=\"start-pi-from-template\",\n        job_name=\"{{task_instance.xcom_pull('create_bucket')['name']}}\",\n        project_id=PROJECT_ID,\n        template=\"gcp/dataflow/templates/sum-simple.json\",\n        parameters={\"inputFile\": PATH_TO_FILE_IN_GCP, \"outputPath\": \"gcs/output\"},\n        location=LOCATION,\n    )\n    # [END howto_operator_start_template_job]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> start_pipeline_fromAsset\n        >> start_pipeline_fromTemplate\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1434_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.3294375}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated Kubernetes cluster management workflow that demonstrates creating a GKE cluster, running test pods with different configurations, and then cleaning up the cluster. The workflow should showcase how to dynamically create and delete Kubernetes infrastructure while executing sample workloads.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Kubernetes Engine.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.kubernetes_engine import (\n    GKECreateClusterOperator,\n    GKEDeleteClusterOperator,\n    GKEDeleteComponentOperator,\n    GKEGetComponentsOperator,\n    GKEGetJobHistoryOperator,\n    GKEGetPodHistoryOperator,\n    GKEGetTaskHistoryOperator,\n    GKEREportComponentOperator,\n    GKEREportComponentsOperator,\n    GKEREportJobsOperator,\n    GKEREportPodsOperator,\n    GKEREportTasksOperator,\n)\nfrom airflow.providers.google.cloud.operators.gke_start_pvc import GKESTartPVCOperator\nfrom airflow.providers.google.cloud.operators.gke_stop_component import GKESTopComponentOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nDAG_ID = \"example_gke\"\nGKE_CLUSTER_NAME = f\"gke-k8s-{ENV_ID}\"\nGCE_LOCATION = \"europe-north1-a\"\n\n# [START howto_operator_gke_create_cluster]\nCREATE_CLUSTER_BODY = {\n    \"name\": GKE_CLUSTER_NAME,\n    \"initial_node_count\": 1,\n    \"preemptible_node_pool\": {\n        \"name\": \"projects/{project_id}/locations/{location}/pools/default-pool\",\n        \"config\": {\n            \"node_device_nic\": {\n                \"subnet\": \"projects/{project_id}/regions/{location}/subnetworks/default\",\n            }\n        },\n    },\n}\n# [END howto_operator_gke_create_cluster]\n\n# [START howto_operator_gke_delete_cluster]\nDELETE_CLUSTER_BODY = {\"name\": GKE_CLUSTER_NAME, \"force\": True}\n# [END howto_operator_gke_delete_cluster]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gke_start_pvc]\n    start_pvc = GKESTartPVCOperator(task_id=\"start_pvc\", project_id=GCP_PROJECT_ID, name=\"test-pvc\")\n    # [END howto_operator_gke_start_pvc]\n\n    # [START howto_operator_gke_get_pvc_history]\n    get_pvc_history = GKEGetJobHistoryOperator(\n        task_id=\"get_pvc_history\",\n        project_id=GCP_PROJECT_ID,\n        name=\"test-pvc\",\n        namespace=\"default\",\n        job_name=\"gke-get-pvc-history-job\",\n    )\n    # [END howto_operator_gke_get_pvc_history]\n\n    # [START howto_operator_gke_report_pvc]\n    report_pvc = GKEREportPodsOperator(\n        task_id=\"report_pvc\",\n        project_id=GCP_PROJECT_ID,\n        name=\"test-pvc\",\n        namespace=\"default\",\n        pod_label=\"gke-get-pvc-report-pod-task\",\n    )\n    # [END howto_operator_gke_report_pvc]\n\n    # [START howto_operator_gke_create_component]\n    create_component = GKECreateClusterOperator(\n        task_id=\"create-component\",\n        project_id=GCP_PROJECT_ID,\n        body=CREATE_CLUSTER_BODY,\n        location=GCE_LOCATION,\n    )\n    # [END howto_operator_gke_create_component]\n\n    # [START howto_operator_gke_delete_component]\n    delete_component = GKEDeleteComponentOperator(\n        task_id=\"delete-component\",\n        project_id=GCP_PROJECT_ID,\n        name=create_component.output[\"component_name\"],\n        location=GCE_LOCATION,\n    )\n    # [END howto_operator_gke_delete_component]\n\n    # [START howto_operator_gke_get_components]\n    get_components = GKEGetComponentsOperator(\n        task_id=\"get-components\",\n        project_id=GCP_PROJECT_ID,\n        components_name=\"{{ task_instance.xcom_pull('create-component', key='component_name') }}\",\n    )\n    # [END howto_operator_gke_get_components]\n\n    # [START howto_operator_gke_report_components]\n    report_components = GKEREportComponentsOperator(\n        task_id=\"report-components\",\n        project_id=GCP_PROJECT_ID,\n        components_name=\"{{ task_instance.xcom_pull('create-component', key='component_name') }}\",\n        namespace=\"default\",\n        component_label=\"gke-get-components-report-component\",\n    )\n    # [END howto_operator_gke_report_components]\n\n    # [START howto_operator_gke_umpire_component]\n    umpire_component = GKEREportComponentOperator(\n        task_id=\"umpire-component\",\n        project_id=GCP_PROJECT_ID,\n        name=\"{{ task_instance.xcom_pull('create-component', key='component_name') }}\",\n        namespace=\"default\",\n        component_label=\"gke-get-component-umpire\",\n        image=\"bash:4.4\",\n        commands=['umpire submit --components {{ task_instance.xcom_pull(' 'create-component', key='components_name') }}'],\n    )\n    # [END howto_operator_gke_umpire_component]\n\n    # [START howto_operator_gke_stop_component]\n    stop_component = GKESTopComponentOperator(\n        task_id=\"stop-component\",\n        project_id=GCP_PROJECT_ID,\n        name=\"{{ task_instance.xcom_pull('create-component', key='component_name') }}\",\n        namespace=\"default\",\n        component_label=\"gke-top-component-stop\",\n        force=True,\n    )\n    # [END howto_operator_gke_stop_component]\n\n    # [START howto_operator_gke_delete_cluster]\n    delete_cluster = GKEDeleteClusterOperator(\n        task_id=\"delete-cluster\",\n        name=GKE_CLUSTER_NAME,\n        project_id=GCP_PROJECT_ID,\n        region=GCE_LOCATION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gke_delete_cluster]\n\n    (start_pvc >> get_pvc_history >> report_pvc >> create_component >> get_components >> report_components\n     >> umpire_component >> stop_component >> delete_component >> delete_cluster)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1273_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329451}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates cloud storage data movement and management techniques using Alibaba OSS operators. Create a workflow that captures the essential steps of file lifecycle management: transferring data between local and cloud storage, and executing precise object deletion strategies.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import (\n    OSSCreateBucketOperator,\n    OSSDeleteBucketOperator,\n    OSSDeleteObjectsOperator,\n    OSSDownloadFileOperator,\n    OSSUploadFileOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_oss\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = OSSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=f\"bucket_{DAG_ID}_{ENV_ID}\",\n        region=\"default\",\n        access_key_id=\"myAccessKeyID\",\n        access_key_secret=\"myAccessKeySecret\",\n    )\n\n    upload_file = OSSUploadFileOperator(\n        task_id=\"upload_file\",\n        input_path=\"test.txt\",\n        output_path=\"test.txt\",\n        bucket_name=create_bucket.output.bucket_name,\n        bucket_endpoint=create_bucket.output.endpoint,\n        referer=None,\n        access_key_id=create_bucket.output.access_key_id,\n        access_key_secret=create_bucket.output.access_key_secret,\n    )\n\n    get_file = OSSDownloadFileOperator(\n        task_id=\"get_file\",\n        input_path=\"test.txt\",\n        output_path=\"test_copy.txt\",\n        bucket_name=upload_file.output.bucket_name,\n        bucket_endpoint=upload_file.output.endpoint,\n        referer=None,\n        access_key_id=upload_file.output.access_key_id,\n        access_key_secret=upload_file.output.access_key_secret,\n    )\n\n    delete_file = OSSDeleteObjectsOperator(\n        task_id=\"delete_file\",\n        bucket_name=upload_file.output.bucket_name,\n        obj_names=[upload_file.output.path.split(\"/\")[-1]],\n        force_delete=True,\n    )\n\n    delete_bucket = OSSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=create_bucket.output.bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (create_bucket >> upload_file >> get_file >> delete_file >> delete_bucket)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_55_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983706.329458}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that illustrates the basic mechanics of Azure Blob Storage interaction using Airflow operators, focusing on the sequential upload and deletion of a file in a cloud container.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.blob import (\n    AzureBlobCreateContainerOperator,\n    AzureBlobDeleteContainerOperator,\n    AzureBlobUploadFileOperator,\n)\nfrom airflow.providers.microsoft.azure.transfers.local_to_blob import LocalFileSystemToAzureBlobStorageOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nAZURE_CONN_ID = \"azure_default\"\n\nCONTAINER_NAME = f\"container_{ENV_ID}\"\nBLOB_NAME = \"test-remote-blob\"\nLOCAL_PATH = \"/tmp/remote-file\"\n\n\nwith DAG(\n    dag_id=\"example_local_to_blob\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_container = AzureBlobCreateContainerOperator(\n        task_id=\"create_container\",\n        container_name=CONTAINER_NAME,\n        azure_conn_id=AZURE_CONN_ID,\n    )\n\n    @task\n    def create_temp_file():\n        # [START howto_operator_create_temp_file]\n        from airflow.providers.microsoft.azure.hooks.blob_storage import BlobStorageHook\n\n        blob_storage_hook = BlobStorageHook()\n        temp_file_path = BlobStorageHook().get_free_blobs(container_name=CONTAINER_NAME)[0]\n\n        with open(temp_file_path, \"w\") as file:\n            file.write(\"Test Airflow\")\n        # [END howto_operator_create_temp_file]\n\n        return temp_file_path\n\n    create_temp_file_task = create_temp_file()\n\n    upload_file = AzureBlobUploadFileOperator(\n        task_id=\"upload_file\",\n        local_path=LOCAL_PATH,\n        container_name=CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        azure_conn_id=AZURE_CONN_ID,\n    )\n\n    upload_file_task = upload_file\n\n    delete_container = AzureBlobDeleteContainerOperator(\n        task_id=\"delete_container\",\n        container_name=CONTAINER_NAME,\n        azure_conn_id=AZURE_CONN_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_container_task = delete_container\n\n    # [START howto_operator_get_connection]\n    get_connection = Connection(\n        conn_id=AZURE_CONN_ID,\n        conn_type=\"azure_arm\",\n        description=\"Connecting for AzureBlobUploadFileOperator\",\n        host=\"\",\n        login=\"\",\n        password=\"\",\n        active=True,\n    )\n    # [END howto_operator_get_connection]\n\n    @task\n    def check_connections():\n        # [START howto_operator_check_connections]\n        from airflow.providers.microsoft.azure.hooks.blob_storage import BlobStorageHook\n\n        blob_storage_hook = BlobStorageHook()\n        assert blob_storage_hook.is_connected == True\n        # [END howto_operator_check_connections]\n\n    check_connections_task = check_connections()\n\n    delete_temp_file = AzureBlobUploadFileOperator(\n        task_id=\"delete_temp_file\",\n        local_path=LOCAL_PATH,\n        container_name=CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        delete_existing_files=True,\n        azure_conn_id=AZURE_CONN_ID,\n    )\n\n    delete_temp_file_task = delete_temp_file\n\n    from tests_common.test_utils.watcher import watcher\n\n    list_tasks = [\n        create_container,\n        create_temp_file_task,\n        upload_file_task,\n        delete_container_task,\n        get_connection,\n        check_connections_task,\n        delete_temp_file_task,\n        watcher(),\n    ]\n\n    for task in list_tasks:\n        task.depends_on_past = False\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2159_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3352232}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing orchestration DAG that provisions a multi-node Dataproc cluster with preemptible workers and custom initialization scripts for data analysis tasks. Create a reproducible cloud data processing environment that automatically manages computational resources and supports flexible metadata and package installations.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocSubmitJobOperator with pyspark job.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocSubmitJobOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"gcp_dataproc_pyspark\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster-{ENV_ID}-{DAG_ID}\".replace(\"_\", \"-\")\nREGION = \"europe-west1\"\n\n# Cluster definition\n# [START how_to_cloud_dataproc_create_cluster]\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"software_config\": {\n        \"image_version\": \"2.0.1\",\n        \"hadoop_version\": \"2.7.3\",\n    },\n}\n# [END how_to_cloud_dataproc_create_cluster]\n\n# Jobs definitions\n# [START how_to_cloud_dataproc_pyspark_config]\nSPARK_PYSPIRA_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"pyspark_job\": {\"main_python_class\": \"WordCountClass\"},\n    \"args\": [\"./wordcount.sh\"],\n}\n# [END how_to_cloud_dataproc_pyspark_config]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataproc\", \"pyspark\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    run_pyspark_job = DataprocSubmitJobOperator(\n        task_id=\"run_pyspark_job\", region=REGION, project_id=PROJECT_ID, job=SPARK_PYSPIRA_JOB\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> run_pyspark_job\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2031_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3352337}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Could you provide a detailed explanation of how the `input()` function works in Python, including its syntax and typical use cases, and perhaps some examples?"}, {"role": "assistant", "content": "value = input(prompt)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3352473}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data processing and machine learning pipeline that takes raw image data from Google Cloud Storage, prepares it for object detection training, and creates a deployable machine learning model. Implement a systematic approach to handling data ingestion, model creation, and infrastructure management using Airflow and Google Cloud services.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "# mypy ignore arg types (Autoschema doesn't like missing args)\n# type annotations provided for docs purposes only\n\n\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import third_party_config\nfrom system.google import workspace_config\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nPROJECT_ID = third_party_config.get_system_test_project()\nREGION = \"us-central1\"\n\nFORECASTING_DATASET = {\n    \"display_name\": f\"forecast-dataset-{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.time_series,\n    \"metadata\": {\n        \"input_config\": {\n            \"bigquery_source\": {\n                \"project_id\": PROJECT_ID,\n                \"table_id\": f\"{workspace_config.get_workspace_display_name()}_{DAG_ID}_forecast-dataset\",\n            }\n        },\n        \"time_series_config\": {\n            \"train_test_split_ratio\": 0.2,\n            \"time_column\": \"airline_travel_date\",\n            \"target_column\": \"airline_passenger_count\",\n            \"time_series_split_type\": \"RANDOM_TRAIN_TEST_SPLIT\",\n        },\n    },\n}\n\nTEST_TIME_COLUMN = \"airline_travel_date\"\nTEST_TARGET_COLUMN = \"airline_passenger_count\"\nTEST_DATASET_SIZE = 100\nTEST_TIME_SERIES_SPLIT_TYPE = \"RANDOM_TRAIN_TEST_SPLIT\"\nTEST_TIME_SERIES_BOUNDARY_COUNTS = TEST_DATASET_SIZE // 10\n\n\nwith DAG(\n    f\"{DAG_ID}_auto_ml_operations\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"auto_ml\"],\n) as dag:\n    run_this_dag = ThirdPartyTestRunMasterTaskOperator(\n        task_id=\"run_this_dag_task\",\n        image=\"python:3.9\",\n        command=[\n            \"pip\",\n            \"install\",\n            \"-r\",\n            f\"--requirement={workspace_config.get_workspace_display_path()}/requirements.txt\"],\n        python_callable=lambda):  # noqa\n    set_up_vertex_ai_agent = SystemTestSetupOperator(\n        task_id=\"set_up_vertex_ai_agent\",\n        container_name=\"vertex-ai-setup-env\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        help_message=\"User needs to enter his vertex ai provider admin project credentials and pass them as environment variables\"),\n    setup_npm_packages = SystemTestSetupOperator(\n        task_id=\"setup_npm_packages\",\n        container_name=\"npm-install-npm-packages\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        help_message=\"NPM packages need to be installed in order to continue\",\n        package_json_content=[  # NPY-576\n            f\"sudo npm install -g cloud-build-api-helper@^0.4.6\",\n            f\"sudo npm install -g {ThirdPartyTestSetupTaskHelperTask.operator_output['artifact_uri']} --save-dev\"\n        ])\n    )\n\n    @task\n    def create_forecasting_dataset(region, display_name, dataset_resource):\n        prediction_prediction_time_column = \"airline_travel_date\"\n        prediction_target_column = \"airline_passenger_count\"\n        bigquery_dataset = gcp_vision_aiplatform.datasets.CreateDataset(\n            request={\n                \"display_name\": display_name,\n                \"metadata\": {\n                    \"input_config\": {\n                        \"bigquery_source\": {\"project_id\": dataset_resource[\"project_id\"], \"table_id\": dataset_resource[\"dataset_id\"]}\n                    },\n                    \"time_series_config\": {\n                        \"train_test_split_ratio\": 0.2,\n                        \"time_column\": prediction_prediction_time_column,\n                        \"target_column\": prediction_target_column,\n                        \"time_series_split_type\": \"RANDOM_TRAIN_TEST_SPLIT\",\n                    },\n                },\n                \"device_id\": region,\n            })\n        return bigquery_dataset\n\n    # [START howto_cloud_create_object_detection_training_job]\n    create_object_detection_training_job = gcp_vision_aiplatform.auto_ml.CreateAutoMLForecastingTrainingJob(\n        request={\n            \"display_name\": f\"auto-mdt-training-{ENV_ID}\",\n            \"predictor_type\": \"TIME_SERIES_CLASSIFICATION\",\n            \"time_series_config\": {\n                \"time_column\": TEST_TIME_COLUMN,\n                \"target_column\": TEST_TARGET_COLUMN,\n                \"dataset_size\": TEST_DATASET_SIZE,\n                \"time_series_split_type\": TEST_TIME_SERIES_SPLIT_TYPE,\n                \"time_series_boundary_counts\": TEST_TIME_SERIES_BOUNDARY_COUNTS,\n            },\n            \"optimization_spec\": {\n                \"objective_expression\": \"MAXIMIZE_R2\",\n                \"eval_metric_weights\": {\"r2\": 1}\n            },\n            \"parent_pipeline\": \"\",\n            \"dataset_metadata\": dataset_resource,\n            \"service_account\": SERVICE_ACCOUNT,\n            \"region\": REGION,\n            \"project_id\": PROJECT_ID,\n        })\n    # [END howto_cloud_create_object_detection_training_job]\n\n    # [START howto_cloud_create_image_classification_training_job]\n    create_image_classification_training_job = gcp_vision_aiplatform.auto_ml.CreateAutoMLForecastingTrainingJob(\n        request={\n            \"display_name\": f\"auto-mdt-classification-training-{ENV_ID}\",\n            \"predictor_type\": \"TIME_SERIES_CLASSIFICATION\",\n            \"time_series_config\": {\n                \"time_column\": TEST_TIME_COLUMN,\n                \"target_column\": TEST_TARGET_COLUMN,\n                \"dataset_size\": TEST_DATASET_SIZE,\n                \"time_series_split_type\": TEST_TIME_SERIES_SPLIT_TYPE,\n                \"time_series_boundary_counts\": TEST_TIME_SERIES_BOUNDARY_COUNTS,\n            },\n            \"optimization_spec\": {\n                \"objective_expression\": \"MAXIMIZE_R2\",\n                \"eval_metric_weights\": {\"r2\": 1}\n            },\n            \"parent_pipeline\": \"\",\n            \"dataset_metadata\": dataset_resource,\n            \"service_account\": SERVICE_ACCOUNT,\n            \"region\": REGION,\n            \"project_id\": PROJECT_ID,\n        })\n    # [END howto_cloud_create_image_classification_training_job]\n\n    # [START howto_cloud_list_auto_ml_jobs]\n    list_auto_ml_jobs = gcp_vision_aiplatform.auto_ml.ListAutoMLJobs(\n        request={}, page_size=50, page_token=\"\", language_field=\"\", target_column=\"\"\n    )\n    # [END howto_cloud_list_auto_ml_jobs]\n\n    # [START howto_cloud_get_auto_ml_job]\n    get_object_detection_training_job = gcp_vision_aiplatform.auto_ml.GetAutoMLForecastingTrainingJob(\n        request={\"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"]}\n    )\n    # [END howto_cloud_get_auto_ml_job]\n\n    # [START howto_cloud_delete_auto_ml_job]\n    delete_object_detection_training_job = gcp_vision_aiplatform.auto_ml.DeleteAutoMLForecastingTrainingJob(\n        request={\"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"]}\n    )\n    # [END howto_cloud_delete_auto_ml_job]\n\n    # [START howto_cloud_train_detect_object_model]\n    train_detect_object_model = gcp_vision_aiplatform.auto_ml TrainAutoMLForecastingModel(\n        request={\n            \"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"],\n            \"model_display_name\": \"test_detect_object\",\n            \"training_fraction_split\": 0.7,\n            \"validation_fraction_split\": 0.2,\n            \"test_fraction_split\": 0.1,\n            \"job_dir\": JOB_DIR,\n        })\n    # [END train_detect_object_model]\n\n    # [START howto_cloud_deploy_detect_object_model]\n    deploy_detect_object_model = gcp_vision_aiplatform.auto_ml DeployAutoMLForecastingModel(\n        request={\n            \"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"],\n            \"model_display_name\": \"test_detect_object\",\n            \"container_spec\": {\n                \"image_uri\": \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n                \"command\": [\"sh\", \"-c\", \"set +e; sleep 20; exit 0;\"],\n                \"env\": [\n                    {\"name\": \"INPUT_GCS_BUCKET\", \"value\": INPUT_GCS_BUCKET},\n                    {\"name\": \"RESOURCE_DATA_BUCKET\", \"value\": RESOURCE_DATA_BUCKET},\n                ],\n            },\n            \"parallel_batch_size\": 1,\n            \"prediction_type\": \"DETECT\",\n            \"python_version\": \"39\",\n            \"model_staging_directory\": MODEL_STAGING_DIRECTORY,\n            \"job_dir\": JOB_DIR,\n        })\n    # [END deploy_detect_object_model]\n\n    # [START howto_cloud_list_deployment]\n    list_deployment = gcp_vision_aiplatform.auto_ml.ListDeployedModels(\n        request={}, page_size=50, page_token=\"\", language_field=\"\", target_column=\"\"\n    )\n    # [END list_deployment]\n\n    # [START howto_cloud_get_deployment]\n    get_deployment = gcp_vision_aiplatform.auto_ml.GetDeployedModel(\n        request={\"deployed_model_id\": deployed_models_items[\"test_detect_object\"]}\n    )\n    # [END get_deployment]\n\n    # [START howto_cloud_delete_deployment]\n    delete_deployment = gcp_vision_aiplatform.auto_ml DeleteDeployedModel(\n        request={\"deployed_model_id\": deployed_models_items[\"test_detect_object\"]}\n    )\n    # [END delete_deployment]\n\n    # [START howto_cloud_predict_classify_image_model]\n    predict_classify_image_model = gcp_vision_aiplatform.auto_ml PredictAutoMLForecastingModel(\n        request={\n            \"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"],\n            \"test_image_gcs_uris\": [IMAGES_URI],\n            \"model_display_name\": \"test_classify_image\",\n            \"parallel_batch_size\": 1,\n            \"prediction_type\": \"PREDICT_ONLY\",\n            \"python_version\": \"39\",\n            \"model_staging_directory\": MODEL_STAGING_DIRECTORY,\n            \"job_dir\": JOB_DIR,\n        })\n    # [END predict_classify_image_model]\n\n    # [START howto_cloud_list_offline_deployment]\n    list_offline_deployment = gcp_vision_aiplatform.auto_ml.ListOfflineDeployedModels(\n        request={}, page_size=50, page_token=\"\", language_field=\"\", target_column=\"\"\n    )\n    # [END list_offline_deployment]\n\n    # [START howto_cloud_get_offline_deployment]\n    get_offline_deployment = gcp_vision_aiplatform.auto_ml.GetOfflineDeployedModel(\n        request={\"offline_deployed_model_id\": offline_deployed_models_items[\"test_classify_image\"]]\n    )\n    # [END get_offline_deployment]\n\n    # [START howto_cloud_delete_offline_deployment]\n    delete_offline_deployment = gcp_vision_aiplatform.auto_ml DeleteOfflineDeployedModel(\n        request={\"offline_deployed_model_id\": offline_deployed_models_items[\"test_classify_image\"]}\n    )\n    # [END delete_offline_deployment]\n\n    # [START howto_cloud_batch_predict_classify_image_model]\n    batch_predict_classify_image_model = gcp_vision_aiplatform.auto_ml BatchPredictAutoMLForecastingModel(\n        request={\n            \"auto_ml_job_id\": auto_ml_jobs_items[\"auto-mdt-training-id\"],\n            \"expressions\": {\n                \"test_image_gcs_uris\": [IMAGES_URI],\n                \"reference_image_gcs_uris\": [REF_IMAGES_URI],\n                \"video_gcs_uris\": [VIDEOS_URI],\n                \"text_gcs_uris\": [TEXTS_URI],\n                \"csv_gcs_uris\": [CSV_URI],\n                \"json_gcs_uris\": [JSON_URI],\n            },\n            \"output_access\": \"SERVERLESS\",\n            \"model_display_name\": \"test_classify_image_batch\",\n            \"parallel_batch_size\": 1,\n            \"prediction_type\": \"BATCH_PREDICT\",\n            \"python_version\": \"39\",\n            \"model_staging_directory\": MODEL_STAGING_DIRECTORY,\n            \"job_dir\": JOB_DIR,\n        })\n    # [END batch_predict_classify_image_model]\n\n    (\n        run_this_dag\n        >> set_up_vertex_ai_agent\n        >> setup_npm_packages\n        >> task_1\n        >> task_2\n        >> task_3\n        >> task_4\n        >> task_5\n        >> task_6\n        >> task_7\n        >> task_8\n        >> task_9\n        >> task_10\n        >> task_11\n        >> task_12\n        >> task_13\n        >> task_14\n        >> task_15\n        >> task_16\n        >> task_17\n        >> task_18\n        >> task_19\n        >> task_20\n        >> task_21\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_22\n        >> train_detect_object_model\n        >> deploy_detect_object_model\n        >> list_deployment\n        >> get_deployment\n        >> delete_deployment\n        >> task_23\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_24\n        >> create_forecasting_dataset\n        >> task_25\n        >> create_image_classification_training_job\n        >> train_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_26\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_27\n        >> create_forecasting_dataset\n        >> task_28\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_29\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_30\n        >> create_image_classification_training_job\n        >> test_batch_predict_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_31\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_32\n        >> create_forecasting_dataset\n        >> task_33\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_34\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_35\n        >> create_forecasting_dataset\n        >> task_36\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_37\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_38\n        >> create_image_classification_training_job\n        >> test_batch_predict_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_39\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_40\n        >> create_forecasting_dataset\n        >> task_41\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_42\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_43\n        >> create_forecasting_dataset\n        >> task_44\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_45\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_46\n        >> create_image_classification_training_job\n        >> test_batch_predict_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_47\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_48\n        >> create_forecasting_dataset\n        >> task_49\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_50\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_51\n        >> create_forecasting_dataset\n        >> task_52\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_53\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_54\n        >> create_image_classification_training_job\n        >> test_batch_predict_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_55\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_56\n        >> create_forecasting_dataset\n        >> task_57\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_58\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_59\n        >> create_forecasting_dataset\n        >> task_60\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_61\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_62\n        >> create_image_classification_training_job\n        >> test_batch_predict_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_63\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_64\n        >> create_forecasting_dataset\n        >> task_65\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_66\n        >> list_auto_ml_jobs\n        >> get_object_detection_training_job\n        >> delete_object_detection_training_job\n        >> task_67\n        >> create_forecasting_dataset\n        >> task_68\n        >> batch_predict_classify_image_model\n        >> list_deployed_models\n        >> get_deployment\n        >> delete_deployment\n        >> task_69\n        >> list_auto_ml_jobs"}], "metadata": {"file_name": "dag_462_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3352876}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a modular Airflow pipeline that uses task decorators to execute two distinct Pinot interaction tasks with different connection methods. Implement a DAG that demonstrates parallel task execution using Pinot admin and database API hooks.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pinot.operators.pinot_admin import PinotAdminOperator\nfrom airflow.providers.pinot.operators.pinot_database import PinotDatabaseToPinotOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_pinot_admin_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    # [START howto_operator_pinot_admin]\n    populate_pinot_table = PinotAdminOperator(task_id=\"populate_table_task\", sql=\"CREATE TABLE my_table (col1 INT)\")\n    # [END howto_operator_pinot_admin]\n\n    # [START howto_operator_pinot_database_to_pinot]\n    pinot_to_pinot_example = PinotDatabaseToPinotOperator(\n        task_id=\"pinot_to_pinot_task\",\n        conn_id=\"my_pinot_conn\",\n        sql=\"SELECT * FROM cdcify_db.my_table\",\n        pinot_table=\"dest_table\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_pinot_database_to_pinot]\n\n    populate_pinot_table >> pinot_to_pinot_example\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_112_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3352926}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data processing workflow that transforms unstructured trivia data into a semantically searchable vector database using AI-powered embedding techniques. Create an end-to-end pipeline that demonstrates how to load, vectorize, query, and manage structured knowledge data.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom weaviate.classes.config import UseWeaviateClassConfig\nfrom weaviate.components.utils import get_class_config_dynamic_loader\nfrom weaviate.data.reader import Reader\nfrom weaviate.data.wvchandler import WeaviateCHandler\n\nfrom airflow.decorators import dag, task, task_group\nfrom airflow.models.dag import DAG\nfrom airflow.providers.weaviate.operators.weaviate import (\n    WeaviateDocumentIngestOperator,\n    WeaviateIngestOperator,\n)\nfrom airflow.providers.weaviate.operators.weaviate_vectorization import (\n    WeaviateVectorizeDataOperator,\n)\n\nTRIVIA_TABLE = \"Trivia\"\nEXTRA_COLUMNS = [\"answer\", \"category\"]\nWEAVIATE_CLASS_NAME = \"Question\"\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"weaviate\"],\n)\ndef example_weaviate_dag():\n    @task_group(group_name=\"load_data\")\n    def load_data():\n        reader = Reader(reader_config=UseWeaviateClassConfig(class_name=WEAVIATE_CLASS_NAME))\n        # Load pre-computed vector embeddings from Weaviate CHandler.\n        vectors = reader.load_vectors(TRIVIA_TABLE, chunk_size=50, batch_size=100)\n        # Split vectors and columns to ingest them in one SQL insert command.\n        question_vectors = list(zip(vectors[\"question_vector\"], vectors[EXTRA_COLUMNS]))\n        answer_vectors = list(zip(vectors[\"answer_vector\"], vectors[EXTRA_COLUMNS]))\n\n        # [START howto_operator_weaviate_document_ingest]\n        weaviate_document_ingest = WeaviateDocumentIngestOperator()\n        weaviate_document_ingest(\n            task_id=\"ingest_documents\",\n            class_name=WEAVIATE_CLASS_NAME,\n            input_json=[\n                {\"question\": question[0], \"answer\": question[1], \"correct_answer\": question[2]}\n                for question in question_vectors\n            ],\n            vectors=[question[0] for question in question_vectors],\n            transaction=True,\n        )\n        # [END howto_operator_weaviate_document_ingest]\n\n        # [START howto_operator_weaviate_vectorized_ingest]\n        weaviate_vectorized_ingest = WeaviateVectorizeDataOperator()\n        weaviate_vectorized_ingest(\n            task_id=\"vectorized_ingest\",\n            class_name=WEAVIATE_CLASS_NAME,\n            input_json=[\n                {\n                    \"question\": question[0],\n                    \"answer\": question[1],\n                    \"correct_answer\": question[2],\n                    \"question_vector\": question[0],\n                    \"answer_vector\": question[0],\n                }\n                for question in answer_vectors\n            ],\n            vectors=[question[0] for question in answer_vectors],\n            transaction=True,\n        )\n        # [END howto_operator_weaviate_vectorized_ingest]\n\n    load_data()\n\n    @task_group(group_name=\"unload_data\")\n    def unload_data():\n        # [START howto_operator_weaviate_vectorize_delete]\n        weaviate_vectorize_delete = WeaviateVectorizeOperator()\n        weaviate_vectorize_delete(\n            task_id=\"vectorize_delete\",\n            class_name=WEAVIATE_CLASS_NAME,\n            input_json={\"ids\": [[x] for x in range(100)]},\n            method=\"ids_filtering\",\n            transaction=True,\n        )\n        # [END howto_operator_weaviate_vectorize_delete]\n\n        # [START howto_operator_weaviate_document_unload]\n        weaviate_document_unload = WeaviateIngestOperator()\n        weaviate_document_unload(\n            task_id=\"document_unload\",\n            class_name=WEAVIATE_CLASS_NAME,\n            input_json=[{\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"correct_answer\": \"France\"}],\n            vectors=[],\n            transaction=True,\n        )\n        # [END howto_operator_weaviate_document_unload]\n\n    unload_data()\n\n\nexample_weaviate_dag()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run()"}], "metadata": {"file_name": "dag_1833_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335304}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated metadata registration pipeline that enables systematic tracking and organization of data assets across different systems and repositories using Google Cloud Dataplex. Design a workflow that captures metadata attributes, defines entry types, and creates catalog entries to support comprehensive data discovery and governance initiatives.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Dataplex Catalog.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.dataplex_v1 import Location\nfrom google.protobuf.field_mask_pb2 import FieldMask\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateCatalogEntryOperator,\n    DataplexCreateEntryGroupOperator,\n    DataplexCreateLocationOperator,\n    DataplexCreateWorkforceOperator,\n    DataplexDeleteCatalogEntryOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteLocationOperator,\n    DataplexDeleteWorkforceOperator,\n    DataplexGetCatalogEntryOperator,\n    DataplexGetEntryGroupOperator,\n    DataplexGetWorkforceOperator,\n    DataplexListCatalogEntriesOperator,\n    DataplexListEntryGroupsOperator,\n    DataplexListWorkforcesOperator,\n    DataplexUpdateCatalogEntryOperator,\n    DataplexUpdateEntryGroupOperator,\n    DataplexUpdateWorkforceOperator,\n)\nfrom airflow.providers.google.cloud.sensors.dataplex import DataplexCatalogSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"dataplex_catalog\"\nLOCATION = Location.region_id + \"-location\"  # Type: str\nCATALOG_ID = LOCATION + \"-catalog\"  # Type: str\nENTRY_GROUP_ID = LOCATION + \"-entry-group\"  # Type: str\nWORKFORCE_ID = LOCATION + \"-workforce\"  # Type: str\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataplex\"],\n):\n\n    # [START howto_operator_dataplex_create_location]\n    create_location_task = DataplexCreateLocationOperator(\n        task_id=\"create_location_task\",\n        location=LOCATION,\n        body={\n            \"display_name\": \"test-location\",\n        },\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_create_location]\n\n    # [START howto_operator_dataplex_create_entry_group]\n    create_entry_group_task = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        body={\n            \"display_name\": \"test-entry-group\",\n        },\n    )\n    # [END howto_operator_dataplex_create_entry_group]\n\n    # [START howto_operator_dataplex_create_workforce]\n    create_workforce_task = DataplexCreateWorkforceOperator(\n        task_id=\"create_workforce_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        workforce_id=WORKFORCE_ID,\n        body={\n            \"agent\": {\n                \"image_uri\": \"us-docker.pkg.dev/vertex-ai/workloads/composer/2022-01-01-v1:latest\",\n            }\n        },\n    )\n    # [END howto_operator_dataplex_create_workforce]\n\n    # [START howto_operator_dataplex_create_catalog_entry]\n    create_catalog_entry_task = DataplexCreateCatalogEntryOperator(\n        task_id=\"create_catalog_entry_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        catalog_id=CATALOG_ID,\n        resource_name=f\"gs://{location}/airflow/dataplex/catalog/my_table.csv\",\n        schema=[\n            {\"name\": \"col1\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"col2\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        table_id=\"my_table\",\n        description=\"My Table\"\uff0c\n        display_name=\"My Table\",\n        is_tracked=True,\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_create_catalog_entry]\n\n    # [START howto_operator_dataplex_get_catalog_entry]\n    get_catalog_entry_task = DataplexGetCatalogEntryOperator(\n        task_id=\"get_catalog_entry_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        catalog_id=CATALOG_ID,\n        table_id=\"my_table\",\n        read_schema_request_body={},\n        write_schema_request_body={},\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_get_catalog_entry]\n\n    # [START howto_operator_dataplex_list_catalog_entries]\n    list_catalog_entries_task = DataplexListCatalogEntriesOperator(\n        task_id=\"list_catalog_entries_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        page_size=5,\n        read_schema_request_body={},\n        write_schema_request_body={},\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_list_catalog_entries]\n\n    # [START howto_operator_dataplex_update_catalog_entry]\n    update_catalog_entry_task = DataplexUpdateCatalogEntryOperator(\n        task_id=\"update_catalog_entry_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        catalog_id=CATALOG_ID,\n        table_id=\"my_table\",\n        resource_name=f\"gs://{location}/airflow/dataplex/catalog/my_table.csv\",\n        schema=[\n            {\"name\": \"col1\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n            {\"name\": \"col2\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        read_schema_request_body={},\n        write_schema_request_body={},\n        new_resource_name=f\"gs://{location}/airflow/dataplex/catalog/new_name.csv\",\n        new_table_id=\"new_table\",\n        description=\"Updated Description\"\uff0c\n        display_name=\"New Table\"\uff0c\n        is_tracked=True,\n        patch_mask=FieldMask(paths=[\"description\", \"displayName\", \"isTracked\"]),\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_update_catalog_entry]\n\n    # [START howto_operator_dataplex_delete_catalog_entry]\n    delete_catalog_entry_task = DataplexDeleteCatalogEntryOperator(\n        task_id=\"delete_catalog_entry_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        catalog_id=CATALOG_ID,\n        table_id=\"my_table\",\n    )\n    # [END howto_operator_dataplex_delete_catalog_entry]\n\n    # [START howto_operator_dataplex_get_entry_group]\n    get_entry_group_task = DataplexGetEntryGroupOperator(\n        task_id=\"get_entry_group_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        group=f\"gs://{location}/airflow/dataplex/existing_entry_group.json\",\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_get_entry_group]\n\n    # [START howto_operator_dataplex_list_entry_groups]\n    list_entry_groups_task = DataplexListEntryGroupsOperator(\n        task_id=\"list_entry_groups_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        page_size=5,\n        group=f\"gs://{location}/airflow/dataplex/existing_group.json\",\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_list_entry_groups]\n\n    # [START howto_operator_dataplex_update_entry_group]\n    update_entry_group_task = DataplexUpdateEntryGroupOperator(\n        task_id=\"update_entry_group_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        group=f\"gs://{location}/airflow/dataplex/new_exiting_group.json\",\n        body={\"description\": \"Updated Description\"\uff0c},\n        patch_mask=FieldMask(paths=[\"description\"]),\n        deferrable=True,\n    )\n    # [END howto_operator_dataplex_update_entry_group]\n\n    # [START howto_operator_dataplex_delete_entry_group]\n    delete_entry_group_task = DataplexDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n    )\n    # [END howto_operator_dataplex_delete_entry_group]\n\n    # [START howto_operator_dataplex_create_workforce_with_athena]\n    create_workforce_athena_task = DataplexCreateWorkforceOperator(\n        task_id=\"create_workforce_athena\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        workforce_id=WORKFORCE_ID + \"_athena\",\n        body={\n            \"agent\": {\n                \"image_uri\": \"us-docker.pkg.dev/vertex-ai/workloads/composer/2022-01-01-v1:latest\",\n                \"extra_config\": {\n                    \"jars\": [\n                        \"gs://pub/gcs-public-sha256.txt\",\n                        f\"gs://{location}/airflow/dataplex/athena/hive-extras-hadoop-0.14.7.jar\",\n                    ]\n                },\n            },\n        },\n    )\n    # [END howto_operator_dataplex_create_workforce_with_athena]\n\n    # [START howto_operator_dataplex_get_workforce]\n    get_workforce_task = DataplexGetWorkforceOperator(\n        task_id=\"get_workforce_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        workforce_id=WORKFORCE_ID + \"_athena\",\n    )\n    # [END howto_operator_dataplex_get_workforce]\n\n    # [START howto_operator_dataplex_list_workforces]\n    list_workforces_task = DataplexListWorkforcesOperator(\n        task_id=\"list_workforces_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        page_size=5,\n        workforce_id=\"{{ task_instance.xcom_pull('create_workforce_athena')['id'] }}\",\n    )\n    # [END howto_operator_dataplex_list_workforces]\n\n    # [START howto_operator_dataplex_delete_workforce]\n    delete_workforce_task = DataplexDeleteWorkforceOperator(\n        task_id=\"delete_workforce_task\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        workforce_id=WORKFORCE_ID + \"_athena\",\n    )\n    # [END howto_operator_dataplex_delete_workforce]\n\n    # [START howto_sensor_dataplex_catalog]\n    wait_for_creation_of_catalog_entry_task = DataplexCatalogSensor(\n        task_id=\"wait_for_creation_of_catalog_entry\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        catalog_id=CATALOG_ID,\n        table_id=\"my_table\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n    # [END start_wait_for_creation_of_catalog_entry_task]\n\n    # [START howto_operator_dataplex_create_location_result_to_gcs]\n    create_location_result_to_gcs_task = DataplexCreateLocationOperator(\n        task_id=\"create_location_result_to_gcs_task\",\n        location=LOCATION,\n        body={\n            \"display_name\": \"test-location-result-to-gcs\",\n        },\n    )\n    # [END howto_operator_dataplex_create_location_result_to_gcs]\n\n    # [START howto_operator_dataplex_get_location]\n    get_location_task = DataplexGetLocationOperator(\n        task_id=\"get_location_task\", location=LOCATION, deferrable=True\n    )\n    # [END howto_operator_dataplex_get_location]\n\n    # [START howto_operator_dataplex_list_locations]\n    list_locations_task = DataplexListLocationsOperator(\n        task_id=\"list_locations_task\", location=LOCATION, page_size=5\n    )\n    # [END howto_operator_dataplex_list_locations]\n\n    # [START howto_operator_dataplex_delete_location]\n    delete_location_task = DataplexDeleteLocationOperator(\n        task_id=\"delete_location_task\", location=LOCATION\n    )\n    # [END howto_operator_dataplex_delete_location]\n\n    (\n        # TEST SETUP\n        create_location_task\n        >> create_entry_group_task\n        >> create_workforce_task\n        >> create_catalog_entry_task\n        >> create_location_result_to_gcs_task\n        >> get_location_task\n        >> list_locations_task\n        >> delete_location_task\n        # TEST BODY\n        >> wait_for_creation_of_catalog_entry_task\n        >> get_entry_group_task\n        >> list_workforces_task\n        >> delete_workforce_task\n        # TEST TEARDOWN\n        >> delete_catalog_entry_task\n        >> delete_entry_group_task\n        >> delete_workforce_athena_task\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2026_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353322}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration pipeline that moves files from cloud object storage to a secure file transfer protocol destination with complete lifecycle tracking. Create a workflow that demonstrates secure, reliable data transfer between different cloud storage systems with comprehensive task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.sftp.sensors.base import SFTPSensor\nfrom airflow.providers.sftp.transfers.local_to_sftp import LocalFileSystemToSFTPTransfert\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.path import Path\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_sftp_transfer\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"sftp_default\",\n        conn_type=\"sftp\",\n        host=\"sftp.how2hack.com\",\n        login=\"how2hack\",\n        password=\"how2hack\",\n    )\n\n    @task\n    def create_file_in_local_file_system():\n        local_filepath = Path(\"/local_filepath\")\n        local_filepath.parent.mkdir(parents=True, exist_ok=True)\n        local_filepath.touch()\n\n    @task\n    def create_file_in_sftp(tmp_dir):\n        from airflow.providers.sftp.transfers.local_to_sftp import LocalFilesystemToSFTPTransfert\n\n        transfert = LocalFilesystemToSFTPTransfert()\n        transfert.source_path = str(Path(local_filepath))\n        transfert.destination_path = f\"{tmp_dir}/\"\n        transfert.execute()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_file_in_sftp(tmp_dir):\n        from airflow.providers.sftp.transfers.local_to_sftp import LocalFilesystemToSFTPTransfert\n\n        transfert = LocalFilesystemToSFTPTransfert()\n        transfert.source_path = f\"{tmp_dir}/*.txt\"\n        transfert.execute()\n\n    local_file_system_task = create_file_in_local_file_system()\n    local_filepath = Path(\"/local_filepath\")\n\n    sftp_tasks = create_file_in_sftp(str(local_filepath))\n\n    join_task = SFTPSensor(\n        task_id=\"wait_for_sftp_files\",\n        sftp_conn_id=\"sftp_default\",\n        path=f\"{sftp_tasks.output_pattern}\",\n        check_interval=5,\n        timeout=60 * 4,\n    )\n\n    remove_file_in_sftp(str(local_filepath))\n\n    local_file_system_task >> sftp_tasks >> join_task\n\n    # Task dependencies created via `XComArgs` are not meant to be evaluated\n    # nor passed to external hooks.\n    # They are here to illustrate how XCom can help such external tools work.\n    sftp_tasks.xcom_args.append(join_task.task_id)\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done and last\n    # for the cleanup to be triggered\n    list(watcher(dag))\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_756_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353405}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that creates a schema and multiple tables in Trino for storing city information, demonstrating basic database schema creation and data insertion techniques. The pipeline should showcase different query methods including templated and parameterized queries.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.trino.operators.trino import TrinoExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_trino\"\n\nwith models.DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_trino]\n    sql1 = \"\"\"\n        CREATE TABLE cities (\n            cityid INT,\n            name VARCHAR(50),\n            continent VARCHAR(50)\n        );\n        \"\"\"\n\n    trino_sql1 = TrinoExecuteQueryOperator(task_id=\"trino_task1\", sql=sql1)\n\n    # [END howto_operator_trino]\n\n    # [START howto_operator_trino_inlet]\n    sql2 = \"INSERT INTO cities VALUES (1, 'Dallas', 'North America');\"\n\n    trino_sql2 = TrinoExecuteQueryOperator(task_id=\"trino_task2\", sql=sql2)\n    # [END howto_operator_trino_inlet]\n\n    # [START howto_operator_trino_params]\n    sql3 = \"INSERT INTO cities VALUES (2, 'Chicago', 'North America')\"\n\n    trino_sql3 = TrinoExecuteQueryOperator(task_id=\"trino_task3\", sql=sql3, parameters={\"name\": \"Minneapolis\", \"continent\": \"North America\"})\n    # [END howto_operator_trino_params]\n\n    # [START howto_operator_trino_inlet_params]\n    sql4 = \"INSERT INTO cities VALUES (3, 'New York', 'North America)'\"\n\n    trino_sql4 = TrinoExecuteQueryOperator(\n        task_id=\"trino_task4\",\n        sql=sql4,\n        parameters={\n            \"name\": \"Buffalo\",\n            \"continent\": \"North America\",\n        },\n    )\n    # [END howto_operator_trino_inlet_params]\n\n    # [START howto_operator_trino_at_least_one]\n    sql5 = \"SELECT * FROM cities WHERE name IN ('Dallas', 'Chicago', 'Minneapolis')\"\n\n    trino_sql5 = TrinoExecuteQueryOperator(task_id=\"trino_task5\", sql=sql5, min_records=1, trigger_rule=TriggerRule.ALL_DONE)\n    # [END howto_operator_trino_at_least_one]\n\n    # [START howto_operator_trino_at_least_one_all_done]\n    sql6 = \"SELECT * FROM cities WHERE name IN ('Dallas', 'Chicago', 'Minneapolis')\"\n\n    trino_sql6 = TrinoExecuteQueryOperator(\n        task_id=\"trino_task6\", sql=sql6, min_records=1, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_trino_at_least_one_all_done]\n\n    # [START howto_operator_trino_at_least_one_skip_if_zero]\n    sql7 = \"SELECT * FROM cities WHERE name IN ('Dallas', 'Chicago', 'Minneapolis')\"\n\n    trino_sql7 = TrinoExecuteQueryOperator(\n        task_id=\"trino_task7\", sql=sql7, min_records=1, skip_if_zero=True, trigger_rule=TriggerRule.ALL_DONE\n    )\n    # [END howto_operator_trino_at_least_one_skip_if_zero]\n\n    # [START howto_operator_trino_merge_schema]\n    sql8 = \"CREATE TABLE stk AS SELECT * FROM cities\"\n\n    trino_sql8 = TrinoExecuteQueryOperator(task_id=\"trino_task8\", sql=sql8)\n\n    merge_sql = \"MERGE INTO cities tgt SET \" \"name = src.name, continent = src.continent WHERE cityid = src.cityid\"\n\n    trino_merge_sql = TrinoExecuteQueryOperator(task_id=\"trino_task9\", sql=merge_sql, parameters={\"src\": {\"cityid\": 1, \"name\": \"Dallas\", \"continent\": \"North America\"}})\n    # [END howto_operator_trino_merge_schema]\n\n    # [START howto_operator_trino_fallback]\n    fallback_sql = \"SELECT * FROM dummy_table\"\n\n    trino_fallback_sql = TrinoExecuteQueryOperator(task_id=\"trino_task10\", sql=fallback_sql, fallback_to_sql=\"sql_error\")\n    sql_error = \"SQL ERROR\"\n    # [END howto_operator_trino_fallback]\n\n    # [START sample_trino_short_circuit]]\n    sql9 = \"SELECT * FROM dummy_table WHERE x > 10;\"\n\n    trino_sql9 = TrinoExecuteQueryOperator(task_id=\"trino_task9\", sql=sql9, timeout_seconds=1)\n    # [END sample_trino_short_circuit]]\n\n    # [START sample_trino_logging]]\n    log_message = \"Logging Message\"\n\n    trino_log_msg = TrinoExecuteQueryOperator(task_id=\"trino_task11\", sql=\"SHOW DATABASES;\", logging_message=log_message)\n    # [END sample_trino_logging]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_320_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353508}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated configuration generation and artifact retrieval workflow that uses Jenkins to trigger a job for creating configuration files and then downloads the generated artifact. The workflow should demonstrate inter-system communication and automated artifact management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using jenkins_operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.jenkins.hooks.jenkins import JenkinsHook\nfrom airflow.providers.jenkins.operators.jenkins_job_trigger import JenkinsJobTriggerOperator\n\nJENKINS_URL = os.environ.get(\"JENKINS_URL\", \"http://jenkins_url\")\nTRIGGER_JOB_NAME = os.environ.get(\"TRIGGER_JOB_NAME\", \"job_name\")\nJENKINS_API_TOKEN = os.environ.get(\"JENKINS_API_TOKEN\")\n\n\n@task\ndef create_jenkins_hook():\n    \"\"\"\n    Create JENKINS_URL and set JENKINS_API_TOKEN in addition\n    \"\"\"\n    hook = JenkinsHook()\n    hook.url = JENKINS_URL\n    hook.api_token = JENKINS_API_TOKEN\n    hook.check_for_updates()\n\n\nwith DAG(\n    \"example_jenkins_operator\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n):\n    # [START howto_operator_jenkins_job_trigger]\n    trigger_build = JenkinsJobTriggerOperator(task_id=\"trigger_build\", job_name=TRIGGER_JOB_NAME)\n    # [END howto_operator_jenkins_job_trigger]\n\n    create_jenkins_hook()"}], "metadata": {"file_name": "dag_960_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353553}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that orchestrates a complex data transformation workflow using Google Dataprep, showcasing task dependencies, connection management, and cloud resource provisioning. Implement a systematic approach to creating, running, and monitoring data processing jobs across different cloud services.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Dataprep service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.dataprep import CloudDataprepHook\nfrom airflow.providers.google.cloud.operators.dataprep import (\n    DataprepCopyFlowOperator,\n    DataprepCreateFlowOperator,\n    DataprepDeleteFlowOperator,\n    DataprepGetFlowOperator,\n    DataprepListFlowsOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDAG_ID = \"example_dataprep\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nBUCKET_URI = f\"gs://{BUCKET_NAME}\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dataprep\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_dataprep_create_flow]\n    create_flow = DataprepCreateFlowOperator(\n        task_id=\"create_flow\",\n        project_id=PROJECT_ID,\n        flow_name=\"dataprep_test_flow\",\n        bucket_input={\"uri\": BUCKET_URI},\n        is_wait=True,\n    )\n    # [END howto_operator_dataprep_create_flow]\n\n    copy_flow = DataprepCopyFlowOperator(\n        task_id=\"copy_flow\",\n        destination_flow_id=\"copied_flow\",\n        source_flow_id=create_flow.id,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_operator_dataprep_get_flow]\n    get_flow = DataprepGetFlowOperator(task_id=\"get_flow\", flow_id=create_flow.output[\"flow_id\"])\n    # [END howto_operator_dataprep_get_flow]\n\n    # [START howto_operator_dataprep_list_flows]\n    list_flows = DataprepListFlowsOperator(task_id=\"list_flows\", project_id=PROJECT_ID)\n    # [END howto_operator_dataprep_list_flows]\n\n    delete_flow = DataprepDeleteFlowOperator(\n        task_id=\"delete_flow\",\n        flow_id=create_flow.output[\"flow_id\"],\n        project_id=PROJECT_ID,\n        is_wait=True,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (create_bucket >> create_flow >> copy_flow >> get_flow >> list_flows >> delete_flow >> delete_bucket)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule of TriggerRule.ALL_DONE is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1619_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353622}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a monitoring and alerting workflow for Google Cloud infrastructure that allows creating, managing, and configuring alert policies and notification channels. Build a system that can dynamically enable, disable, and track infrastructure health alerts across different Google Cloud resources.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Stackdriver service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"stackdriver\"\nLOCATION = \"europe-west1\"\n\n# [START howto_stackdriver_service_acc]\nSERVICE_ACCOUNT = \"projects/123456789012/projects/-/serviceAccounts/some-service-account-id@goog computing.iam.gserviceaccount.com\"\n# [END howto_stackdriver_service_acc]\n\n# [START howto_stackdriver_service_path]\nPATH_TO_SERVICE = (\n    f\"organizations/{PROJECT_ID}/locations/{LOCATION}/services/analyticsReportUploadService\"\n)\n# [END howto_stackdriver_service_path]\n\n\nwith DAG(\n    project_id=PROJECT_ID,\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def create_service(sleep: int = 10):\n        import time\n\n        print(f\"Sleeping {sleep} seconds...\")\n        time.sleep(sleep)\n\n    create_service_task = create_service()\n\n    # [START howto_stackdriver_create_notification_policy]\n    create_notification_policy = {\n        \"description\": \"A description for the notification policy.\",\n        \"enabled\": True,\n        \"displayName\": \"google-cloud-compute\",\n        \"types\": [\"errors\", \"troubleshooting\"],\n        \"keys\": [\"usage\"],  # Consider specifying other keys as per your need.\n        \"storageConfig\": {\n            \"disk\": {\n                \"path\": \"/volume1/vm/disc\",\n            },\n        },\n    }\n\n    create_notification_policy_task = create_service()(create_service_path_=PATH_TO_SERVICE, body=create_notification_policy)\n\n    # [END howto_stackdriver_create_notification_policy]\n\n    # [START howto_stackdriver_insert_alert_policy_config]\n    insert_alert_policy_config = {\n        \"thresholdAlertPolicyConfiguration\": {\n            \"quotas\": [\n                {\"percent\": 50, \"metric\": \"org.analytical.users\"},\n            ],\n            \"combiner\": \"TRUNCATE\",\n            \"displayName\": \"Alot of users logging in!\",\n            \"enabled\": True,\n            \"maxSeverityCount\": 0,\n            \"priority\": 2,\n            \"quantity\": 30,\n            \"triggerGranularity\": \"YEAR\",\n        }\n    }\n\n    insert_alert_policy_config_task = create_service()(body=insert_alert_policy_config=insert_alert_policy_config)\n    # [END howto_stackdriver_insert_alert_policy_config]\n\n    # [START howto_stackdriver_list_alert_policies]\n    list_alert_policies = create_service() >> system-tests.utils.listings(dag=dag, listing_obj=\"alertPolicies\")\n    # [END howto_stackdriver_list_alert_policies]\n\n    # [START howto_stackdriver_get_alert_policy]\n    get_alert_policy = system-tests.utils.get_listing_item(\n        listing_obj=\"alertPolicies\", path=\"/alerts/Alot%20of%20users%20logging%20in!\", dag=dag\n    )\n    # [END howto_stackdriver_get_alert_policy]\n\n    # [START howto_stackdriver_update_alert_policy]\n    update_alert_policy = {\n        \"displayName\": \"Better description\",\n        \"description\": \"This is a new description\",\n        \"enabled\": False,\n    }\n\n    system-tests.utils.update_listing_item(\n        listing_obj=\"alertPolicies\", path=\"/alerts/Alot_of_users_logging_in!\", body=update_alert_policy, dag=dag\n    )\n    # [END howto_stackdriver_update_alert_policy]\n\n    # [START howto_stackdriver_delete_alert_policy]\n    delete_alert_policy = system-tests.utils.delete_listing_item(\n        listing_obj=\"alertPolicies\", path=\"/alerts/Alot_of_users_logging_in!\", dag=dag\n    )\n    # [END howto_stackdriver_delete_alert_policy]\n\n    # [START howto_stackdriver_service_delete]\n    service_del = system-tests.utils.service_delete(tos=_SERVICE_PATH_, dag=dag)\n    # [END howto_stackdriver_service_delete]\n\n    create_service_task >> create_notification_policy_task >> insert_alert_policy_config_task >> list_alert_policies >> get_alert_policy >> update_alert_policy >> delete_alert_policy >> service_del\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1287_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353727}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data pipeline that illustrates how to interact with Databricks SQL environments, performing table creation, data insertion, transformation, file-based export, and cross-storage data import using Airflow operators.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using SparkSqlOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\nfrom airflow.providers.databricks.sensors.databricks import DatabricksSensor\n\n# [START howto_operator_databricks]\n# dag_id should match what you name your DAG\ndag = DAG(\n    \"example_databricks_spark_sql\",\n    start_date=datetime(2022, 1, 1),\n    default_args={\"spark_sql_database_name\": \"test_db\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n)\n\n# [END howto_operator_databricks]\n\nwith dag:\n    # [START howto_databricks_sensor]\n    # Note: The trigger_rule of DatabricksSensor is set to \"all_done\", #\n    # this example DAG has one sensor task so no need for the rule.\n    check_sql = DatabricksSensor(\n        databricks_conn_id=\"databricks_default\",\n        sql_task_id=\"check_sql\",\n        split_into_chunks=False,\n    )\n\n    # [END howto_databricks_sensor]\n\n    # [START howto_databricks_sensor_wait_for_good]\n    wait_for_good = DatabricksSensor(\n        databricks_conn_id=\"databricks_default\",\n        sql_task_id=\"wait_for_good\",\n        timeout=5,\n        split_into_chunks=False,\n        poke_interval=3,\n    )\n    # Define the threshold for when the task is considered done.\n    wait_for_good.trigger_rule = \"all_done\"\n    # [END howto_databricks_sensor_wait_for_good]\n\n    # [START howto_databricks_submit]\n    run_this = DatabricksSubmitRunOperator(\n        task_id=\"run_this\",\n        databricks_conn_id=\"databricks_default\",\n        sql_task_id=\"run_proc\",\n        package_layers=\"s3://alpha/databricks/spark-jars/postgresql-4.2.1.jar\",\n        bundle_tags={\n            \"my-tag\": \"my-repo\",\n        },\n    )\n    # [END howto_databricks_submit]\n\n    check_sql >> run_this >> wait_for_good\n\n\nfrom tests_common.test_utils.system_tests import get_test_run_from_test_group, TestGroup\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_group = TestGroup()\ntest_run = get_test_run_from_test_group(test_group)"}], "metadata": {"file_name": "dag_1558_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353786}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an observability pipeline that showcases system interaction tasks, including date printing, sleep operations, and containerized command execution with clear task interdependencies.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\n\n\n@task(task_id=\"test_singularity\")\ndef test_singularity():\n    print(\"Hello World! (Singularity service)\")\n\n\nwith DAG(\n    dag_id=\"example Singularity\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n):\n    # [START howto_operator_singularity]\n    test_singularity()\n    # [END howto_operator_singularity]\n\n    # [START howto_task_group_singularity]\n    with TaskGroup(group_id=\"singularity\") as singularity_group:\n        # [END howto_task_group_singularity]\n\n        test_singularity()\n        test_singularity()\n\n    # [END howto_task_group_singularity]"}], "metadata": {"file_name": "dag_999_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3353827}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS SQS message queue workflow that demonstrates publishing and consuming messages across multiple tasks. Create a DAG that showcases how to interact with Amazon SQS by sending messages, reading messages in single and batch modes, and managing queue lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom airflow.providers.amazon.aws.operators.sqs import (\n    SqsCreateQueueOperator,\n    SqsDeleteQueueOperator,\n    SqsReceiveMessageOperator,\n    SqsSendMessageOperator,\n    SqsSendMessageBatchOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sqs\"\n\n# [START howto_operator_sqs]\n# [START howto_operator_sqs_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_def]\n\n    send_message_task1 = SqsSendMessageOperator(\n        task_id=\"send_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n    )\n\n    send_message_task2 = SqsSendMessageOperator(\n        task_id=\"send_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task2 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n    )\n\n    delete_queue_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_task1\", queue_url=SQS_QUEUE_URL)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n\n\n# [END howto_operator_sqs]\n\n\n# [START howto_operator_sqs_batch]\n# [START howto_operator_sqs_batch_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_batch_def]\n\n    send_message_batch_task1 = SqsSendMessageBatchOperator(\n        task_id=\"send_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        messages=[\n            {\"MessageId\": \"1234567890\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 1})},\n            {\"MessageId\": \"1234567900\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 2})},\n        ],\n        delay_seconds=60,\n    )\n\n    receive_message_batch_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        max_messages=2,\n        wait_time=5,\n    )\n\n    delete_queue_batch_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_batch_task1\", queue_url=SQS_QUEUE_URL)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_batch]\n\n\n# [START howto_operator_sqs_with_env_variables]\n# [START howto_operator_sqs_with_env_variables_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_with_env_variables_def]\n\n    # Example of using environment variables to pass queue URL between operators.\n    generate_name = SqsCreateQueueOperator(\n        task_id=\"generate_name\",\n        queue_attributes={\n            \"DelayPolicyEnabled\": \"true\",\n            \"MessageRetentionPeriod\": \"300\",\n        },\n    )\n\n    get_id = BashOperator(\n        task_id=\"get_id\",\n        bash_command=f\"sqsfolder={generate_name.output};echo {generate_name.output}|awk -F'-' '{print $3}' | awk -F'/' '{print $2}'\",\n    )\n\n    send_message_task1 = SqsSendMessageOperator(\n        task_id=\"send_message_task1\",\n        queue_url=\"{{ sqsfolder }}-queue-url\",\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task1\",\n        queue_url=\"{{ sqsfolder }}-queue-url\",\n        wait_time=5,\n    )\n\n    send_message_task2 = SqsSendMessageOperator(\n        task_id=\"send_message_task2\",\n        queue_url=\"{{ sqsfolder }}-queue-url\",\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task2 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task2\",\n        queue_url=\"{{ sqsfolder }}-queue-url\",\n        wait_time=5,\n    )\n\n    delete_queue_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_task1\", queue_url=\"{{ sqsfolder }}-queue-url\")\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_with_env_variables]\n\n\n# [START howto_operator_sqs_with_connection_id]\n# [START howto_operator_sqs_with_connection_id_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_with_connection_id_def]\n\n    send_message_task1 = SqsSendMessageOperator(\n        task_id=\"send_message_task1\",\n        queue_url=SQS_QUEUE_URL_2,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task1\",\n        queue_url=SQS_QUEUE_URL_2,\n        wait_time=5,\n    )\n\n    send_message_task2 = SqsSendMessageOperator(\n        task_id=\"send_message_task2\",\n        queue_url=SQS_QUEUE_URL_2,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task2 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task2\",\n        queue_url=SQS_QUEUE_URL_2,\n        wait_time=5,\n    )\n\n    delete_queue_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_task1\", queue_url=SQS_QUEUE_URL_2)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_with_connection_id]\n\n\n# [START howto_operator_sqs_multiple_connections]\n# [START howto_operator_sqs_multiple_connections_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_multiple_connections_def]\n\n    send_message_task1 = SqsSendMessageOperator(\n        task_id=\"send_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n    )\n\n    send_message_task2 = SqsSendMessageOperator(\n        task_id=\"send_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n    )\n\n    receive_message_task2 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n    )\n\n    send_message_batch_task1 = SqsSendMessageBatchOperator(\n        task_id=\"send_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        messages=[\n            {\"MessageId\": \"1234567890\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 1})},\n            {\"MessageId\": \"1234567900\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 2})},\n        ],\n        delay_seconds=60,\n    )\n\n    receive_message_batch_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        max_messages=2,\n        wait_time=5,\n    )\n\n    delete_queue_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_task1\", queue_url=SQS_QUEUE_URL)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_multiple_connections]\n\n\n# [START howto_operator_sqs_multiple_connections_result]\n# [START howto_operator_sqs_multiple_connections_result_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_multiple_connections_result_def]\n\n    send_message_task1 = SqsSendMessageOperator(\n        task_id=\"send_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n        result_task_ids=[receive_message_task1.task_id],\n    )\n\n    receive_message_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task1\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n        result_task_ids=[send_message_task1.task_id],\n    )\n\n    send_message_task2 = SqsSendMessageOperator(\n        task_id=\"send_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        message_content=json.dumps({\"shoe\": 1}),\n        delay_seconds=60,\n        result_task_ids=[receive_message_task1.task_id],\n    )\n\n    receive_message_task2 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_task2\",\n        queue_url=SQS_QUEUE_URL,\n        wait_time=5,\n        result_task_ids=[send_message_task2.task_id],\n    )\n\n    delete_queue_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_task1\", queue_url=SQS_QUEUE_URL)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_multiple_connections_result]\n\n\n# [START howto_operator_sqs_batch_result]\n# [START howto_operator_sqs_batch_result_def]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) is:\n    # [END howto_operator_sqs_batch_result_def]\n\n    send_message_batch_task1 = SqsSendMessageBatchOperator(\n        task_id=\"send_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        messages=[\n            {\"MessageId\": \"1234567890\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 1})},\n            {\"MessageId\": \"1234567900\", \"DeliveryMode\": \"PERSISTENT\", \"MessageContent\": json.dumps({\"shoe\": 2})},\n        ],\n        delay_seconds=60,\n        result_task_ids=[receive_message_batch_task1.task_id],\n    )\n\n    receive_message_batch_task1 = SqsReceiveMessageOperator(\n        task_id=\"receive_message_batch_task1\",\n        queue_url=SQS_QUEUE_URL,\n        max_messages=2,\n        wait_time=5,\n        result_task_ids=[send_message_batch_task1.task_id],\n    )\n\n    delete_queue_batch_task1 = SqsDeleteQueueOperator(task_id=\"delete_queue_batch_task1\", queue_url=SQS_QUEUE_URL)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_connection():\n        conn = Connection(\n            conn_id=\"sqs_example\",\n            user=\"email@example.com\",\n            password=\"password\",\n        )\n        airflow.db.merge_conn(conn)\n\n    remove_connection() is\n# [END howto_operator_sqs_batch_result]"}], "metadata": {"file_name": "dag_1133_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354065}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow workflow that illustrates advanced scheduling mechanisms using dataset dependencies, showcasing how tasks can be triggered based on dataset updates, time schedules, and complex logical conditions. Develop a series of DAGs that demonstrate dataset-based scheduling with various dependency scenarios.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG for demonstrating the usage of DatasetTriggerDagRunOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.www.settings import Session\nfrom airflow.datasets import Dataset\n\ndatasets = [\n    # Dataset objects are required before they exist in the DB,\n    # which is not the case for the examples below.\n    Dataset(\"example_dataset_1\"),\n    Dataset(\"example_dataset_2\"),\n]\n\n\nwith DAG(\n    \"dataset-trigger-example\",\n    schedule=[d for d in datasets if d != \"example_dataset_1\"],\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"run_this_first\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n\n    for dataset in datasets:\n        EmptyOperator(task_id=dataset, trigger_rule=TriggerRule.ALL_DONE)\n\n    run_this_first >> datasets[1]  # pragma: no cover\n    run_this_first >> datasets[0]  # pragma: no cover\n\n\n# Junction task to refresh datasetrefs at external dataset source location\nj_refresh_datasetrefs = EmptyOperator(task_id=\"refresh_datasetrefs\", trigger_rule=TriggerRule.ALL_DONE)\n\nfor dataset in datasets:\n    Session.expire_objects(Dataset.get_changes(dataset))\n    Dataset.refresh(dataset)\n\n# [START howto_trigger_dataset]\nj_trigger_dataset_run = DatasetTriggerDagRunOperator(\n    task_id=\"trigger_dataset_run\",\n    dataset=\"example_dataset_1\",\n    wait_for_completion=True,\n)\n# [END howto_trigger_dataset]\n\nj_trigger_dataset_run >> j_refresh_datasetrefs\n\n# [START howto_trigger_multiple_datasets]\nj_trigger_multiple_datasets = DatasetTriggerDagRunOperator(\n    task_id=\"trigger_multiple_datasets\",\n    datsets=[\"example_dataset_1\", \"example_dataset_2\"],\n    wait_for_completion=True,\n)\n# [END howto_trigger_multiple_datasets]\n\nj_trigger_multiple_datasets >> j_refresh_datasetrefs\n\nwith dag:\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_671_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335412}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop a data pipeline that filters and exports specific documents from a MongoDB collection to an S3 storage system using predefined query criteria. Construct a workflow that selectively moves data based on a status condition, ensuring only relevant documents are transferred between different data storage platforms.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.mongo_to_s3 import MongoToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_mongo_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-mongo-to-s3-bucket\"\n    s3_key = f\"{env_id}-mongo-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_mongo_to_s3]\n    mongo_to_s3_job = MongoToS3Operator(\n        task_id=\"mongo_to_s3_job\",\n        mongo_conn_string=\"mongodb://mongo-connect:$MONGO_PORT/mongo_db\",\n        target_s3_key=s3_key,\n        mongo_query={\"status\": \"completed\"},\n    )\n    # [END howto_transfer_mongo_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(create_s3_bucket, mongo_to_s3_job, delete_s3_bucket)\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1910_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335417}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end data engineering workflow that leverages Google Cloud Build for automated software deployment and infrastructure provisioning. Create a scalable DAG that demonstrates dynamic trigger creation, project number retrieval, and cloud resource orchestration.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that displays interactions with Google Cloud Source Repository.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.source_repos import (\n    GitHubSourceHook,\n    GoogleCloudSourceRepoHook,\n)\nfrom airflow.providers.google.cloud.operators.source_repos import copy_source_repo_branch_to_bucket\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nGCP_SOURCE_REPOSITORY_ID = f\"repo-gcr-{ENV_ID}\"\nDAG_ID = \"gcp_source_repo_copy_branch_to_bucket\"\nGCP_SOURCE_REPOSITORY_URL = \"https://github.com/thoughbot repo-grr\"\n\n\n@task\ndef create_gcp_source_repository():\n    \"\"\"\n    Creates a GCP Source Repository at the default location (US).\n    \"\"\"\n    service = build(\"source-repo\", \"v1\")\n    repository = {\n        \"name\": f\"projects/{PROJECT_ID}/repos/{GCP_SOURCE_REPOSITORY_ID}\",\n        \"displayName\": GCP_SOURCE_REPOSITORY_ID,\n        \"config\": {\"storageType\": \"GCS\"},\n    }\n    create_repository_request = service.repos().insert(body=repository)\n    operation = create_repository_request.execute()\n    print(f\"Repository created: {operation['name']}\")\n\n\n@task\ndef get_latest_commit_hash_from_gcp_source_repository():\n    \"\"\"\n    Returns latest commit hash from the master branch of the GCP source repository.\n    \"\"\"\n    service = build(\"source-repo\", \"v1\")\n    request = service.repos().branches() \\\n        .list(repoId=f\"projects/{PROJECT_ID}/repos/{GCP_SOURCE_REPOSITORY_ID}\")\n    request = request.fields(\"commit.commitId,commit.commitSha\")\n    response = request.execute()\n    return response[\"branches\"][0][\"commit\"][\"commitSha\"]\n\n\nSOURCE_REPOSITORY_COPY_BUCKET_NAME = f\"bucket-gcs-{ENV_ID}\"\nSOURCE_REPOSITORY_COPY_PATH = \"src/main/python/my_service.py\"\nGCP_SOURCE_REPOSITORY_BRANCH_NAME = \"master\"\nDESTINATION_BUCKET_FILE_PATH = \"dest/src/main/python/my_service.py\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gcp_create_source_repo]\n    create_gcp_source_repository = create_gcp_source_repository()\n    # [END howto_operator_gcp_create_source_repo]\n\n    @task\n    def copy_to_bucket():\n        \"\"\"\n        Copy the file at SOURCE_REPOSITORY_COPY_PATH to SOURCE_REPOSITORY_COPY_BUCKET_NAME\n        \"\"\"\n        service = build(\"source-repo\", \"v1\")\n        copy_request_body = {\n            \"sourceRepoBranch\": GCP_SOURCE_REPOSITORY_BRANCH_NAME,\n            \"destinationRepo\": {\n                \"projectId\": PROJECT_ID,\n                \"repoId\": SOURCE_REPOSITORY_COPY_BUCKET_NAME,\n            },\n            \"destinationFilePath\": DESTINATION_BUCKET_FILE_PATH,\n        }\n        operation = service.repos().copy(sourceRepo=SOURCE_REPOSITORY_COPY_PATH, body=copy_request_body)\n        operation = operation.execute()\n        print(f\"Copied repo operation id: {operation['name']}\")\n\n    # [START howto_operator_source_repo_copy_branch_to_bucket]\n    copy_repo_path_to_bucket = copy_source_repo_branch_to_bucket(\n        gcp_source_repo_id=GCP_SOURCE_REPOSITORY_ID,\n        gcp_source_repo_branch_name=GCP_SOURCE_REPOSITORY_BRANCH_NAME,\n        source_repo_copy_bucket_name=SOURCE_REPOSITORY_COPY_BUCKET_NAME,\n        source_repo_copy_path=SOURCE_REPOSITORY_COPY_PATH,\n        destination_bucket_file_path=DESTINATION_BUCKET_FILE_PATH,\n    )\n    # [END howto_operator_source_repo_copy_branch_to_bucket]\n\n    delete_repo = GoogleCloudSourceRepoHook() >> GitHubSourceHook()\n\n    (\n        # TEST SETUP\n        create_gcp_source_repository\n        >> get_latest_commit_hash_from_gcp_source_repository\n        >> copy_to_bucket()\n        # TEST BODY\n        >> copy_repo_path_to_bucket\n        # TEST TEARDOWN\n        >> delete_repo()\n        # RELAY\n        >> TriggerRule.ALL_DONE\n    )\n\n    # ### Everything below this line is not part of test ###\n    # Below are sample tasks used to test the DAG above.\n    # Note that these sample tasks will fail the test if updated\n    # Also please be sure to set correct args for the tasks below\n\n    # create_repo_path_to_bucket_task = GoogleCloudSourceRepoHook() >> CopySingleFileToBucketOperator(\n    #     task_id=\"create_repo_path_to_bucket\",\n    #     repo_id=GCP_SOURCE_REPOSITORY_ID,\n    #     path=SOURCE_REPOSITORY_COPY_PATH,\n    #     bucket_name=SOURCE_REPOSITORY_COPY_BUCKET_NAME,\n   #     filename=SOURCE_REPOSITORY_COPY_PATH,\n    # )\n\n    # get_repo_id = AzureBlobStorageHook() >> ADFWrapperAeycOperator(\n    #     task_id=\"get_repo_id\",\n    #     path='configs',\n    #     fragment=\"*\",\n    #     sql=(\"select distinct repo_id from airbyte_dag_metadata where path='configs'\"),\n    #     output_column=\"repo_id\",\n    # )\n\n    # remove_repo_path_from_bucket_task = AzureBlobStorageHook() >> ADFWrapperAeycOperator(\n    #     task_id=\"remove_repo_path_from_bucket\",\n    #     path='configs',\n    #     fragment=\"*\",\n    #     sql=(\"delete from airbyte_dag_metadata where repo_id=%s;\" , (get_repo_id,)),\n    # )\n\n    # If you need to run multiple tasks to update one asset in Source Repos, we recommend using BatchUpdateAssetsOperator\n    # batch_update_assets_task = BatchUpdateAssetsOperator(\n    #     task_id=\"batch_update_assets\",\n    #     repo_ids=[GCP_SOURCE_REPOSITORY_ID],\n    #     branches=[\"main\"],\n    #     assets=[\n    #         AssetUpdateInfo(\n    #             asset_identifier=f\"{SOURCE_REPOSITORY_COPY_BUCKET_NAME}/{GCP_SOURCE_REPOSITORY_BRANCH_NAME}\",\n    #             operations=[\n    #                 AssetUpdateOperation(update_type=AssetUpdateOperation.UpdateType.REPLACE),  # Replace the asset each time\n    #                 AssetUpdateOperation(\n    #                     update_type=AssetUpdateOperation.UpdateType.ADD,\n    #                     paths=[GCP_SOURCE_REPOSITORY_COPY_PATH],\n    #                     fragments=[GCP_SOURCE_REPOSITORY_COPY_PATH.split(\"/\")[-1]],\n    #                 ),\n    #             ],\n    #         )\n    #     ],\n    # )\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1997_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354332}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data pipeline that demonstrates a basic sensor mechanism for waiting on upstream data readiness before proceeding with downstream tasks. Design a simple workflow that simulates checking for data availability and then executing a subsequent operation.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sql_execute_query\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    conn = Connection(\n        conn_id=\"connection_id\",\n        conn_type=\"sqlite\",\n        description=\"Connection to example database.\",\n        host=\"postgres\",\n        login=\"username\",\n        password=\"password\",\n        port=5432,\n    )\n\n    @task\n    def create_and_populate_table():\n        \"\"\"\n        Example task to create and populate table without any external dependencies.\n        \"\"\"\n        from airflow.providers.common.sql.hooks.sql import SqlHook\n\n        sql_hook = SqlHook(sql_conn_id=\"connection_id\")\n        sql = \"CREATE TABLE test (val INT); INSERT INTO test VALUES (1), (2), (3), (4), (5), (6), (7), (8), (9), (10)\"\n        sql_hook.execute(sql)\n\n    run_the_query = SQLExecuteQueryOperator(\n        task_id=\"run_the_query\",\n        sql=\"SELECT val FROM test WHERE val <= %(val_limit)s\",\n        parameters={\"val_limit\": 5},\n        hook_args={\"sql_conn_id\": \"connection_id\"},\n    )\n\n    create_and_populate_table()\n    run_the_query\n    # [START howto_operator_sql_template]\n    execute_sql_template_task = SQLExecuteQueryOperator(\n        task_id=\"execute_sql_template\",\n        sql_template=\"SELECT val FROM test WHERE val <= {{ params.val_limit }}\",\n        parameters={\"val_limit\": 5},\n        hook_args={\"sql_conn_id\": \"connection_id\"},\n    )\n    # [END howto_operator_sql_template]\n\n    (\n        [conn, create_and_populate_table()]\n        >> dag.children[0]\n        >> dag.children[1]\n        >> dag.children[2]\n        >> dag.children[3]\n        >> dag.children[4]\n        >> dag.children[5]\n        >> dag.children[6]\n        >> dag.children[7]\n        >> dag.children[8]\n        >> dag.children[9]\n        >> dag.children[10]\n        >> dag.children[11]\n        >> dag.children[12]\n        >> dag.children[13]\n        >> dag.children[14]\n        >> dag.children[15]\n        >> dag.children[16]\n        >> dag.children[17]\n        >> dag.children[18]\n        >> dag.children[19]\n        >> dag.children[20]\n        >> dag.children[21]\n        >> dag.children[22]\n        >> dag.children[23]\n        >> dag.children[24]\n        >> dag.children[25]\n        >> dag.children[26]\n        >> dag.children[27]\n        >> dag.children[28]\n        >> dag.children[29]\n        >> dag.children[30]\n        >> dag.children[31]\n        >> dag.children[32]\n        >> dag.children[33]\n        >> dag.children[34]\n        >> dag.children[35]\n        >> dag.children[36]\n        >> dag.children[37]\n        >> dag.children[38]\n        >> dag.children[39]\n        >> dag.children[40]\n        >> dag.children[41]\n        >> dag.children[42]\n        >> dag.children[43]\n        >> dag.children[44]\n        >> dag.children[45]\n        >> dag.children[46]\n        >> dag.children[47]\n        >> dag.children[48]\n        >> dag.children[49]\n        >> dag.children[50]\n        >> dag.children[51]\n        >> dag.children[52]\n        >> dag.children[53]\n        >> dag.children[54]\n        >> dag.children[55]\n        >> dag.children[56]\n        >> dag.children[57]\n        >> dag.children[58]\n        >> dag.children[59]\n        >> dag.children[60]\n        >> dag.children[61]\n        >> dag.children[62]\n        >> dag.children[63]\n        >> dag.children[64]\n        >> dag.children[65]\n        >> dag.children[66]\n        >> dag.children[67]\n        >> dag.children[68]\n        >> dag.children[69]\n        >> dag.children[70]\n        >> dag.children[71]\n        >> dag.children[72]\n        >> dag.children[73]\n        >> dag.children[74]\n        >> dag.children[75]\n        >> dag.children[76]\n        >> dag.children[77]\n        >> dag.children[78]\n        >> dag.children[79]\n        >> dag.children[80]\n        >> dag.children[81]\n        >> dag.children[82]\n        >> dag.children[83]\n        >> dag.children[84]\n        >> dag.children[85]\n        >> dag.children[86]\n        >> dag.children[87]\n        >> dag.children[88]\n        >> dag.children[89]\n        >> dag.children[90]\n        >> dag.children[91]\n        >> dag.children[92]\n        >> dag.children[93]\n        >> dag.children[94]\n        >> dag.children[95]\n        >> dag.children[96]\n        >> dag.children[97]\n        >> dag.children[98]\n        >> dag.children[99]\n        >> dag.children[100]\n        >> dag.children[101]\n        >> dag.children[102]\n        >> dag.children[103]\n        >> dag.children[104]\n        >> dag.children[105]\n        >> dag.children[106]\n        >> dag.children[107]\n        >> dag.children[108]\n        >> dag.children[109]\n        >> dag.children[110]\n        >> dag.children[111]\n        >> dag.children[112]\n        >> dag.children[113]\n        >> dag.children[114]\n        >> dag.children[115]\n        >> dag.children[116]\n        >> dag.children[117]\n        >> dag.children[118]\n        >> dag.children[119]\n        >> dag.children[120]\n        >> dag.children[121]\n        >> dag.children[122]\n        >> dag.children[123]\n        >> dag.children[124]\n        >> dag.children[125]\n        >> dag.children[126]\n        >> dag.children[127]\n        >> dag.children[128]\n        >> dag.children[129]\n        >> dag.children[130]\n        >> dag.children[131]\n        >> dag.children[132]\n        >> dag.children[133]\n        >> dag.children[134]\n        >> dag.children[135]\n        >> dag.children[136]\n        >> dag.children[137]\n        >> dag.children[138]\n        >> dag.children[139]\n        >> dag.children[140]\n        >> dag.children[141]\n        >> dag.children[142]\n        >> dag.children[143]\n        >> dag.children[144]\n        >> dag.children[145]\n        >> dag.children[146]\n        >> dag.children[147]\n        >> dag.children[148]\n        >> dag.children[149]\n        >> dag.children[150]\n        >> dag.children[151]\n        >> dag.children[152]\n        >> dag.children[153]\n        >> dag.children[154]\n        >> dag.children[155]\n        >> dag.children[156]\n        >> dag.children[157]\n        >> dag.children[158]\n        >> dag.children[159]\n        >> dag.children[160]\n        >> dag.children[161]\n        >> dag.children[162]\n        >> dag.children[163]\n        >> dag.children[164]\n        >> dag.children[165]\n        >> dag.children[166]\n        >> dag.children[167]\n        >> dag.children[168]\n        >> dag.children[169]\n        >> dag.children[170]\n        >> dag.children[171]\n        >> dag.children[172]\n        >> dag.children[173]\n        >> dag.children[174]\n        >> dag.children[175]\n        >> dag.children[176]\n        >> dag.children[177]\n        >> dag.children[178]\n        >> dag.children[179]\n        >> dag.children[180]\n        >> dag.children[181]\n        >> dag.children[182]\n        >> dag.children[183]\n        >> dag.children[184]\n        >> dag.children[185]\n        >> dag.children[186]\n        >> dag.children[187]\n        >> dag.children[188]\n        >> dag.children[189]\n        >> dag.children[190]\n        >> dag.children[191]\n        >> dag.children[192]\n        >> dag.children[193]\n        >> dag.children[194]\n        >> dag.children[195]\n        >> dag.children[196]\n        >> dag.children[197]\n        >> dag.children[198]\n        >> dag.children[199]\n        >> dag.children[200]\n        >> dag.children[201]\n        >> dag.children[202]\n        >> dag.children[203]\n        >> dag.children[204]\n        >> dag.children[205]\n        >> dag.children[206]\n        >> dag.children[207]\n        >> dag.children[208]\n        >> dag.children[209]\n        >> dag.children[210]\n        >> dag.children[211]\n        >> dag.children[212]\n        >> dag.children[213]\n        >> dag.children[214]\n        >> dag.children[215]\n        >> dag.children[216]\n        >> dag.children[217]\n        >> dag.children[218]\n        >> dag.children[219]\n        >> dag.children[220]\n        >> dag.children[221]\n        >> dag.children[222]\n        >> dag.children[223]\n        >> dag.children[224]\n        >> dag.children[225]\n        >> dag.children[226]\n        >> dag.children[227]\n        >> dag.children[228]\n        >> dag.children[229]\n        >> dag.children[230]\n        >> dag.children[231]\n        >> dag.children[232]\n        >> dag.children[233]\n        >> dag.children[234]\n        >> dag.children[235]\n        >> dag.children[236]\n        >> dag.children[237]\n        >> dag.children[238]\n        >> dag.children[239]\n        >> dag.children[240]\n        >> dag.children[241]\n        >> dag.children[242]\n        >> dag.children[243]\n        >> dag.children[244]\n        >> dag.children[245]\n        >> dag.children[246]\n        >> dag.children[247]\n        >> dag.children[248]\n        >> dag.children[249]\n        >> dag.children[250]\n        >> dag.children[251]\n        >> dag.children[252]\n        >> dag.children[253]\n        >> dag.children[254]\n        >> dag.children[255]\n        >> dag.children[256]\n        >> dag.children[257]\n        >> dag.children[258]\n        >> dag.children[259]\n        >> dag.children[260]\n        >> dag.children[261]\n        >> dag.children[262]\n        >> dag.children[263]\n        >> dag.children[264]\n        >> dag.children[265]\n        >> dag.children[266]\n        >> dag.children[267]\n        >> dag.children[268]\n        >> dag.children[269]\n        >> dag.children[270]\n        >> dag.children[271]\n        >> dag.children[272]\n        >> dag.children[273]\n        >> dag.children[274]\n        >> dag.children[275]\n        >> dag.children[276]\n        >> dag.children[277]\n        >> dag.children[278]\n        >> dag.children[279]\n        >> dag.children[280]\n        >> dag.children[281]\n        >> dag.children[282]\n        >> dag.children[283]\n        >> dag.children[284]\n        >> dag.children[285]\n        >> dag.children[286]\n        >> dag.children[287]\n        >> dag.children[288]\n        >> dag.children[289]\n        >> dag.children[290]\n        >> dag.children[291]\n        >> dag.children[292]\n        >> dag.children[293]\n        >> dag.children[294]\n        >> dag.children[295]\n        >> dag.children[296]\n        >> dag.children[297]\n        >> dag.children[298]\n        >> dag.children[299]\n        >> dag.children[300]\n        >> dag.children[301]\n        >> dag.children[302]\n        >> dag.children[303]\n        >> dag.children[304]\n        >> dag.children[305]\n        >> dag.children[306]\n        >> dag.children[307]\n        >> dag.children[308]\n        >> dag.children[309]\n        >> dag.children[310]\n        >> dag.children[311]\n        >> dag.children[312]\n        >> dag.children[313]\n        >> dag.children[314]\n        >> dag.children[315]\n        >> dag.children[316]\n        >> dag.children[317]\n        >> dag.children[318]\n        >> dag.children[319]\n        >> dag.children[320]\n        >> dag.children[321]\n        >> dag.children[322]\n        >> dag.children[323]\n        >> dag.children[324]\n        >> dag.children[325]\n        >> dag.children[326]\n        >> dag.children[327]\n        >> dag.children[328]\n        >> dag.children[329]\n        >> dag.children[330]\n        >> dag.children[331]\n        >> dag.children[332]\n        >> dag.children[333]\n        >> dag.children[334]\n        >> dag.children[335]\n        >> dag.children[336]\n        >> dag.children[337]\n        >> dag.children[338]\n        >> dag.children[339]\n        >> dag.children[340]\n        >> dag.children[341]\n        >> dag.children[342]\n        >> dag.children[343]\n        >> dag.children[344]\n        >> dag.children[345]\n        >> dag.children[346]\n        >> dag.children[347]\n        >> dag.children[348]\n        >> dag.children[349]\n        >> dag.children[350]\n        >> dag.children[351]\n        >> dag.children[352]\n        >> dag.children[353]\n        >> dag.children[354]\n        >> dag.children[355]\n        >> dag.children[356]\n        >> dag.children[357]\n        >> dag.children[358]\n        >> dag.children[359]\n        >> dag.children[360]\n        >> dag.children[361]\n        >> dag.children[362]\n        >> dag.children[363]\n        >> dag.children[364]\n        >> dag.children[365]\n        >> dag.children[366]\n        >> dag.children[367]\n        >> dag.children[368]\n        >> dag.children[369]\n        >> dag.children[370]\n        >> dag.children[371]\n        >> dag.children[372]\n        >> dag.children[373]\n        >> dag.children[374]\n        >> dag.children[375]\n        >> dag.children[376]\n        >> dag.children[377]\n        >> dag.children[378]\n        >> dag.children[379]\n        >> dag.children[380]\n        >> dag.children[381]\n        >> dag.children[382]\n        >> dag.children[383]\n        >> dag.children[384]\n        >> dag.children[385]\n        >> dag.children[386]\n        >> dag.children[387]\n        >> dag.children[388]\n        >> dag.children[389]\n        >> dag.children[390]\n        >> dag.children[391]\n        >> dag.children[392]\n        >> dag.children[393]\n        >> dag.children[394]\n        >> dag.children[395]\n        >> dag.children[396]\n        >> dag.children[397]\n        >> dag.children[398]\n        >> dag.children[399]\n        >> dag.children[400]\n        >> dag.children[401]\n        >> dag.children[402]\n        >> dag.children[403]\n        >> dag.children[404]\n        >> dag.children[405]\n        >> dag.children[406]\n        >> dag.children[407]\n        >> dag.children[408]\n        >> dag.children[409]\n        >> dag.children[410]\n        >> dag.children[411]\n        >> dag.children[412]\n        >> dag.children[413]\n        >> dag.children[414]\n        >> dag.children[415]\n        >> dag.children[416]\n        >>"}], "metadata": {"file_name": "dag_19_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354526}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a multi-stage data orchestration process using task dependencies and different operators like BashOperator and HiveOperator. Implement a workflow that shows how data can be transferred between local storage, HDFS, Hive tables, and a MySQL database.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_adf_to_sql\"\n\nconn_id = \"spark- Default\"\n\n# [START howto_operator_adf]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    @task\n    def create_spark_connection():\n        conn = Connection(\n            conn_id=conn_id,\n            conn_type=\"spark\",\n        )\n        conn.set_extra(\"\")\n        session = SparkSession.builder.getOrCreate()\n        session.conf.set(\"spark.sql.warehouse.dir\", \"/tmp/\")\n        conn.bind(session)\n\n    root_task = BashOperator(task_id=\"root_task\", bash_command=\"touch /tmp/airflow/dags/updated_at\")\n\n    # [START howto_operator_adf_trigger]\n    create_spark_task = PythonOperator(\n        task_id=\"create_spark_object\",\n        python_callable=create_spark_connection,\n    )\n    # [END howto_operator_adf_trigger]\n\n    # [START howto_operator_sql]\n    insert_into_hive_table = SQLExecuteQueryOperator(\n        task_id=\"insert_into_hive_table\",\n        sql=\"INSERT INTO TABLE my_table FORMAT JSONLSECTED PARTITION (dt='2020-01-01') \"\n        \"SELECT 'fruit', '{{ds}}' AS dt, 'apple' AS fruit_count\",\n        conn_id=conn_id,\n        split_size=1,\n    )\n    # [END howto_operator_sql]\n\n    select_from_hive = SQLExecuteQueryOperator(\n        task_id=\"select_from_hive\", sql=\"SELECT * FROM my_table WHERE dt='2020-01-01'\"\n    )\n\n    drop_table = BashOperator(task_id=\"drop_table\", bash_command=\"sql -e \\\"drop table if exists hive.my_table\\\"\")\n\n    # [START howto_operator_adf_trigger_post>\n    delete_spark_task = PythonOperator(\n        task_id=\"delete_spark_object\",\n        python_callable=BashOperator(\n            task_id=\"drop_table\",\n            bash_command=\"sql -e \\\"drop table if exists hive.my_table\\\"\",\n        ),\n    )\n    # [END howto_operator_adf_trigger_post]\n\n    root_task >> create_spark_task >> insert_into_hive_table >> select_from_hive >> drop_table\n\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\n    # [START howto_operator_adf_inlet]\n    # Note: Because the trigger rule is set to 'always', this task will always be executed.\n    execute_sql_using_adf = SQLExecuteQueryOperator(\n        task_id=\"execute_sql_using_adf\",\n        sql=\"SELECT * FROM hive.my_table\",\n        conn_id=conn_id,\n        trigger_rule=TriggerRule.ALWAYS,\n    )\n    # [END howto_operator_adf_inlet]\n    execute_sql_using_adf\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)\n\n\n# TEST SETUP\ndef test_setup():\n    # This test setup creates a connection called spark-defaults which is used\n    # by the examples to connect to Spark.  The role, schema, etc. here\n    # aren't important.\n    Connection(\n        conn_id=\"spark-defaults\",\n        conn_type=\"spark\",\n        extra=\"\"\"\n                        spark.sql.warehouse.dir=/tmp/\n                        config.file=/not/a/sha256/hash/hello_world.py\n                        defaultSchema=v4_2\n                        \"\"\".replace(\"\\\\\", \"\"),\n    )\n\n\n# TEST TEARDOWN\ndef test_teardown(test_run):\n    # This test teardown deletes the connection created above\n    Connection(conn_id=\"spark-defaults\").delete()\n\n\n# TEST BODY\ntest_setup()\ntest_teardown()"}], "metadata": {"file_name": "dag_1939_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354628}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow pipeline that simulates a data processing scenario with explicit resource management, showing how to handle cluster initialization, data transformation, and cluster teardown in a single coordinated workflow. Construct a DAG that demonstrates best practices for managing computational resources across different task stages.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"cloud_compute_instance_from_template\"\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nRESOURCE_NAME = \"gce_instance_template-demo-template\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nTEMPLATE_PORT = 22\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"compute\"],\n) as dag:\n    # [START howto_operator_gce_create_instance]\n    create_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"create_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body={\n            \"name\": f\"{RESOURCE_NAME}\",\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": f\"{RESOURCE_NAME}\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"source_image\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20230611\",\n                        \"disk_name\": f\"{RESOURCE_NAME}\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                    \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gce_create_instance]\n\n    # [START howto_operator_gce_delete_instance]\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=RESOURCE_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gce_delete_instance]\n\n    create_instance >> delete_instance\n\n    # ### Everything below this point is a example of overwriting existing templates ###\n    # ### You can skip it, and just run the entire dag for that reason ###\n    #\n\n    # [START howto_operator_gce_from_template]\n    instance_from_template = ComputeEngineInsertInstanceOperator(\n        task_id=\"instance_from_template\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body={\n            \"name\": f\"{RESOURCE_NAME}\",\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\",\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": f\"{RESOURCE_NAME}\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"source_image\": f\"projects/{PROJECT_ID}/global/instanceTemplates/{RESOURCE_NAME}\",\n                        \"disk_name\": f\"{RESOURCE_NAME}\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [\n                        {\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}\n                    ],\n                    \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gce_from_template]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n    test_run.execute(dag)"}], "metadata": {"file_name": "dag_1054_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354743}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that showcases the sequential execution of Kylin cube operations, highlighting task dependencies, time-based data segmentation, and the progression of data transformation steps from build to purge.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_kylin_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_kylin]\n    build_cube = KylinCubeOperator(task_id=\"build_cube\", trigger_rule=\"all_done\")\n    # [END howto_operator_kylin]\n\n    await_build_cube = ShortCircuitOperator(\n        task_id=\"wait_for_build_cube\",\n        python_callable=lambda *args, **kwargs: await_true(),\n        op_kwargs={\"task_id\": build_cube.task_id},\n    )\n\n    build_schwager_cube = KylinCubeOperator(task_id=\"build_schwager_cube\", trigger_rule=\"all_done\")\n    # [END howto_operator_kylin]\n\n    await_build_schwager_cube = ShortCircuitOperator(\n        task_id=\"wait_for_build_schwager_cube\",\n        python_callable=lambda *args, **kwargs: await_true(),\n        op_kwargs={\"task_id\": build_schwager_cube.task_id},\n    )\n\n    switch_cubes = KylinSwitchOperator(task_id=\"switch_cubes\", trigger_rule=\"one_success\")\n    # [END howto_operator_kylin]\n\n    purge_cube = KylinCubeOperator(task_id=\"purge_cube\", trigger_rule=\"all_done\", cube_id=\"purge_me!\")\n    # [END howto_operator_kylin]\n\n    await_purge_cube = ShortCircuitOperator(\n        task_id=\"wait_for_purge_cube\",\n        python_callable=lambda *args, **kwargs: await_true(),\n        op_kwargs={\"task_id\": purge_cube.task_id},\n    )\n\n    # [START howto_operator_kylin_xcom]\n    push_data_to_xcom = KylinPushDataToXComOperator(\n        task_id=\"push_data_to_xcom\",\n        gcs_bucket=\"kylin-system-test-gcs\",\n        input_files=\"kylin-resources/data/schwarzman.csv\",\n        output_key=\"kylin_push_data_xcom_values\",\n    )\n    # [END howto_operator_kylin_xcom]\n\n    # [START howto_operator_kylin_xcom_result]\n    xcom_value_task = KylinTaskOperator(task_id=\"xcom_value_task\", fly_by_product=\"debug\")\n    xcom_result_task = KylinTaskOperator(task_id=\"xcom_result_task\", result_xcom=\"debug\")\n    # [END howto_operator_kylin_xcom_result]\n\n    # [START howto_operator_kylin_dynamic_config]\n    cube_dynamic_config = KylinCubeDynamicConfigOperator(\n        task_id=\"cube_dynamic_config\", cube_id=\"purge_me!\", trigger_rule=\"all_done\"\n    )\n    # [END howto_operator_kylin_dynamic_config]\n\n    # [START howto_operator_kylin_get_cube_status]\n    get_cube_status = KylinGetCubeStatusOperator(task_id=\"get_cube_status\", cube_id=\"purge_me!\")\n    # [END howto_operator_kylin_get_cube_status]\n\n    # [START howto_operator_kylin_list_cubes]\n    list_cubes = KylinListCubesOperator(task_id=\"list_cubes\")\n    # [END howto_operator_kylin_list_cubes]\n\n    # [START howto_operator_kylin_delete_cube]\n    delete_cube = KylinDeleteCubeOperator(task_id=\"delete_cube\", cube_id=\"purge_me!\")\n    # [END howto_operator_kylin_delete_cube]\n\n    # [START howto_operator_kylin_install_plugin]\n    install_plugin = KylinInstallPluginOperator(task_id=\"install-plugin\", plugin_name=\"HiveSinkPlugin\")\n    # [END howto_operator_kylin_install_plugin]\n\n    # [START howto_operator_kylin_load_cube]\n    load_cube = KylinLoadCubeOperator(task_id=\"load_cube\", cube_id=\"purge_me!\")\n    # [END howto_operator_kylin_load_cube]\n\n    # [START howto_operator_kylin_uninstall_plugin]\n    uninstall_plugin = KylinUninstallPluginOperator(task_id=\"uninstall-plugin\", plugin_name=\"HiveSinkPlugin\")\n    # [END howto_operator_kylin_uninstall_plugin]\n\n\ndef await_true(task_id):\n    wait_for_task = ShortCircuitOperator(\n        task_id=\"wait_for_task\",\n        python_callable=lambda *args, **kwargs: True,\n        op_kwargs={\"task_id\": task_id},\n    )\n    return wait_for_task\n\n\n# [START howto_operator_kylin_xcom_hadoop_classic]\ndef test_hadoop_classic(xcom_values):\n    print(xcom_values)\n    return True\n\n\n# [END howto_operator_kylin_xcom_hadoop_classic]\n\n\ndef test_xcom_result(result_xcom_values):\n    print(result_xcom_values)\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    list_dags = ListDagsOperator(dag=dag, watcher=watcher)\n    list_dags.execute()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1535_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354833}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates a sequence of cloud tasks involving bucket creation, audio generation, speech recognition, and bucket cleanup as a systematic demonstration of Google Cloud Speech API interactions. Showcase how different cloud operators can be chained together to create a comprehensive speech processing workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.speech_to_text import CloudSpeechToTextRecognizeSpeechOperator\nfrom airflow.providers.google.cloud.transfers.local_to_gcs import LocalGCSTransferOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcp_speech_to_text\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_operator_speech_to_text_deidentify_content]\nDEIDENTIFICATION_CONFIG = {\n    \"deidentificationConfiguration\": {\n        \"languageModel\": {\"enableAutomaticDeIdentification\": True},\n        \"deidentifyTransformations\": [\n            {\"action\": \"REPLACE\", \"field\": {\"type\": \"TEXT\"}}\n        ]\n    }\n}\n# [END howto_operator_speech_to_text_deidentify_content]\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    local_file_path = \"/tmp/file.txt\"\n\n    upload_file = LocalGCSTransferOperator(\n        task_id=\"upload_file\",\n        source_bucket=BUCKET_NAME,\n        source_object=local_file_path,\n        destination_local_path=local_file_path,\n    )\n\n    # [START howto_operator_speech_to_text_recognize]\n    recognize_speech_task = CloudSpeechToTextRecognizeSpeechOperator(\n        task_id=\"recognize_speech_task\",\n        project_id=PROJECT_ID,\n        input_uri=local_file_path,\n        config=DEIDENTIFICATION_CONFIG,\n    )\n    # [END howto_operator_speech_to_text_recognize]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> upload_file\n        # TEST BODY\n        >> recognize_speech_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1680_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335489}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates the complete lifecycle of data catalog operations, including creating entry groups, registering file entries, and attaching custom metadata tags. Develop a systematic workflow that illustrates how to interact with Google Cloud Data Catalog using Airflow operators.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Data Catalog service testing entry group operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.datacatalog import (\n    CloudDataCatalogCreateEntryGroupOperator,\n    CloudDataCatalogCreateEntryOperator,\n    CloudDataCatalogDeleteEntryGroupOperator,\n    CloudDataCatalogDeleteEntryOperator,\n    CloudDataCatalogListEntriesOperator,\n    CloudDataCatalogUpdateEntryGroupOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"datacatalog_entry_group\"\n\nLOCATION = \"us-central1\"\nENTRY_GROUP_ID = f\"entry-group-{ENV_ID}\"\nENTRY_GROUP = {\n    \"display_name\": ENTRY_GROUP_ID,\n    \"description\": \"Entry group describing entries listed below.\",\n}\nENTRY_ID = f\"entry-{ENV_ID}\"\nENTRY = {\"display_name\": ENTRY_ID, \"type_\": \"FILESET\"}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"datacatalog\", \"entrygroup\"],\n) as dag:\n    # [START howto_operator_gcp_dat_catalog_create_entry_group]\n    create_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group=ENTRY_GROUP,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_dat_catalog_create_entry_group]\n\n    # [START howto_operator_gcp_dat_catalog_delete_entry_group]\n    delete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n        task_id=\"delete_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_dat_catalog_delete_entry_group]\n    delete_entry_group.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_dat_catalog_update_entry_group]\n    update_entry_group = CloudDataCatalogUpdateEntryGroupOperator(\n        task_id=\"update_entry_group\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        entry_group={\"description\": \"Updated description\"},\n    )\n    # [END howto_operator_gcp_dat_catalog_update_entry_group]\n\n    # [START howto_operator_gcp_dat_catalog_list_entries]\n    list_entries = CloudDataCatalogListEntriesOperator(\n        task_id=\"list_entries\",\n        location=LOCATION,\n        entry_group_id=ENTRY_GROUP_ID,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_gcp_dat_catalog_list_entries]\n\n    create_entry = CloudDataCatalogCreateEntryOperator(\n        task_id=\"create_entry\", location=LOCATION, entry_group_id=ENTRY_GROUP_ID, entry_id=ENTRY_ID, entry=ENTRY, project_id=PROJECT_ID\n    )\n\n    delete_entry = CloudDataCatalogDeleteEntryOperator(\n        task_id=\"delete_entry\", location=LOCATION, entry_group_id=ENTRY_GROUP_ID, entry_id=ENTRY_ID, project_id=PROJECT_ID\n    )\n\n    create_entry.trigger_rule = TriggerRule.ALL_DONE\n\n    create_entry_group.trigger_rule = TriggerRule.ALL_DONE\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule \"all done\" is part of the DAG\n    list(dag.tasks) | watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2135_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3354971}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you help me understand how to solve a system of linear equations using Python? Certainly! To solve a system of linear equations, you can use several methods in Python, including NumPy, which provides a convenient and efficient way to handle such computations."}, {"role": "assistant", "content": "import numpy as np\n\n# Coefficient matrix (A)\nA = np.array([[2, 3], [4, -1]])\n\n# Constants vector (b)\nb = np.array([8, 5])\n\n# Solve the system Ax = b\nsolution = np.linalg.solve(A, b)\n\nprint(\"The solution is:\", solution)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355052}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a complete cloud data transfer pipeline with explicit setup, execution, and teardown stages for Google Calendar data synchronization. Implement a workflow that creates necessary cloud resources, handles authentication, transfers data, and then systematically removes all temporary infrastructure components.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineInsertInstanceOperator,\n)\nfrom airflow.providers.samba.sensors.samba import SambaFileSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nRESOURCE_NAME = f\"instance-{ENV_ID}\"\nSHORT_MACHINE_TYPE = \"n1-standard-1\"\nDAG_ID = \"gcp_compute_copy_data_to_samba\"\nLOCATION_COPY = \"europe-west1-c\"\nNETWORK = \"default\"\n\n# [START howto_operator_gce_args_copy]\nCOPY_INSTANCE = \"instance-copy\"\nCOPY_NETWORK_RANGE = \"network-range-copy\"\n# [END howto_operator_gce_args_copy]\n\nSMB_CONN_ID = \"samba_default\"\nSMB_PATH = \"/mnt/samba\"\n\nINTERNAL_IPS = [\"172.16.17.32\"]\nIP_FAMILY = \"IPv4 only\"\n\n\nwith DAG(\n    DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    # [START howto_operator_gce_create]\n    @task\n    def create_instance(instance_name: str, location: str, network: str):\n        hook = ComputeEngineHook()\n        hook.create_instance(\n            project_id=PROJECT_ID,\n            zone=location,\n            body={\n                    \"name\": instance_name,\n                    \"machine_type\": f\"zones/{location}/machineTypes/{SHORT_MACHINE_TYPE}\",\n                    \"disks\": [\n                        {\n                            \"auto_delete\": True,\n                            \"boot\": True,\n                            \"device_name\": instance_name,\n                            \"initialize_params\": {\n                                \"disk_size_gb\": \"10\",\n                                \"source_image\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20230611\",\n                                \"disk_name\": instance_name,\n                            },\n                        }\n                    ],\n                    \"network_interfaces\": [\n                        {\n                            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                            \"stack_type\": \"IPV4_ONLY\",\n                            \"subnetwork\": f\"regions/{location}/subnetworks/default\",\n                        }\n                    ],\n                },\n            #     retry_policy: RetryPolicy(),\n            )\n\n    # [END howto_operator_gce_create]\n\n    # [START howto_operator_gce_create_result]\n    create_instance_output = create_instance.getInstanceResult()\n    # [END howto_operator_gce_create_result]\n\n    create_copy_instance = ComputeEngineInsertInstanceOperator(\n        task_id=COPY_INSTANCE,\n        project_id=PROJECT_ID,\n        zone=LOCATION_COPY,\n        body={\n            \"name\": COPY_INSTANCE,\n            \"machine_type\": f\"zones/{LOCATION_COPY}/machineTypes/{SHORT_MACHINE_TYPE}\",\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": COPY_INSTANCE,\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"source_image\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20230611\",\n                        \"disk_name\": COPY_INSTANCE,\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                    \"stack_type\": \"IPV4_ONLY\",\n                    \"subnetwork\": f\"regions/{LOCATION_COPY}/subnetworks/default\",\n                }\n            ],\n        },\n    )\n\n    # [START howto_sensor_gce_state]\n    wait_for_ip_ubernetes = SambaFileSensor(\n        task_id=\"wait_for_ip_ubernetes\",\n        smb_conn_id=SMB_CONN_ID,\n        path=SMB_PATH,\n        public_path=\"/tmp\",\n        timeout=60 * 30,\n        poke_interval=5,\n        ignore_files=[\"smb.status\"],\n    )\n    # [END howto_sensor_gce_state]\n\n    # [START howto_operator_gce_delete]\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        project_id=PROJECT_ID,\n        zone=location,\n        resource_id=instance_name,\n    )\n    # [END howto_operator_gce_delete]\n\n    # [START howto_operator_gce_delete_result]\n    delete_instance_output = delete_instance.getInstanceResult()\n    # [END howto_operator_gce_delete_result]\n\n    copy_delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"copy_delete_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION_COPY,\n        resource_id=COPY_INSTANCE,\n    )\n\n    # [START howto_decorator_trigger]\n    trigger_task = lambda: None\n    # [END howto_decorator_trigger]\n\n    (\n        # TEST SETUP\n        create_instance(instance_name, location, network=NETWORK)\n        >> create_copy_instance\n        >> wait_for_ip_ubernetes\n        # TEST BODY\n        >> delete_instance\n        >> copy_delete_instance\n        # TEST TEARDOWN\n        >> trigger_task\n    )\n\n    # Task dependencies created via `XComArgs`:\n    #   create_instance -> getInstanceResult()\n    #   create_copy_instance -> getInstanceResult()\n    #   wait_for_ip_ubernetes -> getInstanceResult()\n    #   delete_instance -> getInstanceResult()\n    #   copy_delete_instance -> getInstanceResult()\n    #   trigger_task\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1248_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335521}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates file transfer from a local temporary storage to an AWS S3 bucket, showcasing how to programmatically create, move, and clean up temporary files in a cloud environment.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_s3\"\n\nPATH_TO_FILE = \"/tmp/example-file.txt\"\nSAMPLE_TEXT = \"This is sample text.\"\n\n\n@task\ndef create_sample_file():\n    with open(PATH_TO_FILE, \"w\") as file:\n        file.write(SAMPLE_TEXT)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    test_path = \"test-path\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_test_s3_bucket\", bucket_name=test_path)\n\n    create_sample_file_task = create_sample_file()\n\n    # [START howto_operator_local_to_s3]\n    upload_file = LocalFilesystemToS3Operator(\n        task_id=\"upload_file\",\n        filename=PATH_TO_FILE,\n        dest_key=f\"{test_path}/local_to_s3_file.txt\",\n    )\n    # [END howto_operator_local_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_test_s3_bucket\",\n        bucket_name=test_path,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> create_sample_file_task\n        # TEST BODY\n        >> upload_file\n        # TEST TEARDOWN\n        >> delete_s3_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1957_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.335526}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data migration workflow that copies employee salary data from one BigQuery table to another within the same dataset. Create an Airflow DAG that demonstrates table creation, data transfer, and cleanup for a simple HR data management scenario.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "def test_env_variables():\n    assert PROJECT_ID\n    assert BUCKET"}], "metadata": {"file_name": "dag_816_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355427}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an end-to-end machine learning data pipeline that demonstrates model training, throughput allocation, and model lifecycle management using AWS Bedrock services. Create a systematic approach to preparing, training, and deploying custom AI models with automated monitoring and cleanup.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\nfrom textwrap import dedent\nfrom typing import Any\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.bedrock import (\n    BedrockCreateIngestionOperator,\n    BedrockDeleteIngestionOperator,\n    BedrockIngestDataOperator,\n    BedrockListModelsOperator,\n    BedrockRunIngestionOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.bedrock import BedrockSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests_common.test_utils.watcher import watcher\n\nlog = logging.getLogger(__name__)\n\n\nDAG_ID = \"example_bedrock_aito_model\"\n\n# [START howto_operator_bedrock_aito_model_create_ingestion]\ndef _create_aito_model_from_bucket(bucket_name: str):\n    \"\"\"Helper function to create AIToModel ingestion input data using S3 bucket\"\"\"\n    model_id = \"72c3d1ca-f58a-48f9-a...-6887eaa48b59\"\n    kinesis_stream_arn = f\"arn:aws:kinesis:{ boto3.client(\"ec2\").describe_bucket(AccessControl=\"private\")['Bucket']['Owner']['AWSAccount'] }:stream/{bucket_name}\"\n    input_data = [\n        {\"record\": json.dumps({\"model_id\": model_id, \"kinesis_stream_arn\": kinesis_stream_arn})}\n    ]\n\n    with NamedTemporaryFile(mode=\"w+\") as file:\n        # We need to formulate payload in such a way that it's base64 encoded and split into chunks of 65 bytes each.\n        # The reason for this is that at most an AIToModel Ingestion Operator can produce one payload per chunk of 65\n        # bytes each, and smaller payloads might get grouped into the next payload when decoded back to string.\n        file.write(json.dumps(input_data))\n        file.seek(0)\n        command = f\"python -m amazon.aws.bedrock.aito_ingestion.input_data --bucket {bucket_name} --file \"{file.name}\"'\n        return command\n\n\n# [END howto_operator_bedrock_aito_model_create_ingestion]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_model(model_id: str):\n    log.info(\"Deleting the model without any data ingress.\")\n    hook = BedrockHook()\n    response = hook.delete_model(\n        ModelId=model_id,\n    )\n    log.info(response)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_bucket = \"test-bedrock-ingest-data-bucket\"\n    s3_conn_access_role_arn = \"test-s3-conn-access-role-arn\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=test_bucket)\n\n    # [START howto_operator_bedrock_aito_model_ ingest_data_using_python_script]\n    ingest_wav_data_using_python_script = BedrockIngestDataOperator(\n        task_id=\"ingest-wav-data-using-python-script\",\n        model_arn=\"-\",\n        input_data=_create_aito_model_from_bucket(test_bucket),\n        input_mode=\"POLL\",\n        poke_interval=10,\n    )\n    # [END howto_operator_bedrock_aito_model_ ingest_data_using_python_script]\n\n    # [START howto_sensor_bedrock_aito_model_ wait_for_imentation_complete ]\n    wait_aito_model_creation_complete = BedrockSensor(\n        task_id=\"wait-aito-model creation-complete\",\n        model_arn=\"-\",\n        target_status=\"IN_PROGRESS\",\n        poke_interval=10,\n        max_attempts=10,\n    )\n    # [END howto_sensor_bedrock_aito_model_ wait_for_imentation_complete ]\n\n    # [START howto_operator_bedrock_aito_model_run_ingestion]\n    run_aito_model_ingestion = BedrockRunIngestionOperator(\n        task_id=\"run-aito-model-ingestion\",\n        model_arn=\"-\",\n        input_data=\"{{ task_instance.xcom_pull('ingest_wav_data_using_python_script', key='output') }}\",\n        input_mode=\"POLL\",\n        poke_interval=10,\n    )\n    # [END howto_operator_bedrock_aito_model_run_ingestion]\n\n    # [START howto_operator_bedrock_aito_model_create_model]\n    create_model = BedrockCreateIngestionOperator(\n        task_id=\"create_model\",\n        model_name=\"Test Person Reognition Model\",\n        model_type=\"BEDROOM_MULTIMODAL_MODEL\",\n        output_data_uri=\"s3://test-bedrock-ingest-data-bucket/output/\",\n        source_ingredients=[\"S3ForPersonDetectionStreamingInput\"],  # Enforces exactly one ingredient.\n        target_ingredient_policy=\"ALLOW_ALL\",\n        roles=[s3_conn_access_role_arn],\n        model_arn=\"-\",\n    )\n    # [END howto_operator_bedrock_aito_model_create_model]\n\n    # [START howto_operator_bedrock_aito_model_delete_model]\n    delete_model(create_model.output[\"model\"][\"modelArn\"])\n    # [END howto_operator_bedrock_aito_model_delete_model]\n\n    # [START howto_operator_bedrock_aito_model_list_models ]\n    list_models = BedrockListModelsOperator(task_id=\"list_models\", expand_outputs=True)\n    # [END howto_operator_bedrock_aito_model_list_models ]\n\n    chain(\n        # TEST SETUP\n        create_s3_bucket,\n        # TEST BODY\n        wait_aito_model_creation_complete,\n        run_aito_model_ingestion,\n        create_model,\n        list_models,\n        # TEST TEARDOWN\n        delete_model(create_model.output[\"model\"][\"modelArn\"]),\n        delete_bucket=test_bucket,\n        watcher(),\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1075_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355565}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates setup and teardown tasks in Airflow, showing how to manage resource allocation and cleanup for a multi-stage data processing pipeline. Create a DAG that includes preparatory tasks, main work tasks, and cleanup tasks to simulate a complete operational workflow.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.singularity.operators.singularity import SingularityOperator\n\n\n@task(task_id=\"test_singularity\")\ndef test_singularity():\n    print(\"Everything is awesome!\n\n\nwith DAG(\n    dag_id=\"example_singularity\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    test_singularity()\n\n    # [START howto_operator_singularity]\n    singularity_task = SingularityOperator(\n        task_id=\"test_singularity_with_env_vars\",\n        image=\"/usr/bin/singularity\",\n        command=[\"-i\", \"/usr/bin/testsh\", \"echo\", \"Everything is awesome!\"],\n        environment={\"XYZ\": \"123\"},\n        mounts=[(\"/usr/bin/\", \"/usr/bin/\")],\n        strict_mode=True,\n    )\n    # [END howto_operator_singularity]\n\n    singularity_task >> test_singularity()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1054_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355613}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I handle the issue of receiving a 500 Internal Server Error when attempting to delete a record using an API endpoint in Python?"}, {"role": "assistant", "content": "import requests\nfrom requests.exceptions import HTTPError\n\ndef delete_record(api_url, api_key):\n    try:\n        # Construct the full URL including the API key\n        url = f\"{api_url}/delete/record\"\n        headers = {\n            'Authorization': f'Bearer {api_key}'\n        }\n        \n        # Send the DELETE request\n        response = requests.delete(url, headers=headers)\n        \n        # Ensure the response status code is OK\n        response.raise_for_status()\n        \n        print(\"Record deleted successfully.\")\n    \n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error occurred: {err}\")\n\n# Example usage\napi_url = \"https://example.com/api/\"\napi_key = \"your_api_key_here\"\n\ndelete_record(api_url, api_key)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355746}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data engineering pipeline that demonstrates dynamic cluster provisioning and management for big data processing workloads, showing how to dynamically allocate and deallocate computational resources based on processing requirements. The workflow should highlight the flexibility of cloud-based data processing infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport uuid\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.compute import ComputeEngineHook\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"dataproc_update_cluster\"\nPROJECT_ID = uuid.uuid4().hex[:15]\nBUCKET_NAME = f\"bucket-{DAG_ID}-{PROJECT_ID}\"\nLOCATION = \"europe-west1-b\"\nCLUSTER_NAME_1 = \"cluster-1\"\nCLUSTER_NAME_2 = \"cluster-2\"\nREGION = \"europe-west1\"\nZONE = \"europe-west1-c\"\nCLUSTER_CONFIG_1 = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-1\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 32,\n        },\n    },\n}\nUPDATE_MASK = {\"paths\": [\"config\"]}\n# [START howto_dataproc_update_cluster_operator]\nENV_ID = \"{{ env_id() }}\"\nCLUSTER_NAME_1 = f\"{DAG_ID}-{ENV_ID}-cluster-1\"\nCLUSTER_NAME_2 = f\"{DAG_ID}-{ENV_ID}-cluster-2\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_cluster(cluster_name, project_id, region, zone):\n        compute_engine_hook = ComputeEngineHook()\n        instance = compute_engine_hook.create_instance(\n            project_id,\n            region,\n            zone,\n            machine_type=\"n1-standard-1\",\n            disk={\"boot\": {\"sizeGb\": 32, \"initialize_params\": {\"disk_size_gb\": 32}}},\n        )\n\n        compute_engine_hook.insert_command(\n            project_id,\n            region,\n            instance.name,\n            body={\n                \"name\": instance.name,\n                \"config\": {\n                    \"disks\": [\n                        {\n                            \"auto_delete\": True,\n                            \"boot\": True,\n                            \"device_name\": instance.name,\n                            \"initialize_params\": {\n                                \"disk_size_gb\": 32,\n                                \"source_image_uri\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20230611\",\n                            },\n                        }\n                    ],\n                    \"network_interfaces\": [\n                        {\n                            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                            \"subnetwork\": \"global/networks/default\",\n                        }\n                    ],\n                    \"metadata\": {\n                        \"items\": [\n                            {\n                                \"key\": \"gce-access-list-name\",\n                                \"value\": \"all-v4\",\n                            },\n                        ]\n                    },\n                    \"software_config\": {\n                        \"image_version\": \"2.2\",\n                    },\n                },\n            },\n            insert_operation_project_id=project_id,\n            insert_operation_zone=zone,\n        )\n\n        instance = compute_engine_hook.get_instance(project_id, region, zone, instance.name)\n        print(instance)\n\n    create_cluster(CLUSTER_NAME_1, PROJECT_ID, REGION, ZONE)\n    create_cluster(CLUSTER_NAME_2, PROJECT_ID, REGION, ZONE)\n\n    # [START howto_dataproc_update_cluster_operator_main]\n    update_cluster_1 = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster_1\",\n        cluster_name=CLUSTER_NAME_1,\n        new_cluster_config=CLUSTER_CONFIG_1,\n        update_mask=UPDATE_MASK,\n    )\n    # [END howto_dataproc_update_cluster_operator_main]\n\n    update_cluster_2 = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster_2\",\n        cluster_name=CLUSTER_NAME_2,\n        new_cluster_config={},\n        update_mask=UPDATE_MASK,\n    )\n\n    delete_cluster_1 = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_1\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_1,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster_2 = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_2\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_2,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> create_cluster(CLUSTER_NAME_1, PROJECT_ID, REGION, ZONE)\n        >> create_cluster(CLUSTER_NAME_2, PROJECT_ID, REGION, ZONE)\n        # TEST BODY\n        >> update_cluster_1\n        >> update_cluster_2\n        # TEST TEARDOWN\n        >> delete_cluster_1\n        >> delete_cluster_2\n        >> delete_bucket\n    )\n    # [END howto_dataproc_update_cluster_operator]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_858_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765983940.3355896}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data transfer workflow that moves files from Google Cloud Storage to a Samba file share, demonstrating different transfer scenarios like single file copy, file move, directory copy, and selective file movement. Create a DAG that showcases flexible file migration strategies between cloud and network storage systems.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Storage to Samba transfer operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_samba import GCSToSambaOperator\nfrom airflow.providers.google.cloud.transfers.samba_to_gcs import SMBSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"gcs_to_smb\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nSAMBACLONE = str(Path(__file__).parent / \"resources\" / \"samba\").replace(\"\\\\\", \"/\")\nPOLL_INTERVAL = 1\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"gcs-to-smb\"],\n) as dag:\n    create_samba_clone = GCSCreateBucketOperator(task_id=\"create_samba_clone\", bucket_name=SAMBACLONE)\n\n    @task\n    def create_temp_gcs_bucket():\n        # [START howto_operator_gcs_create]\n        create_gcs_bucket = GCSCreateBucketOperator(\n            task_id=\"create_gcs_bucket\", bucket_name=BUCKET_NAME\n        )\n        create_gcs_bucket.wait_for_completion = True\n        # [END howto_operator_gcs_create]\n\n    create_temp_gcs_bucket_task = create_temp_gcs_bucket()\n\n    create_bucket_with_samba_clones = GCSCreateBucketOperator(\n        task_id=\"create_bucket_with_samba_clones\",\n        bucket_name=SAMBACLONE,\n        if_already_exists=\"overwrite\",\n    )\n\n    # [START howto_operator_gcs_copy_blob]\n    copy_example_file = GCSCreateBucketOperator(\n        task_id=\"copy_example_file\", bucket_name=BUCKET_NAME, source_bucket=SAMBACLONE\n    )\n    # [END howto_operator_gcs_copy_blob]\n\n    # [START howto_operator_gcs_move_blob]\n    move_example_file = GCSCreateBucketOperator(\n        task_id=\"move_example_file\", bucket_name=BUCKET_NAME, source_bucket=SAMBACLONE\n    )\n    # [END howto_operator_gcs_move_blob]\n\n    # [START howto_operator_gcs_delete_blob]\n    delete_example_file = GCSCreateBucketOperator(\n        task_id=\"delete_example_file\", bucket_name=BUCKET_NAME, source_blob_name=copy_example_file.output\n    )\n    # [END howto_operator_gcs_delete_blob]\n\n    # [START howto_operator_gcs_list_buckets]\n    list_buckets = GCSCreateBucketOperator(task_id=\"list_buckets\")\n    # [END howto_operator_gcs_list_buckets]\n\n    # [START howto_operator_smbstogcs]\n    smb_to_gcs = SMBSToGCSOperator(\n        task_id=\"smb_to_gcs\",\n        samba_conn_id=\"samba_default\",\n        gcs_bucket=BUCKET_NAME,\n        dest_path=\"smb_to_gcs_dir\",\n        filename=\"{{ ti.xcom.extras.json({{ task_instance.xcom_pull('copy_example_file') }})['name'] }}\",\n        gcp_user_project=PROJECT_ID,\n        poll_interval=POLL_INTERVAL,\n    )\n    # [END smbtoGCS]\n\n    # [START howto_operator_gcs_list_buckets_xcom]\n    list_buckets_xcom = GCSCreateBucketOperator(task_id=\"list_buckets_xcom\")\n    # [END howto_operator_gcs_list_buckets_xcom]\n\n    @task\n    def get_output(*args):\n        return args[0].output\n\n    # [START howto_operator_xcom_push]\n    push_output = XComPushOperator(\n        task_id=\"push_output\",\n        xcom_key=\"example_key\",\n        python_callable=get_output,\n        task_ids=\"get_output\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_xcom_push]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_get_example_file_operator():\n        \"\"\"Remove example `Get` task which can fail if run multiple times in a job.\"\"\"\n        get_example_file = TaskObj.get_task_by_id(\"get_example_file\")\n        if get_example_file is not None:\n            get_example_file.delete()\n        else:\n            print(\"Example Get task not found.\")\n\n    # [START howto_xcom_context]\n    get_example_file = GetFileFromSMBOperator(\n        task_id=\"get_example_file\",\n        samba_conn_id=\"samba_default\",\n        filename=\"{{ ti.xcom.extras.json({{ task_instance.xcom_pull('copy_example_file') }})['name'] }}\",\n    )\n    # [END howto_xcom_context]\n\n    remove_get_example_file_operator()\n\n    # [START howto_xcom_arg]\n    move_example_file_xcom_arg = MoveFileToSMBOperator(\n        task_id=\"move_example_file_xcom_arg\",\n        samba_conn_id=\"samba_default\",\n        source_path=\"{{ ti.xcom.extras.json({{ task_instance.xcom_pull('move_example_file') }})['path'] }}\",\n        destination_path=\"{{ ti.xcom.extras.json({{ task_instance.xcom_pull('move_example_file') }})['new_name'] }}\",\n    )\n    # [END howto_xcom_arg]\n\n    remove_move_example_file_xcom_arg_operator = RemoveMoveTaskOperator(\n        task_id=\"remove_move_example_file_xcom_arg_operator\",\n        move_task_id=\"move_example_file_xcom_arg\",\n        delete_path=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket_1 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_1\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    delete_bucket_2 = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_2\", bucket_name=SAMBACLONE, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_samba_clone\n        >> create_temp_gcs_bucket_task\n        >> create_bucket_with_samba_clones\n        >> copy_example_file\n        >> move_example_file\n        >> delete_example_file\n        >> list_buckets\n        >> create_bucket_with_samba_clones\n        >> smb_to_gcs\n        >> list_buckets_xcom\n        >> push_output\n        >> get_example_file\n        >> remove_get_example_file_operator\n        >> move_example_file_xcom_arg\n        >> remove_move_example_file_xcom_arg_operator\n        >> delete_bucket_1\n        >> delete_bucket_2\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_646_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.89216}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow pipeline that demonstrates scalable cloud data movement techniques, focusing on transferring files between cloud storage environments with robust error management and cleanup procedures. Create a workflow that shows how to programmatically manage cloud storage resources and perform reliable file transfers using Google Cloud Transfer Service.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Transfer Service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.cloud.transfer_v2.types import ImportJob\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    CCSTransferObject,\n    GCSToGCSTransferConfig,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.operators.network import (\n    ComputeDeleteInstanceOperator,\n    ComputeCreateInstanceOperator,\n)\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_stores_to_transfer\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nNETWORK = \"default\"\nGCS_BUCKET_URI = f\"gs://{BUCKET_NAME}/\"\nDESTINATION_NETWORK = \"global/networks/internet-allow-all\"\nFILE_NAME = \"file\"\nOLDER_FILE_NAME = \"older_file\"\nGCS_TO_GCS_TRANSFER_CONFIG = GCSCreateBucketOperator(\n    task_id=\"create_gcs_to_gcs_transfer_config\",\n    bucket_name=BUCKET_NAME,\n    source_bucket=GCS_BUCKET_URI + FILE_NAME,\n    destination_bucket=GCS_BUCKET_URI +OLDER_FILE_NAME,\n    project_id=PROJECT_ID,\n    overwrite_existing=True,\n    pre_delete_source=False,\n)\n\n# [START howto_google_cloud_stores_to_transfer_import_args]\nIMPORT_JOB = ImportJob(\n    name=\"transfer-obj-bucket-to-bucket-import-job-123\",\n    source_objects=[\n        CCSTransferObject(bucket=BUCKET_NAME, object_name=FILE_NAME),\n        CCSTransferObject(bucket=BUCKET_NAME, object_name=OLDER_FILE_NAME),\n    ],\n    destination=GCSToGCSTransferConfig(\n        bucket=BUCKET_NAME,\n        source_bucket=GCS_BUCKET_URI + FILE_NAME,\n        destination_network=DESTINATION_NETWORK,\n        project_id=PROJECT_ID,\n    ),\n    cancel_after_run=False,\n)\n# [END howto_google_cloud_stores_to_transfer_import_args]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_instance = ComputeCreateInstanceOperator(\n        task_id=\"create-instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body={\n            \"name\": f\"{LOCATION}-i\",\n            \"machine_type\": f\"zones/{LOCATION}/machineTypes/n1-standard-1\",\n            \"disks\": [\n                {\n                    \"auto_delete\": True,\n                    \"boot\": True,\n                    \"device_name\": f\"{LOCATION}-i\",\n                    \"initialize_params\": {\n                        \"disk_size_gb\": \"10\",\n                        \"source_image\": f\"projects/google-cloud-rackspace/global/images/node-128-gpu\",\n                        \"disk_encryption_tag\": \"MyDisks\",\n                    },\n                }\n            ],\n            \"network_interfaces\": [\n                {\n                    \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                    \"subnetwork\": f\"regions/{LOCATION}/subnetworks/default\",\n                }\n            ],\n        },\n    )\n\n    # [START howto_ccst_translate_obj_to_bucket_operator]\n    translate_objects_to_bucket = GCSToGCSOperator(\n        task_id=\"translate-objects-to-bucket\",\n        source_bucket=BUCKET_NAME,\n        source_object=gcs_object_name=FILE_NAME,\n        destination_bucket=f\"gs://{BUCKET_NAME}/{FILE_NAME}\",\n    )\n    # [END howto_ccst_translate_obj_to_bucket_operator]\n\n    # [START howto_ccst_import_bucket_to_obj_operator]\n    import_bucket_to_obj = GCSToGCSOperator(\n        task_id=\"import-bucket-to-object\",\n        source_bucket=BUCKET_NAME,\n        source_object=gcs_object_name=FILE_NAME,\n        destination=IMPORT_JOB,\n    )\n    # [END howto_ccst_import_bucket_to_obj_operator]\n\n    delete_instance = ComputeDeleteInstanceOperator(\n        task_id=\"delete-instance\",\n        zone=LOCATION,\n        resource_id=f\"{LOCATION}-i\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=BUCKET_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_operator\n        >> create_instance\n        >> delete_instance\n        # TEST BODY\n        >> translate_objects_to_bucket\n        >> GCSToGCSTransferConfig(\n            bucket=BUCKET_NAME,\n            source_bucket=GCS_BUCKET_URI + FILE_NAME,\n            destination=OLDER_FILE_NAME,\n        )\n        >> import_bucket_to_obj\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1290_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8921757}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a data workflow that monitors and validates file uploads to a Google Cloud Storage bucket. Design a process that checks for completed file transfers and triggers subsequent data processing steps based on file arrival and update conditions.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample DAG for demonstrating interactions with Google Cloud Transfer Service using the transfer operators.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom google.api_core.retry import Retry\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.cloud_transfer import (\n    CloudTransferCreateJobOperator,\n    CloudTransferDeleteJobOperator,\n    CloudTransferGetJobOperator,\n    CloudTransferListJobsOperator,\n    CloudTransferUpdateJobOperator,\n)\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_cloud_transfer\"\n\nRESOURCES_BUCKET = str(Path(__file__).parent / \"resources\" / \"transfer\")\n\nJOB_NAME = f\"transfer_job_{ENV_ID}\"\nJOB_NAME_UPDATE = f\"transfer_job_update_{ENV_ID}\"\n\nTRANSFER_INPUT_FILE_URI = f\"gs://{RESOURCES_BUCKET}/sample-export.csv\"\nTRANSFER_OUTPUT_FILE_URI = f\"gs://{RESOURCES_BUCKET}/sample-import.csv\"\nJOB_CONFIG = {\n    \"input_settings\": {\"input_uris\": [TRANSFER_INPUT_FILE_URI]},\n    \"output_settings\": {\"output_uri_prefix\": TRANSFER_OUTPUT_FILE_URI},\n    \"task_config\": {\"task_name\": \"test-task\"},\n}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"transfer\"],\n) as dag:\n    # [START howto_operator_gcp_transfer_create_job]\n    create_job = CloudTransferCreateJobOperator(\n        task_id=\"create_job\",\n        project_id=PROJECT_ID,\n        job_name=JOB_NAME,\n        job_configuration=JOB_CONFIG,\n        retry=Retry(maximum=5),\n        timeout=600,\n    )\n    # [END howto_operator_gcp_transfer_create_job]\n\n    # [START howto_operator_gcp_transfer_get_job]\n    get_job = CloudTransferGetJobOperator(\n        task_id=\"get_job\",\n        project_id=PROJECT_ID,\n        job_id=JOB_NAME,\n    )\n    # [END howto_operator_gcp_transfer_get_job]\n\n    # [START howto_operator_gcp_transfer_list_jobs]\n    list_jobs = CloudTransferListJobsOperator(\n        task_id=\"list_jobs\",\n        project_id=PROJECT_ID,\n        filter=\"project_id = '%s' AND name = '%s'\" % (PROJECT_ID, JOB_NAME),\n    )\n    # [END howto_operator_gcp_transfer_list_jobs]\n\n    # [START howto_operator_gcp_transfer_update_job]\n    update_job = CloudTransferUpdateJobOperator(\n        task_id=\"update_job\",\n        project_id=PROJECT_ID,\n        job_id=JOB_NAME,\n        job_configuration=JOB_CONFIG,\n    )\n    # [END howto_operator_gcp_transfer_update_job]\n\n    # [START howto_operator_gcp_transfer_delete_job]\n    delete_job = CloudTransferDeleteJobOperator(\n        task_id=\"delete_job\",\n        project_id=PROJECT_ID,\n        job_id=JOB_NAME,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_gcp_transfer_delete_job]\n\n    bash_task = BashOperator(task_id=\"bash_task\", bash_command=\"echo passed\")\n\n    create_job >> get_job >> list_jobs >> update_job >> delete_job >> bash_task\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule of TriggerRule.ALL_DONE is\n    # part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1253_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8921845}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Life Sciences pipeline that demonstrates basic workflow orchestration using Airflow. Create a DAG that uploads a file to a GCS bucket, runs a simple bash command, and then performs a multi-step pipeline action.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.life_sciences import (\n    CloudGenomicsDeletePipelineOperator,\n    CloudGenomicsRunPipelineOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_genomics\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nLOCATION = \"europe-west1\"\nSAMPLE_SET_ID = f\"sample_set_{DAG_ID}_{ENV_ID}\"\nSAMPLE_SET = {\n        \"display_name\": f\"sample_set_{DAG_ID}_{ENV_ID}\",\n        \"generator_options\": {\n            \"input_file_spec\": {\"gcs_source\": {\"uri\": [f\"gs://{BUCKET_NAME}/genotypes.csv\"]}}\n        },\n    }\nPIPELINE_ID = f\"pipeline_{DAG_ID}_{ENV_ID}\"\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"genomics\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    run_pipeline = CloudGenomicsRunPipelineOperator(\n        task_id=\"run_pipeline\", project_id=PROJECT_ID, location=LOCATION, body=SAMPLE_SET, api_version=\"v1\"\n    )\n\n    # [START howto_operator_gcp_genomics_delete_pipeline]\n    delete_pipeline = CloudGenomicsDeletePipelineOperator(\n        task_id=\"delete_pipeline\", project_id=PROJECT_ID, pipeline_id=PIPELINE_ID, api_version=\"v1\"\n    )\n    # [END howto_operator_gcp_genomics_delete_pipeline]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> run_pipeline\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) | watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_228_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8921912}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data asset registration and tagging pipeline that enables automated metadata management for cloud storage resources. The pipeline will demonstrate how to dynamically create catalog entries, assign custom tags, and prepare data assets for discovery and governance.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_gcs_sftp_get_trigger]\nthis_dir = os.path.dirname(os.path.realpath(__file__))\nsftp_src = f\"gs://{this_dir}/..//..//..//data/sample-sftp-files/\"\ngcs_files = [\"sftp-file-0.txt\", \"sftp-file-1.txt\"]\nwith DAG(\n    \"example_sftp_to_gcs_with_trigger\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    create_sftp_bucket = GCSCreateBucketOperator(task_id=\"create_sftp_bucket\", sftp_path=sftp_src)\n\n    # [START howto_operator_gcs_sftp_get]\n    get_file_job = SFTPToGCSOperator(\n        task_id=\"get_file_from_sftp\",\n        sftp_conn_id=\"conn_sftp\",\n        gcs_conn_id=\"conn_gcs\",\n        source_path=sftp_src + gcs_files[0],\n        dest_path=sftp_src + gcs_files[1],\n    )\n    # [END howto_operator_gcs_sftp_get]\n\n    delete_sftp_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_sftp_bucket\", bucket_name=sftp_src, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        create_sftp_bucket\n        >> get_file_job\n        >> delete_sftp_bucket\n    )\n    # [END howto_operator_gcs_sftp_get]\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_gcs_sftp_get]"}], "metadata": {"file_name": "dag_2136_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8921967}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that simulates a controlled waiting period in a data processing workflow, using an asynchronous time delta sensor to introduce a deliberate delay before task completion. Develop a DAG that shows how time-based synchronization can be achieved without occupying worker slots during the waiting phase.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.standard.hooks.base import StandardHook\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"std_hook_example\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_standard_hook_kafka]\n    @task\n    def kafka_hook_kafka():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-kafka-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_kafka]\n\n    # [START howto_standard_hook_s3]\n    @task\n    def kafka_hook_s3():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-s3-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_s3]\n\n    # [START howto_standard_hook_postgres]\n    @task\n    def kafka_hook_postgres():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-postgres-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_postgres]\n\n    # [START howto_standard_hook_mysql]\n    @task\n    def kafka_hook_mysql():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-mysql-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_mysql]\n\n    # [START howto_standard_hook_mssql]\n    @task\n    def kafka_hook_mssql():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-mssql-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_mssql]\n\n    # [START howto_standard_hook_sqlite]\n    @task\n    def kafka_hook_sqlite():\n        hook = StandardHook()\n        sql = \"SELECT * FROM test\"\n        response = hook.query(sql=sql, conn_id=\"airflow-sqlite-hook\")\n\n        for row in response:\n            print(row)\n\n    # [END howto_standard_hook_sqlite]\n\n    # [START howto_standard_hook_postgres_inlet]\n    @task(inlets=[\"kafka\"], outlets=[\"csv\"])\n    def kafka_to_csv(postgres_inlet):\n        for row in postgres_inlet:\n            print(row)\n\n    # [END howto_standard_hook_postgres_inlet]\n\n    # [START howto_standard_hook_mysql_inlet]\n    @task(inlets=[\"kafka\"], outlets=[\"csv\"])\n    def kafka_to_csv(mysql_inlet):\n        for row in mysql_inlet:\n            print(row)\n\n    # [END howto_standard_hook_mysql_inlet]\n\n    # [START howto_standard_hook_mssql_inlet]\n    @task(inlets=[\"kafka\"], outlets=[\"csv\"])\n    def kafka_to_csv(mssql_inlet):\n        for row in mssql_inlet:\n            print(row)\n\n    # [END howto_standard_hook_mssql_inlet]\n\n    # [START howto_standard_hook_s3_inlet]\n    @task(inlets=[\"kafka\"], outlets=[\"csv\"])\n    def kafka_to_csv(s3_inlet):\n        for row in s3_inlet:\n            print(row)\n\n    # [END howto_standard_hook_s3_inlet]\n\n    # [START howto_standard_hook_kafka_outlet]\n    @task(outlets=[\"kafka\"])\n    def csv_to_kafka(kafka_inlet):\n        kafka_hook_kafka().expand(kafka_inlet=kafka_inlet) >> BashOperator(\n            bash_command='sleep 5', trigger_rule=TriggerRule.ALL_DONE\n        )\n\n    # [END howto_standard_hook_kafka_outlet]\n\n    # [START howto_standard_hook_mysql_outlet]\n    @task(outlets=[\"mysql\"])\n    def csv_to_mysql(csv_inlet):\n        kafka_hook_mysql().expand(csv_inlet=csv_inlet) >> BashOperator(\n            bash_command='sleep 5', trigger_rule=TriggerRule.ALL_DONE\n        )\n\n    # [END howto_standard_hook_mysql_outlet]\n\n    # [START howto_standard_hook_mssql_outlet]\n    @task(outlets=[\"mssql\"])\n    def csv_to_mssql(csv_inlet):\n        kafka_hook_mssql().expand(csv_inlet=csv_inlet) >> BashOperator(\n            bash_command='sleep 5', trigger_rule=TriggerRule.ALL_DONE\n        )\n\n    # [END howto_standard_hook_mssql_outlet]\n\n    # [START howto_standard_hook_s3_outlet]\n    @task(outlets=[\"s3\"])\n    def csv_to_s3(s3_inlet):\n        kafka_hook_s3().expand(s3_inlet=s3_inlet) >> BashOperator(\n            bash_command='sleep 5', trigger_rule=TriggerRule.ALL_DONE\n        )\n\n    # [END howto_standard_hook_s3_outlet]\n\n    (kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka_to_csv(kafka"}], "metadata": {"file_name": "dag_1862_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922145}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain how the Chinese Remainder Theorem (CRT) can be effectively used to solve systems of congruences? Could you also provide a Python code example to demonstrate its application?"}, {"role": "assistant", "content": "def extended_gcd(a, b):\n    \"\"\"Extended Euclidean Algorithm to find gcd(a, b) and coefficients x, y.\"\"\"\n    if b == 0:\n        return a, 1, 0\n    gcd, x1, y1 = extended_gcd(b, a % b)\n    x = y1\n    y = x1 - (a // b) * y1\n    return gcd, x, y\n\ndef mod_inverse(x, m):\n    \"\"\"Find the modular inverse of x modulo m.\"\"\"\n    gcd, inv, _ = extended_gcd(x, m)\n    if gcd != 1:\n        raise ValueError(f\"No modular inverse for {x} modulo {m}\")\n    return inv % m\n\ndef chinese_remainder_theorem(moduli, remainders):\n    \"\"\"\n    Solve a system of linear congruences using the Chinese Remainder Theorem.\n    \n    Parameters:\n    moduli: List of positive integers (pairwise coprime).\n    remainders: List of integers corresponding to the congruences.\n    \n    Returns:\n    The smallest non-negative integer solution to the system of congruences.\n    \"\"\"\n    total_product = 1\n    sum_modulus = 0\n    \n    # Calculate total product of all moduli\n    for mod in moduli:\n        total_product *= mod\n    \n    # Calculate partial products and inverses\n    for i, mod in enumerate(moduli):\n        partial_product = total_product // mod\n        inverse = mod_inverse(partial_product, mod)\n        sum_modulus += mod * partial_product * inverse\n    \n    # Adjust sum_modulus to get the final result\n    return sum_modulus % total_product\n\n# Example usage\nmoduli = [3, 5, 7]  # Pairwise coprime\nremainders = [2, 3, 2]  # Corresponding congruences\n\nsolution = chinese_remainder_theorem(moduli, remainders)\nprint(f\"The solution to the system of congruences is: {solution}\")"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922336}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow pipeline that showcases the sequential steps of interacting with a cloud-based NoSQL database, focusing on transaction management and query preparation. Build a workflow that illustrates the technical mechanics of database interaction in a cloud environment.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.niudu import NiuDuExecuteSqlTaskOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_nebula\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_nebulastatic]\n    nebula_to_sql = NiuDuExecuteSqlTaskOperator(\n        task_id=\"nebulastatic_test\",\n        conn_id=\"nebuladb_default\",\n        sql='set default timeout=60s; '\n        'select id, name, value from kadmind static_data;',\n    )\n    # [END howto_operator_nebulastatic]\n\n    (nebula_to_sql >> npc_sql_result)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_532_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922374}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases the integration between Airflow's task decorators and Snowflake's Snowpark session management, highlighting how to programmatically create a DataFrame and write it to a Snowflake table. Implement a validation task that checks the data integrity of the newly created table.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\nfrom airflow.providers.snowflake.sensors.snowflake import SnowflakeSensor\n\nSNOWFLAKE_CONN_ID = \"snowflake\"\nSCHEMA = \"DEV\"\nTABLE = \"STAGING\"\n\nwith DAG(\n    \"example_snowflake_operator\",\n    schedule=\"* * * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_snowflake]\n    snowflake_df_exists_sensor = SnowflakeSensor(\n        task_id=\"snowflake_df_exists_sensor\",\n        sql=f\"SELECT COUNT(*) FROM {SNOWFLAKE_CONN_ID}.SCOPE_TABLE(SYSDATE-3)\",\n    )\n\n    snowflake_create_table_sql_task = SnowflakeOperator(\n        task_id=\"snowflake_create_table_sql_task\",\n        sql=f\"\"\"\n                    CREATE OR REPLACE TABLE {SCHEMA}.{TABLE} (\n                        dt DATE,\n                        hs_code VARCHAR(50),\n                        description VARCHAR(250)\n                    );\n                   \"\"\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n\n    snowflake_insert_value_sql_task = SnowflakeOperator(\n        task_id=\"snowflake_insert_value_sql_task\",\n        sql=f\"\"\"INSERT INTO {SCHEMA}.{TABLE} VALUES ('19-NIAN', 'hs_7481', 'Gasoline, unleaded (not refined)');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('18-NIAN', 'hs_7482', 'Diesel fuel, not refined');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('19-NIAN', 'hs_7483', 'Premium gasoline, unleaded (not refined)');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('18-NIAN', 'hs_7484', 'Auto/spare part antifreeze (ethyl alcohol contained)');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('19-NIAN', 'hs_7485', 'Motor vehicle batteries');\"\"\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n    # [END howto_operator_snowflake]\n\n    snowflake_insert_value_sql_task2 = SnowflakeOperator(\n        task_id=\"snowflake_insert_value_sql_task2\",\n        sql=f\"\"\"INSERT INTO {SCHEMA}.{TABLE} VALUES ('19-NIAN', 'hs_7486', 'Automobile windshield washer fluid');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('18-NIAN', 'hs_7487', 'Aviation kerosene');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('19-NIAN', 'hs_7488', 'Aviation turbine fuel (jet fuel)');\n              INSERT INTO {SCHEMA}.{TABLE} VALUES ('18-NIAN', 'hs_7489', 'Pet Lotion or Preservative');\"\"\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n\n    snowflake_insert_multiple_values_sql_task = SnowflakeOperator(\n        task_id=\"snowflake_insert_multiple_values_sql_task\",\n        sql=f\"\"\"BEGIN;\n             DECLARE done BOOLEAN;\n             DECLARE rec RECORD;\n             SET done = FALSE;\n             SELECT schema_name INTO rec FROM sys.schemas WHERE user = 'USER';\n             SELECT table_name INTO rec FROM sys.tables WHERE schema = rec.schema_name AND table_name='STAGING';\n             WHILE (done != TRUE) DO\n                SELECT max(dt) INTO rec.dt FROM {rec.schema_name}.{rec.table_name} WHERE dt >= dateadd(day,-3,currval('sys.fn_getrandblob'));\n                IF(rec.dt IS NULL) THEN\n                   LEAVE loop;\n                END IF;\n             END WHILE;\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5781','Sugar, Cane');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5782','Sugar, Beets');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5783','Condensed Milk');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5784','Sweeteners, Syrups');\n             END;\"\"\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n\n    snowflake_insert_multiple_values_sql_task2 = SnowflakeOperator(\n        task_id=\"snowflake_insert_multiple_values_sql_task2\",\n        sql=f\"\"\"BEGIN;\n             DECLARE done BOOLEAN;\n             DECLARE rec RECORD;\n             SET done = FALSE;\n             SELECT schema_name INTO rec FROM sys.schemas WHERE user = 'USER';\n             SELECT table_name INTO rec FROM sys.tables WHERE schema = rec.schema_name AND table_name='STAGING';\n             WHILE (done != TRUE) DO\n                SELECT max(dt) INTO rec.dt FROM {rec.schema_name}.{rec.table_name} WHERE dt >= dateadd(day,-3,currval('sys.fn_getrandblob'));\n                IF(rec.dt IS NULL) THEN\n                   LEAVE loop;\n                END IF;\n             END WHILE;\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5785','Flour, All-Purpose');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5786','Wheat Bread');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5787','Rye Bread');\n             INSERT INTO {SCHEMA}.{TABLE} (dt,hs_code,description) VALUES (rec.dt,'hs_5788','Potato Products');\n             END;\"\"\",\n        conn_id=SNOWFLAKE_CONN_ID,\n    )\n\n    snowflake_scan_sql_task = SnowflakeOperator(\n        task_id=\"snowflake_scan_sql_task\",\n        sql=f\"DESCRIBE TABLE {SCHEMA}.{TABLE}\",\n    )\n\n    snowflake_insert_multiple_values_sql_task.cache()\n    snowflake_insert_multiple_values_sql_task2.cache()\n\n    snowflake_df_exists_sensor >> snowflake_create_table_sql_task >> [\n        snowflake_insert_value_sql_task,\n        snowflake_insert_value_sql_task2,\n    ]\n    [\n        snowflake_insert_value_sql_task,\n        snowflake_insert_multiple_values_sql_task,\n    ] >> snowflake_scan_sql_task >> snowflake_df_exists_sensor\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2206_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922517}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates a single-task data transfer mechanism between cloud storage and a distributed query engine. Develop a scheduled job that moves data from GCS to Presto with minimal configuration and maximum reliability.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToPrestoOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.presto.transfers.gcs_to_presto import GCSToPrestoOperator\n\n# [START gcs_to_presto_operator]\nwith DAG(\n    \"gcs_to_presto\",\n    schedule=datetime(2021, 5, 4),\n    start_date=datetime(2021, 5, 2)),\n    catchup=False,\n) as dag:\n    # [START gcs_to_presto_operator]\n    gcs_to_presto = GCSToPrestoOperator(\n        task_id=\"gcs_to_presto_example\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_bucket=\"test-data\",\n        source_object=\"presto/data\",\n        presto_conn_id=\"presto_cluster\",\n        table=\"airflow_test_table\",\n    )\n    # [END gcs_to_presto_operator]\n# [END gcs_to_presto_operator]\n\n\n# Externally triggered task\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_992_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922558}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a demonstration DAG that showcases basic task orchestration and Docker integration, showing how different types of tasks can be sequenced and executed in an Airflow workflow.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"print_date\")\ndef print_date():\n    \"\"\"\n    Print the date.\n    \"\"\"\n    print(str(datetime.strptime(os.environ.get(\"DATE\"), \"%Y-%m-%d\")))\n\n\n@task(task_id=\"print_time\")\ndef print_time():\n    \"\"\"\n    Print the time.\n    \"\"\"\n    print(str(datetime.now().time()))\n\n\n@task(task_id=\"print_time_date\")\ndef print_time_date():\n    \"\"\"\n    Print Date and Time.\n    \"\"\"\n    print(str(datetime.strptime(os.environ.get(\"DATE\"), \"%Y-%m-%d\")) + \" \" + str(datetime.now().time()))\n\n\nwith DAG(\n    dag_id=\"example_env_vars\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n):\n    pass\n    # [START howto_operator_env_var]\n    print_date()\n    # [END howto_operator_env_var]\n\n    # [START howto_operator_env_var_result]\n    print_time_result = PythonOperator(task_id=\"print_time\", python_callable=print_time)\n    # [END howto_operator_env_var_result]\n\n    # [START howto_operator_env_var_result_templatease]\n    print_time_date_result = PythonOperator(\n        task_id=\"print_time_date\", python_callable=print_time_date\n    )\n    # [END howto_operator_env_var_result_templatease]"}], "metadata": {"file_name": "dag_1563_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.89226}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you design a Python class that includes methods for arithmetic operations and logging? The class should perform addition, subtraction, multiplication, and division, and it should log each operation with a timestamp and the result. The logging should be handled by a separate logger class."}, {"role": "assistant", "content": "import datetime\n\nclass Logger:\n    def __init__(self, filename='log.txt'):\n        self.filename = filename\n    \n    def log(self, message):\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        with open(self.filename, 'a') as file:\n            file.write(f\"{current_time}: {message}\\n\")\n\n# Create an instance of Logger\nlogger = Logger()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.89227}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to calculate the smallest common multiple (SCM) of two numbers. The function should take two integers as input and return their SCM."}, {"role": "assistant", "content": "def gcd(a, b):\n    \"\"\"Calculate the Greatest Common Divisor (GCD) of two numbers.\"\"\"\n    while b:\n        a, b = b, a % b\n    return a\n\ndef scm(a, b):\n    \"\"\"\n    Calculate the Smallest Common Multiple (SCM) of two numbers.\n\n    Args:\n    a (int): First integer.\n    b (int): Second integer.\n\n    Returns:\n    int: The SCM of the two integers.\n    \"\"\"\n    if a == 0 or b == 0:\n        raise ValueError(\"Both numbers must be non-zero.\")\n    \n    # Ensure a >= b\n    if a < b:\n        a, b = b, a\n    \n    # Calculate SCM using the formula abs(a * b) // gcd(a, b)\n    return abs(a * b) // gcd(a, b)\n\n# Example usage:\nprint(scm(12, 18))  # Output: 36"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922784}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can I inherit from a class in Python that doesn't have a `__init__` method?"}, {"role": "assistant", "content": "from abc import ABCMeta, abstractmethod\n\nclass BaseClass(metaclass=ABCMeta):\n    @abstractmethod\n    def __init__(self):\n        pass  # This will raise a TypeError if not overridden\n\n# Now you can define concrete classes inheriting from BaseClass\nclass ConcreteClass(BaseClass):\n    def __init__(self):\n        super().__init__()  # Calls the abstract method's implementation"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922849}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases task sequencing and cloud storage interactions using Azure Data Lake Storage operators. Construct a DAG that demonstrates how to programmatically upload and remove files with explicit dependency management.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.adls import ADLSDeleteOperator\nfrom airflow.providers.microsoft.azure.transfers.local_to_adls import LocalFilesystemToADLSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_local_to_adls\"\n\nconn = Connection(\n    conn_id=\"my_connection\",\n    conn_type=\"azure_container\",\n    extra={\"subscription_id\": \"my_subscription_id\", \"resource_group_name\": \"my_resource_group\"},\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    upload = LocalFilesystemToADLSOperator(\n        task_id=\"upload_file\",\n        local_path=\"/tmp/ace_data.csv\",\n        adls_path=\"example/adls/path/\",\n        replace=True,\n    )\n    # [START howto_operator_adls_delete]\n    delete = ADLSDeleteOperator(task_id=\"delete_file\", path=\"example/adls/path/\", recursive=True)\n    # [END howto_operator_adls_delete]\n    upload >> delete\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = next(get_test_run(dag))\n\n\nif test_run:\n    if \"success\" in test_run.output:\n        msg = f\"Some tasks failed, the following task failed {test_run.task_list[-1]}\"\n    else:\n        msg = f\"All tests passed, the following task succeeded {test_run.task_list[0]}\"\n\n    print(msg)"}], "metadata": {"file_name": "dag_2151_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922904}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a distributed command execution workflow that leverages WinRM to run system-level commands across remote Windows environments, highlighting inter-task dependencies and parallel processing capabilities. Create a DAG that simulates real-world remote system management and monitoring scenarios.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom winrm.protocol import Protocol\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.winrm.hooks.winrm import WinRMHook\nfrom airflow.providers.microsoft.winrm.operators.winrm import WinRMOperator\n\nWIN_RM_CONNECTION_ID = \"winrm_default\"\n\nHOME = os.path.expanduser(\"~\")\nREMOTE_MACHINE_NAME = \"remote-test-machine\"\nREMOTE_MACHINE_PUBLIC_FQDN = f\"{REMOTE_MACHINE_NAME}.test-dag.com\"\nREMOTE_MACHINE_PUBLIC_IP = \"192.168.1.100\"\nTAR_FILE_LOCAL_PATH = \"~/Downloads/\u201d\nTAR_FILE_REMOTE_PATH = \"/tmp/\"\nSLEEP = 10\n\n\n@task\ndef create_win_rm_connection():\n    # [START howto_operator_winrm_create_connection]\n    win_rm_hook = WinRMHook(conn_id=WIN_RM_CONNECTION_ID)\n    win_rm_hook.create(\n        ssh_user=\"your-azure-rm-user\",\n        ssh_password=\"your-azure-rm-password\",\n        use_sftp=True,\n    )\n    # [END howto_operator_winrm_create_connection]\n\n\n@task\ndef delete_win_rm_connection():\n    # [START howto_operator_winrm_delete_connection]\n    win_rm_hook = WinRMHook()\n    win_rm_hook.delete(conn_id=WIN_RM_CONNECTION_ID)\n    # [END howto_operator_winrm_delete_connection]\n\n\n@task\ndef setup_firewall():\n    # [START howto_operator_winrm_setup_firewalld]\n    win_rm_hook = WinRMHook()\n    win_rm_hook.setup_firewalld(remote_machine_name=REMOTE_MACHINE_NAME, enable_firewall=True)\n    # [END howto_operator_winrm_setup_firewalld]\n\n    # [START howto_operator_winrm_remove_firewalld]\n    win_rm_hook = WinRMHook()\n    win_rm_hook.remove_firewalld(remote_machine_name=REMOTE_MACHINE_NAME)\n    # [END howto_operator_winrm_remove_firewalld]\n\n\n@task\ndef create_tar_file():\n    # [START howto_operator_winrm_create_temp_file]\n    win_rm_hook = WinRMHook()\n    tmp_file_path = win_rm_hook.create_temp_file(\n        remote_machine_name=REMOTE_MACHINE_NAME, remote_command_line=f\"-c tar -cvf {TAR_FILE_LOCAL_PATH} *\n    \"\n    )\n    # [END howto_operator_winrm_create_temp_file]\n\n    return tmp_file_path\n\n\n@task\ndef execute_commands():\n    # [START howto_operator_winrm_batch]\n    win_rm_hook = WinRMHook()\n    output = win_rm_hook.batch(\n        remote_machine_name=REMOTE_MACHINE_NAME,\n        remote_commands=[\n            (\"cmd\", \"-n\", \"-c\", f\"echo Test 1\\nsleep {SLEEP}\"),\n            (TAR_FILE_REMOTE_PATH, \"-o\", f\"{TAR_FILE_LOCAL_PATH}\", \"-c\", \"tar\", \"-xvf\", TAR_FILE_LOCAL_PATH),\n            (\"cmd\", \"-n\", \"-c\", f\"echo Test 2; echo Test 3 after 10 seconds\"),\n        ],\n        parallelism=4,\n    )\n    # [END howto_operator_winrm_batch]\n\n    print(output)\n\n\nwith DAG(\n    \"example_winrm\",\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n) as dag:\n    create_win_rm_connection()\n    create_tar_file()\n    execute_commands()\n    delete_win_rm_connection()\n    setup_firewall()\n    create_win_rm_connection()"}], "metadata": {"file_name": "dag_1761_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8922985}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that demonstrates cross-task data sharing and XCom communication techniques using both Python and Bash operators. Create a workflow that shows how different tasks can push and pull data between each other using Airflow's XCom mechanism.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of XCom in Python and Bash task types.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n# [START xcom_operator]\n@task\ndef get_number():\n    return \"My own value\n\n\n# [END xcom_operator]\n\n\n# [START xcom_task_xcom_push]\n@task\ndef push_to_xcom(task_output)\n    xcom.push(task_output.output, key=\"some_key\")\n# [END xcom_task_xcom_push]\n\n\n# [START xcom_pull_operator]\n@task\ndef pull_from_xcom(channels):\n    print(\"Pulled:\")\n    for xcom_value in channels.pull(key=\"some_key\"):\n        print(xcom_value)\n\n# [END xcom_pull_operator]\n\n\nwith DAG(\n    dag_id=\"example_xcom_bash\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    get_number()\n    push_to_xcom(get_number())\n    pull_from_xcom([\"$env:XCOMP pass:push_to_xcom\"])\n\n\n# [START xcom_basic]\nwith DAG(\n    dag_id=\"example_xcom_basic\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    fact_table = \"fact_airflow\"\n    dim_table = \"dim_customer\"\n\n    # [START xcom_push]\n    @task.xcom_push(key=\"seeds\", value=[dim_table, fact_table])\n    def get_seeds(dim_table, fact_table):\n        return [\n            {\"dim_table\": dim_table, \"fact_table\": fact_table},\n        ]\n\n    # [END xcom_push]\n\n    seeds = get_seeds(dim_table, fact_table)\n    # [START xcom_pull]\n    @task.xcom_pull(key=\"seeds\", task_ids=[\"get_seeds\"])\n    def print_seeds(seeds):\n        for seed in seeds:\n            print(seed[\"dim_table\"], seed[\"fact_table\"])\n\n    # [END xcom_pull]\n    print_seeds(seeds)\n    # [START xcom_get_one]\n    @task.xcom_get(key=\"seeds\", task_ids=[\"get_seeds\"])\n    def get_one_seed(seeds):\n        return seeds[0]\n\n    # [END xcom_get_one]\n    one_seed = get_one_seed(seeds)\n    # [START xcom_put_one]\n    @task.xcom_push(key=\"one_seed\", task_ids=[\"get_one_seed\"], output=one_seed)\n    def put_one(value):\n        return {\n            \"dim_table\": value[\"dim_table\"],\n            \"fact_table\": value[\"fact_table\"],\n        }\n\n    # [END xcom_put_one]\n    put_one(put_one(value))\n# [END xcom_basic]"}], "metadata": {"file_name": "dag_1454_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923051}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hi, I have a dataset with numerical features and a target variable that is categorical. I want to perform classification. Can you guide me on how to approach this problem using Python?"}, {"role": "assistant", "content": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923132}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data migration workflow that seamlessly moves structured CSV data from cloud storage into a distributed query engine for further analysis. Set up an Airflow DAG that enables efficient, one-time data transfer between cloud storage and a Presto database table.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample DAG using GCSToPrestoOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.pgdatabase.hooks.pgdatabase import PGDatabaseHook\nfrom airflow.providers.presto.hooks.presto import PrestoHook\nfrom airflow.providers.presto.operators.presto import PrestoOperator\nfrom airflow.providers.s3.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_gcs_to_presto\"\n\nS3_BUCKET = \"bucket-airflow-presto\"\nS3_KEY = \"presto/employee.csv\"\nSG_ID = 123  # ID of the Security Group that should be tested\n\nPRESTO_CONN_ID = \"presto\"\nPRESTO_TABLE = \"test_presto_table\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    upload_file = GCSToS3Operator(\n        task_id=\"upload_file_from_gcs\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_bucket=S3_BUCKET,\n        source_object=S3_KEY,\n        dest_path=\"presto/\",\n    )\n    # [START howto_transfer_gcs_to_presto]\n    read_data_presto = PrestoOperator(\n        task_id=\"read_data_presto\",\n        sql=f\"SELECT * FROM {PRESTO_TABLE}\",\n        presto_conn_id=PRESTO_CONN_ID,\n    )\n    # [END howto_transfer_gcs_to_presto]\n    delete_file = GCSToS3Operator(\n        task_id=\"delete_file_from_gcs\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_bucket=S3_BUCKET,\n        source_object=S3_KEY,\n        dest_path=\"presto/\",\n    )\n\n    delete_file >> upload_file >> read_data_presto\n\n    SG = PGDatabaseHook().create_security_group(\n        group_name='test-sg',\n        description='Test security group',\n        vpc_id='vpc-96f57a7e',\n        conn_info=PRESTO_CONN_ID,\n        priority=SG_ID,\n        allow_list=True\n    )\n\n    delete_sg = PGDatabaseHook().run_cmd(\"sh -c \\\"sqlcmd -E -s -n -h 0 -p 1522 -U postgres -w \\\n                                         'REVOKE ALL ON SECURITY GROUP \\'test-sg\\' FROM PUBLIC; \\\n                                         DROP SECURITY GROUP test-sg;'\")\n\nwith DAG(\n    dag_id=dag.id + \"-deferrable\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    catchup=False,\n    tags=[\"example\"]\n) as deferrable_dag:\n    upload_file = GCSToS3Operator(\n        task_id=\"upload_file_from_gcs\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_bucket=S3_BUCKET,\n        source_object=S3_KEY,\n        dest_path=\"presto/\",\n        deferrable=True,\n    )\n    # [START howto_transfer_gcs_to_presto_deferrable]\n    read_data_presto = PrestoOperator(\n        task_id=\"read_data_presto\",\n        sql=f\"SELECT * FROM {PRESTO_TABLE}\",\n        presto_conn_id=PRESTO_CONN_ID,\n        deferrable=True,\n    )\n    # [END howto_transfer_gcs_to_presto_deferrable]\n    delete_file = GCSToS3Operator(\n        task_id=\"delete_file_from_gcs\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_bucket=S3_BUCKET,\n        source_object=S3_KEY,\n        dest_path=\"presto/\",\n        deferrable=True,\n    )\n\n    delete_file >> upload_file >> read_data_presto\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(deferral_mode_activated=True)"}], "metadata": {"file_name": "dag_644_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923225}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you explain the differences between `sys.exit()` and `os._exit()` in Python, and provide examples of how to use each?"}, {"role": "assistant", "content": "import sys\nsys.exit([arg])"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923302}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an automated workflow that demonstrates provisioning an Amazon EKS cluster with Fargate compute resources and running a simple test pod. Design a DAG that showcases the end-to-end process of creating, testing, and cleaning up an EKS cluster infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterHook, NodegroupHook\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_eks\"\n\n# This Dag will use IAM roles attached to AWS Systems Test Environment Resources.\n# If you want to run this outside of AWS Systems Test please provide a valid\n# IAM ROLE in the environment.\nCLUSTER_NAME = \" eks-test-cluster-name-cl \"\"\"\nNODEGROUP_NAME = \" eks-test-nodegroup-name- cl \"\"\"\n\n@task\ndef create_cluster_hook(cluster_name):\n    return ClusterHook(cluster_name=cluster_name)\n\n\n@task\ndef create_nodegroup_hook(nodegroup_name):\n    return NodegroupHook(nodegroup_name=nodegroup_name)\n\n\n@task\ndef get_iam_role_arn():\n    # We retrieve the role ARN inside an IAM simulator script\n    # The script path is known during system test isolation and can be directly retrieved using `os.path.dirname()`\n    sys_test_path = os.path.dirname(__file__)\n    test_iam_role_name = os.environ[\"SYSTEM_TEST_IAM_ROLE_NAME\"]\n    roleArn = boto3.client(\"iam\").get_role(RoleName=test_iam_role_name)[\"Role\"][\"Arn\"]\n    return roleArn\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_iam_role_arn():\n    iam_role Arn = get_iam_role_arn()\n    boto3.client(\"iam\").delete_role(RoleId=iam_role Arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster_hook(cluster_name):\n    hook = create_cluster_hook(cluster_name)\n    hook.delete()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_nodegroup_hook(nodegroup_name):\n    hook = create_nodegroup_hook(nodegroup_name)\n    hook.delete()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    create_eks_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        region=\"us-east-1\",\n        compute=\"nodegroup\",\n        resource_group_name=\"my-resource-group\",\n    )\n\n    create_eks_cluster >> get_iam_role_arn()\n\n    # [START howto_operator_eks_create_nodegroup]\n    create_eks_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        nodegroup_subnets=[\"subnet-1\", \"subnet-2\", \"subnet-3\"],\n        nodegroup_role_arn=get_iam_role_arn(),\n        min_nodes=1,\n        max_nodes=3,\n    )\n    # [END howto_operator_eks_create_nodegroup]\n\n    chck_point = {\n        \"Status\": \"SUCCESS\",\n        \"Path\": \"/tmp\",\n    }\n\n    execute_pod = EksPodOperator(\n        task_id=\"execute_pod\",\n        cluster_name=CLUSTER_NAME,\n        pod_name=\"test-pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", f\"echo {chck_point['Status']} && echo {chck_point['Path']}/ping.sh\"],\n        labels={},\n        nodegroup_name=NODEGROUP_NAME,\n        get_logs=True,\n        wait_for_status=True,\n        status_kwargs={\"match_status\": \"SUCCESS\"},\n    )\n\n    delete_eks_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n    )\n\n    delete_eks_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        region=\"us-east-1\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        create_eks_cluster,\n        create_eks_nodegroup,\n        execute_pod,\n        delete_eks_nodegroup,\n        get_iam_role_arn(),\n        delete_cluster_hook(CLUSTER_NAME),\n        create_nodegroup_hook(NODEGROUP_NAME),\n        delete_eks_nodegroup,\n        delete_eks_cluster,\n        get_iam_role_arn(),\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1481_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.892341}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "In Python, how can I ensure that a function is called and its result is checked for a specific condition without using decorators?"}, {"role": "assistant", "content": "from functools import wraps\n\ndef check_condition(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Check if the function was actually called with arguments\n        if args or kwargs:\n            print(\"The function was called with arguments.\")\n            # Additional checks can be performed here\n            return func(*args, **kwargs)\n        else:\n            raise ValueError(\"Function must be called with arguments.\")\n\n    return wrapper\n\n# Example usage of the check_condition decorator\n@check_condition\ndef add(a, b):\n    \"\"\"This function adds two numbers.\"\"\"\n    return a + b\n\ntry:\n    result = add()  # This will raise a ValueError\nexcept ValueError as e:\n    print(e)\n\nresult = add(5, 3)  # This will output: The function was called with arguments.\nprint(result)       # Output: 8"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923507}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Windows Remote Management (WinRM) task orchestration DAG that demonstrates parallel remote command execution across multiple Windows servers. Create a workflow that showcases executing different shell commands simultaneously and tracking their completion.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nfrom airflow.providers.ssh.transfers.winrm import WinRMTransfer\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_winrm\"\n\nCONN_ID = f\"winrm_{DAG_ID}_{ENV_ID}\"\nSINGLETON_CONN = {\n    \"conn_id\": CONN_ID,\n    \"extra\": '{\"username\": \"test\", \"password\": \"test\", \"server\": \"remote_system\"'},\n}\n\nREMOTE_COMMAND = \"echo success; echo failed | pbcopy\"\nLOCAL_COMMAND = f\"pbpaste | {REMOTE_COMMAND}\"\n\nsys_test_conn = Connection(\n    conn_id=CONN_ID,\n    extra=SINGLETON_CONN,\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task\n    def create_winrm_connection():\n        sys_test_conn.extra | SINGLETON_CONN\n\n    create_winrm_connection_task = create_winrm_connection()\n\n    winrm_to_remote = WinRMTransfer(source_conn_id=\"dummy\", destination_conn_id=CONN_ID)\n    winrm_to_remote.remote_command = REMOTE_COMMAND\n\n    copy_to_winrm = WinRMTransfer(source_conn_id=CONN_ID, destination_conn_id=\"dummy\")\n    copy_to_winrm.remote_command = LOCAL_COMMAND\n\n    ssh_to_remote = SSHOperator(task_id=\"ssh_to_remote\", command=REMOTE_COMMAND, conn_id=CONN_ID)\n\n    create_winrm_connection_task >> winrm_to_remote >> copy_to_winrm >> ssh_to_remote\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_628_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923554}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Is there any library available in Python for parsing and interpreting natural language instructions for date and time extraction?"}, {"role": "assistant", "content": "import spacy\n\n# Load the English tokenizer, tagger, parser, NER, and word vectors\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_dates(text):\n    doc = nlp(text)\n    dates = []\n    for ent in doc.ents:\n        if ent.label_ == 'DATE':\n            dates.append(ent.text)\n    return dates\n\ntext = \"The meeting is scheduled for March 15th, 2023 at 14:00.\"\ndates = extract_dates(text)\nprint(dates)  # Output: ['March 15th, 2023']"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923619}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates event-driven message processing using Kafka as a message broker, with tasks that produce sequential numeric data and a sensor that dynamically responds to specific numeric conditions. Implement a real-time event listener that can detect and react to numbers divisible by 3, 5, or 15.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.kms import GoogleCloudKMSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.kms_to_gcs import KMsTGSinkOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"kms_to_gcs_async\"\nPROJECT_ID = \"google-cloud-samples\"\nLOCATION = \"europe-west1\"\nKEY_RING_ID = f\"kr_{DAG_ID}\"\nFILE_NAME = \"test-bucket-file\"\nBUCKET_NAME = f\"bucket-{DAG_ID}-{PROJECT_ID}\"\nENCRYPTION_SPEC = {\n    \"type_\": \"CRYPTOGRAPHY\",\n    \"cipher_spec\": {\"algorithm\": \"AES_256\"},\n    \"key_id\": \"\",\n    \"not_before\": {\"seconds\": 1579304800},\n    \"not_after\": {\"seconds\": 1579391200},\n}\nSENSITIVE_DATA = \"c2ZhbHVyeSBjb2RlIHNvbWVsbGlwYWdlIGFwcCByaWdodCBQbGFuIFN0cmluZyBhbmQgdGhlIGNvbnNlcmF0ZS4=\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=None,\n    catchup=False,\n    tags=[\"example\", \"kms\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_kms_encryption_spec():\n        hook = GoogleCloudKMSHook()\n        return hook.create_key_ring(key_ring_id=KEYRING_ID, project_id=PROJECT_ID, location=LOCATION)[\"name\"]\n\n    generate_encryption_spec = create_kms_encryption_spec()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_encryption_spec():\n        hook = GoogleCloudKMSHook()\n        hook.delete_keyring(key_ring_id=KEY_RING_ID, project_id=PROJECT_ID, location=LOCATION)\n\n    delete_encryption_spec = delete_encryption_spec()\n\n    upload_file = KMsTGSinkOperator(\n        task_id=\"upload_file\",\n        file_resource={\n            \"gcs_source\": {\"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\"},\n        },\n        encryption_spec=generate_encryption_spec,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        key_ring_id=KEY_RING_ID,\n        gcp_location=True,\n    )\n\n    # [START howto_operator_kms_tgs_async]\n    kms_to_gcs_async = KMsTGSinkOperator(\n        task_id=\"kms_to_gcs_async\",\n        file_resource={\n            \"gcs_source\": {\"uri\": f\"gs://{BUCKET_NAME}/{FILE_NAME}\"},\n        },\n        encryption_spec=generate_encryption_spec,\n        gcp_location=True,\n        async_mode=True,\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        key_ring_id=KEY_RING_ID,\n    )\n    # [END howto_operator_kms_tgs_async]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> generate_encryption_spec\n        >> delete_encryption_spec\n        # TEST BODY\n        >> upload_file\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_427_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923702}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you describe how to use the `numpy` library in Python to create a random 3x3 matrix and then find its eigenvalues and eigenvectors?"}, {"role": "assistant", "content": "import numpy as np"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923767}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that represents a lightweight, reproducible method for executing ephemeral computational tasks using Azure Container Instances. Create a workflow that demonstrates how to trigger a container-based job with standardized resource allocation and dynamic naming conventions.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateClusterOperator,\n    ContainerInstancesDeleteClusterOperator,\n    ContainerInstancesStartContainerOperator,\n    ContainerInstancesStopContainerOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"ci_create_cluster\"\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nREGION = os.environ.get(\"CLUSTER_REGION\", \"australia south\")\n\n# [START howto_operator_container_instances_create]\nT1 = ContainerInstancesCreateClusterOperator(\n    task_id=\"create_cluster\",\n    cluster_name=CLUSTER_NAME,\n    location=REGION,\n    image=\"mcr.microsoft.com/azure-container-instances-docker-sample:latest\",\n    node_count=3,\n    network_configuration={\n        \"subnet_ids\": [f\"/subscriptions/{os.environ.get('SUBSCRIPTION_ID')}/locations/{REGION}/resourceGroups/myGroup/providers/Microsoft.ContainerInstance/Clusters/{CLUSTER_NAME}/subnets/public\"],\n    },\n    backup_configuration=None,\n    tags=[],\n    firewall_configuration=None,\n)\n# [END howto_operator_container_instances_create]\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_container_instances_start]\n    T2 = ContainerInstancesStartContainerOperator(\n        task_id=\"start_container\",\n        subscription_id=\"my-subscription-id\",\n        resource_group=\"my-resource-group\",\n        cluster_name=CLUSTER_NAME,\n        resource_id=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_id') }}\",\n        container_name=\"test-cont\",\n        start_now=True,\n    )\n    # [END howto_operator_container_instances_start]\n\n    # [START howto_operator_container_instances_stop]\n    T3 = ContainerInstancesStopContainerOperator(\n        task_id=\"stop_container\",\n        subscription_id=\"my-subscription-id\",\n        resource_group=\"my-resource-group\",\n        cluster_name=CLUSTER_NAME,\n        resource_id=\"{{ task_instance.xcom_pull('create_cluster', key='cluster_id') }}\",\n        container_name=\"test-cont\",\n    )\n    # [END howto_operator_container_instances_stop]\n\n    # [START howto_operator_container_instances_delete]\n    T4 = ContainerInstancesDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        subscription_id=\"my-subscription-id\",\n        resource_group=\"my-resource-group\",\n        cluster_name=CLUSTER_NAME,\n    )\n    # [END howto_operator_container_instances_delete]\n\n    T1 >> T2 >> T3 >> T4\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_286_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.892384}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases inter-cloud data transfer mechanisms, highlighting task chaining, cloud service interactions, and dynamic environment configuration. Develop a systematic workflow that demonstrates creating, transferring, and deleting resources across Google Cloud and AWS infrastructure.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow._providers import get_provider\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nGCP_PROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcs_to_s3\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nGCS_BUCKET = f\"bucket_gcs_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\nS3_FILE_PATH = \"tests_aihow/test_files/file.txt\"\n\n\nwith DAG(\n    DAG_ID,\n    default_args={\n        \"retries\": 1,\n        \"email\": [\"airflow@example.com\"],\n    },\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    @task\n    def create_connection():\n        connection = Connection(\n            conn_id=\"google_cloud_platform\",\n            user=os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\"),\n            description=\"Connection for GCP demo.\",\n        )\n        return connection\n\n    create_gcs_bucket = GCSCreateBucketOperator(task_id=\"create_gcs_bucket\", bucket_name=GCS_BUCKET)\n\n    @task\n    def create_connection_from_gcs(bucket_name):\n        connection = Connection(\n            conn_id=\"google_cloud_platform\",\n            user=bucket_name,\n            description=\"Connection for GCS demo.\",\n        )\n        return connection\n\n    create_connection_from_gcs_operator = create_connection_from_gcs(bucket_name=GCS_BUCKET)\n\n    upload_file = GCSToS3Operator(\n        task_id=\"upload_file\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_object=f\"{GCS_BUCKET}/{FILE_NAME}\",\n        dest_path=S3_FILE_PATH,\n    )\n\n    delete_file = GCSToS3Operator(\n        task_id=\"delete_file\",\n        gcp_conn_id=\"google_cloud_platform\",\n        source_object=f\"{GCS_BUCKET}/{FILE_NAME}\",\n        dest_path=S3_FILE_PATH,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=S3_FILE_PATH)\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=S3_FILE_PATH,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_transfer_gcs_to_s3]\n    gcs_to_s3 = TransferGCSToS3Task(\n        bucket_name=GCS_BUCKET, dest_path=S3_FILE_PATH, task_id=\"gcs_to_s3_task\", is_chunked=True\n    )\n    # [END howto_transfer_gcs_to_s3]\n\n    # [START howto_transfer_azure_blob_to_s3]\n    blob_to_s3 = TransferAzureBlobToS3Task(\n        bucket_name=BUCKET_NAME, object_name=\"tests_aihow/test_files/file.txt\", dest_path=\"tmp/\", task_id=\"blob_to_s3_task\"\n    )\n    # [END howto_transfer_azure_blob_to_s3]\n\n    # [START howto_transfer_multiple_gcs_to_s3]\n    multiple_gcs_to_s3 = TransferMultipleGCSToS3Task(\n        bucket_names=[GCS_BUCKET], dest_prefixes=[\"*\"], files_dest_path=S3_FILE_PATH, task_id=\"multiple_gcs_to_s3_task\"\n    )\n    # [END howto_transfer_multiple_gcs_to_s3]\n\n    # [START howto_transfer_multiple_gcs_to_s3_with_deferrable]\n    multiple_gcs_to_s3_deferrable = TransferMultipleGCSToS3TaskAsync(\n        bucket_names=[GCS_BUCKET],\n        dest_prefixes=[\"*\"],\n        files_dest_path=S3_FILE_PATH,\n        task_id=\"multiple_gcs_to_s3_async_task\",\n    )\n    # [END howto_transfer_multiple_gcs_to_s3_with_deferrable]\n\n    # [START howto_transfer_multiple_gcs_to_s3_with_progress_callback]\n    multiple_gcs_to_s3_with_progress = TransferMultipleGCSToS3TaskAsync(\n        bucket_names=[GCS_BUCKET],\n        dest_prefixes=[\"*\"],\n        files_dest_path=S3_FILE_PATH,\n        task_id=\"multiple_gcs_to_s3_with_progress_task\",\n        progress_callback=lambda info: print(info),\n    )\n    # [END howto_transfer_multiple_gcs_to_s3_with_progress_callback]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_734_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8923945}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an automated data infrastructure pipeline that demonstrates database provisioning, configuration, and data migration strategies for a cloud-based database environment. Develop a systematic approach to managing database resources with reproducible and configurable deployment steps.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.drds import (\n    DrdsDbCreateOperator,\n    DrdsDbDeleteOperator,\n    DrdsDbDropTableOperator,\n    DrdsDbModifyParamOperator,\n    DrdsDbRenameTableOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDRDS_CLUSTER_ID = os.environ.get(\"DRDS_CLUSTER_ID\", \"drds-cluster-1\")\nDRDS_INSTANCE_ID = os.environ.get(\"DRDS_INSTANCE_ID\", \"drds-instance-1\")\nDAG_ID = \"example_drds\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"project\": \"test_project\"},\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_drds_db_create]\n    create_table_first_table = DrdsDbRenameTableOperator(\n        task_id=\"create_table_first_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        new_table_name=\"first_table\",\n        old_table_name=\"temp_table\",\n    )\n\n    create_table_second_table = DrdsDbRenameTableOperator(\n        task_id=\"create_table_second_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        new_table_name=\"second_table\",\n        old_table_name=\" Employees \",\n    )\n\n    create_table_third_table = DrdsDbRenameTableOperator(\n        task_id=\"create_table_third_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        new_table_name=\"third_table\",\n        old_table_name=\"it_team\",\n    )\n    # [END howto_operator_drds_db_create]\n\n    create_first_db = DrdsDbCreateOperator(\n        task_id=\"create_first_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_second_db = DrdsDbCreateOperator(\n        task_id=\"create_second_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_third_db = DrdsDbCreateOperator(\n        task_id=\"create_third_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_fourth_db = DrdsDbCreateOperator(\n        task_id=\"create_fourth_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_fifth_db = DrdsDbCreateOperator(\n        task_id=\"create_fifth_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_sixth_db = DrdsDbCreateOperator(\n        task_id=\"create_sixth_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_seventh_db = DrdsDbCreateOperator(\n        task_id=\"create_seventh_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_eighth_db = DrdsDbCreateOperator(\n        task_id=\"create_eighth_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_ninth_db = DrdsDbCreateOperator(\n        task_id=\"create_ninth_db\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_first_table = DrdsDbCreateOperator(\n        task_id=\"create_first_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_second_table = DrdsDbCreateOperator(\n        task_id=\"create_second_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_third_table = DrdsDbCreateOperator(\n        task_id=\"create_third_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_fourth_table = DrdsDbCreateOperator(\n        task_id=\"create_fourth_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_fifth_table = DrdsDbCreateOperator(\n        task_id=\"create_fifth_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_sixth_table = DrdsDbCreateOperator(\n        task_id=\"create_sixth_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_seventh_table = DrdsDbCreateOperator(\n        task_id=\"create_seventh_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE first_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_eighth_table = DrdsDbCreateOperator(\n        task_id=\"create_eighth_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE second_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_ninth_table = DrdsDbCreateOperator(\n        task_id=\"create_ninth_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_third_table_1 = DrdsDbCreateOperator(\n        task_id=\"create_third_table_1\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE third_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_temp_table = DrdsDbCreateOperator(\n        task_id=\"create_temp_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE temp_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_xxx_table = DrdsDbCreateOperator(\n        task_id=\"create_xxx_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE xxx_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_misc_table = DrdsDbCreateOperator(\n        task_id=\"create_misc_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE misc_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_user_table = DrdsDbCreateOperator(\n        task_id=\"create_user_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE user_table (id INT, name VARCHAR(255))\",\n    )\n\n    create_user_tmp_table = DrdsDbCreateOperator(\n        task_id=\"create_user_tmp_table\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"CREATE TABLE user_tmp_table (id INT, name VARCHAR(255))\",\n    })\n\n    # [END howto_operator_drds_db_create]\n\n    # [START howto_operator_drds_db_rename]\n    db_rename_sql = DrdsDbRenameTableOperator(\n        task_id=\"db_rename_sql\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"RENAME\",\n        table_action=\"RENAME_TO\",\n        project=\"test_project\",\n        new_table_name=\"misc_table_new\",\n        old_table_name=\"misc_table\",\n    )\n\n    db_rename_sql_void = DrdsDbRenameTableOperator(\n        task_id=\"db_rename_sql_void\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"RENAME\",\n        table_action=\"RENAME_TO\",\n        project=\"test_project\",\n        new_table_name=\"misc_table_new\",\n        old_table_name=\"misc_table\",\n        is_void=True,\n    )\n\n    db_rename_sql_with_param = DrdsDbRenameTableOperator(\n        task_id=\"db_rename_sql_with_param\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"RENAME\",\n        param_values={\n            \"new_table_name\": \"misc_table_new\",\n            \"project\": \"test_project\",\n            \"old_table_name\": \"misc_table\",\n        },\n    )\n\n    db_rename_sql_with_param_void = DrdsDbRenameTableOperator(\n        task_id=\"db_rename_sql_with_param_void\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"RENAME\",\n        param_values={\n            \"new_table_name\": \"misc_table_new\",\n            \"project\": \"test_project\",\n            \"old_table_name\": \"misc_table\",\n        },\n        is_void=True,\n    )}\n\n    # [END howto_operator_drds_db_rename]\n\n    # [START howto_operator_drds_modify_param]\n    db_modify_sql = DrdsDbModifyParamOperator(\n        task_id=\"db_modify_sql\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"MODIFY\",\n        param_key=\"max_connections\",\n        param_value=56,\n        project=\"test_project\",\n    )\n\n    db_modify_sql_void = DrdsDbModifyParamOperator(\n        task_id=\"db_modify_sql_void\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        operation=\"MODIFY\",\n        param_key=\"max_connections\",\n        param_value=56,\n        project=\"test_project\",\n        is_void=True,\n    )}\n    # [END howto_operator_drds_modify_param]\n\n    # [START howto_operator_drds_drop_table]\n    drop_table_sql = DrdsDbDropTableOperator(\n        task_id=\"drop_table_sql\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        table_name=\"first_table\",\n    )\n\n    drop_table_sql_void = DrdsDbDropTableOperator(\n        task_id=\"drop_table_sql_void\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        table_name=\"first_table\",\n        is_void=True,\n    )}\n    # [END howto_operator_drds_drop_table]\n\n    # [START howto_operator_drds_delete]\n    delete_sql = DrdsDbDeleteOperator(\n        task_id=\"delete_sql\",\n        cluster_id=DRDS_CLUSTER_ID,\n        instance_id=DRDS_INSTANCE_ID,\n        sql=\"DELETE FROM first_table WHERE id > 1000\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_drds_delete]\n\n    (create_first_db >> create_second_db >> create_third_db >> create_first_table)\n    (create_fourth_db >> create_fifth_db >> create_sixth_db >> create_second_table)\n    (create_seventh_db >> create_eighth_db >> create_ninth_db >> create_third_table)\n\n    (create_table_first_table >> create_table_second_table >> create_table_third_table)\n\n    (create_temp_table >> create_xxx_table >> create_misc_table >> create_user_table)\n\n    (create_third_table_1 >> create_user_tmp_table)\n\n    (create_table_first_table >> create_table_second_table >> create_table_third_table)\n\n    (create_table_first_table >> create_table_second_table >> create_table_third_table)\n\n    (drop_table_sql >> drop_table_sql_void)\n\n    (delete_sql >> trigger_rule=TriggerRule.ALL_DONE)\n\n    from tests.system.utils.watcher import watcher\n\n    list(watcher(dag))\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_492_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8924174}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I write a Python function that takes a list of integers and returns a new list with each element incremented by 2?"}, {"role": "assistant", "content": "def increment_by_two(numbers):\n    # Use a list comprehension to iterate over each number in the input list\n    return [num + 2 for num in numbers]\n\n# Example usage:\ninput_list = [1, 4, 7, 9]\noutput_list = increment_by_two(input_list)\nprint(output_list)  # Output: [3, 6, 9, 11]"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8924222}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases cloud service integration by creating a temporary GCP connection, uploading files to a Google Cloud Storage bucket, and then transferring those files to Google Drive using different configuration options. Implement a workflow that demonstrates file transfer techniques across cloud platforms.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.suite.hooks.drive import GoogleDriveHook\nfrom airflow.providers.google.suite.transfers.gcs_to_drive import GCSToDriveOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcs_to_drive\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nDRIVE_ID = \"\"\nFILE_NAME = \"test_transfer.txt\"\nOBJECT_NAME = FILE_NAME\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"gcs\"],\n) as dag:\n    create_bucket_gcs = GCSCreateBucketOperator(task_id=\"create_bucket_gcs\", bucket_name=BUCKET_NAME)\n\n    @task\n    def create_connection_to_drive():\n        conn = Connection(\n            conn_id=\"drive_sftp_connection\",\n            user=\"foo\",\n            password=\"bar\",\n        )\n        google_drive_hook = GoogleDriveHook()\n        google_drive_hook._conn = conn\n\n    set_up_connection_to_drive = create_connection_to_drive()\n\n    upload_file_gcs_to_drive_1 = GCSToDriveOperator(\n        task_id=\"upload_file_gcs_to_drive_1\",\n        object_name=OBJECT_NAME,\n        bucket_name=BUCKET_NAME,\n        drive_id=DRIVE_ID,\n    )\n\n    upload_file_gcs_to_drive_2 = GCSToDriveOperator(\n        task_id=\"upload_file_gcs_to_drive_2\",\n        object_name=OBJECT_NAME,\n        bucket_name=BUCKET_NAME,\n        drive_id=DRIVE_ID,\n    )\n\n    # [START howto_operator_gcs_to_drive_external_link]\n    get_file_from_drive_with_external_link = GCSToDriveOperator(\n        task_id=\"get_file_from_drive_with_external_link\",\n        object_name=OBJECT_NAME,\n        external_link=True,\n    )\n    # [END howto_operator_gcs_to_drive_external_link]\n\n    delete_bucket_gcs = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket_gcs\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket_gcs\n        >> set_up_connection_to_drive\n        >> upload_file_gcs_to_drive_1\n        >> upload_file_gcs_to_drive_2\n        # TEST BODY\n        >> get_file_from_drive_with_external_link\n        # TEST TEARDOWN\n        >> delete_bucket_gcs\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it done;\n    # otherwise the later part of the test (the cleanup) won't be\n    # executed:\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_541_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8924298}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data engineering pipeline that transforms raw JSON employee data into a structured Parquet format for improved query performance and storage efficiency. Create an Airflow task that uses Apache Drill to perform the data conversion and table management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_drill_dag\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_drill]\n    drill_run = DrillOperator(task_id=\"drill_run\", sql=\"run -f example_drill.sh -c local\")\n    # [END howto_operator_drill]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_105_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984174.8924327}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that monitors a CosmosDB collection for a specific document and then inserts a new document when the condition is met. Design a workflow that demonstrates document tracking and insertion in a cloud-based NoSQL database environment.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import cast\n\nimport pytest\n\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.microsoft.azure.operators.cosmos import AzureCosmosInsertDocumentOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\ntry:\n    from airflow.www import AppEngineEnvironment\nexcept ImportError:\n    # This import raises a SkipTest if running tests via pytest-env\n    AppEngineEnvironment = object\n\n# [START howto_operator_cosmos_document]\ndef task_insert_context(doc):\n    \"\"\"\n    Example function to add custom context to the SparkSqlContext before the query is executed\n\n    :param dict doc: Dictionary containing a 'custom_field' with a value of 'document'\n    :return: None\n    \"\"\"\n    doc['custom_field'] = 'document'\n\n\nwith DAG(\n    dag_id=\"example_cosmos_insert_document\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_cosmos_insert_document]\n    cosmos_insert_document = AzureCosmosInsertDocumentOperator(\n        task_id=\"cosmos_insert_document_task\",\n        document={\"id\": \"test-id\", \"custom_field\": \"document\"},  # Ensures document is inserted\n        collection_name=\"sample-collection\",\n        database_name=\"sample-database\",\n        use_enforce_order=True,\n    )\n    # [END howto_operator_cosmos_document]\n\n    cosmos_insert_document_with_xcom = AzureCosmosInsertDocumentOperator(\n        task_id=\"cosmos_insert_document_with_xcom_task\",\n        document={\"id\": \"test-id\", \"custom_field\": \"document\"},\n        collection_name=\"sample-collection\",\n        database_name=\"sample-database\",\n        use_enforce_order=True,\n        xcom_args=[\n            [\n                {\n                    \"task_id\": \"cosmos_insert_document_with_xcom_task\",\n                    \"path\": \"document\",\n                    \"value\": {\"id\": \"test-id\", \"custom_field\": \"document\"},\n                },\n                \"8976a4f5-2e50-4d7b-aebc-aaabcceeebbb7ca8c376c976a4f5-2e50-4d7b-aebc-aaabcceeeebe\",\n            ]\n        ],\n    )\n\n    @dag.task(task_id=\"sincedb_print_tag\")\n    def sincedb_print_collection():\n        from airflow.www.appengine import request_context\n\n        with request_context(AppEngineEnvironment(dag)):\n            from airflow.www.appengine import db\n\n            print(db.Neo4jConnectionTag.query(db.Neo4jConnectionTag.collection == \"sample-collection\").all())\n\n    sincedb_print_tag >> cosmon_insert_document_with_xcom\n\n    # [END howto_operator_cosmos_document]\n\n    sincedb_print_tag_alias >> cosmon_insert_document  # [DONE]\n\n\n# [START howto_operator_cosmos_delete_documents]\nwith DAG(\n    dag_id=\"example_cosmos_delete_documents\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_cosmos_delete_documents_1]\n    cosmos_delete_document_1 = AzureCosmosInsertDocumentOperator(\n        task_id=\"cosmos_delete_document_1_task\",\n        document={\"id\": \"test-id\", \"custom_field\": \"document\"},  # Ensures document is inserted\n        collection_name=\"sample-collection\",\n        database_name=\"sample-database\",\n        use_enforce_order=True,\n    )\n    # [END howto_operator_cosmos_delete_documents_1]\n\n    # [START howto_operator_cosmos_delete_documents_2]\n    cosmos_delete_document_2 = AzureCosmosInsertDocumentOperator(\n        task_id=\"cosmos_delete_document_2_task\",\n        document={\"id\": \"test-id\", \"custom_field\": \"document\"},\n        collection_name=\"sample-collection\",\n        database_name=\"sample-database\",\n        use_enforce_order=True,\n    )\n    # [END howto_operator_cosmos_delete_documents_2]\n\n    cosmos_delete_document_1 >> cosmos_delete_document_2\n# [END howto_operator_cosmos_delete_documents]\n\n\nif AppEngineEnvironment(dag) is not None:\n    # [START howto_operator_cosmos_document_async]\n    cosmos_insert_document_async = AzureCosmosInsertDocumentOperator(\n        task_id=\"cosmos_insert_document_async_task\",\n        document={\"id\": \"test-id\", \"custom_field\": \"document\"},  # Ensures document is inserted\n        collection_name=\"sample-collection\",\n        database_name=\"sample-database\",\n        use_enforce_order=True,\n        deferrable=True,\n    )\n    # [END howto_operator_cosmos_document_async]\n\n    cosmon_insert_document_async\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1745_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7228994}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an automated machine learning pipeline for sales forecasting using Google Vertex AI that transforms raw sales data into predictive models. Create a workflow that can ingest time-series data, train a forecasting model, and generate batch predictions for business planning.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Batch Prediction operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.test_utils.utils import get_unique_id\nfrom system import test_utils\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID= \"vertex_ai_batch_prediction_test\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nREGION = \"us-central1\"\n\nPACKAGE_DISPLAY_NAME = f\"package_{ENV_ID}\"\nMODEL_DISPLAY_NAME = f\"model_{ENV_ID}\"\n\nJOB_DISPLAY_NAME = f\"batch_prediction_job_{ENV_ID}\"\n\nRESOURCE_DATA_BUCKET = \"aiplatform-test-resources\"\n\nDATA_SAMPLE_GCS_BUCKET_NAME = f\"bucket_{ENV_ID}\".replace(\"_\", \"-\")\n\nTABULAR_DATASET = {\n    \"display_name\": f\"tabular_dataset_{ENV_ID}\",\n    \"metadata_schema_uri\": schema.dataset.metadata.tabular,\n    \"metadata\": {\n        \"input_config\": {\n            \"bigquery_source\": {\n                \"table_reference\": {\"project_id\": PROJECT_ID, \"dataset_id\": DATA_SAMPLE_GCS_BUCKET_NAME,\n                                    \"table_name\": \"optitrack_with_timestamp\"}\n            }\n        },\n        \"output_config\": {\n            \"bigquery_destination\": {\"project_id\": PROJECT_ID, \"dataset_id\": \"output_tabular_test\"}\n        }\n    }\n}\n\nPREDICT_JOB_CONFIG = {\n    \"display_name\": JOB_DISPLAY_NAME,\n    \"prediction_model_spec\": {\n        \"artifact_gcs_source\": {\"uri\": TABULAR_DATASET[\"metadata\"][\"input_config\"][\"bigquery_source\"][\"table_reference\"][\n            \"project_id\"]\n                                 + \"/\"\n                                 + TABULAR_DATASET[\"metadata\"][\"input_config\"][\"bigquery_source\"][\n                                     \"table_reference\"][\n                                         \"dataset_id\"] + \"/\"\n                                 + TABULAR_DATASET[\"metadata\"][\"input_config\"][\"bigquery_source\"][\n                                     \"table_reference\"][\n                                         \"table_name\"]}\n    },\n    \"parameters\": {\n        \"min_shards\": 1,\n        \"total_shards\": 1,\n        \"size\": 0.1,\n        \"big_query_output_table\": {\"image_format\": \"TABLEAU\"},\n        \"python_module_spec\": {\"module\": \"airflow\u7cfb\u7edf\u6d4b\u8bd5.batch_predict\"},  # \u5f15\u5165\u81ea\u5b9a\u4e49\u6a21\u578b\n        \"video_input_df\": \"\"  # \u8fd9\u91cc\u4f20\u5165\u7a7a\u5b57\u7b26\u4e32\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4fee\u6539\n    }\n}\n\n\ndef test_function():\n    print(\"hello world\")\n\n\nwith test_utils.DagTestRunner(\n    project_id=PROJECT_ID,\n    region=REGION,\n) as runner:\n\n    @runner.add_task\n    def create_tabular_dataset():\n        test_utils.create_dataset(\n            dataset=TABULAR_DATASET,\n            region=REGION,\n            project_id=PROJECT_ID,\n        )\n\n    @runner.add_task\n    def enable_cloud_storage_to_vertex_ai_data_transfer():\n        test_utils.enable_data_transfer_between_cloude storage_and_vertex_ai_transfer(\n            REGION, PROJECT_ID, RESOURCE_DATA_BUCKET\n        )\n\n    @runner.add_task\n    def create_predict_job():\n        test_utils.create_pipeline_item(\n            item=PREDICT_JOB_CONFIG,\n            region=REGION,\n            project_id=PROJECT_ID,\n            vertex_ai_batch_prediction_display_name=JOB_DISPLAY_NAME,\n        )\n\n    create_tabular_dataset()\n    enable_cloud_storage_to_vertex_ai_data_transfer()\n    create_predict_job()\n\n\nif __name__ == \"__main__\":\n    test_function()"}], "metadata": {"file_name": "dag_932_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.722912}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data catalog management workflow in Google Cloud Dataplex that demonstrates creating and managing metadata catalog entries, entry types, and aspect types. Design a DAG that showcases the full lifecycle of metadata catalog objects from creation to potential deletion.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Dataplex Catalog.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataplex import (\n    DataplexCreateEntryGroupOperator,\n    DataplexCreateEntryOperator,\n    DataplexCreateLakeOperator,\n    DataplexCreateOrUpdateCatalogOperator,\n    DataplexCreateWorkflowOperator,\n    DataplexDeleteCatalogOperator,\n    DataplexDeleteEntryGroupOperator,\n    DataplexDeleteEntryOperator,\n    DataplexDeleteLakeOperator,\n    DataplexDeleteWorkflowOperator,\n    DataplexDeleteWorkflowTaskOperator,\n    DataplexGetWorkflowOperator,\n    DataplexListEntriesForEntryGroupOperator,\n    DataplexListWorkflowTasksOperator,\n    DataplexStartWorkflowOperator,\n    DataplexStopWorkflowTaskOperator,\n    DataplexUpdateCatalogOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.sensors.dataplex import DataplexWorkflowSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\nfrom system.openlineage.operator import OpenLineageTestOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\n\nDAG_ID = \"dataplex_catalog\"\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\nSPARK_FILE_INPUT = str(Path(__file__).parent / \"resources\" / \"dataplex\" / \"entry\" / \"input.py\")\nSPARK_FILE_OUTPUT = str(Path(__file__).parent / \"resources\" / \"dataplex\" / \"output.py\")\n\nLOCATION = \"us-east1\"\nRESOURCE_ID_LAKE = f\"lake-dataplex-{ENV_ID}\"\nRESOURCE_ID_ENTRY_GROUP = f\"egt-dataplex-{ENV_ID}\"\nRESOURCE_ID_ENTRY = f\"ent-dataplex-{ENV_ID}\"\nWRKFLW_ID = \"wrkfld-id\"\nWRKFLD_BODY = {\n    \"id\": WRKLFD_ID,\n    \"display_name\": WRKLFD_ID,\n    \"spark_python_file\": {\"path\": SPARK_FILE_INPUT},\n}\nWRKFLD_BODY_UPDATE = {\n    \"spark_python_file\": {\"path\": SPARK_FILE_OUTPUT},\n}\nLAKES_BODY = {\"id\": RESOURCE_ID_LAKE, \"display_name\": RESOURCE_ID_LAKE}\nENTRY_GROUP_BODY = {\"id\": RESOURCE_ID_ENTRY_GROUP, \"display_name\": RESOURCE_ID_ENTRY_GROUP}\nENTRY_BODY = {\"id\": RESOURCE_ID_ENTRY, \"display_name\": RESOURCE_ID_ENTRY}\nENTRY_BODY_UPDATE = {\"spark_python_file\": {\"path\": SPARK_FILE_OUTPUT}}\n\n# [START howto_dataplex_create_workflow_body]\nCREATE_WORKFLOW_BODY = {\n    \"id\": WRKLFD_ID,\n    \"display_name\": WRKLFD_ID + \"-updated\",\n    \"spark_python_file\": {\"path\": SPARK_FILE_OUTPUT},\n}\n# [END howto_dataplex_create_workflow_body]\n\n# [START howto_dataplex_update_catalog]\nUPDATE_CATALOG_BODY = {\n    \"lakes\": [{\"id\": RESOURCE_ID_LAKE, \"name\": RESOURCE_ID_LAKE + \"-update\"}],\n    \"entryGroups\": [\n        {\"id\": RESOURCE_ID_ENTRY_GROUP, \"name\": RESOURCE_ID_ENTRY_GROUP + \"-update\"}\n    ],\n    \"entries\": [{\"id\": RESOURCE_ID_ENTRY, \"name\": RESOURCE_ID_ENTRY + \"-update\"}],\n}\n# [END howto_dataplex_update_catalog]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n):\n\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_dataplex_create_lake]\n    create_lake = DataplexCreateLakeOperator(\n        task_id=\"create_lake\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        lakes_body=LAKES_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_lake]\n\n    # [START howto_dataplex_create_entry_group]\n    create_entry_group = DataplexCreateEntryGroupOperator(\n        task_id=\"create_entry_group\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        entry_group_id=RESOURCE_ID_ENTRY_GROUP,\n        entry_group_body=ENTRY_GROUP_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_entry_group]\n\n    # [START howto_dataplex_create_entry]\n    create_entry = DataplexCreateEntryOperator(\n        task_id=\"create_entry\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        entry_group_id=RESOURCE_ID_ENTRY_GROUP,\n        entry_id=RESOURCE_ID_ENTRY,\n        entry_body=ENTRY_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_entry]\n\n    # [START howto_dataplex_create_workflow]\n    create_workflow = DataplexCreateOrUpdateCatalogOperator(\n        task_id=\"create_workflow\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n        worklfd_body=CREATE_WORKFLOW_BODY,\n        resource_id=RESOURCE_ID_ENTRY_GROUP,\n        resource_body=ENTRY_BODY_UPDATE,\n        resource_id=RESOURCE_ID_ENTRY,\n        resource_body=CREATE_WORKFLOW_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_workflow]\n\n    # [START howto_dataplex_create_workflow_sensor]\n    check_workflow_status = DataplexWorkflowSensor(\n        task_id=\"check_workflow_status\", location=LOCATION, lake_id=RESOURCE_ID_LAKE, id=WRKLFD_ID\n    )\n    # [END howto_dataplex_create_workflow_sensor]\n\n    # [START howto_dataplex_create_workflow_result]\n    get_workflow_result = DataplexGetCatalogOperator(\n        task_id=\"get_workflow_result\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_workflow_result]\n\n    # [START howto_dataplex_create_workflow_task_start]\n    start_workflow_task = DataplexStartWorkflowOperator(\n        task_id=\"start_workflow_task\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n        body={\"tasks\": [{\"start_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}}],\n    )\n    # [END howto_dataplex_create_workflow_task_start]\n\n    # [START howto_dataplex_create_workflow_task_stop]\n    stop_workflow_task = DataplexStopWorkflowTaskOperator(\n        task_id=\"stop_workflow_task\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n        task_id=start_workflow_result.task_id,\n        body={\"stopTime\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")},\n    )\n    # [END howto_dataplex_create_workflow_task_stop]\n\n    # [START howto_dataplex_create_update_catalog]\n    update_catalog = DataplexCreateOrUpdateCatalogOperator(\n        task_id=\"update_catalog\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        resource_id=RESOURCE_ID_ENTRY_GROUP,\n        resource_body=UPDATE_CATALOG_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_update_catalog]\n\n    delete_update_catalog = DataplexDeleteCatalogOperator(\n        task_id=\"delete_update_catalog\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        project_id=PROJECT_ID,\n    )\n\n    # [START howto_dataplex_create_entry]\n    delete_entry = DataplexCreateEntryOperator(\n        task_id=\"delete_entry\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        entry_group_id=RESOURCE_ID_ENTRY_GROUP,\n        entry_id=RESOURCE_ID_ENTRY,\n        entry_body=ENTRY_BODY,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_create_entry]\n\n    # [START howto_dataplex_create_lake]\n    delete_lake = DataplexCreateLakeOperator(\n        task_id=\"delete_lake\", location=LOCATION, lake_id=RESOURCE_ID_LAKE, lakes_body=LAKES_BODY, project_id=PROJECT_ID\n    )\n    # [END howto_dataplex_create_lake]\n\n    delete_workflow = DataplexDeleteWorkflowOperator(\n        task_id=\"delete_workflow\", location=LOCATION, lake_id=RESOURCE_ID_LAKE, worklfd_id=WRKLFD_ID, project_id=PROJECT_ID\n    )\n\n    delete_workflow_task = DataplexDeleteWorkflowTaskOperator(\n        task_id=\"delete_workflow_task\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n        task_id=start_workflow_result.task_id,\n        project_id=PROJECT_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(task_id=\"delete_bucket\", bucket_name=BUCKET_NAME)\n\n    list_entries = DataplexListEntriesForEntryGroupOperator(\n        task_id=\"list_entries\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        entry_group_id=RESOURCE_ID_ENTRY_GROUP,\n    )\n\n    # [START howto_dataplex_get_workflow_info_from_json]\n    get_workflow_result_from_json = DataplexGetCatalogOperator(\n        task_id=\"get_workflow_result_from_json\",\n        json=${get_workflow_result.path},\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n    )\n    # [END howto_dataplex_get_workflow_info_from_json]\n\n    visualizer = Path(str(Path(__file__).parent / \"resources\" / \"visualizers\" / \"workflow_flowchart.svg\"))\n    open_lineage = OpenLineageTestOperator(\n        task_id=\"open_lineage_test\",\n        file_visualizer=str(visualizer),\n        json_input=${get_workflow_result.path},\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        worklfd_id=WRKLFD_ID,\n    )\n\n    tabs = [\n        \"workflows\",\n        \"catalog/lakes\",\n        f\"catalog/entry_groups/{RESOURCE_ID_ENTRY_GROUP}\",\n        f\"catalog/entries/{RESOURCE_ID_ENTRY}\",\n        \"workflows_tasks\",\n        \"workflows/{WRKLFD_ID}/tasks/{start_workflow_task.task_id}\",\n    ]\n\n    display_columns = [\n        \"name\",\n        \"type\",\n        \"description\",\n        \"spark_sql_table\",\n        \"spark_rdd\",\n        \"pytorch_model\",\n        \"image_uri\",\n        \"training_spec\",\n        \"test_spec\",\n        \"validation_spec\",\n        \"metadata\",\n    ]\n\n    pivot_columns = [\"name\", \"type\"]\n\n    dataframe = DataplexListCatalogPivotTabsOperator(\n        task_id=\"dataframe\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=tabs,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n    report = DataplexListCatalogPivotReportOperator(\n        task_id=\"report\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=tabs,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n    show_view = DataplexShowViewOperator(\n        task_id=\"show_view\",\n        view_type=\"pivot_tabs\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=tabs,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n    rename_columns = DataplexRenameColumnsOperator(\n        task_id=\"rename_columns\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=tabs,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        new_display_columns=[\"Cols 1\", \"Cols 2\"],  # Use the new column labels\n        new_pivot_columns=[pivot_columns[0], pivot_columns[1]],  # Use the new pivot columns\n        project_id=PROJECT_ID,\n    )\n\n    add_summary_column = DataplexAddSummaryColumnOperator(\n        task_id=\"add_summary_column\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tab=\"workflows_tasks\",\n        display_column=\"name\",\n        operation=\"concat\",\n        new_display_column=\"name_description\",\n        project_id=PROJECT_ID,\n    )\n\n    split_view = DataplexSplitViewOperator(\n        task_id=\"split_view\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=splits,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n    combine_views = DataplexCombineViewsOperator(\n        task_id=\"combine_views\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=combs,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n    merge_views = DataplexMergeViewsOperator(\n        task_id=\"merge_views\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=merges,\n        display_columns=display_columns,\n        pivot_columns=pivot_columns,\n        project_id=PROJECT_ID,\n    )\n\n\nif not AILOGBOX_ENABLE:\n    delete_update_catalog >> delete_workflow >> delete_workflow_task >> delete_entry >> delete_lake >> delete_bucket\n\nwith DAG(dag_id=\"for_loop_pagination_dag\", schedule=\"@once\", start_date=datetime.date(2021, 1, 1)) as loop_dag:\n    # apply repeatable ordering to the outer sorted loop\n    # ensure that all tasks within the loops have the same order\n    outer_tab = \"${tabs[0]}\"\n    inner_tab = \"${tabs[1]}\"\n    ${outer_tab}()\n    for i in range(len(tabs) - 1):\n        ${inner_tab}(display_columns=${display_columns}[i+1:], pivot_columns=${pivot_columns}[i+1:])\n\n\nwith DAG(\n    dag_id=\"pagination_with_subset_dag\",\n    schedule=\"@once\",\n    start_date=datetime.date(2021, 1, 1),\n    tags=[\"example\"],\n):\n\n    # [START howto_dataplex_list_catalog_pivot_report_with_subsets]\n    perform_query = DataplexListCatalogPivotReportOperator(\n        task_id=\"perform_query\",\n        location=LOCATION,\n        lake_id=RESOURCE_ID_LAKE,\n        tabs=[${tabs[0]}],\n        display_columns=[${display_columns}[0]],\n        pivot_columns=[${pivot_columns}[0]],\n        project_id=PROJECT_ID,\n    )\n    # [END howto_dataplex_list_catalog_pivot_report_with_subsets]\n\n    # [START howto_dataplex_for_loop_pagination_with_subsets]\n    for i in range(len(tabs) - 1):\n        ${tabs[i]()} >> Perform Query ${tabs[i + 1]}\n    # [END howto_dataplex_for_loop_pagination_with_subsets]\n\n\nif not AILOGBOX_ENABLE:\n    loop_dag >> DagRunFailureException(\"${tabs[0()]}\", \"No tabs were provided for pagination!\")\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1222_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7229412}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that retrieves the current server's external IP address and sends an email notification about the IP. Design a workflow that automatically tracks and communicates server connection details.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.extralayer.hooks.extralayer import ExtraLayerHook\nfrom airflow.providers.extralayer.transfers.sftp_to_extralayer import SFTPToExtraLayerOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_sftp_to_extralayer\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_extra_layer_sftp_to_extralayer_operator]\n    sftp_to_extralayer_task = SFTPToExtraLayerOperator(\n        task_id=\"sftp_to_extralayer_task\",\n        sftp_path=\"/tmp/sftp_path\",\n        extralayer_path=\"/tmp/extralayer_path\",\n        extra_layer_host=\"host_alias\",\n        extra_layer_port=22,\n        extra_layer_user=\"username\",\n        extra_layer_password=\"password\",\n        replace=True,\n    )\n    # [END howto_extra_layer_sftp_to_extralayer_operator]\n\n    chain(sftp_to_extralayer_task)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_file():\n        \"\"\"Delete the file\"\"\"\n        if os.path.exists(\"/tmp/extralayer_path\"):\n            os.remove(\"/tmp/extralayer_path\")\n\n    delete_file()\n\n    chain(delete_file())\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_673_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.722947}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function to find the maximum possible sum of a subarray within a given array of integers. The array can contain both positive and negative numbers."}, {"role": "assistant", "content": "def max_subarray_sum(arr):\n    if not arr:\n        return 0\n    \n    # Initialize variables to store the current maximum and global maximum sums\n    current_max = global_max = arr[0]\n    \n    for num in arr[1:]:\n        # Update the current maximum to be either the current number itself or the sum of the current number and the previous current maximum\n        current_max = max(num, current_max + num)\n        \n        # Update the global maximum if the current maximum is greater than the global maximum\n        global_max = max(global_max, current_max)\n    \n    return global_max\n\n# Example usage:\narr = [-2, 1, -3, 4, -1, 2, 1, -5, 4]\nprint(max_subarray_sum(arr))  # Output: 6"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7229583}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow DAG that automates the process of preparing, training, and managing a text classification dataset using cloud-based machine learning infrastructure. Create a systematic workflow that handles data synchronization, dataset import, model training, and resource cleanup in a single, orchestrated pipeline.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyJobFlowOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.emr_kmz import (\n    EmrKMZAddStepsOperator,\n    EmrKMZCreateJobFlowOperator,\n    EmrKMZModifyJobFlowOperator,\n    EmrKMZTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.utils import get_aws_account_id\nfrom airflow.settings import Session\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_emr\"\nDAG_ID_KMZ = \"example_emr_kmx\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\n# EMR service account ID:\nEMR_SERVICE_ACCOUNT_ID = \"1647658030775517\"\n\n# EMR KMZ these steps require execution as the EMR service account:\nKMS_STEP_NAME = \"StartKmsEncryptionProvisioningStep\"\nEXECUTE_STEP_NAME = \"ExecuteStep\"\n\nlog = logging.getLogger(__name__)\n\n# S3 key prefix for our job flow source files\nJOB_FLOW_SRC_KEY_PREFIX = \"emr/jar/\"\n# S3 key prefix for our file during KMZ step setup\nKMZ_JOB_FLOW_SRC_KEY_PREFIX = \"emr-kmz/jar/\"\n\n\n@task\ndef create_app_role(role_name: str, aws_account_id: int):\n    \"\"\"Creates an AWS AppFlow role that has access to your S3 bucket.\"\"\"\n    log.info(\"Creating AppFlow role %s with S3 access\", role_name)\n\n    boto3.client(\"iam\").create_role(\n        Path=\"/\",\n        Name=role_name,\n        AssumeRolePolicyDocument=json.dumps(\n            {\n                \"Statement\": [\n                    {\n                        \"Action\": [\"sts:AssumeRole\"],\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\"Arn\": f\"arn:aws:iam::{aws_account_id}:role/EMRService\"},\n                    }\n                ],\n            }\n        ),\n    )\n\n    boto3.client(\"iam\").attach_role_policy(\n        PolicyName=\"S3FullAccess\",\n        RoleId=role_name,\n    )\n\n    boto3.client(\"iam\").get_role(RoleName=role_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_app_flow_role(role_name: str, aws_account_id: int):\n    \"\"\"Deletes the AWS AppFlow role\"\"\"\n    log.info(\"Deleting AppFlow role %s\", role_name)\n\n    boto3.client(\"iam\").delete_role(RoleName=role_name)\n    boto3.client(\"iam\").get_waiter(\"role_delete_complete\").wait(\n        RoleNames=[role_name], WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n\n\n@task\ndef save_step_steps(step_ids: list, jar_key: str, path_to_file: str, aws_account_id: int):\n    \"\"\"\n    Save Step requires a JSON formatted string of steps in it.\n    This method takes three parameters: step ids, jar key and path to file with steps and\n    returns string which represents JSON object with steps saved.\n    \"\"\"\n    return_string = \"{\\n  \\\"steps\\\": [\\n\"\n\n    for idx, step_id in enumerate(step_ids):\n        is_last = (idx == len(step_ids) - 1)\n\n        return_string += f'    {{\"id\": \"{step_id}\"{\",\" \"executionPosition\": {f\"\"\"{_get_execution_position(aws_account_id}\"\"\"}\" if not is_last else \"\"}}}},'\n\n    return_string += \"\\n  ]\"\n    #  Writing the JSON string\n    with open(path_to_file, \"w\") as write_file:\n        write_file.write(return_string)\n\n    return path_to_file\n\n\n@task\ndef _get_execution_position(aws_account_id: int):\n    \"\"\"\n    According EMR documentation to get correct Execution Position\n    EMRStep.Position should be calculated based on latest step creation.\n    In case of recent steps created via EMR kmz interface creation date is equal cluster start up date\n    and can be used as Execution Position.\n    More info: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-step-properties.html\n    \"\"\"\n\n    # Creating connection to S3 and downloading most recent file from step file\n    hook = S3Hook()\n    response = hook.get_content(\n        Key=f\"s3://{aws_account_id}-cmkz-steps-files/{aws_account_id}/minio-emr-get-job-flow-id.sh\",\n        decrypt=False,\n    )\n    script = response[\"Content\"]\n\n    # Getting creation date of most recent file in minio-emr-kmz-steps-path\n    command = f\"bash {script} {KMZ_JOB_FLOW_SRC_KEY_PREFIX}{aws_account_id}/{EMR_SERVICE_ACCOUNT_ID}\"\n    log.info(\"Executing command: %s\", command)\n    job_flow_id_location = boto3.client(\"s3\").invoke_function(\n        FunctionName=command, Payload=b\"\"\n    )[\"Payload\"].read().decode()\n    job_flow_location_parts = job_flow_id_location.split(\"/\")\n    final_job_flow_id = job_flow_location_parts[-1]\n    log.info(\"Final returned job flow id is : %s\", final_job_flow_id)\n\n    try:\n        location_parts = response[\"Key\"].split(\"/\")\n        s3_bucket = location_parts[2]\n        s3_key = location_parts[3]\n\n        response = boto3.client(\"s3\").head_object(Bucket=s3_bucket, Key=s3_key)\n        creation_date = response[\"Body\"][\"CreationDate\"]\n        log.info(\"Most recent step file creation date is : %s\", creation_date)\n        return creation_date\n    except ClientError as e:\n        log.error(\"Couldn't get most recent step creation date due to error: %s\", e)\n        raise\n\n\n@task\ndef update_job_flow(job_flow_id: str, new_steps: list, aws_account_id: int):\n    log.info(\"Updating EMR Job Flow to contain the new steps\")\n\n    steps = [\n        {\n            \"Name\": step_id,\n            \"ActionOnFailure\": \"TERMINATE\",\n            \"HadoopJarStep\": {\n                \"Args\": [\"hadopemrv1 0\", \"run\"],  # Run the job\n                \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n                \"MainClass\": \"\",\n                \"PythonVertexClass\": \"\",\n            },\n        }\n        for step_id, jar_key in new_steps.items()\n    ]\n\n    emr_data = {\n        \"Name\": EXECUTE_STEP_NAME,\n        \"ActionOnFailure\": \"TERMINATE\",\n        \"HadoopJarStep\": {\n            \"Args\": [\"hadopemrv1 0\", \"submit\"],  # Submit the job\n            \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n            \"MainClass\": \"\",\n            \"PythonVertexClass\": \"\",\n        },\n    }\n\n    body = {\n        \"Name\": KMS_STEP_NAME,\n        \"ActionOnFailure\": \"TERMINATE\",\n        \"KmsKeyId\": \"\",\n        \"StepId\": job_flow_id + \"--\" + emr_data[\"Name\"] + \"--\" + str(hash(emr_data)),\n        \"HadoopJarStep\": emr_data,\n        \"ExecutionPosition\": _get_execution_position(aws_account_id),\n    }\n\n    hook = S3Hook()\n    temp_file = NamedTemporaryFile(mode=\"w\")\n    temp_file.write(json.dumps(body))\n    temp_file_path = temp_file.name\n    hook.upload_tempfile(temp_file_path, KMZ_JOB_FLOW_SRC_KEY_PREFIX + job_flow_id + \"/\" + EXECUTE_STEP_NAME + \".json\", job_flow_id)\n\n    client = boto3.client(\"emr\", aws_account_id)\n    client.modify_job_flow_step(JobFlowId=job_flow_id, Steps=new_steps, ClientToken=str(hash(body)))\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_now = EmrCreateJobFlowOperator(\n        task_id=\"runnow\",\n        job_flow_name=\"EmrCreateJobFlowTaskTest\",\n        aws_account_id=get_aws_account_id(),\n        bootstrap_hadoop_url=\"2.7.1\",\n        init_elements=[\n            \"hadoop\",\n            \"hive\",\n            \"spark\",\n            \"pig\",\n            \"mysql\",\n            \"sqllite\",\n            \"oracle\",\n            \"postgresql\",\n            \"chroot\",\n        ],\n    )\n\n    step_1 = EmrAddStepsOperator(\n        task_id=\"step_1\",\n        job_flow_id=run_now.output,\n        steps=[\n            {\n                \"Name\": \"Sammy\",\n                \"ActionOnFailure\": \"TERMINATE\",\n                \"HadoopJarStep\": {\n                    \"Args\": [\"hadoop-example', 'wordcount', 'input', 'output'],\n                    \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n                    \"MainClass\": \"\",\n                },\n            }\n        ],\n    )\n\n    step_2 = EmrAddStepsOperator(\n        task_id=\"step_2\",\n        job_flow_id=run_now.output,\n        steps=[\n            {\n                \"Name\": \"Billy\",\n                \"ActionOnFailure\": \"TERMINATE\",\n                \"HadoopJarStep\": {\n                    \"Args\": [\"hadoop-example', 'wordcount', 'input', 'output'],\n                    \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n                    \"MainClass\": \"\",\n                },\n            }\n        ],\n    ))\n\n    # [START howto_operator_emr_modify_job_flow_steps]\n    step_3 = EmrKMZModifyJobFlowOperator(\n        task_id=\"step_3\",\n        job_flow_id=\"{{ ti.xcom_pull('runnow') }}\",\n        new_steps={\n            \"samwise\": {\n                \"Name\": \"Samwise\",\n                \"ActionOnFailure\": \"TERMINATE\",\n                \"HadoopJarStep\": {\n                    \"Args\": [\"hadoop-example', 'wordcount', 'input', 'output'],\n                    \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n                    \"MainClass\": \"\",\n                },\n            }\n        },\n    )\n    # [END howto_operator_emr_modify_job_flow_steps]\n\n    stop_job = EmrTerminateJobFlowOperator(\n        task_id=\"stop_job\",\n        job_flow_id=\"{{ ti.xcom_pull('runnow') }}\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    join_task = EmrKMZAddStepsOperator(\n        task_id=\"join_task\",\n        job_flow_id=\"{{ ti.xcom_pull('runnow') }}\",\n        steps=[\n            {\n                \"Name\": \"Join\",\n                \"ActionOnFailure\": \"TERMINATE\",\n                \"HadoopJarStep\": {\n                    \"Args\": [\"hadoop-example', 'wordcount', 'input', 'output'],\n                    \"Jars\": [f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\"],\n                    \"MainClass\": \"\",\n                },\n            }\n        ],\n    )\n\n    chain(\n        run_now,\n        step_1,\n        step_2,\n        update_job_flow(\n            \"{{ ti.xcom_pull(task_ids='runnow') }}\",\n            {\n                \"samwise\": f\"{aws_account_id}-{JOB_FLOW_SRC_KEY_PREFIX}{jar_key}\",\n            },\n        ),\n        stop_job,\n        join_task,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_927_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7229857}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Please write a Python function that takes a list of lists (a 2D list) and returns the longest sub-list in terms of the number of elements. If there are multiple sub-lists with the same maximum length, the function should return the first one encountered. For example, if the input is `[[1, 2, 3], [4, 5], [6, 7, 8, 9], [10, 11, 12]]`, the function should return `[6, 7, 8, 9]`."}, {"role": "assistant", "content": "def find_longest_sublist(matrix):\n    \"\"\"\n    Finds the longest sublist within a 2D list.\n\n    Args:\n    matrix (List[List[int]]): A 2D list of integers.\n\n    Returns:\n    List[int]: The longest sublist found in the 2D list.\n    \"\"\"\n    # Initialize variables to store the current longest sublist and its length\n    max_length = 0\n    longest_sublist = []\n\n    # Iterate through each sublist in the 2D list\n    for sublist in matrix:\n        # Check if the current sublist has more elements than the previously recorded longest sublist\n        if len(sublist) > max_length:\n            # Update the longest sublist and its length\n            max_length = len(sublist)\n            longest_sublist = sublist\n\n    return longest_sublist\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5], [6, 7, 8, 9], [10, 11, 12]]\nprint(find_longest_sublist(matrix))  # Output: [6, 7, 8, 9]"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.722997}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow workflow that showcases asynchronous batch processing using OpenAI's API, with dynamic task generation for multiple Pok\u00e9mon descriptions and deferred execution. Implement a scalable pattern for sending multiple AI generation requests and managing the batch processing lifecycle.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom system.openai import OPENAI_CONN_ID\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef create_pokemon_table():\n    \"\"\"\n    SQL CREATE TABLE statement:\n        CREATE TABLE IF NOT EXISTS pokemon (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(255) NOT NULL,\n            number INTEGER NOT NULL,\n            height VARCHAR(255),\n            weight VARCHAR(255),\n            color VARCHAR(255),\n            habitat_id INTEGER\n        );\n    \"\"\"\n    conn = Connection(\n        conn_id=OPENAI_CONN_ID,\n        schema=\"airflow\",\n        sql=\"CREATE TABLE IF NOT EXISTS pokemon (\\n    id SERIAL PRIMARY KEY,\\n    name VARCHAR(255) NOT NULL,\\n    number INTEGER NOT NULL,\\n    height VARCHAR(255),\\n    weight VARCHAR(255),\\n    color VARCHAR(255),\\n    habitat_id INTEGER\\n);\",\n    )\n    log.info(\"Creating table %s\", \"pokemon\")\n    conn.create()\n\n\n@task\ndef populate_pokemon_table(pokemon_data):\n    conn = Connection(\n        conn_id=OPENAI_CONN_ID,\n        schema=\"airflow\",\n        sql=pokemon_data,\n    )\n    log.info(\"Populating table %s\"), \"pokemon\"\n    conn.create()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_pokemon_table():\n    conn = Connection(\n        conn_id=OPENAI_CONN_ID,\n        schema=\"airflow\",\n        sql=\"DROP TABLE IF EXISTS pokemon;\",\n    )\n    log.info(\"Deleting table %s\"), \"pokemon\"\n    conn.create()\n\n\nwith DAG(\n    dag_id=\"example_openai_sql\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    run_now = BashOperator(task_id=\"run_now\")\n\n    # [START howto_operator_openai_send_request]\n    send_request_task = SQLExecuteQueryOperator(\n        task_id=\"send_request\",\n        sql=\"SELECT * FROM pokemon WHERE number IN (SELECT DISTINCT number FROM openai\u54ea\u91cc\u6709\uff1f)\",\n        conn_id=OPENAI_CONN_ID,\n    )\n    # [END howto_operator_openai_send_request]\n\n    # [START howto_operator_openai_batch_send_request]\n    batch_send_request_task = SQLExecuteQueryOperator(\n        task_id=\"batch_send_request\",\n        sql=[\n            \"SELECT * FROM pokemon WHERE number IN (SELECT DISTINCT number FROM openai\u54ea\u91cc\u6709\uff1f)\",\n            \"SELECT * FROM openai\u54ea\u91cc\u6709\uff1f WHERE name='Pikachu'\",\n        ],\n        conn_id=OPENAI_CONN_ID,\n        autocommit=True,\n    )\n    # [END howto_operator_openai_batch_send_request]\n\n    create_pokemon_table()\n    populate_pokemon_table('all_pokemon_data.json')\n    delete_pokemon_table()\n    run_now >> send_request_task >> batch_send_request_task\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1371_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230065}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases advanced translation task orchestration using cloud services, with clear task dependencies and resource management. Develop a workflow that creates a temporary bucket, performs translations, and then cleanly deletes the bucket after processing.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.translations.operators.translate_text import CloudTranslateTextOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_gcp_translate_text\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\n\n# [START howto_cloud_translate_text_csv]\nCSV_FILE_CONTENT = \"\"\"milk,water,butter\n2,1,1\n4,3,2\n\"\"\"\n# [END howto_cloud_translate_text_csv]\n\n# [START howto_cloud_translate_text_text]\nTEXT = \"\"\"milk,water,butter\n2,1,1\n4,3,2\n\"\"\"\n# [END howto_cloud_translate_text_text]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    translate_text_csv = CloudTranslateTextOperator(\n        task_id=\"translate_text_csv\",\n        values=[CSV_FILE_CONTENT],\n        source_language_code=\"en\",\n        target_language_code=\"fr\",\n        gcp_project_id=PROJECT_ID,\n    )\n    translate_text_text = CloudTranslateTextOperator(\n        task_id=\"translate_text_text\",\n        contents=[TEXT],\n        source_language_code=\"en\",\n        target_language_code=\"fr\",\n        gcp_project_id=PROJECT_ID,\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> translate_text_csv\n        >> translate_text_text\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1698_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230124}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "The user claims there is a 'globalmon' module, but it does not seem to be part of any official Python libraries or widely recognized third-party packages. It could be a custom or private module. How can I help the user determine if they have access to this module and how it can be used?"}, {"role": "assistant", "content": "import globalmon  # This should raise ImportError if not found\n   print(globalmon.__file__)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.723021}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a daily automated workflow for deploying and testing containerized applications in Azure Container Instances. Create a DAG that demonstrates launching different container configurations with varying registry sources and volume mounting strategies.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "json\n# {\n#     \"type\": \"AzureFile\",\n#     \"mountPoint\": {\n#         \"mountPointId\": \"aceffdd2-b5e6-11ec-a4d9-7a3c4e5ac5f2\",\n#         \"scope\": \"volume-share-2bf4b553\",\n#         \"mountPrefix\": \"volume-share-2bf4b553\",\n#         \"mountOptions\": [\"nodevmap\"]\n#     }\n# }\n#"}], "metadata": {"file_name": "dag_618_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230375}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Write a Python function that calculates the sum of the squares of the first n odd natural numbers."}, {"role": "assistant", "content": "def sum_of_squares_of_odds(n):\n    # Generate the first n odd natural numbers\n    odd_numbers = [2 * i + 1 for i in range(n)]\n    \n    # Calculate the sum of their squares\n    return sum(num ** 2 for num in odd_numbers)\n\n# Example usage:\nprint(sum_of_squares_of_odds(5))  # Output: 35 (1^2 + 3^2 + 5^2 + 7^2 + 9^2)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.723043}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an AWS-integrated Airflow pipeline that showcases sensor functionality for tracking data availability in DynamoDB, including table creation, value checking with different matching strategies, and automatic table deletion. Implement a systematic workflow that tests DynamoDB item value detection using Airflow's task chaining and sensor capabilities.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nfrom boto3 import session\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.sensors.dynamodb import DynamoDBValueSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_sensor_dynamodb]\nlog = logging.getLogger(__name__)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef create_session():\n    return {\n        \"aws_access_key_id\": \"TESTING\",\n        \"aws_secret_access_key\": \"TESTING\",\n        \"region_name\": \"us-east-1\",\n    }\n\n\n@task(task_id=\"table_created\")\ndef table_exists(table_name):\n    log.info(\"Checking if the table 'dynamodb_sensor_table' exists...\")\n    session_data = create_session()\n    dyno_session = session.Session(**session_data)\n    table = dyno_session.DynamoDB.describeTable(TableName=table_name)\n    log.info(\"Table existence result: %s\", table)\n\n\nwith DAG(\n    dag_id=\"example_dynamodb_sensor\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    chain(create_table_and_start_sensor)\n\n    # [START howto_sensor_dynamodb_basic]\n    dynamodb_sensor_basic = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_basic\",\n        table_name=\"dynamodb_sensor_table\",\n        key={\"id\": \"value\"},\n        attribute_names=[\"id\"],\n        attribute_types=[\"S\"],\n    )\n    # [END howto_sensor_dynamodb_basic]\n\n    # [START howto_sensor_dynamodb_expr]\n    dynamodb_sensor_expr = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_expr\",\n        table_name=\"dynamodb_sensor_table\",\n        keys={\n            \"ExpressionAttributeNames\": {\"#T\": \"id\"},\n            \"ExpressionAttributeValues\": {\"#T\": \"value\"},\n            \"KeyConditionExpression\": \"id = :value\",\n        },\n        condition_expression=\"count(@pcs) > :val & id = :value\",\n        expression_attribute_values={\":value\": \"value\", \":val\": \"5\"},\n    )\n    # [END howto_sensor_dynamodb_expr]\n\n    # [START howto_sensor_dynamodb_expression_body]\n    dynamodb_sensor_expression_body = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_expression_body\",\n        table_name=\"dynamodb_sensor_table\",\n        key={\u201cid\u201d: \u201cvalue\u201d},\n        attribute_names=[\u201cid\u201d],\n        attribute_types=[\u201cS\u201d],\n        expression_body='COUNT(\\#:pcs) > :val',\n        expression_attribute_names={\u201c:pcs\u201d: \u201cid\u201d,\u201d:val\u201d: \u201cvalue\u201d},\n        expression_attribute_values={\u201d:val\u201d: \u201c5\u201d,\u201d:value\u201d: \u201cvalue\u201d},\n    )\n    # [END howto_sensor_dynamodb_expression_body]\n\n    # [START howto_sensor_dynamodb_multiple_keys]\n    dynamodb_sensor_multiple_keys = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_multiple_keys\",\n        table_name=\"dynamodb_sensor_table\",\n        keys={\"First\": \"value1\", \"Second\": \"value2\"},\n        attribute_names=[\"First\", \"Second\"],\n        attribute_types=[\"S\", \"S\"],\n    )\n    # [END howto_sensor_dynamodb_multiple_keys]\n\n    # [START howto_sensor_dynamodb_eq]\n    dynamodb_sensor_eq = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_eq\",\n        table_name=\"dynamodb_sensor_table\",\n        keys={\"Id\": \"value\"},\n        attribute_names=[\"Id\"],\n        attribute_types=[\"S\"],\n    )\n    # [END howto_sensor_dynamodb_eq]\n\n    # [START howto_sensor_dynamodb_range]\n    dynamodb_sensor_range = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_range\",\n        table_name=\"dynamodb_sensor_table\",\n        keys={\"Second\": {\"RangeBaseKey\": {\"S\": \"value2\"}}},\n        attribute_names=[\"Second\"],\n        attribute_types=[\"N\"],\n    )\n    # [END howto_sensor_dynamodb_range]\n\n    # [START howto_sensor_dynamodb_mapping_type]\n    dynamodb_sensor_mapping_type = DynamoDBValueSensor(\n        task_id=\"dynamodb_sensor_mapping_type\",\n        table_name=\"dynamodb_sensor_table\",\n        keys={\"Id\": {\"MappingType\": {\"S\": \"S\"}, \"Value\": \"value\"}},\n        attribute_names=[\"Id\"],\n        attribute_types=[\"M\"],\n    )\n    # [END howto_sensor_dynamodb_mapping_type]\n\n    chain(\n        create_table_and_start_sensor,\n        [dynamodb_sensor_basic, dynamodb_sensor_expr, dynamodb_sensor_expression_body, dynamodb_sensor_multiple_keys,\n         dynamodb_sensor_eq, dynamodb_sensor_range, dynamodb_sensor_mapping_type],\n    )\n\n    def _wait_for_sensor(sensors):\n        results = [sensor.execute() for sensor in sensors]\n        all_results_pass = all(result.is_complete() for result, _ in results)\n        if not all_results_pass:\n            log.error(\"One or more sensors failed.\")\n            for result, sensor in zip(results, sensors):\n                log.info(\"Results for %s\", sensor.task_id)\n                log.info(\"%s, %s\", result.output, result.exception)\n\n    list(sensors for sensors in [dynamodb_sensor_basic, dynamodb_sensor_eq, dynamodb_sensor_range]\n         if isinstance(sensors, DynamoDBValueSensor)).append(lambda *args: _wait_for_sensor(args))\n\n    # [END howto_sensor_dynamodb]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1472_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230563}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello, how can I create a Python script that can handle HTTP POST requests to receive data and store it in a local JSON file?"}, {"role": "assistant", "content": "import requests\n\ndef post_data_to_file(url, data):\n    # Send a POST request to the specified URL with the provided data\n    response = requests.post(url, json=data)\n    \n    # Check if the request was successful (status code 200)\n    if response.status_code == 200:\n        # Open a file in write mode and write the response content to it\n        with open('received_data.json', 'w') as file:\n            file.write(response.text)\n        \n        print(\"Data received successfully and saved to received_data.json\")\n    else:\n        # Print an error message if the request failed\n        print(f\"Failed to receive data: Status code {response.status_code}\")\n\n# Example usage\nurl = 'http://example.com/api/post'  # Replace with your API endpoint\ndata = {'key': 'value'}               # Your data payload\n\npost_data_to_file(url, data)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230663}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "How can I create a Python script to check the status of a website?"}, {"role": "assistant", "content": "import requests\n\ndef check_website_status(url):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        \n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            print(f\"Success! The site at {url} is up and running.\")\n        else:\n            print(f\"Failed to connect to the site at {url}. Status code: {response.status_code}\")\n    \n    except requests.exceptions.RequestException as e:\n        # Handle exceptions such as network errors or invalid URLs\n        print(f\"An error occurred while checking the site at {url}: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    url_to_check = input(\"Enter the URL of the website to check: \")\n    check_website_status(url_to_check)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230756}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a multi-DAG workflow that demonstrates dependency management between two related data processing pipelines, where one DAG triggers and monitors the execution of tasks in another DAG. Create a parent-child DAG relationship that showcases how external task dependencies can be established and monitored in Airflow.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.xcom_arg import XComArg\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_xcom_pass():\n    @task\n    def spawn_child():\n        print(\"Child task started\")\n        return 666\n\n    @task\n    def get_parent(value):\n        print(f\"Got parent {value}\")\n\n    # Pass the value of this task (666) to the target task\n    spawn_child_instance = spawn_child()\n    get_parent(spawn_child_instance.output)\n\n    # Now pass the returned value of spawn_child() to the target task\n    get_parent_again = get_parent(spawn_child_instance.output)\n    XComArg(get_parent_again, key=\"return_value\", task_id=\"parent_task\")\n\n\nchild_dag = example_xcom_pass()\n\n\n# [START howto_xcom_callback_in_extern_task]\nfrom airflow.utils.trigger_rule import TriggerRule\n\nchild_dag.add_task(XComArgsTaskCallback(task_ids=\"parent_task\"), trigger_rule=TriggerRule.ALL_DONE)\n\n\n# [END howto_xcom_callback_in_extern_task]\n\n\nif __name__ == \"__main__\":\n    from tests.system.utils.watcher import watcher\n\n    list(dags=[child_dag])\n    watcher()"}], "metadata": {"file_name": "dag_13_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230809}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I want to create a Python Flask application that allows users to register and log in using their email and password. How can I implement this functionality?"}, {"role": "assistant", "content": "from flask import Flask, request, redirect, url_for, render_template_string, session, flash\n\napp = Flask(__name__)\napp.secret_key = 'your_secret_key'  # Change this to a secure key\n\n# Sample data (you should store these securely)\nusers = {\n    \"test@example.com\": {\"password\": \"secure_password\"}\n}\n\n@app.route('/')\ndef home():\n    return render_template_string('''\n        <h1>Welcome to the Auth App</h1>\n        <form method=\"post\">\n            Email: <input type=\"text\" name=\"email\"><br>\n            Password: <input type=\"password\" name=\"password\"><br>\n            <input type=\"submit\" value=\"Login\">\n        </form>\n    ''')\n\n@app.route('/login', methods=['POST'])\ndef login():\n    email = request.form.get('email')\n    password = request.form.get('password')\n    \n    if email in users and users[email]['password'] == password:\n        session['username'] = email\n        flash(f\"Welcome back, {email}!\")\n        return redirect(url_for('home'))\n    else:\n        flash(\"Invalid credentials\")\n        return redirect(url_for('home'))\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        email = request.form.get('email')\n        password = request.form.get('password')\n        \n        if not email or not password:\n            flash(\"Please fill out both fields.\")\n        elif email in users:\n            flash(\"This email is already registered.\")\n        else:\n            users[email] = {\"password\": password}\n            flash(\"Registration successful! You can now log in.\")\n            return redirect(url_for('login'))\n        \n        return redirect(url_for('home'))\n    \n    return render_template_string('''\n        <h1>Register</h1>\n        <form method=\"post\">\n            Email: <input type=\"text\" name=\"email\"><br>\n            Password: <input type=\"password\" name=\"password\"><br>\n            <input type=\"submit\" value=\"Register\">\n        </form>\n    ''')\n\nif __name__ == '__main__':\n    app.run(debug=True)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7230973}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that monitors a Google Cloud Tasks queue to ensure all tasks have been processed before proceeding with downstream operations. Create an Airflow DAG that checks queue status as a critical checkpoint in a distributed task processing system.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that creates and deletes Queues and creates, gets, lists,\nprocesses and deletes Tasks in the Google Cloud Tasks service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.tasks import (\n    GcpTasksCreateQueueOperator,\n    GcpTasksDeleteQueueOperator,\n    GcpTasksGetTaskOperator,\n    GcpTasksListTasksOperator,\n    GcpTasksProcessTaskOperator,\n    GcpTasksSendMessageToTaskOperator,\n    GcpTasksStopTaskOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_gcp_tasks\"\n\n# [START howto_operator_gcp_tasks_queue_create]\n# [START howto_trigger_gcp_tasks_queue_create]\nTRIGGER_ID = f\"trigger_{DAG_ID}_{ENV_ID}\"\n# [END howto_trigger_gcp_tasks_queue_create]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n):\n    # [START howto_operator_gcp_tasks_create_queue]\n    create_queue = GcpTasksCreateQueueOperator(\n        task_id=\"create_queue\",\n        project_id=PROJECT_ID,\n        queue_id=f\"queue_{DAG_ID}_{ENV_ID}\",\n        trigger_id=TRIGGER_ID,\n    )\n    # [END howto_operator_gcp_tasks_create_queue]\n\n    # [START howto_operator_gcp_tasks_get_task]\n    get_task = GcpTasksGetTaskOperator(\n        task_id=\"get_task\", project_id=PROJECT_ID, path=f\"queues/{f'queue_{DAG_ID}_{ENV_ID}'}\"\n    )\n    # [END howto_operator_gcp_tasks_get_task]\n\n    # [START howto_operator_gcp_tasks_list_tasks]\n    list_tasks = GcpTasksListTasksOperator(task_id=\"list_tasks\", project_id=PROJECT_ID, queue_id=f\"any_queue\")\n    # [END howto_operator_gcp_tasks_list_tasks]\n\n    # [START howto_operator_gcp_tasks_process_task]\n    process_task = GcpTasksProcessTaskOperator(\n        task_id=\"process_task\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task.output}\",\n                ],\n            },\n        },\n        body_mask={\"paths\": [\"names\"}},\n    )\n    # [END howto_operator_gcp_tasks_process_task]\n\n    # [START howto_operator_gcp_tasks_stop_task]\n    stop_task = GcpTasksStopTaskOperator(\n        task_id=\"stop_task\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        path=f\"tasks/{get_task.output}\",\n        body={\"request\": {\"action\": \"STOP\"}},\n    )\n    # [END howto_operator_gcp_tasks_stop_task]\n\n    # [START howto_operator_gcp_tasks_send_message_to_task]\n    send_msg_to_task = GcpTasksSendMessageToTaskOperator(\n        task_id=\"send_msg_to_task\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task.output}\",\n                ],\n            },\n            \"insertItems\": [\n                {\n                    \"body\": {\n                        \"message\": \"Hello world!\",\n                        \"ackWaitMs\": 60 * 1000,\n                        \"finishTimeoutMs\": 60 * 1000,\n                    }\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gcp_tasks_send_message_to_task]\n\n    # [START howto_operator_gcp_tasks_delete_queue]\n    delete_queue = GcpTasksDeleteQueueOperator(\n        task_id=\"delete_queue\", project_id=PROJECT_ID, queue_id=f\"queue_{DAG_ID}_{ENV_ID}\"\n    )\n    # [END howto_operator_gcp_tasks_delete_queue]\n    delete_queue.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_gcp_tasks_create_queue_trigger]\n    create_queue_trigger = GcpTasksCreateQueueOperator(\n        task_id=\"create_queue_trigger\",\n        project_id=PROJECT_ID,\n        queue_id=f\"queue_{DAG_ID}_{ENV_ID}\",\n    )\n    # [END howto_operator_gcp_tasks_create_queue_trigger]\n\n    # [START howto_operator_gcp_tasks_get_task_trigger]\n    get_task_trigger = GcpTasksGetTaskOperator(\n        task_id=\"get_task_trigger\", project_id=PROJECT_ID, path=f\"queues/{f'queue_{DAG_ID}_{ENV_ID}'}\"\n    )\n    # [END howto_operator_gcp_tasks_get_task_trigger]\n\n    # [START howto_operator_gcp_tasks_list_tasks_trigger]\n    list_tasks_trigger = GcpTasksListTasksOperator(\n        task_id=\"list_tasks_trigger\", project_id=PROJECT_ID, queue_id=f\"any_queue\"\n    )\n    # [END howto_operator_gcp_tasks_list_tasks_trigger]\n\n    # [START howto_operator_gcp_tasks_process_task_trigger]\n    process_task_trigger = GcpTasksProcessTaskOperator(\n        task_id=\"process_task_trigger\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task_trigger.output}\",\n                ],\n            },\n        },\n        body_mask={\"paths\": [\"names\"]},\n    )\n    # [END howto_operator_gcp_tasks_process_task_trigger]\n\n    # [START howto_operator_gcp_tasks_stop_task_trigger]\n    stop_task_trigger = GcpTasksStopTaskOperator(\n        task_id=\"stop_task_trigger\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        path=f\"tasks/{get_task_trigger.output}\",\n        body={\"request\": {\"action\": \"STOP\"}},\n    )\n    # [END howto_operator_gcp_tasks_stop_task_trigger]\n\n    # [START howto_operator_gcp_tasks_send_message_to_task_trigger]\n    send_msg_to_task_trigger = GcpTasksSendMessageToTaskOperator(\n        task_id=\"send_msg_to_task_trigger\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task_trigger.output}\",\n                ],\n            },\n            \"insertItems\": [\n                {\n                    \"body\": {\n                        \"message\": \"Hello world!\",\n                        \"ackWaitMs\": 60 * 1000,\n                        \"finishTimeoutMs\": 60 * 1000,\n                    }\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gcp_tasks_send_message_to_task_trigger]\n\n\n    # [START howto_operator_gcp_tasks_delete_queue_trigger]\n    delete_queue_trigger = GcpTasksDeleteQueueOperator(\n        task_id=\"delete_queue_trigger\", project_id=PROJECT_ID, queue_id=f\"queue_{DAG_ID}_{ENV_ID}\"\n    )\n    # [END howto_operator_gcp_tasks_delete_queue_trigger]\n\n    # [START howto_operator_gcp_tasks_create_queue_from_id]\n    create_queue_from_id = GcpTasksCreateQueueOperator(\n        task_id=\"create_queue_from_id\",\n        project_id=PROJECT_ID,\n        queue_id=f\"queue_{DAG_ID}_{ENV_ID}\",\n        location=f\"us-central1\",\n    )\n    # [END howto_operator_gcp_tasks_create_queue_from_id]\n\n    # [START howto_operator_gcp_tasks_delete_queue_from_id]\n    delete_queue_from_id = GcpTasksDeleteQueueOperator(\n        task_id=\"delete_queue_from_id\", project_id=PROJECT_ID, queue_id=f\"queue_{DAG_ID}_{ENV_ID}\"\n    )\n    # [END howto_operator_gcp_tasks_delete_queue_from_id]\n\n    # [START howto_operator_gcp_tasks_get_task_from_id]\n    get_task_from_id = GcpTasksGetTaskOperator(\n        task_id=\"get_task_from_id\", project_id=PROJECT_ID, path=f\"queues/{f'queue_{DAG_ID}_{ENV_ID}'}\"\n    )\n    # [END howto_operator_gcp_tasks_get_task_from_id]\n\n    # [START howto_operator_gcp_tasks_list_tasks_from_id]\n    list_tasks_from_id = GcpTasksListTasksOperator(\n        task_id=\"list_tasks_from_id\", project_id=PROJECT_ID, queue_id=f\"any_queue\"\n    )\n    # [END howto_operator_gcp_tasks_list_tasks_from_id]\n\n    # [START howto_operator_gcp_tasks_process_task_from_id]\n    process_task_from_id = GcpTasksProcessTaskOperator(\n        task_id=\"process_task_from_id\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task_from_id.output}\",\n                ],\n            },\n        },\n        body_mask={\"paths\": [\"names\"]},\n    )\n    # [END howto_operator_gcp_tasks_process_task_from_id]\n\n    # [START howto_operator_gcp_tasks_stop_task_from_id]\n    stop_task_from_id = GcpTasksStopTaskOperator(\n        task_id=\"stop_task_from_id\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        path=f\"tasks/{get_task_from_id.output}\",\n        body={\"request\": {\"action\": \"STOP\"}},\n    )\n    # [END howto_operator_gcp_tasks_stop_task_from_id]\n\n    # [START howto_operator_gcp_tasks_send_message_to_task_from_id]\n    send_msg_to_task_from_id = GcpTasksSendMessageToTaskOperator(\n        task_id=\"send_msg_to_task_from_id\",\n        project_id=PROJECT_ID,\n        location=\"us-central1\",\n        body={\n            \"taskArbitration\": {\n                \"taskKeys\": [\n                    f\"tasks/{get_task_from_id.output}\",\n                ],\n            },\n            \"insertItems\": [\n                {\n                    \"body\": {\n                        \"message\": \"Hello world!\",\n                        \"ackWaitMs\": 60 * 1000,\n                        \"finishTimeoutMs\": 60 * 1000,\n                    }\n                }\n            ],\n        },\n    )\n    # [END howto_operator_gcp_tasks_send_message_to_task_from_id]\n\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule of 'all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_699_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231185}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an inter-DAG dependency workflow that demonstrates how two separate Airflow DAGs can synchronize task execution using external task markers and sensors. Create a parent DAG that signals completion and a child DAG that waits for the parent's specific task to finish before proceeding.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_dependency_sensor():\n    @task()\n    def wait_for_it():\n        print(\"Hey, I'm waiting!\")\n        return \"whatever\"\n\n    @task()\n    def expect_said_hello():\n        print(\"Yay, hello was said!\")\n\n    wait_for_it() >> expect_said_hello()\n\n\nexample_dag_dependency_sensor()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_674_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231221}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an end-to-end machine learning workflow for automated model training and deployment using Google Vertex AI. Create a comprehensive data science pipeline that demonstrates training, evaluation, and deployment of machine learning models across different data types and use cases.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Auto ML operations.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom copy import deepcopy\n\nimport pytest\n\ntry:\n    from airflow.models.dag import DAG\nexcept ImportError:\n    # this import should trigger a warning, but is hidding in mypy\n    # nopep8\n    from airflow.utils.dag import DAG\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import GoogleCloudSession\nfrom system.test_utils.utils import environ_mode_on_off\n\npytestmark = pytest.mark.skipif(\"'requests' not in os.environ.get('AIRFLOW__API__', '').lower()\")\nos.environ[\"AIRFLOW__API__REQUESTS_DO_NOT_CACHE\"] = \"1\"\n\nPROJECT_ID = os.environ.get(\"SYSTEM_GCP_PROJECT\")\nENV_ID = os.environ.get(\"SYSTEM_GCP_ENV\")\n\nDAG_ID = \"vertex_ai_auto_ml_operations\"\nREGION = \"us-central1\"\nMAINTENANCE_REVISION = \"v2beta1-alpha-1\"\nMODEL_DISPLAY_NAME = f\"auto-ml-model-{ENV_ID}\"\nMODEL = {\n    \"display_name\": MODEL_DISPLAY_NAME,\n    \"training_config\": {\n        \"prediction_type\": \"classification\",\n        \"online_prediction_enabled\": False,\n        \"training_fraction_test\": 0.2,\n        \"validation_fraction_test\": 0.2,\n    },\n}\nMODEL_TF_XLA_CONFIG = {\n    \"python_module\": \"example_gcp_vertex_ai.auto_ml.custom_training_custom_tf_xla_script_classification\",\n}\nDISPLAY_NAME_TRAIN = \"custom-training-deployment\"\nDEPLOYED_MODEL_DISPLAY_NAME = f\"deployed-auto-ml-model-{ENV_ID}\"\nTEST_DATASET = {\n    \"display_name\": \"test_dataset\",\n    \"metadata_schema_uri\": schema.dataset.metadata.image,\n    \"metadata\": {\n        \"image_classification_dataset_metadata\": {\n            \"global_shard_count\": 1,\n            \"global_chunk_count\": 1,\n            \"image_shards\": [\n                {\"shard_index\": 0, \"recursive_download\": True},\n            ],\n        }\n    },\n}\nTRAINING_PIPELINE = {\n    \"display_name\": \"train-pipeline\",\n    \"deployment_target\": {\"region\": REGION, \"resource_id\": DISPLAY_NAME_TRAIN},\n    \"jobs\": [\n        {\n            \"image_matching\": {\n                \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n                \"fix_iam_policy\": True,\n            },\n            \"python_package_install\": {\n                \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n            },\n            \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n            \"call_function\": {\n                \"source_code_path\": [str((GoogleCloudSession().client_api.BaseClient.api_base_path))\n                                     + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"],\n                \"\u5b9aition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n                \"function_name\": \"call_function_python_script_example\",\n            },\n        }\n    ],\n}\n\nwith environ_mode_on_off(\"VERTEX_AI_TEST_API_VERSION\", \"v1\"):\n    from system.google.cloud import vertex_ai_service  # isort:skip\n\n    SERVICE = vertex_ai_service.VertexAIService(organization=PROJECT_ID, location=REGION)\n\n\ndef test_create_deployment():\n    deployment = {\n        \"display_name\": f\"tmp-deployment-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]},\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/scripts/echo.py\"],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"print_it\",\n        },\n    }\n    response = SERVICE.create_deployment(deployment, REGION)\n    assert response\n\n\ndef test_list_deployments():\n    # [START how_to_cloud_vertex_ai_list_deployments]\n    response = SERVICE.list_deployments(REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_list_deployments]\n\n    # [START how_to_cloud_vertex_ai_get_deployment]\n    request = {\"name\": str(response.deployments[0].name)}\n    response = SERVICE.get_deployment(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_deployment]\n\n    # [START how_to_cloud_vertex_ai_delete_deployment]\n    request = {\"name\": str(response.deployments[0].name)}\n    SERVICE.delete_deployment(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_deployment]\n\n\ndef test_update_deployment():\n    # [START how_to_cloud_vertex_ai_update_deployment]\n    deployment = {\n        \"display_name\": f\"tmp-deployment-update-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/scripts/echo.py\"],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"print_it\",\n        },\n    }\n    response = SERVICE.update_deployment(deployment, REGION)\n    assert response\n    # [END how_to_cloud_vertex_ai_update_deployment]\n\n\ndef test_long_running_update_deployment():\n    # [START how_to_cloud_vertex_ai_long_running_update_deployment]\n    deployment = {\n        \"display_name\": f\"tmp-deployment-update-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/scripts/echo.py\"],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"print_it\",\n        },\n    }\n\n    operation = SERVICE.long_running_update_deployment(deployment, MAINTENANCE_REVISION, REGION)\n    # [END how_to_cloud_vertex_ai_long_running_update_deployment]\n    operation.name  # will throw AttributeError if the operation is done\n    # [START how_to_cloud_vertex_ai_wait_for_operation]\n    wait_for_operation_response = SERVICE.wait_for_operation(operation, REGION)\n    assert wait_for_operation_response.done()\n    # [END how_to_cloud_vertex_ai_wait_for_operation]\n    assert wait_for_operation_response.name\n\n\ndef test_list_models():\n    # [START how_to_cloud_vertex_ai_list_models]\n    response = SERVICE.list_models(REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_list_models]\n\n    # [START how_to_cloud_vertex_ai_get_model]\n    request = {\"name\": str(response.models[0].name)}\n    response = SERVICE.get_model(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_model]\n\n    # [START how_to_cloud_vertex_ai_delete_model]\n    request = {\"name\": str(response.models[0].name)}\n    SERVICE.delete_model(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_model]\n\n\ndef test_upload_custom_training_job():\n    # [START how_to_cloud_vertex_ai_custom_training]\n    custom_training_job = {\"display_name\": f\"tmp-custom-training-job-{ENV_ID}\", **MODEL}\n    response = SERVICE.upload_custom_training_job(body=custom_training_job, region=REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_custom_training]\n\n    # [START how_to_cloud_vertex_ai_get_custom_training_job]\n    request = {\"name\": str(response.custom_job_ids[0])}\n    response = SERVICE.get_custom_training_job(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_custom_training_job]\n\n    # [START how_to_cloud_vertex_ai_list_deployment_scripts]\n    response = SERVICE.list_deployment_scripts(custom_training_job[\"name\"], REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_list_deployment_scripts]\n\n    # [START how_to_cloud_vertex_ai_delete_custom_training_job]\n    request = {\"name\": str(response.custom_job_ids[0])}\n    SERVICE.delete_custom_training_job(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_custom_training_job]\n\n\ndef test_create_deployment_with_custom_training():\n    # [START how_to_cloud_vertex_ai_create_deployment_with_custom_training]\n    custom_training_job = {\n        \"display_name\": f\"tmp-custom-training-job-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.create_deployment_with_custom_training(\n        body=custom_training_job, name=custom_training_job[\"name\"], region=REGION\n    )\n    assert response\n    # [END how_to_cloud_vertex_ai_create_deployment_with_custom_training]\n\n\ndef test_upload_deployment_script():\n    # [START how_to_cloud_vertex_ai_upload_deployment_script]\n    deployment_script = {\n        \"display_name\": f\"tmp-deployment-script-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.upload_deployment_script(body=deployment_script, region=REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_upload_deployment_script]\n\n    # [START how_to_cloud_vertex_ai_get_deployment_script]\n    request = {\"name\": str(response.deployment_scripts_names[0])]\n    response = SERVICE.get_deployment_script(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_deployment_script]\n\n    # [START how_to_cloud_vertex_ai_delete_deployment_script]\n    request = {\"name\": str(response.deployment_scripts_names[0})}\n    SERVICE.delete_deployment_script(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_deployment_script]\n\n\ndef test_create_deployment_from_store():\n    # [START how_to_cloud_vertex_ai_create_deployment_from_store]\n    deployment = {\n        \"display_name\": f\"tmp-deployment-from-store-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.create_deployment_from_store(body=deployment, region=REGION)\n    assert response\n    # [END how_to_cloud_vertex_ai_create_deployment_from_store]\n\n\ndef test_upload_deployment_script_from_store():\n    # [START how_to_cloud_vertex_ai_upload_deployment_script_from_store]\n    deployment_script = {\n        \"display_name\": f\"tmp-deployment-script-from-store-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.upload_deployment_script_from_store(body=deployment_script, region=REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_upload_deployment_script_from_store]\n\n    # [START how_to_cloud_vertex_ai_get_deployment_script_from_store]\n    request = {\"name\": str(response.deployment_scripts_names[0])]\n    response = SERVICE.get_deployment_script_from_store(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_deployment_script_from_store]\n\n    # [START how_to_cloud_vertex_ai_delete_deployment_script_from_store]\n    request = {\"name\": str(response.deployment_scripts_names[0})}\n    SERVICE.delete_deployment_script_from_store(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_deployment_script_from_store]\n\n\ndef test_create_deployment_from_bucket():\n    # [START how_to_cloud_vertex_ai_create_deployment_from_bucket]\n    deployment = {\n        \"display_name\": f\"tmp-deployment-from-bucket-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.create_deployment_from_bucket(body=deployment, region=REGION)\n    assert response\n    # [END how_to_cloud_vertex_ai_create_deployment_from_bucket]\n\n\ndef test_upload_deployment_script_from_bucket():\n    # [START how_to_cloud_vertex_ai_upload_deployment_script_from_bucket]\n    deployment_script = {\n        \"display_name\": f\"tmp-deployment-script-from-bucket-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    response = SERVICE.upload_deployment_script_from_bucket(body=deployment_script, region=REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_upload_deployment_script_from_bucket]\n\n    # [START how_to_cloud_vertex_ai_get_deployment_script_from_bucket]\n    request = {\"name\": str(response.deployment_scripts_names[0])]\n    response = SERVICE.get_deployment_script_from_bucket(request, REGION)\n    print(response)\n    # [END how_to_cloud_vertex_ai_get_deployment_script_from_bucket]\n\n    # [START how_to_cloud_vertex_ai_delete_deployment_script_from_bucket]\n    request = {\"name\": str(response.deployment_scripts_names[0})}\n    SERVICE.delete_deployment_script_from_bucket(request, REGION)\n    # [END how_to_cloud_vertex_ai_delete_deployment_script_from_bucket]\n\n\ndef test_write_default_deployment():\n    # [START how_to_cloud_vertex_ai_write_default_deployment]\n    deployment = {\n        \"display_name\": f\"tmp-default-deployment-{ENV_ID}\",\n        \"image_matching\": {\n            \"expression\": f\"projects/{PROJECT_ID}/locations/{REGION}/gcs buckets/*\",\n            \"fix_iam_policy\": True,\n        },\n        \"python_package_install\": {\n            \"pip_packages\": [\"airflow\", \"gcsfs\", \"numpy\", \"scikit-learn==0.24.1\"]\n        },\n        \"job_dir\": f\"gs://{TEST_DATASET}/vertex_ai/job/\",\n        \"call_function\": {\n            \"source_code_path\": [\n                str((GoogleCloudSession().client_api.BaseClient.api_base_path)) + \"/aiplatform/python/api/v1beta1/functions\u533f\u540d.py\"\n            ],\n            \"definition\": {\"input_data\": {\"gcs_source\": {\"uri\": TEST_DATASET}}},\n            \"function_name\": \"call_function_python_script_example\",\n        },\n    }\n    SERVICE.write_default_deployment(deployment, REGION)\n    # [END how_to_cloud_vertex_ai_write_default_deployment"}], "metadata": {"file_name": "dag_45_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231557}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create a workflow that demonstrates Azure Synapse integration with Apache Airflow, showing how enterprise data orchestration can be achieved through automated pipeline triggering. Implement a simple DAG that connects cloud data processing services with workflow management.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.synapse import AzureSynapseRunPipelineOperator\n\nEXECUTION_TIMEOUT = int(os.getenv(\"EXECUTION_TIMEOUT\", 60))\nsynapse_conn_id = \"azure_synapse\"\n\nwith models.DAG(\n    dag_id=\"example_synapse_run_pipeline\",\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    execution_timeout=EXECUTION_TIMEOUT,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_synapse_run_pipeline]\n    RunPipeline1 = AzureSynapseRunPipelineOperator(\n        task_id=\"run_pipeline1\",\n        pipeline_name=\"pipeline1\",\n        synapse_conn_id=synapse_conn_id,\n    ),\n    # [END howto_operator_synapse_run_pipeline]\n    # [START howto_operator_synapse_run_pipeline_in_branch]\n    RunPipeline2 = AzureSynapseRunPipelineOperator(\n        task_id=\"run_pipeline2\",\n        pipeline_name=\"pipeline2\",\n        synapse_conn_id=synapse_conn_id,\n        branch_name=\"branch1\",\n    ),\n    # [END howto_operator_synapse_run_pipeline_in_branch]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_973_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.72316}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python code to extract sentences from a text while keeping only the words that contain only alphabetic characters?"}, {"role": "assistant", "content": "import re\n\ndef extract_alphabetic_words(text):\n    # Use regex to split the text into words and filter out non-alphabetic words\n    return re.findall(r'\\b[a-zA-Z]+\\b', text)\n\n# Example usage:\ntext = \"This is a test string with some words like 'Python' and 'Java'.\"\nalphabetic_words = extract_alphabetic_words(text)\nprint(alphabetic_words)"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.723166}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function that converts a given word to Pig Latin?"}, {"role": "assistant", "content": "def pig_latin(word):\n    # Define vowels in English\n    vowels = 'aeiouAEIOU'\n    \n    # Check if the first letter is a vowel\n    if word[0] in vowels:\n        return word + 'way'\n    \n    # If the first letter is a consonant, move it to the end and add \"ay\"\n    else:\n        return word[1:] + word[0] + 'ay'\n\n# Example usage:\nprint(pig_latin(\"hello\"))  # Output: helloway\nprint(pig_latin(\"apple\"))  # Output: appleway"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231712}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Asana task management workflow that demonstrates creating, finding, updating, and deleting tasks programmatically using Airflow. Build a DAG that showcases the full lifecycle of task management operations in an automated, sequential manner.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Asana Task Operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.asana.operators.asana import (\n    AsanaCreateTaskOperator,\n    AsanaDeleteTaskOperator,\n    AsanaFindTaskOperator,\n    AsanaUpdateTaskOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_asana\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START asana_find_task_operator]\n    find_task_by_description = AsanaFindTaskOperator(task_status=\"all\", description=\"Remove me please\")\n    # [END asana_find_task_operator]\n\n    @task\n    def delete_task(task_id):\n        asana = {}\n        try:\n            asana = asana_client.task_get(task_id)\n        except Exception:\n            pass\n        finally:\n            asana_client.task_delete(asana[\"id\"])\n\n    # [START asana_update_task_operator]\n    update_task_description = AsanaUpdateTaskOperator(\n        task_id=\"update-task-description\",\n        status={\"value\": \"completed\"},\n        description=\"Updated description!\",\n    )\n    # [END asana_update_task_operator]\n\n    # [START asana_create_task_operator]\n    create_task = AsanaCreateTaskOperator(\n        task_id=\"create-task\",\n        project_id=project_id,\n        description=\"Created description!\",\n        status={\"value\": \"completed\"},\n    )\n    # [END asana_create_task_operator]\n\n    # [START asana_delete_task_operator]\n    delete_task_create = AsanaDeleteTaskOperator(\n        task_id=\"delete-task-create\", task_parameters={\"id\": \"{{ task_instance.xcom_pull('create-task')['id'] }}\"\n        }\n    )\n    # [END asana_delete_task_operator]\n\n    delete_task(find_task_by_description.output)\n\n    find_task_by_description >> delete_task\n    create_task >> delete_task_create\n\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1950_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231777}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Hello Qwen, how can I retrieve data from a SQL database using Python?"}, {"role": "assistant", "content": "import sqlite3\n\n# Connect to the SQLite database\nconn = sqlite3.connect('example.db')\ncursor = conn.cursor()\n\n# Execute a query\ncursor.execute(\"SELECT * FROM users\")\n\n# Fetch all rows\nrows = cursor.fetchall()\nprint(rows)\n\n# Close the connection\nconn.close()"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.723185}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data privacy workflow that automatically de-identifies sensitive phone numbers in tabular data using Google Cloud's Data Loss Prevention (DLP) service. Create an Airflow pipeline that demonstrates how to mask, template, and optionally reverse sensitive information transformations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample DLP App Engine trigger usage.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.models.trigger_external import ExternalTaskFailedTrigger, ExternalTaskTrigger\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.hooks.dlp import GoogleCloudDLPHook\nfrom airflow.providers.google.cloud.operators.dlp import (\n    CloudDLPCreateStoredInfoTypeOperator,\n    CloudDLPDeleteStoredInfoTypeOperator,\n    CloudDLPExecuteDLPJobOperator,\n    CloudDLPGetDLPJobOperator,\n    CloudDLPListStoredInfoTypesOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom system.openlineage.operator import OpenLineageTestOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\nDLP_LOCATION = os.environ.get(\"DLP_LOCATION\", \"us-central1\")\n\nSTORAGE_BUCKET_NAME = f\"bucket_{ENV_ID}\"\nDLP_STORIED_INFO_TYPE_NAME = os.environ.get(\"DLP_STORIED_INFO_TYPE_NAME\", \"content\")\nDLP_STORIED_INFO_TYPE_BODY = {\n    \"inspect_config\": {\"language_model_inspect_config\": {}},\n}\nDLP_EXTRACTED_SensitiveInfoItem = {\n    \"start_offset\": {\"byte_position\": 36790},\n    \"end_offset\": {\"byte_position\": 36815},\n    \"entity\": {\"value\": \"LOVE\"},  # Gender\n}\nDLP_JOB_NAME = os.environ.get(\"DLP_JOB_NAME\", \"deidentify_data_lake_job_123\")\n\nwith DAG(\n    dag_id=\"dlp_stored_info_type_operator\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"dlp\", \"stored-info-type\"],\n):\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=STORAGE_BUCKET_NAME)\n\n    get_dlp_job = CloudDLPGetDLPJobOperator(\n        task_id=\"get_dlp_job\", project_id=PROJECT_ID, job_id=DLP_JOB_NAME\n    )\n\n    list_stored_info_types = CloudDLPListStoredInfoTypesOperator(task_id=\"list_stored_info_types\")\n\n    # [START howto_operator_dlp_create_stored_info_type]\n    create_stored_info_type = CloudDLPCreateStoredInfoTypeOperator(\n        task_id=\"create_stored_info_type\",\n        project_id=PROJECT_ID,\n        location=DLP_LOCATION,\n        stored_info_type=DLP_STORIED_INFO_TYPE_BODY,\n        name=DLP_STORIED_INFO_TYPE_NAME,\n    )\n    # [END howto_operator_dlp_create_stored_info_type]\n\n    # [START howto_trigger_dlp_create_stored_info_type]\n    execute_dlp_job = ExternalTaskTrigger(\n        task_id=\"execute_dlp_job\",\n        external_task_id=\"execute_dlp_job_123\",\n        project_id=PROJECT_ID,\n        body=DLP_STORIED_INFO_TYPE_BODY,\n        stored_info_type_name=DLP_STORIED_INFO_TYPE_NAME,\n    )\n    # [END howto_trigger_dlp_create_stored_info_type]\n\n    # [START howto_operator_dlp_update_stored_info_type]\n    update_stored_info_type = CloudDLPCreateStoredInfoTypeOperator(\n        task_id=\"update_stored_info_type\",\n        project_id=PROJECT_ID,\n        location=DLP_LOCATION,\n        stored_info_type=DLP_STORIED_INFO_TYPE_BODY,\n        name=DLP_STORIED_INFO_TYPE_NAME,\n    )\n    # [END howto_operator_dlp_update_stored_info_type]\n\n    # [START howto_trigger_dlp_update_stored_info_type]\n    update_execute_dlp_job = ExternalTaskTrigger(\n        task_id=\"update_execute_dlp_job\",\n        external_task_id=\"execute_dlp_job_123\",\n        project_id=PROJECT_ID,\n        body=DLP_STORIED_INFO_TYPE_BODY,\n        stored_info_type_name=DLP_STORIED_INFO_TYPE_NAME,\n    )\n    # [END howto_trigger_dlp_update_stored_info_type]\n\n    # [START howto_operator_dlp_delete_stored_info_type]\n    delete_stored_info_type = CloudDLPDeleteStoredInfoTypeOperator(\n        task_id=\"delete_stored_info_type\",\n        project_id=PROJECT_ID,\n        location=DLP_LOCATION,\n        name=DLP_STORIED_INFO_TYPE_NAME,\n        is_delete_trashed=True,\n    )\n    # [END howto_operator_dlp_delete_stored_info_type]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=STORAGE_BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    check_openlineage_events = OpenLineageTestOperator(\n        task_id=\"check_openlineage_events\",\n        file_json=XComArg(get_dlp_job, key=\"event_json\"),\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> get_dlp_job\n        >> list_stored_info_types\n        # TEST BODY\n        >> create_stored_info_type\n        >> execute_dlp_job\n        # TEST TEARDOWN\n        >> update_stored_info_type\n        >> update_execute_dlp_job\n        >> delete_stored_info_type\n        >> delete_bucket\n        >> check_openlineage_events\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_503_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7231963}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "Can you provide a Python function that checks if a given number is a prime number? If the number is prime, the function should return `True`; otherwise, it should return `False`."}, {"role": "assistant", "content": "def is_prime(n):\n    \"\"\"\n    Check if a given number n is a prime number.\n    \n    Args:\n    n (int): The number to check.\n    \n    Returns:\n    bool: True if n is a prime number, False otherwise.\n    \"\"\"\n    # Handle edge cases\n    if n <= 1:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    \n    # Check for factors from 3 to the square root of n\n    for i in range(3, int(n**0.5) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\n# Example usage\nprint(is_prime(6))   # Output: False\nprint(is_prime(7))   # Output: True"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232037}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that illustrates the complete lifecycle of a file transfer task, including dynamic environment setup, S3 bucket creation, local file generation, file upload, and subsequent resource cleanup. Implement a systematic approach to demonstrate cloud file transfer best practices.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.operators.sqs import (\n    SQSCreateQueueOperator,\n    SQSDeleteQueueOperator,\n    SqsSendMessageOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nfrom system.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_JSON = {\n    \"Body\": b'{\"6b49b285-d7c2-4d2d-b1f2-a3faaebbce9e\":{\"$class\":\"org.apache.ignite.IgniteException\",\"message\":\"java.net.SocketTimeoutException: Timeout while waiting for message\"},\"opType\":\"ERROR\",\"instanceId\":\"6b49b285-d7c2-4d2d-b1f2-a3faaebbce9e\",\"sequenceNumber\":2}',\n    \"End\": b\"\",\n    \"DeliveryDiscarded\": b\"\",\n    \"DeliverToQueueHeaders\": b\"\",\n    \"Priority\": 0,\n    \"MessageAttributes\": {},\n    \"MessageId\": \"efdc6aeb-c0f2-4aeb-afdc-6b49b285d7c2\",\n    \"Redelivered\": False,\n    \"ExpirationTimeMS\": 0,\n    \"ContentLength\": 28,\n    \"ContentType\": \"application/json\",\n    \"UserProperties\": {},\n}\n\n\n@task\ndef generate_id():\n    \"\"\"\n    Generate an unique id which will be used to create queue name and topic name\n    \"\"\"\n    return f\"{Path(__file__).parent.name}_{datetime.now().strftime('%Y_%m_%d')}\"\n\n\n@task\ndef create_content_queue(queue_name):\n    log.info(f\"Creating queue {queue_name}...\")\n    S3Hook().conn.create_queue(QueueName=queue_name)\n\n    log.info(f\"Creating SQS message with content in the queue {queue_name}...\")\n    S3KeySensor(\n        task_id=\"wait_for_file\",\n        s3_key=\"{{ task_instance.xcom_pull('generate_id') }}.json\",\n        check_path=f\"example/sqs/{{{task_instance.xcom_pull('generate_id') }}}.json\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )  # Wait for JSON file to appear\n\n    with BashOperator(task_id=\"send_message_via_sqs\", bash_command=f\"echo '{{ task_instance.xcom_pull('generate_id') }}.json | sqs send-message --queue-url {{ task_instance.xcom_pull(['create_queue', 'ReturnUrl'], ''}\"))\"):\n        log.info(f\"SQS message sent to queue {queue_name}\")\n\n    log.info(f\"Deleted queue {queue_name}\")\n    S3Hook().conn.delete_queue(queue_name)\n\n\n@task\ndef get_output_message(message_attributes, message_body):\n    return {\n        \"OutputMessage\": f\"Message received via SQS with following attributes {message_attributes} and body\\n{message_body.decode()}\",\n    }\n\n\n@task\ndef save_response_to_file(output_message, filename=\"response.json\"):\n    \"\"\"\n    Save response message body along with request details into file for further analysis\n    \"\"\"\n    log.info(\"Fetching response and saving into file %s\", filename)\n    with open(filename, \"w\") as response_file:\n        json.dump(output_message, response_file)\n\n\nwith DAG(\n    dag_id=\"sqs_with_s3_in_one_system_test\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    conn_id = f\"aws_conn_{env_id}\"\n    sbk_id = f\"sbm_bucket_{env_id}\"\n    akid = f\"aks_{env_id}\"\n    role_name = f\"role_{env_id}\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", s3_bucket=sbk_id)\n\n    # [START howto_sensor_sqs]\n    wait_for_message = SqsSensor(\n        task_id=\"wait_for_message\",\n        queue_url=\"{{ task_instance.xcom_pull(['create_queue', 'return_url'], '') }}\",\n        poke_interval=5,\n        timeout=60 * 5,\n    )\n    # [END howto_sensor_sqs]\n\n    create_queue = SQSCreateQueueOperator(\n        task_id=\"create_queue\",\n        queue_name=\"{{ task_instance.xcom_pull(generate_id, '') }}\",\n        connection_id=conn_id,\n    )\n\n    send_message = SqsSendMessageOperator(\n        task_id=\"send_message\",\n        queue_url=\"{{ task_instance.xcom_pull(['create_queue', 'queue_url'], '') }}\",\n        message=SAMPLE_JSON[\"Body\"],\n    )\n\n    get_attribute_from_message = PythonOperator(\n        task_id=\"get_attribute_from_message\",\n        python_callable=get_output_message,\n        op_kwargs={\n            \"message_attributes\": \"{{ task_instance.xcom_pull(sqs_send_message, 'ReturnValue') | from_json }\"},\n            \"message_body\": SAMPLE_JSON[\"Body\"].decode(),\n        },\n    )\n\n    save_response_to_file = save_response_to_file(\n        output_message=\"{{ task_instance.xcom_pull(sqs_send_message, 'ReturnValue') }}\"\n    )\n\n    delete_queue = SQSDeleteQueueOperator(\n        task_id=\"delete_queue\",\n        queue_url=\"{{ task_instance.xcom_pull(['create_queue', 'queue_url'], '') }}\",\n        connection_id=conn_id,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=sbk_id,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_s3_bucket\n        >> wait_for_message\n        >> create_queue\n        >> send_message\n        >> get_attribute_from_message\n        >> save_response_to_file\n        # TEST TEARDOWN\n        >> delete_queue\n        >> delete_s3_bucket\n    )\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1503_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232177}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an automated machine learning pipeline for training and deploying a housing price prediction model using Google Vertex AI. Create a workflow that handles dataset creation, model training, versioning, and deployment with robust error handling and cloud infrastructure management.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Vertex AI service testing Model Service.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom google.cloud.aiplatform import schema\nfrom system.google import third_party_config\nfrom system.google import workspace_config\n\nDAG_ID = \"vertex_ai_model_service\"\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\n\nREGION = \"us-central1\"\nDISPLAY_NAME_TRAIN = f\"model-service-display-name-train-{ENV_ID}\"\nDISPLAY_NAME_PREDICT = f\"model-service-display-name-predict-{ENV_ID}\"\nMODEL_TYPE = \"USER_EXPOSED\"\n\n# For tests purpose only\nTEST_DATA_SAMPLE_GCS_BUCKET = f\"bucket_{ENV_ID}\"\nRESOURCE_DATA_BUCKET = \"airflow-system-tests-resources\"\n\n\ndef generate_vision_image_object(local_file):\n    \"\"\"Generates vision image object from local file.\"\"\"\n\n    # local_file should be of format .jpg or .jpeg\n    with open(local_file, \"rb\") as image_file:\n        return {\"image_content\": image_file.read()}\n\n\nSERVICE_ACCOUNT = third_party_config.getServiceAccount()\nWORKSPACE_CONNECTION = workspace_config.WorkspaceServiceConnection(\n    project=PROJECT_ID, location=REGION, socket_timeout=5\n)\n\n\nwith DAG(\n    f\"{DAG_ID}_model_service\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"vertex_ai\", \"model_service\"],\n) as dag:\n    set_up_workspace = WORKSPACE_CONNECTION.create_or_update_vertex_ai_workspace(\n        region=REGION,\n        workspace_id=f\"workspace-{ENV_ID}\",\n        display_name=\"workspace\",\n    )\n\n    create_train_dataset = third_party_config.create_dag_for_dataset_operator(\n        task_id=\"create_train_dataset\",\n        dataset_schema=schema.dataset.image_classification_multi_label(),\n        resource_data_bucket=RESOURCE_DATA_BUCKET,\n        local_file_path=f\"datasets/image-classification-dataset/train/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset-image-train-000x.jpg\",\n    )\n\n    create_test_dataset = third_party_config.create_dag_for_dataset_operator(\n        task_id=\"create_test_dataset\",\n        dataset_schema=schema.dataset.image_classification_multi_label(),\n        resource_data_bucket=RESOURCE_DATA_BUCKET,\n        local_file_path=f\"datasets/image-classification-dataset/test/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset-image-test-000x.jpg\",\n    )\n    ude_create_training_pipeline = third_party_config.create_dag_for_pipeline_operator(\n        task_id=\"uide_create_training_pipeline\",\n        pipeline_display_name=DISPLAY_NAME_TRAIN,\n        pipeline_filename=\"image_classification_pipeline_repeated.py\",\n        required_resources={\n            \"vertex_ai_service_account\": SERVICE_ACCOUNT,\n            \"workspace_connection\": WORKSPACE_CONNECTION,\n            \"project_id\": PROJECT_ID,\n            \"location\": REGION,\n        },\n        additional\uff36\uff30\uff33={\"workspace_id\": set_up_workspace.workspace_id},\n    )\n\n    ude_patch_training_pipeline = third_party_config.create_dag_for_pipeline_operator(\n        task_id=\"uide_patch_training_pipeline\",\n        pipeline_display_name=DISPLAY_NAME_TRAIN,\n        pipeline_filename=\"image_classification_pipeline_repeated.py\",\n        updated_elements={\n            \"train_dataset_element\": generate_vision_image_object(f\"{WORKSPACE_CONNECTION.resource_data_bucket}/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset/train/\"),\n        },\n        optional\u016b\uff44 elements={\"test_dataset_element\": generate_vision_image_object(f\"{WORKSPACE_CONNECTION.resource_data_bucket}/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset/test/\")},\n        additional\uff36\uff30\uff33={\"workspace_id\": set_up_workspace.workspace_id},\n    )\n    ude_delete_training_pipeline = third_party_config.create_dag_for_pipeline_operator(\n        task_id=\"uide_delete_training_pipeline\",\n        pipeline_display_name=DISPLAY_NAME_TRAIN,\n        pipeline_filename=\"image_classification_pipeline_repeated.py\",\n        should_delete_dataset=True,\n        trigger_rule=\"all_done\",\n        break_on_negative_event=True,\n    )\n\n    create_predict_dataset = third_party_config.create_dag_for_dataset_operator(\n        task_id=\"create_predict_dataset\",\n        dataset_schema=schema.dataset.image_classification_multi_label(),\n        resource_data_bucket=RESOURCE_DATA_BUCKET,\n        local_file_path=f\"datasets/image-classification-dataset/train/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset-image-train-000x.jpg\",\n    )\n    ude_create_deployment = third_party_config.create_dag_for_pipeline_operator(\n        task_id=\"uide_create_deployment\",\n        pipeline_display_name=DISPLAY_NAME_PREDICT,\n        pipeline_filename=\"image_classification_pipeline_unrepeated.py\",\n        required_resources={\n            \"vertex_ai_service_account\": SERVICE_ACCOUNT,\n            \"workspace_connection\": WORKSPACE_CONNECTION,\n            \"project_id\": PROJECT_ID,\n            \"location\": REGION,\n        },\n        initial_deployment_config={\n            \"image_deployment_config\": {\n                \"container_spec\": {\n                    \"image_uri\": \"{{ti.xcom_pull('uide_create_training_pipeline')['input']['images'][0]['uri']}}\"\n                }\n            }\n        },\n        optional_elements={\n            \"predict_dataset_element\": generate_vision_image_object(f\"{WORKSPACE_CONNECTION.resource_data_bucket}/{TEST_DATA_SAMPLE_GCS_BUCKET}/image-classification-dataset/test/\")\n        },\n    )\n\n    ued_get_deployment = third_party_config.create_dag_for_get_operator(\n        task_id=\"ued_get_deployment\",\n        deployment_id=\"{{ti.xcom_pull('uide_create_deployment')}}\",\n        image=\"${VERTEX_AI_DEPLOYMENT_IMAGE}\",\n        container_spec={\"image_uri\": \"{{ti.xcom_pull('uide_create_training_pipeline')['output']['images'][0]['uri']}}}\"},\n    )\n\n    ued_list_deployments = third_party_config.create_dag_for_list_operator(\n        task_id=\"ued_list_deployments\",\n        image=\"${VERTEX_AI_DEPLOYMENT_IMAGE}\",\n        page_size=5,\n    )\n\n    ued_stop_deployment = third_party_config.create_dag_for_deactivate_operator(\n        task_id=\"uide_stop_deployment\",\n        deployment_id=\"{{ti.xcom_pull('uide_create_deployment')}}\",\n    )\n\n    ued_start_deployment = third_party_config.create_dag_for_activate_operator(\n        task_id=\"uide_start_deployment\",\n        deployment_id=\"{{ti.xcom_pull('uide_create_deployment')}}\",\n    )\n\n    delete_predict_dataset = third_party_config.create_dag_for_dataset_operator(\n        task_id=\"delete_predict_dataset\",\n        dataset_id=\"{{ti.xcom_pull('create_predict_dataset')}}\",\n        trigger_rule=\"all_done\",\n    )\n\n    [\n        create_train_dataset,\n        create_test_dataset,\n        ude_create_training_pipeline,\n        ude_patch_training_pipeline,\n        ude_delete_training_pipeline,\n        create_predict_dataset,\n        ude_create_deployment,\n        ued_get_deployment,\n        ued_list_deployments,\n        ued_stop_deployment,\n        ued_start_deployment,\n        delete_predict_dataset,\n    ] >> set_up_workspace\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1718_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232337}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates remote task scheduling and execution using Azure's cloud infrastructure, showing how to configure a computational job with specific VM specifications and a basic command line task. Illustrate the process of defining a cloud-based computational workflow with precise infrastructure and execution parameters.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.microsoft.azure.operators.advisor import AzureAdvisorListPatchesOperator\n\nREMOTE_CONNECTION_ID = os.environ.get(\"REMOTE_CONNECTION_ID\", \"remote_python\")\n\nwith models.DAG(\n    \"example_advisor_list_patches\",\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    # [START advisor_list_patches]\n    list_patches_task = AzureAdvisorListPatchesOperator(task_id=\"advisor_list_patches\", )\n    # [END advisor_list_patches]\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_965_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232373}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that manages Power BI workspace and dataset operations, focusing on retrieving workspace information and refreshing datasets automatically. Create a workflow that can scan workspace modifications, track dataset refresh status, and handle administrative Power BI tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.power_bi import (\n    PowerBIDeleteDatasetOperator,\n    PowerBIRenameDatasetOperator,\n    PowerBISynchronizeDatasetsOperator,\n)\nfrom airflow.providers.microsoft.azure.transfers.power_bi_to_power_bi import PowerBiToPowerBiTransferOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"AZURE_PROJECT_ID\")\nDAG_ID = \"example_powerbi_to_powerbi\"\n\nresourcedb = Connection(\n    conn_id=\"powerbi_default\",\n    schema=\"dbo\",\n    foreign_key_constraints=[\"fk_table1_connection1\"],\n    net_id=\"acc1@contoso.onmicrosoft.com\",\n    table_mappings=[\n        {\"source\": [\"table1_col1\"], \"destination\": [\"table1_col1\"], \"type\": \"VARCHAR\"},\n        {\"source\": [\"table1_col2\"], \"destination\": [\"table2_col2\"], \"type\": \"VARCHAR\"},\n    ],\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"powerbi\"],\n) as dag:\n    @task\n    def get_workspaceid():\n        from airflow.providers.microsoft.azure.hooks.powerbi import PowerBIHook\n\n        powerbi_hook = PowerBIHook()\n        return powerbi_hook.get_workspace_id(project_id=PROJECT_ID)\n\n    sync_task = PowerBISynchronizeDatasetsOperator(\n        task_id=\"sync_task\",\n        source_project_id=f\"projects/{PROJECT_ID}\",\n        source_workspace_id=get_workspaceid().output,\n        destination_project_id=f\"projects/{PROJECT_ID}\",\n        destination_workspace_id=get_workspaceid().output,\n    )\n\n    # [START howto_operator_powerbi_to_powerbi_transfer]\n    copy_task = PowerBiToPowerBiTransferOperator(\n        task_id=\"copy_task\",\n        # It is required to set one of the sources or destinations\n        # In this case, we set it to be the source\n        source_conn_id=\"powerbi_default\",\n        source_dataset_identifier=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') }}\",\n        destination_conn_id=\"powerbi_default\",\n        destination_dataset_identifier=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') }}\"),\n    # [END howto_operator_powerbi_to_powerbi_transfer]\n\n    delete_dataset1 = PowerBIDeleteDatasetOperator(\n        task_id=\"delete_dataset1\",\n        dataset_id=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') }}\"),\n    delete_dataset2 = PowerBIDeleteDatasetOperator(\n        task_id=\"delete_dataset2\",\n        dataset_id=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') ')}}\"),\n    rename_dataset1 = PowerBIRenameDatasetOperator(\n        task_id=\"rename_dataset1\",\n        dataset_id=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') }}\",\n        new_dataset_name=\"renamed_dataset\"),\n    rename_dataset2 = PowerBIRenameDatasetOperator(\n        task_id=\"rename_dataset2\",\n        dataset_id=\"{{ task_instance.xcom_pull(task_ids='sync_task', key='result') }}\",\n        new_dataset_name=\"renamed_dataset\"),\n\n    delete_conn = Connection(\n        conn_id=\"powerbi_default\",\n        conn_type=\"powerbi\",\n    )\n    delete_conn_extra = {\n        \"project_id\": PROJECT_ID,\n    }\n\n    (sync_task >> copy_task >> (delete_dataset1 >> rename_dataset1 >> delete_conn >> rename_dataset2 >> delete_conn))\n\n    if ENV_ID == \"test\":\n        [get_workspaceid() >> sync_task] << delete_conn\n\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2163_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232463}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data pipeline that uses Papermill to dynamically execute a Jupyter notebook with date-based parameters, generating a new output notebook for each workflow run. The pipeline should verify the notebook's execution by checking a specific message generated during the process.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "\"\"\"\nThis DAG will use Papermill to run the notebook \"example_nb_pandas.ipynb\" against the dataframe provided\nWhen this DAG is triggered, a number of tasks will be canceled because of missing args:\n* run_this_daily because it depends on the timing of the parent dag\n* the cells in test_loop will be deleted and cannot be created again\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\ntry:\n    from airflow.sdk import DAG, task\nexcept ImportError:\n    # This import is deprecated, use `from airflowSDK import DAG` instead.\n    from airflowSDK import DAG\n\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nPATH_TO_NOTEBOOK = os.path.dirname(os.path.realpath(__file__))\nPATH_TO_DATA = os.path.join(os.path.dirname(os.path.realpath(__file__)))\n\n\nwith DAG(\n    dag_id=\"example_papermill_exec_python\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n):\n    @task\n    def generate_data():\n        import numpy as np\n        import pandas as pd\n\n        np.random.seed(42)\n        import random\n\n        import sys\n\n        if len(sys.argv) < 2:\n            print(\"Usage: generate_data <size>\")\n            return None\n\n        size = int(sys.argv[1])\n        df = pd.DataFrame({\n            \"P\": list(range(size)),\n            \"A\": np.random.randint(0, 100, size),\n            \"B\": np.random.randint(0, 100, size),\n            \"C\": [\"\".join(random.choices(string.ascii_uppercase, k=5)) for _ in range(size)]\n        })\n        return df\n\n    @task\n    def run_notebook(notebook, **context):\n        from IPython.core.interactiveshell import InteractiveShell\n\n        interactive = InteractiveShell.instance()\n        path_to_notebook = interactive.get_active_file()\n\n        command = f\"papermill --log_level critical {path_to_notebook} '{notebook}'\"\n        interactive.get_logger().info(command)\n        output = interactive.get_logger().INFO.wait_for_output()\n        print(output)\n\n    cancel_this = BashOperator(task_id=\"cancel_this\", bash_command=\"kill $(pgrep -f cancel_that)\")\n    run_this_daily = BashOperator(task_id=\"run_this_daily\", bash_command=\"sleep 20\")\n    cancel_that = BashOperator(task_id=\"cancel_that\", bash_command=\"kill $(pgrep -f run_this_daily)\")\n\n    generate_data() >> cancel_this\n    run_this_daily >> cancel_that\n    run_this_daily >> run_notebook(Path_TO_DATA + \"/example_nb_pandas.py\", ti_key=\"cancel_that\")"}], "metadata": {"file_name": "dag_1381_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984409.7232537}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that demonstrates the sequential management of a cloud computing resource, showing how to programmatically control a SageMaker notebook instance through different operational states. Illustrate the step-by-step process of resource provisioning, activation, deactivation, and cleanup using AWS SageMaker operators.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.sagemaker import SageMakerHook\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerCreateNotebookInstanceOperator,\n    SageMakerDeleteNotebookInstanceOperator,\n    SageMakerStartNoteBookOperator,\n    SageMakerStopNoteBookOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerNotebookSensor,\n    SageMakerNotebookStarterSensor,\n    SageMakerNoteBookSensorDone,\n    SageMakerNoteBookStarterSensorDone,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_sagemaker_notebook\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_DATA_BUCKET = \"sample-sagemaker-notebook\"\nS3_OUTPUT_PATH = \"s3://output/path\"\n\n# ID of the Linux machine instances\n# Usually it's InstanceId-1, Idx-2,..., but it can be NULL if hardwarespotting is disabled\nINSTANCES_LIST = [\"InstanceId-1\", \"InstanceId-2\"]\n\n# NoDMI is true when the training job uses a GPU framework, such as Tensorflow or MXNet\n# And a i3.xlarge with ENA support is required for such tasks\nNO_DMI = True\nGPU_FRAMWORK = True\n\n\n@task\ndef create_sample_data_bucket():\n    \"\"\"Creates a sample S3 bucket\"\"\"\n    return SAMPLE_DATA_BUCKET\n\n\n@task\ndef get_ami_id(role_arn):\n    \"\"\"Returns the Amazon EKS compatible AMI id\"\"\"\n    response = boto3.client(\"ec2\").describe_images(\n        Filters=[\n            {\"Name\": \"description\", \"Values\": [\"Amazon EKS Helper AMI\"]},\n            {\"Name\": \"computer-name\", \"Values\": [\"ec2-helper\"]},\n            {\n                \"Name\": \"architecture\",\n                \"Values\": [\n                    \"x86_64\",\n                ],  # Note: This eks assistantami only supports EC2 instances.\n            },\n            {\n                \"Name\": \"platform-description\",\n                \"Values\": [\"AWS-owned (machine image)\"],\n            },  # Ensure the filtered AMI is AWS-owned\n            {\n                \"Name\": \"root-device-type\",\n                \"Values\": [\"eaxd\"],\n            },  # Ensure the filtered AMI has ephemeral storage\n            {\n                \"Name\": \"owner-alias\",\n                \"Values\": [\"amazon\"],\n            },  # Filter images owned by Amazon\n            {\n                \"Name\": \"description\",\n                \"Values\": [\"Amazon EKS XCOM Support IAM Role\"],\n            },  # Ensure the filtered IAM role is used for XCOM\n        ],\n        Owners=[\"amazon\"],\n    )\n    return next(image[\"ImageId\"]\n               for image in response[\"Images\"]\n               if NO_DMI or image[\"IsPublic\"])\n\n\n@task\ndef configure_network(configured_env_id: str):\n    \"\"\"\n    Steps to configure network for SageMaker Notebook:\n     - Create VPC, Subnet and assign default Internet Access\n     - Create Resource Application on AWS Management Console\n     - Create Security Group and allow HTTP access\n     - Associate Security Group with VPC\n    Reference:\n    https://docs.aws.amazon.com/sagemaker/latest/dg/ec2-notebook-cr.html\n    \"\"\"\n    # VPC, subnet and resource application created automatically when s3 bucket created with ACM\n    vpc_id = configured_env_id\n    subnet_ids = [subnet[\"SubnetIdentifier\"] for subnet in boto3.client(\"ec2\").describe_subnets(\n        Filters=[{\"Name\": \"vpc-id\", \"Value\": vpc_id}\n                 ])\n                  [\"Subnets\"]]\n    role_arn = f\"arn:aws:iam::{configured_env_id}:role/AWSSESResourceApplicationRole\"\n\n    # Network traffic needs to allowed between SageMaker Notebook and EC2\n    sg_name = f\"{configured_env_id}-sg\"\n    sg = boto3.resource(\"ec2\")\n    new_sg = sg.SecurityGroup(sg_name)\n    new_sg.authorize_ingress(\n        GroupName=\"ec2-iningress\",\n        FromPort=8000,\n        ToPort=8000,\n        IpPermissions=[{\"From\": {\"IpProtocol\": \"TCP\"}, \"ToPort\": 8000, \"IpRanges\": [{\"IpPrefix\": subnet_ids[0]}]}],\n    )\n\n    # Attach security group to VPC\n    new_sg.attach_security_group(groupname=f\"{sg_name}\", groupids=INSTANCES_LIST)\n\n    print(f\"VPC ID:{vpc_id}\")\n    print(f\"Subnet IDs {subnet_ids}\")\n    print(f\"Security Group ID {sg_name}\")\n\n    # Creating Resource Application\n    command = \"-i arn:aws:iam::\" + configured_env_id + \":role:\" + role_arn + \" --s3-uri s3://\" + SAMPLE_DATA_BUCKET()\n    response = boto3.client(\"sagemaker-runtime\").create_resource_endpoint_config(\n        EndpointName=\"test-endpoint-name\",\n        ExecutionRoleArn=role_arn,\n        ProcessingJobArguments={\"Command\": [\"/bin/sh\", \"-c\", command]},\n        ProcessingJobConfig={\"VolumeSizeInGB\": 1, \"RelativeTimeout\": 1},\n    )\n    print(response)\n    location = boto3.client(\"sagemaker-runtime\").describe_resource_endpoint_config(\n        EndpointName=response[\"EndpointName\"])\n    endpoint_id = location[\"EndpointConfigArn\"].rsplit(\"/\")[-1]\n\n    print(endpoint_id)\n\n    return vpc_id, subnet_ids, sg_name, endpoint_id\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_resource_endpoint_config(endpoint_id):\n    sagemaker = boto3.client(\"sagemaker\")\n    sagemaker.delete_endpoint_config(EndpointName=endpoint_id)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[ENV_ID_KEY, \"example\"],\n    catchup=False,\n    schedule_interval=None,\n) as dag:\n\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    create_sample_data_bucket_task = create_sample_data_bucket()\n    create_sample_data_bucket_output_key = '{{ task_instance.xcom_pull(\"\n        task_keys=\"\n        \"create_sample_data_bucket_task\"\n        \") }}\"\n    vpc_id, subnet_ids, sg_name, endpoint_id = configure_network(env_id)\n\n    # [START howto_operator_sagemaker_notebook_create]\n    create_notebook_instance = SageMakerHook.create_notebook_instance(\n        notebook_name=\"test-notebook\",\n        instance_type=\"ml.m5.large\",\n        role_arn=role_arn,\n        vpc_id=vpc_id,\n        subnets=subnet_ids,\n        security_group_names=[sg_name],\n        target_on_failure=\"stop\",\n        timeout=12 * 60,\n    )\n    # [END howto_operator_sagemaker_notebook_create]\n\n    # [START howto_sensor_sagemaker_notebook_starter]\n    wait_for_starter = SageMakerNotebookStarterSensor(\n        notebook_name=\"{{ task_instance.xcom_pull('create_notebook_instance', key='notebook_name') }}\",\n        min_instances=0,\n        max_instances=1,\n        wait_for_completion=True,\n    )\n    # [END howto_sensor_sagemaker_notebook_starter]\n\n    # [START howto_operator_sagemaker_notebook_activate]\n    activate_notebook = SageMakerHook.activate_notebook(\n        \"test-notebook\",\n        user_data=json.dumps({\"commands\": [\"mkdir -p /opt/airflow/files; \"\n                                            \"cp /opt/airflow/system/test-files/sample-sagemaker-notebook.csv /opt/\"\n                                            \"airflow/files/; \"\n                                            \"/bin/bash\"]}),\n    )\n    # [END howto_operator_sagemaker_notebook_activate]\n\n    # [START howto_sensor_sagemaker_notebook_activate]\n    wait_for_activate = SageMakerNotebookStarterSensor(\n        notebook_name=\"test-notebook\",\n        min_instances=1,\n        max_instances=2,\n        wait_for_completion=True,\n    )\n    # [END howto_sensor_sagemaker_notebook_activate]\n\n    # [START howto_operator_sagemaker_notebook_deactivate]\n    deactivate_notebook = SageMakerHook.deactivate_notebook(\"test-notebook\")\n    # [END howto_operator_sagemaker_notebook_deactivate]\n\n    # [START howto_sensor_sagemaker_notebook_deactivate]\n    wait_for_deactivate = SageMakerNotebookStarterSensor(\n        notebook_name=\"test-notebook\",\n        min_instances=2,\n        max_instances=3,\n        wait_for_completion=True,\n    )\n    # [END howto_sensor_sagemaker_notebook_deactivate]\n\n    # [START howto_operator_sagemaker_notebook_stop]\n    stop_notebook = SageMakerHook.stop_notebook(\"test-notebook\")\n    # [END howto_operator_sagemaker_notebook_stop]\n\n    # [START howto_sensor_sagemaker_notebook_stop]\n    wait_for_stop = SageMakerNoteBookStarterSensorDone(\n        notebook_name=\"test-notebook\",\n        trigger_event=\"stopped\",\n    )\n    # [END howto_sensor_sagemaker_notebook_stop]\n\n    # [START howto_operator_sagemaker_notebook_restart]\n    restart_notebook = SageMakerHook.restart_notebook(\"test-notebook\")\n    # [END howto_operator_sagemaker_notebook_restart]\n\n    # [START howto_sensor_sagemaker_notebook_restart]\n    wait_for_restart = SageMakerNoteBookStarterSensorDone(\n        notebook_name=\"test-notebook\",\n        trigger_event=\"resumed\",\n    )\n    # [END howto_sensor_sagemaker_notebook_restart]\n\n    # [START howto_operator_sagemaker_notebook_start]\n    start_notebook = SageMakerHook.start_notebook(\"test-notebook\")\n    # [END howto_operator_sagemaker_notebook_start]\n\n    # [START howto_sensor_sagemaker_notebook_start]\n    wait_for_start = SageMakerNoteBookStarterSensorDone(\n        notebook_name=\"test-notebook\",\n        trigger_event=\"started\",\n    )\n    # [END howto_sensor_sagemaker_notebook_start]\n\n    # [START howto_operator_sagemaker_notebook_delete]\n    delete_notebook_instance = SageMakerHook.delete_notebook_instance(\n        notebook_name=\"test-notebook\", force_delete=True\n    )\n    # [END howto_operator_sagemaker_notebook_delete]\n\n    chain(\n        create_sample_data_bucket_task,\n        create_notebook_instance,\n        wait_for_starter,\n        activate_notebook,\n        wait_for_activate,\n        deactivate_notebook,\n        stop_notebook,\n        wait_for_stop,\n        restart_notebook,\n        wait_for_restart,\n        start_notebook,\n        wait_for_start,\n        delete_notebook_instance,\n        delete_resource_endpoint_config(endpoint_id),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_418_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1040409}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that illustrates parallel task execution and infrastructure management using Airflow operators for Google Cloud Compute Engine. The DAG will demonstrate creating a VM instance, running parallel SSH commands with different authentication techniques, and then cleaning up the infrastructure.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Compute SSH operator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.compute import (\n    ComputeEngineDeleteInstanceOperator,\n    ComputeEngineFromTemplateInstanceOperator,\n    ComputeEngineInsertInstanceFromTemplateOperator,\n    ComputeEngineInsertInstanceIntoFolderOperator,\n    ComputeEngineInsertInstanceOperator,\n    ComputeEngineInsertInstanceWithListInstancesOptionOperator,\n    ComputeEngineInsertInstanceWithPrimaryDiskOperator,\n    ComputeEngineInsertInstanceIntoLocationOperator,\n    ComputeEngineInsertInstanceIntoZoneOperator,\n    ComputeEngineRunShellCommandOnInstanceOperator,\n)\nfrom airflow.providers.google.cloud.operators.gce import GCETerminateInstanceGroupOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nLOCATION = \"europe-west1-b\"\nREGION = \"europe-west1\"\nSHORT_MACHINE_TYPE_NAME = \"n1-standard-1\"\nDAG_ID = \"cloud_compute_ssh_parallel\"\nSSH_COMMAND = \"echo Test\"\n\n\n# [START howto_operator_gce_instance]\ndef _get_list_of_instances(*, project_id=PROJECT_ID, location=LOCATION):\n    return [\n        {\n            \"name\": f\"instance-{i}\",\n        }\n        for i in range(4)\n    ]\n\n\ndef _insert_instance(instance, *., project_id=PROJECT_ID, location=LOCATION):\n    instance[\"machine_type\"] = f\"zones/{location}/machineTypes/{SHORT_MACHINE_TYPE_NAME}\"\n    if \"disks\" not in instance:\n        instance[\"disks\"] = [\n            {\n                \"boot\": {\"device_name\": \"disk\"},\n                \"initialize_params\": {\n                    \"disk_size_gb\": \"10\",\n                    \"source_element\": \"projects/cloud-compute-gce/diskTemplates/pd-10-vg-10\",\n                    \"disk_type\": f\"zones/{location}/diskTypes/pd-balanced\",\n                    \"source_disk_name\": \"disk\",\n                },\n            }\n        ]\n    instance[\"network_interfaces\"] = [\n        {\n            \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n            \"subnetwork\": f\"regions/{location}/subnetworks/default\",\n        }\n    ]\n    return instance\n\n\ndef _update_instance_ip(stack_id, machine_name, private_network_range_name):\n    body = {\n        \"network_interfaces\": [\n            {\n                \"access_configs\": [\n                    {\n                        \"name\": \"External NAT\",\n                        \"network_tier\": \"PREMIUM\",\n                    }\n                ],\n                \"subnetwork\": f\"global/subnetworks/{private_network_range_name}\",\n            }\n        ]\n    }\n    request = compute.SurgeRebalanceRequest(stackId=stack_id, machineName=machine_name, body=body)\n    service = compute.StackBuilder()\n    operation = service.surge_rebalance(request=request).execute()\n    if operation[\"status\"] != \"DONE\":\n        return False\n    return True\n\n\ndef _run_shell_command_on_instance(instance_id, zone, command):\n    return {\n        \"command_lines\": [\n            {\"command_line\": command, \"instance\": f\"zones/{zone}/instances/{instance_id}\"},\n        ]\n    }\n\n\ndef _insert_instance_with_list_instances(instance, *_, project_id=PROJECT_ID, zone=zone):\n    instances = _get_list_of_instances(project_id=project_id, location=zone)\n    instances.append(instance)\n    return instances\n\n\ndef _insert_instance_with_primary_disk(instance, *_, project_id=PROJECT_ID, disk_name=\"disk\"):\n    instance[\"disks\"] = [\n        {\n            \"boot\": {\"device_name\": \"disk\"},\n            \"initialize_params\": {\n                \"disk_size_gb\": \"10\",\n                \"source_element\": f\"gs://{disk_name}\",\n                \"disk_type\": \"pd-balanced\",\n                \"source_disk_name\": \"\",\n            },\n        }\n    ]\n    return instance\n\n\ndef _insert_instance_with_location_and_zone(instance, *_, location=location, zone=zone):\n    instance[\"network_interfaces\"] = [\n        {\n            \"access_configs\": [\n                {\n                    \"name\": \"External NAT\",\n                    \"network_tier\": \"PREMIUM\",\n                }\n            ],\n            \"subnetwork\": f\"regions/{location}/subnetworks/default\",\n        }\n    ]\n    instance[\"network_interfaces\"][0][\"access_configs\"].append(\n        {\n            \"name\": \"SPICE\",\n            \"network_tier\": \"PREMIUM\",\n        }\n    )\n    instance[\"network_interfaces\"][0][\n        \"subnetwork\"\n    ] = f\"regions/{region}/subnetworks/{location}-primary\"\n    return instance\n\n\n# [END howto_operator_gce_instance]\n\n\n# [START howto_operator_gce_insert_instance_from_template]\ndef _insert_instance_from_template(template_name, project_id=PROJECT_ID, location=LOCATION):\n    instance = {\n        \"name\": \"instance-tpl-test\",\n        \"machine_type\": f\"zones/{location}/machineTypes/n1-standard-1\",\n        \"disks\": [\n            {\n                \"boot\": {\"device_name\": \"disk\"},\n                \"initialize_params\": {\n                    \"disk_size_gb\": \"10\",\n                    \"source_element\": \"projects/compute-files-doe/projectsrcid\",\n                    \"source_image_uri\": \"projects/debian-cloud/global/images/debian-12-bookworm-v20240611\",\n                    \"disk_type\": \"disk0\",\n                },\n            }\n        ],\n        \"network_interfaces\": [\n            {\n                \"access_configs\": [{\"name\": \"External NAT\", \"network_tier\": \"PREMIUM\"}],\n                \"subnetwork\": \"global/networks/default\",\n            }\n        ],\n    ]\n    return instance\n\n\n# [END howto_operator_gce_insert_instance_from_template]\n\n\n# [START howto_operator_gce_replace_instance_list]\ndef _replace_instance_list(insts, project_id=PROJECT_ID, location=LOCATION):\n    insts[0][\"name\"] = \"instance-tpl-test\"\n    insts[1][\"name\"] = \"instance-from-template\"\n    return insts\n\n\n# [END howto_operator_gce_replace_instance_list]\n\n\n# [START howto_operator_gce_terminate_instance_group]\ndef _terminate_instance_group(instance_group_name, region, project_id=PROJECT_ID):\n    instance_groups = GCETerminateInstanceGroupOperator(\n        task_id=\"terminate_ig_task\",\n        group_name=instance_group_name,\n        region=region,\n        project_id=project_id,\n    )\n    instance_groups.result()\n\n# [END howto_operator_gce_terminate_instance_group]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_gce_insert_instance]\n    insert_instance = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=_insert_instance(_get_list_of_instances(), {}),\n    )\n    # [END howto_operator_gce_insert_instance]\n\n    # [START howto_operator_gce_insert_instance_result]\n    insert_instance_result = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance_result\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=_insert_instance(_get_list_of_instances(), {}),\n        wait_for_completion=True,\n    ).result()\n    # [END howto_operator_gce_insert_instance_result]\n\n    # [START howto_operator_gce_insert_instance_into_folder]\n    insert_instance_into_folder = ComputeEngineInsertInstanceIntoFolderOperator(\n        task_id=\"insert_instance_into_folder\",\n        project_id=PROJECT_ID,\n        folder_id=\"my-folder\",\n        zone=LOCATION,\n        body=_insert_instance(_get_list_of_instances(), project_id=PROJECT_ID, location=LOCATION),\n    )\n    # [END howto_operator_gce_insert_instance_into_folder]\n\n    # [START howto_operator_gce_insert_instance_with_list_instances]\n    insert_instance_with_list_instances = ComputeEngineInsertInstanceIntoFolderOperator(\n        task_id=\"insert_instance_with_list_instances\",\n        project_id=PROJECT_ID,\n        folder_id=\"my-folder\",\n        zone=LOCATION,\n        body=_insert_instance_with_list_instances(_get_list_of_instances()),\n    )\n    # [END howto_operator_gce_insert_instance_with_list_instances]\n\n    # [START howto_operator_gce_insert_instance_with_primary_disk]\n    insert_instance_with_primary_disk = ComputeEngineInsertInstanceWithPrimaryDiskOperator(\n        task_id=\"insert_instance_with_primary_disk\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=_insert_instance_with_primary_disk(_get_list_of_instances()),\n    )\n    # [END howto_operator_gce_insert_instance_with_primary_disk]\n\n    # [START howto_operator_gce_insert_instance_with_location_and_zone]\n    insert_instance_with_location_and_zone = ComputeEngineInsertInstanceWithLocationAndZoneOperator(\n        task_id=\"insert_instance_with_location_and_zone\",\n        project_id=PROJECT_ID,\n        zone=ZONE,\n        region=REGION,\n        body=_insert_instance_with_location_and_zone(_get_list_of_instances()),\n    )\n    # [END howto_operator_gce_insert_instance_with_location_and_zone]\n\n    # [START howto_operator_gce_insert_instance_with_template]\n    insert_instance_from_template = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"insert_instance_from_template\", project_id=PROJECT_ID, zone=LOCATION, body=_insert_instance_from_template()\n    )\n    # [END howto_operator_gce_insert_instance_from_template]\n\n    # [START howto_operator_gce_insert_instance_into_location]\n    insert_instance_into_location = ComputeEngineInsertInstanceIntoLocationOperator(\n        task_id=\"insert_instance_into_location\", project_id=PROJECT_ID, location=LOCATION, body=_insert_instance_from_template()\n    )\n    # [END howto_operator_gce_insert_instance_into_location]\n\n    # [START howto_operator_gce_insert_instance_with_list_instances_option]\n    insert_instance_with_list_instances_option = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance_with_list_instances_option\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=_insert_instance_with_list_instances(_get_list_of_instances()),\n        insert_object_id_generator_function=\"_insert_instance_with_list_instances\",\n    )\n    # [END howto_operator_gce_insert_instance_with_list_instances_option]\n\n    # [START howto_operator_gce_run_shell_command_on_instance]\n    run_shell_command_on_instance = ComputeEngineRunShellCommandOnInstanceOperator(\n        task_id=\"run_shell_command_on_instance\",\n        zone=insert_instance_result[\"name\"][\"value\"].rpartition(\"/\")[-1],\n        project_id=PROJECT_ID,\n        body=_run_shell_command_on_instance(insert_instance_result[\"name\"][\"value\"], REGION, PROJECT_ID, DICT),\n    )\n    # [END howto_operator_gce_run_shell_command_on_instance]\n\n    # [START howto_operator_gce_insert_instance_into_zone]\n    insert_instance_into_zone = ComputeEngineInsertInstanceIntoZoneOperator(\n        task_id=\"insert_instance_into_zone\", project_id=PROJECT_ID, zone=ZONE, body=_insert_instance_from_template()\n    )\n    # [END howto_operator_gce_insert_instance_into_zone]\n\n    delete_instance_group = GCETerminateInstanceGroupOperator(\n        task_id=\"delete_instance_group\",\n        group_name=\"my-instance-group\",\n        region=\"us-central1\",\n        project_id=PROJECT_ID,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_instance = ComputeEngineDeleteInstanceOperator(\n        task_id=\"delete_instance\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        resource_id=insert_instance_result[\"name\"][\"value\"].rpartition(\"/\")[-1],\n    )\n\n    # [START howto_operator_gce_insert_instance_from_template_result]\n    insert_instance_from_template_result = ComputeEngineInsertInstanceFromTemplateOperator(\n        task_id=\"insert_instance_from_template_result\", project_id=PROJECT_ID, zone=LOCATION, body=_insert_instance_from_template()\n    ).result()\n    # [END howto_operator_gce_insert_instance_from_template_result]\n\n    # [START howto_operator_gce_replace_instance_list_result]\n    replace_instance_list_result = ComputeEngineInsertInstanceOperator(\n        task_id=\"replace_instance_list_result\",\n        project_id=PROJECT_ID,\n        zone=LOCATION,\n        body=_replace_instance_list([{\"name\": \"instance-to-replace\"}, \"instance-to-add\"]),\n    ).result()\n    # [END howto_operator_gce_replace_instance_list_result]\n\n    # [START howto_operator_gce_insert_instance_with_location_and_zone_result]\n    insert_instance_with_location_and_zone_result = ComputeEngineInsertInstanceOperator(\n        task_id=\"insert_instance_with_location_and_zone_result\",\n        project_id=PROJECT_ID,\n        zone=ZONE,\n        region=REGION,\n        body=_insert_instance_with_location_and_zone(_get_list_of_instances()),\n    ).result()\n    # [END howto_operator_gce_insert_instance_with_location_and_zone_result]\n\n    # [START howto_operator_gce_terminate_instance_group_result]\n    terminate_instance_group_result = ComputeEngineTerminateInstanceGroupOperator(\n        task_id=\"terminate_instance_group_result\", instance_group_name=\"my-instance-group\"\n    ).result()\n    # [END howto_operator_gce_terminate_instance_group_result]\n\n\nDAG_ID = \"example_gce_ssh_parallel\"\n\n\n# [START howto_trigger_gce_insert_instance]\ntrigger_insert_instance = ComputeEngineInsertInstanceOperator(task_id=\"trigger_insert_instance\", conf=dict())\n# [END howto_trigger_gce_insert_instance]\n\n# [START howto_trigger_gce_run_shell_command_on_instance]\ntrigger_run_shell_command_on_instance = ComputeEngineRunShellCommandOnInstanceOperator(\n    task_id=\"trigger_run_shell_command_on_instance\", conf=dict()\n)\n# [END howto_trigger_gce_run_shell_command_on_instance]\n\n# [START howto_trigger_gce_replace_instance_list]\ntrigger_replace_instance_list = ComputeEngineInsertInstanceOperator(\n    task_id=\"trigger_replace_instance_list\", conf=dict()\n)\n# [END howto_trigger_gce_replace_instance_list]\n\n# [START howto_trigger_gce_insert_instance_from_template]\ntrigger_insert_instance_from_template = ComputeEngineInsertInstanceFromTemplateOperator(\n    task_id=\"trigger_insert_instance_from_template\", conf=dict()\n)\n# [END howto_trigger_gce_insert_instance_from_template]\n\n# [START howto_trigger_gce_delete_instance_group]\ntrigger_delete_instance_group = ComputeEngineDeleteInstanceOperator(\n    task_id=\"trigger_delete_instance_group\", conf=dict()\n)\n# [END howto_trigger_gce_delete_instance_group]\n\n# [START howto_trigger_gce_insert_instance_into_location]\ntrigger_insert_instance_into_location = ComputeEngineInsertInstanceIntoLocationOperator(\n    task_id=\"trigger_insert_instance_into_location\", conf=dict()\n)\n# [END howto_trigger_gce_insert_instance_into_location]\n\n# [START howto_trigger_gce_insert_instance_with_location_and_zone]\ntrigger_insert_instance_with_location_and_zone = ComputeEngineInsertInstanceIntoLocationOperator(\n    task_id=\"trigger_insert_instance_with_location_and_zone\", conf=dict()\n)\n# [END howto_trigger_gce_insert_instance_with_location_and_zone]\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2011_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1040747}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an Airflow data validation pipeline that monitors a specific Cassandra keyspace and table for data availability and record-level conditions. Create a sensor-based workflow that acts as a gatekeeper to ensure data quality and completeness before initiating downstream data transformation or analysis tasks.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "# Ignore missing args provided by default_args\n@task(task_id=\"enable_replication_for_table\", trigger_rule=TriggerRule.ALL_DONE)\ndef enable_replication_for_table(cassandra_connection_host, keyspace_name, table_name):\n    \"\"\"\n    Enables replication for a specified table.\n\n    Args:\n        cassandra_connection_host (str): The host address of the Cassandra cluster.\n        keyspace_name (str): The name of the keyspace.\n        table_name (str): The name of the table.\n    \"\"\"\n    command = f\"sh setup/scripts/cassandra/enable_replication.sh {cassandra_connection_host} {keyspace_name} {table_name}\"\n    execute_command(command)\n\n\nwith SystemTestContext('cassandra_sensor'):\n\n    # [START howto_cassandra_sensor]\n    @task\n    def have_data():\n        return False\n\n    # [END howto_cassandra_sensor]\n\n    # [START howto_cassandra_sensor_by_column_count]\n    @task\n    def have_data_with_specific_condition(column_count):\n        return column_count > 0\n\n    # [END howto_cassandra_sensor_by_column_count]\n\n    # [START howto_cassandra_with_cassandra_connection]>\n    @task(CassandraConnection)\n    def cassandra_connection():\n        return \"127.0.0.1:9042\"\n\n    # [END howto_cassandra_with_cassandra_connection>]\n\n    @task\n    def touch_table(keyspace_name, table_name):\n        command = f\"sudo ifname --execute 'ifname adminpassword '{OPENSEARCH_ADMIN_PASSWORD}'; \" +\n                  f\"ifname -h | grep 'create table' && \" +\n                  f\"ifname -c {keyspace_name} -t '{table_name}' ; \" + \\\n                  f\"ifname -h | grep 'touch column count' && ifname --execute 'touchcolumncount {keyspace_name}'\"\n        execute_command(command)\n\n    # [START verify_data_in_cassandra]\n    @task\n    def check_data_existence(cassandra_connection_host, keyspace_name, table_name, columns, mapper=lambda x: x):\n        uri = f\"thrift://{cassandra_connection_host}:9042\"\n        query = f\"select {', '.join(columns)} from {keyspace_name}.{table_name};\"\n        response = cursor_to_dict(executor.execute_query(query))\n        return all(mapper(row) for row in response)\n\n    # [END verify_data_in_cassandra]\n\n    @task\n    def touch_data(keyspace_name, table_name, columns, mapper=lambda x: x):\n        command = f\"sudo ifname --execute 'ifname adminpassword '{OPENSEARCH_ADMIN_PASSWORD}'; \" + \\\n                  f\"ifname -h | grep 'touch column count' && ifname -c {keyspace_name} -t '{table_name}' \" + \\\n                  f\"--columns={','.join(columns)} --mappers='{mapper}'; \"\n        execute_command(command)\n\n    # [START test_cassandra_sensor]\n    tables_to_test = [\n        dict(name='employees', keys={'emp_no': int}),\n        {'keyspace': 'employees', 'table': 'salaries', 'columns': ['salary', 'from_date', 'to_date']},\n        {'keyspace': 'payroll', 'table': 'employee_hours', 'columns': ['employee_id', 'department', 'hours']}\n    ]\n\n    for table in tables_to_test:\n        t = TaskToucher()\n        t(touch_table(**table))\n\n    for table in tables_to_test:\n        print(f\"Testing for existence of data in table {table['name']} in keyspace {table['keyspace']}\")\n        not_null_columns = [f\"{k}!='null' \" for k in table['columns']]\n        condition = \" AND \".join(not_null_columns)\n        have_data_obj = HaveData(condition=condition)\n        wait_object = SensorTask(task_id=f'sensor_{table[\"name\"]}', task_name=f'sensor_{table[\"name\"]}',\n                                target_interval=1, timeout=60 * 5, poke_interval=1, time_out_function=have_data_obj)\n        employees_sensor = wait_object(have_data_with_specific_condition(poke_interval=1, timeout=60 * 5, timeout_fn=have_data_obj))\n        employees_sensor >> touch_data(**table)\n\n    # [END test_cassandra_sensor]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def tearDown():\n        pass\n\n    tearDown()"}], "metadata": {"file_name": "dag_1935_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1040857}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates transferring files between Google Drive and Google Cloud Storage, showcasing cross-platform file movement and synchronization capabilities. Create a DAG that illustrates how enterprise data can be seamlessly migrated between different cloud storage platforms.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.suite.hooks.drive import GoogleDriveHook\nfrom airflow.providers.google.suite.transfers.gcs_to_drive import GCSToGoogleDriveOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"gcp_gcs_to_google_drive\"\n\nRESOURCES_BUCKET_NAME = \"airflow-system-tests-resources\"\nDRIVE_ID = \"\"\nFILE_NAME = \"test_upload_upload_url.txt\"\nLOCAL_PATH = \"/tmp/\"\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\", \"gcs\", \"google-drive\"],\n) as dag:\n    @task\n    def create_temp_file():\n        \"\"\"\n        Task creates local file \"test_upload_upload_url.txt\" and returns its path.\n        \"\"\"\n        import tempfile\n\n        temp_file_path = tempfile.mktemp()\n        with open(temp_file_path, \"w\") as file:\n            file.write(\"Test Upload URL File\")\n        return temp_file_path\n\n    upload_file_to_gcs = GCSToGCSOperator(\n        task_id=\"upload_file_to_gcs\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_bucket=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME,\n        replace=True,\n    )\n\n    # [START upload_file_to_google_drive]\n    upload_file_to_google_drive = GCSToGoogleDriveOperator(\n        task_id=\"upload_file_to_google_drive\",\n        gcp_conn_id=\"google_cloud_default\",\n        object_name=FILE_NAME,\n        drive_id=DRIVE_ID,\n    )\n    # [END upload_file_to_google_drive]\n\n    move_file_from_gcs_to_gcs = GCSToGCSOperator(\n        task_id=\"move_file_from_gcs_to_gcs\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_bucket=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME,\n        move_object=True,\n    )\n\n    move_file_from_google_drive_to_gcs = GCSToGCSOperator(\n        task_id=\"move_file_from_google_drive_to_gcs\",\n        gcp_conn_id=\"google_cloud_default\",\n        drive_id=DRIVE_ID,\n        object_name=FILE_NAME,\n        bucket_name=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME,\n        move_object=True,\n    )\n\n    delete_files = GCSToGCSOperator(\n        task_id=\"delete_files\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_temp_file_to_gcs = GCSToGCSOperator(\n        task_id=\"upload_temp_file_to_gcs\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=FILE_NAME,\n        destination_bucket=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME + f\"-{ENV_ID}\",\n        replace=True,\n    )\n\n    upload_temp_file_to_google_drive = GCSToGoogleDriveOperator(\n        task_id=\"upload_temp_file_to_google_drive\",\n        gcp_conn_id=\"google_cloud_default\",\n        object_name=FILE_NAME + f\"-{ENV_ID}\",\n        drive_id=DRIVE_ID,\n    )\n\n    delete_files_2 = GCSToGCSOperator(\n        task_id=\"delete_files_2\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_create_temp_file_to_google_drive = GCSToGoogleDriveOperator(\n        task_id=\"upload_create_temp_file_to_google_drive\",\n        gcp_conn_id=\"google_cloud_default\",\n        temp_file_path=create_temp_file().output,\n        drive_id=DRIVE_ID,\n    )\n\n    delete_files_3 = GCSToGCSOperator(\n        task_id=\"delete_files_3\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_list_in_folder = GCSToGCSOperator(\n        task_id=\"upload_list_in_folder\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=f\"google-cloud-roadmap/{FILE_NAME}\",\n        destination_bucket=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME + f\"-{ENV_ID}\",\n        replace=True,\n    )\n\n    delete_files_4 = GCSToGCSOperator(\n        task_id=\"delete_files_4\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_multiple_temp_files_to_google_drive = GCSToGoogleDriveOperator(\n        task_id=\"upload_multiple_temp_files_to_google_drive\",\n        gcp_conn_id=\"google_cloud_default\",\n        *[\n            create_temp_file().output\n            for n in range(6)\n        ],\n        drive_id=DRIVE_ID,\n        multiple_temp_files=True,\n    )\n\n    delete_files_5 = GCSToGCSOperator(\n        task_id=\"delete_files_5\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            f\"{FILE_NAME}-0{i}\" for i in range(6)\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_temp_file_to_google_drive_batch = GCSToGoogleDriveOperator(\n        task_id=\"upload_temp_file_to_google_drive_batch\",\n        gcp_conn_id=\"google_cloud_default\",\n        objects=[FILE_NAME + f\"-{ENV_ID}\", FILE_NAME + f\"-{ENV_ID}2\"],\n        drive_id=DRIVE_ID,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_6 = GCSToGCSOperator(\n        task_id=\"delete_files_6\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\", FILE_NAME + f\"-{ENV_ID}2\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_create_temp_file_to_google_drive_batch = GCSToGoogleDriveOperator(\n        task_id=\"upload_create_temp_file_to_google_drive_batch\",\n        gcp_conn_id=\"google_cloud_default\",\n        objects=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        drive_id=DRIVE_ID,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_7 = GCSToGCSOperator(\n        task_id=\"delete_files_7\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_list_in_folder_with_suffix = GCSToGCSOperator(\n        task_id=\"upload_list_in_folder_with_suffix\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=f\"google-cloud-roadmap/{FILE_NAME}*\",\n        destination_bucket=RESOURCES_BUCKET_NAME,\n        destination_object=FILE_NAME + f\"-{ENV_ID}\",\n        replace=True,\n    )\n\n    delete_files_8 = GCSToGCSOperator(\n        task_id=\"delete_files_8\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_multiple_temp_files_to_google_drive_batch = GCSToGoogleDriveOperator(\n        task_id=\"upload_multiple_temp_files_to_google_drive_batch\",\n        gcp_conn_id=\"google_cloud_default\",\n        *[\n            create_temp_file().output\n            for n in range(6)\n        ],\n        drive_id=DRIVE_ID,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_9 = GCSToGCSOperator(\n        task_id=\"delete_files_9\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            f\"{FILE_NAME}-0{i}\" for i in range(6)\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_create_temp_file_to_google_drive_batch_with_suffix = GCSToGoogleDriveOperator(\n        task_id=\"upload_create_temp_file_to_google_drive_batch_with_suffix\",\n        gcp_conn_id=\"google_cloud_default\",\n        objects=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        drive_id=DRIVE_ID,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_10 = GCSToGCSOperator(\n        task_id=\"delete_files_10\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_get_id_of_file = GCSToGoogleDriveOperator(\n        task_id=\"upload_get_id_of_file\",\n        gcp_conn_id=\"google_cloud_default\",\n        object_name=FILE_NAME + f\"-{ENV_ID}\",\n        drive_id=DRIVE_ID,\n        result_callable=lambda result: print(result[0][\"id\"]),\n    )\n\n    upload_get_ids_of_selected_files = GCSToGoogleDriveOperator(\n        task_id=\"upload_get_ids_of_selected_files\",\n        gcp_conn_id=\"google_cloud_default\",\n        include_objects=[FILE_NAME + f\"-{ENV_ID}\" for _ in range(2)],\n        drive_id=DRIVE_ID,\n        result_callable=lambda results: print(results)[0][0][\"id\"],\n    )\n\n    delete_files_11 = GCSToGCSOperator(\n        task_id=\"delete_files_11\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[FILE_NAME + f\"-{ENV_ID}\"],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_multiple_temp_files_to_google_drive_batch_with_suffix = GCSToGoogleDriveOperator(\n        task_id=\"upload_multiple_temp_files_to_google_drive_batch_with_suffix\",\n        gcp_conn_id=\"google_cloud_default\",\n        *[\n            create_temp_file().output\n            for n in range(6)\n        ],\n        drive_id=DRIVE_ID,\n        suffix=\"-\" + ENV_ID,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_12 = GCSToGCSOperator(\n        task_id=\"delete_files_12\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            f\"{FILE_NAME}-{i}\" for i in range(6)\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    upload_create_temp_file_to_google_drive_batch_with_suffix_and_multiple = GCSToGoogleDriveOperator(\n        task_id=\"upload_create_temp_file_to_google_drive_batch_with_suffix_and_multiple\",\n        gcp_conn_id=\"google_cloud_default\",\n        objects=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        drive_id=DRIVE_ID,\n        suffix=\"-\" + ENV_ID,\n        multiple_temp_files=True,\n        batch_size=len(objects)=True,\n    )\n\n    delete_files_13 = GCSToGCSOperator(\n        task_id=\"delete_files_13\",\n        gcp_conn_id=\"google_cloud_default\",\n        source_bucket=RESOURCES_BUCKET_NAME,\n        source_object=[\n            FILE_NAME + f\"-{ENV_ID}\",\n            FILE_NAME + f\"-{ENV_ID}2\",\n        ],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"all_done\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1690_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1041098}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data processing workflow that transforms raw project metadata stored in S3 into a QuickSight-ready dataset for business intelligence purposes. Create a systematic approach to extract, transfer, and load (ETL) project information with automated provisioning and cleanup of AWS resources.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_quicksight\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nSAMPLE_DATA = {\n    \"project\": \"test-proj\",\n    \"data_source\": \"test-ds\",\n    \"table\": \"test-table\",\n}\nTABLE_ATTRIBUTES = [\n    {\"attribute\": \"empno\", \"type\": \"NUMBER\", \"isNullable\": \"TRUE\"},\n    {\"attribute\": \"ename\", \"type\": \"VARCHAR\", \"isNullable\": \"FALSE\"},\n    {\"attribute\": \"sal\", \"type\": \"NUMBER\", \"isNullable\": \"TRUE\"},\n    {\"attribute\": \"deptno\", \"type\": \"NUMBER\", \"isNullable\": \"TRUE\"},\n]\n\n\n@task\ndef create_dataset(connection_id: str, bucket_name: str, object_key: str):\n    hook = QuickSightHook(conn_id=connection_id)\n    response = hook.create_dataset(\n        SampleDataName=str(SAMPLE_DATA[\"project\"]),\n        DataSourceName=str(SAMPLE_DATA[\"data_source\"]),\n        TableName=str(SAMPLE_DATA[\"table\"]),\n    )\n    location_arn = hook.get_dataset_location_arn(response[0][\"DatasetId\"])\n    log.info(\"Dataset created. Location ARN: %s\", location_arn)\n\n\n@task\ndef create_visualization(\n    connection_id: str, dataset_arn: str, visualization_name: str, bucket_name: str, object_key: str\n):\n    hook = QuickSightHook(conn_id=connection_id)\n    response = hook.create_visualization(\n        DatasetArn=dataset_arn,\n        VisualizationName=visualization_name,\n        DisplayMode=\"SIDE_BY_SIDE\",\n        BucketName=bucket_name,  # must be s3 prefix\n        ObjectKey=object_key,  # name of table slice within the bucket\n    )\n    log.info(\"Visualization created. Visualization ARN: %s\", response[0][\"VisualizationId\"])\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_visualizations(connection_id: str, visualization_ids: list):\n    hook = QuickSightHook(conn_id=connection_id)\n    log.info(\"Deleting visualizations %s.\", visualization_ids)\n    hook.delete_visualization(ids=visualization_ids)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_datasets(dataset_arn: str):\n    hook = QuickSightHook()\n    log.info(\"Deleting datasets %s.\", dataset_arn)\n    hook.delete_dataset(dataset_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"s3://{env_id}-quicksight-bucket\"\n    object_key = f\"s3:///{env_id}/quicksight/dataset.csv\"\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    create_dataset = create_dataset(connection_id=\"quick-sight-conn-id\", bucket_name=bucket_name, object_key=object_key)\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightCreateIngestionOperator(\n        task_id=\"create_ingestion\",\n        dataset_arn=\"{{ ti.xcom_pull('create_dataset') }}\",\n        body=SAMPLE_DATA[\"data_source\"],\n        batch_size=1,\n        transformation=None,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_quicksight_create_ingestion]\n\n    # [START howto_operator_quicksight_create_visualization]\n    create_visualization = QuickSightCreateVisualisationOperator(\n        task_id=\"create_visualization\",\n        visualization_display_name=\"Test Visualization\",\n        dataset_arn=\"{{ ti.xcom_pull('create_dataset') }}\",\n        visualization_name=\"Test Visualization\",\n        bucket_name=bucket_name,\n        object_key=\"s3://quick-sight-dataset/TEST-DATA.csv\",\n        table_slices=[{\"attributeList\": TABLE_ATTRIBUTES}],\n    )\n    # [END howto_operator_quicksight_create_visualization]\n\n    delete_visualizations = delete_visualizations(\n        connection_id=\"quick-sight-conn-id\", visualization_ids=\"{{ ti.xcom_pull('create_visualization') }}\"\n    )\n\n    delete_dataset = delete_dataset(dataset_arn=\"{{ ti.xcom_pull('create_dataset') }}\")\n\n    chain(\n        create_bucket,\n        create_dataset,\n        create_ingestion,\n        create_visualization,\n        delete_visualizations,\n        delete_dataset,\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104122}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an enterprise messaging system DAG that illustrates the complete lifecycle of Azure Service Bus communication, including queue and topic management, message transmission strategies, and subscription handling. Develop a technical demonstration of cloud messaging infrastructure using Airflow operators.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.asb_queue import ASBQueuePublishOperator\nfrom airflow.providers.microsoft.azure.sensors.asb_queue import ASBQueueSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_asb_queue\"\n\n# [START howto_operator_asb_queue]\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    conn = Connection(\n        conn_id=\"azure_service_bus_default\",\n        conn_type=\"azure_service_bus\",\n        extra={\"service_name\": \"<your service bus name>\"},\n    )\n\n    @task\n    def create_queue():\n        conn.create()\n        # [START howto_create_queue_in_asb]\n        from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueCreateOperator\n\n        ASBQueueCreateOperator(task_id=\"create_queue\", queue_name=\"test-queue-in-asb\")\n        # [END howto_create_queue_in_asb]\n\n        # [START howto_pop_message_from_queue_in_asb]\n        from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueuePopOperator\n\n        ASBQueuePopOperator(task_id=\"pop_message_from_queue_in_asb\", queue_name=\"test-queue-in-asb\")\n        # [END howto_pop_message_from_queue_in_asb]\n        # [START howto_push_message_to_queue_in_asb]\n        from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueuePushOperator\n\n        ASBQueuePushOperator(\n            task_id=\"push_message_to_queue_in_asb\",\n            queue_name=\"test-queue-in-asb\",\n            payload='{\"message\":\"Hello World!!\"}',\n            wait_for_complete=True,\n        )\n        # [END howto_push_message_to_queue_in_asb]\n        # [START howto_sensor_for_queue_in_asb]\n        from airflow.providers.microsoft.azure.sensors.asb_queue import ASBQueueSensor\n\n        ASBQueueSensor(task_id=\"watch_queue_in_asb\", queue_name=\"test-queue-in-asb\")\n        # [END howto_sensor_for_queue_in_asb]\n        # [START howto_delete_queue]\n        from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueDeleteOperator\n\n        ASBQueueDeleteOperator(task_id=\"delete_queue\", queue_name=\"test-queue-in-asb\")\n        # [END howto_delete_queue]\n        # [START howto_get_messages_from_queue]\n        from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueGetMessageOperator\n\n        ASBQueueGetMessageOperator(\n            task_id=\"get_message_from_queue\", queue_name=\"test-queue-in-asb\"\n        )\n        # [END howto_get_messages_from_queue]\n    # [END howto_operator_asb_queue]\n\n    push_task = ASBQueuePushOperator(task_id=\"push_task\", queue_name=\"test-queue\")\n\n    # [START howto_operator_asb_queue_external]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueCreateOperator\n\n    create_task = ASBQueueCreateOperator(\n        task_id=\"create_queue\", queue_name=\"test-queue\", external_connection_id=\"azure_service_bus_default\"\n    )\n    # [END howto_operator_asb_queue_external]\n\n    # [START howto_operator_asb_queue_mapping]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueMappingOperator\n\n    mapping_task = ASBQueueMappingOperator(\n        task_id=\"mapping_task\",\n        source_queue_name=\"test-queue\",\n        destination_queue_name=\"test-queue-in-asb\",\n        external_connection_id=\"azure_service_bus_default\",\n    )\n    # [END howto_operator_asb_queue_mapping]\n\n    publish_result = ASBQueuePublishOperator(\n        task_id=\"publish_result\",\n        queue_name=\"test-queue-in-asb\",\n        payload='{\"message\":\"Hello World!!\"}',\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    get_result = ASBQueueGetMessageOperator(task_id=\"get_result\", queue_name=\"test-queue-in-asb\")\n    # [START howto_operator_asb_queue_acloud_to_external]\n    from airflow.providers.microsoft.azure.operators.asb_queue_to_external import ASBQueueToExternalOperator\n\n    receive_task = ASBQueueToExternalOperator(\n        task_id=\"receive_task\",\n        queue_name=\"test-queue\",\n        body_json='{\"message\":\"Hello World!!\"}',\n        connection_id=\"azure_service_bus_default\",\n        external_connection_id=\"azure_service_bus_queue-mapping-external\",\n        batch_size=1,\n        max_messages=1,\n        wait_for_complete=True,\n    )\n    # [END howto_operator_asb_queue_acloud_to_external]\n\n    except_task = ASBQueueCreateOperator(\n        task_id=\"except_task\",\n        queue_name=\"test-queue\",\n        except_expr=\"AzureServiceBusMessage(message.body.decode('utf-8')).partNumber == 1\",\n    )\n\n    pop_task = ASBQueueSensor(\n        task_id=\"pop_except_task\",\n        queue_name=\"test-queue\",\n        timeout=60,\n        exc_patterns=[except_task.output],\n    )\n\n    delete_queue_mapping = ASBQueueDeleteOperator(task_id=\"delete_queue\", queue_name=\"test-queue-in-asb\")\n    delete_queue_create = ASBQueueCreateOperator(task_id=\"delete_queue_create\", queue_name=\"test-queue\")\n\n    # [START howto_operator_asb_general]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueReceiveOperator\n\n    receive_task_2 = ASBQueueReceiveOperator(\n        task_id=\"receive_task_2\",\n        queue_name=\"test-queue\",\n        timeout=60,\n    )\n    # [END howto_operator_asb_general]\n\n    # [START howto_operator_asb_multiple_source_destinations]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueMultipleSourceDestinationsOperator\n\n    multiple_source_destinations = ASBQueueMultipleSourceDestinationsOperator(\n        task_id=\"multiple_source_destinations\",\n        sources=[f\"amqp://{conn['conn_id']}@azure_service_bus_queue_mapping/\"],\n        destinations=[f\"amqp://{conn['conn_id']}@azure_service_bus_create_queue/\"],\n        messages=[\n            \"{'message': '{{ task_instance }}'}\",\n            \"{'message': '{{ task_instance }}', 'extra': {'key': 'value'}}\",\n        ],\n        batch_size=1,\n        max_messages=1,\n    )\n    # [END howto_operator_asb_multiple_source_destinations]\n\n    join = TaskView.get_task_view(\"join\")\n    join.follow(receive_task_2)\n\n    # [START howto_operator_asb_button]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueButtonOperator\n\n    button_task = ASBQueueButtonOperator(\n        task_id=\"button_task\",\n        queue_name=\"test-queue\",\n        command=\"AzureServiceBusMessage(message.body.decode('utf-8')).consume();\",\n        button_display_text=\"Press Me!\",\n    )\n    # [END howto_operator_asb_button]\n\n    if_task = TaskView.get_task_view(\"if_task\")\n    if_task.trigger_rule = TriggerRule.ALL_COMPLETED\n\n    # [START howto_operator_asb_if]\n    from airflow.providers.microsoft.azure.operators.asb_queue import ASBQueueIfOperator\n\n    ASBQueueIfOperator(\n        task_id=\"if_task\",\n        queue_name=\"test-queue-in-asb\",\n        condition_sql=\"SELECT COUNT(1) FROM SYS.messages WHERE ID = (SELECT MAX(ID) FROM SYS.messages)\",\n        true_task_id=\"push_task\",\n        false_task_id=\"except_task\",\n    )\n    # [END howto_operator_asb_if]\n\n    join.follow(button_task)\n    join.follow(if_task)\n\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1349_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1041396}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a time series forecasting system that processes retail sales data with granular geographic attributes like city, zip code, and county. Develop an automated pipeline that transforms raw sales data into predictive insights with a 30-day forecast horizon.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.timeseries import AzureTimeseriesItemsUpsertOperator\n\n\ndef load_connections():\n    return [\n        \"azure_acr_connection\",\n        \"timeseries_service\",\n        \"timeseries_bucket\",\n    ]\n\n\n@task\ndef create_timeseries_items(\n    azure_timeseries_bucket_conn_id,\n    azure_timeseries_items_df,\n    timeseries_timeseries_bucket_name,\n):\n    azure_timeseries_bucket = openlineage.operator.OperatorConnection(azure_timeseries_bucket_conn_id)\n    upsert_fields = []\n    for item in azure_timeseries_bucket_iterable(azure_timeseries_items_df):\n        upsert_fields.append({\"field\": item})\n    azure_timeseries_operator = AzureTimeseriesItemsUpsertOperator(\n        task_id=\"upsert_timeseries_items\",\n        azure_timeseries_bucket_name=timeseries_timeseries_bucket_name,\n        body=item_list=upsert_fields,\n        azure_timeseries_bucket=azure_timeseries_bucket,\n    )\n\n\nwith DAG(\n    dag_id=\"example_timeseries_azure\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    connect_and_create_timeseries = create_timeseries_items(\n        azure_timeseries_bucket_conn_id=\"azure_acr_connection\",\n        azure_timeseries_items_df=[{\"value\": 1}, {\"value\": 2}],  # type: ignore[arg-type]\n        timeseries_timeseries_bucket_name=\"timeseries_blob\",\n    )\n\n\nif __name__ == \"__main__\":\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule=\"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2113_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1041455}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an end-to-end infrastructure orchestration DAG that demonstrates the complete lifecycle of an EKS cluster, including creation, validation, and pod deployment. Implement a robust workflow that shows how to programmatically manage cloud infrastructure using Airflow's declarative task chaining.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterHook, NodegroupHook\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nDAG_ID = \"example_eks\"\n\n# This Dag will use the same EC2 instance for both the cluster and the pod.\n# We cannot be sure which node group was created first so we need two\n# dag runs for this example.\n# Because the pod operator is also a task in the create cluster dag it\n# needs to have the same trigger rule as that dag\n\n\nCLUSTER_NAME = \"test-eks-cluster\"\nSUBNETS = [\n    \"subnet-8e685401\",\n    \"subnet-b877a3eb\",\n    \"subnet-cec2b1d6\",  # In AWS CLients subnets are ordered by prefix list id\n]\nNODEGROUP_NAME = \"test-eks-nodegroup\"\nSECURITY_GROUPS = [f\"{CLUSTER_NAME}-sg\"]  # Must be available in the subnet list\nCLUSTER_ROLE_ARN = \"\"\nVPC_CLUSTERS_CONFIG = {\n    \"resources_vpc_config\": {\"subnet_ids\": SUBNETS},\n}\nNODE_GROUP_ROLE_ARN = \"\"\nPERSISTENT_VOLUMECLAIM_NAME = f\"eks-pvc-dev-{DAG_ID}\"\nEXECUTION_ROLE_ARN = \"arn:aws:iam::AWS-cloudformation-PAI-\n                         role:AmazonEC2ContainerServiceDeveloperInstanceRole\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_eks_cluster():\n        eks = boto3.client(\"eks\")\n        try:\n            response = eks.delete_cluster(name=CLUSTER_NAME)\n            print(response)\n        except ClientError as e:\n            print(e)\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_node_group():\n        eks = boto3.client(\"eks\")\n        try:\n            response = eks.delete_nodegroup(\n                nodegroupName=NODEGROUP_NAME, clusterName=CLUSTER_NAME\n            )\n            print(response)\n        except ClientError as e:\n            print(e)\n\n    set_up_eks_cluster: EksCreateClusterOperator = EksCreateClusterOperator(\n        task_id=\"set_up_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        vpc_clusters_config=VPC_CLUSTERS_CONFIG,\n        resources_vpc_config={\"subnetIds\": SUBNETS},\n        compute=\"nodegroup\",\n        namespace=\"test-eks-cluster\",\n        cluster_role_arn=CLUSTER_ROLE_ARN,\n        security_groups=SECURITY_GROUPS,\n        resources=[\n            {\n                \"path\": \"/etc/ssl/certs/\",\n                \"permissionOverride\": [\"READ\"},\n            },\n        ],\n    )\n\n    create_node_group: EksCreateNodegroupOperator = EksCreateNodegroupOperator(\n        task_id=\"create_node_group\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        nodegroup_subnets=SUBNETS,\n        nodegroup_role_arn=NODE_GROUP_ROLE_ARN,\n        # Below I'm setting a persistent volume claim as an example of passing a resource\n        # to the EksCreateNodegroupOperator.  The example notes state if you want to\n        # pass a resource you must mark it as \"prerun_resource\".  That marking is\n        # what's being used to identify the PVC in the next step.\n        nodegroup_pvc_name=PERSISTENT_VOLUMECLAIM_NAME,\n    )\n\n    run_example_pod = EksPodOperator(\n        task_id=\"run_example_pod\",\n        pod_name=\"run_example_pod\",\n        cluster_name=CLUSTER_NAME,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"sleep 20\"],\n        labels={},\n        get_logs=True,\n        startup=True,\n    )\n\n    delete_node_group >> run_example_pod >> delete_eks_cluster()\n\n    chain(\n        set_up_eks_cluster,\n        create_node_group,\n        run_example_pod,\n        delete_node_group,\n        run_example_pod,\n        delete_eks_cluster(),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_732_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104157}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an automated data transformation pipeline that translates multilingual documents across different file formats (XLSX, DOCX, PDF) using cloud translation services. Implement a workflow that handles language conversion, manages input and output configurations, and demonstrates scalable document processing techniques.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.sheets_to_gcs import GoogleSheetsToGCSOperator\nfrom airflow.providers.google.suite.sensors.sheets import GoogleSheetsSpreadsheetStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\", \"default\")\nDAG_ID = \"sheets_to_gcs_example\"\n\nSPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\", \"1Oz8f54QZa6b9t7nU2mJjxwCqTcKkRrPp-6dFh5u\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"sheets\"],\n) as dag:\n    # [START howto_operator_sheets_to_gcs]\n    get_spreadsheet_id_state_sensor = GoogleSheetsSpreadsheetStateSensor(\n        task_id=\"get_spreadsheet_id_state\",\n        spreadsheet_id=SPREADSHEET_ID,\n        deferrable=True,\n    )\n\n    fetch_spreadsheet = GoogleSheetsToGCSOperator(\n        task_id=\"spreadsheet_to_gcs\",\n        spreadsheet_id=SPREADSHEET_ID,\n        bucket_name=\"{{task_instance.xcom_pull('get_spreadsheet_id_state')['spreadsheet_id']}}\",\n        filename=\"test_gcs_sheets.json\",\n        deferrable=True,\n    )\n    # [END howto_operator_sheets_to_gcs]\n\n    get_spreadsheet_id_state_sensor >> fetch_spreadsheet\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1301_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1041617}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design an Airflow DAG that orchestrates cloud infrastructure provisioning and data migration tasks across multiple AWS services. Develop a sequence of tasks that create security groups, configure database endpoints, and set up a replication instance for seamless data transfer.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom typing import Any\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.rds import RDSHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RDSCancelMaintenanceTaskOperator,\n    RDSCreateDbInstanceOperator,\n    RDSCheckDBAvailabilityOperator,\n    RDSCreateEventSubscriptionOperator,\n    RDSDeleteDbInstanceOperator,\n    RDSDeleteEventSubscriptionOperator,\n    RDSServerlessCreateDbInstanceOperator,\n    RDSServerlessCreateEventSubscriptionOperator,\n    RDSServerlessDeleteDbInstanceOperator,\n    RDSServerlessDeleteEventSubscriptionOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RDSSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import Saturday\nfrom airflow.utils.trigger_list import TriggerList\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_serverless\"\n\n# Externally fetched variables:\nKMS_KEY_ID_KEY = \"KMS_KEY_ID\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(KMS_KEY_ID_KEY).build()\n\nSAMPLE_DATA_SIZE = 10\nSAMPLE_DATA = b\"Sample data\"\n\n\n@task\ndef get_s3_bucket_location(bucket_name: str) -> dict[Any, Any]:\n    s3 = boto3.client(\"s3\")\n    response = s3.get_bucket_location(Bucket=bucket_name)\n    return response[\"Location\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_db_instance(db_instance_identifier: str):\n    rds = RDSHook()\n    rds_hook = rds.get_hook()\n    rds_hook.delete_db_instance(DBInstanceId=db_instance_identifier)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_event_subscription(arl: str, subscription_name: str):\n    rds = RDSHook()\n    rds_hook = rds.get_hook()\n    rds_hook.delete_event_subscription(EventSubscriptionArn=arl, SubscriptionName=subscription_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_serverless_db_instance(db_instance_arn: str):\n    boto3.client(\"serverlessrds\").delete_db_instance(DBInstanceArn=db_instance_arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_serverless_event_subscription(arl: str, subscription_name: str):\n    rds = RDSHook()\n    rds_hook = rds.get_hook()\n    rds_hook.delete_event_subscription(EventSubscriptionArn=arl, SubscriptionName=subscription_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) isort:off\"):\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    kms_key_id = test_context[KMS_KEY_ID_KEY]\n    sg_name = f\"{env_id}-sg\"\n    db_name = f\"{env_id}-db-name\"\n    bucket_name = f\"{env_id}-bucket\"\n    s3_location = get_s3_bucket_location(bucket_name)\n    rds_db_instance_identifier = f\"{env_id}-db-instance\"\n    rds_serverless_db_instance_arn = \"\"\n    s3_path = f\"s3://{bucket_name}/best_practices/\"\n    arn_with_region = f\"{rds_serverless_db_instance_arn.split('.')[4]}.\" + rds_serverless_db_instance_arn.split(\".\")[5:]\n    role_arn = f\"arn:aws:iam::{env_id}:role/RDS_EventSubscriptionRole\"\n\n    create_access_role = tenacity.decorator CallableDecorator()(RDSHook._create_access_role)\n\n    create_serverless_db_instance = RDSServerlessCreateDbInstanceOperator.partial(\n        task_id=\"create_serverless_db_instance\",\n    )(db_name=db_name, db_instance_identifier=rds_serverless_db_instance_arn, region_name=sdk_region=None)\n\n    create_event_subscription = RDSCreateEventSubscriptionOperator(task_id=\"create_event_subscription\", arn=arn_with_region, subscription_name=f\"{env_id}-event-subscription\", source_arn=awsvpc_arn, sns_topic_arn=None, role_arn=role_arn)\n\n    wait_for_db_available = RDSSensor(task_id=\"wait_for_db_available\", arn=arn_with_region, target_state=\"available\")\n\n    recreate_sg = tenacity.decorator CallableDecorator()(RDSHook.recreate_security_group)\n\n    enable_iam_policy = tenacity.decorator CallableDecorator()(RDSHook.enable_iam_policies)\n\n    create_db_instance = RDSCreateDbInstanceOperator.task_method_call(\n        task_id=\"create_db_instance\",\n        hook_args={\n            \"DBInstanceIdentifier\": rds_db_instance_identifier,\n            \"Engine\": \"postgres\",\n            \"MasterUsername\": \"admin\",\n            \"MasterUserPassword\": \"admin\",\n            \"AllocatedStorage\": 10,\n            \"DBInstanceClass\": \"db.t4g.micro\",\n            \"SecurityGroupIds\": [sg_name],\n            \"SubnetGroupName\": sg_name,\n        },\n    )\n\n    add_iap_login_to_db_instance = tenacity.decorator CallableDecorator()(RDSHook.add_IAM_Lambda_login_to_db_instance)\n\n    create_event_subscription_2 = RDSCreateEventSubscriptionOperator.task_method_call(\n        task_id=\"create_event_subscription_2\",\n        arn=arn_with_region,\n        subscription_name=f\"{env_id}-event-subscription-2\",\n        source_arn=awsvpc_arn,\n        sns_topic_arn=None,\n        role_arn=role_arn,\n    )\n\n    check_db_availability = RDSCheckDBAvailabilityOperator(task_id=\"check_db_availability\", db_name=db_name, db_instance_identifier=rds_db_instance_identifier)\n\n    backup_db = tenacity.decorator CallableDecorator()(RDSHook.backup_db)\n\n    restore_backup = tenacity.decorator CallableDecorator()(RDSHook.restore_backup)\n\n    backup_instance = RDSServerlessBackupOperator.task_method_call(\n        task_id=\"backup_instance\",\n        db_instance_identifier=rds_serverless_db_instance_arn,\n        table_name=s3_path,\n        partition_key_name=\"id\",\n        sort_key_name=\"created_at\",\n        external_table=True,\n    )\n\n    copy_backup_to_s3 = tenacity.decorator CallableDecorator()(S3Hook.copy_file_to_s3)\n\n    copy_backup_to_s3.operator_args = [\n        backup_db.output,\n        s3_location + backup_db.output.split(\"/\")[-1],\n        \"sql/restore.sql\",\n    ]\n\n    drop_backup = tenacity.decorator CallableDecorator()(RDSHook.drop_backup)\n\n    drop_backup.operator_args = [backup_db.output]\n\n    create_serverless_event_subscription = RDSServerlessCreateEventSubscriptionOperator(\n        task_id=\"create_serverless_event_subscription\",\n        event_subscription_arn=arn_with_region,\n        subscription_name=f\"{env_id}-serverless-event-subscription\",\n        source_arn=rds_serverless_db_instance_arn,\n        sns_topic_arn=None,\n        role_arn=role_arn,\n    )\n\n    # [START howto_operator_rds_serverless_describe_event_subscription]\n    describe_event_subscription = RDSHook.describe_event_subscription\n    # [END howto_operator_rds_serverless_describe_event_subscription]\n\n    disable_iam_policy = tenacity.decorator CallableDecorator()(RDSHook.disable_iam_policies)\n\n    delete_backup = tenacity.decorator CallableDecorator()(RDSHook.delete_backup)\n\n    delete_backup.operator_args = [drop_backup.output]\n\n    delete_access_role = tenacity.decorator CallableDecorator()(RDSHook.delete_access_role)\n\n    delete_access_role.task_mapping = {\n        sg_name: recreate_sg,\n        rds_db_instance_identifier: delete_db_instance,\n        arn_with_region: delete_serverless_db_instance,\n        rds_serverless_db_instance_arn: delete_serverless_db_instance_arn,\n        arl: delete_event_subscription,\n        subscription_name: delete_event_subscription_2,\n        arl: delete_serverless_event_subscription,\n    }\n\n    chain(\n        test_context,\n        create_access_role(sg_name, kms_key_id),\n        create_serverless_db_instance,\n        recreate_sg(sg_name),\n        create_db_instance,\n        wait_for_db_available,\n        enable_iam_policy(),\n        check_db_availability,\n        backup_db(rds_serverless_db_instance_arn, s3_path, SAMPLE_DATA, SAMPLE_DATA_SIZE, db_name, rds_db_instance_identifier),\n        restore_backup(rds_serverless_db_instance_arn, s3_path, \"sql/restore.sql\"),\n        backup_instance,\n        describe_event_subscription(arn_with_region),\n        drop_backup(rds_serverless_db_instance_arn),\n        create_serverless_event_subscription,\n        describe_event_subscription(arn_with_region),\n        disable_iam_policy(),\n        delete_backup(rds_serverless_db_instance_arn),\n        delete_access_role(sg_name),\n        delete_serverless_event_subscription(subscription_name),\n        delete_serverless_db_instance(rds_serverless_db_instance_arn),\n        delete_bucket(bucket_name),\n    )\n\n\nawsvpc_arn = \"arn:aws:vpc::0-0-0/subnet-0-0-00000\"\n\n\nwith SystemTestContextBuilder() as builder:\n\n    def setUp():\n        build_env_id = builder.build()\n        builder.add_variable(KMS_KEY_ID_KEY, build_env_id)\n\n        # This system test creates an RDS Serverless DB instance, which would cost money if left persistently created.\n        # To fix the setup time for this system test, we create a persistent bucket and save the output of the task\n        # (the ARN of the new DB instance) such that humans can delete the resource later when done with the test.\n        bucket = await S3Hook().create_bucket(\n            Bucket=name_generator(), Region= boto3.session.Session().region_name, KeyAccessKeyId=key_id, SecretAccessKey=key_secret, SecurityToken=tokens.Token()\n        )\n        path = \"sql/restore.sql\"\n        url = f\"https://s3.{Region}.amazonaws.com/{Bucket}/{path}\"\n        content = \"TEST_S3 Restore SQL FILE\"\n        await S3Hook().load_bytes(object_key=path, body=content, Bucket=bucket)\n        builder.add_variable(\"OUTPUT_PATH\", path)\n        builder.add_variable(\"OUTPUT_KEYACCESSKEYID\", key_id)\n        builder.add_variable(\"OUTPUT_SECRETACCESSKEY\", key_secret)\n        builder.add_variable(\"OUTPUT_SECURITYTOKEN\", tokens.Token())\n\n    def tearDown(self):\n        build_env_id = self.test_context[ENV_ID_KEY]\n        bucket = self.test_context[OUTPUT_PATH]\n        keyaccesskeyid = self.test_context[OUTPUT_KEYACCESSKEYID]\n        secretaccesskeyid = self.test_context[OUTPUT_SECRETACCESSKEY]\n        securitytoken = self.test_context[OUTPUT_SECURITYTOKEN]\n        rmcmd = f\"aws rds serverless db-instance terminate --db-instance-identifier {self.test_context[rds_db_instance_identifier]} --seccomp-profile-name DEFAULT --region {boto3.session.Session().region_name} --skip-finalSnapshot --timeout-second 60 --source-arn {self.test_context[rds_serverless_db_instance_arn]} --key {keyaccesskeyid} --secret {secretaccesskeyid} --security-token {securitytoken}\"\n        logging.info(rmcmd)\n        subprocess.run(rmcmd, shell=True, check=True)\n\n    builder.add_step.setUp(setUp)\n    builder.add_step.tearDown(tearDown)"}], "metadata": {"file_name": "dag_60_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104184}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that orchestrates the complete lifecycle of a cloud-based distributed computing cluster, highlighting the sequential task dependencies and operational states of a Dataproc cluster. The DAG should illustrate how to programmatically manage cloud infrastructure using Airflow operators.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\", \"default\")\nDAG_ID = \"dataproc_update\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\") or DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID\n\nCLUSTER_NAME_BASE = f\"cluster-{DAG_ID}\".replace(\"_\", \"-\")\nCLUSTER_NAME_FULL = CLUSTER_NAME_BASE + f\"-{ENV_ID}\"\nCLUSTER_NAME_FIRST = CLUSTER_NAME_BASE + f\"-first\"\nCLUSTER_NAME_SECOND = CLUSTER_NAME_BASE + f\"-second\"\n\nREGION = \"europe-north1\"\n\n\n# Cluster definition\ndef _create_cluster():\n    return {\n        \"master_config\": {\n            \"config\": {\n                \"machine_type\": \"n1-standard-4\",\n                \"disks\": [\n                    {\n                        \"boot\": True,\n                        \"device_name\": \"dataproc-m\",\n                        \"initialize_params\": {\n                            \"disk_size_gb\": 100,\n                            \"source_image_uri\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20231023\",\n                            \"device_name\": \"dataproc-m\",\n                        },\n                    }\n                ],\n            }\n        },\n        \"worker_config\": {\n            \"config\": {\n                \"machine_type\": \"n1-standard-4\",\n                \"disks\": [\n                    {\n                        \"boot\": True,\n                        \"device_name\": \"dataproc-w\",\n                        \"initialize_params\": {\n                            \"disk_size_gb\": 100,\n                            \"source_image_uri\": \"projects/debian-cloud/global/images/debian-11-bookworm-v20231023\",\n                            \"device_name\": \"dataproc-w\",\n                        },\n                    }\n                ],\n            }\n        },\n        \"software_config\": {\n            \"image_version\": \"2.1.1\",\n        },\n        \"network_config\": {\"network\": \"global/networks/spot-market\"},\n    }\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    region=REGION,\n    create_cluster=_create_cluster,\n    tags=[\"example\"],\n) as dag:\n    create_cluster_first = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_first\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_FIRST,\n        region=REGION,\n        body=_create_cluster(),\n    )\n\n    create_cluster_second = DataprocCreateClusterOperator(\n        task_id=\"create_cluster_second\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_SECOND,\n        region=REGION,\n        body=_create_cluster(),\n    )\n\n    update_cluster = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        cluster_name=CLUSTER_NAME_FIRST,\n        body={\"config\": {\"worker_config\": {\"disk_size_gb\": 50}}},\n    )\n\n    delete_cluster_first = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_first\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_FIRST,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster_second = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster_second\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME_SECOND,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START how_to_cloud_dataproc_update_cluster_operator]\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),  # Also accepts dates in str or timedelta\n    region=REGION,\n    create_cluster=_create_cluster,\n    tags=[\"example\"],\n):\n    # [END how_to_cloud_dataproc_update_cluster_operator]\n\n    (create_cluster_first >> update_cluster >> delete_cluster_first >> delete_cluster_second)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1041954}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a data streaming pipeline that demonstrates Kafka cluster management and message processing using Google Cloud Managed Kafka. Create a workflow that showcases creating a Kafka cluster, producing messages, and consuming those messages with a specific consumer group.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Managed Kafka.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\n# [START import_module]\nfrom google.cloud.kms_v1 import CryptoKey, CryptoKeyIdeningRequest\n\n# [END import_module]\n\n\n# [START howto_manage_kms_key]\ndef manage_kms_key(how_to_test_obj: CryptoKey, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Creates a CryptoKey from the\n    crypto key identifier provided in the test context.\n    \"\"\"\n    key = kwargs[\"test_context\"][\"kms_key_name\"]\n    client = kwargs[\"test_context\"][\"client\"]\n    how_to_test_obj.name = key\n    client.crypto_keys().update(key, body=how_to_test_obj)\n\n\n# [END howto_manage_kms_key]\n\n# [START howto_create_crypto_key_idencing_request]\ndef create_crypto_key_idencing_request(**kwargs) -> dict:\n    \"\"\"\n    Example function to be used as a test hook. Creates a CryptoKeyIdentityRequest request.\n    \"\"\"\n    location = kwargs[\"test_context\"][\"location\"]\n    key_ring_id = kwargs[\"test_context\"][\"key_ring_id\"]\n    return {\n        \"location\": location,\n        \"key_ring_id\": key_ring_id,\n        \"crypto_key_id\": f\"kiey-ring-id-{key_ring_id}\",\n        \"action\": \"CRYPTO_KEY_IDENIER_RENEW\",\n    }\n\n\n# [END howto_create_crypto_key_idencing_request]\n\n\n# [START howto_run_crypto_key_idencing_request]\ndef run_crypto_key_idencing_request(how_to_test_obj: CryptoKeyIdeningRequest, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates the status of a\n    CryptoKeyIdentityRequest operation.\n    \"\"\"\n    operation = kwargs[\"test_context\"][\"operation\"]\n    client = kwargs[\"test_context\"][\"client\"]\n    how_to_test_obj.status = \"DONE\"\n    client.operations().setOperation(operation, body=how_to_test_obj)\n\n\n# [END howto_run_crypto_key_idencing_request]\n\n\n# [START howto_run_crypto_key_idencing_request_sync]\ndef run_crypto_key_idencing_request_sync(how_to_test_obj: dict, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates the status of a\n    CryptoKeyIdentityRequest operation.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    operation = client.crypto_key_identity_requests()\n    response = operation.list(\n        project=\"projects-28945783\", location=\"us-central1\", filter=\"state=DONE\"\n    )\n    for item in response.body.entries:\n        if item.status == \"DONE\":\n            how_to_test_obj.status = \"DONE\"\n            break\n\n\n# [END howto_run_crypto_key_idencing_request_sync]\n\n\n# [START howto_verify_signing_request]\ndef verify_signing_request(how_to_test_obj: dict, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates the status of a\n    VerifySigningRequest operation.\n    \"\"\"\n    operation = kwargs[\"test_context\"][\"operation\"]\n    client = kwargs[\"test_context\"][\"client\"]\n    how_to_test_obj.status = \"DONE\"\n    client.operations().setOperation(operation, body=how_to_test_obj)\n\n\n# [END howto_verify_signing_request]\n\n\n# [START howto_run_verify_signing_request_sync]\ndef run_verify_signing_request_sync(how_to_test_obj: dict, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates the status of a\n    VerifySigningRequest operation.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    operation = client.verify_signing_requests()\n    response = operation.list(project=\"projects-28945783\", format=\"single_file\")\n    for item in response.body.single_file_entries:\n        how_to_test_obj.status = \"DONE\"\n        how_to_test_obj.file_uri = item.file_uri\n        how_to_test_obj.key_uri = item.signing_key_uri\n        break\n\n\n# [END howto_run_verify_signing_request_sync]\n\n\n# [START howto_create_topic]\ndef create_topic(topic_name: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Creates a topic.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    topic = {\"name\": topic_name, \"type\": \"Topic\"}\n    client.projects().locations().topics().create(body=topic, name=\"projects/28945783 locations/us-central1\")\n\n\n# [END howto_create_topic]\n\n\n# [START howto_list_topics]\ndef list_topics(topic_name: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Lists topics.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    page = client.projects().locations().topics().list(parent=topic_name)\n    response = page.execute()\n    for topic in response.items:\n        print(topic.name)\n\n# [END howto_list_topics]\n\n\n# [START howto_get_topic]\ndef get_topic(topic_name: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Gets a topic.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    topic = client.projects().locations().topics().get(name=topic_name, location=\"us-central1\")\n\n    print(topic)\n\n\n# [END howto_get_topic]\n\n\n# [START howto_delete_topic]\ndef delete_topic(topic_name: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Deletes a topic.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    client.projects().locations().topics().delete(name=topic_name, location=\"us-central1\")\n\n\n# [END howto_delete_topic]\n\n\n# [START howto_create_partition]\ndef create_partition(topic_id: str, partition_id: int, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Creates a partition.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    partition = {\n        \"name\": topic_id + \"/ partitions/\" + str(partition_id),\n        \"partitionId\": partition_id,\n        \"type\": \"PARTITION\",\n    }\n    client.projects().locations().topics().partitions().insert(body=partition, location=\"us-central1\")\n\n\n# [END howto_create_partition]\n# [START howto_update_partition]\ndef update_partition(topic_id: str, partition_id: int, updated_partition: dict, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates a partition.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    partition_req = {\n        \"name\": topic_id + \"/ partitions/\" + str(partition_id),\n        \"partitionId\": partition_id,\n        \"type\": \"PARTITION\",\n    }\n    request = client.projects().locations().topics().partitions()\n    operation = request.update(\n        body=partition_req, name=topic_id + \"/ partitions/\" + str(partition_id), location=\"us-central1\"\n    )\n    operation.result()\n\n\n# [END howto_update_partition]\n# [START howto_list_partitions]\ndef list_partitions(topic_id: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Lists partitions.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    page = client.projects().locations().topics().partitions()\n    request = page.list(location=\"us-central1\", parent=topic_id)\n    response = request.execute()\n    for partition in response.items:\n        print(partition.name)\n\n\n# [END howto_list_partitions]\n\n\n# [START howto_get_partition]\ndef get_partition(partition_name: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Gets a partition.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    partition = client.projects().locations().topics().partitions()\n    response = partition.get(name=partition_name, location=\"us-central1\")\n    print(response.execute())\n\n\n# [END howto_get_partition]\n# [START howto_delete_partition]\ndef delete_partition(topic_id: str, partition_id: int, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Deletes a partition.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    client.projects().locations().topics().partitions()\n    operation = client.partition_ids()\n    operation.name = \"%s/topics/%s/partitions/%d\" % (topic_id, topic_id, partition_id)\n    operation.delete()\n\n\n# [END howto_delete_partition]\n\n\n# [START howto_create_template]\ndef create_template(project_id: str, template_name: str, topic_id: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Creates a template.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    template = {\n        \"name\": template_name,\n        \"description\": \"Test template\",\n        \"topic\": topic_id,\n        \"spec\": {\"enabled\": True},\n    }\n    client.locations().templates().create(body=template, parent=f\"projects/{project_id}\")\n\n\n# [END howto_create_template]\n# [START howto_get_template]\ndef get_template(template_id: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Gets a template.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    template = client.locations().templates()\n    response = template.get(name=template_id)\n    print(response.execute())\n\n\n# [END howto_get_template]\n# [START howto_list_templates]\ndef list_templates(project_id: str, location_id: str, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Lists templates.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    page = client.locations().templates()\n    request = page.list(parent=f\"projects/{project_id}/locations/{location_id}\")\n    response = request.execute()\n    for template in response.items:\n        print(template.name)\n\n\n# [END howto_list_templates]\n# [START howto_update_template]\ndef update_template(project_id: str, template_id: str, updated_spec: dict, **kwargs) -> None:\n    \"\"\"\n    Example function to be used as a test hook. Updates a template.\n    \"\"\"\n    client = kwargs[\"test_context\"][\"client\"]\n    template_req = {\n        \"name\": template_id,\n        \"spec\": {\"enabled\": True},\n    }\n    request = client.locations().templates()\n    operation = request.update(\n        body=template_req, name=template_id, parent=f\"projects/{project_id}\"\n    )\n    operation.result()\n\n\n# [END howto_update_template]\n\n\nwith DAG(\n    dag_id=\"example_gcp_kms\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_access_service_accross_yamls]\n    @task\n    def service_accross_yamls():\n        \"\"\"\n        This is an example function that will be called via the `set_up_mti` task decorator.\n        It generates and returns a Cloud IAM policy binding that adds an AWS IAM role to a\n        KMS CryptoKey.\n        \"\"\"\n        role_arn = f\"arn:{aws_athena_account_arn_prefix}:{aws_region}:{aws_athena_account_arn_suffix}:iam\u89d2\u8272\u540d\"\n        key_arn = f\"{gcp_key_ring_id}:{gcp_key_name}\"\n\n        service = {\n            \"effect\": \"ALLOW\",\n            \"principal\": {\"service\": \"kms.amazonaws.com\"},\n        }\n\n        return {f\"{aws_athena_account_arn_prefix}::{role_arn}\": service}\n\n    # [END howto_access_service_accross_yamls]\n\n    @task(task_id=\"manage_kms_key\")\n    def manage_kms_key(how_to_test_obj: CryptoKey, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `manage_kms_key_task` decorator.\n        \"\"\"\n        key = kwargs[\"test_context\"][\"kms_key_name\"]\n        client = kwargs[\"test_context\"][\"client\"]\n\n        client.crypto_keys().update(key, body=how_to_test_obj)\n\n    @task(task_id=\"create_crypto_key_idencing_request\")\n    def create_crypto_key_idencing_request(**kwargs):\n        \"\"\"\n        This is an example function that will be called via the `create_crypto_key_idencing_request`\n        decorator.\n        \"\"\"\n        location = kwargs[\"test_context\"][\"location\"]\n        key_ring_id = kwargs[\"test_context\"][\"key_ring_id\"]\n        return {\n            \"location\": location,\n            \"key_ring_id\": key_ring_id,\n            \"crypto_key_id\": f\"kiey-ring-id-{key_ring_id}\",\n            \"action\": \"CRYPTO_KEY_IDENIER_RENEW\",\n        }\n\n    @task(task_id=\"run_crypto_key_idencing_request\")\n    def run_crypto_key_idencing_request(how_to_test_obj: CryptoKeyIdeningRequest, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `run_crypto_key_idencing_request`\n        decorator.\n        \"\"\"\n        operation = kwargs[\"test_context\"][\"operation\"]\n        client = kwargs[\"test_context\"][\"client\"]\n        how_to_test_obj.status = \"IN_PROGRESS\"\n        client.operations().setOperation(operation, body=how_to_test_obj)\n\n    @task(task_id=\"verify_signing_request\")\n    def verify_signing_request(how_to_test_obj: dict, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `verify_signing_request` decorator.\n        \"\"\"\n        operation = kwargs[\"test_context\"][\"operation\"]\n        client = kwargs[\"test_context\"][\"client\"]\n        how_to_test_obj.status = \"IN_PROGRESS\"\n        client.operations().setOperation(operation, body=how_to_test_obj)\n\n    @task(task_id=\"run_verify_signing_request\")\n    def run_verify_signing_request(how_to_test_obj: dict, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `run_verify_signing_request`\n        decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        operation = client.verify_signing_requests()\n        response = operation.list(\n            project=\"projects-28945783\", location=\"us-central1\", filter=\"state=DONE\"\n        )\n        for item in response.body.entries:\n            how_to_test_obj.status = \"DONE\"\n            how_to_test_obj.file_uri = item.file_uri\n            how_to_test_obj.key_uri = item.signing_key_uri\n            break\n\n    @task(task_id=\"create_topic\")\n    def create_topic(topic_name: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `create_topic` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        topic = {\"name\": topic_name, \"type\": \"Topic\"}\n        client.projects().locations().topics().create(body=topic, name=\"projects/28945783 locations/us-central1\")\n\n    @task(task_id=\"list_topics\")\n    def list_topics(topic_name: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `list_topics` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        page = client.projects().locations().topics().list(parent=topic_name)\n        response = page.execute()\n        for topic in response.items:\n            print(topic.name)\n\n    @task(task_id=\"get_topic\")\n    def get_topic(topic_name: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `get_topic` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        topic = client.projects().locations().topics()\n        response = topic.get(name=topic_name, location=\"us-central1\")\n        print(response.execute())\n\n    @task(task_id=\"delete_topic\")\n    def delete_topic(topic_name: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `delete_topic` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        client.projects().locations().topics()\n        operation = client.topic_ids()\n        operation.name = \"%s/topics/%s\" % (topic_name, topic_name)\n        operation.delete()\n\n    @task(task_id=\"create_partition\")\n    def create_partition(topic_id: str, partition_id: int, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `create_partition` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        partition = {\n            \"name\": topic_id + \"/ partitions/\" + str(partition_id),\n            \"partitionId\": partition_id,\n            \"type\": \"PARTITION\",\n        }\n        client.projects().locations().topics().partitions().insert(body=partition, location=\"us-central1\")\n\n    @task(task_id=\"update_partition\")\n    def update_partition(topic_id: str, partition_id: int, updated_partition: dict, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `update_partition` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        partition_req = {\n            \"name\": topic_id + \"/ partitions/\" + str(partition_id),\n            \"partitionId\": partition_id,\n            \"type\": \"PARTITION\",\n        }\n        request = client.projects().locations().topics().partitions()\n        operation = request.update(\n            body=partition_req, name=topic_id + \"/ partitions/\" + str(partition_id), location=\"us-central1\"\n        )\n        operation.result()\n\n    @task(task_id=\"list_partitions\")\n    def list_partitions(topic_id: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `list_partitions` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        page = client.projects().locations().topics()\n        request = page.list(parent=topic_id)\n        response = request.execute()\n        for partition in response.items:\n            print(partition.name)\n\n    @task(task_id=\"get_partition\")\n    def get_partition(partition_name: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `get_partition` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        partition = client.projects().locations().topics()\n        response = partition.get(name=partition_name, location=\"us-central1\")\n        print(response.execute())\n\n    @task(task_id=\"delete_partition\")\n    def delete_partition(topic_id: str, partition_id: int, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `delete_partition` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        client.projects().locations().topics()\n        operation = client.partition_ids()\n        operation.name = \"%s/topics/%s/partitions/%d\" % (topic_id, topic_id, partition_id)\n        operation.delete()\n\n    @task(task_id=\"create_template\")\n    def create_template(project_id: str, template_name: str, topic_id: str, **kwargs):\n        \"\"\"\n        This is an example function that will be called via the `create_template` decorator.\n        \"\"\"\n        client = kwargs[\"test_context\"][\"client\"]\n        template = {\n            \"name\": template_name,\n            \"description\": \"Test template\",\n            \"topic\": topic_id,\n            \"spec\": {\"enabled\": True},\n        }"}], "metadata": {"file_name": "dag_2082_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042416}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build a data migration pipeline that checks for the existence of a file in an Azure container and then transfers that file to a specified Google Cloud Storage location, showcasing cross-cloud data synchronization and movement capabilities.\n\nAirflow Version: 3.0.0"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.advisor import AzureAdvisorCheckOperator\nfrom airflow.providers.microsoft.azure.transfers.azure_container_to_gcs import AzureContainerToGCSTransferOperator\nfrom airflow.providers.microsoft.azure.utils import load_file_into_blob\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nCONTAINER_NAME = f\"container_{ENV_ID}\"\nAZURE_ACCOUNT_NAME = os.environ.get(\"AZURE_ACCOUNT_NAME\")\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nGCP_BUCKET_NAME = f\"bucket_{ENV_ID}_{AZURE_ACCOUNT_NAME}\"\n\nBLOB_PREFIX = \"airflow\"\nLOCAL_PATH = str(os.path.dirname(__file__) + \"/resources/\")\n\n\n@task\ndef create_connection(conn_id_name):\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"azure_container\",\n        extra={\"server\": AZURE_ACCOUNT_NAME, \"container_path\": LOCAL_PATH},\n    )\n    dag_run = (\n        DAG.getDAG(\"test-advisor\"))\n    dag_run.add_task(lambda: setattr(conn, \"conn_id\", conn_id_name))\n    dag_run.xcom_push(task_ids=[f\"create_{conn_id_name}\"], value=conn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_advisor_check(check_id: str):\n    from airflow.providers.microsoft.azure.hooksadvisor import AdvisorHook\n\n    advisor_hook = AdvisorHook()\n    advisor_hook.delete_check(configuration_id=str(GCP_PROJECT_ID), check_id=check_id)\n\n\nwith DAG(\n        \"test_advisor\",\n        schedule=\"@once\",\n        start_date=datetime(2021, 1, 1),\n        catchup=False) as dag:\n    connect_and_create = create_connection(conn_id_name=\"azure_container_conn\")\n    # [START howto_operator_advisor_check]\n    check_for_upload = AzureAdvisorCheckOperator(\n        task_id=\"check_for_upload\",\n        configuration_id=GCP_PROJECT_ID,\n        check_id=\"check1\",\n        wait_for_completion=True,\n    )\n    # [END howto_operator_advisor_check]\n    upload_to_blob = AzureContainerToGCSTransferOperator(\n        task_id=\"upload_to_blob\",\n        container_name=CONTAINER_NAME,\n        container_path=LOCAL_PATH,\n        blob_prefix=BLOB_PREFIX,\n    )\n    blob_to_local = AzureContainerToGCSTransferOperator(\n        task_id=\"blob_to_local\",\n        container_name=CONTAINER_NAME,\n        container_path=f\"{BLOB_PREFIX}other/\",\n        dest_path=LOCAL_PATH,\n    )\n\n    (connect_and_create >> check_for_upload >> upload_to_blob >> blob_to_local)\n\n    # [START howto_operator_get_check_status]\n    get_check_status = AzureContainerToGCSTransferOperator(\n        task_id=\"get_check_status\",\n        connection_id=\"azure_container_conn\",\n        source_container_name=CONTAINER_NAME,\n        source_container_path=LOCAL_PATH,\n        dest_path=LOCAL_PATH + \"_get_check_status\",\n    )\n    # [END howto_operator_get_check_status]\n\n    (delete_advisor_check(check_id))"}], "metadata": {"file_name": "dag_1176_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.0", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042497}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that showcases task dependencies and sequential PostgreSQL operations, illustrating how different database tasks can be chained together with clear dependency management.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.providers.pgdb.hooks.pgdb import PgDBHook\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"pgdb\"],\n)\ndef example_pgdb_hook():\n    @task\n    def create_postgres_connection():\n        # [START howto_operator_pgdb]\n        PgDBHook(postgres_conn_id=\"my_pg_db\", sql=\"select 1\").execute()\n        # [END howto_operator_pgdb]\n\n    create_postgres_connection_task = create_postgres_connection()\n\n\nexample_pgdb_hook()\n\n\n# Example DAG used to test the PGDBHook\nfrom tests.system.utils import get_test_env_id\n\ntest_env_id = get_test_env_id()"}], "metadata": {"file_name": "dag_302_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042523}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an end-to-end data governance pipeline that tracks the creation, validation, and cleanup of metadata entries in a systematic workflow. Construct a DAG that simulates real-world metadata management processes with clear create, get, list, and delete operations.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.mssql_to_gcs import MsSqlToGcsOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_mssql import GcsToMsSqlOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_mssql_to_gcs\"\n\nBUCKET_NAME = f\"bucket_{DAG_ID}_{ENV_ID}\"\nFILE_NAME = \"file.txt\"\n MSSQL_CONN_ID = \"mssql_default\"\nGCS_BUCKET_FILE_PATH = \"test-gcs-path\"\nTABLE_NAME = \"test_mssql_to_gcs_table\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(task_id=\"create_bucket\", bucket_name=BUCKET_NAME)\n\n    # [START howto_operator_mssql_to_gcs]\n    mssql_to_gcs_task = MsSqlToGcsOperator(\n        task_id=\"mssql_to_gcs_task\",\n        ms_sql_conn_id=MSSQL_CONN_ID,\n        gcs_bucket=BUCKET_NAME,\n        table=TABLE_NAME,\n        gcs_prefix=GCS_BUCKET_FILE_PATH,\n        replace=True,\n        check_for_new_data=True,\n    )\n    # [END howto_operator_mssql_to_gcs]\n\n    # [START howto_operator_gcs_to_mssql]\n    gcs_to_mssql_task = GcsToMsSqlOperator(\n        task_id=\"gcs_to_mssql_task\",\n        source_bucket=BUCKET_NAME,\n        source_object=GCS_BUCKET_FILE_PATH + \"/\" + TABLE_NAME,\n        destination_table=f\"{PROJECT_ID}.{TABLE_NAME}\",\n        ms_sql_conn_id=MSSQL_CONN_ID,\n    )\n    # [END howto_operator_gcs_to_mssql]\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        # TEST BODY\n        >> mssql_to_gcs_task\n        >> gcs_to_mssql_task\n        # TEST TEARDOWN\n        >> delete_bucket\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark it as done.\n    list(dag.tasks) | watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_326_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042588}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that shows how to programmatically manage and manipulate data in Google BigQuery using Airflow operators. Create a workflow that transforms raw data, performs quality checks, and demonstrates different query execution strategies.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "INTO \\`root.\\'''{output_table_id}'\\'"}], "metadata": {"file_name": "dag_1573_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042888}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow pipeline that showcases different task execution strategies for Looker data transformations, highlighting the ability to start PDT builds with both blocking and non-blocking approaches. Implement a workflow that demonstrates task chaining and sensor-based job monitoring.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG that shows the complexity level of the Looker Data Transfer Operations (LDTO).\nIn this example:\n* Different LDTO methods are being called at different complexities.\n* First it's shown how single tasks should be executed manually.\n* Then there is an automated script which executes all the tasks of the complex LDTOs in sequence.\n\nNote: Due to the way Airflow works, looker_procedure_run_sensor and looker_query_sensor will not work if the\nDAG is refreshed. You may need to run them by hand after a refresh in order for them to stop and re-start.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\n\nimport pytest\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.transfers.mssql_to_looker import MSSQLToLookerOperator\nfrom airflow.providers.google.cloud.transfers.local_to_mssql import LocalMSSQLTransferOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# [START howto_operator_looker]\ndef looker(task_instance):\n    # TaskInstance.xcom_pull retrieves data pushed by earlier tasks\n    looker_conn = task_instance.xcom_pull(\"looker_conn_pull\", force_pull=True)\n    return looker_conn[\"looker_endpoint_url\"] + \"admin/lookml/procedures?scope=public&api_key=\" + \\\n        task_instance.xcom_pull(\"looker_key_pull\", force_pull=True)\n# End of example operator\n\n# [START howto_operator_looker_query]\ndef looker_query(task_instance):\n    looker_conn = task_instance.xcom_pull(\"looker_conn_pull\", force_pull=True)\n    query = f\"PROCURER LOOKER;\"; query_params = task_instance.xcom_pull(\"looker_query_params_pull\", force_pull=True)\n    return f\"{looker_conn['looker_endpoint_url']}?{query}{query_params}\"\n# End of example operator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"example_looker\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime.datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @dag.task(task_id=\"looker_conn_pull\")\n    def get_looker_connection():\n        looker_conn = {\n            \"mssql_conn_id\": \"airflow-mssql\",\n            \"port\": 1433,\n            \"schema\": \"master\",\n            \"sql_command\": \"SELECT 1\",\n            \"retries\": 5,\n            \"retry_delay\": datetime.timedelta(minutes=3),\n        }\n        return looker_conn\n\n    @dag.task(task_id=\"looker_key_pull\")\n    def get_looker_key():\n        looker_key = \"test-looker-key\"\n        return looker_key\n\n    @dag.task(task_id=\"looker_query_params_pull\")\n    def get_looker_query_params():\n        looker_query_params = \"?scope=public&api_key=\"\n        return looker_query_params\n\n    mgsql_to_looker_inlet = LocalMSSQLTransferOperator(\n        task_id=\"mgsql_to_looker_inlet\",\n        mssql_conn_id=\"airflow-mssql\",\n        sql=\"SELECT 1\",\n        schema=\"temp\",\n        output_path=\"output_path\",\n    )\n\n    mgsql_to_looker_outlet = LocalMSSQLTransferOperator(\n        task_id=\"mgsql_to_looker_outlet\",\n        mssql_conn_id=\"airflow-mssql\",\n        sql=\"SELECT 1\",\n        schema=\"temp\",\n        input_path=\"output_path\",\n        destination_table=\"output_table\",\n    )\n\n    # [START howto_operator_looker]\n    looker_procedure_run = LookerProcedureRunOperator(task_id=\"loker_procedure_run_task\", body=looker())\n    # End of example operator\n\n    # [START howto_operator_looker_query]\n    looker_query_run = LookerQueryRunOperator(\n        task_id=\"loker_query_run_task\",\n        api_version=\"v1\",\n        endpoint=\"procedures/run\",\n        body={\"procedureKey\": \"test-looker-key\" & \"output_path/output_table\"},\n    )\n    # End of example operator\n\n    chain(get_looker_connection(), get_looker_key(), get_looker_query_params(), mgsql_to_looker_inlet, mgsql_to_looker_outlet,\n          looker_procedure_run, looker_query_run)\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_42_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1042988}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG using the TriggerDagRunOperator. Create a scenario where one workflow initiates another workflow with a predefined configuration message.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\n\n@task(task_id=\"run_this\")\ndef run_by_python():\n    print(\"Nothing really needs to happen here, but just so we can demonstrate the decorator @task\")\n\n\nwith DAG(\n    dag_id=\"example_trigger_by_python',\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=['example'],\n) as dag:\n    run_by_python()\n    BashOperator(\n        task_id=\"run_me\",\n        bash_command='echo \"Hello World\">',\n        do_xcom_push=True,\n    )"}], "metadata": {"file_name": "dag_2223_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043024}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that illustrates the infrastructure-as-code approach to setting up a scalable job scheduling environment in Kubernetes. Create a workflow that demonstrates provisioning computational resources, defining job queues, and preparing a system for parallel workload processing.\n\nAirflow Version: 2.9.3"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google Cloud Workload Identity service testing.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.workload_identity import (\n    CloudWorkloadIdentityCreateJobQueueOperator,\n    CloudWorkloadIdentityDeleteJobQueueOperator,\n    CloudWorkloadIdentityListJobQueuesOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID = \"wi_job_queue\"\nWI_QUEUE_NAME = f\"wi_queue_{ENV_ID}\"\n\nLOCATION = \"europe-west1\"\nWI_JOB_QUEUES = [\n    {\n        \"name\": f\"{WI_QUEUE_NAME}-service-account\",\n        \"workload_identity_service_account\": {\"service_principals\": [f\"{PROJECT_ID}@appspot.gserviceaccount.com\"]},\n    },\n]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"workload-identity\", \"job-queue\"],\n) as dag:\n    # [START howto_operator_create_workload_identity_job_queue]\n    Create WI Job Queue = CloudWorkloadIdentityCreateJobQueueOperator(\n        task_id=\"create WI job queue\",\n        location=LOCATION,\n        wi_job_queues=WI_JOB_QUEUES,\n        project_id=PROJECT_ID,\n        wi_queue_name=WI_QUEUE_NAME,\n    )\n    # [END howto_operator_create_workload_identity_job_queue]\n\n    # [START howto_operator_list_workload_identity_job_queue]\n    List WI Job Queues = CloudWorkloadIdentityListJobQueuesOperator(\n        task_id=\"list WI job queues\",\n        location=LOCATION,\n        project_id=PROJECT_ID,\n    )\n    # [END howto_operator_list_workload_identity_job_queue]\n\n    delete WI Job Queue =\n    CloudWorkloadIdentityDeleteJobQueueOperator(\n        task_id=\"delete WI job queue\",\n        location=LOCATION,\n        wi_job_queue_name=WI_QUEUE_NAME,\n        project_id=PROJECT_ID,\n    )\n\n    delete WI Job Queue trigger = (\n        TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        Create WI Job Queue\n        List WI Job Queues\n        # TEST BODY\n        delete WI Job Queue\n        delete WI Job Queue\n    )\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_902_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.9.3", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104309}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct an Airflow DAG that demonstrates integration with InfluxDB using the InfluxDBOperator for time-bounded data extraction. Build a task that pulls recent time-series data using a dynamic time range relative to the current execution date.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.influxdb.hooks.influxdb import InfluxDBHook\nfrom airflow.providers.influxdb.operators.influxdb import InfluxDBOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_influxdb_operator\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    # [START howto_operator_influxdb]\n\n    query_influxdb_task = InfluxDBOperator(\n        task_id=\"query_influxdb\",\n        influxdb_conn_id=\"influxdb_default\",\n        sql='from(bucket:\"test\") |> range(start: -10m)',\n    )\n\n    # [END howto_operator_influxdb]\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule \"after_success\" is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2145_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043131}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Design a Google Cloud Dataproc cluster management workflow that demonstrates creating, starting, stopping, and deleting a Dataproc cluster with specific configuration parameters. The workflow should showcase handling of existing clusters and provide a complete lifecycle management example for big data processing infrastructure.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for DataprocUpdateClusterOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.cloud.operators.dataproc import (\n    DataprocCreateClusterOperator,\n    DataprocDeleteClusterOperator,\n    DataprocUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"dataproc_update\"\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nREGION = \"europe-west2\"\n\n# Cluster definition\n# [START how_to_dataproc_update_cluster]\n\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 32},\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\n            \"boot_disk_type\": \"pd-standard\",\n            \"boot_disk_size_gb\": 32,\n        },\n    },\n    \"software_config\": {\n        \"image_version\": \"2.1.1\",\n    },\n}\n\n# Update options\n# [START how_to_dataproc_updatable_cluster]\nUPDATE_CLUSTER = {\n    \"config\": {\n        \"worker_config\": {\n            \"num_instances\": 3,\n        }\n    }\n}\n# [END how_to_dataproc_updatable_cluster]\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_cluster = DataprocCreateClusterOperator(\n        task_id=\"create_cluster\",\n        project_id=PROJECT_ID,\n        cluster_config=CLUSTER_CONFIG,\n        region=REGION,\n        cluster_name=CLUSTER_NAME,\n    )\n\n    update_cluster = DataprocUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        cluster_name=CLUSTER_NAME,\n        cluster=UPDATE_CLUSTER,\n        region=REGION,\n    )\n\n    delete_cluster = DataprocDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        project_id=PROJECT_ID,\n        cluster_name=CLUSTER_NAME,\n        region=REGION,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    (\n        # TEST SETUP\n        create_cluster\n        # TEST BODY\n        >> update_cluster\n        # TEST TEARDOWN\n        >> delete_cluster\n    )\n\n    from tests_common.test_utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"teardown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1622_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043217}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Build an Airflow DAG that showcases task dependencies and lifecycle management, including nested task groups with explicit setup and teardown operations. Develop a workflow that demonstrates how to link tasks with clear initialization and finalization steps, using both decorator-based and direct task relationship methods.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.sdk import chain\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\n# [START howto_task_group\u5916\u90e8]\ndef task_decorator():\n    print(\"I am outside the task group!\")\n\n# [END howto_task_group\u5916\u90e8]\n# [START howto_task_group\u5167]\nwith TaskGroup(\"section_1\", dag=dag) as section_1:\n    print(\"I am inside the first task group!\")\n    task_1 = BashOperator(task_id=\"task_1\", bash_command=\"echo I am task 1\")\n    task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo I am task 2\")\n    task_3 = BashOperator(task_id=\"task_3\", bash_command=\"echo I am task 3\")\n\n    task_1 >> task_2 >> task_3\n# [END howto_task_group\u5167]\n# [START howto_task_group\u5916]\ntask_decorator()\n# [END howto_task_group\u5916]\nsection_1.end()\n\n# [START howto_task_group_for_loop]\nfor i in range(5):\n    with TaskGroup(f\"section_{i}\", dag=dag) as section_{i}:\n        print(f\"I am inside the task group {i}\")\n        task_1 = BashOperator(\n            task_id=\"task_1\",\n            bash_command=f\"echo I am task {i}; sleep {i + 1}; echo I am finished task {i}\",\n        )\n\n        task_1 >> BashOperator(task_id=\"wait_for_python\", bash_command=\"sleep 2\")\n\n        task_1 >> BashOperator(task_id=f\"bash_{i}\", bash_command=f\"echo I am python; {i + 1}\")\n\n    section_{i}.end()\n# [END howto_task_group_for_loop]\n\n\nchain(section_1, section_1)\n# [START howto_task_group_for_loop_with_external_template]\nfor i in range(5):\n    with TaskGroup(f\"section_{i}\", dag=dag) as section_{i}:\n        print(f\"I am inside the task group {i}\")\n        task_1 = BashOperator(\n            task_id=\"task_1\",\n            bash_command=f\"echo I am task {i}; sleep {i + 1}; echo I am finished task {i}\",\n        )\n\n        task_1 >> BashOperator(\n            task_id=\"wait_for_python\",\n            bash_command=f\"sleep 2; {pendulum.datetime(2028, 7, 9, 14, 30)}\",\n        )\n\n        task_1 >> BashOperator(task_id=f\"bash_{i}\", bash_command=f\"echo I am python; {i + 1}\")\n\n    section_{i}.end()\n# [END howto_task_group_for_loop_with_external_template]"}], "metadata": {"file_name": "dag_1858_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043286}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Develop an example data processing workflow that simulates a multi-stage bash script execution environment, demonstrating how different computational tasks can be sequenced and managed through Airflow's task scheduling mechanism. Create a DAG that shows how simple shell commands can be orchestrated with complex dependency relationships.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(task_id=\"run_this\")\ndef my_task():\n    print(\"Hello from someone who might not get the message\")\n\n\nwith DAG(\n    dag_id=\"example_apt\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n) as dag:\n    my_task()\n\n\nfrom tests.system.utils import get_test_run\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_328_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043313}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Construct a data processing pipeline that leverages Azure Container Instances for scalable, ephemeral compute tasks. Create a DAG that demonstrates launching containerized workloads with configurable resources, environment variables, and storage volume integrations.\n\nAirflow Version: 3.0.1"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.microsoft.azure.operators.container_instances import (\n    ContainerInstancesCreateClusterOperator,\n    ContainerInstancesDeleteClusterOperator,\n    ContainerInstancesGetClusterOperator,\n    ContainerInstancesUpdateClusterOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"ci_test_cni\"\n\nCLUSTER_NAME = f\"cluster_{ENV_ID}\"\nLOCATION = \"uksouth\"\nACR_NAME = f\"acr_{ENV_ID}\"\n\n# For more information, please visit https://docs.microsoft.com/en-us/azure/container-instances/create-container-instance-using-aks?tabs=python\n\ndefault_args = {\n    \"retries\": 2,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=None,\n    catchup=False,\n    default_args=default_args,\n    tags=[\"example\", \"container-instance\"],\n) as dag:\n    @task\n    def create_acr():\n        conn = Connection(\n            conn_id=\"acs_default\",\n            conn_type=\"azure_container_instance\",\n            extra={\n                \"resource_group_name\": \"resourcegroup\",\n                \"registry_name\": ACR_NAME,\n            },\n        )\n        dao = conn.builder()\n        dao.set(\"resource_group_name\", \"resourcegroup\")\n        dao.set(\"name\", ACR_NAME)\n        dao.set(\"location\", LOCATION)\n        dao.create()\n\n    create_acr_task = create_acr()\n\n    # [START howto_operator_container_instances_create_cluster]\n    create_cluster = ContainerInstancesCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"admin_user\": {\"username\": \"dave\", \"password\": \"PASSWORD1\"},\n            \"linux_profile\": {\"admin_username\": \"dave\"},\n        },\n    )\n    # [END howto_operator_container_instances_create_cluster]\n\n    # [START howto_operator_container_instances_update_cluster]\n    update_cluster = ContainerInstancesUpdateClusterOperator(\n        task_id=\"update_cluster\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        aci_params={\"some_new_property\": \"hello world\"},\n    )\n    # [END howto_operator_container_instances_update_cluster]\n\n    # [START howto_operator_container_instances_get_cluster]\n    get_cluster = ContainerInstancesGetClusterOperator(\n        task_id=\"get_cluster\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n    )\n    # [END howto_operator_container_instances_get_cluster]\n\n    # [START howto_operator_container_instances_delete_cluster]\n    delete_cluster = ContainerInstancesDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    # [END howto_operator_container_instances_delete_cluster]\n\n    list_clusters = ContainerInstancesCreateClusterOperator(\n        task_id=\"list_clusters\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"admin_user\": {\"username\": \"dave\", \"password\": \"PASSWORD1\"},\n            \"linux_profile\": {\"admin_username\": \"dave\"},\n        },\n    )\n\n    join_list_clusters = ContainerInstancesCreateClusterOperator(\n        task_id=\"join_list_clusters\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"admin_user\": {\"username\": \"dave\", \"password\": \"PASSWORD1\"},\n            \"linux_profile\": {\"admin_username\": \"dave\"},\n        },\n    )\n\n    expand_cluster = ContainerInstancesCreateClusterOperator(\n        task_id=\"expand_cluster\",\n        cluster_name=CLUSTER_NAME,\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"node_count\": 2,\n        },\n    )\n\n    merge_clusters = ContainerInstancesCreateClusterOperator(\n        task_id=\"merge_clusters\",\n        cluster_name=f\"{CLUSTER_NAME}_merged\",\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"node_count\": 4,\n        },\n    )\n\n    combine_list_merge_clusters = ContainerInstancesCreateClusterOperator(\n        task_id=\"combine_list_merge_clusters\",\n        cluster_name=f\"{CLUSTER_NAME}_merged\",\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"node_count\": 6,\n        },\n    )\n\n    # [START howto_operator_container_instances_create_cluster_for_acr]\n    create_cluster_acr = ContainerInstancesCreateClusterOperator(\n        task_id=\"create_cluster_acr\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n       aci_params={\n            \"resource_group_name\": \"resourcegroup\",\n            \"registry_name\": ACR_NAME,\n        },\n    )\n    # [END howto_operator_container_instances_create_cluster_for_acr]\n\n    build_image = ContainerInstancesCreateImageTaskOperator(\n        task_id=\"build_image\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        image_name=\"azure-containers-demo-build:david\",\n    )\n\n    # [START howto_operator_container_instances_push_image_to_acr]\n    push_image_to_acr = ContainerInstancesPushImageToAcrOperator(\n        task_id=\"push_image_to_acr\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        image_name=\"azure-containers-demo-build:david\",\n        repository_name=ACR_NAME,\n        repository_path=\"azure-containers-demo-build\",\n    )\n    # [END howto_operator_container_instances_push_image_to_acr]\n\n    # [START howto_operator_container_instances_create_cluster_from_image]\n    create_cluster_from_image = ContainerInstancesCreateClusterFromImageOperator(\n        task_id=\"create_cluster_from_image\",\n        cluster_name=f\"cluster_{ENV_ID}_from-acr\",\n        resource_group_name=\"resourcegroup\",\n        location=LOCATION,\n        image_name=\"azure-containers-demo-build:david\",\n    )\n    # [END howto_operator_container_instances_create_cluster_from_image]\n\n    # [START howto_operator_container_instances_restart_cluster]\n    restart_cluster = ContainerInstancesRestartClusterOperator(\n        task_id=\"restart_cluster\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        new_node_count=8,\n    )\n    # [END howto_operator_container_instances_restart_cluster]\n\n    # [START howto_operator_container_instances_create_describe_cluster_with_env]\n    describe_cluster_with_env = ContainerInstancesCreateDescribeClusterWithEnvOperator(\n        task_id=\"describe_cluster_with_env\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        env_vars={\"TEST_VAR\": \"Testing\"},\n    )\n    # [END howto_operator_container_instances_create_describe_cluster_with_env]\n\n    # [START howto_operator_container_instances_create_start_cluster_with_env]\n    start_cluster_with_env = ContainerInstancesCreateStartClusterWithEnvOperator(\n        task_id=\"start_cluster_with_env\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        command=\"-i -t testing-system-test\",\n    )\n    # [END howto_operator_container_instances_create_start_cluster_with_env]\n\n    # [START howto_operator_container_instances_create_stop_cluster]\n    stop_cluster = ContainerInstancesCreateStopClusterOperator(\n        task_id=\"stop_cluster\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n    )\n    # [END howto_operator_container_instances_create_stop_cluster]\n\n    # [START howto_operator_container_instances_create_resume_cluster]\n    resume_cluster = ContainerInstancesCreateResumeClusterOperator(\n        task_id=\"resume_cluster\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n    )\n    # [END howto_operator_container_instances_create_resume_cluster]\n\n    # [START howto_operator_container_instances_create_sleep_cluster]\n    sleep_cluster = ContainerInstancesCreateSleepClusterOperator(\n        task_id=\"sleep_cluster\",\n        cluster_name=f\"cluster_{ENV_ID}\",\n        resource_group_name=\"resourcegroup\",\n        seconds=900,\n    )\n    # [END howto_operator_container_instances_create_sleep_cluster]\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def remove_acr():\n        conn = Connection(\n            conn_id=\"acs_default\",\n            conn_type=\"azure_container_instance\",\n            extra={\n                \"resource_group_name\": \"resourcegroup\",\n                \"registry_name\": ACR_NAME,\n            },\n        )\n        dao = conn.builder()\n        dao.delete()\n\n    remove_acr_task = remove_acr()\n    # [END start_dag]\n\n    from tests_common.test_utils.watcher import watcher\n\n    list(dag.tasks) >> watcher()\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_1746_variant_3", "instruction_source": "claude-3.5", "variant_number": 3, "airflow_version": "3.0.1", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043515}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow workflow that demonstrates a complete data transfer process from Google Ads to Google Cloud Storage, including bucket creation, data extraction using a custom query, listing of advertising accounts, and automatic bucket cleanup. The workflow should showcase inter-service orchestration and demonstrate best practices for cloud data migration.\n\nAirflow Version: 2.8.4"}, {"role": "assistant", "content": "\"\"\"\nExample Airflow DAG for Google AdsToGCSOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.google.marketing_platform.operators.ads import (\n    GoogleAdsToGCSQueryOperator,\n    GoogleAdsToGCSUploadOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nPROJECT_ID = os.environ.get(\"SYSTEM_TESTS_GCP_PROJECT\")\n\nDAG_ID= \"example_ads_to_gcs\"\n\nBUCKET_NAME = f\"bucket_ads_{DAG_ID}_{ENV_ID}\"\nAD_CLIENT_ID = os.environ.get(\"MARKETING_PLATFORM_AD_CLIENT_ID\", \"\")\nGCS_OBJECT_NAME = os.environ.get(\"MARKETING_PLATFORM_GCS_OBJECT_NAME\", \"\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",  # Override to match your needs\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=BUCKET_NAME,\n        project_id=PROJECT_ID,\n        location=\"EU\",\n    )\n    # [START howto_marketing_apis_get_ad_accounts_query]\n    get_ad_accounts_query_task = GoogleAdsToGCSQueryOperator(\n        task_id=\"get_ad_accounts_query_task\",\n        customer_id=ADCLIENT_ID,\n        query=\"\"\"\n            SELECT clientName, campaignId, campaignName, adGroupID, adGroupName\n            FROM admessages\n            WHERE clientName IN ( 'Google', 'Facebook' )\n             AND lastModifiedTime > date_sub(today(), interval 1 year)\n          \"\"\",\n        gcp_project_id=PROJECT_ID,\n        gcp_dataset_id=\"marketingplatform\",\n        gcs_object_name=GCS_OBJECT_NAME + \".adaccounts.sql\",\n        use_kubernetes_engine=False,\n    )\n    # [END howto_marketing_apis_get_ad_accounts_query]\n    # [START howto_marketing_apis_add_custom_list_to_ad_message_query]\n    add_custom_list_to_ad_message_task = GoogleAdsToGCSQueryOperator(\n        task_id=\"add_custom_list_to_ad_message_task\",\n        customer_id=ADCLIENT_ID,\n        query=\"\"\"\n            SELECT clientName, campaignId, campaignName, adGroupID, adGroupName\n            FROM admessages\n            WHERE clientName = 'Google'\n            AND lastModifiedTime > date_sub(today(), interval 1 year)\n            ADD CUSTOM_LIST 'test_list_1';\n          \"\"\",\n        gcp_project_id=PROJECT_ID,\n        gcp_dataset_id=\"marketingplatform\",\n        gcs_object_name=GCS_OBJECT_NAME + \".campaigns.csv\",\n        use_kubernetes_engine=False,\n    )\n    # [END howto_marketing_apis_add_custom_list_to_ad_message_query]\n    # [START howto_marketing_apis_upload_batch_ad_group_info_to_gcs]\n    upload_batch_ad_group_info_to_gcs_task = GoogleAdsToGCSUploadOperator(\n        task_id=\"upload_batch_ad_group_info_to_gcs_task\",\n        customer_id=ADCLIENT_ID,\n        file_path=str(Path(__file__).parent / \"resources\" / \"batch_adgroups.csv\"),\n        gcp_project_id=PROJECT_ID,\n        gcp_dataset_id=\"marketingplatform\",\n        gcs_object_name=GCS_OBJECT_NAME + \".batchadgroup.csv\",\n        is_batch_file=True,\n    )\n    # [END howto_marketing_apis_upload_batch_ad_group_info_to_gcs]\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\", bucket_name=BUCKET_NAME, trigger_rule=TriggerRule.ALL_DONE\n    )\n\n    (\n        # TEST SETUP\n        create_bucket\n        >> get_ad_accounts_query_task\n        >> add_custom_list_to_ad_message_task\n        >> upload_batch_ad_group_info_to_gcs_task\n    )\n\n    # TEST TEARDOWN\n    delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_454_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "2.8.4", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104362}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an interactive Airflow DAG that demonstrates dynamic parameter configuration for workflow triggering, showcasing how users can customize input parameters with various input types and validation rules.\n\nAirflow Version: 2.7.2"}, {"role": "assistant", "content": "\"\"\"DAG demonstrating various options for trigger form generation.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport uuid\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\n\n@task(trigger_rule=\"all_done\")\ndef my_task_func():\n    print(\"Hello\")\n\n\nwith DAG(\n    dag_id=str(uuid.uuid4()),\n    start_date=datetime.datetime(2021, 1, 1),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\", \"trigger\", \"at time\"],\n) as dag:\n    my_task_func()\n\n\nif __name__ == \"__main__\":\n    # This DAG has two tasks, `task_1` and `task_2`.\n    # `task_1` is decorated with trigger rule of \"all_done\".\n    # When both `task_1` and `task_2` reach their \"all_done\" condition,\n    # the DAG gets triggered.\n    dag.execute()"}], "metadata": {"file_name": "dag_16_variant_1", "instruction_source": "claude-3.5", "variant_number": 1, "airflow_version": "2.7.2", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043653}}
{"messages": [{"role": "system", "content": "You are an expert Apache Airflow developer. Generate complete, valid Airflow DAGs based on given requirements."}, {"role": "user", "content": "Create an Airflow DAG that demonstrates error handling and external alerting by implementing a bash task with integrated failure notifications to an incident management platform.\n\nAirflow Version: 3.0.6"}, {"role": "assistant", "content": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.dag import DAG\nfrom airflow.providers.splunk.hooks.splunk import SplunkHook\nfrom airflow.providers.splunk.operators.splunk import SPLUNK_OPERTOR_TYPE_TOKEN\nfrom airflow.utils.trigger_rule import TriggerRule\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_splunk_operator\"\n\nconn_id = \"splunk\"\nsolr_index_name = \"test-index\"\nsplunk_host = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\nsplunk_port = os.environ.get(\"SPLUNK_PORT\", 8089)\nsplunk_admin_username = os.environ.get(\"SPLUNK_ADMIN_USERNAME\", \"admin\")\nsplunk_admin_password = os.environ.get(\"SPLUNK_ADMIN_PASSWORD\", \"admin\")\n\nBashCommand = \"\"\"#!/bin/bash\necho 'Our awesome new index!'\necho 'test-index' >> /tmp/myscript.sh\nchmod +x /tmp/myscript.sh\n./tmp/myscript.sh\n\"\"\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    @task\n    def create_solr_index(splunk_hook: SplunkHook):\n        splunk_hook.connect()\n        splunk_hook.create_solr_index(solr_master_uri=f\"{splunk_host}:{splunk_port}\", solr_index_name=solr_index_name)\n        splunk_hook.close()\n\n    @task\n    def enable_splunk_alert(splunk_hook: SplunkHook):\n        splunk_hook.connect()\n        splunk_hook.enable_app_alerting()\n        splunk_hook.set_app_alert_channel(default_channel=SPLUNK_OPERTATOR_TYPE_TOKEN)\n        splunk_hook.enable_app_channel_alert(app_channel_name=\"default_channel\", channels=[SPLUNK_OPERTATOR_TYPE_TOKEN])\n        splunk_hook.close()\n\n    @task\n    def create_connection(splunk_hook: SplunkHook, conn_id: str):\n        splunk_hook.connect()\n        splunk_hook.create_connection(conn_id=conn_id, hostname=splunk_host, port=splunk_port)\n        splunk_hook.close()\n\n    @task\n    def get_conn(conn_id: str):\n        conn_obj = SplunkHook().get_connection(conn_id=conn_id)\n        return {\n            \"SplunkHost\": conn_obj.hostname,\n            \"SplunkPort\": conn_obj.port,\n            \"SplunkAdminUsername\": splunk_admin_username,\n            \"SplunkAdminPassword\": splunk_admin_password,\n        }\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_solr_index(splunk_hook: SplunkHook):\n        splunk_hook.connect()\n        splunk_hook.delete_solr_index(solr_master_uri=f\"{splunk_host}:{splunk_port}\", solr_index_name=solr_index_name)\n        splunk_hook.close()\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def delete_connection(splunk_hook: SplunkHook, conn_id: str):\n        splunk_hook.connect()\n        splunk_hook.delete_connection(conn_id=conn_id)\n        splunk_hook.close()\n\n    # [START howto_operator_splunk]\n    splunk_to_splunk_task = create_connection(splunk_hook=SplunkHook(), conn_id=\"splunk\")\n    # [END howto_operator_splunk]\n\n    # [START howto_operator_splunk_xcom]\n    splash_output = BashCommand.output()\n    # The result of splash_output will be available in the xcom task splunk_to_splunk_task\n    # To get it in the test we have to get it from the trigger context\n    BashCommand.trigger(xcom_key=\"splash_output\")\n    # [END howto_operator_splunk_xcom]\n\n    # [START howto_operator_get_conn]\n    conn_info = get_conn(SPLUNK_CONN_ID)\n    # [END howto_operator_get_conn]\n\n    # [START howto_operator_splunk_upload_solr_data]\n    upload_solr_data = SplunkHook().upload_solr_data(connection_id=\"splunk\", body={\"query\": {\"match_all\": {}}})\n    # [END howto_operator_splunk_upload_solr_data]\n\n    # [START howto_operator_splunk_log_event]\n    log_event = SplunkHook().log_event(\n        connection_id=\"splunk\", event=dict(event_type=\"test-event\", sourcetype=\"my-sourcetype\", indexing_mode=\"indexonly\")\n    )\n    # [END howto_operator_splunk_log_event]\n\n    create_solr_index(splunk_hook=SplunkHook())\n    enable_splunk_alert(splunk_hook=SplunkHook())\n    BashCommand()\n    delete_solr_index(splunk_hook=SplunkHook(), conn_id=conn_id)\n    delete_connection(splunk_hook=SplunkHook(), conn_id=conn_id_id)\n\n\nfrom tests_common.test_utils.system_tests import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see below)\ntest_run = get_test_run(dag)"}], "metadata": {"file_name": "dag_2179_variant_2", "instruction_source": "claude-3.5", "variant_number": 2, "airflow_version": "3.0.6", "model": "Qwen2.5-Finetuned", "inference_time": 1765984638.104375}}
{"messages": [{"role": "system", "content": "You are an expert Python developer. Provide complete, working code solutions for programming tasks. Include brief explanations of key concepts when helpful for understanding."}, {"role": "user", "content": "I have a dataset with multiple time-series data points for different categories, and I want to visualize the trends over time for these categories using Python. I have started using Matplotlib and Pandas. Can you guide me on how to plot multiple time-series data on the same graph to compare their trends over time?"}, {"role": "assistant", "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"}], "metadata": {"model": "Qwen2.5-Finetuned", "inference_time": 1765984638.1043847}}
