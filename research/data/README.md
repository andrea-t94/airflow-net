# AirflowNet Data Pipeline

This directory contains the tools and scripts for creating the **AirflowNet** training dataset. The pipeline consists of three sequential steps: mining raw DAGs from Apache Airflow repositories, generating instruction-following pairs using Claude, and formatting/uploading the final dataset.

## ğŸ“‚ Directory Structure

```
research/data/
â”œâ”€â”€ config/             # Configuration files
â”‚   â”œâ”€â”€ mining_config.yaml
â”‚   â””â”€â”€ generation_config.yaml
â”œâ”€â”€ lib/                # Shared library code
â”‚   â”œâ”€â”€ mining.py       # GitHub mining logic
â”‚   â”œâ”€â”€ instruction.py  # Claude API interaction
â”‚   â””â”€â”€ config_loader.py
â”œâ”€â”€ scripts/            # Executable pipeline scripts
â”‚   â”œâ”€â”€ 01_mine_dags.py
â”‚   â”œâ”€â”€ 02_gen_instruct.py
â”‚   â””â”€â”€ 03_create_dataset.py
â””â”€â”€ artifacts/      # Research artifacts (gitignored)
    â”œâ”€â”€ 01_raw_dags/
    â”œâ”€â”€ 02_instruct_dags/
    â”œâ”€â”€ magpie_data/
    â””â”€â”€ inference/
```

## ğŸ› ï¸ Prerequisites

Ensure you have the project dependencies installed:
```bash
pip install -e .
```

You will need the following environment variables in a `.env` file at the project root:
```ini
GITHUB_TOKEN=your_github_token          # For Step 1 (Mining)
ANTHROPIC_API_KEY=your_anthropic_key    # For Step 2 (Generation)
HF_TOKEN=your_huggingface_token         # For Step 3 (Upload)
```

---

## ğŸš€ Pipeline Steps

### Step 1: Mining Raw DAGs
**Script**: `scripts/01_mine_dags.py`

Fetches `example_*.py` DAGs from the official Apache Airflow repository across multiple versions defined in `config/mining_config.yaml`.

- **Input**: GitHub API (Apache Airflow Repo)
- **Output**: `research/artifacts/01_raw_dags/dags.jsonl`
- **Features**:
  - Validates DAG syntax using AST.
  - Extracts metadata (task count, dependencies).
  - Skips non-DAG Python files.

**Usage**:
```bash
# Run full mining process
python research/data/scripts/01_mine_dags.py
```

### Step 2: Instruction Generation
**Script**: `scripts/02_gen_instruct.py`

Uses Anthropic's Claude API (via Message Batches) to analyze each DAG and generate high-quality natural language instructions (e.g., "Create a DAG that runs every daily and extracts data from S3").

- **Input**: `research/artifacts/01_raw_dags/dags.jsonl`
- **Output**: `research/artifacts/02_instruct_dags/airflow_instructions.jsonl`
- **Features**:
  - **Filters out invalid DAGs** (syntax errors, circular dependencies).
  - Generates multiple instruction variants per DAG.
  - Formats data for ChatML (System/User/Assistant).

**Usage**:
```bash
# Generate instructions for all mined DAGs
python research/data/scripts/02_gen_instruct.py

# Run a test batch (limit to 5 DAGs)
python research/data/scripts/02_gen_instruct.py --test
```

### Step 3: Dataset Creation & Upload
**Script**: `scripts/03_create_dataset.py`

Validates, splits, and uploads the final dataset to the HuggingFace Hub.

- **Input**: `research/artifacts/02_instruct_dags/airflow_instructions.jsonl`
- **Output**: HuggingFace Dataset
- **Features**:
  - Validates ChatML format.
  - Performs train/test split.
  - Uploads to the configured repository.

**Usage**:
python research/data/scripts/03_create_dataset.py
```
