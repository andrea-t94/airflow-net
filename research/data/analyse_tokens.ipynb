{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dataset Token Analysis\n",
                "\n",
                "This notebook analyzes the token distribution of the Airflow DAG dataset hosted on Hugging Face.\n",
                "It uses the `transformers` library for accurate tokenization matching the target model (Qwen2.5-Coder)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install datasets transformers matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "DATASET_ID = \"andrea-t94/airflow-dag-dataset\"  # Replace with your actual dataset ID\n",
                "MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"  # Using the target model's tokenizer\n",
                "SPLIT = \"train\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Dataset and Tokenizer\n",
                "print(f\"Loading dataset: {DATASET_ID}...\")\n",
                "dataset = load_dataset(DATASET_ID, split=SPLIT)\n",
                "\n",
                "print(f\"Loading tokenizer: {MODEL_ID}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_row(row):\n",
                "    messages = row['messages']\n",
                "    user_content = next((m['content'] for m in messages if m['role'] == 'user'), \"\")\n",
                "    assistant_content = next((m['content'] for m in messages if m['role'] == 'assistant'), \"\")\n",
                "    \n",
                "    # Accurate token counting\n",
                "    user_tokens = len(tokenizer.encode(user_content))\n",
                "    assistant_tokens = len(tokenizer.encode(assistant_content))\n",
                "    total_tokens = user_tokens + assistant_tokens\n",
                "    \n",
                "    return {\n",
                "        \"user_tokens\": user_tokens,\n",
                "        \"assistant_tokens\": assistant_tokens,\n",
                "        \"total_tokens\": total_tokens\n",
                "    }\n",
                "\n",
                "print(\"Analyzing tokens (this might take a moment)...\")\n",
                "analysis_ds = dataset.map(analyze_row)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to lists for statistics\n",
                "user_lens = analysis_ds['user_tokens']\n",
                "assistant_lens = analysis_ds['assistant_tokens']\n",
                "total_lens = analysis_ds['total_tokens']\n",
                "\n",
                "def print_stats(name, data):\n",
                "    print(f\"\\n{name} Statistics:\")\n",
                "    print(f\"  Mean:   {np.mean(data):.2f}\")\n",
                "    print(f\"  Median: {np.median(data):.2f}\")\n",
                "    print(f\"  Min:    {np.min(data)}\")\n",
                "    print(f\"  Max:    {np.max(data)}\")\n",
                "    print(f\"  P90:    {np.percentile(data, 90):.2f}\")\n",
                "    print(f\"  P95:    {np.percentile(data, 95):.2f}\")\n",
                "    print(f\"  P99:    {np.percentile(data, 99):.2f}\")\n",
                "\n",
                "print_stats(\"Prompt (User)\", user_lens)\n",
                "print_stats(\"Response (Assistant)\", assistant_lens)\n",
                "print_stats(\"Total Context\", total_lens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizations\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "plt.subplot(1, 3, 1)\n",
                "sns.histplot(user_lens, kde=True)\n",
                "plt.title(\"User Prompts\")\n",
                "plt.xlabel(\"Tokens\")\n",
                "\n",
                "plt.subplot(1, 3, 2)\n",
                "sns.histplot(assistant_lens, kde=True)\n",
                "plt.title(\"Assistant Responses\")\n",
                "plt.xlabel(\"Tokens\")\n",
                "\n",
                "plt.subplot(1, 3, 3)\n",
                "sns.histplot(total_lens, kde=True, color='green')\n",
                "plt.title(\"Total Context Length\")\n",
                "plt.xlabel(\"Tokens\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}