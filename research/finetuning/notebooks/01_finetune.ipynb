{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac8e2bb",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andrea-t94/airflow-net/blob/master/research/finetuning/notebooks/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Finetuning Qwen2.5 on Airflow DAGs\n",
    "\n",
    "This notebook demonstrates how to fine-tune the **Qwen2.5-1.5B-Instruct** model on a dataset of **Airflow DAGs** using [Unsloth](https://github.com/unslothai/unsloth) \u2014 a library that makes LLM fine-tuning 2x faster and uses 50% less memory.\n",
    "\n",
    "### \ud83c\udfaf Goal\n",
    "Train a model that can generate valid Airflow DAGs from natural language instructions.\n",
    "\n",
    "### \ud83d\udee0\ufe0f Runtime Requirements\n",
    "- **GPU**: Tesla T4 (Free Colab) or A100 (Colab Pro).\n",
    "- **RAM**: Standard.\n",
    "\n",
    "### \ud83d\udccb Steps\n",
    "1. **Setup**: Install dependencies.\n",
    "2. **Config**: Define model and dataset parameters.\n",
    "3. **Data**: Load and format the Airflow DAG dataset.\n",
    "4. **Train**: Fine-tune with QLoRA (4-bit quantization).\n",
    "5. **Save**: Push the fine-tuned model (GGUF & LoRA) to Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6e456",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup & Installation\n",
    "We install `unsloth` and other necessary libraries. If you are running this in Google Colab, it will automatically detect the environment and install the correct versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe8a91",
   "metadata": {
    "id": "setup-code"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install Unsloth and dependencies\n",
    "if IN_COLAB:\n",
    "    # Unsloth installation for Colab\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "else:\n",
    "    # Standard installation\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Additional Requirements\n",
    "!pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e21f4a",
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "# Verify GPU and PyTorch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Detected: {gpu_name}\")\n",
    "    # Unsloth optimization info\n",
    "    major_version, minor_version = torch.cuda.get_device_capability()\n",
    "    if major_version >= 8:\n",
    "        print(\"\u2705 GPU supports bfloat16 (Ampere or newer). Operations will be faster.\")\n",
    "    else:\n",
    "        print(\"\u2139\ufe0f GPU is older than Ampere (e.g., T4). Using float16.\")\n",
    "else:\n",
    "    print(\"\u274c No GPU detected! Please change runtime type to GPU in 'Runtime > Change runtime type'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a668a9a",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## 2. Configuration & Authentication\n",
    "Log in to Hugging Face to access datasets and push your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea81d2",
   "metadata": {
    "id": "auth-code"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Try to get token from Colab secrets, otherwise prompt\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, add_to_git_credential=True)\n",
    "except (ImportError, KeyError, AttributeError):\n",
    "    print(\"Please provide your Hugging Face Token (Permissions: Write)\")\n",
    "    login(add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf217ab",
   "metadata": {
    "id": "constants"
   },
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "DATASET_NAME = \"andrea-t94/airflow-dag-dataset\"\n",
    "\n",
    "# Output Model Name (Change username if needed)\n",
    "NEW_MODEL_NAME = \"andrea-t94/qwen2.5-1.5b-airflow-instruct\"\n",
    "\n",
    "# Training Parameters\n",
    "MAX_SEQ_LENGTH = 4096 # Fits most DAG files\n",
    "LOAD_IN_4BIT = True   # Enable 4-bit quantization (QLoRA) to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0a532",
   "metadata": {
    "id": "model-header"
   },
   "source": [
    "## 3. Load Model with Unsloth\n",
    "We load the model in 4-bit precision to fit within standard Colab GPU memory (15GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356a09a",
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None, # Auto-detect float16 or bfloat16 based on GPU\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Add LoRA (Low-Rank Adaptation) adapters\n",
    "# This allows us to train only a small fraction of parameters (~1%)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,            # LoRA Rank (8, 16, 32, 64 are common)\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable for long context\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522f1cd",
   "metadata": {
    "id": "data-header"
   },
   "source": [
    "## 4. Load & Format Dataset\n",
    "We specificy a formatting function to apply the ChatML template (which Qwen uses) to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8abd58",
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "# Inspect dataset sizes\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "if 'eval' in dataset: print(f\"Eval size:  {len(dataset['eval'])}\")\n",
    "\n",
    "# Format function for ChatML\n",
    "# The dataset should have a 'messages' column matching standard chat format\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Apply chat template but do NOT tokenize yet\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa512f9",
   "metadata": {
    "id": "train-header"
   },
   "source": [
    "## 5. Training\n",
    "Configure the `SFTTrainer`. We use `gradient_accumulation_steps` to simulate a larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f011b",
   "metadata": {
    "id": "train-setup"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset.get(\"eval\"),\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Set to True to speed up training if sequences are short\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,  # Increase if GPU memory allows\n",
    "        gradient_accumulation_steps = 4,   # effective_batch = 2 * 4 = 8\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,                   # Set to -1 for full epochs\n",
    "        # num_train_epochs = 1,           # Uncomment for full training\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",             # Use 8-bit optimizer to save memory\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",               # Disable WandB for simplicity\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e77c41",
   "metadata": {
    "id": "train-run"
   },
   "outputs": [],
   "source": [
    "# Start Training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f3334",
   "metadata": {
    "id": "save-header"
   },
   "source": [
    "## 6. Save & Push to Hub\n",
    "Unsloth allows saving both LoRA adapters (small) and the full merged model (large). We'll also push a GGUF version for local inference (e.g., with Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f7818",
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "# 1. Save LoRA Adapters only (Small file size, fast)\n",
    "model.save_pretrained(\"lora_adapters\")\n",
    "model.push_to_hub(f\"{NEW_MODEL_NAME}-lora\", token=True)\n",
    "\n",
    "# 2. Save Merged Model (Full model, slower but easier to use)\n",
    "print(\"Saving merged model... this might take a while.\")\n",
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "model.push_to_hub_merged(NEW_MODEL_NAME, tokenizer, save_method = \"merged_16bit\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d638018",
   "metadata": {
    "id": "save-gguf"
   },
   "outputs": [],
   "source": [
    "# 3. Convert to GGUF (for Ollama/Llama.cpp)\n",
    "# Options: q4_k_m, q8_0, f16\n",
    "print(\"Converting to GGUF...\")\n",
    "model.push_to_hub_gguf(\n",
    "    NEW_MODEL_NAME,\n",
    "    tokenizer,\n",
    "    quantization_method = [\"q4_k_m\"],\n",
    "    token = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}