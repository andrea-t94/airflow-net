{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andrea-t94/airflow-net/blob/master/research/finetuning/notebooks/generate_test_samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Generate Test Samples (Model Inference)\n",
    "\n",
    "This notebook generates Airflow DAGs using both the **Base Model** and the **Fine-Tuned Model** on the test dataset. The output is saved as JSONL files which are then used for evaluation.\n",
    "\n",
    "### \ud83c\udfaf Goal\n",
    "Produce DAG samples from unseen test instructions to measure model performance.\n",
    "\n",
    "### \u26a0\ufe0f Runtime Note\n",
    "To avoid Out-of-Memory (OOM) errors on T4 GPUs, we generate samples in two distinct passes:\n",
    "1. **Pass 1**: Load Base Model -> Generate -> Unload/Clear RAM.\n",
    "2. **Pass 2**: Load Fine-Tuned Model -> Generate -> Unload/Clear RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-code"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install Unsloth & libraries\n",
    "if IN_COLAB:\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "!pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-info"
   },
   "outputs": [],
   "source": [
    "# GPU Info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\u274c No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-vars"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Models\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "FINETUNED_MODEL_ID = \"andrea-t94/qwen2.5-1.5b-airflow-instruct\" # Adapter ID\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"andrea-t94/airflow-dag-dataset\"\n",
    "\n",
    "# Generation Config\n",
    "MAX_NEW_TOKENS = 2048 # Limit to avoid extremely long generations\n",
    "BATCH_SIZE = 4        # Conservative batch size for T4\n",
    "\n",
    "# Auth\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        login(token=userdata.get('HF_TOKEN'), add_to_git_credential=True)\n",
    "    except:\n",
    "        login(add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data-header"
   },
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"test\")\n",
    "print(f\"\u2705 Loaded {len(dataset)} test examples\")\n",
    "\n",
    "# Optional: Sample a subset for quick testing\n",
    "# dataset = dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helpers-header"
   },
   "source": [
    "## 4. Helper Functions\n",
    "Utilities for generating text and parsing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helpers"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "def extract_code(text):\n",
    "    \"\"\"Extract python code block from response.\"\"\"\n",
    "    if \"```python\" in text:\n",
    "        return text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in text:\n",
    "        return text.split(\"```\")[1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def run_inference_pass(model_id, dataset, output_filename, is_adapter=False):\n",
    "    \"\"\"Loads a model, generates responses, saves to file, and clears memory.\"\"\"\n",
    "    print(f\"\\n\ud83d\ude80 Starting Pass for: {model_id}\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_id,\n",
    "        max_seq_length = 4096,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True, # Use 4bit for speed & memory\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # 2. Prepare Inputs\n",
    "    prompts = []\n",
    "    for x in dataset:\n",
    "        # Use Chat Template\n",
    "        msgs = x[\"messages\"]\n",
    "        # Ensure we only input the user/system messages, excluding any assistant response if present\n",
    "        input_msgs = [m for m in msgs if m[\"role\"] != \"assistant\"]\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            input_msgs,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # 3. Batch Generation\n",
    "    results = []\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    print(f\"Generating {len(prompts)} samples...\")\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = MAX_NEW_TOKENS,\n",
    "            use_cache = True,\n",
    "            do_sample = True,\n",
    "            temperature = 0.1,\n",
    "            top_p = 0.9,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        # Only decode the new tokens\n",
    "        generated_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Store results\n",
    "        for j, text in enumerate(decoded):\n",
    "            original_idx = i + j\n",
    "            results.append({\n",
    "                \"prompt\": batch_prompts[j],\n",
    "                \"generated_text\": text,\n",
    "                \"code\": extract_code(text),\n",
    "                \"model\": model_id,\n",
    "                \"original_dataset_idx\": original_idx\n",
    "            })\n",
    "\n",
    "    # 4. Save Results (JSONL)\n",
    "    import json\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for r in results:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "    print(f\"\u2705 Saved results to {output_filename}\")\n",
    "\n",
    "    # 5. Cleanup\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\ud83e\uddf9 Memory Cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-base"
   },
   "source": [
    "## 5. Run Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-base-code"
   },
   "outputs": [],
   "source": [
    "run_inference_pass(\n",
    "    model_id=BASE_MODEL_ID,\n",
    "    dataset=dataset,\n",
    "    output_filename=\"base_model_samples.jsonl\",\n",
    "    is_adapter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-finetuned"
   },
   "source": [
    "## 6. Run Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-finetuned-code"
   },
   "outputs": [],
   "source": [
    "run_inference_pass(\n",
    "    model_id=FINETUNED_MODEL_ID,\n",
    "    dataset=dataset,\n",
    "    output_filename=\"finetuned_model_samples.jsonl\",\n",
    "    is_adapter=True # Unsloth handles adapter loading automatically if passed as model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 7. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-code"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(\"base_model_samples.jsonl\")\n",
    "    time.sleep(2)\n",
    "    files.download(\"finetuned_model_samples.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}