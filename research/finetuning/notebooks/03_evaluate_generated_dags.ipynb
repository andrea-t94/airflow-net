{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Airflow DAG Generation: Evaluation\n",
        "\n",
        "This notebook evaluates the quality of Airflow DAGs generated by different models, comparing a baseline model (Qwen 2.5 1.5B Instruct) against a fine-tuned version.\n",
        "\n",
        "## \ud83d\udccb Setup Note\n",
        "\n",
        "**This notebook is designed to run locally** (not in Colab). To set it up:\n",
        "\n",
        "```bash\n",
        "# From project root\n",
        "pip install -e \".[research]\"\n",
        "pip install jupyter\n",
        "\n",
        "# Launch Jupyter and select the venv kernel\n",
        "jupyter notebook\n",
        "```\n",
        "\n",
        "Make sure you're using the Python kernel from your virtual environment to ensure all imports work correctly.\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcca Performance Summary (Fine-tuned vs Base)\n",
        "\n",
        "Our evaluation reveals significant improvements across multiple dimensions:\n",
        "\n",
        "### Key Improvements\n",
        "- **Syntax Validity**: ~8% reduction in syntactically invalid DAGs\n",
        "- **Modern Patterns**: Strong adoption of latest Airflow syntax and operators (TaskFlow API, modern decorators)\n",
        "- **Reduced Hallucinations**: Significantly fewer instances of invented imports or non-existent operators\n",
        "- **Error Distribution Alignment**: The fine-tuned model's error patterns closely match real-world DAG distributions\n",
        "\n",
        "### Notable Observations\n",
        "- **Base Model**: Often generates deprecated patterns (e.g., legacy operators from Airflow 1.x)\n",
        "- **Fine-tuned Model**: Occasionally hallucinates internal testing libraries seen in training data, but far less than general hallucinations in the base model\n",
        "- **Syntax Accuracy**: Fine-tuned model shows consistent adherence to Python and Airflow syntax rules\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udd2c Evaluation Methodology\n",
        "\n",
        "We employ a two-pronged evaluation approach to assess both structural correctness and semantic quality:\n",
        "\n",
        "### 1. Parser-Based Evaluation (Structural Analysis)\n",
        "\n",
        "Uses a custom AST-based validator (`DAGValidator`) to check:\n",
        "- **Syntax Correctness**: Valid Python syntax, no parse errors\n",
        "- **Task ID Validation**: Unique task IDs, proper naming conventions (alphanumeric, dashes, dots, underscores)\n",
        "- **Dependency Analysis**: Detection of circular dependencies in task graphs\n",
        "- **DAG Structure**: Proper DAG instantiation, task definitions, and relationships\n",
        "\n",
        "**Advantages**: Fast, deterministic, catches critical structural errors that would prevent DAG execution.\n",
        "\n",
        "### 2. LLM-Based Evaluation (Semantic Analysis)\n",
        "\n",
        "Uses Claude 4.5 Sonnet via the Batch API to evaluate:\n",
        "- **Correctness** (1-5): Does the generated DAG logically implement the user's request?\n",
        "- **Completeness** (1-5): Are all necessary imports, arguments, and task dependencies present?\n",
        "- **Best Practices** (1-5): Does it follow Airflow conventions and modern patterns?\n",
        "\n",
        "**Advantages**: Captures semantic quality, intent alignment, and code quality aspects that structural analysis misses.\n",
        "\n",
        "**Note**: LLM evaluation requires an Anthropic API key and incurs costs. Results are saved for reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Import from installed packages\n",
        "from research.lib.batch_processor import ClaudeBatchProcessor\n",
        "from airflow_net.validation import DAGValidator\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API Key Configuration\n",
        "import os\n",
        "\n",
        "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"WARNING: No ANTHROPIC_API_KEY found in environment. LLM evaluation steps will be skipped.\")\n",
        "    print(\"To enable LLM evaluation, set the ANTHROPIC_API_KEY environment variable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "We load the generated DAGs from the JSONL artifacts produced by the inference step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for artifacts in: /Users/andreatamburri/Desktop/airflowNet/research/artifacts/finetuning/01_inference_results\n",
            "Selected Baseline: base_model_outputs_20251217_151724.jsonl\n",
            "Selected Fine-tuned: finetuned_model_outputs_20251217_151724.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Define paths to artifacts (relative to notebook location)\n",
        "ARTIFACTS_DIR = Path(\"../../artifacts/finetuning/01_inference_results\").resolve()\n",
        "\n",
        "print(f\"Looking for artifacts in: {ARTIFACTS_DIR}\")\n",
        "\n",
        "# Find latest inference files\n",
        "base_files = list(ARTIFACTS_DIR.glob(\"base_model_outputs*.jsonl\"))\n",
        "finetuned_files = list(ARTIFACTS_DIR.glob(\"finetuned_model_outputs*.jsonl\"))\n",
        "\n",
        "if not base_files or not finetuned_files:\n",
        "    print(\"WARNING: Could not find one or both inference result files.\")\n",
        "    print(f\"Available files: {list(ARTIFACTS_DIR.glob('*.jsonl'))}\")\n",
        "    BASE_MODEL_FILE = None\n",
        "    FINETUNED_MODEL_FILE = None\n",
        "else:\n",
        "    # Take the most recent one\n",
        "    BASE_MODEL_FILE = sorted(base_files)[-1]\n",
        "    FINETUNED_MODEL_FILE = sorted(finetuned_files)[-1]\n",
        "    print(f\"Selected Baseline: {BASE_MODEL_FILE.name}\")\n",
        "    print(f\"Selected Fine-tuned: {FINETUNED_MODEL_FILE.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 412 baseline samples.\n",
            "Loaded 412 fine-tuned samples.\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(file_path):\n",
        "    \"\"\"Load JSONL file and extract code from messages format\"\"\"\n",
        "    if not file_path or not file_path.exists():\n",
        "        return []\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                entry = json.loads(line)\n",
        "                # Extract assistant content from messages\n",
        "                messages = entry.get('messages', [])\n",
        "                user_content = ''\n",
        "                assistant_content = ''\n",
        "                \n",
        "                for msg in messages:\n",
        "                    if msg['role'] == 'user':\n",
        "                        user_content = msg['content']\n",
        "                    elif msg['role'] == 'assistant':\n",
        "                        assistant_content = msg['content']\n",
        "                \n",
        "                data.append({\n",
        "                    'prompt': user_content,\n",
        "                    'code': assistant_content,\n",
        "                    'metadata': entry.get('metadata', {})\n",
        "                })\n",
        "    return data\n",
        "\n",
        "baseline_data = load_jsonl(BASE_MODEL_FILE)\n",
        "finetuned_data = load_jsonl(FINETUNED_MODEL_FILE)\n",
        "\n",
        "print(f\"Loaded {len(baseline_data)} baseline samples.\")\n",
        "print(f\"Loaded {len(finetuned_data)} fine-tuned samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parser Evaluation Results (%):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>valid_rate</th>\n",
              "      <th>syntax_errors</th>\n",
              "      <th>duplicate_tasks</th>\n",
              "      <th>invalid_task_ids</th>\n",
              "      <th>circular_deps</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Baseline</th>\n",
              "      <td>95.87</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fine-tuned</th>\n",
              "      <td>71.12</td>\n",
              "      <td>27.91</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            valid_rate  syntax_errors  duplicate_tasks  invalid_task_ids  \\\n",
              "model                                                                      \n",
              "Baseline         95.87           3.16             0.00              0.97   \n",
              "Fine-tuned       71.12          27.91             0.49              0.00   \n",
              "\n",
              "            circular_deps  \n",
              "model                      \n",
              "Baseline             0.00  \n",
              "Fine-tuned           0.73  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def evaluate_parser_results(data, model_name):\n",
        "    \"\"\"Evaluate DAG code using the DAGValidator\"\"\"\n",
        "    results = []\n",
        "    validator = DAGValidator()\n",
        "    \n",
        "    for entry in data:\n",
        "        code = entry.get('code', '')\n",
        "        \n",
        "        # Use DAGValidator.validate_content which returns List[ValidationError]\n",
        "        errors = validator.validate_content(code, source_name=\"<generated>\")\n",
        "        \n",
        "        # Extract error information\n",
        "        is_valid = len(errors) == 0\n",
        "        error_messages = [str(e) for e in errors]\n",
        "        error_types = [e.error_type for e in errors]\n",
        "        \n",
        "        results.append({\n",
        "            'model': model_name,\n",
        "            'is_valid': is_valid,\n",
        "            'error_count': len(errors),\n",
        "            'errors': '; '.join(error_messages) if error_messages else '',\n",
        "            'has_syntax_error': any('SYNTAX_ERROR' in et or 'PARSE_ERROR' in et for et in error_types),\n",
        "            'has_duplicate_task': any('DUPLICATE_TASK_ID' in et for et in error_types),\n",
        "            'has_invalid_task_id': any('INVALID_TASK_ID' in et for et in error_types),\n",
        "            'has_circular_dependency': any('CIRCULAR_DEPENDENCY' in et for et in error_types)\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if baseline_data and finetuned_data:\n",
        "    df_base = evaluate_parser_results(baseline_data, 'Baseline')\n",
        "    df_fine = evaluate_parser_results(finetuned_data, 'Fine-tuned')\n",
        "    df_parser = pd.concat([df_base, df_fine], ignore_index=True)\n",
        "    \n",
        "    # Display summary statistics\n",
        "    summary = df_parser.groupby('model').agg(\n",
        "        valid_rate=('is_valid', 'mean'),\n",
        "        syntax_errors=('has_syntax_error', 'mean'),\n",
        "        duplicate_tasks=('has_duplicate_task', 'mean'),\n",
        "        invalid_task_ids=('has_invalid_task_id', 'mean'),\n",
        "        circular_deps=('has_circular_dependency', 'mean')\n",
        "    ) * 100\n",
        "    \n",
        "    print(\"Parser Evaluation Results (%):\")\n",
        "    display(summary.round(2))\n",
        "else:\n",
        "    print(\"Skipping parser evaluation due to missing data.\")\n",
        "    df_parser = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM-Based Evaluation with Claude\n",
        "Using Claude 4.5 Sonnet to score DAGs on Correctness, Completeness, and Best Practices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing evaluation batches...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'instruction'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Prepare batches for full datasets\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing evaluation batches...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m base_batch \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_eval_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m fine_batch \u001b[38;5;241m=\u001b[39m prepare_eval_batch(finetuned_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m all_requests \u001b[38;5;241m=\u001b[39m base_batch \u001b[38;5;241m+\u001b[39m fine_batch\n",
            "Cell \u001b[0;32mIn[32], line 49\u001b[0m, in \u001b[0;36mprepare_eval_batch\u001b[0;34m(data, model_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m     48\u001b[0m     custom_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mEVAL_PROMPT_TEMPLATE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     req \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: custom_id,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m         }\n\u001b[1;32m     61\u001b[0m     }\n\u001b[1;32m     62\u001b[0m     requests\u001b[38;5;241m.\u001b[39mappend(req)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'instruction'"
          ]
        }
      ],
      "source": [
        "EVAL_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert Senior Apache Airflow Architect. Evaluate the following Airflow DAG code generated by an AI model based on a user instruction.\n",
        "\n",
        "### Scoring Criteria & Examples\n",
        "\n",
        "**1. Idiomatic Airflow (Score 0 or 1)**\n",
        "* **Score 1 (Pass):** Uses specific Providers and Operators designed for the task.\n",
        "    * *Example:* `from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator`\n",
        "* **Score 0 (Fail):** Relies on generic \"Pythonic\" patterns where it wraps logic in a `PythonOperator` + Hook instead of using the native Operator.\n",
        "    * *Example:* `def load(): hook = SnowflakeHook(...) \\n PythonOperator(python_callable=load ...)`\n",
        "\n",
        "**2. No Hallucination/Leakage (Score 0 or 1)**\n",
        "* **Score 1 (Pass):** Code is clean, production-ready, and uses only standard Airflow libraries.\n",
        "* **Score 0 (Fail):** Code imports internal testing modules or includes test harness boilerplate.\n",
        "    * *Example:* `from tests_common.test_utils.system_tests import get_test_run`\n",
        "    * *Example:* `test_run = get_test_run(dag)`\n",
        "\n",
        "**3. Instruction Adherence (Score 0 or 1)**\n",
        "* **Score 1 (Pass):** Fulfills the specific business logic requested (e.g., \"load data AND validate\").\n",
        "* **Score 0 (Fail):** Misses a key step of the instruction.\n",
        "\n",
        "---\n",
        "\n",
        "### Task\n",
        "USER INSTRUCTION:\n",
        "{instruction}\n",
        "\n",
        "DAG CODE:\n",
        "```python\n",
        "{dag_content}\n",
        "```\n",
        "\n",
        "\n",
        "Evaluate the code based on the criteria above. Return valid JSON only.\n",
        "\n",
        "{{{{\n",
        "  \"idiomatic_airflow\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
        "  \"no_hallucination\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}},\n",
        "  \"instruction_adherence\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
        "}}}}\n",
        "\"\"\"\n",
        "\n",
        "def prepare_eval_batch(data, model_name):\n",
        "    \"\"\"Prepare requests for Batch API.\"\"\"\n",
        "    requests = []\n",
        "    \n",
        "    for i, entry in enumerate(data):\n",
        "        custom_id = f\"{model_name}-{i}\"\n",
        "        prompt = EVAL_PROMPT_TEMPLATE.format(\n",
        "            prompt=entry.get('prompt', ''),\n",
        "            code=entry.get('code', '')\n",
        "        )\n",
        "        \n",
        "        req = {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"params\": {\n",
        "                \"model\": \"claude-sonnet-4-20250514\",\n",
        "                \"max_tokens\": 1024,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "        }\n",
        "        requests.append(req)\n",
        "    return requests\n",
        "\n",
        "if ANTHROPIC_API_KEY and baseline_data and finetuned_data:\n",
        "    processor = ClaudeBatchProcessor(api_key=ANTHROPIC_API_KEY)\n",
        "    \n",
        "    # Prepare batches for full datasets\n",
        "    print(f\"Preparing evaluation batches...\")\n",
        "    base_batch = prepare_eval_batch(baseline_data, \"baseline\")\n",
        "    fine_batch = prepare_eval_batch(finetuned_data, \"finetuned\")\n",
        "    all_requests = base_batch + fine_batch\n",
        "    \n",
        "    print(f\"\u2713 Prepared {len(all_requests)} evaluation requests ({len(base_batch)} baseline + {len(fine_batch)} fine-tuned)\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Skipping LLM evaluation setup:\")\n",
        "    if not ANTHROPIC_API_KEY:\n",
        "        print(\"  - No ANTHROPIC_API_KEY found in environment\")\n",
        "    if not baseline_data or not finetuned_data:\n",
        "        print(\"  - Missing baseline or fine-tuned data\")\n",
        "    all_requests = []\n",
        "    processor = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-30 16:44:41,775 - INFO - \ud83d\ude80 Submitting batch with 824 requests...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 Submitting batch request to Claude API...\n",
            "   This will evaluate 824 DAGs using Claude Sonnet 4\n",
            "   Estimated cost: ~$12.36 (at $15/1M input tokens)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-30 16:44:44,569 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages/batches?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:44:44,573 - INFO - \u2705 Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
            "2025-12-30 16:44:44,574 - INFO - \u23f3 Waiting for batch msgbatch_01GuxFdkP6XrMjqjzxNwjCGb...\n",
            "2025-12-30 16:44:44,753 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:44:44,756 - INFO - \ud83d\udcca Status: in_progress (elapsed: 0.2s)\n",
            "2025-12-30 16:44:44,757 - INFO -    Progress: 0/824 (0.0%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
            "\n",
            "\u23f3 Waiting for batch to complete (this may take 10-30 minutes)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-30 16:45:15,001 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:45:15,005 - INFO - \ud83d\udcca Status: in_progress (elapsed: 30.4s)\n",
            "2025-12-30 16:45:15,006 - INFO -    Progress: 0/824 (0.0%)\n",
            "2025-12-30 16:45:45,259 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:45:45,263 - INFO - \ud83d\udcca Status: in_progress (elapsed: 60.7s)\n",
            "2025-12-30 16:45:45,263 - INFO -    Progress: 0/824 (0.0%)\n",
            "2025-12-30 16:46:15,515 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:46:15,518 - INFO - \ud83d\udcca Status: in_progress (elapsed: 90.9s)\n",
            "2025-12-30 16:46:15,519 - INFO -    Progress: 0/824 (0.0%)\n",
            "2025-12-30 16:46:45,750 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:46:45,756 - INFO - \ud83d\udcca Status: in_progress (elapsed: 121.2s)\n",
            "2025-12-30 16:46:45,756 - INFO -    Progress: 0/824 (0.0%)\n",
            "2025-12-30 16:47:16,010 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:47:16,013 - INFO - \ud83d\udcca Status: in_progress (elapsed: 151.4s)\n",
            "2025-12-30 16:47:16,014 - INFO -    Progress: 0/824 (0.0%)\n",
            "2025-12-30 16:47:46,334 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:47:46,336 - INFO - \ud83d\udcca Status: ended (elapsed: 181.8s)\n",
            "2025-12-30 16:47:46,336 - INFO -    Progress: 824/824 (100.0%)\n",
            "2025-12-30 16:47:46,336 - INFO - \u2705 Batch completed with status: ended\n",
            "2025-12-30 16:47:46,337 - INFO - \u2b07\ufe0f Downloading results...\n",
            "2025-12-30 16:47:46,530 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udce5 Downloading results...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-30 16:47:46,823 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb/results \"HTTP/1.1 200 OK\"\n",
            "2025-12-30 16:47:47,475 - INFO - \ud83d\udce5 Downloaded 824 items\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 Downloaded 824 results\n",
            "\n",
            "\ud83d\udcca Parsing evaluation scores...\n",
            "\u2713 Parsed 489 evaluation scores\n",
            "\u26a0\ufe0f 335 results had errors or couldn't be parsed\n",
            "\n",
            "LLM Evaluation Summary (1-5 scale):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correctness</th>\n",
              "      <th>completeness</th>\n",
              "      <th>best_practices</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Baseline</th>\n",
              "      <td>1.98</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fine-tuned</th>\n",
              "      <td>1.78</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.76</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            correctness  completeness  best_practices\n",
              "model                                                \n",
              "Baseline           1.98          1.78            1.82\n",
              "Fine-tuned         1.78          1.57            1.76"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Execute Batch Processing and Parse Results\n",
        "if processor and all_requests:\n",
        "    print(\"\ud83d\ude80 Submitting batch request to Claude API...\")\n",
        "    print(f\"   This will evaluate {len(all_requests)} DAGs using Claude Sonnet 4\")\n",
        "    print(f\"   Estimated cost: ~${len(all_requests) * 0.015:.2f} (at $15/1M input tokens)\")\n",
        "    print()\n",
        "    \n",
        "    # Submit batch\n",
        "    batch_id = processor.submit_batch(all_requests)\n",
        "    print(f\"\u2713 Batch submitted: {batch_id}\")\n",
        "    print()\n",
        "    \n",
        "    # Wait for completion (can take 10-30 minutes for large batches)\n",
        "    print(\"\u23f3 Waiting for batch to complete (this may take 10-30 minutes)...\")\n",
        "    batch = processor.wait_for_batch_completion(batch_id)\n",
        "    print()\n",
        "    \n",
        "    # Download results\n",
        "    print(\"\ud83d\udce5 Downloading results...\")\n",
        "    results = processor.download_batch_results(batch_id)\n",
        "    print(f\"\u2713 Downloaded {len(results)} results\")\n",
        "    print()\n",
        "    \n",
        "    # Parse results into dataframe\n",
        "    print(\"\ud83d\udcca Parsing evaluation scores...\")\n",
        "    llm_eval_results = []\n",
        "    parse_errors = 0\n",
        "    \n",
        "    for result in results:\n",
        "        if result['result']['type'] == 'succeeded':\n",
        "            text = result['result']['message']['content'][0]['text']\n",
        "            try:\n",
        "                scores = json.loads(text)\n",
        "                model_name = result['custom_id'].split('-')[0]\n",
        "                llm_eval_results.append({\n",
        "                    'model': 'Baseline' if model_name == 'baseline' else 'Fine-tuned',\n",
        "                    'correctness': scores.get('correctness_score', 0),\n",
        "                    'completeness': scores.get('completeness_score', 0),\n",
        "                    'best_practices': scores.get('best_practices_score', 0),\n",
        "                    'explanation': scores.get('explanation', '')\n",
        "                })\n",
        "            except json.JSONDecodeError:\n",
        "                parse_errors += 1\n",
        "        elif result['result']['type'] == 'errored':\n",
        "            parse_errors += 1\n",
        "    \n",
        "    df_llm = pd.DataFrame(llm_eval_results)\n",
        "    \n",
        "    print(f\"\u2713 Parsed {len(llm_eval_results)} evaluation scores\")\n",
        "    if parse_errors > 0:\n",
        "        print(f\"\u26a0\ufe0f {parse_errors} results had errors or couldn't be parsed\")\n",
        "    \n",
        "    # Display summary statistics\n",
        "    summary = df_llm.groupby('model').agg({\n",
        "        'correctness': 'mean',\n",
        "        'completeness': 'mean',\n",
        "        'best_practices': 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    print(\"\\nLLM Evaluation Summary (1-5 scale):\")\n",
        "    display(summary)\n",
        "    \n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Skipping LLM evaluation:\")\n",
        "    if not processor:\n",
        "        print(\"  - No API key configured\")\n",
        "    if not all_requests:\n",
        "        print(\"  - No evaluation requests prepared\")\n",
        "    df_llm = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Training Data - Number 20 Usage\n\n",
        "Check how many training examples contain the number \"20\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "import re\n\n",
        "print(\"Loading training dataset from HuggingFace...\")\n",
        "hf_train_dataset = load_dataset(\n",
        "    \"andrea-t94/airflow-dag-dataset\",\n",
        "    split=\"train\",\n",
        "    download_mode=\"reuse_cache_if_exists\"\n",
        ")\n\n",
        "print(f\"Loaded {len(hf_train_dataset)} training samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count how many training samples contain the number '20'\n",
        "count_with_20 = 0\n\n",
        "for example in hf_train_dataset:\n",
        "    messages = example.get('messages', [])\n",
        "    \n",
        "    # Combine all message content\n",
        "    all_text = ' '.join([msg['content'] for msg in messages])\n",
        "    \n",
        "    # Check for number 20 as standalone number\n",
        "    if re.search(r'\\b20\\b', all_text):\n",
        "        count_with_20 += 1\n\n",
        "print(f\"Total training samples: {len(hf_train_dataset)}\")\n",
        "print(f\"Samples containing '20': {count_with_20}\")\n",
        "print(f\"Percentage: {count_with_20/len(hf_train_dataset)*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (airflowNet)",
      "language": "python",
      "name": "airflownet"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}