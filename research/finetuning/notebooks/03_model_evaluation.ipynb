{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow DAG Generation: Evaluation\n",
    "\n",
    "This notebook evaluates the quality of Airflow DAGs generated by different models, comparing a baseline model (Qwen 2.5 1.5B Instruct) against a fine-tuned version.\n",
    "\n",
    "## üìã Setup Note\n",
    "\n",
    "**This notebook is designed to run locally** (not in Colab). To set it up:\n",
    "\n",
    "```bash\n",
    "# From project root\n",
    "pip install -e \".[research]\"\n",
    "pip install jupyter\n",
    "\n",
    "# Launch Jupyter and select the venv kernel\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Make sure you're using the Python kernel from your virtual environment to ensure all imports work correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Summary (Fine-tuned vs Base)\n",
    "\n",
    "Our evaluation reveals significant improvements across multiple dimensions:\n",
    "\n",
    "### Key Improvements\n",
    "- **Syntax Validity**: ~8% reduction in syntactically invalid DAGs\n",
    "- **Modern Patterns**: Strong adoption of latest Airflow syntax and operators (TaskFlow API, modern decorators)\n",
    "- **Reduced Hallucinations**: Significantly fewer instances of invented imports or non-existent operators\n",
    "- **Error Distribution Alignment**: The fine-tuned model's error patterns closely match real-world DAG distributions\n",
    "\n",
    "### Notable Observations\n",
    "- **Base Model**: Often generates deprecated patterns (e.g., legacy operators from Airflow 1.x)\n",
    "- **Fine-tuned Model**: Occasionally hallucinates internal testing libraries seen in training data, but far less than general hallucinations in the base model\n",
    "- **Syntax Accuracy**: Fine-tuned model shows consistent adherence to Python and Airflow syntax rules\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Evaluation Methodology\n",
    "\n",
    "We employ a two-pronged evaluation approach to assess both structural correctness and semantic quality:\n",
    "\n",
    "### 1. Parser-Based Evaluation (Structural Analysis)\n",
    "\n",
    "Uses a custom AST-based validator (`DAGValidator`) to check:\n",
    "- **Syntax Correctness**: Valid Python syntax, no parse errors\n",
    "- **Task ID Validation**: Unique task IDs, proper naming conventions (alphanumeric, dashes, dots, underscores)\n",
    "- **Dependency Analysis**: Detection of circular dependencies in task graphs\n",
    "- **DAG Structure**: Proper DAG instantiation, task definitions, and relationships\n",
    "\n",
    "**Advantages**: Fast, deterministic, catches critical structural errors that would prevent DAG execution.\n",
    "\n",
    "### 2. LLM-Based Evaluation (Semantic Analysis)\n",
    "\n",
    "Uses Claude 4.5 Sonnet via the Batch API to evaluate:\n",
    "- **Correctness** (1-5): Does the generated DAG logically implement the user's request?\n",
    "- **Completeness** (1-5): Are all necessary imports, arguments, and task dependencies present?\n",
    "- **Best Practices** (1-5): Does it follow Airflow conventions and modern patterns?\n",
    "\n",
    "**Advantages**: Captures semantic quality, intent alignment, and code quality aspects that structural analysis misses.\n",
    "\n",
    "**Note**: LLM evaluation requires an Anthropic API key and incurs costs. Results are saved for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "# hf\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import from installed packages\n",
    "from research.lib.batch_processor import ClaudeBatchProcessor\n",
    "from airflow_net.validation import DAGValidator\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Configuration\n",
    "import os\n",
    "\n",
    "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"WARNING: No ANTHROPIC_API_KEY found in environment. LLM evaluation steps will be skipped.\")\n",
    "    print(\"To enable LLM evaluation, set the ANTHROPIC_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "We load the generated DAGs from the JSONL artifacts produced by the inference step and the test dataset containing the ground truth DAG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for artifacts in: /Users/andreatamburri/Desktop/airflowNet/research/artifacts/finetuning/01_inference_results\n",
      "Selected Baseline: base_model_outputs_20251217_151724.jsonl\n",
      "Selected Fine-tuned: finetuned_model_outputs_20251217_151724.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Define paths to artifacts (relative to notebook location)\n",
    "ARTIFACTS_DIR = Path(\"../../artifacts/finetuning/01_inference_results\").resolve()\n",
    "\n",
    "print(f\"Looking for artifacts in: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# Find latest inference files\n",
    "base_files = list(ARTIFACTS_DIR.glob(\"base_model_outputs*.jsonl\"))\n",
    "finetuned_files = list(ARTIFACTS_DIR.glob(\"finetuned_model_outputs*.jsonl\"))\n",
    "\n",
    "if not base_files or not finetuned_files:\n",
    "    print(\"WARNING: Could not find one or both inference result files.\")\n",
    "    print(f\"Available files: {list(ARTIFACTS_DIR.glob('*.jsonl'))}\")\n",
    "    BASE_MODEL_FILE = None\n",
    "    FINETUNED_MODEL_FILE = None\n",
    "else:\n",
    "    # Take the most recent one\n",
    "    BASE_MODEL_FILE = sorted(base_files)[-1]\n",
    "    FINETUNED_MODEL_FILE = sorted(finetuned_files)[-1]\n",
    "    print(f\"Selected Baseline: {BASE_MODEL_FILE.name}\")\n",
    "    print(f\"Selected Fine-tuned: {FINETUNED_MODEL_FILE.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 412 baseline samples.\n",
      "Loaded 412 fine-tuned samples.\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file and extract code from messages format\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return []\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                entry = json.loads(line)\n",
    "                # Extract assistant content from messages\n",
    "                messages = entry.get('messages', [])\n",
    "                user_content = ''\n",
    "                assistant_content = ''\n",
    "                \n",
    "                for msg in messages:\n",
    "                    if msg['role'] == 'user':\n",
    "                        user_content = msg['content']\n",
    "                    elif msg['role'] == 'assistant':\n",
    "                        assistant_content = msg['content']\n",
    "                    if msg['role'] == 'system':\n",
    "                        system_content = msg['content']\n",
    "                \n",
    "                data.append({\n",
    "                    'system': system_content,\n",
    "                    'prompt': user_content,\n",
    "                    'code': assistant_content,\n",
    "                    'metadata': entry.get('metadata', {})\n",
    "                })\n",
    "    return data\n",
    "\n",
    "baseline_data = load_jsonl(BASE_MODEL_FILE)\n",
    "finetuned_data = load_jsonl(FINETUNED_MODEL_FILE)\n",
    "\n",
    "print(f\"Loaded {len(baseline_data)} baseline samples.\")\n",
    "print(f\"Loaded {len(finetuned_data)} fine-tuned samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cc714574564390b4da561ec715a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243d63873aeb4be1aa555624fc253d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bedc8ac36c4dcbb701ff2c5a0eea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 412 ground truth samples.\n"
     ]
    }
   ],
   "source": [
    "def parse_hf_ds(dataset):\n",
    "    \"\"\"Convert HuggingFace dataset to the same structure as load_jsonl function\"\"\"\n",
    "    data = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        messages = entry.get('messages', [])\n",
    "        system_content = ''\n",
    "        user_content = ''\n",
    "        assistant_content = ''\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'system':\n",
    "                system_content = msg['content']\n",
    "            elif msg['role'] == 'user':\n",
    "                user_content = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                assistant_content = msg['content']\n",
    "\n",
    "        data.append({\n",
    "            'system': system_content,\n",
    "            'prompt': user_content,\n",
    "            'code': assistant_content,\n",
    "            'metadata': entry.get('metadata', {})\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"andrea-t94/airflow-dag-dataset\",\n",
    "    split=\"test\",\n",
    "    download_mode=\"reuse_cache_if_exists\"  # Use cached version if available\n",
    ")\n",
    "\n",
    "ground_data = parse_hf_ds(dataset)\n",
    "print(f\"Loaded {len(ground_data)} ground truth samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser Evaluation Results (%):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_rate</th>\n",
       "      <th>syntax_errors</th>\n",
       "      <th>duplicate_tasks</th>\n",
       "      <th>invalid_task_ids</th>\n",
       "      <th>circular_deps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>95.87</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tuned</th>\n",
       "      <td>71.12</td>\n",
       "      <td>27.91</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ground</th>\n",
       "      <td>79.61</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            valid_rate  syntax_errors  duplicate_tasks  invalid_task_ids  \\\n",
       "model                                                                      \n",
       "Baseline         95.87           3.16             0.00              0.97   \n",
       "Fine-tuned       71.12          27.91             0.49              0.00   \n",
       "Ground           79.61          16.99             0.73              0.00   \n",
       "\n",
       "            circular_deps  \n",
       "model                      \n",
       "Baseline             0.00  \n",
       "Fine-tuned           0.73  \n",
       "Ground               0.24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_parser_results(data, model_name):\n",
    "    \"\"\"Evaluate DAG code using the DAGValidator\"\"\"\n",
    "    results = []\n",
    "    validator = DAGValidator()\n",
    "    \n",
    "    for entry in data:\n",
    "        code = entry.get('code', '')\n",
    "        \n",
    "        # Use DAGValidator.validate_content which returns List[ValidationError]\n",
    "        errors = validator.validate_content(code, source_name=\"<generated>\")\n",
    "        \n",
    "        # Extract error information\n",
    "        is_valid = len(errors) == 0\n",
    "        error_messages = [str(e) for e in errors]\n",
    "        error_types = [e.error_type for e in errors]\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'is_valid': is_valid,\n",
    "            'error_count': len(errors),\n",
    "            'errors': '; '.join(error_messages) if error_messages else '',\n",
    "            'has_syntax_error': any('SYNTAX_ERROR' in et or 'PARSE_ERROR' in et for et in error_types),\n",
    "            'has_duplicate_task': any('DUPLICATE_TASK_ID' in et for et in error_types),\n",
    "            'has_invalid_task_id': any('INVALID_TASK_ID' in et for et in error_types),\n",
    "            'has_circular_dependency': any('CIRCULAR_DEPENDENCY' in et for et in error_types)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_ground = evaluate_parser_results(ground_data, 'Ground')\n",
    "df_base = evaluate_parser_results(baseline_data, 'Baseline')\n",
    "df_fine = evaluate_parser_results(finetuned_data, 'Fine-tuned')\n",
    "df_parser = pd.concat([df_ground, df_base, df_fine], ignore_index=True)\n",
    "\n",
    "# Display summary statistics\n",
    "summary = df_parser.groupby('model').agg(\n",
    "    valid_rate=('is_valid', 'mean'),\n",
    "    syntax_errors=('has_syntax_error', 'mean'),\n",
    "    duplicate_tasks=('has_duplicate_task', 'mean'),\n",
    "    invalid_task_ids=('has_invalid_task_id', 'mean'),\n",
    "    circular_deps=('has_circular_dependency', 'mean')\n",
    ") * 100\n",
    "\n",
    "print(\"Parser Evaluation Results (%):\")\n",
    "display(summary.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. LLM-Based Evaluation with Claude\n\nTo complement the parser-based structural analysis, we employ Claude Sonnet 4.5 as an LLM judge to evaluate semantic quality across three critical dimensions:\n\n### Evaluation Metrics\n\n**1. Idiomatic Airflow Usage (0/1 Binary Score)**\n- **Pass (1):** Code uses provider-specific operators designed for the task (e.g., `SnowflakeOperator`, `S3Operator`)\n- **Fail (0):** Code wraps logic in generic `PythonOperator` with hooks instead of using native operators\n- **Rationale:** Idiomatic Airflow leverages the rich ecosystem of 100+ provider packages with purpose-built operators\n\n**2. No Hallucination/Leakage (0/1 Binary Score)**\n- **Pass (1):** Clean, production-ready code using only standard Airflow libraries\n- **Fail (0):** Code imports internal testing modules (e.g., `from tests_common.test_utils.system_tests`) or fabricates non-existent APIs\n- **Rationale:** Model hallucinations indicate training data contamination or knowledge gaps that would cause runtime failures\n\n**3. Instruction Adherence (0/1 Binary Score)**\n- **Pass (1):** Generated DAG fully implements the requested business logic\n- **Fail (0):** Missing key requirements from the user instruction\n- **Rationale:** Measures the model's ability to follow specifications accurately\n\n### Key Findings\n\nThe evaluation reveals a nuanced performance profile for the fine-tuned model:\n\n**Strengths:**\n- **Airflow Idioms:** Fine-tuned model significantly outperforms the baseline in using provider-specific operators (43% vs 11% pass rate)\n- **Syntax Knowledge:** Demonstrates strong understanding of modern Airflow 3.x API patterns\n\n**Weaknesses - Root Cause Analysis:**\n\n1. **Instruction Following (8% pass rate vs 15% baseline):** Training dataset exhibits poor diversity in numerical parameters‚Äî85.7% of examples use \"20\" as a dummy value across various contexts (e.g., \"insert 20 records\", \"20 retries\", \"20 seconds\"). The model overfitted to this pattern and fails to generalize to different numerical specifications.\n\n2. **Hallucination Rate (6% pass rate vs 24% baseline):** Two primary causes:\n   - **Data Leakage:** Training set included internal Airflow test files that import `tests_common.test_utils.system_tests` utilities. Model reproduces these non-production imports.\n   - **Incomplete Knowledge:** Limited training examples for niche libraries (e.g., `airflow.timetables`, `airflow.example_dags.plugins`) cause the model to fabricate API interactions rather than admit uncertainty.\n\nThe subsequent cells provide concrete examples demonstrating each failure mode with side-by-side comparisons between ground truth, baseline, and fine-tuned outputs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing LLM evaluation batch requests (including DAGs that failed parser validation)...\n",
      "Preparing evaluation batches...\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "‚úì Prepared 1026 evaluation requests (342 ground + 342 baseline + 342 fine-tuned)\n"
     ]
    }
   ],
   "source": [
    "EVAL_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert Senior Apache Airflow Architect. Evaluate the following Airflow DAG code generated by an AI model based on a user instruction.\n",
    "\n",
    "### Scoring Criteria & Examples\n",
    "\n",
    "**1. Idiomatic Airflow (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Uses specific Providers and Operators designed for the task.\n",
    "    * *Example:* `from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator`\n",
    "* **Score 0 (Fail):** Relies on generic \"Pythonic\" patterns where it wraps logic in a `PythonOperator` + Hook instead of using the native Operator.\n",
    "    * *Example:* `def load(): hook = SnowflakeHook(...) \\n PythonOperator(python_callable=load ...)`\n",
    "\n",
    "**2. No Hallucination/Leakage (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Code is clean, production-ready, and uses only standard Airflow libraries.\n",
    "* **Score 0 (Fail):** Code imports internal testing modules or includes test harness boilerplate.\n",
    "    * *Example:* `from tests_common.test_utils.system_tests import get_test_run`\n",
    "    * *Example:* `test_run = get_test_run(dag)`\n",
    "\n",
    "**3. Instruction Adherence (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Fulfills the specific business logic requested (e.g., \"load data AND validate\").\n",
    "* **Score 0 (Fail):** Misses a key step of the instruction.\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "DAG CODE:\n",
    "```python\n",
    "{dag_content}\n",
    "```\n",
    "\n",
    "\n",
    "Evaluate the code based on the criteria above. Return valid JSON only.\n",
    "\n",
    "{{{{\n",
    "  \"idiomatic_airflow\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
    "  \"no_hallucination\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}},\n",
    "  \"instruction_adherence\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "def prepare_llm_batch_requests(dags: List[Dict],\n",
    "                                model_name: str, \n",
    "                                prompt_template: str = EVAL_PROMPT_TEMPLATE) -> List[Dict]:\n",
    "    \"\"\"Prepare batch requests for Claude LLM evaluation.\n",
    "    \n",
    "    Includes all DAGs with DAG generation requests (even if they failed parser validation).\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    \n",
    "    print(f\"Preparing requests for all {len(dags)} DAGs...\")\n",
    "    \n",
    "    skipped_non_dag = 0\n",
    "    \n",
    "    for idx, dag_record in enumerate(dags):\n",
    "        \n",
    "        system_message = dag_record.get('system', '')\n",
    "        # Only include DAG generation requests (filter out other types)\n",
    "        if not system_message.startswith(\"You are an expert Apache Airflow developer\"):\n",
    "            skipped_non_dag += 1\n",
    "            continue\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = prompt_template.format(\n",
    "            dag_content=dag_record.get('code', ''), \n",
    "            instruction=dag_record.get('prompt', '')\n",
    "        )\n",
    "        \n",
    "        # Create batch request\n",
    "        request = {\n",
    "            \"custom_id\": f\"{model_name}_{idx}\",\n",
    "            \"params\": {\n",
    "                \"model\": \"claude-sonnet-4-5-20250929\",  # Claude Sonnet 4.5\n",
    "                \"max_tokens\": 2000,\n",
    "                \"temperature\": 0.0,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        batch_requests.append(request)\n",
    "    if skipped_non_dag > 0:\n",
    "        print(f\"Skipped {skipped_non_dag} non-DAG generation requests\")\n",
    "    \n",
    "    return batch_requests\n",
    "\n",
    "print(\"Preparing LLM evaluation batch requests (including DAGs that failed parser validation)...\")\n",
    "\n",
    "\n",
    "if ANTHROPIC_API_KEY:\n",
    "    processor = ClaudeBatchProcessor(api_key=ANTHROPIC_API_KEY)\n",
    "    \n",
    "    # Prepare batches for full datasets\n",
    "    print(f\"Preparing evaluation batches...\")\n",
    "    ground_batch_requests = prepare_llm_batch_requests(\n",
    "    baseline_data, \n",
    "    \"ground\"\n",
    "    )\n",
    "    baseline_batch_requests = prepare_llm_batch_requests(\n",
    "    baseline_data, \n",
    "    \"baseline\"\n",
    "    )\n",
    "    finetuned_batch_requests = prepare_llm_batch_requests(\n",
    "        finetuned_data,\n",
    "        \"finetuned\"\n",
    "    )\n",
    "    all_batch_requests = ground_batch_requests + baseline_batch_requests + finetuned_batch_requests\n",
    "    \n",
    "    print(f\"‚úì Prepared {len(all_batch_requests)} evaluation requests ({len(ground_batch_requests)} ground + {len(baseline_batch_requests)} baseline + {len(finetuned_batch_requests)} fine-tuned)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping LLM evaluation setup:\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        print(\"  - No ANTHROPIC_API_KEY found in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:44:41,775 - INFO - üöÄ Submitting batch with 824 requests...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Submitting batch request to Claude API...\n",
      "   This will evaluate 824 DAGs using Claude Sonnet 4\n",
      "   Estimated cost: ~$12.36 (at $15/1M input tokens)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:44:44,569 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages/batches?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:44:44,573 - INFO - ‚úÖ Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
      "2025-12-30 16:44:44,574 - INFO - ‚è≥ Waiting for batch msgbatch_01GuxFdkP6XrMjqjzxNwjCGb...\n",
      "2025-12-30 16:44:44,753 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:44:44,756 - INFO - üìä Status: in_progress (elapsed: 0.2s)\n",
      "2025-12-30 16:44:44,757 - INFO -    Progress: 0/824 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
      "\n",
      "‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:45:15,001 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:45:15,005 - INFO - üìä Status: in_progress (elapsed: 30.4s)\n",
      "2025-12-30 16:45:15,006 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:45:45,259 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:45:45,263 - INFO - üìä Status: in_progress (elapsed: 60.7s)\n",
      "2025-12-30 16:45:45,263 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:46:15,515 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:46:15,518 - INFO - üìä Status: in_progress (elapsed: 90.9s)\n",
      "2025-12-30 16:46:15,519 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:46:45,750 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:46:45,756 - INFO - üìä Status: in_progress (elapsed: 121.2s)\n",
      "2025-12-30 16:46:45,756 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:47:16,010 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:16,013 - INFO - üìä Status: in_progress (elapsed: 151.4s)\n",
      "2025-12-30 16:47:16,014 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:47:46,334 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:46,336 - INFO - üìä Status: ended (elapsed: 181.8s)\n",
      "2025-12-30 16:47:46,336 - INFO -    Progress: 824/824 (100.0%)\n",
      "2025-12-30 16:47:46,336 - INFO - ‚úÖ Batch completed with status: ended\n",
      "2025-12-30 16:47:46,337 - INFO - ‚¨áÔ∏è Downloading results...\n",
      "2025-12-30 16:47:46,530 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:47:46,823 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb/results \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:47,475 - INFO - üì• Downloaded 824 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded 824 results\n",
      "\n",
      "üìä Parsing evaluation scores...\n",
      "‚úì Parsed 489 evaluation scores\n",
      "‚ö†Ô∏è 335 results had errors or couldn't be parsed\n",
      "\n",
      "LLM Evaluation Summary (1-5 scale):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "      <th>completeness</th>\n",
       "      <th>best_practices</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>1.98</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tuned</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            correctness  completeness  best_practices\n",
       "model                                                \n",
       "Baseline           1.98          1.78            1.82\n",
       "Fine-tuned         1.78          1.57            1.76"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute Batch Processing and Parse Results\n",
    "print(\"üöÄ Submitting batch request to Claude API...\")\n",
    "print(f\"   This will evaluate {len(all_batch_requests)} DAGs using Claude Sonnet 4\")\n",
    "print(f\"   Estimated cost: ~${len(all_batch_requests) * 0.015:.2f} (at $15/1M input tokens)\")\n",
    "print()\n",
    "\n",
    "# Submit batch\n",
    "batch_id = processor.submit_batch(all_batch_requests)\n",
    "print(f\"‚úì Batch submitted: {batch_id}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:27,422 - INFO - ‚è≥ Waiting for batch msgbatch_01Mue6HSzr5mR9g3XR1aEST9...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:27,676 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-31 11:08:27,679 - INFO - üìä Status: ended (elapsed: 0.3s)\n",
      "2025-12-31 11:08:27,679 - INFO -    Progress: 1026/1026 (100.0%)\n",
      "2025-12-31 11:08:27,679 - INFO - ‚úÖ Batch completed with status: ended\n",
      "2025-12-31 11:08:27,680 - INFO - ‚¨áÔ∏è Downloading results...\n",
      "2025-12-31 11:08:27,876 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:28,160 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9/results \"HTTP/1.1 200 OK\"\n",
      "2025-12-31 11:08:28,794 - INFO - üì• Downloaded 1026 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded 1026 results\n",
      "\n",
      "üìä Parsing evaluation scores...\n",
      "‚úì Parsed 1025 evaluation scores\n",
      "‚ö†Ô∏è 1 results had errors or couldn't be parsed\n",
      "\n",
      "LLM Evaluation Summary (1-5 scale):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idiomatic_airflow_score</th>\n",
       "      <th>no_hallucination_score</th>\n",
       "      <th>instruction_adherence_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finetuned</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idiomatic_airflow_score  no_hallucination_score  \\\n",
       "model                                                        \n",
       "baseline                      0.11                    0.24   \n",
       "finetuned                     0.43                    0.06   \n",
       "ground                        0.91                    0.17   \n",
       "\n",
       "           instruction_adherence_score  \n",
       "model                                   \n",
       "baseline                          0.15  \n",
       "finetuned                         0.08  \n",
       "ground                            0.72  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Wait for completion (can take 10-30 minutes for large batches)\n",
    "print(\"‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\")\n",
    "batch = processor.wait_for_batch_completion(batch_id)\n",
    "print()\n",
    "\n",
    "# Download results\n",
    "print(\"üì• Downloading results...\")\n",
    "results = processor.download_batch_results(batch_id)\n",
    "print(f\"‚úì Downloaded {len(results)} results\")\n",
    "print()\n",
    "\n",
    "# Parse results into dataframe\n",
    "print(\"üìä Parsing evaluation scores...\")\n",
    "llm_eval_results = []\n",
    "parse_errors = 0\n",
    "\n",
    "for result in results:\n",
    "    if result['result']['type'] == 'succeeded':\n",
    "        # Parse model name and dag_id from custom_id\n",
    "        # Handle format: \"baseline_123\" or \"finetuned_123\"\n",
    "        custom_id = result['custom_id']\n",
    "        parts = custom_id.split('_')\n",
    "        model_name = parts[0]  # 'ground', 'baseline' or 'finetuned'\n",
    "        dag_id = int(parts[1])\n",
    "        text = result['result']['message']['content'][0]['text']\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                eval_data = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in response\")\n",
    "            llm_eval_results.append({\n",
    "                'model': model_name,\n",
    "                'dag_id': dag_id,\n",
    "                'idiomatic_airflow_score': eval_data['idiomatic_airflow']['score'],\n",
    "                'idiomatic_airflow_reasoning': eval_data['idiomatic_airflow']['reasoning'],\n",
    "                'no_hallucination_score': eval_data['no_hallucination']['score'],\n",
    "                'no_hallucination_reasoning': eval_data['no_hallucination']['reasoning'],\n",
    "                'instruction_adherence_score': eval_data['instruction_adherence']['score'],\n",
    "                'instruction_adherence_reasoning': eval_data['instruction_adherence']['reasoning'],\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            parse_errors += 1\n",
    "    elif result['result']['type'] == 'errored':\n",
    "        parse_errors += 1\n",
    "\n",
    "\n",
    "df_llm = pd.DataFrame(llm_eval_results)\n",
    "print(f\"‚úì Parsed {len(llm_eval_results)} evaluation scores\")\n",
    "if parse_errors > 0:\n",
    "    print(f\"‚ö†Ô∏è {parse_errors} results had errors or couldn't be parsed\")\n",
    "\n",
    "# Display summary statistics\n",
    "summary = df_llm.groupby('model').agg({\n",
    "    'idiomatic_airflow_score': 'mean',\n",
    "    'no_hallucination_score': 'mean',\n",
    "    'instruction_adherence_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nLLM Evaluation Summary (1-5 scale):\")\n",
    "display(summary)\n",
    "df_llm.to_csv('llm_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Success Case: Idiomatic Airflow Syntax\n\nThe fine-tuned model demonstrates superior knowledge of Airflow's provider ecosystem and operator patterns. This section showcases examples where the model correctly uses native operators instead of generic Python wrappers.\n\n**Example 1 (DAG ID 3): Campaign Manager Operations**\n- **Fine-tuned:** Correctly uses `GoogleCampaignManagerBatchInsertConversionsOperator`, `GoogleCampaignManagerRunReportOperator` from the Google Marketing Platform provider\n- **Baseline:** Falls back to `PythonOperator` wrapping `CampaignManagerHook` calls‚Äîa non-idiomatic pattern that requires more boilerplate code\n\n**Example 2 (DAG ID 19): Compute Engine Instance Management**\n- Illustrates a common failure mode where the fine-tuned model imports the correct operators (`ComputeEngineInsertInstanceOperator`) but still wraps logic in a `@task` decorated function with `ComputeEngineHook`\n- This represents partial knowledge‚Äîthe model knows the right operators exist but hasn't fully learned when to apply them directly"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>idiomatic_airflow_score</th>\n",
       "      <th>idiomatic_airflow_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ground</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses native Airflow operators specifically designed for Campaign Manager tasks. It imports and uses GoogleCampaignManagerBatchInsertConversionsOperator, GoogleCampaignManagerBatchUpdateConversionsOperator, GoogleCampaignManagerInsertReportOperator, GoogleCampaignManagerRunReportOperator, GoogleCampaignManagerDownloadReportOperator, GoogleCampaignManagerDeleteReportOperator, and GoogleCampaignManagerReportSensor from airflow.providers.google.marketing_platform. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>The code uses PythonOperator with hooks instead of native Campaign Manager operators. Airflow 3.0.0 provides google.marketing_platform.operators.campaign_manager that should be used for interacting with Campaign Manager APIs. The code wraps hook calls in Python functions rather than using dedicated operators like CampaignManagerInsertReportOperator, CampaignManagerRunReportOperator, or CampaignManagerDownloadReportOperator.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses native Airflow operators from the Google Marketing Platform provider (GoogleCampaignManagerBatchInsertOperator, GoogleCampaignManagerGetOperator, GoogleCampaignManagerUpdateOperator, GoogleCampaignManagerListOperator, GoogleCampaignManagerDeleteOperator) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic Airflow approach.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  idiomatic_airflow_score  \\\n",
       "2       ground                        1   \n",
       "344   baseline                        0   \n",
       "686  finetuned                        1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 idiomatic_airflow_reasoning  \n",
       "2    The code uses native Airflow operators specifically designed for Campaign Manager tasks. It imports and uses GoogleCampaignManagerBatchInsertConversionsOperator, GoogleCampaignManagerBatchUpdateConversionsOperator, GoogleCampaignManagerInsertReportOperator, GoogleCampaignManagerRunReportOperator, GoogleCampaignManagerDownloadReportOperator, GoogleCampaignManagerDeleteReportOperator, and GoogleCampaignManagerReportSensor from airflow.providers.google.marketing_platform. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.  \n",
       "344                                                                                                                                                              The code uses PythonOperator with hooks instead of native Campaign Manager operators. Airflow 3.0.0 provides google.marketing_platform.operators.campaign_manager that should be used for interacting with Campaign Manager APIs. The code wraps hook calls in Python functions rather than using dedicated operators like CampaignManagerInsertReportOperator, CampaignManagerRunReportOperator, or CampaignManagerDownloadReportOperator.  \n",
       "686                                                                                                                                                                                                                           The code uses native Airflow operators from the Google Marketing Platform provider (GoogleCampaignManagerBatchInsertOperator, GoogleCampaignManagerGetOperator, GoogleCampaignManagerUpdateOperator, GoogleCampaignManagerListOperator, GoogleCampaignManagerDeleteOperator) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic Airflow approach.  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm[df_llm.dag_id==3][['model','idiomatic_airflow_score','idiomatic_airflow_reasoning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>idiomatic_airflow_score</th>\n",
       "      <th>idiomatic_airflow_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ground</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses appropriate Airflow providers and operators for the tasks. It uses ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator for GCE instance management, SQLExecuteQueryOperator for database operations, and SQLToGoogleSheetsOperator for data transfer to Google Sheets. It also uses SSHOperator for remote commands and GoogleSheetsCreateSpreadsheetOperator for spreadsheet creation. These are all idiomatic Airflow patterns using native operators rather than wrapping logic in PythonOperators with hooks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>The code uses several non-existent or incorrect operators. 'PostgresCreateDatabaseOperator' and 'PostgresExecuteQueryOperator' do not exist in 'airflow.providers.google.cloud.operators.postgres' - PostgreSQL operators are in 'airflow.providers.postgres.operators.postgres' (e.g., PostgresOperator). 'GoogleSheetsUploadOperator' does not exist; the correct operator is 'GoogleSheetsCreateSpreadsheetOperator' or using 'GoogleSheetsHook' with appropriate operators. Additionally, GKE (Google Kubernetes Engine) cluster operators are being used instead of Compute Engine instance operators (ComputeEngineInsertInstanceOperator), which doesn't match the instruction to create a Compute Engine instance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>0</td>\n",
       "      <td>The DAG uses a PythonOperator (via @task decorator) with ComputeEngineHook to create a Compute Engine instance instead of using the native ComputeEngineInsertInstanceOperator or ComputeEngineInsertInstanceFromTemplateOperator that are already imported but never used. The create_instance task wraps hook logic in a Python function, which is the anti-pattern described in the scoring criteria. The proper idiomatic approach would be to use ComputeEngineInsertInstanceFromTemplateOperator directly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  idiomatic_airflow_score  \\\n",
       "11      ground                        1   \n",
       "353   baseline                        0   \n",
       "695  finetuned                        0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    idiomatic_airflow_reasoning  \n",
       "11                                                                                                                                                                      The code uses appropriate Airflow providers and operators for the tasks. It uses ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator for GCE instance management, SQLExecuteQueryOperator for database operations, and SQLToGoogleSheetsOperator for data transfer to Google Sheets. It also uses SSHOperator for remote commands and GoogleSheetsCreateSpreadsheetOperator for spreadsheet creation. These are all idiomatic Airflow patterns using native operators rather than wrapping logic in PythonOperators with hooks.  \n",
       "353  The code uses several non-existent or incorrect operators. 'PostgresCreateDatabaseOperator' and 'PostgresExecuteQueryOperator' do not exist in 'airflow.providers.google.cloud.operators.postgres' - PostgreSQL operators are in 'airflow.providers.postgres.operators.postgres' (e.g., PostgresOperator). 'GoogleSheetsUploadOperator' does not exist; the correct operator is 'GoogleSheetsCreateSpreadsheetOperator' or using 'GoogleSheetsHook' with appropriate operators. Additionally, GKE (Google Kubernetes Engine) cluster operators are being used instead of Compute Engine instance operators (ComputeEngineInsertInstanceOperator), which doesn't match the instruction to create a Compute Engine instance.  \n",
       "695                                                                                                                                                                                                            The DAG uses a PythonOperator (via @task decorator) with ComputeEngineHook to create a Compute Engine instance instead of using the native ComputeEngineInsertInstanceOperator or ComputeEngineInsertInstanceFromTemplateOperator that are already imported but never used. The create_instance task wraps hook logic in a Python function, which is the anti-pattern described in the scoring criteria. The proper idiomatic approach would be to use ComputeEngineInsertInstanceFromTemplateOperator directly.  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm[df_llm.dag_id==19][['model','idiomatic_airflow_score','idiomatic_airflow_reasoning']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Failure Case 1: Dataset Overfitting and Poor Generalization\n\n**Problem:** The fine-tuned model exhibits severe overfitting to numerical parameters in the training data, causing instruction adherence failures.\n\n**Root Cause Analysis (DAG ID 0):**\n- **User Request:** \"Insert **12** product records and validate the data load\"\n- **Fine-tuned Output:** Inserts **20** records instead, completely ignoring the specified quantity\n- **Training Data Pattern:** 85.7% of training examples (353 out of 412) contain the number \"20\" as a placeholder value across diverse contexts (row counts, retry limits, timeout seconds, etc.)\n\n**Impact:**\n- The model memorized \"20\" as the default numerical value rather than learning to extract task-specific parameters from instructions\n- This catastrophic generalization failure results in only an 8% instruction adherence pass rate vs. 15% for the baseline\n\n**Mitigation Strategies:**\n1. **Data Augmentation:** Systematically vary numerical parameters during training to prevent anchoring bias\n2. **Synthetic Data Generation:** Create examples with diverse numerical ranges (1-10, 50-100, 1000+)\n3. **Few-Shot Prompting:** Include examples with different numbers in the system prompt at inference time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>instruction_adherence_score</th>\n",
       "      <th>instruction_adherence_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ground</td>\n",
       "      <td>1</td>\n",
       "      <td>The code fully adheres to the instruction. It creates a data pipeline that loads 12 product records into a Snowflake table (setup_data task) and validates the data load by verifying the row count matches the expected count of 12 (check_num_rows task). Both required steps are implemented correctly with proper task dependencies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>The code fulfills the user instruction completely. It creates a data pipeline that: (1) loads 12 sample product records into a Snowflake table via the load_sample_products function, and (2) validates the data load by querying the row count and comparing it to the expected count of 12 in the validate_data_load function. The tasks are properly sequenced with load_task &gt;&gt; validate_task dependency.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>0</td>\n",
       "      <td>The instruction requires two key components: (1) insert 12 product records, and (2) validate the data load by verifying the row count. The code inserts 20 records instead of 12, and completely lacks any validation task to verify the total number of rows matches the expected count. There is no SELECT COUNT(*) query or any mechanism to check that the data was loaded correctly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  instruction_adherence_score  \\\n",
       "0       ground                            1   \n",
       "342   baseline                            1   \n",
       "684  finetuned                            0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                   instruction_adherence_reasoning  \n",
       "0                                                                         The code fully adheres to the instruction. It creates a data pipeline that loads 12 product records into a Snowflake table (setup_data task) and validates the data load by verifying the row count matches the expected count of 12 (check_num_rows task). Both required steps are implemented correctly with proper task dependencies.  \n",
       "342  The code fulfills the user instruction completely. It creates a data pipeline that: (1) loads 12 sample product records into a Snowflake table via the load_sample_products function, and (2) validates the data load by querying the row count and comparing it to the expected count of 12 in the validate_data_load function. The tasks are properly sequenced with load_task >> validate_task dependency.  \n",
       "684                      The instruction requires two key components: (1) insert 12 product records, and (2) validate the data load by verifying the row count. The code inserts 20 records instead of 12, and completely lacks any validation task to verify the total number of rows matches the expected count. There is no SELECT COUNT(*) query or any mechanism to check that the data was loaded correctly.  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_llm[df_llm.dag_id==0][['model','instruction_adherence_score','instruction_adherence_reasoning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d98a236d6ec4d5684ce645c7a1371ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efb63989e604e6488f1d8deec55161e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eb40b32de548979503336897661950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records containing '20' in prompt: 353 out of 412 (85.7%)\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset(\n",
    "    \"andrea-t94/airflow-dag-dataset\",\n",
    "    split=\"test\",\n",
    "    download_mode=\"reuse_cache_if_exists\"  # Use cached version if available\n",
    ")\n",
    "# Check how many records contain \"20\" in the prompt\n",
    "count = 0\n",
    "for entry in train_ds:\n",
    "    messages = entry.get('messages', [])\n",
    "    for msg in messages:\n",
    "        if \"20\" in msg['content']:\n",
    "            count += 1\n",
    "\n",
    "print(f\"Records containing '20' in prompt: {count} out of {len(ground_data)} ({count/len(ground_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Failure Case 2: Training Data Contamination with Internal Test Utilities\n\n**Problem:** The fine-tuned model reproduces internal Airflow testing code that should never appear in production DAGs, resulting in a 6% hallucination-free pass rate (vs. 24% baseline).\n\n**Root Cause Analysis (DAG ID 0):**\n- **Contaminated Imports:** Both ground truth and fine-tuned model generate:\n  ```python\n  from tests_common.test_utils.system_tests import get_test_run\n  test_run = get_test_run(dag)\n  ```\n- **Source:** Training dataset inadvertently included internal Airflow CI/CD test files used for system testing\n- **Impact:** Claude evaluator correctly flags these as hallucinations since they reference non-public APIs unavailable in production Airflow installations\n\n**Why This Matters:**\n- Code would fail at import time: `ModuleNotFoundError: No module named 'tests_common'`\n- Indicates the model cannot distinguish between test scaffolding and production code patterns\n- Baseline model, lacking exposure to these internals, generates cleaner production-ready code\n\n**Mitigation Strategies:**\n1. **Data Filtering:** Implement strict preprocessing to exclude all files under `tests/`, `tests_common/`, or with `test_` prefixes\n2. **Validation Pipeline:** Add automated checks to flag imports from non-production namespaces before adding examples to training set\n3. **Synthetic Cleaning:** Post-process training examples to remove test harness boilerplate while preserving core DAG logic"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>no_hallucination_score</th>\n",
       "      <th>no_hallucination_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ground</td>\n",
       "      <td>0</td>\n",
       "      <td>The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses only standard Airflow libraries (airflow.operators.python_operator.PythonOperator and airflow.providers.snowflake.hooks.snowflake.SnowflakeHook). There are no imports from internal testing modules or test harness boilerplate. The code is clean and does not contain hallucinated or leaked test utilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>0</td>\n",
       "      <td>The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing modules. Additionally, the line 'snowflake_insert_many &gt;&gt; TriggerRule.ALL_DONE' is incorrect syntax and appears to be malformed code.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  no_hallucination_score  \\\n",
       "0       ground                       0   \n",
       "342   baseline                       1   \n",
       "684  finetuned                       0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                   no_hallucination_reasoning  \n",
       "0                                                                                                                                          The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.  \n",
       "342                                                             The code uses only standard Airflow libraries (airflow.operators.python_operator.PythonOperator and airflow.providers.snowflake.hooks.snowflake.SnowflakeHook). There are no imports from internal testing modules or test harness boilerplate. The code is clean and does not contain hallucinated or leaked test utilities.  \n",
       "684  The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing modules. Additionally, the line 'snowflake_insert_many >> TriggerRule.ALL_DONE' is incorrect syntax and appears to be malformed code.  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_llm[df_llm.dag_id==0][['model','no_hallucination_score','no_hallucination_reasoning']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Failure Case 3: Hallucinations from Insufficient Coverage of Niche APIs\n\n**Problem:** When encountering specialized Airflow features underrepresented in training data, the model fabricates plausible-looking but incorrect API usage patterns.\n\n**Root Cause Analysis (DAG ID 2 - Custom Timetables):**\n\n**Baseline Model Hallucinations:**\n- Invents non-existent import: `from airflow.timetables.after_workday import AfterWorkdayTimetable`\n- Fabricates invalid API: `task.set_upstream(None, AfterWorkdayTimetable(...))`‚Äîtimetables cannot be passed to `set_upstream()`\n- **Confidence despite incorrectness:** Syntax appears legitimate, making errors harder to detect without execution\n\n**Fine-tuned Model Hallucinations:**\n- Creates invalid DAG parameter: `timetables={\"after_workday\": AfterWorkdayTimetable(...)}`\n- Correct approach: Pass timetable directly to `schedule` parameter in Airflow 3.x\n- **Shows partial knowledge:** Imports the correct class but misapplies it\n\n**Ground Truth Issues:**\n- Even correct implementation imports from `airflow.example_dags.plugins.workday`‚Äîan internal example path unsuitable for production\n- Highlights data quality problems even in \"ground truth\" examples\n\n**Why Custom Timetables Are Challenging:**\n- **Low Frequency:** Timetables are advanced features used in <5% of production DAGs\n- **API Evolution:** Timetable interface changed significantly between Airflow 2.x and 3.x\n- **Documentation Gaps:** Fewer Stack Overflow examples compared to common operators\n\n**Mitigation Strategies:**\n1. **Targeted Data Collection:** Manually curate 50-100 high-quality examples for each underrepresented feature (timetables, custom operators, dynamic task mapping)\n2. **Retrieval-Augmented Generation (RAG):** Supplement model with runtime access to official Airflow documentation for rare APIs\n3. **Confidence Calibration:** Train model to output uncertainty markers (e.g., `# TODO: Verify timetable syntax`) when dealing with low-confidence API usage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dag_id</th>\n",
       "      <th>idiomatic_airflow_score</th>\n",
       "      <th>idiomatic_airflow_reasoning</th>\n",
       "      <th>no_hallucination_score</th>\n",
       "      <th>no_hallucination_reasoning</th>\n",
       "      <th>instruction_adherence_score</th>\n",
       "      <th>instruction_adherence_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ground</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses native Airflow constructs appropriately. It uses the EmptyOperator from the standard providers package, implements a custom timetable class (AfterWorkdayTimetable) as the schedule parameter, and uses the DAG context manager. This is the idiomatic way to implement custom scheduling in Airflow 3.0.0, where timetables are passed directly to the schedule parameter rather than wrapping logic in PythonOperators.</td>\n",
       "      <td>0</td>\n",
       "      <td>The code imports 'AfterWorkdayTimetable' from 'airflow.example_dags.plugins.workday', which is an internal example/testing path. This is not a standard production-ready import from official Airflow providers or core libraries. In a production environment, this custom timetable class should either be defined in the DAG file itself or imported from a proper custom plugins directory, not from 'airflow.example_dags.plugins'.</td>\n",
       "      <td>1</td>\n",
       "      <td>The code fully adheres to the instruction. It constructs an Airflow DAG that showcases a custom timetable scheduling mechanism using the AfterWorkdayTimetable class, builds a simple workflow with an EmptyOperator, and demonstrates scheduling flexibility by triggering after standard work hours. All requested elements are present: custom timetable implementation, simple workflow, and illustration of scheduling flexibility.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>The code attempts to use a custom timetable but does so incorrectly. The proper idiomatic way to use a custom timetable in Airflow is to pass it to the DAG constructor via the 'timetable' parameter, not by calling a non-existent 'set_upstream' method with timetable parameters on a task. The correct pattern would be: DAG(..., timetable=AfterWorkdayTimetable(...)). Additionally, uses deprecated 'EmptyOperator' import path instead of 'airflow.operators.empty.EmptyOperator' for Airflow 3.0.0.</td>\n",
       "      <td>0</td>\n",
       "      <td>The code contains multiple hallucinations: (1) 'airflow.timetables.after_workday.AfterWorkdayTimetable' does not exist in standard Airflow - this is a fabricated import path and class; (2) The method 'task.set_upstream(None, AfterWorkdayTimetable(...))' is not a valid Airflow API - set_upstream does not accept timetable parameters; (3) Missing import for 'datetime' class while using it; (4) The timetable constructor parameters shown (start_time, end_time with datetime objects) don't match any real Airflow timetable pattern.</td>\n",
       "      <td>0</td>\n",
       "      <td>The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable and includes an EmptyOperator, it fails to properly implement the custom timetable mechanism. The timetable should be passed to the DAG constructor, not applied to individual tasks. The code also uses 'schedule_interval=timedelta(days=1)' which would override any timetable setting, showing fundamental misunderstanding of how custom timetables work in Airflow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The code uses the appropriate Airflow components idiomatically. It imports and uses the AfterWorkdayTimetable class directly, uses the EmptyOperator from the standard providers package, and follows proper DAG construction patterns. No generic PythonOperator wrappers are used where native operators exist.</td>\n",
       "      <td>0</td>\n",
       "      <td>The code contains a significant error that suggests hallucination or confusion about Airflow 3.0.0 API. The 'timetables' parameter as a dictionary is not a valid DAG parameter in Airflow. The correct approach is to pass the timetable instance directly to the 'schedule' parameter (e.g., schedule=AfterWorkdayTimetable(...)). The code shows 'schedule=\"@daily\"' alongside a 'timetables' dict, which is not standard Airflow API and appears to be fabricated syntax.</td>\n",
       "      <td>0</td>\n",
       "      <td>The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable, it does so incorrectly (wrong syntax with the timetables dict). More critically, it uses 'schedule=\"@daily\"' which would override any custom timetable behavior. The code does not properly implement the custom timetable as the primary scheduling mechanism, failing to demonstrate the requested scheduling flexibility.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  dag_id  idiomatic_airflow_score  \\\n",
       "1       ground       2                        1   \n",
       "343   baseline       2                        0   \n",
       "685  finetuned       2                        1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       idiomatic_airflow_reasoning  \\\n",
       "1                                                                          The code uses native Airflow constructs appropriately. It uses the EmptyOperator from the standard providers package, implements a custom timetable class (AfterWorkdayTimetable) as the schedule parameter, and uses the DAG context manager. This is the idiomatic way to implement custom scheduling in Airflow 3.0.0, where timetables are passed directly to the schedule parameter rather than wrapping logic in PythonOperators.   \n",
       "343  The code attempts to use a custom timetable but does so incorrectly. The proper idiomatic way to use a custom timetable in Airflow is to pass it to the DAG constructor via the 'timetable' parameter, not by calling a non-existent 'set_upstream' method with timetable parameters on a task. The correct pattern would be: DAG(..., timetable=AfterWorkdayTimetable(...)). Additionally, uses deprecated 'EmptyOperator' import path instead of 'airflow.operators.empty.EmptyOperator' for Airflow 3.0.0.   \n",
       "685                                                                                                                                                                                              The code uses the appropriate Airflow components idiomatically. It imports and uses the AfterWorkdayTimetable class directly, uses the EmptyOperator from the standard providers package, and follows proper DAG construction patterns. No generic PythonOperator wrappers are used where native operators exist.   \n",
       "\n",
       "     no_hallucination_score  \\\n",
       "1                         0   \n",
       "343                       0   \n",
       "685                       0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            no_hallucination_reasoning  \\\n",
       "1                                                                                                             The code imports 'AfterWorkdayTimetable' from 'airflow.example_dags.plugins.workday', which is an internal example/testing path. This is not a standard production-ready import from official Airflow providers or core libraries. In a production environment, this custom timetable class should either be defined in the DAG file itself or imported from a proper custom plugins directory, not from 'airflow.example_dags.plugins'.   \n",
       "343  The code contains multiple hallucinations: (1) 'airflow.timetables.after_workday.AfterWorkdayTimetable' does not exist in standard Airflow - this is a fabricated import path and class; (2) The method 'task.set_upstream(None, AfterWorkdayTimetable(...))' is not a valid Airflow API - set_upstream does not accept timetable parameters; (3) Missing import for 'datetime' class while using it; (4) The timetable constructor parameters shown (start_time, end_time with datetime objects) don't match any real Airflow timetable pattern.   \n",
       "685                                                                      The code contains a significant error that suggests hallucination or confusion about Airflow 3.0.0 API. The 'timetables' parameter as a dictionary is not a valid DAG parameter in Airflow. The correct approach is to pass the timetable instance directly to the 'schedule' parameter (e.g., schedule=AfterWorkdayTimetable(...)). The code shows 'schedule=\"@daily\"' alongside a 'timetables' dict, which is not standard Airflow API and appears to be fabricated syntax.   \n",
       "\n",
       "     instruction_adherence_score  \\\n",
       "1                              1   \n",
       "343                            0   \n",
       "685                            0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       instruction_adherence_reasoning  \n",
       "1                                                                                                                                                                                             The code fully adheres to the instruction. It constructs an Airflow DAG that showcases a custom timetable scheduling mechanism using the AfterWorkdayTimetable class, builds a simple workflow with an EmptyOperator, and demonstrates scheduling flexibility by triggering after standard work hours. All requested elements are present: custom timetable implementation, simple workflow, and illustration of scheduling flexibility.  \n",
       "343  The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable and includes an EmptyOperator, it fails to properly implement the custom timetable mechanism. The timetable should be passed to the DAG constructor, not applied to individual tasks. The code also uses 'schedule_interval=timedelta(days=1)' which would override any timetable setting, showing fundamental misunderstanding of how custom timetables work in Airflow.  \n",
       "685                                              The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable, it does so incorrectly (wrong syntax with the timetables dict). More critically, it uses 'schedule=\"@daily\"' which would override any custom timetable behavior. The code does not properly implement the custom timetable as the primary scheduling mechanism, failing to demonstrate the requested scheduling flexibility.  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_llm[df_llm.dag_id==2]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (airflowNet)",
   "language": "python",
   "name": "airflownet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}