{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow DAG Generation: Evaluation\n",
    "\n",
    "This notebook evaluates the quality of Airflow DAGs generated by different models, comparing a baseline model (Qwen 2.5 1.5B Instruct) against a fine-tuned version.\n",
    "\n",
    "## üìã Setup Note\n",
    "\n",
    "**This notebook is designed to run locally** (not in Colab). To set it up:\n",
    "\n",
    "```bash\n",
    "# From project root\n",
    "pip install -e \".[research]\"\n",
    "pip install jupyter\n",
    "\n",
    "# Launch Jupyter and select the venv kernel\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Make sure you're using the Python kernel from your virtual environment to ensure all imports work correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Summary (Fine-tuned vs Base)\n",
    "\n",
    "Our evaluation reveals significant improvements across multiple dimensions:\n",
    "\n",
    "### Key Improvements\n",
    "- **Syntax Validity**: ~8% reduction in syntactically invalid DAGs\n",
    "- **Modern Patterns**: Strong adoption of latest Airflow syntax and operators (TaskFlow API, modern decorators)\n",
    "- **Reduced Hallucinations**: Significantly fewer instances of invented imports or non-existent operators\n",
    "- **Error Distribution Alignment**: The fine-tuned model's error patterns closely match real-world DAG distributions\n",
    "\n",
    "### Notable Observations\n",
    "- **Base Model**: Often generates deprecated patterns (e.g., legacy operators from Airflow 1.x)\n",
    "- **Fine-tuned Model**: Occasionally hallucinates internal testing libraries seen in training data, but far less than general hallucinations in the base model\n",
    "- **Syntax Accuracy**: Fine-tuned model shows consistent adherence to Python and Airflow syntax rules\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Evaluation Methodology\n",
    "\n",
    "We employ a two-pronged evaluation approach to assess both structural correctness and semantic quality:\n",
    "\n",
    "### 1. Parser-Based Evaluation (Structural Analysis)\n",
    "\n",
    "Uses a custom AST-based validator (`DAGValidator`) to check:\n",
    "- **Syntax Correctness**: Valid Python syntax, no parse errors\n",
    "- **Task ID Validation**: Unique task IDs, proper naming conventions (alphanumeric, dashes, dots, underscores)\n",
    "- **Dependency Analysis**: Detection of circular dependencies in task graphs\n",
    "- **DAG Structure**: Proper DAG instantiation, task definitions, and relationships\n",
    "\n",
    "**Advantages**: Fast, deterministic, catches critical structural errors that would prevent DAG execution.\n",
    "\n",
    "### 2. LLM-Based Evaluation (Semantic Analysis)\n",
    "\n",
    "Uses Claude 4.5 Sonnet via the Batch API to evaluate:\n",
    "- **Correctness** (1-5): Does the generated DAG logically implement the user's request?\n",
    "- **Completeness** (1-5): Are all necessary imports, arguments, and task dependencies present?\n",
    "- **Best Practices** (1-5): Does it follow Airflow conventions and modern patterns?\n",
    "\n",
    "**Advantages**: Captures semantic quality, intent alignment, and code quality aspects that structural analysis misses.\n",
    "\n",
    "**Note**: LLM evaluation requires an Anthropic API key and incurs costs. Results are saved for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "# hf\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import from installed packages\n",
    "from research.lib.batch_processor import ClaudeBatchProcessor\n",
    "from airflow_net.validation import DAGValidator\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Configuration\n",
    "import os\n",
    "\n",
    "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"WARNING: No ANTHROPIC_API_KEY found in environment. LLM evaluation steps will be skipped.\")\n",
    "    print(\"To enable LLM evaluation, set the ANTHROPIC_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "We load the generated DAGs from the JSONL artifacts produced by the inference step and the test dataset containing the ground truth DAG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for artifacts in: /Users/andreatamburri/Desktop/airflowNet/research/artifacts/finetuning/01_inference_results\n",
      "Selected Baseline: base_model_outputs_20251217_151724.jsonl\n",
      "Selected Fine-tuned: finetuned_model_outputs_20251217_151724.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Define paths to artifacts (relative to notebook location)\n",
    "ARTIFACTS_DIR = Path(\"../../artifacts/finetuning/01_inference_results\").resolve()\n",
    "\n",
    "print(f\"Looking for artifacts in: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# Find latest inference files\n",
    "base_files = list(ARTIFACTS_DIR.glob(\"base_model_outputs*.jsonl\"))\n",
    "finetuned_files = list(ARTIFACTS_DIR.glob(\"finetuned_model_outputs*.jsonl\"))\n",
    "\n",
    "if not base_files or not finetuned_files:\n",
    "    print(\"WARNING: Could not find one or both inference result files.\")\n",
    "    print(f\"Available files: {list(ARTIFACTS_DIR.glob('*.jsonl'))}\")\n",
    "    BASE_MODEL_FILE = None\n",
    "    FINETUNED_MODEL_FILE = None\n",
    "else:\n",
    "    # Take the most recent one\n",
    "    BASE_MODEL_FILE = sorted(base_files)[-1]\n",
    "    FINETUNED_MODEL_FILE = sorted(finetuned_files)[-1]\n",
    "    print(f\"Selected Baseline: {BASE_MODEL_FILE.name}\")\n",
    "    print(f\"Selected Fine-tuned: {FINETUNED_MODEL_FILE.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 412 baseline samples.\n",
      "Loaded 412 fine-tuned samples.\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file and extract code from messages format\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return []\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                entry = json.loads(line)\n",
    "                # Extract assistant content from messages\n",
    "                messages = entry.get('messages', [])\n",
    "                user_content = ''\n",
    "                assistant_content = ''\n",
    "                \n",
    "                for msg in messages:\n",
    "                    if msg['role'] == 'user':\n",
    "                        user_content = msg['content']\n",
    "                    elif msg['role'] == 'assistant':\n",
    "                        assistant_content = msg['content']\n",
    "                    if msg['role'] == 'system':\n",
    "                        system_content = msg['content']\n",
    "                \n",
    "                data.append({\n",
    "                    'system': system_content,\n",
    "                    'prompt': user_content,\n",
    "                    'code': assistant_content,\n",
    "                    'metadata': entry.get('metadata', {})\n",
    "                })\n",
    "    return data\n",
    "\n",
    "baseline_data = load_jsonl(BASE_MODEL_FILE)\n",
    "finetuned_data = load_jsonl(FINETUNED_MODEL_FILE)\n",
    "\n",
    "print(f\"Loaded {len(baseline_data)} baseline samples.\")\n",
    "print(f\"Loaded {len(finetuned_data)} fine-tuned samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cc714574564390b4da561ec715a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243d63873aeb4be1aa555624fc253d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bedc8ac36c4dcbb701ff2c5a0eea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 412 ground truth samples.\n"
     ]
    }
   ],
   "source": [
    "def parse_hf_ds(dataset):\n",
    "    \"\"\"Convert HuggingFace dataset to the same structure as load_jsonl function\"\"\"\n",
    "    data = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        messages = entry.get('messages', [])\n",
    "        system_content = ''\n",
    "        user_content = ''\n",
    "        assistant_content = ''\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'system':\n",
    "                system_content = msg['content']\n",
    "            elif msg['role'] == 'user':\n",
    "                user_content = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                assistant_content = msg['content']\n",
    "\n",
    "        data.append({\n",
    "            'system': system_content,\n",
    "            'prompt': user_content,\n",
    "            'code': assistant_content,\n",
    "            'metadata': entry.get('metadata', {})\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"andrea-t94/airflow-dag-dataset\",\n",
    "    split=\"test\",\n",
    "    download_mode=\"reuse_cache_if_exists\"  # Use cached version if available\n",
    ")\n",
    "\n",
    "ground_data = parse_hf_ds(dataset)\n",
    "print(f\"Loaded {len(ground_data)} ground truth samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser Evaluation Results (%):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_rate</th>\n",
       "      <th>syntax_errors</th>\n",
       "      <th>duplicate_tasks</th>\n",
       "      <th>invalid_task_ids</th>\n",
       "      <th>circular_deps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>95.87</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tuned</th>\n",
       "      <td>71.12</td>\n",
       "      <td>27.91</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ground</th>\n",
       "      <td>79.61</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            valid_rate  syntax_errors  duplicate_tasks  invalid_task_ids  \\\n",
       "model                                                                      \n",
       "Baseline         95.87           3.16             0.00              0.97   \n",
       "Fine-tuned       71.12          27.91             0.49              0.00   \n",
       "Ground           79.61          16.99             0.73              0.00   \n",
       "\n",
       "            circular_deps  \n",
       "model                      \n",
       "Baseline             0.00  \n",
       "Fine-tuned           0.73  \n",
       "Ground               0.24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_parser_results(data, model_name):\n",
    "    \"\"\"Evaluate DAG code using the DAGValidator\"\"\"\n",
    "    results = []\n",
    "    validator = DAGValidator()\n",
    "    \n",
    "    for entry in data:\n",
    "        code = entry.get('code', '')\n",
    "        \n",
    "        # Use DAGValidator.validate_content which returns List[ValidationError]\n",
    "        errors = validator.validate_content(code, source_name=\"<generated>\")\n",
    "        \n",
    "        # Extract error information\n",
    "        is_valid = len(errors) == 0\n",
    "        error_messages = [str(e) for e in errors]\n",
    "        error_types = [e.error_type for e in errors]\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'is_valid': is_valid,\n",
    "            'error_count': len(errors),\n",
    "            'errors': '; '.join(error_messages) if error_messages else '',\n",
    "            'has_syntax_error': any('SYNTAX_ERROR' in et or 'PARSE_ERROR' in et for et in error_types),\n",
    "            'has_duplicate_task': any('DUPLICATE_TASK_ID' in et for et in error_types),\n",
    "            'has_invalid_task_id': any('INVALID_TASK_ID' in et for et in error_types),\n",
    "            'has_circular_dependency': any('CIRCULAR_DEPENDENCY' in et for et in error_types)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_ground = evaluate_parser_results(ground_data, 'Ground')\n",
    "df_base = evaluate_parser_results(baseline_data, 'Baseline')\n",
    "df_fine = evaluate_parser_results(finetuned_data, 'Fine-tuned')\n",
    "df_parser = pd.concat([df_ground, df_base, df_fine], ignore_index=True)\n",
    "\n",
    "# Display summary statistics\n",
    "summary = df_parser.groupby('model').agg(\n",
    "    valid_rate=('is_valid', 'mean'),\n",
    "    syntax_errors=('has_syntax_error', 'mean'),\n",
    "    duplicate_tasks=('has_duplicate_task', 'mean'),\n",
    "    invalid_task_ids=('has_invalid_task_id', 'mean'),\n",
    "    circular_deps=('has_circular_dependency', 'mean')\n",
    ") * 100\n",
    "\n",
    "print(\"Parser Evaluation Results (%):\")\n",
    "display(summary.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-Based Evaluation with Claude\n",
    "Using Claude 4.5 Sonnet to score DAGs on Correctness, Completeness, and Best Practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing LLM evaluation batch requests (including DAGs that failed parser validation)...\n",
      "Preparing evaluation batches...\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "Preparing requests for all 412 DAGs...\n",
      "Skipped 70 non-DAG generation requests\n",
      "‚úì Prepared 1026 evaluation requests (342 ground + 342 baseline + 342 fine-tuned)\n"
     ]
    }
   ],
   "source": [
    "EVAL_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert Senior Apache Airflow Architect. Evaluate the following Airflow DAG code generated by an AI model based on a user instruction.\n",
    "\n",
    "### Scoring Criteria & Examples\n",
    "\n",
    "**1. Idiomatic Airflow (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Uses specific Providers and Operators designed for the task.\n",
    "    * *Example:* `from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator`\n",
    "* **Score 0 (Fail):** Relies on generic \"Pythonic\" patterns where it wraps logic in a `PythonOperator` + Hook instead of using the native Operator.\n",
    "    * *Example:* `def load(): hook = SnowflakeHook(...) \\n PythonOperator(python_callable=load ...)`\n",
    "\n",
    "**2. No Hallucination/Leakage (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Code is clean, production-ready, and uses only standard Airflow libraries.\n",
    "* **Score 0 (Fail):** Code imports internal testing modules or includes test harness boilerplate.\n",
    "    * *Example:* `from tests_common.test_utils.system_tests import get_test_run`\n",
    "    * *Example:* `test_run = get_test_run(dag)`\n",
    "\n",
    "**3. Instruction Adherence (Score 0 or 1)**\n",
    "* **Score 1 (Pass):** Fulfills the specific business logic requested (e.g., \"load data AND validate\").\n",
    "* **Score 0 (Fail):** Misses a key step of the instruction.\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "DAG CODE:\n",
    "```python\n",
    "{dag_content}\n",
    "```\n",
    "\n",
    "\n",
    "Evaluate the code based on the criteria above. Return valid JSON only.\n",
    "\n",
    "{{{{\n",
    "  \"idiomatic_airflow\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
    "  \"no_hallucination\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}},\n",
    "  \"instruction_adherence\": {{{{ \"score\": 0, \"reasoning\": \"...\" }}}}\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "def prepare_llm_batch_requests(dags: List[Dict],\n",
    "                                model_name: str, \n",
    "                                prompt_template: str = EVAL_PROMPT_TEMPLATE) -> List[Dict]:\n",
    "    \"\"\"Prepare batch requests for Claude LLM evaluation.\n",
    "    \n",
    "    Includes all DAGs with DAG generation requests (even if they failed parser validation).\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    \n",
    "    print(f\"Preparing requests for all {len(dags)} DAGs...\")\n",
    "    \n",
    "    skipped_non_dag = 0\n",
    "    \n",
    "    for idx, dag_record in enumerate(dags):\n",
    "        \n",
    "        system_message = dag_record.get('system', '')\n",
    "        # Only include DAG generation requests (filter out other types)\n",
    "        if not system_message.startswith(\"You are an expert Apache Airflow developer\"):\n",
    "            skipped_non_dag += 1\n",
    "            continue\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = prompt_template.format(\n",
    "            dag_content=dag_record.get('code', ''), \n",
    "            instruction=dag_record.get('prompt', '')\n",
    "        )\n",
    "        \n",
    "        # Create batch request\n",
    "        request = {\n",
    "            \"custom_id\": f\"{model_name}_{idx}\",\n",
    "            \"params\": {\n",
    "                \"model\": \"claude-sonnet-4-5-20250929\",  # Claude Sonnet 4.5\n",
    "                \"max_tokens\": 2000,\n",
    "                \"temperature\": 0.0,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        batch_requests.append(request)\n",
    "    if skipped_non_dag > 0:\n",
    "        print(f\"Skipped {skipped_non_dag} non-DAG generation requests\")\n",
    "    \n",
    "    return batch_requests\n",
    "\n",
    "print(\"Preparing LLM evaluation batch requests (including DAGs that failed parser validation)...\")\n",
    "\n",
    "\n",
    "if ANTHROPIC_API_KEY:\n",
    "    processor = ClaudeBatchProcessor(api_key=ANTHROPIC_API_KEY)\n",
    "    \n",
    "    # Prepare batches for full datasets\n",
    "    print(f\"Preparing evaluation batches...\")\n",
    "    ground_batch_requests = prepare_llm_batch_requests(\n",
    "    baseline_data, \n",
    "    \"ground\"\n",
    "    )\n",
    "    baseline_batch_requests = prepare_llm_batch_requests(\n",
    "    baseline_data, \n",
    "    \"baseline\"\n",
    "    )\n",
    "    finetuned_batch_requests = prepare_llm_batch_requests(\n",
    "        finetuned_data,\n",
    "        \"finetuned\"\n",
    "    )\n",
    "    all_batch_requests = ground_batch_requests + baseline_batch_requests + finetuned_batch_requests\n",
    "    \n",
    "    print(f\"‚úì Prepared {len(all_batch_requests)} evaluation requests ({len(ground_batch_requests)} ground + {len(baseline_batch_requests)} baseline + {len(finetuned_batch_requests)} fine-tuned)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping LLM evaluation setup:\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        print(\"  - No ANTHROPIC_API_KEY found in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:44:41,775 - INFO - üöÄ Submitting batch with 824 requests...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Submitting batch request to Claude API...\n",
      "   This will evaluate 824 DAGs using Claude Sonnet 4\n",
      "   Estimated cost: ~$12.36 (at $15/1M input tokens)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:44:44,569 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages/batches?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:44:44,573 - INFO - ‚úÖ Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
      "2025-12-30 16:44:44,574 - INFO - ‚è≥ Waiting for batch msgbatch_01GuxFdkP6XrMjqjzxNwjCGb...\n",
      "2025-12-30 16:44:44,753 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:44:44,756 - INFO - üìä Status: in_progress (elapsed: 0.2s)\n",
      "2025-12-30 16:44:44,757 - INFO -    Progress: 0/824 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Batch submitted: msgbatch_01GuxFdkP6XrMjqjzxNwjCGb\n",
      "\n",
      "‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:45:15,001 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:45:15,005 - INFO - üìä Status: in_progress (elapsed: 30.4s)\n",
      "2025-12-30 16:45:15,006 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:45:45,259 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:45:45,263 - INFO - üìä Status: in_progress (elapsed: 60.7s)\n",
      "2025-12-30 16:45:45,263 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:46:15,515 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:46:15,518 - INFO - üìä Status: in_progress (elapsed: 90.9s)\n",
      "2025-12-30 16:46:15,519 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:46:45,750 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:46:45,756 - INFO - üìä Status: in_progress (elapsed: 121.2s)\n",
      "2025-12-30 16:46:45,756 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:47:16,010 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:16,013 - INFO - üìä Status: in_progress (elapsed: 151.4s)\n",
      "2025-12-30 16:47:16,014 - INFO -    Progress: 0/824 (0.0%)\n",
      "2025-12-30 16:47:46,334 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:46,336 - INFO - üìä Status: ended (elapsed: 181.8s)\n",
      "2025-12-30 16:47:46,336 - INFO -    Progress: 824/824 (100.0%)\n",
      "2025-12-30 16:47:46,336 - INFO - ‚úÖ Batch completed with status: ended\n",
      "2025-12-30 16:47:46,337 - INFO - ‚¨áÔ∏è Downloading results...\n",
      "2025-12-30 16:47:46,530 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:47:46,823 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01GuxFdkP6XrMjqjzxNwjCGb/results \"HTTP/1.1 200 OK\"\n",
      "2025-12-30 16:47:47,475 - INFO - üì• Downloaded 824 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded 824 results\n",
      "\n",
      "üìä Parsing evaluation scores...\n",
      "‚úì Parsed 489 evaluation scores\n",
      "‚ö†Ô∏è 335 results had errors or couldn't be parsed\n",
      "\n",
      "LLM Evaluation Summary (1-5 scale):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "      <th>completeness</th>\n",
       "      <th>best_practices</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>1.98</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tuned</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            correctness  completeness  best_practices\n",
       "model                                                \n",
       "Baseline           1.98          1.78            1.82\n",
       "Fine-tuned         1.78          1.57            1.76"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute Batch Processing and Parse Results\n",
    "print(\"üöÄ Submitting batch request to Claude API...\")\n",
    "print(f\"   This will evaluate {len(all_batch_requests)} DAGs using Claude Sonnet 4\")\n",
    "print(f\"   Estimated cost: ~${len(all_batch_requests) * 0.015:.2f} (at $15/1M input tokens)\")\n",
    "print()\n",
    "\n",
    "# Submit batch\n",
    "batch_id = processor.submit_batch(all_batch_requests)\n",
    "print(f\"‚úì Batch submitted: {batch_id}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:27,422 - INFO - ‚è≥ Waiting for batch msgbatch_01Mue6HSzr5mR9g3XR1aEST9...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:27,676 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9?beta=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-31 11:08:27,679 - INFO - üìä Status: ended (elapsed: 0.3s)\n",
      "2025-12-31 11:08:27,679 - INFO -    Progress: 1026/1026 (100.0%)\n",
      "2025-12-31 11:08:27,679 - INFO - ‚úÖ Batch completed with status: ended\n",
      "2025-12-31 11:08:27,680 - INFO - ‚¨áÔ∏è Downloading results...\n",
      "2025-12-31 11:08:27,876 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 11:08:28,160 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_01Mue6HSzr5mR9g3XR1aEST9/results \"HTTP/1.1 200 OK\"\n",
      "2025-12-31 11:08:28,794 - INFO - üì• Downloaded 1026 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded 1026 results\n",
      "\n",
      "üìä Parsing evaluation scores...\n",
      "‚úì Parsed 1025 evaluation scores\n",
      "‚ö†Ô∏è 1 results had errors or couldn't be parsed\n",
      "\n",
      "LLM Evaluation Summary (1-5 scale):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idiomatic_airflow_score</th>\n",
       "      <th>no_hallucination_score</th>\n",
       "      <th>instruction_adherence_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finetuned</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idiomatic_airflow_score  no_hallucination_score  \\\n",
       "model                                                        \n",
       "baseline                      0.11                    0.24   \n",
       "finetuned                     0.43                    0.06   \n",
       "ground                        0.91                    0.17   \n",
       "\n",
       "           instruction_adherence_score  \n",
       "model                                   \n",
       "baseline                          0.15  \n",
       "finetuned                         0.08  \n",
       "ground                            0.72  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Wait for completion (can take 10-30 minutes for large batches)\n",
    "print(\"‚è≥ Waiting for batch to complete (this may take 10-30 minutes)...\")\n",
    "batch = processor.wait_for_batch_completion(batch_id)\n",
    "print()\n",
    "\n",
    "# Download results\n",
    "print(\"üì• Downloading results...\")\n",
    "results = processor.download_batch_results(batch_id)\n",
    "print(f\"‚úì Downloaded {len(results)} results\")\n",
    "print()\n",
    "\n",
    "# Parse results into dataframe\n",
    "print(\"üìä Parsing evaluation scores...\")\n",
    "llm_eval_results = []\n",
    "parse_errors = 0\n",
    "\n",
    "for result in results:\n",
    "    if result['result']['type'] == 'succeeded':\n",
    "        # Parse model name and dag_id from custom_id\n",
    "        # Handle format: \"baseline_123\" or \"finetuned_123\"\n",
    "        custom_id = result['custom_id']\n",
    "        parts = custom_id.split('_')\n",
    "        model_name = parts[0]  # 'ground', 'baseline' or 'finetuned'\n",
    "        dag_id = int(parts[1])\n",
    "        text = result['result']['message']['content'][0]['text']\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                eval_data = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in response\")\n",
    "            llm_eval_results.append({\n",
    "                'model': model_name,\n",
    "                'dag_id': dag_id,\n",
    "                'idiomatic_airflow_score': eval_data['idiomatic_airflow']['score'],\n",
    "                'idiomatic_airflow_reasoning': eval_data['idiomatic_airflow']['reasoning'],\n",
    "                'no_hallucination_score': eval_data['no_hallucination']['score'],\n",
    "                'no_hallucination_reasoning': eval_data['no_hallucination']['reasoning'],\n",
    "                'instruction_adherence_score': eval_data['instruction_adherence']['score'],\n",
    "                'instruction_adherence_reasoning': eval_data['instruction_adherence']['reasoning'],\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            parse_errors += 1\n",
    "    elif result['result']['type'] == 'errored':\n",
    "        parse_errors += 1\n",
    "\n",
    "\n",
    "df_llm = pd.DataFrame(llm_eval_results)\n",
    "print(f\"‚úì Parsed {len(llm_eval_results)} evaluation scores\")\n",
    "if parse_errors > 0:\n",
    "    print(f\"‚ö†Ô∏è {parse_errors} results had errors or couldn't be parsed\")\n",
    "\n",
    "# Display summary statistics\n",
    "summary = df_llm.groupby('model').agg({\n",
    "    'idiomatic_airflow_score': 'mean',\n",
    "    'no_hallucination_score': 'mean',\n",
    "    'instruction_adherence_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nLLM Evaluation Summary (1-5 scale):\")\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (airflowNet)",
   "language": "python",
   "name": "airflownet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
