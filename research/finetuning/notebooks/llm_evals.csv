,model,dag_id,idiomatic_airflow_score,idiomatic_airflow_reasoning,no_hallucination_score,no_hallucination_reasoning,instruction_adherence_score,instruction_adherence_reasoning,overall_score
0,Ground Truth (HuggingFace),0,1,"The code uses the SnowparkOperator from airflow.providers.snowflake.operators.snowpark, which is the idiomatic and native Airflow operator for Snowpark operations. It does not wrap Snowflake logic in a PythonOperator with hooks, but instead uses the purpose-built operator designed for Snowpark Python operations.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,1,The code fully adheres to the instruction. It creates a data pipeline that loads 12 product records into a Snowflake table (setup_data task) and validates the data load by verifying the row count matches the expected count of 12 (check_num_rows task). Both required steps are implemented correctly with proper task dependencies.,0.6666666666666666
1,Ground Truth (HuggingFace),2,1,"The code uses native Airflow constructs appropriately. It uses the EmptyOperator from the standard providers package, implements a custom timetable class (AfterWorkdayTimetable) as the schedule parameter, and uses the DAG context manager. This is the idiomatic way to implement custom scheduling in Airflow 3.0.0, where timetables are passed directly to the schedule parameter rather than wrapping logic in PythonOperators.",0,"The code imports 'AfterWorkdayTimetable' from 'airflow.example_dags.plugins.workday', which is an internal example/testing path. This is not a standard production-ready import from official Airflow providers or core libraries. In a production environment, this custom timetable class should either be defined in the DAG file itself or imported from a proper custom plugins directory, not from 'airflow.example_dags.plugins'.",1,"The code fully adheres to the instruction. It constructs an Airflow DAG that showcases a custom timetable scheduling mechanism using the AfterWorkdayTimetable class, builds a simple workflow with an EmptyOperator, and demonstrates scheduling flexibility by triggering after standard work hours. All requested elements are present: custom timetable implementation, simple workflow, and illustration of scheduling flexibility.",0.6666666666666666
2,Ground Truth (HuggingFace),3,1,"The code uses native Airflow operators specifically designed for Campaign Manager tasks. It imports and uses GoogleCampaignManagerBatchInsertConversionsOperator, GoogleCampaignManagerBatchUpdateConversionsOperator, GoogleCampaignManagerInsertReportOperator, GoogleCampaignManagerRunReportOperator, GoogleCampaignManagerDownloadReportOperator, GoogleCampaignManagerDeleteReportOperator, and GoogleCampaignManagerReportSensor from airflow.providers.google.marketing_platform. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.",0,"The code imports from 'system.google' module with 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which appears to be an internal testing module. The code also includes extensive test harness boilerplate with comments like 'This system test relies on a service account...' and 'before running this system test locally', indicating this is test infrastructure code rather than production-ready code. The presence of system test patterns and internal test utilities constitutes hallucination/leakage.",1,"The code fulfills the user instruction requirements: (1) It retrieves Campaign Manager metrics through the report definition with dimensions and metricNames, (2) It generates a standard performance report across campaigns using the STANDARD report type with campaign dimensions, (3) It demonstrates batch conversion tracking capabilities through GoogleCampaignManagerBatchInsertConversionsOperator and GoogleCampaignManagerBatchUpdateConversionsOperator, and (4) It shows interaction with Campaign Manager's reporting APIs (insert, run, download, delete report operators) and conversion APIs (batch insert/update conversion operators). The REPORT structure and CONVERSION structure are properly defined according to the Campaign Manager API specifications.",0.6666666666666666
3,Ground Truth (HuggingFace),4,1,"The code uses the specific GCSToPrestoOperator from airflow.providers.presto.transfers.gcs_to_presto, which is the idiomatic Airflow approach for transferring data from GCS to Presto. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The code fulfills the user instruction to create a data migration workflow that transfers structured CSV data from cloud object storage (GCS) to a columnar analytics database (Presto). The GCSToPrestoOperator supports efficient, automated data loading from cloud storage to a queryable data platform as requested. The task is properly configured with source bucket, source object, and destination table parameters.",0.6666666666666666
4,Ground Truth (HuggingFace),7,1,"The DAG correctly uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic and recommended approach for executing SQL queries in Airflow 3.0.0. It does not wrap database operations in PythonOperator with hooks, but instead uses the native SQL operator designed for this purpose. The conn_id is properly passed via default_args, making it available to all SQL operators.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses these with 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is test/example code rather than clean, production-ready code.",1,"The DAG fully satisfies the user instruction to create a data management workflow for tracking employee information in Vertica. It demonstrates all four required operations: (1) creating an employee records table with appropriate schema, (2) populating it with sample employee data, (3) querying the data, and (4) cleaning up by dropping the table. The workflow follows a logical sequence with proper task dependencies.",0.6666666666666666
5,Ground Truth (HuggingFace),8,1,"The code correctly uses the ADLSListOperator from airflow.providers.microsoft.azure.operators.adls, which is the idiomatic Airflow approach for listing files in Azure Data Lake Storage. It does not wrap functionality in a PythonOperator with hooks, but instead uses the native operator designed for this task.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than production-ready code.",1,"The code fulfills the user instruction to develop a data exploration DAG that lists Parquet files from Azure Data Lake Storage. It uses ADLSListOperator to perform a remote file inventory with the path pattern 'folder/output/*.parquet', which targets Parquet-formatted files in a specific directory. The operator retrieves file metadata that can support data validation and tracking processes as requested.",0.6666666666666666
6,Ground Truth (HuggingFace),9,1,"The code uses native Airflow operators designed for the specific tasks: GCSToSFTPOperator for transferring files from GCS to SFTP, GCSCreateBucketOperator for bucket management, LocalFilesystemToGCSOperator for uploading files, and SFTPSensor for validation. It does not wrap functionality in PythonOperator with hooks, which would be non-idiomatic. The code properly leverages the airflow.providers.google.cloud.transfers.gcs_to_sftp module and other provider-specific operators.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. This is a clear hallucination/leakage of test infrastructure code. Additionally, the code references 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables, and uses Path(__file__).parent / 'resources' pattern typical of test harnesses. The code also includes START/END comment markers like '[START howto_operator_gcs_to_sftp_copy_single_file]' which are documentation/example boilerplate rather than production code.",1,"The DAG fulfills the user instruction to create a data migration workflow that transfers files from cloud storage (GCS) to SFTP with multiple transfer strategies. It implements: (1) copying individual files (copy_file_from_gcs_to_sftp), (2) moving files (move_file_from_gcs_to_sftp with move_object=True), and (3) transferring directory contents (copy_dir_from_gcs_to_sftp and move_dir_from_gcs_to_sftp). The code demonstrates all three requested transfer modes with appropriate validation using SFTPSensor.",0.6666666666666666
7,Ground Truth (HuggingFace),11,1,"The code uses appropriate Airflow providers and operators for AWS services. It leverages EksCreateClusterOperator, EksDeleteClusterOperator, EmrContainerOperator, EmrEksCreateClusterOperator, S3CreateBucketOperator, EksClusterStateSensor, and EmrContainerSensor from the official airflow.providers.amazon.aws package. While there are some @task decorated functions for auxiliary operations (like creating launch templates and enabling EMR access), these are appropriate uses of TaskFlow API for operations that don't have dedicated operators. The core EMR on EKS and cluster management operations use the proper native operators rather than wrapping hooks in PythonOperators.",0,"The code contains clear evidence of test harness boilerplate and internal testing modules. Specifically, it imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and uses 'sys_test_context_task = SystemTestContextBuilder()...' which are internal testing utilities not meant for production code. This is a system test example that has been presented as production code, which constitutes hallucination/leakage of test infrastructure.",1,"The code fulfills the user instruction comprehensively. It demonstrates: (1) launching a Kubernetes cluster using EksCreateClusterOperator, (2) configuring IAM roles through multiple tasks (create_iam_oidc_identity_provider, update_trust_policy_execution_role, enable_access_emr_on_eks), (3) running a sample Python computation job (the pi.py calculation script stored in S3 and executed via EmrContainerOperator), and (4) managing the full lifecycle including cleanup operations. The workflow creates an automated pipeline for deploying and managing EMR jobs on EKS as requested, even though the code is truncated.",0.6666666666666666
8,Ground Truth (HuggingFace),12,1,"The code uses native BashOperator from airflow.operators.bash, which is the appropriate and idiomatic operator for executing bash commands. It also leverages Airflow 2.7+ specific features like .as_setup() and .as_teardown() methods, which are the proper way to designate setup and teardown tasks. The code does not wrap bash commands in PythonOperator or use hooks unnecessarily. TaskGroup is also used idiomatically for logical grouping.",1,"The code imports only standard Airflow libraries (airflow.models.dag, airflow.operators.bash, airflow.utils.task_group) and the pendulum library for datetime handling, which is a standard dependency in Airflow. There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no non-existent or hallucinated modules. The code is clean and production-ready.",1,"The DAG fully satisfies the user instruction. It creates a pipeline that simulates a data processing workflow with: (1) explicit initialization stages using .as_setup() for both root-level and taskgroup-level setup tasks, (2) main processing logic represented by normal bash tasks, (3) cleanup stages using .as_teardown() for finalization operations, and (4) proper task dependencies showing how setup tasks prepare the environment and teardown tasks perform cleanup around the main processing logic. The DAG also demonstrates this pattern at multiple levels (root and within a TaskGroup), illustrating the concept comprehensively.",1.0
9,Ground Truth (HuggingFace),16,1,The code uses native Airflow Kafka providers with specific operators designed for the task: ProduceToTopicOperator for producing messages to Kafka and AwaitMessageTriggerFunctionSensor for consuming and processing messages. These are idiomatic Airflow patterns for Kafka integration rather than wrapping Kafka logic in generic PythonOperators with hooks.,0,"The code explicitly imports testing modules and includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production code. Additionally, there are missing imports at the top (Connection, json, datetime, DAG, ProduceToTopicOperator, AwaitMessageTriggerFunctionSensor) which suggests this is incomplete example/test code rather than production-ready code.",1,"The code fulfills the instruction to create a Kafka-based FizzBuzz game demonstrating message streaming and event processing. It generates numbers (1-50 via producer_function), checks for divisibility by 3 and 5 (in await_function and wait_for_event), and triggers specific actions based on number characteristics (printing 'FIZZ' for mod 3, 'BUZZ' for mod 5, and returning special message for mod 15). The system has both producer and consumer components as requested.",0.6666666666666666
10,Ground Truth (HuggingFace),17,1,The code uses native Airflow operators (BashOperator) and leverages Airflow 3.x's built-in setup/teardown functionality through the .as_setup() and .as_teardown() methods. It properly uses TaskGroup for organizing tasks and follows idiomatic patterns for task dependency management with the >> operator. No generic PythonOperator wrappers are used where specific operators would be more appropriate.,1,"The code is clean and production-ready. It imports only standard Airflow libraries: airflow.providers.standard.operators.bash.BashOperator, airflow.sdk.DAG, airflow.utils.task_group.TaskGroup, and pendulum for datetime handling. There are no imports from test modules, no test harness boilerplate, and no internal testing utilities like get_test_run().",1,"The DAG fully satisfies the user instruction. It demonstrates setup and teardown tasks using .as_setup() and .as_teardown() methods, showing how preparatory (root_setup, inner_setup) and cleanup operations (root_teardown, inner_teardown) are automatically managed around core task execution (root_normal, inner_normal). It illustrates task dependency management with clear >> operators and uses TaskGroup (section_1) to demonstrate group management with initialization and finalization steps at both the root level and within a task group.",1.0
11,Ground Truth (HuggingFace),19,1,"The code uses appropriate Airflow providers and operators for the tasks. It uses ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator for GCE instance management, SQLExecuteQueryOperator for database operations, and SQLToGoogleSheetsOperator for data transfer to Google Sheets. It also uses SSHOperator for remote commands and GoogleSheetsCreateSpreadsheetOperator for spreadsheet creation. These are all idiomatic Airflow patterns using native operators rather than wrapping logic in PythonOperators with hooks.",1,"The code uses only standard Airflow libraries and Google Cloud providers. All imports are from legitimate Airflow packages (airflow.providers.google.cloud, airflow.providers.ssh, airflow.providers.common.sql, etc.). There are no imports from test utilities or internal testing modules. The code appears to be production-ready without test harness boilerplate.",1,"The code fulfills all requirements from the user instruction: (1) Creates a Compute Engine instance using ComputeEngineInsertInstanceOperator, (2) Configures a PostgreSQL database (via SSH commands to install Docker and run PostgreSQL container on the GCE instance), and (3) Transfers data to Google Sheets using SQLToGoogleSheetsOperator. The DAG showcases a multi-step cloud data workflow with infrastructure provisioning (GCE instance creation), database setup, and data migration to Google Sheets. The code also includes proper cleanup with ComputeEngineDeleteInstanceOperator.",1.0
12,Ground Truth (HuggingFace),20,1,"The DAG uses native Airflow providers and operators specifically designed for Google Compute Engine tasks. It imports and uses ComputeEngineInsertInstanceTemplateOperator, ComputeEngineCopyInstanceTemplateOperator, ComputeEngineInstanceGroupUpdateManagerTemplateOperator, and other GCE-specific operators from airflow.providers.google.cloud.operators.compute. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. This is a hallucination/leakage issue as it references test infrastructure that would not exist in a production environment. The code also uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are test harness patterns, not production-ready code.",0,"The DAG code is incomplete (cuts off mid-line with '# [END howto_operator_gce_ig'). While it demonstrates creating and copying instance templates, the code does not show the complete workflow including the instance group manager update operation and cleanup tasks that would be expected based on the imports and variable definitions. The instruction asks for a complete workflow demonstrating create, copy, and update operations, but the provided code is truncated and cannot be executed as-is.",0.3333333333333333
13,Ground Truth (HuggingFace),21,1,"The DAG uses native Airflow operators designed for the specific tasks: CloudDataTransferServiceCreateJobOperator for creating transfer jobs, GCSCreateBucketOperator for bucket management, LocalFilesystemToGCSOperator for file uploads, and CloudDataTransferServiceUpdateJobOperator for updating jobs. It does not wrap functionality in PythonOperator + Hook patterns, but instead uses the appropriate providers from airflow.providers.google.cloud.",1,The code imports only standard Airflow libraries from the official Google Cloud provider package (airflow.providers.google.cloud). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears production-ready with proper use of environment variables and standard Airflow patterns.,1,"The DAG fulfills all requirements from the user instruction: (1) Creates a GCS transfer job using CloudDataTransferServiceCreateJobOperator, (2) Demonstrates file upload using LocalFilesystemToGCSOperator, (3) Shows bucket management with GCSCreateBucketOperator for both source and destination buckets, and (4) Includes transfer job update functionality. The workflow demonstrates automatic file movement between source and destination buckets as requested.",1.0
14,Ground Truth (HuggingFace),22,1,"The code uses native Airflow operators (EmptyOperator, BashOperator) and the TaskGroup utility appropriately. It does not wrap logic unnecessarily in PythonOperator with hooks. The TaskGroup feature is used idiomatically to organize tasks into logical sections, which is the proper Airflow pattern for grouping related tasks.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.models.dag, airflow.operators.bash, airflow.operators.empty, airflow.utils.task_group) and pendulum for date handling. There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) multi-stage workflow with multiple sections (section_1, section_2), (2) task grouping using TaskGroup utility, (3) sequential processing with clear dependencies (start >> section_1 >> section_2 >> end), and (4) nested task groups (inner_section_2 within section_2). The workflow is organized into logical sections showing how complex workflows can be structured, exactly as requested.",1.0
15,Ground Truth (HuggingFace),23,0,"The DAG heavily relies on @task decorated functions that wrap boto3 client calls instead of using native Airflow operators. For example, 'create_quicksight_data_source', 'create_quicksight_dataset', 'delete_quicksight_data_source', 'delete_dataset', and 'delete_ingestion' all use boto3.client('quicksight') directly within PythonOperator tasks. While the DAG does use some native operators (QuickSightCreateIngestionOperator, QuickSightSensor, S3CreateBucketOperator, etc.), the core QuickSight data source and dataset creation logic should ideally use dedicated operators if available, or at minimum use Hooks rather than raw boto3 clients. This is not idiomatic Airflow as it bypasses the abstraction layer that Airflow provides.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing module (the 'system' package is not a standard Airflow library). Additionally, the code uses 'sys_test_context_task = SystemTestContextBuilder().build()' and references 'test_context[ENV' (incomplete in the provided code), which are clear indicators of test harness boilerplate. The comments also mention 'If Create Ingestion fails for any reason' and 'changing the template for the ingestion name or the ENV_ID and re-running the test', further confirming this is test code rather than production-ready code.",1,"The DAG fulfills the user instruction to build an automated data pipeline that transfers sample project data from S3 to AWS QuickSight. It demonstrates: (1) S3 bucket and object creation for sample data, (2) programmatic setup of a QuickSight data source, (3) creation of a QuickSight dataset, (4) ingestion process using QuickSightCreateIngestionOperator and QuickSightSensor, and (5) cleanup tasks. The workflow covers all the key requirements: S3 → QuickSight data source → dataset → ingestion, which enables visualization and analysis as requested.",0.3333333333333333
16,Ground Truth (HuggingFace),24,1,"The code uses native Google Cloud DLP operators from airflow.providers.google.cloud.operators.dlp, including CloudDLPCreateDLPJobOperator, CloudDLPListDLPJobsOperator, CloudDLPGetDLPJobOperator, CloudDLPCancelDLPJobOperator, and CloudDLPDeleteDLPJobOperator. It does not wrap DLP functionality in PythonOperators with hooks, which demonstrates idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. This is not production-ready code and contains testing infrastructure that should not be present in a production DAG.",1,"The DAG successfully demonstrates the complete lifecycle of a DLP job in Google Cloud as requested. It creates a DLP job (create_job), lists jobs (list_jobs), retrieves job details (get_job), cancels the job (cancel_job), and deletes the job (delete_job). The workflow showcases programmatic management of sensitive data inspection tasks using Airflow and Google Cloud DLP operators, fulfilling the user's instruction to demonstrate the complete lifecycle from creation to deletion.",0.6666666666666666
17,Ground Truth (HuggingFace),25,0,"The DAG heavily relies on generic PythonOperator patterns (using @task decorator) with boto3 clients instead of using native Airflow SageMaker operators. Airflow provides dedicated operators like SageMakerTrainingOperator, SageMakerModelOperator, SageMakerEndpointConfigOperator, and SageMakerEndpointOperator from airflow.providers.amazon.aws.operators.sagemaker that should be used for this ML workflow. The code wraps boto3 SDK calls in Python tasks rather than leveraging the idiomatic Airflow approach of using provider-specific operators.",0,"The code references 'KNN_IMAGES_BY_REGION' which appears to be from a test harness or example system test file (as evidenced by the error message mentioning 'system testfile'). This is not a standard Airflow library constant and suggests code was copied from internal testing infrastructure. Additionally, the code is incomplete (cuts off at 'endpoint_name = f""{env_id}-en'), indicating it may not be production-ready.",0,"While the code attempts to address the ML lifecycle (training, model creation, endpoint deployment), it fails to demonstrate 'provisioning AWS resources' (no S3 bucket creation visible in the provided code), lacks 'robust error handling' (no try-catch blocks, error notifications, or retry logic), and has incomplete 'resource management' (cleanup tasks exist but the overall DAG structure and dependencies are not shown). The code is also incomplete/truncated, making it impossible to verify if all required steps are present and properly orchestrated.",0.0
18,Ground Truth (HuggingFace),26,1,"The code uses the native FileTransferOperator from airflow.providers.common.io.operators.file_transfer, which is the idiomatic Airflow approach for file transfer operations. It does not wrap functionality in PythonOperator + Hook patterns. The use of @task decorators for auxiliary operations (create, delete) is also idiomatic for Airflow 2.0+.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'list(dag.tasks) >> watcher()', and 'test_run = get_test_run(dag)'. These are not production-ready elements and represent test infrastructure leakage.",1,"The DAG fully implements the requested workflow: (1) generates a temporary file via create_temp_file(), (2) transfers it to AWS S3 using FileTransferOperator, and (3) cleans up temporary resources via delete_temp_file() and remove_bucket(). All three required steps from the instruction are present and correctly orchestrated.",0.6666666666666666
19,Ground Truth (HuggingFace),27,1,"The DAG uses native Airflow operators specifically designed for Campaign Manager 360 tasks. It imports and uses GoogleCampaignManagerInsertReportOperator, GoogleCampaignManagerRunReportOperator, GoogleCampaignManagerDownloadReportOperator, GoogleCampaignManagerBatchInsertConversionsOperator, GoogleCampaignManagerBatchUpdateConversionsOperator, and GoogleCampaignManagerReportSensor from airflow.providers.google.marketing_platform. This is the idiomatic approach rather than wrapping hooks in PythonOperators.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is an internal testing utility. The docstring explicitly states 'This system test relies on a service account' and references 'system test locally'. The code also includes test-specific naming conventions like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT'. This is not production-ready code but rather a system test example.",1,"The DAG fulfills the user instruction to create an automated Campaign Manager reporting and conversion tracking workflow. It includes: (1) Report creation with GoogleCampaignManagerInsertReportOperator, (2) Report execution with GoogleCampaignManagerRunReportOperator, (3) Report download with GoogleCampaignManagerDownloadReportOperator, (4) Conversion tracking with GoogleCampaignManagerBatchInsertConversionsOperator and GoogleCampaignManagerBatchUpdateConversionsOperator, (5) Dynamic data retrieval using XCom and task decorators for secrets, and (6) Report monitoring with GoogleCampaignManagerReportSensor. The system dynamically pulls campaign data, generates reports, and records conversion activities as requested.",0.6666666666666666
20,Ground Truth (HuggingFace),28,1,"The code uses the PigOperator, which is the appropriate Airflow operator for executing Pig scripts. This is idiomatic Airflow usage - it leverages the specific operator designed for Pig execution rather than wrapping Pig logic in a PythonOperator with hooks.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. This is exactly the type of hallucination/leakage that indicates non-production-ready code.",1,"The code fulfills the user instruction to create a data pipeline demonstrating basic Pig script execution in a local Hadoop environment. It uses PigOperator with pig_opts='-x local' for local execution, and the pig script 'ls /;' lists directory contents as requested for demonstrating system interaction. The workflow is simple and matches the requirements.",0.6666666666666666
21,Ground Truth (HuggingFace),29,1,"The code uses idiomatic Airflow patterns throughout. It leverages the TaskFlow API with @task decorators, @task.virtualenv, and @task.external_python decorators which are the recommended Airflow 2.0+ patterns for Python tasks. It does not wrap logic in PythonOperator + Hook patterns but instead uses native TaskFlow decorators designed for these purposes. The use of task.override() for dynamic task generation is also idiomatic.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.sdk.dag, airflow.sdk.task) and standard Python libraries (logging, sys, time, pprint, pendulum). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears to be a legitimate example DAG without test artifacts.",1,The DAG fully adheres to the instruction requirements: (1) Simulates multi-stage system monitoring/logging workflow - achieved through multiple task stages; (2) Prints context information - implemented in print_context task; (3) Logs SQL queries - implemented in log_sql task with template rendering; (4) Generates sleep-based parallel tasks - implemented with 5 dynamically generated sleeping tasks; (5) Executes environment-specific Python functions - implemented with virtualenv_task and external_python_task; (6) Demonstrates task chaining - clear dependency chains with >> operator; (7) Dependency management - proper task dependencies established; (8) Runtime flexibility - uses dynamic task generation and different execution environments.,1.0
22,Ground Truth (HuggingFace),30,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator, GCSDeleteBucketOperator, and GoogleCalendarToGCSOperator from the Google Cloud provider package. While it uses @task decorators for connection management, this is acceptable as connection CRUD operations don't have dedicated operators and the TaskFlow API is idiomatic for such utility functions. The core data transfer operation uses the proper GoogleCalendarToGCSOperator rather than wrapping a hook in a PythonOperator.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not appear in production DAG code. The comments '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' also indicate this is test code rather than production-ready code.",1,"The DAG fully implements the requested requirements: (1) demonstrates a complete cloud data transfer pipeline with Google Calendar event synchronization, (2) includes explicit setup tasks (create_bucket, create_connection), (3) includes execution task (upload_calendar_to_gcs using GoogleCalendarToGCSOperator), (4) includes teardown tasks (delete_connection, delete_bucket with appropriate trigger rule), and (5) showcases connection management, data transfer, and infrastructure cleanup using GCP operators as requested.",0.6666666666666666
23,Ground Truth (HuggingFace),31,1,"The code uses native Airflow providers and operators designed for Google Cloud services. It imports and uses specific operators like GCSCreateBucketOperator, GCSToGCSOperator, and various CloudVision operators (CloudVisionCreateProductSetOperator, CloudVisionCreateProductOperator, etc.) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for GCP integrations.",0,"The code imports from 'system.google' module with the line 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which appears to be an internal testing module. This is test harness boilerplate that should not be present in production-ready code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, which are testing-specific patterns.",0,"The code is incomplete - it cuts off mid-line with 'product_set_create = CloudVisionCreateProductSetOper'. More importantly, while it sets up infrastructure (bucket creation, file copying) and defines various Vision API objects (product sets, products, reference images), it does not demonstrate a complete image processing workflow with actual image analysis. The instruction asks for 'a complete image processing workflow' with 'image analysis', but the DAG only shows setup tasks and doesn't include the actual image analysis/processing operations that would use the annotate_image_request or DETECT_IMAGE configurations that were defined.",0.3333333333333333
24,Ground Truth (HuggingFace),32,1,"The code uses the specific GCSToTrinoOperator from airflow.providers.trino.transfers.gcs_to_trino, which is the idiomatic Airflow approach for transferring data from GCS to Trino. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,1,"The DAG fulfills the user instruction to build an automated data pipeline that extracts CSV files from a specified GCS bucket and loads them into a predefined Trino table. The GCSToTrinoOperator is configured with source_bucket, source_object, and trino_table parameters, which supports efficient and reproducible data ingestion as requested.",0.6666666666666666
25,Ground Truth (HuggingFace),33,1,"The code uses the native AnalyticDBSparkSQLOperator from airflow.providers.alibaba.cloud.operators.analyticdb_spark, which is the idiomatic way to interact with Alibaba AnalyticDB Spark. It does not wrap logic in PythonOperator with hooks, but instead uses the provider-specific operator designed for this task.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that represent hallucination/leakage from example/test code.",0,"While the DAG attempts to perform database introspection with SHOW DATABASES and SHOW TABLES commands, there is a critical typo in the first SQL query: 'SHOE DATABASES;' instead of 'SHOW DATABASES;'. This would cause the query to fail. Additionally, the instruction asks for 'systematic database introspection' to 'discover and catalog data resources' and 'provide visibility into available data schemas and table structures', but the DAG only shows databases and tables without actually cataloging them, describing schemas, or providing detailed table structures (e.g., DESCRIBE TABLE commands). The workflow is incomplete for the stated business requirements.",0.3333333333333333
26,Ground Truth (HuggingFace),35,1,"The code uses the specific CohereEmbeddingOperator from airflow.providers.cohere.operators.embedding, which is the idiomatic way to interact with Cohere AI services in Airflow. It does not wrap functionality in PythonOperator with hooks, but instead uses the native operator designed for this task.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",1,The DAG fulfills the user instruction to demonstrate text embedding using Cohere AI service. It shows how to convert text into vector representations by processing multiple text inputs (a list of texts) and demonstrates two usage patterns: embedding multiple texts at once and embedding a single text. The workflow successfully demonstrates the core requirement of processing text inputs and generating embeddings.,0.6666666666666666
27,Ground Truth (HuggingFace),36,1,"The DAG uses native Airflow providers and operators designed specifically for GCS operations. It properly imports and uses GCSSynchronizeBucketsOperator, GCSToGCSOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator, and LocalFilesystemToGCSOperator from airflow.providers.google.cloud packages. No generic PythonOperator wrappers around hooks are used - all operations leverage the appropriate specialized operators.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID' environment variable, PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', and includes naming patterns typical of system tests (e.g., bucket names with ENV_ID suffix). While it doesn't explicitly import test utilities like 'get_test_run', the overall structure and variable naming (SYSTEM_TESTS_*) strongly suggest this is test infrastructure code rather than clean production-ready code.",1,"The DAG fully addresses the user instruction to create a GCS data synchronization workflow demonstrating file transfer and bucket management techniques with different synchronization strategies. It showcases: (1) bucket creation and management (GCSCreateBucketOperator), (2) multiple synchronization strategies including basic sync, full sync with delete/overwrite options, sync to subdirectory, and sync from subdirectory (GCSSynchronizeBucketsOperator with various configurations), and (3) file transfer operations (GCSToGCSOperator for single file copy, LocalFilesystemToGCSOperator for uploads). The DAG comprehensively demonstrates different approaches to bucket synchronization as requested.",0.6666666666666666
28,Ground Truth (HuggingFace),37,1,"The code uses native Airflow Tableau providers correctly. It imports and uses TableauOperator for refresh operations and TableauJobStatusSensor for monitoring job status. These are the appropriate, idiomatic operators for Tableau operations rather than wrapping logic in PythonOperator with hooks.",0,The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code.,1,"The DAG fulfills the instruction requirements: (1) demonstrates data extract refresh processes for a BI workbook (Tableau), (2) shows how to trigger and monitor data updates across different execution modes (blocking and non-blocking refresh), (3) simulates real-world refresh scenarios, (4) includes explicit job status tracking via TableauJobStatusSensor, and (5) demonstrates different refresh strategies (synchronous blocking vs asynchronous non-blocking with separate status checking).",0.6666666666666666
29,Ground Truth (HuggingFace),38,1,"The code uses the native LatestOnlyOperator from airflow.providers.standard.operators.latest_only, which is the idiomatic way to implement conditional execution based on scheduling windows. It does not wrap functionality in PythonOperator or use hooks unnecessarily. The EmptyOperator is also appropriately used for demonstration purposes.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.providers.standard.operators.empty, airflow.providers.standard.operators.latest_only, and airflow.sdk). There are no test harness imports, internal testing modules, or test boilerplate code present.",0,"The instruction asks to 'showcase conditional task execution' and 'prevent redundant processing of older scheduled tasks'. While the LatestOnlyOperator is correctly implemented, the DAG has 'catchup=False', which fundamentally contradicts the purpose of demonstrating the LatestOnlyOperator. The LatestOnlyOperator is specifically designed to handle scenarios where catchup=True and multiple backfill runs occur - it ensures only the latest scheduled run executes downstream tasks. With catchup=False, there will never be older scheduled tasks to skip, making the demonstration ineffective. Additionally, the pipeline is minimal with only one downstream task, which doesn't adequately 'showcase' the conditional behavior (e.g., comparing tasks that run always vs. only on latest).",0.6666666666666666
30,Ground Truth (HuggingFace),39,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator for bucket creation, AzureFileShareToGCSOperator for cross-cloud file transfer, and GCSDeleteBucketOperator for cleanup. These are proper provider-specific operators rather than generic PythonOperator wrappers around hooks, which is the idiomatic Airflow approach.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules. Additionally, the code includes test-specific logic like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)', along with comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' that indicate this is test code rather than production-ready code.",1,"The DAG fulfills all requirements from the user instruction: (1) demonstrates cross-cloud file transfer between Azure and Google Cloud using AzureFileShareToGCSOperator, (2) creates a destination GCS bucket before transfer, (3) transfers files from Azure FileShare to GCS, and (4) cleanly removes the temporary storage infrastructure using trigger_rule=TriggerRule.ALL_DONE to ensure cleanup happens regardless of transfer success/failure. The workflow implements the complete lifecycle requested.",0.6666666666666666
31,Ground Truth (HuggingFace),40,0,"The DAG heavily relies on @task decorated functions that wrap boto3 client calls (create_quicksight_data_source, create_quicksight_dataset, delete_quicksight_data_source, delete_dataset, delete_ingestion) instead of using native Airflow operators. While QuickSightCreateIngestionOperator and QuickSightSensor are imported, the core QuickSight operations for creating data sources and datasets are implemented as PythonOperator-style tasks with direct boto3 calls. This is not idiomatic Airflow - proper operators should be used where available, or at minimum, hooks should be used instead of raw boto3 clients.",0,"The code explicitly imports from 'tests.system.providers.amazon.aws.utils' (line: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder') and uses 'SystemTestContextBuilder().build()' which is test harness boilerplate. This is internal testing infrastructure that should not appear in production DAG code. The code also references 'sys_test_context_task()' and 'test_context' variables, further indicating this is test code rather than production-ready code.",0,"The DAG code is incomplete - it cuts off mid-definition with 'env_id = t' at the end, making it non-functional. Additionally, while the instruction asks for a complete automated pipeline that transfers data from S3 to QuickSight with data source, dataset, and ingestion setup, the DAG is missing critical components: no S3 data upload is shown in the workflow chain, the manifest file creation is not included, and the task dependencies using 'chain()' are not defined. The code has the building blocks but doesn't demonstrate a complete, working pipeline as requested.",0.0
32,Ground Truth (HuggingFace),41,1,"The code uses native Airflow providers and operators specifically designed for Google Campaign Manager tasks. It imports and uses GoogleCampaignManagerInsertReportOperator, GoogleCampaignManagerRunReportOperator, GoogleCampaignManagerDownloadReportOperator, and GoogleCampaignManagerReportSensor from airflow.providers.google.marketing_platform. It does not wrap functionality in PythonOperator with hooks, but instead uses the idiomatic operator pattern throughout.",0,"The code appears to be derived from Airflow's system tests or example code. The docstring states 'Example Airflow DAG that shows how to use CampaignManager' which is typical of test/example code. It includes ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID') and PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), which are clear indicators of test harness boilerplate. While it doesn't directly import from tests_common, the presence of SYSTEM_TESTS variables and the example-oriented structure indicates this is test/example code rather than production-ready code.",0,"The DAG is incomplete (cuts off mid-comment) and does not fully implement the requested workflow. The instruction asks for a complete pipeline that 'creates, runs, and processes marketing performance reports' and 'prepares it for business intelligence analysis.' While the code shows report creation and running, it's truncated and doesn't show the download/processing steps or any transformation logic for BI analysis. Additionally, the code includes unrelated conversion tracking operations (BatchInsertConversions, BatchUpdateConversions) that weren't part of the instruction, and uses test data (FLOODLIGHT_ACTIVITY_ID, DEVICE_ID, etc.) rather than focusing on the core reporting workflow requested.",0.3333333333333333
33,Ground Truth (HuggingFace),42,1,"The code uses native Airflow operators specifically designed for Looker integration: LookerStartPdtBuildOperator and LookerCheckPdtBuildSensor from airflow.providers.google.cloud.operators.looker and airflow.providers.google.cloud.sensors.looker. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operators for the task. This is the idiomatic Airflow approach.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules (like tests_common.test_utils.system_tests) or test harness boilerplate. The code is clean and production-ready, using only legitimate Airflow 2.x provider operators and sensors.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) starting Looker PDT builds using LookerStartPdtBuildOperator, (2) checking/monitoring PDT builds using LookerCheckPdtBuildSensor, (3) task dependencies via the >> operator (start_pdt_task_async >> check_pdt_task_async_sensor), (4) asynchronous job monitoring through the asynchronous=True parameter and sensor-based polling with poke_interval, and (5) cloud-based data transformation context using specialized Airflow operators for Looker. The workflow clearly illustrates the technical process requested.",1.0
34,Ground Truth (HuggingFace),43,1,"The code uses native Azure Service Bus operators from airflow.providers.microsoft.azure.operators.asb throughout. It properly leverages AzureServiceBusCreateQueueOperator, AzureServiceBusSendMessageOperator, AzureServiceBusReceiveMessageOperator, AzureServiceBusTopicCreateOperator, AzureServiceBusSubscriptionCreateOperator, and other specialized operators instead of wrapping logic in PythonOperator with hooks. This is idiomatic Airflow usage.",0,"The code imports pytest and uses pytest.skip() which is a testing framework component, not appropriate for production DAG code. The line 'import pytest' and 'pytest.skip(""Azure Service Bus not available"", allow_module_level=True)' are clear indicators of test harness boilerplate that should not be present in production DAG code.",0,"The DAG code is incomplete - it cuts off mid-comment after the update_service_bus_subscription task definition. The instruction requested a 'complete lifecycle' including queue creation, topic creation, message transmission, and subscription management, but the code is truncated and doesn't show: (1) sending messages to topics, (2) receiving messages from subscriptions, (3) cleanup/deletion tasks, or (4) proper task dependencies using chain(). The incomplete nature means it cannot demonstrate the full enterprise messaging lifecycle as requested.",0.3333333333333333
35,Ground Truth (HuggingFace),44,1,"The code uses native Airflow operators designed for their specific tasks: CloudFirestoreExportDatabaseOperator for Firestore export, GCSCreateBucketOperator for GCS bucket creation, and BigQueryCreateExternalTableOperator for creating the external BigQuery table. No PythonOperator wrappers with hooks are used where native operators exist.",0,"The code contains clear test harness boilerplate and references to system testing patterns. Specifically: ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), the extensive docstring mentioning 'If you want to run this example', and the use of test-oriented naming patterns like 'example_gcp_firestore' and tags=['example', 'firestore']. The variable naming with ENV_ID suffixes (f'bucket_{DAG_ID}_{ENV_ID}') is typical of Airflow's internal system tests rather than production code.",0,"The DAG is incomplete and does not fulfill the instruction. While it exports Firestore to GCS and creates an external BigQuery table, it lacks task dependencies (no >> or << operators connecting the tasks), so the workflow execution order is undefined. The instruction asks for an 'automated pipeline' demonstrating 'cross-project data transfer and transformation', but there's no transformation step shown, and critically, the tasks are not orchestrated in sequence. The code also includes setup tasks (create_bucket, create_dataset) but doesn't show them being used in the workflow dependency chain.",0.3333333333333333
36,Ground Truth (HuggingFace),45,1,"The code uses the native PineconeIngestOperator from airflow.providers.pinecone.operators.pinecone, which is the idiomatic way to interact with Pinecone in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for vector ingestion.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code.,1,"The DAG fulfills the instruction requirements: (1) populates a Pinecone vector database with predefined embedding vectors, (2) allows flexible configuration via environment variables for index_name and namespace, (3) supports metadata as shown in the first vector tuple with {'key': 'value'}, and (4) supports custom namespaces through the namespace parameter. The use of input_vectors parameter with tuples containing IDs, vectors, and optional metadata demonstrates the requested flexibility.",0.6666666666666666
37,Ground Truth (HuggingFace),46,1,"The code correctly uses native Airflow Azure provider operators (WasbBlobSensor and WasbPrefixSensor) from airflow.providers.microsoft.azure.sensors.wasb. These are the appropriate, idiomatic operators for sensing blobs in Azure Blob Storage rather than wrapping hooks in PythonOperators.",0,The code includes test harness boilerplate at the bottom: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production DAG code and represent hallucination/leakage from example/test code.,0,"The instruction asks for a monitoring system that 'checks for specific file conditions before initiating downstream data processing tasks.' While the DAG implements the sensor components correctly, it does not include any downstream data processing tasks. The sensors are defined but have no dependencies or downstream tasks, making this an incomplete implementation of the requested monitoring system that should trigger processing workflows.",0.3333333333333333
38,Ground Truth (HuggingFace),48,1,"The code uses the native DynamoDBToS3Operator from airflow.providers.amazon.aws.transfers.dynamodb_to_s3, which is the idiomatic Airflow approach for transferring data from DynamoDB to S3. It also uses S3CreateBucketOperator and S3DeleteBucketOperator from the AWS provider. While there are @task decorated functions using boto3 directly for setup/teardown operations (creating tables, enabling point-in-time recovery), the core data transfer logic uses the appropriate Airflow operator rather than wrapping it in a PythonOperator with hooks.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing module (system.amazon.aws.utils). It also uses 'sys_test_context_task = SystemTestContextBuilder().build()' which is test harness boilerplate. This indicates the code is from a test/example file rather than production-ready code, containing test infrastructure that should not be present in a production DAG.",1,"The code fulfills the user instruction requirements: (1) Creates a data migration pipeline from DynamoDB to S3 using DynamoDBToS3Operator, (2) Implements point-in-time recovery capabilities by enabling continuous backups with enable_point_in_time_recovery(), (3) Demonstrates time-based incremental export capabilities through the incremental_export task group that captures start and end times for exports, (4) Shows advanced cloud data transfer techniques using native AWS operators and boto3 for configuration. The code is incomplete (cuts off mid-parameter), but the implemented portions demonstrate all requested capabilities.",0.6666666666666666
39,Ground Truth (HuggingFace),49,1,"The DAG uses native Airflow providers and operators specifically designed for Google Cloud DLP tasks. It properly uses CloudDLPCreateInspectTemplateOperator, CloudDLPGetInspectTemplateOperator, CloudDLPListInspectTemplatesOperator, CloudDLPUpdateInspectTemplateOperator, CloudDLPInspectContentOperator, and CloudDLPDeleteInspectTemplateOperator from airflow.providers.google.cloud.operators.dlp. No generic PythonOperator wrappers are used where native operators are available.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. It also imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID'. These are not standard Airflow libraries and represent test infrastructure that should not be in production code.",1,"The DAG fulfills the user instruction to create an automated pipeline for managing sensitive data detection configurations. It demonstrates programmatic handling of data privacy templates across different stages: creates an inspect template, lists templates, gets/inspects a specific template, updates it, uses it to inspect content for sensitive data (phone numbers), and finally cleans up by deleting the template. The workflow shows clear, sequential steps for the complete lifecycle of data protection configurations as requested.",0.6666666666666666
40,Ground Truth (HuggingFace),50,1,"The code correctly uses the Neo4jOperator from airflow.providers.neo4j.operators.neo4j, which is the idiomatic and native operator for executing Neo4j queries in Airflow. It does not wrap logic in a PythonOperator with a Hook, but instead uses the purpose-built operator for the task.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,1,"The code fulfills the user instruction by: (1) creating an Airflow workflow for data extraction from a graph database, (2) performing a targeted query for Tom Hanks node attributes, (3) using a parameterized Cypher query with date templating ('{{ds}}'), and (4) configuring the pipeline to flexibly retrieve graph data through the Neo4jOperator with a configurable connection ID. The Cypher query 'MATCH (tom {name: ""Tom Hanks"", date: ""{{ds}}""}) RETURN tom' correctly queries for Tom Hanks on a given date using Airflow's templating.",0.6666666666666666
41,Ground Truth (HuggingFace),51,1,"The code uses native Airflow providers and operators designed for the task. It properly uses ComprehendStartPiiEntitiesDetectionJobOperator for PII detection, ComprehendStartPiiEntitiesDetectionJobCompletedSensor for monitoring job completion, and S3 operators for bucket/object management. The code does not wrap these in PythonOperators with hooks, which would be non-idiomatic.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: (1) 'from system.amazon.aws.utils import SystemTestContextBuilder' - internal test utility, (2) 'from tests_common.test_utils.watcher import watcher' - test module import, (3) 'from tests_common.test_utils.system_tests import get_test_run' - test module import, (4) Uses 'sys_test_context_task' and 'test_context' variables for test setup, (5) Includes 'watcher()' task for test monitoring. This is clearly a system test example, not production-ready code.",1,"The DAG fulfills the user instruction to create a data privacy transformation pipeline using AWS Comprehend. It: (1) Takes raw data (SAMPLE_DATA with PII), (2) Uploads it to S3, (3) Uses ComprehendStartPiiEntitiesDetectionJobOperator with mode='ONLY_REDACTION' to identify and anonymize PII (NAME, ADDRESS), (4) Applies intelligent PII masking with RedactionConfig using REPLACE_WITH_PII_ENTITY_TYPE, (5) Generates redacted output to a specified S3 location. All key requirements are met.",0.6666666666666666
42,Ground Truth (HuggingFace),52,1,"The code correctly uses the AzureContainerInstancesOperator from airflow.providers.microsoft.azure.operators.container_instances, which is the idiomatic and native Airflow operator for Azure Container Instances. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator with appropriate parameters.",0,The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG successfully demonstrates the three requested Azure Container Instances deployment patterns: (1) t1 launches a standard Docker image ('hello-world'), (2) t2 uses a custom container registry (via CONTAINER_REGISTRY_SERVER environment variable), and (3) t3 configures persistent volume attachments using Azure File Share volumes. The systematic approach showcases different configuration scenarios as requested.",0.6666666666666666
43,Ground Truth (HuggingFace),54,1,"The code uses native Spark operators (SparkSubmitOperator, SparkJDBCOperator, SparkSqlOperator) from Airflow providers rather than wrapping Spark logic in PythonOperators with hooks. This is the idiomatic approach for Spark orchestration in Airflow.",0,The code explicitly imports testing utilities ('from tests_common.test_utils.system_tests import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). This is internal testing infrastructure that should not appear in production DAG code.,0,"The instruction explicitly requested 'sequential execution' and 'task chaining' to illustrate 'modular task dependencies'. However, the code defines four independent tasks (submit_job, jdbc_to_spark_job, spark_to_jdbc_job, spark_sql_job) with no dependencies established between them. There are no task chains (e.g., task1 >> task2 >> task3) demonstrating sequential execution or workflow orchestration, which was the core requirement.",0.3333333333333333
44,Ground Truth (HuggingFace),55,1,"The code uses native Airflow operators designed for their specific tasks: S3CreateBucketOperator and S3DeleteBucketOperator for AWS S3 operations, GCSCreateBucketOperator and GCSDeleteBucketOperator for GCS operations, GCSToS3Operator for file transfers, and CloudDataTransferService operators for Google Cloud Transfer Service management. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which is clearly a testing module import. This is test harness boilerplate that should not appear in production code. The presence of 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables further confirms this is test infrastructure leakage.",0,"The DAG code is incomplete - it cuts off mid-definition without closing the DAG context or defining the complete pipeline. While it demonstrates bucket creation (S3 and GCS), file transfer (GCSToS3Operator), and imports transfer job management operators, the actual transfer job creation, monitoring, and management tasks are not instantiated or connected in a workflow. The instruction requires a complete pipeline showcasing automated cloud storage interactions including transfer job management, but the code is truncated and non-functional.",0.3333333333333333
45,Ground Truth (HuggingFace),56,1,"The code uses native Google Cloud Composer operators from airflow.providers.google.cloud.operators.cloud_composer, including CloudComposerCreateEnvironmentOperator, CloudComposerListImageVersionsOperator, CloudComposerGetEnvironmentOperator, CloudComposerUpdateEnvironmentOperator, and CloudComposerDeleteEnvironmentOperator. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear indicators of test harness boilerplate. It uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, which are typical of Airflow's internal system tests. The naming conventions (e.g., 'test-{DAG_ID}-{ENV_ID}') and the presence of START/END comment markers like '[START howto_operator_composer_simple_environment]' are characteristic of Airflow's documentation examples and test infrastructure, not production-ready code.",0,"The code is incomplete - it cuts off mid-statement in the delete_env task definition ('environm' instead of 'environment_id'). Additionally, while the code defines multiple operators for environment provisioning, version listing, and state management, it does not include the chain() call or task dependencies to actually 'illustrate the technical workflow' as a step-by-step process. The operators are defined but not connected in a meaningful workflow sequence, failing to demonstrate the operational flow requested in the instruction.",0.3333333333333333
46,Ground Truth (HuggingFace),57,1,"The code uses native Airflow operators designed for the specific tasks: ComputeEngineInsertInstanceOperator for instance creation, ComputeEngineDeleteInstanceOperator for instance deletion, and SSHOperator with ComputeEngineSSHHook for SSH command execution. This is the idiomatic approach rather than wrapping logic in PythonOperators with hooks.",0,The code includes test harness boilerplate at the end with 'from tests.system.utils.watcher import watcher' and a comment indicating 'Just for system tests purpose'. This is internal testing infrastructure that should not be present in production-ready code.,1,"The DAG fulfills all requirements from the instruction: (1) creates a GCE instance using ComputeEngineInsertInstanceOperator, (2) executes remote commands via SSH using SSHOperator with ComputeEngineSSHHook, (3) deletes the instance using ComputeEngineDeleteInstanceOperator, and (4) demonstrates OS Login authentication by setting 'use_oslogin=True' and enabling OS Login in the instance metadata. The lifecycle management is properly chained.",0.6666666666666666
47,Ground Truth (HuggingFace),58,1,"The code uses the native AzureBatchOperator from airflow.providers.microsoft.azure.operators.batch, which is the idiomatic and correct approach for triggering Azure Batch tasks. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator with appropriate parameters.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code generated for a user.,1,"The DAG successfully demonstrates triggering a computational task on Azure Batch with a simple shell command ('/bin/bash -c 'set -e; set -o pipefail; echo hello world!; wait''). It showcases cloud-based distributed computing by configuring the batch pool, VM specifications, and task execution as requested in the instruction.",0.6666666666666666
48,Ground Truth (HuggingFace),59,1,"The code uses native Airflow operators appropriately: LocalFilesystemToS3Operator for file transfer, S3CreateBucketOperator and S3DeleteBucketOperator for S3 bucket management. Task dependencies are established using the chain() function. TaskFlow API (@task decorator) is used for simple Python functions. This is idiomatic Airflow usage.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. It also includes test-specific logic like 'sys_test_context_task = SystemTestContextBuilder().build()' and 'test_run = get_test_run(dag)'. These are clear indicators this is test code, not production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) dynamically generates a local file using the create_temp_file() task, (2) transfers it to AWS S3 using LocalFilesystemToS3Operator, (3) showcases task chaining with the chain() function, (4) demonstrates task dependencies, (5) includes error handling via trigger_rule=TriggerRule.ALL_DONE for cleanup tasks, and (6) shows cloud storage interaction with S3 bucket creation, file upload, and bucket deletion.",0.6666666666666666
49,Ground Truth (HuggingFace),60,1,"The code uses native Airflow operators (PythonOperator, PythonVirtualenvOperator, ExternalPythonOperator) from the standard providers package. It does not wrap logic unnecessarily in PythonOperator when better alternatives exist. The operators are used correctly for their intended purposes: PythonOperator for standard Python execution, PythonVirtualenvOperator for virtual environment execution, and ExternalPythonOperator for external Python binary execution.",0,"The code contains a clear hallucination at the end with the incomplete import statement 'from tests_common.test_u'. This appears to be test harness boilerplate that should not be present in production code. The presence of this testing module import violates the requirement for clean, production-ready code using only standard Airflow libraries.",1,"The DAG successfully fulfills the user instruction to create a multi-stage data processing scenario using Python operators with different runtime environments. It demonstrates: (1) Standard Python execution with PythonOperator (print_context, log_sql, sleeping tasks), (2) Virtual environment execution with PythonVirtualenvOperator (virtualenv_task), and (3) External Python context with ExternalPythonOperator (external_python_task). The pipeline maintains clear task dependencies (run_this >> log_the_sql >> sleeping_task >> virtualenv_task and run_this >> external_python_task >> virtualenv_task), showing a proper multi-stage workflow structure.",0.6666666666666666
50,Ground Truth (HuggingFace),61,1,"The code uses native Airflow GKE operators (GKECreateClusterOperator, GKEDeleteClusterOperator, GKEStartPodOperator) from the official Google Cloud provider package. It properly leverages deferrable mode for asynchronous execution and uses BashOperator for XCom result demonstration. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 1) Imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 2) Imports 'from tests_common.test_utils.watcher import watcher' and uses it in the DAG, 3) Imports 'from tests_common.test_utils.system_tests import get_test_run' and creates 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production-ready code.",1,"The DAG fully implements the requested workflow: 1) Creates a GKE cluster (GKECreateClusterOperator), 2) Deploys and runs sample workloads as pods (GKEStartPodOperator), 3) Demonstrates asynchronous task execution using deferrable=True on all GKE operators, 4) Shows cross-communication between tasks via XCom (pod_task_xcom_async pushes data, pod_task_xcom_result pulls it), 5) Deletes the cluster at the end (GKEDeleteClusterOperator with ALL_DONE trigger rule). All requirements are met.",0.6666666666666666
51,Ground Truth (HuggingFace),62,1,"The code uses native Airflow providers and operators appropriately. It leverages ComprehendCreateDocumentClassifierOperator, ComprehendCreateDocumentClassifierCompletedSensor, S3 operators, and ComprehendHook. The only PythonOperator usage (via @task decorator) is for the cleanup task which appropriately uses ComprehendHook for deletion, and for creating dynamic kwargs. This is idiomatic Airflow - using specific operators for AWS Comprehend and S3 operations rather than wrapping everything in generic PythonOperators.",0,"The code imports from 'system.amazon.aws.utils' which appears to be an internal testing module: 'from system.amazon.aws.utils import SystemTestContextBuilder'. It also uses 'SystemTestContextBuilder()' which is test harness boilerplate, not production-ready code. This is a clear hallucination/leakage of internal testing infrastructure into what should be production DAG code.",0,"The code is incomplete - it cuts off mid-line ('dest_bucket_key') and doesn't show the complete DAG structure. While it does implement a task_group for the classifier workflow with creation, monitoring (sensor), and cleanup (delete_classifier), the incomplete nature means we cannot verify if all required components are present. Additionally, the instruction asks for 'proper error handling and resource management' but the error handling is minimal (only TriggerRule.ALL_DONE on the delete task). The code lacks comprehensive error handling mechanisms, retry logic configuration, or robust resource cleanup patterns beyond the basic delete operation.",0.3333333333333333
52,Ground Truth (HuggingFace),63,1,"The code uses native Airflow operators designed for the specific tasks: TrinoToGCSOperator for transferring data from Trino to GCS, BigQueryCreateTableOperator for creating external tables, BigQueryCreateEmptyDatasetOperator for dataset creation, and GCS operators for bucket management. This is idiomatic Airflow - using provider-specific operators rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear hallucination/test harness imports that should not be in production code. Specifically: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' and 'from system.openlineage.operator import OpenLineageTestOperator'. These are internal testing modules. Additionally, the code uses test-related environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and includes comments like '[START howto_operator_trino_to_gcs_basic]' which are documentation/testing artifacts.",0,"The code is incomplete (truncated at the end with 'SOURCE_SCHEMA_COLU') and lacks proper task dependencies to demonstrate a complete step-by-step pipeline. While it includes components for inter-cloud data transfer (Trino to GCS) and external table creation (BigQuery), the DAG doesn't show the complete orchestration flow with proper task chaining. The instruction asked for a showcase of technical orchestration demonstrating cloud data integration techniques, but the incomplete code and missing task dependencies fail to deliver a working, demonstrable pipeline.",0.3333333333333333
53,Ground Truth (HuggingFace),64,1,"The code uses native Airflow Dataplex operators from airflow.providers.google.cloud.operators.dataplex, such as DataplexCatalogCreateEntryGroupOperator, DataplexCatalogCreateEntryTypeOperator, and DataplexCatalogCreateAspectTypeOperator. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which is an internal testing module. This is test harness boilerplate that should not appear in production-ready code. Additionally, the code references SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, which are testing artifacts.",0,"The code is incomplete - it cuts off mid-task definition ('create_aspect_type' is not completed). While it demonstrates creating entry groups, entry types, and begins to create aspect types as requested, the DAG is not functional due to the truncation. Additionally, the instruction asks for 'systematically creating and configuring' these components with 'clear, configurable parameters', but the code lacks task dependencies and a complete workflow that would demonstrate proper orchestration of these metadata management tasks.",0.3333333333333333
54,Ground Truth (HuggingFace),65,1,"The code uses native Airflow providers and operators designed specifically for the task. It imports and uses CreateAutoMLImageTrainingJobOperator, DeleteAutoMLTrainingJobOperator, CreateDatasetOperator, DeleteDatasetOperator, and ImportDataOperator from airflow.providers.google.cloud.operators.vertex_ai. These are the appropriate, idiomatic operators for Google Cloud Vertex AI AutoML tasks rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, at the end of the DAG, it imports 'from tests_common.test_utils.watcher import watcher' and includes a comment about needing the watcher for test success/failure marking. The line 'list(dag.tasks) >> watc' (appears to be truncated 'watcher') is test infrastructure code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, and includes comments like '# For testing only' and '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN', which are all indicators of test harness code rather than production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) Data ingestion - implemented via CreateDatasetOperator and ImportDataOperator to process a predefined dataset from GCS; (2) Model training - implemented via CreateAutoMLImageTrainingJobOperator with configurable training parameters (training_fraction_split, validation_fraction_split, test_fraction_split, budget_milli_node_hours, etc.); (3) Automated resource management - implemented with cleanup tasks (DeleteAutoMLTrainingJobOperator and DeleteDatasetOperator) using TriggerRule.ALL_DONE; (4) Cloud environment scalability - uses Google Cloud Vertex AI AutoML which is a managed, scalable cloud service. The workflow specifically addresses object detection training as requested.",0.6666666666666666
55,Ground Truth (HuggingFace),68,1,"The code uses native Airflow Redis providers and operators (RedisPublishOperator, RedisKeySensor) instead of wrapping Redis logic in PythonOperators with hooks. This is the idiomatic approach for working with Redis in Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and represent test infrastructure leakage.",0,"The instruction asks for a system where 'one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding' with 'real-time message coordination and signaling between different stages'. The code only demonstrates: (1) publishing a message to a channel, and (2) waiting for a key to exist. It does NOT use RedisPubSubSensor (which is mentioned in the docstring but not implemented) to actually wait for the published message, creating a disconnect between the publish and sensor tasks. The workflow doesn't demonstrate proper message coordination - the key_sensor_task waits for 'your_key' which has no relationship to the 'Start processing' message published to 'your_channel'. A proper implementation would have tasks waiting for the published signal before proceeding.",0.3333333333333333
56,Ground Truth (HuggingFace),69,0,"The code uses 'TimeDeltaSensorAsync' from 'airflow.sensors.time_delta', but this is not a standard Airflow 2.7.2 operator. The standard async sensor in Airflow 2.7.2 is 'TimeDeltaSensor' with deferrable=True parameter, or it may be available through the 'astronomer-providers' package. The import path 'airflow.sensors.time_delta.TimeDeltaSensorAsync' does not exist in core Airflow 2.7.2, making this non-idiomatic for the specified version.",0,"The code imports 'TimeDeltaSensorAsync' from 'airflow.sensors.time_delta', which does not exist in standard Airflow 2.7.2. This appears to be either a hallucinated import path or confusion with third-party providers (like astronomer-providers). The correct approach in Airflow 2.7.2 would be to use 'TimeDeltaSensor' with 'deferrable=True' or import from the appropriate provider package if using an external library.",1,"Despite the implementation issues, the code does fulfill the user's instruction conceptually. It creates a DAG that demonstrates a controlled delay scenario with a time-based sensor (30 seconds wait) followed by a completion task, showing how asynchronous sensors can manage workflow timing. The pipeline structure matches the requirement of waiting for a predetermined time interval before proceeding to the next task.",0.3333333333333333
57,Ground Truth (HuggingFace),70,1,"The DAG uses native Airflow operators appropriately: GCSCreateBucketOperator, GCSDeleteBucketOperator, GCSFileTransformOperator, and GCSToGCSOperator from the Google Cloud provider package. These are the idiomatic, provider-specific operators designed for GCS operations rather than wrapping logic in PythonOperator with hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from system.openlineage.operator import OpenLineageTestOperator', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. The DAG also includes test setup/teardown comments and a watcher() call, which are all testing artifacts that leak into what should be production code.",1,"The DAG fulfills all requirements from the instruction: (1) demonstrates cloud storage file manipulation using GCS operators, (2) showcases task dependencies with clear upstream/downstream relationships, (3) includes file transfer between buckets via GCSToGCSOperator, (4) executes a custom transformation script using GCSFileTransformOperator, (5) implements proper error handling through trigger rules (TriggerRule.ALL_DONE), and (6) includes cleanup with the delete_bucket task that runs regardless of upstream success/failure.",0.6666666666666666
58,Ground Truth (HuggingFace),71,1,"The code uses native Airflow S3 operators from airflow.providers.amazon.aws.operators.s3 (S3CreateBucketOperator, S3CreateObjectOperator, S3PutBucketTaggingOperator, S3GetBucketTaggingOperator, S3DeleteBucketTaggingOperator, S3ListOperator, S3ListPrefixesOperator, etc.) instead of wrapping S3Hook calls in PythonOperators. This is the idiomatic Airflow approach for S3 operations.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is clearly a test harness module from Airflow's internal testing framework. It also uses 'sys_test_context_task = SystemTestContextBuilder().build()' and 'test_context = sys_test_context_task()' which are test boilerplate patterns, not production-ready code. This is a clear hallucination/leakage of test infrastructure into what should be production DAG code.",1,"The DAG fulfills the user instruction to demonstrate S3 data management workflow including: bucket creation (S3CreateBucketOperator), object manipulation (S3CreateObjectOperator for uploading files, S3ListOperator for listing contents), and metadata tagging (S3PutBucketTaggingOperator, S3GetBucketTaggingOperator, S3DeleteBucketTaggingOperator). All requested operations are present in the code, even though the code is incomplete at the end.",0.6666666666666666
59,Ground Truth (HuggingFace),73,1,"The code uses native Azure Blob Storage sensors (WasbBlobSensor and WasbPrefixSensor) from airflow.providers.microsoft.azure.sensors.wasb, which are the idiomatic Airflow operators designed specifically for this task. It does not wrap logic in PythonOperator with hooks, but instead uses the appropriate provider operators.",0,The code contains test harness boilerplate markers like '# [START wasb_blob_sensor]' and '# [END wasb_blob_sensor]' which are typically used in Airflow's example/test documentation. These are not production-ready patterns and indicate this was copied from internal testing or documentation examples rather than being clean production code.,0,"The instruction explicitly asks to 'trigger downstream processes based on the presence of expected files or file groups.' The provided DAG only defines sensors but does not include any downstream tasks or processes that would be triggered after the files are detected. The sensors are not connected to any downstream operators, making this an incomplete implementation of the requested monitoring workflow.",0.3333333333333333
60,Ground Truth (HuggingFace),76,1,"The code uses native Airflow operators designed for the specific tasks: GCSToSFTPOperator for transfers, SFTPSensor for verification, GCSCreateBucketOperator for bucket management, and LocalFilesystemToGCSOperator for uploads. It does not wrap functionality in PythonOperator with hooks, which would be non-idiomatic. All operators are from official Airflow providers (airflow.providers.google.cloud and airflow.providers.sftp).",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a testing module import. This is internal test infrastructure boilerplate that should not appear in production code. Additionally, the code uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' which are test harness patterns. This indicates the code is from Airflow's system tests rather than being clean, production-ready code.",1,"The DAG fulfills the instruction requirements: (1) Showcases advanced cloud storage transfer techniques using GCS and SFTP operators - demonstrated through GCSToSFTPOperator with various configurations; (2) Handles various file transfer scenarios - includes single file copy, single file move, directory copy, and directory move operations; (3) Implements robust file transfer verification mechanisms - uses SFTPSensor to verify each transfer operation; (4) Demonstrates different transfer methods - shows copy vs move operations, single files vs directories; (5) Ensures data integrity through sensor checks - each transfer is followed by a corresponding SFTPSensor task. The workflow is comprehensive and addresses all aspects of the instruction, though the code is incomplete (cuts off at the end).",0.6666666666666666
61,Ground Truth (HuggingFace),77,0,"The DAG uses WeaviateHook wrapped in @task decorated functions instead of using native Weaviate Operators. Airflow providers.weaviate package includes operators like WeaviateIngestOperator that should be used for idiomatic Airflow patterns. The code wraps hook methods (create_collection, batch_data, delete_collections) inside PythonOperator-style @task decorators, which is the anti-pattern described in the scoring criteria.",1,"The code uses only standard Airflow libraries (airflow.providers.weaviate.hooks.weaviate.WeaviateHook, airflow.decorators) and legitimate Weaviate client libraries (weaviate.classes.config, weaviate.collections.classes.config). There are no imports from test utilities or test harness boilerplate. The code is clean and production-ready from an import perspective.",0,"The DAG code is incomplete - it cuts off mid-statement at 'batch_data_with_vectors(data_with_vectors[""return_value""]'. More critically, the workflow is missing key dependencies and orchestration. The task 'batch_data_without_vectors' is defined but never called in the DAG dependencies. The data loading tasks reference files ('jeopardy_data_without_vectors.json', 'jeopardy_data_with_vectors.json') that would not exist in a typical Airflow environment without prior setup. The instruction asks for a complete pipeline showing batch import, different vectorization approaches, and collection management - while the components exist, the incomplete code and missing task dependencies mean it doesn't fulfill the complete workflow requirement.",0.3333333333333333
62,Ground Truth (HuggingFace),78,1,"The code uses the specific LevelDBOperator from airflow.providers.google.leveldb.operators.leveldb, which is the idiomatic way to interact with LevelDB in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,"The code contains multiple test harness imports and boilerplate that should not be in production DAG code. Specifically: 'import pytest', 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. The watcher() function call and test infrastructure are clear indicators this is test code, not production-ready code.",1,"The DAG fulfills the user instruction by demonstrating basic data persistence and retrieval using LevelDB (an embedded, file-based key-value store). It creates a sequential task flow with a 'get' operation (get_key_leveldb_task) followed by a 'put' operation (put_key_leveldb_task), showcasing both reading and writing operations as requested.",0.6666666666666666
63,Ground Truth (HuggingFace),79,1,"The code uses native Databricks provider operators (DatabricksReposCreateOperator, DatabricksReposUpdateOperator, DatabricksReposDeleteOperator, DatabricksSubmitRunOperator) instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Databricks operations.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is test/example code, not production-ready code.",1,"The DAG fully implements the requested workflow: it creates a Databricks repository (create_repo), updates it (update_repo), executes a data preparation notebook (notebook_task), and deletes the repository (delete_repo). The task dependencies are properly chained to demonstrate the complete repository lifecycle management with notebook execution as requested.",0.6666666666666666
64,Ground Truth (HuggingFace),80,1,"The code uses native Airflow providers and operators designed specifically for the task. It properly uses GKECreateClusterOperator, GKEStartPodOperator, and GKEDeleteClusterOperator from airflow.providers.google.cloud.operators.kubernetes_engine. These are the idiomatic, purpose-built operators for GKE operations rather than wrapping logic in PythonOperator with hooks. The code also appropriately uses BashOperator for the XCom result demonstration.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', then calls 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. The code also uses test-specific environment variables like SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT.",1,"The code fully adheres to the instruction requirements. It demonstrates: (1) Dynamic cluster provisioning using GKECreateClusterOperator to programmatically spin up a GKE cluster, (2) Ephemeral pod execution with GKEStartPodOperator running sample workloads (perl and alpine containers) with on_finish_action='delete_pod' for ephemeral behavior, (3) Clean infrastructure teardown using GKEDeleteClusterOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of task success/failure. The workflow illustrates cloud-native Kubernetes data processing with proper resource lifecycle management.",0.6666666666666666
65,Ground Truth (HuggingFace),81,1,"The code uses appropriate Airflow operators for the tasks: BashOperator for system commands (date, sleep, echo) and DockerOperator from airflow.providers.docker.operators.docker for containerized operations. This is idiomatic Airflow - it does not wrap these operations in PythonOperator with hooks, but uses the native operators designed for these purposes.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)' at the bottom. These are internal testing utilities that represent hallucination/leakage from test examples into what should be production-ready code.",1,"The DAG fulfills the instruction requirements: it simulates a system monitoring and testing pipeline with system commands (validates system time with 'date' command in t1), performs wait operations (sleep 5 in t2), executes a containerized process (DockerOperator with sleep 30 in t3), and demonstrates task dependency with t1 branching to both t2 and t3, which then converge to t4. The execution strategy showcases parallel execution (t2 and t3) followed by a final task.",0.6666666666666666
66,Ground Truth (HuggingFace),82,0,"The code uses TaskFlow API @task decorators with raw boto3 clients instead of leveraging Airflow's AWS provider operators. For AWS services like RDS, EC2, S3, and DMS, Airflow provides specific operators (e.g., RdsCreateDbInstanceOperator, S3CreateBucketOperator, EC2CreateInstanceOperator) that should be used instead of wrapping boto3 calls in Python tasks. This is the anti-pattern of using PythonOperator/TaskFlow + raw SDK instead of native Operators.",0,"The code is incomplete (cuts off at 'def delete_dms_assets('), missing critical imports (boto3, create_engine, Table, MetaData, Column, String, json, TriggerRule, task decorator), and references undefined constants (RDS_PROTOCOL, RDS_USERNAME, RDS_PASSWORD, RDS_ENGINE, TABLE_HEADERS). While it doesn't import test harness modules, the incomplete and non-runnable state with missing dependencies makes it not production-ready.",1,"Despite implementation issues, the code does attempt to fulfill the instruction's requirements: it creates cloud infrastructure (security groups, RDS endpoints, DMS replication instances), configures security settings (security group with ingress rules), and executes data migration (creates sample table, sets up DMS source/target endpoints for replication from RDS to S3). The modular structure with separate tasks for each component aligns with the requested pipeline design.",0.3333333333333333
67,Ground Truth (HuggingFace),83,0,"The DAG uses @task decorators (TaskFlow API) for core business logic (fetch_tweets, clean_tweets, analyze_tweets, transfer_to_db) but these are empty placeholder functions that would require PythonOperator-style implementations. While HiveOperator is used appropriately for Hive operations, the critical tasks like fetching tweets from Twitter API, sentiment analysis, and database transfer are not using idiomatic provider-specific operators. For example, it should use HttpOperator or a Twitter provider for API calls, and a proper database operator (e.g., MySQLOperator, PostgresOperator) for database transfers instead of placeholder Python functions.",0,"The code is incomplete and contains several issues: (1) Missing imports - 'os', 'datetime', 'date', 'timedelta', 'DAG', 'task', and 'HiveOperator' are not imported; (2) The code is truncated and incomplete - the second for loop is cut off mid-statement ('task_id' without completion); (3) References 'HiveOperator' which is not imported from the correct path for Airflow 3.0.6; (4) Contains a caveat comment stating 'This Dag will not run because of missing scripts' which suggests this is example/test code rather than production-ready code.",0,"The DAG only partially fulfills the instruction requirements. While it has placeholder tasks for the main components (fetch tweets, clean, analyze, transfer to database), none of these are actually implemented - they are empty functions with docstrings. The instruction asks for a pipeline that 'collects tweets from multiple accounts, processes them for insights, and stores the results in a database' with sentiment analysis, but the code provides no actual implementation of Twitter API integration, sentiment analysis logic, or database storage. The workflow structure is outlined but not functional. Additionally, the use of HDFS and Hive seems like an architectural choice not explicitly requested in the instruction, which asked for storing results 'in a database' for business intelligence.",0.0
68,Ground Truth (HuggingFace),85,1,"The code uses appropriate native Airflow operators for all tasks: BigQueryCreateEmptyDatasetOperator, BigQueryCreateEmptyTableOperator, BigQueryToBigQueryOperator, BigQueryToGCSOperator, GCSCreateBucketOperator, and GCSDeleteBucketOperator. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is test code, not production-ready code.",1,"The DAG fully implements the requested functionality: (1) programmatically creates table schemas using BigQueryCreateEmptyTableOperator with schema_fields, (2) copies data between BigQuery tables using BigQueryToBigQueryOperator, (3) exports data to cloud storage using BigQueryToGCSOperator, and (4) includes complete workflow stages with setup (create bucket/dataset/tables), data movement (copy and export), and cleanup (delete dataset/bucket). All requirements are met.",0.6666666666666666
69,Ground Truth (HuggingFace),86,1,"The code uses Airflow 2.7+ native setup/teardown decorators (@setup, @teardown) and the as_teardown() method, which are the idiomatic way to handle resource lifecycle management in modern Airflow. It leverages TaskFlow API with @task decorator and demonstrates proper use of task_group. No generic PythonOperator wrapping is used where native features exist.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.decorators, airflow.models.dag, and pendulum for dates). There are no imports from internal testing modules like 'tests_common.test_utils' or test harness boilerplate like 'get_test_run()'. All imports and patterns are legitimate Airflow 2.7.2 features.",1,"The DAG fully demonstrates setup and teardown tasks for a cloud infrastructure workflow as requested. It shows: (1) resource allocation via @setup decorated tasks that return cluster IDs, (2) cleanup via @teardown decorated tasks that receive and tear down resources, (3) multiple task stages with both outer and inner (task_group) setup/teardown patterns, (4) proper dependency management using as_teardown() method and context manager syntax, and (5) clear demonstration of how resources are managed across the pipeline lifecycle. The instruction to 'demonstrate' and 'show how to manage' is satisfied with comprehensive examples and inline comments explaining the patterns.",1.0
70,Ground Truth (HuggingFace),87,0,"The code uses PythonOperator (via @task decorator) with boto3 client directly for creating and deleting security configurations instead of using native Airflow operators. Specifically, `configure_security_config()` and `delete_security_config()` wrap boto3.client('emr') calls in TaskFlow tasks rather than using dedicated EMR operators. While EMR operators are used for cluster management (EmrCreateJobFlowOperator, EmrAddStepsOperator, etc.), the security configuration management should ideally use Airflow's native patterns or hooks rather than raw boto3 calls wrapped in @task decorators.",0,"The code imports and uses testing utilities that should not be present in production code: `from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder`. The line `sys_test_context_task = SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN_KEY).build()` and its usage with `test_context = sys_test_context_task()` are clear indicators of test harness boilerplate that has leaked into what should be production-ready code.",1,"The DAG fulfills all the requirements from the user instruction: (1) Creates an S3 bucket using S3CreateBucketOperator, (2) Configures security settings via the security configuration task, (3) Launches an EMR cluster with EmrCreateJobFlowOperator, (4) Executes a Spark step (SparkPi calculation defined in SPARK_STEPS), and (5) Handles cluster lifecycle management with termination logic (delete_security_config with ALL_DONE trigger rule, and the code structure suggests cleanup operations). The workflow demonstrates AWS cloud infrastructure orchestration with dynamic security configuration and EMR cluster management as requested.",0.3333333333333333
71,Ground Truth (HuggingFace),89,1,"The code uses native Airflow providers and operators specifically designed for Cloud SQL operations (CloudSQLCreateInstanceOperator, CloudSQLExportInstanceOperator, CloudSQLImportInstanceOperator, CloudSQLInstancePatchOperator, etc.) and GCS operations (GCSCreateBucketOperator, GCSBucketCreateAclEntryOperator, etc.). It does not wrap these operations in PythonOperator with hooks, which demonstrates idiomatic Airflow usage.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. Additionally, the code uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' which are typical of test harness boilerplate. The docstring also indicates this is an 'Example' DAG rather than production code. These are clear indicators of test/example code leakage rather than clean, production-ready code.",0,"The code is incomplete - it only defines configuration bodies and imports operators but does not actually instantiate a DAG or create any tasks. There is no DAG definition with task dependencies that would orchestrate the workflow. The instruction asks for a complete DAG that 'orchestrates complex Cloud SQL operations' with a 'systematic workflow', but the provided code cuts off before defining the actual DAG structure, tasks, or their dependencies. It's essentially just setup/configuration without the actual orchestration logic.",0.3333333333333333
72,Ground Truth (HuggingFace),90,1,"The DAG correctly uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. It does not wrap these in PythonOperators or use hooks directly, which is the idiomatic Airflow approach for Google Cloud Dataproc operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes test setup code like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and represent test infrastructure leakage.",1,"The DAG fully implements the requested functionality: it orchestrates a cloud-based data processing job using Google Dataproc with sequential task dependencies (create_cluster >> presto_task >> delete_cluster). It handles infrastructure provisioning (cluster creation), job execution (Presto job submission), and cleanup (cluster deletion) with clear, predictable task progression. The use of TriggerRule.ALL_DONE on the delete_cluster task ensures robust cleanup even if the job fails.",0.6666666666666666
73,Ground Truth (HuggingFace),91,1,"The code uses native Airflow operators designed for the specific tasks: OpenAIEmbeddingOperator for generating embeddings and PgVectorIngestOperator for inserting data into PostgreSQL. The setup and teardown tasks use PgVectorHook directly within @task decorators, which is acceptable for DDL operations where no specific operator exists. This is idiomatic Airflow usage.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG fulfills the user instruction completely. It creates an automated workflow that: (1) generates vector embeddings for text data using OpenAI (via OpenAIEmbeddingOperator), (2) stores them in a PostgreSQL database with vector capabilities (using PgVectorIngestOperator with pgvector extension), and (3) demonstrates the transformation of text into machine-readable vector representations. The pipeline includes proper setup (table creation with vector column) and cleanup (teardown task), making it a complete demonstration for ML/semantic search applications.",0.6666666666666666
74,Ground Truth (HuggingFace),92,1,"The code uses native Databricks operators from airflow.providers.databricks.operators.databricks including DatabricksCreateJobsOperator, DatabricksRunNowOperator, DatabricksSubmitRunOperator, and DatabricksNotebookOperator. These are the idiomatic, provider-specific operators designed for Databricks integration rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear indicators of test harness boilerplate. It includes 'ENV_ID = os.environ.get(""SYSTEM_TESTS_ENV_ID"")' which is a testing pattern, and the DAG_ID is set to 'example_databricks_operator' which follows example/test naming conventions. The docstring and comments also indicate this is example/test code (e.g., '[START howto_operator_databricks_json]' markers are documentation generation tags). Additionally, the code appears to be truncated at the end ('[END howt'), suggesting it's incomplete.",1,"The code fulfills the user instruction comprehensively. It demonstrates: (1) Dynamic job creation using DatabricksCreateJobsOperator with both JSON and named parameters, (2) Running notebooks via DatabricksSubmitRunOperator and notebook_task configuration, (3) Managing computational resources through flexible cluster configuration (new_cluster with spark_version, node_type_id, num_workers, aws_attributes), and (4) Notebook-based data preparation strategies (notebook_path references like '/Users/airflow@example.com/PrepareData'). The workflow shows task dependencies and demonstrates scalable data processing patterns.",0.6666666666666666
75,Ground Truth (HuggingFace),93,1,"The code uses the native CloudTranslateTextOperator from airflow.providers.google.cloud.operators.translate, which is the idiomatic way to interact with Google Cloud Translate in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for the task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than clean production-ready code.",1,"The DAG successfully demonstrates task chaining and cross-task communication as requested. It translates text using CloudTranslateTextOperator, then accesses the translation result through XCom using a BashOperator with xcom_pull. The tasks are chained sequentially (product_set_create >> translation_access), showcasing how different operators can interact and pass data between tasks in a workflow.",0.6666666666666666
76,Ground Truth (HuggingFace),95,1,"The code uses native Airflow Spanner operators from airflow.providers.google.cloud.operators.spanner, including SpannerDeployInstanceOperator, SpannerDeployDatabaseInstanceOperator, SpannerUpdateDatabaseInstanceOperator, SpannerDeleteDatabaseInstanceOperator, and SpannerDeleteInstanceOperator. It does not wrap Spanner logic in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package (airflow.providers.google.cloud.operators.spanner) and core Airflow modules (airflow.models.dag, airflow.utils.trigger_rule). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code is clean and production-ready.",1,"The DAG comprehensively addresses all requirements from the user instruction: (1) Instance deployment via SpannerDeployInstanceOperator, (2) Database schema evolution through SpannerDeployDatabaseInstanceOperator and SpannerUpdateDatabaseInstanceOperator with DDL statements, (3) Idempotent database modifications demonstrated with operation_id parameter in two update tasks, and (4) Various administrative tasks including instance updates, database creation, schema updates, and cleanup operations. The code is incomplete (cuts off at spanner_instance_query_task), but the visible portion fully demonstrates the requested operational capabilities.",1.0
77,Ground Truth (HuggingFace),96,1,"The code uses native Airflow Vertex AI operators (PromptLanguageModelOperator, GenerateTextEmbeddingsOperator, PromptMultimodalModelOperator, PromptMultimodalModelWithMediaOperator) from the Google Cloud provider package. These are the appropriate, idiomatic operators for interacting with Vertex AI generative models rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'list(dag.tasks) >> watcher()', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production DAG code.",1,"The DAG successfully demonstrates multiple generative AI model interactions using Vertex AI as requested. It showcases: (1) text generation via PromptLanguageModelOperator with text-bison model, (2) embedding generation via GenerateTextEmbeddingsOperator with textembedding-gecko model, (3) multimodal text processing via PromptMultimodalModelOperator with gemini-pro, and (4) multimodal vision processing via PromptMultimodalModelWithMediaOperator with gemini-pro-vision analyzing an image. This fulfills the requirement for an AI-powered content analysis pipeline with text generation, embedding, and multimodal processing capabilities.",0.6666666666666666
78,Ground Truth (HuggingFace),97,1,The code uses native Azure-specific operators (LocalFilesystemToADLSOperator and ADLSDeleteOperator) from the airflow.providers.microsoft.azure package rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Azure file operations.,0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', uses 'list(dag.tasks) >> watcher()', imports 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test infrastructure that have leaked into what should be production-ready code.",1,"The DAG fulfills the user instruction completely. It showcases task sequencing (upload_file >> delete_file), demonstrates cloud file management using Azure operators, and illustrates how to programmatically upload (LocalFilesystemToADLSOperator) and remove (ADLSDeleteOperator) files from a remote storage system (Azure Data Lake Storage).",0.6666666666666666
79,Ground Truth (HuggingFace),98,0,"The code uses TaskFlow API @task decorators with raw boto3 clients instead of using Airflow's native SageMaker operators. Airflow provides specific operators like SageMakerTrainingOperator, SageMakerModelOperator, SageMakerEndpointConfigOperator, and SageMakerEndpointOperator from airflow.providers.amazon.aws.operators.sagemaker that should be used for these tasks. The current implementation wraps boto3 calls in Python tasks, which is the anti-pattern described in the scoring criteria.",0,The code references 'KNN_IMAGES_BY_REGION' which is not defined in the provided code snippet and appears to be from a test harness or system test file (as evidenced by the error message mentioning 'system testfile'). This suggests the code was copied from or influenced by internal testing modules rather than being production-ready code.,1,"The code appears to fulfill the core requirements: it handles data upload setup (set_up task with S3 configuration), model training (training_config), endpoint deployment (endpoint_config_config and deploy_endpoint_config), endpoint invocation (call_endpoint), and automatic cleanup of resources (delete_endpoint, delete_endpoint_config, archive_logs tasks with ALL_DONE trigger rule). All major components of the ML pipeline requested in the instruction are present, even though the implementation approach is not idiomatic.",0.3333333333333333
80,Ground Truth (HuggingFace),99,1,"The code uses the native PineconeIngestOperator from airflow.providers.pinecone.operators.pinecone, which is the idiomatic way to interact with Pinecone in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for the task.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. This is a clear example of test infrastructure leaking into what should be clean DAG code.,1,"The code fulfills the user instruction to create a data pipeline that showcases: (1) vector embedding ingestion - demonstrated with input_vectors containing vector data, (2) flexible metadata handling - shown with optional metadata dict in the first vector, (3) batch processing capabilities - implemented via the batch_size parameter, and (4) programmatic insertion into Pinecone using Airflow's integration - accomplished through the PineconeIngestOperator. All requested features are present.",0.6666666666666666
81,Ground Truth (HuggingFace),100,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Fusion tasks. It imports and uses CloudDataFusionCreateInstanceOperator, CloudDataFusionCreatePipelineOperator, CloudDataFusionStartPipelineOperator, CloudDataFusionDeleteInstanceOperator, and other specialized operators from airflow.providers.google.cloud.operators.datafusion. It also uses CloudDataFusionPipelineStateSensor for monitoring. There is one @task decorator usage for getting artifact versions, which is appropriate for a small utility function. The code does not wrap Data Fusion operations in generic PythonOperators with hooks, making it idiomatic Airflow.",0,"The code contains a clear hallucination/test leakage. It imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which is an internal testing module reference. This is not a standard Airflow library and appears to be part of Airflow's internal system test infrastructure. Additionally, the code uses environment variables like 'SYSTEM_TESTS_GCP_PROJECT' and 'SYSTEM_TESTS_ENV_ID', which are typical of test harness boilerplate rather than production code. The comments and structure also suggest this is adapted from Airflow's example/test code rather than being clean production-ready code.",1,"The code fulfills the user instruction to orchestrate the complete lifecycle of a Cloud Data Fusion instance. It includes: (1) Creation - CloudDataFusionCreateInstanceOperator for instance creation, (2) Pipeline deployment - CloudDataFusionCreatePipelineOperator for deploying pipelines, (3) Execution - CloudDataFusionStartPipelineOperator for running pipelines with CloudDataFusionPipelineStateSensor for monitoring, and (4) Cleanup - CloudDataFusionDeletePipelineOperator and CloudDataFusionDeleteInstanceOperator for teardown. The code also includes supporting infrastructure (GCS buckets) and additional operations like GetInstance, UpdateInstance, RestartInstance, ListPipelines, and StopPipeline, demonstrating comprehensive cloud data infrastructure management using Google Cloud services.",0.6666666666666666
82,Ground Truth (HuggingFace),101,1,The code properly uses MsSqlOperator for SQL operations and MsSqlHook for programmatic data insertion. It demonstrates idiomatic Airflow patterns by using native operators (MsSqlOperator) for most database operations and only uses the Hook pattern (via @dag.task decorator) for the insert operation where row-level insertion is more appropriate. This is the correct use of Airflow providers rather than wrapping everything in PythonOperators.,0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'pytest' and uses 'pytest.skip()' for conditional imports. Additionally, it references 'ENV_ID = os.environ.get(""SYSTEM_TESTS_ENV_ID"")' which is a testing pattern. These are clear indicators of test/example code leakage into what should be production-ready code.",0,"The DAG is incomplete - it cuts off mid-definition with an incomplete task dependency chain '(create_table_mssql_task >> insert_mssql_hook() >>'. While it does demonstrate data insertion, parameterized queries, and different methods of database interaction (operators vs hooks) as requested, the incomplete nature means it cannot be executed. Additionally, the workflow creates a Country table and Users table but the dependency chain and overall orchestration is not fully defined, making it impossible to verify if it properly handles the complete data processing pipeline as instructed.",0.3333333333333333
83,Ground Truth (HuggingFace),102,1,"The code uses native Airflow operators designed for the specific tasks: OracleToGCSOperator for transferring data from Oracle to GCS, GCSCreateBucketOperator for bucket creation, and GCSDeleteBucketOperator for bucket deletion. This is idiomatic Airflow - using provider-specific operators rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'list(dag.tasks) >> watcher()', and 'test_run = get_test_run(dag)'. These are testing utilities not appropriate for production enterprise-level code. The comments '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' also indicate this is test code rather than production-ready.",1,"The DAG fulfills all requirements from the instruction: (1) Extracts data from Oracle using SQL query, (2) Transfers data to Google Cloud Storage in CSV format via the OracleToGCSOperator with export_format='csv', (3) Ensures proper resource management through automated bucket creation (GCSCreateBucketOperator) and deletion (GCSDeleteBucketOperator with ALL_DONE trigger rule). The workflow demonstrates the complete data extraction and cloud storage transfer process as requested.",0.6666666666666666
84,Ground Truth (HuggingFace),103,1,"The code uses native SageMaker operators from airflow.providers.amazon.aws.operators.sagemaker (SageMakerCreateNotebookOperator, SageMakerDeleteNotebookOperator, SageMakerStartNoteBookOperator, SageMakerStopNotebookOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for AWS SageMaker operations.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test code, not production-ready DAG code. The SystemTestContextBuilder and watcher patterns are test infrastructure leakage.",1,"The DAG successfully demonstrates sequential management of AWS SageMaker notebook resources as requested. It shows: 1) Creating a notebook instance, 2) Stopping it, 3) Starting it again (manipulation), 4) Stopping it before deletion, and 5) Deleting it (cleanup). The chain() function clearly establishes the sequential dependencies, fulfilling the requirement to 'illustrate how to programmatically control cloud computing infrastructure through defined task dependencies' with a complete create-manipulate-cleanup lifecycle.",0.6666666666666666
85,Ground Truth (HuggingFace),104,1,"The code correctly uses the OracleStoredProcedureOperator from airflow.providers.oracle.operators.oracle, which is the idiomatic and native Airflow operator designed specifically for calling Oracle stored procedures. It does not wrap logic in PythonOperator with hooks, demonstrating proper use of Airflow's provider pattern.",1,The code is clean and production-ready. It only imports standard Airflow libraries (airflow.DAG and airflow.providers.oracle.operators.oracle.OracleStoredProcedureOperator). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or any test harness boilerplate code.,1,The DAG successfully demonstrates the requested functionality: calling Oracle stored procedures with different parameter passing techniques. It showcases two distinct approaches - list-based parameters (positional with input value 3 and output type int) and dictionary-based parameters (named parameters 'val_in' and 'val_out'). This directly addresses the instruction to demonstrate 'input and output parameter handling for database operations' using 'different parameter passing techniques'.,1.0
86,Ground Truth (HuggingFace),106,1,"The DAG uses native Airflow GCS operators throughout: GCSCreateBucketOperator for bucket creation, GCSToGCSOperator for file transfers/copying, GCSListObjectsOperator for listing, GCSDeleteObjectsOperator for file deletion, and GCSDeleteBucketOperator for bucket deletion. No PythonOperator + Hook anti-patterns are present. This is idiomatic Airflow code that leverages the Google Cloud provider operators as intended.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. Additionally, it uses 'list(dag.tasks) >> watcher()' which is test infrastructure code. These are not production-ready patterns and represent test system leakage into what should be a clean DAG example.",1,"The DAG fully satisfies the user instruction to design a data migration workflow demonstrating GCS bucket management. It creates two dynamically generated buckets (BUCKET_NAME_SRC and BUCKET_NAME_DST with ENV_ID suffixes), performs file transfer operations (upload and copy between buckets), lists bucket contents, deletes files, and cleans up by deleting both buckets. All requested operations (creating, copying, and deleting files and buckets) are present and properly orchestrated.",0.6666666666666666
87,Ground Truth (HuggingFace),107,1,"The code uses native Airflow operators specifically designed for Google Cloud Datastore operations: CloudDatastoreBeginTransactionOperator and CloudDatastoreRollbackOperator from airflow.providers.google.cloud.operators.datastore. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also adds a watcher task with 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production-ready code.",1,The code fully implements the requested functionality: it creates an Airflow pipeline that demonstrates safe data transaction practices in Google Cloud Datastore by starting a transaction (CloudDatastoreBeginTransactionOperator) and then safely rolling it back (CloudDatastoreRollbackOperator) without persisting data modifications. The workflow clearly shows the complete transaction management scenario as requested.,0.6666666666666666
88,Ground Truth (HuggingFace),108,1,"The code uses native Airflow operators appropriately: PostgresToGCSOperator for data transfer, ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator for GCE management, GCSCreateBucketOperator and GCSDeleteBucketOperator for GCS bucket operations, and SQLExecuteQueryOperator for database operations. It does not wrap these in PythonOperators with hooks, which would be anti-idiomatic.",0,"The code appears to be from Airflow's system tests or example suite, as evidenced by: (1) the docstring mentioning 'Example DAG', (2) use of 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are testing conventions, (3) the ENV_ID pattern used for resource naming typical of test infrastructure, and (4) the overall structure matches Airflow's provider system test patterns. While it doesn't explicitly import test utilities, it contains test harness boilerplate and is not production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) extracts data from PostgreSQL database using PostgresToGCSOperator, (2) transfers data to Google Cloud Storage bucket, (3) programmatically creates cloud resources (GCE instance for PostgreSQL, GCS bucket, firewall rules), (4) configures database connections dynamically, and (5) demonstrates secure data migration with proper infrastructure provisioning and teardown. The code is incomplete (SQL_SELECT is cut off) but the architecture and approach clearly address all specified requirements.",0.6666666666666666
89,Ground Truth (HuggingFace),109,1,"The code uses native Airflow sensor operators throughout (TimeSensor, TimeDeltaSensor, BashSensor, FileSensor, PythonSensor, DayOfWeekSensor) from the appropriate providers (airflow.providers.standard.sensors.*). It does not wrap sensor logic in PythonOperator + Hook patterns. All sensors are used idiomatically with their specific operators designed for each task type.",1,"The code is clean and production-ready, using only standard Airflow libraries from airflow.providers.standard and airflow.sdk. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate like 'get_test_run(dag)'. All imports are legitimate Airflow 3.0.0 components.",1,"The DAG fully adheres to the instruction by demonstrating various sensor types: time-based sensors (TimeSensor, TimeDeltaSensor, DayOfWeekSensor), file-based sensor (FileSensor), bash command sensor (BashSensor), and Python function sensor (PythonSensor). It showcases how these sensors wait for specific conditions before triggering downstream tasks (the final 'print_date_in_bash' task). The DAG is well-structured for learning purposes with clear examples of conditional task execution, including both synchronous and asynchronous variants, timeout handling, and soft_fail patterns.",1.0
90,Ground Truth (HuggingFace),110,1,"The code uses native Airflow providers and operators specifically designed for Azure CosmosDB tasks. It correctly imports and uses AzureCosmosDocumentSensor for monitoring document availability and AzureCosmosInsertDocumentOperator for document insertion. The dependency is established idiomatically using the >> operator (t1 >> t2), which is the standard Airflow pattern for task dependencies.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', then uses 'watcher()' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. The comment '# Needed to run the example DAG with pytest' further confirms this is test infrastructure leakage.",1,"The DAG fulfills the user instruction completely. It monitors a specific Azure CosmosDB collection for document availability using AzureCosmosDocumentSensor (t1), triggers a subsequent document insertion task using AzureCosmosInsertDocumentOperator (t2), and implements a dependency-based workflow (t1 >> t2) that ensures document existence before performing write operations. The business logic requested is fully implemented.",0.6666666666666666
91,Ground Truth (HuggingFace),112,1,"The code uses the native SqlToSlackWebhookOperator from airflow.providers.slack.transfers.sql_to_slack_webhook, which is the idiomatic Airflow approach for transferring SQL query results to Slack. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator designed for this exact task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)' at the bottom. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",1,The code fulfills the user instruction to design a data monitoring workflow that extracts specific column data from a SQL table (using 'SELECT col FROM {SQL_TABLE}') and automatically sends the results to a designated Slack channel ('my_channel') for real-time reporting and team awareness. The SqlToSlackWebhookOperator accomplishes both the extraction and Slack notification requirements in a single operator.,0.6666666666666666
92,Ground Truth (HuggingFace),113,1,"The code uses native Airflow operators appropriately: LocalFilesystemToS3Operator for file transfer, S3CreateBucketOperator and S3DeleteBucketOperator for S3 bucket management. It uses TaskFlow API (@task decorator) for simple Python operations and proper operators for AWS interactions, which is idiomatic Airflow practice.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules. Additionally, it includes test setup/teardown logic (sys_test_context_task, watcher) and the 'test_run = get_test_run(dag)' pattern which are all test infrastructure leakage.",1,"The DAG fulfills all requirements from the instruction: (1) generates a sample text file via create_temp_file(), (2) uploads it to S3 using LocalFilesystemToS3Operator, and (3) performs cleanup operations by deleting both the S3 bucket and the temporary local file. The workflow demonstrates file management and cloud storage integration as requested.",0.6666666666666666
93,Ground Truth (HuggingFace),114,1,The code uses native Airflow providers and operators appropriately. It imports and uses HttpOperator from airflow.providers.http.operators.http and HttpSensor from airflow.providers.http.sensors.http. All tasks are created using these specific operators rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for HTTP operations.,0,"The code ends with an incomplete import statement 'from te' at the bottom, which appears to be either a truncation error or an attempt to import test utilities. This is not production-ready code. Additionally, the code includes documentation markers like [START howto_operator_http_task_post_op] and [END howto_operator_http_task_post_op] which are typically used in example/test code rather than production DAGs. The ENV_ID variable suggests this is from a system tests context.",1,"The DAG fulfills the user instruction completely. It implements: (1) Service availability checks using HttpSensor (both standard and deferrable versions), (2) Sequential HTTP request tasks with different HTTP methods (POST, GET, PUT, DELETE), and (3) Different configuration parameters including various headers, data formats (JSON, form-encoded), response checks, response filters, and pagination. The workflow structure shows sensors checking connectivity first, followed by a series of HTTP operations with varied configurations.",0.6666666666666666
94,Ground Truth (HuggingFace),115,1,"The DAG uses native Airflow providers and operators specifically designed for Google Cloud DLP tasks. It imports and uses CloudDLPCreateStoredInfoTypeOperator, CloudDLPUpdateStoredInfoTypeOperator, CloudDLPDeleteStoredInfoTypeOperator, CloudDLPListInfoTypesOperator, CloudDLPGetStoredInfoTypeOperator, and CloudDLPListStoredInfoTypesOperator from airflow.providers.google.cloud.operators.dlp. It also uses GCS operators for bucket management and file uploads. No generic PythonOperator wrappers are used where native operators are available.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. Additionally, the code uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' which are typical of test harness boilerplate. The DAG also references a resources file path using 'Path(__file__).parent / ""resources"" / FILE_NAME' which suggests this is part of a test system rather than production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It demonstrates: (1) uploading custom dictionaries to GCS (upload_file task), (2) creating custom stored info types with dictionary-based detection rules (create_info_type), (3) listing available info types (list_possible_info_types), (4) managing stored info types through CRUD operations (create, get, list, update, delete), and (5) programmatically configuring data classification rules using StoredInfoTypeConfig with large_custom_dictionary. The workflow enables dynamic configuration of sensitive data detection rules as requested.",0.6666666666666666
95,Ground Truth (HuggingFace),116,1,"The code uses native Airflow providers and operators designed specifically for Google Cloud Dataproc tasks. It properly uses DataprocCreateClusterOperator, DataprocSubmitJobOperator, and DataprocDeleteClusterOperator from airflow.providers.google.cloud.operators.dataproc. The code does not wrap these in PythonOperators or use hooks directly, which demonstrates idiomatic Airflow patterns.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific code like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and represent test infrastructure leakage.",1,"The code fully adheres to the user instruction. It creates a cloud-based data discovery pipeline using Google Dataproc that: (1) dynamically creates a multi-node cluster with 1 master, 2 workers, and 1 preemptible secondary worker, (2) executes a simple Hive query ('SHOW DATABASES;'), and (3) tears down the cluster after execution. This demonstrates scalable, on-demand data infrastructure that can spin up and down for lightweight analytical tasks as requested.",0.6666666666666666
96,Ground Truth (HuggingFace),118,1,"The code uses native Airflow providers and operators designed for GCS operations. It imports and uses GCSObjectExistenceSensor, GCSObjectsWithPrefixExistenceSensor, GCSObjectUpdateSensor, GCSUploadSessionCompleteSensor, GCSCreateBucketOperator, GCSDeleteBucketOperator, and GCSToGCSOperator from airflow.providers.google.cloud packages. These are the appropriate, idiomatic operators for GCS operations rather than wrapping logic in PythonOperator with hooks.",0,"The code imports from 'system.google' module ('from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID'), which appears to be an internal testing module. It also uses test-related environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and includes a 'workaround_in_debug_executor' function that is clearly test harness boilerplate. Additionally, it references 'RESOURCES_BUCKET_NAME = ""airflow-system-tests-resources""' which is test infrastructure. These are clear indicators of test/system test code rather than production-ready code.",1,"The DAG successfully demonstrates various GCS sensor techniques as requested. It showcases: (1) synchronous sensors (GCSUploadSessionCompleteSensor, GCSObjectUpdateSensor, GCSObjectExistenceSensor in poke mode), (2) asynchronous sensors (same sensors with deferrable=True), (3) different monitoring strategies including upload session completion detection, object update detection, and object existence checking. The workflow illustrates file transfer validation through the copy_file task followed by sensors. Although the code is truncated, the visible portion clearly demonstrates the requested functionality.",0.6666666666666666
97,Ground Truth (HuggingFace),120,1,"The code uses appropriate Airflow operators for the task: DockerOperator from airflow.providers.docker.operators.docker for Docker container operations, BashOperator for shell commands, and ShortCircuitOperator for conditional logic. These are the idiomatic, provider-specific operators rather than wrapping everything in PythonOperators with hooks.",0,The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG fulfills all requirements from the instruction: (1) monitors/detects new files in a source directory using BashOperator with find command, (2) checks for file availability using ShortCircuitOperator, (3) moves the first detected file to a destination directory using DockerOperator, and (4) prints the file contents using another DockerOperator. The workflow correctly chains these operations and uses Docker containers as specified.",0.6666666666666666
98,Ground Truth (HuggingFace),121,1,"The code correctly uses the TrinoOperator from airflow.providers.trino.operators.trino, which is the idiomatic and native operator for Trino database operations. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operator throughout for all database operations (CREATE SCHEMA, CREATE TABLE, INSERT, SELECT queries).",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) sequential execution of Trino operations with clear dependency chaining using the >> operator, (2) schema creation, (3) table definition (multiple tables), (4) data insertion, (5) template-based queries using Jinja templating with params, (6) parameterized queries using the parameters argument, and (7) proper task dependencies that illustrate the workflow. The structure clearly shows how operations flow from schema creation through various query types.",0.6666666666666666
99,Ground Truth (HuggingFace),122,1,"The code uses native Airflow operators throughout: GCSCreateBucketOperator, BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryToGCSOperator, GCSDeleteBucketOperator, and BigQueryDeleteDatasetOperator. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages provider-specific operators for all operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 1) Imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 2) Imports 'from tests_common.test_utils.watcher import watcher' and uses it in the DAG, 3) Imports 'from tests_common.test_utils.system_tests import get_test_run' and creates 'test_run = get_test_run(dag)'. The docstring also explicitly states this is an 'Airflow System Test DAG'. This is not production-ready code but rather a test fixture.",1,"The DAG fully demonstrates the requested end-to-end workflow: 1) Provisioning cloud resources (creates GCS bucket, BigQuery dataset, and table), 2) Performing data transfer (uses BigQueryToGCSOperator to move data from BigQuery to GCS), and 3) Cleaning up resources (deletes bucket and dataset with trigger_rule=ALL_DONE to ensure cleanup happens). The systematic workflow clearly shows provisioning → data movement → teardown as requested.",0.6666666666666666
100,Ground Truth (HuggingFace),123,1,"The code uses the specific CohereEmbeddingOperator from airflow.providers.cohere.operators.embedding, which is the idiomatic way to handle Cohere embeddings in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the native operator designed for this task.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",1,"The code fulfills the user instruction to build an Airflow DAG that transforms raw text data into dense vector representations using Cohere's embedding technology. It demonstrates the use of CohereEmbeddingOperator with text inputs, which is exactly what was requested for preparing unstructured text for NLP tasks.",0.6666666666666666
101,Ground Truth (HuggingFace),124,1,"The code uses native Airflow operators designed specifically for Oracle database operations. It uses SQLExecuteQueryOperator for SQL query execution and OracleStoredProcedureOperator for stored procedure calls, both from official Airflow providers (airflow.providers.common.sql and airflow.providers.oracle). No PythonOperator wrappers with hooks are used, which demonstrates proper idiomatic Airflow patterns.",1,"The code is clean and production-ready, using only standard Airflow libraries from official providers. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The imports are from legitimate Airflow 2.9.3 provider packages (airflow.providers.common.sql and airflow.providers.oracle).",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) basic SQL query execution using SQLExecuteQueryOperator with a simple SELECT statement, (2) stored procedure interactions using OracleStoredProcedureOperator with two different parameter passing methods (list-based and dict-based), and (3) uses Oracle connection parameters (oracle_conn_id/conn_id='oracle'). The workflow showcases different methods of executing database operations as requested.",1.0
102,Ground Truth (HuggingFace),125,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, MSSQLToGCSOperator (a specialized transfer operator) for data transfer, and GCSDeleteBucketOperator for cleanup. These are all provider-specific operators rather than generic PythonOperator wrappers, which is the idiomatic Airflow approach.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', 'list(dag.tasks) >> watcher()', 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from system.openlineage.operator import OpenLineageTestOperator', and 'import pytest'. The OpenLineageTestOperator and check_openlineage_events task are also test-specific components. This is clearly a system test file, not production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) dynamically creates a GCS bucket using GCSCreateBucketOperator, (2) transfers data using a specialized operator (MSSQLToGCSOperator), and (3) cleans up cloud resources after transfer is complete using GCSDeleteBucketOperator with appropriate trigger rule (ALL_DONE) to ensure cleanup happens regardless of transfer success/failure. The business logic flow is correct.",0.6666666666666666
103,Ground Truth (HuggingFace),126,1,"The code uses the appropriate SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic way to execute SQL queries in Airflow. It does not wrap SQL logic in PythonOperator with hooks, but instead uses the native SQL operator designed for this purpose. The operator is properly configured with conn_id in default_args and demonstrates proper task chaining.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that represent hallucination/leakage from test examples into what should be production-ready code.",1,"The DAG fully adheres to the instruction. It creates a sequential workflow that executes SQL tasks (CREATE TABLE -> INSERT DATA -> SELECT DATA -> DROP TABLE) with clear dependency management using the >> operator. The DAG demonstrates task ordering and atomic database interactions as requested, showing a complete database operation lifecycle from table creation through cleanup.",0.6666666666666666
104,Ground Truth (HuggingFace),127,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator, GCSDeleteBucketOperator, GCSTimeSpanFileTransformOperator, and GCSToGCSOperator from the Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, but instead uses the appropriate operators for GCS operations.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', 'list(dag.tasks) >> watcher()', 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from system.openlineage.operator import OpenLineageTestOperator', and the OpenLineageTestOperator task. These are internal testing modules and test infrastructure that indicate this is test code, not production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) performs time-span based file processing in GCS using GCSTimeSpanFileTransformOperator, (2) applies a custom transformation script to selected files from a source bucket, (3) transfers results to a destination bucket, (4) includes automated setup (create_bucket_src, create_bucket_dst, copy_file), (5) includes transformation (gcs_timespan_transform_files_task), and (6) includes cleanup (delete_bucket_src, delete_bucket_dst). The systematic approach with proper task chaining is implemented.",0.6666666666666666
105,Ground Truth (HuggingFace),128,1,The code uses native Airflow operators specifically designed for ADLS operations: ADLSCreateObjectOperator for file upload and ADLSDeleteOperator for file deletion. This is the idiomatic approach rather than wrapping hooks in PythonOperators.,0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are internal testing modules and patterns that represent hallucination/leakage from test examples.",1,"The DAG fulfills the user instruction completely. It demonstrates an ADLS workflow with file upload (using ADLSCreateObjectOperator) and immediate deletion (using ADLSDeleteOperator), showcasing basic file management operations in a cloud storage environment. The tasks are properly chained with upload_data >> delete_file to ensure sequential execution.",0.6666666666666666
106,Ground Truth (HuggingFace),129,1,The code uses native Airflow Redis providers and operators (RedisPublishOperator and RedisKeySensor) rather than wrapping Redis logic in PythonOperators with hooks. This is the idiomatic approach for Airflow.,0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are not production-ready elements and should not be present in a clean DAG implementation.",1,The DAG fulfills the user instruction to showcase inter-service communication using Redis as a message broker and synchronization mechanism. It publishes a start signal ('Start processing' message) using RedisPublishOperator and waits for a confirmation key using RedisKeySensor before advancing. The workflow demonstrates the requested publish-then-wait pattern for synchronization.,0.6666666666666666
107,Ground Truth (HuggingFace),130,1,"The code correctly uses the native AzureSynapseRunPipelineOperator from airflow.providers.microsoft.azure.operators.synapse, which is the idiomatic way to interact with Azure Synapse in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than production-ready code.",0,"While the code does set up a daily scheduled DAG that triggers an Azure Synapse pipeline, it does not fully meet the instruction requirements. The instruction specifically asks for 'appropriate error handling and logging' and to 'reliably trigger and monitor the pipeline execution'. The code lacks explicit error handling mechanisms (no retry configuration, no on_failure_callback, no error handling tasks) and does not include any monitoring or logging configuration beyond the default operator behavior. Additionally, the presence of test boilerplate (ENV_ID, watcher, test_run) suggests this is example code rather than a production workflow automation process as requested.",0.3333333333333333
108,Ground Truth (HuggingFace),131,1,"The code correctly uses the WinRMOperator from airflow.providers.microsoft.winrm.operators.winrm, which is the idiomatic Airflow approach for executing WinRM commands. It does not wrap WinRM logic in a PythonOperator, but instead uses the native operator designed for this purpose. The WinRMHook is also properly instantiated and passed to the operators.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', then calls 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production-ready DAG code.",0,"While the DAG does use WinRM to execute commands and has parallel task execution ([t1, t2, t3] run in parallel before run_this_last), it fails to simulate real-world remote Windows server management scenarios. The commands used are Unix/Linux commands ('ls -altr', 'sleep 60', 'echo') rather than Windows commands (e.g., 'dir', 'Get-Service', 'Get-Process', etc.). Additionally, the comment 'Caveat: This Dag will not run because of missing scripts' and the connection ID 'ssh_POC1' (SSH instead of WinRM) indicate this is not a properly configured Windows infrastructure example. The instruction requested a realistic Windows server management workflow, which this does not provide.",0.3333333333333333
109,Ground Truth (HuggingFace),132,1,"The code uses native Airflow providers and operators specifically designed for Cloud Bigtable operations. It imports from 'airflow.providers.google.cloud.operators.bigtable' and 'airflow.providers.google.cloud.sensors.bigtable', using operators like BigtableCreateInstanceOperator, BigtableCreateTableOperator, BigtableUpdateClusterOperator, BigtableDeleteTableOperator, BigtableDeleteInstanceOperator, and BigtableTableReplicationCompletedSensor. This is the idiomatic Airflow approach rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear evidence of test harness boilerplate and internal testing patterns. It uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables, which are typical of Airflow's internal system tests. The extensive docstring mentions 'Example Airflow DAG' which is characteristic of example/test code rather than production code. The variable naming pattern with 'ENV_ID' suffixes and the overall structure strongly suggest this is from Airflow's test suite rather than clean production-ready code.",0,"The code is incomplete (truncated at 'create_i') and cannot be fully evaluated. However, from what is visible, it appears to address some requirements: instance creation (dynamic resource allocation), cluster updates, table replication monitoring (BigtableTableReplicationCompletedSensor), and likely includes teardown operations (BigtableDeleteTableOperator and BigtableDeleteInstanceOperator are imported). The code demonstrates configurable parameters through environment variables. However, the incompleteness and the fact that the full workflow cannot be verified (missing task dependencies, incomplete second create_instance_task2, no visible teardown orchestration) means it fails to fully demonstrate the requested 'data infrastructure pipeline that automates provisioning and management' with all specified operations clearly implemented and connected.",0.3333333333333333
110,Ground Truth (HuggingFace),133,1,"The code uses native Airflow operators throughout: S3CreateBucketOperator, S3CreateObjectOperator, S3DeleteBucketOperator, S3DeleteObjectsOperator, and S3ToAzureBlobStorageOperator from the appropriate provider packages. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages the operator pattern correctly.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test/example code rather than clean production-ready code. The watcher() task and test_run variable are also test-specific artifacts.",1,"The DAG fully implements the requested functionality: (1) Creates cloud storage resources programmatically (S3 bucket and object), (2) Transfers a CSV file between cloud platforms (S3 to Azure Blob Storage using S3ToAzureBlobStorageOperator), (3) Implements robust teardown mechanisms (removes S3 objects and bucket with TriggerRule.ALL_DONE to ensure cleanup runs regardless of transfer success), and (4) Uses infrastructure-as-code principles with declarative operators. All requirements from the instruction are met.",0.6666666666666666
111,Ground Truth (HuggingFace),134,1,"The DAG uses native Google Analytics Admin Operators from airflow.providers.google.marketing_platform.operators.analytics_admin, including GoogleAnalyticsAdminCreateDataStreamOperator, GoogleAnalyticsAdminCreatePropertyOperator, GoogleAnalyticsAdminDeleteDataStreamOperator, GoogleAnalyticsAdminDeletePropertyOperator, GoogleAnalyticsAdminGetGoogleAdsLinkOperator, GoogleAnalyticsAdminListAccountsOperator, and GoogleAnalyticsAdminListGoogleAdsLinksOperator. This is idiomatic Airflow usage rather than wrapping hooks in PythonOperators.",1,"The code uses standard Airflow libraries and Google Analytics provider packages. All imports are from legitimate Airflow modules (airflow.decorators, airflow.models, airflow.providers.google, etc.) and Google Analytics SDK (google.analytics.admin_v1beta). There are no test harness imports or internal testing modules present.",0,"The code is incomplete - it cuts off mid-function at 'return get_secr'. While the DAG imports and declares the necessary operators for creating, listing, and deleting properties and data streams as requested, the actual task instantiation and orchestration logic is not visible in the provided snippet. The code shows setup (imports, connection creation, secret retrieval) but doesn't demonstrate the complete workflow with task dependencies that was requested in the instruction. Without seeing the full DAG implementation including all task definitions and their dependencies, we cannot confirm full instruction adherence.",0.6666666666666666
112,Ground Truth (HuggingFace),135,1,"The code uses the native YQExecuteQueryOperator from airflow.providers.yandex.operators.yq to execute the Yandex Cloud query, and EmptyOperator for the completion marker. This is the idiomatic approach rather than wrapping logic in a PythonOperator with hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests_common.test_utils.system_tests import get_test_env_id', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific logic like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'.",1,"The DAG fulfills the user instruction: it executes a trivial SQL query ('select 33 as d, 44 as t') using YQExecuteQueryOperator and marks completion with an EmptyOperator ('run_this_last'). The basic orchestration flow is correctly implemented with the query operator chained to the final empty operator.",0.6666666666666666
113,Ground Truth (HuggingFace),136,1,"The code uses idiomatic Airflow patterns throughout. It leverages TaskFlow API decorators (@task.branch(), @task.branch_external_python(), @task.branch_virtualenv(), @task.external_python(), @task.virtualenv()) which are the recommended Airflow 2.0+ approach for defining tasks. It uses EmptyOperator from the standard providers package, and properly utilizes TriggerRule for join patterns. The code does not fall back to wrapping logic in PythonOperator + Hook patterns where native operators exist.",1,"The code is clean and production-ready, using only standard Airflow libraries. All imports are from legitimate Airflow packages (airflow.providers.standard.operators.empty, airflow.sdk, airflow.utils.trigger_rule). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code includes documentation comments and example markers ([START]/[END]) which are standard practice in Airflow example DAGs, not test artifacts.",0,"The DAG is incomplete and does not properly connect the final virtualenv branch tasks to the join_venv operator. In the virtualenv section, the loop creates tasks and connects them from random_choice_venv with Labels, but never connects them to join_venv (the line 't >> join_venv' is missing). This breaks the workflow routing demonstration. While the code demonstrates advanced branching techniques, random selection, external Python execution, and virtual environment execution as requested, the incomplete task dependencies mean the DAG would not execute correctly, failing to fully demonstrate 'complex workflow routing' as instructed.",0.6666666666666666
114,Ground Truth (HuggingFace),137,1,The code uses native Airflow operators specifically designed for Google Cloud Datastore operations: CloudDatastoreBeginTransactionOperator and CloudDatastoreRollbackOperator from airflow.providers.google.cloud.operators.datastore. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.,0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'list(dag.tasks) >> watcher()', and 'test_run = get_test_run(dag)'. It also imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be a test utility. These are not production-ready patterns and should not be in a production DAG.",1,"The DAG fulfills the user instruction to create a pipeline that simulates a controlled database transaction process with explicit start and cancellation steps in a Google Cloud environment. It demonstrates transaction management using CloudDatastoreBeginTransactionOperator to start a transaction and CloudDatastoreRollbackOperator to cancel/rollback it, ensuring no persistent changes are made. This directly addresses the requirement for 'explicit start and cancellation steps' and 'prevents unintended persistent changes'.",0.6666666666666666
115,Ground Truth (HuggingFace),139,1,"The code uses the native PineconeIngestOperator from airflow.providers.pinecone.operators.pinecone, which is the idiomatic way to interact with Pinecone in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for vector ingestion.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG successfully demonstrates automated vector data insertion with configurable parameters as requested. It shows: (1) configurable index_name from environment variable with default, (2) configurable namespace from environment variable with default, (3) configurable batch_size parameter set to 1, and (4) dynamic loading of vector embeddings from environment variables (for config) and predefined lists (input_vectors with hardcoded embeddings). All key requirements from the instruction are met.",0.6666666666666666
116,Ground Truth (HuggingFace),140,1,"The code uses native Tableau-specific operators and sensors from the official Airflow Tableau provider package. It properly uses TableauOperator for refresh operations and TableauJobStatusSensor for monitoring job status. The code demonstrates proper use of XComArgs for task dependencies, which is idiomatic Airflow 2.0+ pattern. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fully addresses the user instruction to demonstrate enterprise-level data extract refresh techniques for BI reporting systems. It shows two different execution strategies: (1) blocking refresh that waits for completion, and (2) non-blocking refresh with separate monitoring via sensor. This demonstrates both synchronous and asynchronous patterns for triggering workbook refreshes and monitoring completion status, which aligns perfectly with the requirement to show 'different execution strategies'.",0.6666666666666666
117,Ground Truth (HuggingFace),141,0,"The DAG uses PythonOperator (via @task decorator) with boto3 client calls for creating and deleting DataBrew datasets and jobs, instead of using native Airflow operators. While GlueDataBrewStartJobOperator is used correctly for starting the job, the create_dataset, create_job, delete_dataset, and delete_job tasks should use dedicated operators like GlueDataBrewCreateDatasetOperator or similar if available in the provider package. The pattern of wrapping boto3 calls in @task decorators is not idiomatic when specific operators exist or should be used.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically: (1) 'from system.amazon.aws.utils import SystemTestContextBuilder' - this is an internal testing utility, (2) 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' - test context builder pattern, (3) 'from tests_common.test_utils.watcher import watcher' - explicit import from test utilities, and (4) the comment '# This test needs watcher in order to properly mark success/fail'. These are all indicators of test/example code that has leaked into what should be production-ready code.",1,"The DAG successfully demonstrates the requested end-to-end AWS Glue DataBrew data profiling workflow. It includes: (1) Creating resources (S3 buckets, dataset, profile job), (2) Executing the profiling job via GlueDataBrewStartJobOperator, and (3) Cleaning up all resources (deleting job, dataset, and buckets) with proper trigger rules (ALL_DONE) for automated resource management. The workflow showcases data preparation (uploading JSON data), profiling (creating and running a profile job), and cleanup, fulfilling all aspects of the user instruction.",0.3333333333333333
118,Ground Truth (HuggingFace),142,1,"The code uses the native `SupervisedFineTuningTrainOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model`, which is the idiomatic Airflow approach for Vertex AI generative model tuning. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports `from tests_common.test_utils.watcher import watcher` and `from tests_common.test_utils.system_tests import get_test_run`, and includes `list(dag.tasks) >> watcher()` and `test_run = get_test_run(dag)`. These are internal testing modules and test execution patterns that represent hallucination/leakage from test code into what should be a clean production example.",1,The DAG fulfills the user instruction to demonstrate fine-tuning a Google Vertex AI generative model using supervised learning techniques. It correctly showcases taking a base Gemini model (gemini-1.0-pro-002) and customizing it with training data (TRAIN_DATASET) using the SupervisedFineTuningTrainOperator. The business logic requested is complete and accurate.,0.6666666666666666
119,Ground Truth (HuggingFace),143,1,"The code uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the appropriate and idiomatic operator for executing SQL queries in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native SQL operator designed for this purpose. The operator is properly configured with connection parameters, handlers, and various SQL execution patterns.",0,"The code contains test harness boilerplate at the end that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than clean, production-ready code.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) Sequential execution of SQL operations against Trino with proper task dependencies using the >> operator, (2) Schema creation with trino_create_schema, (3) Table generation with trino_create_table and trino_multiple_queries, (4) Data insertion with trino_insert and within trino_multiple_queries, (5) Template-based queries using Jinja templating in trino_templated_query with params, and (6) Parameterized queries in trino_parameterized_query using the parameters argument. All requested features are present and correctly implemented.",0.6666666666666666
120,Ground Truth (HuggingFace),144,1,"The code uses the native CopyFromExternalStageToSnowflakeOperator from airflow.providers.snowflake.transfers, which is the idiomatic Airflow approach for copying data from external stages into Snowflake. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator designed for this exact task.",0,The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready code.,1,"The code fulfills the user instruction to construct an ETL process that ingests CSV files from an external data source into a Snowflake table using Airflow's Snowflake integration. It implements flexible file parsing through the file_format parameter (CSV with custom delimiter) and pattern matching, and provides consistent data loading through the CopyFromExternalStageToSnowflakeOperator. The implementation supports the requested robust data movement strategy.",0.6666666666666666
121,Ground Truth (HuggingFace),145,1,"The code uses the native MSGraphAsyncOperator from airflow.providers.microsoft.azure.operators.msgraph, which is the appropriate provider-specific operator for interacting with Microsoft Graph API (used by Microsoft Fabric). It does not wrap logic in a PythonOperator with hooks, demonstrating idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses these to add a watcher task and create a test_run variable. These are internal testing utilities that represent hallucination/leakage from test examples.",1,"The DAG fulfills the user instruction by demonstrating scheduling and executing a data pipeline in Microsoft Fabric. It triggers a data pipeline job through the Microsoft Fabric REST API using the MSGraphAsyncOperator, which represents a cloud-native orchestration mechanism. The code includes proper configuration with workspace and item IDs, job type specification (Pipeline), and uses Assets for lineage tracking, all of which align with the requested demonstration of data transformation and movement task orchestration.",0.6666666666666666
122,Ground Truth (HuggingFace),146,0,"The DAG heavily relies on @task decorated functions that use boto3 client directly instead of using Airflow's native DataSync operators and sensors. Functions like create_source_location, create_destination_location, create_task, delete_task, list_locations, and delete_locations all wrap boto3 calls in PythonOperators (via @task decorator) rather than using providers like DataSyncOperator for task creation or appropriate hooks. While DataSyncOperator is used for execution, the creation and deletion operations should ideally use native Airflow providers if available, or at minimum use DataSyncHook instead of raw boto3 clients.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is clearly from Airflow's internal testing framework. This is test harness boilerplate that should not appear in production DAG code. The sys_test_context_task using SystemTestContextBuilder is a testing utility pattern, not production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It demonstrates: (1) Creating two S3 buckets dynamically using S3CreateBucketOperator, (2) Creating DataSync locations for source and destination, (3) Creating a DataSync task, (4) Executing the task using DataSyncOperator, (5) Cleanup operations including delete_task, list_locations, and delete_locations functions with proper trigger rules (ALL_DONE). The complete lifecycle management workflow from creation through execution to cleanup is implemented as requested, using both Python and AWS SDK (boto3).",0.3333333333333333
123,Ground Truth (HuggingFace),147,1,"The DAG uses native Airflow providers and operators designed for the task: CloudDataTransferServiceGCSToGCSOperator for GCS-to-GCS transfers, GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket management, and LocalFilesystemToGCSOperator for file uploads. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be in production code. Comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' further indicate this is test code rather than production-ready.",1,"The DAG fulfills the user instruction requirements: (1) demonstrates file transfer between cloud storage buckets using CloudDataTransferServiceGCSToGCSOperator, (2) shows programmatic file movement with bucket creation and file upload, (3) includes explicit error handling through TriggerRule.ALL_DONE on cleanup tasks ensuring buckets are deleted regardless of success/failure, (4) demonstrates resource management by creating and cleaning up buckets, and (5) ensures reliable data transfer with wait=True and includes a deferrable option. The workflow comprehensively addresses data migration with proper resource lifecycle management.",0.6666666666666666
124,Ground Truth (HuggingFace),148,1,"The code uses native Airflow operators designed for the specific tasks: ComputeEngineInsertInstanceOperator for provisioning GCE instances, SSHOperator with ComputeEngineSSHHook for executing remote commands, and ComputeEngineDeleteInstanceOperator for teardown. This is idiomatic Airflow - it does not wrap these operations in PythonOperator with hooks, but uses the proper provider operators from airflow.providers.google.cloud and airflow.providers.ssh.",0,"The code contains test harness boilerplate at the end with 'from tests.system.utils.watcher import watcher' and a comment indicating 'Just for system tests purpose'. This is internal testing infrastructure that should not be present in production DAG code, representing hallucination/leakage from test environments.",1,"The DAG fully adheres to the instruction requirements: (1) Programmatically provisions a compute resource using ComputeEngineInsertInstanceOperator, (2) Executes predefined remote commands via SSHOperator (two commands: 'echo os_login1' and 'echo os_login2'), and (3) Systematically tears down the infrastructure using ComputeEngineDeleteInstanceOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of task success/failure. The workflow is properly chained to simulate a scalable cloud computing task execution environment.",0.6666666666666666
125,Ground Truth (HuggingFace),149,1,"The code correctly uses the InfluxDBOperator from airflow.providers.influxdb.operators.influxdb, which is the idiomatic and native operator for interacting with InfluxDB. It does not wrap functionality in a PythonOperator with hooks, demonstrating proper use of Airflow's provider pattern.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code, indicating hallucination/leakage from test examples.",1,"The code fulfills the user instruction by: (1) demonstrating interaction with InfluxDB using InfluxDBOperator, (2) showing how to connect via influxdb_conn_id, (3) executing a Flux query, (4) retrieving data from a predefined bucket ('test-influx'), and (5) implementing a configurable time window (range with start: -10m). The instruction requirements are met despite the test boilerplate issue.",0.6666666666666666
126,Ground Truth (HuggingFace),153,1,"The code uses the native TaskQueueEmptySensor from airflow.providers.google.cloud.sensors.tasks, which is the idiomatic Airflow approach for sensing Cloud Tasks queue status. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built sensor operator designed specifically for this task.",0,"The code contains test harness boilerplate with the comments '[START cloud_tasks_empty_sensor]' and '[END cloud_tasks_empty_sensor]', which are markers typically used in Airflow's system tests and documentation examples. While the imports are standard and production-ready, these test markers indicate this is example/test code rather than clean production code.",1,"The code fully implements the requested functionality: it creates a monitoring DAG that validates the completion status of a specific cloud task queue in Google Cloud, and implements a sensor task (TaskQueueEmptySensor) that pauses workflow execution until the designated queue is completely empty. All requirements from the instruction are met.",0.6666666666666666
127,Ground Truth (HuggingFace),154,1,"The code correctly uses WinRMOperator and WinRMHook from airflow.providers.microsoft.winrm, which are the specific providers designed for WinRM task execution. It does not wrap logic in PythonOperator but uses native operators throughout.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is test code, not production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) It demonstrates remote task scheduling and execution using WinRM hooks and operators, (2) It shows how to connect to Windows machines programmatically via WinRMHook with connection ID, (3) It implements a parallel execution pattern with three different command types (ls, sleep, echo) running concurrently [t1, t2, t3], and (4) It includes a final consolidation task 'run_this_last' that depends on all parallel tasks completing.",0.6666666666666666
128,Ground Truth (HuggingFace),155,1,"The code uses native Google Cloud Run operators from airflow.providers.google.cloud.operators.cloud_run, including CloudRunCreateJobOperator, CloudRunDeleteJobOperator, CloudRunExecuteJobOperator, CloudRunListJobsOperator, and CloudRunUpdateJobOperator. These are the appropriate provider-specific operators for Cloud Run tasks rather than wrapping logic in PythonOperator with hooks. The PythonOperators present are used for validation/assertion logic, which is appropriate.",0,"The code contains clear indicators of test harness boilerplate. Variable names like 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_GCP_PROJECT', task names with 'clean' prefix (clean1_task_name, clean2_task_name), and assertion functions (_assert_executed_jobs_xcom, _assert_created_jobs_xcom, _assert_updated_job, _assert_jobs, _assert_one_job) suggest this is test/example code rather than production-ready code. While it doesn't explicitly import from tests_common, the structure and naming conventions are characteristic of Airflow system test examples.",1,"The code demonstrates the complete lifecycle management of containerized jobs using Google Cloud Run operators as requested. It includes: (1) Job creation with CloudRunCreateJobOperator for multiple jobs, (2) Job execution with CloudRunExecuteJobOperator, (3) Job modification with CloudRunUpdateJobOperator, (4) Job cleanup with CloudRunDeleteJobOperator, (5) Robust error checking through assertion functions that validate XCom outputs, and (6) Cross-task dependency management through task naming and XCom pulls. The workflow systematically showcases creation, execution, modification, listing, and cleanup operations.",0.6666666666666666
129,Ground Truth (HuggingFace),156,1,"The code uses native Airflow providers and operators designed for the specific tasks. It imports and uses GCS operators (GCSCreateBucketOperator, GCSDeleteBucketOperator), Vertex AI operators (CreateCustomTrainingJobOperator, CreateDatasetOperator, UploadModelOperator, ExportModelOperator, etc.), and transfer operators (LocalFilesystemToGCSOperator). These are all idiomatic Airflow patterns that leverage purpose-built operators rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear indicators of test harness boilerplate and internal testing patterns. It uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables, includes comments like 'Example Airflow DAG for Google Vertex AI service testing Model Service operations', and follows naming conventions typical of Airflow's internal system tests (e.g., ENV_ID suffixes on resource names). The file path reference 'Path(__file__).parent / ""resources""' and the overall structure suggest this is adapted from Airflow's test suite rather than being production-ready code.",1,"The DAG fulfills the instruction requirements comprehensively. It demonstrates: (1) Cloud resource management via GCS bucket creation, (2) Data transfer using LocalFilesystemToGCSOperator, (3) Dataset creation with CreateDatasetOperator for tabular data, (4) Model training setup with CreateCustomTrainingJobOperator (referenced in imports), (5) Model deployment operations including UploadModelOperator and ExportModelOperator, and (6) Integration of multiple Google Cloud services (GCS, Vertex AI). The code showcases a complete ML lifecycle from data preparation through model deployment, meeting the request for a robust, scalable ML infrastructure example. Although the code is truncated, the visible portions demonstrate all key components requested.",0.6666666666666666
130,Ground Truth (HuggingFace),157,0,"The DAG uses only EmptyOperator placeholders instead of actual Airflow operators designed for data processing tasks. For a data validation and reporting system, it should use specific operators like EmailOperator for error notifications, SQLCheckOperator or similar for data validation, BranchPythonOperator or BranchSQLOperator for conditional logic based on validation results, and appropriate operators for report generation (e.g., PythonOperator with reporting libraries, or provider-specific operators). The current implementation is just a skeleton with no actual functionality.",1,"The code uses only standard Airflow libraries (airflow.providers.standard.operators.empty, airflow.sdk) and does not import any testing modules or include test harness boilerplate. The imports are clean and production-appropriate, even though the implementation is incomplete.",0,"The instruction requires building an automated data processing system that performs daily data validation, generates reports, detects data integrity issues, and triggers error communication. While the DAG structure shows a conceptual flow (ingest → analyze → check → branch on errors), it fails to implement any actual functionality. There is no real data validation logic, no actual error detection mechanism, no email/notification implementation for error communication, and no report generation. All tasks are EmptyOperators that do nothing. The DAG is merely a visual representation without fulfilling the core requirements of the instruction.",0.3333333333333333
131,Ground Truth (HuggingFace),158,1,"The code uses the idiomatic @task.sensor decorator, which is the modern, TaskFlow API approach for creating sensors in Airflow. It properly returns a PokeReturnValue object and configures sensor-specific parameters (poke_interval, timeout, mode). This is the recommended pattern for Airflow 3.x and avoids wrapping sensor logic in a generic PythonOperator.",0,"The code contains test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production DAG code. Additionally, there's a reference to an undefined variable 'dag' instead of 'tutorial_etl_dag'.",0,"While the code demonstrates the sensor decorator pattern, it fails to meet the instruction's requirement to 'simulate checking for data availability or external system readiness'. The sensor always returns True (is_done=True) immediately, which doesn't demonstrate any actual checking logic or conditional behavior. A proper example should include some simulation of checking conditions (e.g., checking a file path, time-based logic, or random success/failure) to show how sensors conditionally control task execution.",0.3333333333333333
132,Ground Truth (HuggingFace),159,1,"The code uses native Airflow SQL operators (SQLColumnCheckOperator and SQLTableCheckOperator) from airflow.providers.common.sql.operators.sql, which are the idiomatic and recommended operators for SQL-based data validation. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operators designed for these validation tasks.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,The DAG fully adheres to the instruction. It demonstrates SQL-based data validation techniques using both SQLColumnCheckOperator (for column-level checks like null_check and distinct_check) and SQLTableCheckOperator (for table-level checks like row_count_check) against a predefined database table (ab_role). The validation tasks are implemented sequentially (column_check >> row_count_check) and check specific data quality metrics as requested.,0.6666666666666666
133,Ground Truth (HuggingFace),160,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, MSSQLToGCSOperator for data transfer, and GCSDeleteBucketOperator for cleanup. These are the correct provider-specific operators rather than wrapping logic in PythonOperator with hooks. The use of TriggerRule.ALL_DONE for cleanup is also idiomatic Airflow pattern for ensuring teardown runs regardless of upstream success/failure.",0,"The code contains multiple test harness imports and boilerplate that should not be in production DAG code. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', 'list(dag.tasks) >> watcher()', 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from system.openlineage.operator import OpenLineageTestOperator', and 'import pytest' with pytest.skip logic. The OpenLineageTestOperator checking events against a JSON file is also test-specific infrastructure. This is clearly a system test file, not production code.",1,"The DAG fulfills all requirements from the instruction: (1) Bucket creation via GCSCreateBucketOperator, (2) Data extraction/transfer via MSSQLToGCSOperator, (3) Post-transfer cleanup via GCSDeleteBucketOperator with trigger_rule=ALL_DONE for error handling, and (4) Observability through the OpenLineageTestOperator. The workflow demonstrates a complete cloud data transfer process with proper error handling ensuring cleanup runs even on failure.",0.6666666666666666
134,Ground Truth (HuggingFace),161,1,"The code uses native Airflow 3.0.0 TaskFlow API patterns with @task, @setup, @teardown, and @task_group decorators. It leverages the as_teardown() method and context manager syntax for teardown tasks, which are idiomatic Airflow 3.0 features. No generic PythonOperator wrappers are used where specific operators would be more appropriate - the demonstration appropriately uses TaskFlow decorators for its educational purpose.",1,"The code imports only standard Airflow 3.0.0 libraries (airflow.sdk modules including DAG, setup, task, task_group, teardown) and the standard pendulum library for datetime handling. There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no hallucinated or non-existent Airflow components. The code is clean and production-ready.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) task dependencies through multiple chaining patterns (>>, as_teardown method, context managers), (2) lifecycle management with explicit @setup and @teardown decorators, (3) nested task groups (section_1 task group nested within outer setup/teardown), (4) clear initialization and finalization steps (outer_setup/outer_teardown and inner_setup/inner_teardown), and (5) complex workflow structure showing how setup/teardown relationships work with task clearing behavior. The code is clearly educational and demonstrates Airflow's resource management capabilities as requested.",1.0
135,Ground Truth (HuggingFace),162,1,"The code uses native Airflow providers and operators throughout. It leverages GCP-specific operators like GCSCreateBucketOperator, GCSToGCSOperator, and multiple CloudVision operators (CloudVisionCreateProductSetOperator, CloudVisionCreateProductOperator, etc.) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic Airflow approach for GCP integrations.",0,"The code contains clear indicators of test harness boilerplate. It uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, includes 'example' in the DAG_ID ('example_gcp_vision_explicit_id'), has 'example' in the tags list, and demonstrates idempotence testing with duplicate operators (product_set_create_2_idempotence). The variable naming pattern (ENV_ID, SYSTEM_TESTS_*) and structure strongly suggest this is adapted from Airflow's internal system tests rather than being production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It: (1) transfers image files between cloud storage buckets using GCSToGCSOperator, (2) prepares images for ML-based product recognition by creating product sets, products, and reference images using Cloud Vision API operators, (3) demonstrates programmatic management of image datasets and product metadata through various CRUD operations on Vision API resources, and (4) creates a reproducible workflow using cloud services (GCS and Cloud Vision). The pipeline structure aligns well with the requested image data processing and ML preparation workflow.",0.6666666666666666
136,Ground Truth (HuggingFace),163,1,"The code uses native Airflow providers and operators designed specifically for CloudFormation tasks. It properly uses CloudFormationCreateStackOperator, CloudFormationDeleteStackOperator, CloudFormationCreateStackSensor, and CloudFormationDeleteStackSensor from airflow.providers.amazon.aws. The code does not wrap these in PythonOperators or use hooks directly, which demonstrates idiomatic Airflow patterns.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. These are internal testing modules. The code also includes test setup tasks like 'sys_test_context_task = SystemTestContextBuilder().build()' and 'list(dag.tasks) >> watcher()' which are test infrastructure, not production-ready code.",1,"The DAG fulfills the user instruction completely. It demonstrates: (1) Creating an AWS SQS queue using CloudFormation with CloudFormationCreateStackOperator, (2) Waiting for creation with CloudFormationCreateStackSensor, (3) Deleting the stack with CloudFormationDeleteStackOperator, (4) Waiting for deletion with CloudFormationDeleteStackSensor. The workflow shows clear sequential steps for infrastructure provisioning and teardown as requested, with proper use of operators and sensors for CloudFormation stack management.",0.6666666666666666
137,Ground Truth (HuggingFace),164,1,"The code uses native Airflow sensor operators throughout (TimeDeltaSensor, TimeSensor, BashSensor, FileSensor, PythonSensor, DayOfWeekSensor) rather than wrapping logic in PythonOperator with hooks. All sensors are imported from appropriate airflow.sensors modules and used directly as intended by the framework.",1,"The code is clean and production-ready, using only standard Airflow libraries from airflow.sensors, airflow.operators, airflow.models, and airflow.utils. There are no imports from internal testing modules like 'tests_common.test_utils' or test harness boilerplate code. The code comments with START/END markers are standard Airflow documentation patterns, not test artifacts.",1,"The DAG fully satisfies the user instruction to demonstrate various sensor types with conditional triggering of downstream tasks. It showcases: (1) waiting mechanisms (TimeDeltaSensor, TimeSensor with sync/async variants), (2) file checks (FileSensor), (3) time-based conditions (TimeSensor, DayOfWeekSensor), and (4) different sensor behaviors (success/failure scenarios with soft_fail, timeout configurations, BashSensor, PythonSensor). The downstream task 'tx' is conditionally triggered using TriggerRule.NONE_FAILED, demonstrating how sensors can control workflow execution.",1.0
138,Ground Truth (HuggingFace),165,1,"The code uses native Airflow Dataproc operators (DataprocCreateWorkflowTemplateOperator, DataprocInstantiateWorkflowTemplateOperator, DataprocInstantiateInlineWorkflowTemplateOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which is the correct idiomatic approach for Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also has comments like 'TEST SETUP', 'TEST BODY', 'TEST TEARDOWN' and uses a watcher() function for test execution. This is not production-ready code.",1,"The DAG fulfills the user instruction by: (1) creating a Dataproc workflow template with DataprocCreateWorkflowTemplateOperator, (2) executing a Pig job on a managed Hadoop cluster as defined in the workflow template configuration, (3) demonstrating asynchronous job instantiation using the 'deferrable=True' parameter in DataprocInstantiateWorkflowTemplateOperator, and (4) showcasing template management through both stored template instantiation and inline template instantiation. All key requirements are met.",0.6666666666666666
139,Ground Truth (HuggingFace),166,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator for bucket creation, S3ToFTPOperator for cross-platform file transfer, and S3DeleteBucketOperator for cleanup. These are specific provider operators designed for their respective tasks, not generic PythonOperator wrappers around hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. It also includes test-specific logic like 'sys_test_context_task', 'test_context = sys_test_context_task()', 'watcher()', and 'test_run = get_test_run(dag)'. This is clearly test infrastructure code, not production-ready DAG code.",1,"The DAG fulfills the core requirements: (1) initializes temporary storage with S3CreateBucketOperator, (2) executes cross-platform file transfer using S3ToFTPOperator (S3 to FTP), and (3) ensures cleanup with S3DeleteBucketOperator using trigger_rule=TriggerRule.ALL_DONE to guarantee execution regardless of task outcomes. The lifecycle management and resource cleanup are explicitly handled as requested.",0.6666666666666666
140,Ground Truth (HuggingFace),167,1,"The code correctly uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic and recommended operator for executing SQL queries in Airflow. It does not wrap database logic in a PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG fulfills the user instruction completely. It demonstrates: (1) basic SQL query execution against an Airflow metadata database, (2) shows how to connect to a Postgres database (via conn_id configuration), (3) runs multiple SQL statements (SELECT 1 and SELECT from ab_role table), and (4) showcases SQLExecuteQueryOperator's capabilities including split_statements and return_last parameters. The business logic requested is fully implemented.",0.6666666666666666
141,Ground Truth (HuggingFace),168,1,"The DAG uses native Airflow operators throughout: CloudFirestoreExportDatabaseOperator for Firestore export, GCSCreateBucketOperator for Cloud Storage operations, BigQueryCreateExternalTableOperator and BigQueryInsertJobOperator for BigQuery operations, and CloudDatastoreCommitOperator for Datastore operations. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages provider-specific operators.",0,"The code contains clear indicators of test harness boilerplate. It uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', includes comments referencing 'system tests', and has a docstring that says 'Example Airflow DAG'. The naming conventions (e.g., 'example_firestore_to_gcp') and tags (['example', 'firestore']) suggest this is test/example code rather than production-ready code. While it doesn't import test utilities directly, the structure and naming clearly indicate this is from Airflow's test/example suite.",1,"The DAG fulfills the instruction to orchestrate a complete data pipeline from NoSQL database export to cloud storage staging and then into a queryable data warehouse table. It: (1) Exports data from Firestore (NoSQL database) to Cloud Storage using CloudFirestoreExportDatabaseOperator, (2) Stages data in GCS bucket, (3) Creates an external table in BigQuery (data warehouse) from the exported data, and (4) Includes a query operation (BigQueryInsertJobOperator) to make the data queryable. The workflow demonstrates integration of multiple cloud services (Firestore, GCS, BigQuery) as requested.",0.6666666666666666
142,Ground Truth (HuggingFace),169,1,"The code uses native AWS S3 Operators and Sensors from airflow.providers.amazon.aws (S3CreateBucketOperator, S3CreateObjectOperator, S3ListOperator, S3CopyObjectOperator, S3KeySensor, etc.) rather than wrapping S3Hook calls in PythonOperators. This is the idiomatic Airflow approach for S3 interactions.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing module, not a standard Airflow library. It also uses 'sys_test_context_task = SystemTestContextBuilder().build()' and 'test_context = sys_test_context_task()' which are test harness patterns. This is test boilerplate that should not appear in production DAG code.",1,"The DAG fulfills the instruction requirements: it creates S3 buckets (S3CreateBucketOperator), uploads sample data (S3CreateObjectOperator with DATA variable), performs metadata operations (tagging operations with S3PutBucketTaggingOperator, S3GetBucketTaggingOperator, S3DeleteBucketTaggingOperator), lists objects (S3ListOperator, S3ListPrefixesOperator), and includes sensors (S3KeySensor). It demonstrates programmatic interaction with S3 using Apache Airflow providers as a learning exercise. The code is incomplete (cuts off at 'tas') but the visible portions show all requested functionality.",0.6666666666666666
143,Ground Truth (HuggingFace),171,0,"The code uses TaskFlow API (@task decorator) with Hooks directly instantiated inside Python functions, but does nothing with them. Hooks are meant to be used within Operators or to perform actual operations. The code instantiates PinotAdminHook and PinotDbApiHook but doesn't call any methods on them to execute commands or queries. This is not idiomatic Airflow - it should either use dedicated Pinot Operators if they exist, or properly use the Hooks to execute operations (e.g., calling methods like run_cli() or get_records()). Simply instantiating a Hook object without using it serves no purpose.",0,"The code explicitly imports and uses test harness boilerplate: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production DAG code. This is a clear example of test infrastructure leakage into what should be clean, production-ready code.",0,"The instruction asks to 'execute administrative commands and retrieve database information using specialized Pinot hooks.' The code instantiates the hooks but never actually executes any commands or retrieves any information. The PinotAdminHook is created but no admin command is run. The PinotDbApiHook is instantiated with what appears to be incorrect parameters (task_id, pinot, pinot_options are not typical Hook constructor parameters) and no query execution or data retrieval occurs. The DAG does not demonstrate functional system-level or query-level interactions with Pinot - it only shows incomplete Hook instantiation.",0.0
144,Ground Truth (HuggingFace),172,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket management, LocalFilesystemToGCSOperator for file uploads, and DataflowTemplatedJobStartOperator/DataflowStartFlexTemplateOperator for Dataflow job execution. It does not wrap functionality in PythonOperator + Hook patterns where native operators exist.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a test harness module. It also includes test-related environment variables (SYSTEM_TESTS_ENV_ID, SYSTEM_TESTS_GCP_PROJECT) and documentation markers like '[START howto_operator_start_template_job]' that are typical of Airflow's internal testing/documentation framework. This is not production-ready code.",0,"The instruction asks for CSV to AVRO conversion using Dataflow templates. While the code includes a DataflowStartFlexTemplateOperator with the correct File_Format_Conversion template and proper parameters for CSV to AVRO conversion, it also includes an unnecessary DataflowTemplatedJobStartOperator using the Word_Count template (which doesn't perform CSV to AVRO conversion). Additionally, the code is incomplete (truncated at the end). The presence of irrelevant Word_Count jobs and incomplete code structure means it doesn't cleanly fulfill the specific business requirement.",0.3333333333333333
145,Ground Truth (HuggingFace),173,1,"The code uses native Airflow operators designed for the specific tasks: GKECreateClusterOperator and GKEDeleteClusterOperator for GKE cluster management, and DataprocCreateClusterOperator and DataprocDeleteClusterOperator for Dataproc cluster management. It does not wrap these operations in PythonOperators with hooks, which would be non-idiomatic. The code properly imports from airflow.providers.google.cloud.operators and uses the appropriate operators for GCP infrastructure management.",0,"The code imports from 'system.google' module with the line 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID'. This appears to be an internal testing module that would not be available in a standard Airflow installation. Additionally, the code includes test harness patterns like SYSTEM_TESTS_ENV_ID, SYSTEM_TESTS_GCP_PROJECT environment variables, and references to system tests throughout the variable naming conventions. This is test boilerplate that should not be in production code.",0,"The DAG code is incomplete - it ends abruptly with an open parenthesis '(' and no task dependencies are defined. The instruction requested a coordinated workflow that manages creation and deletion of infrastructure, but the task dependencies are missing. While the individual operators for creating and deleting both GKE and Dataproc clusters are present, the orchestration logic (the actual workflow sequence) is not completed. A complete implementation should show the dependency chain: create_gke_cluster >> create_cluster_in_gke >> delete_dataproc_cluster >> delete_gke_cluster.",0.3333333333333333
146,Ground Truth (HuggingFace),174,1,"The DAG uses native Airflow operators throughout: CloudFirestoreExportDatabaseOperator for Firestore export, GCSCreateBucketOperator for GCS bucket creation, BigQueryCreateEmptyDatasetOperator and BigQueryCreateExternalTableOperator for BigQuery operations. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages provider-specific operators.",1,"The code uses only standard Airflow libraries from official providers (airflow.providers.google.cloud and airflow.providers.google.firebase). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code is production-ready, though it includes example/documentation comments which are acceptable.",0,"The DAG is incomplete and does not fully satisfy the instruction. While it successfully: (1) exports a Firestore collection to GCS using CloudFirestoreExportDatabaseOperator, (2) creates infrastructure (bucket and dataset), and (3) creates an external table in BigQuery, it is missing task dependencies to orchestrate the workflow properly. The code shows task definitions but no dependency chains (e.g., create_bucket >> export_database_to_gcs >> create_external_table). Additionally, the DAG defines cleanup operators (BigQueryDeleteDatasetOperator, GCSDeleteBucketOperator) that are imported but never instantiated, and the external table creation doesn't follow the export task, making it non-functional for the stated goal of 'preparing data for analytical querying'. The instruction requires a complete, scalable data movement solution, but this implementation lacks proper orchestration.",0.6666666666666666
147,Ground Truth (HuggingFace),175,0,"The DAG uses TaskFlow API (@task decorators) for core data processing tasks (fetch_tweets, clean_tweets, analyze_tweets, transfer_to_db) which are all empty placeholder functions. These should ideally use specific Airflow providers and operators. For example, fetching tweets should use a Twitter/social media provider operator if available, or at minimum a proper HTTP operator. The cleaning and analysis tasks wrap what should be native operators in Python callables. While HiveOperator is used correctly for Hive operations, the core pipeline logic relies on generic Python functions rather than idiomatic Airflow operators designed for these specific tasks.",1,"The code does not import any internal testing modules or include test harness boilerplate like 'from tests_common.test_utils.system_tests import get_test_run'. It uses standard Airflow libraries (BashOperator, HiveOperator, @task decorator, DAG). The code is incomplete and has placeholders, but it doesn't hallucinate non-existent imports or testing frameworks.",0,"The instruction requires a complete data engineering pipeline that 'retrieves tweets from different Twitter accounts, transforms the raw data through cleaning and analysis stages, and prepares aggregated insights for storage.' While the DAG structure outlines these steps (fetch, clean, analyze, transfer), all core tasks are empty placeholders with no actual implementation. The fetch_tweets task has no Twitter API integration, clean_tweets and analyze_tweets contain no transformation logic, and transfer_to_db has no database operations. The instruction asks for a functional pipeline with 'clear transformation steps,' but the code provides only a skeleton with TODO comments. The DAG is explicitly marked as non-functional with the caveat 'This Dag will not run because of missing scripts.'",0.3333333333333333
148,Ground Truth (HuggingFace),176,1,"The code uses the specific LevelDBOperator from airflow.providers.google.leveldb.operators.leveldb, which is the idiomatic way to interact with LevelDB in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)'. It also uses pytest.skip and includes test-related comments. This is not production-ready code and contains test infrastructure leakage.",1,The DAG fulfills the user instruction by demonstrating basic key-value retrieval (get operation with 'get_key_leveldb_task') and insertion operations (put operation with 'put_key_leveldb_task') using LevelDB. It creates a simple workflow showing how to get and put data into the embedded key-value database as requested.,0.6666666666666666
149,Ground Truth (HuggingFace),177,1,"The code uses native Airflow providers and operators specifically designed for Display Video 360 integration. It imports from 'airflow.providers.google.marketing_platform.operators.display_video' and 'airflow.providers.google.marketing_platform.sensors.display_video', using operators like GoogleDisplayVideo360DownloadLineItemsOperator, GoogleDisplayVideo360UploadLineItemsOperator, and GCSToBigQueryOperator. This is idiomatic Airflow rather than wrapping logic in PythonOperators with hooks.",1,The code uses only standard Airflow libraries and official Google Marketing Platform providers. There are no imports from internal testing modules (like 'tests_common.test_utils.system_tests') or test harness boilerplate. All imports are from legitimate Airflow 2.7.2 provider packages. The code appears to be production-ready example code.,0,"The instruction requires a complete pipeline that 'collects, downloads, AND transfers' DV360 data to a centralized data warehouse. While the code includes operators for downloading line items and uploading to BigQuery, the DAG is incomplete and lacks proper task orchestration. The DAG ends abruptly with 'upload_line_items' operator without any task dependencies defined (no >> or << operators). More critically, it doesn't implement the full report generation workflow: there's no task to create a query, run the query, wait for completion, download the report, and transfer to BigQuery in a connected pipeline. The code shows individual operators but doesn't fulfill the complete end-to-end pipeline requirement specified in the instruction.",0.6666666666666666
150,Ground Truth (HuggingFace),178,1,"The code uses idiomatic Airflow patterns throughout. It leverages the TaskFlow API with @task decorators for Python tasks, uses BashOperator for bash commands, and demonstrates proper XCom usage with XComArg for implicit passing. The code does not wrap operators in unnecessary PythonOperator calls and uses native Airflow constructs appropriately.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.DAG, airflow.decorators.task, airflow.operators.bash.BashOperator, airflow.XComArg) and the pendulum library for datetime handling. There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",1,"The DAG fully adheres to the instruction to demonstrate cross-task data sharing and communication using XComs. It showcases multiple XCom methods: (1) direct pushing via ti.xcom_push() in the 'push' task, (2) implicit pushing by returning values in 'push_by_returning', (3) pulling values via ti.xcom_pull() in the 'puller' task, (4) BashOperator pushing XComs both manually and via return value, (5) pulling from bash tasks in Python using ti.xcom_pull(), and (6) using XComArg for implicit data passing between tasks. The workflow demonstrates comprehensive XCom interactions as requested.",1.0
151,Ground Truth (HuggingFace),179,1,"The code uses native Airflow providers and operators appropriately. It uses SqsHook for queue creation/deletion, SqsPublishOperator for publishing messages, and SqsSensor for message retrieval. These are the idiomatic Airflow components for SQS operations. The @task decorator is used appropriately for custom logic (create/delete queue) where no specific operator exists.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. It also includes test-specific logic like 'sys_test_context_task', 'test_context', 'env_id', and 'watcher()' which are all part of Airflow's internal testing framework.",1,"The DAG fulfills all requirements from the instruction: (1) dynamically creates an SQS queue using create_queue task, (2) publishes multiple messages using two SqsPublishOperator instances, (3) demonstrates different message retrieval strategies using two SqsSensor configurations (single message vs batch retrieval with max_messages=10 and num_batches=3), and (4) includes queue deletion with delete_queue task. The workflow is complete and covers all requested aspects.",0.6666666666666666
152,Ground Truth (HuggingFace),180,0,"The code heavily relies on PythonOperators with boto3 client calls wrapped in @task decorators instead of using native Airflow SageMaker operators. Airflow provides dedicated operators like SageMakerTrainingOperator, SageMakerModelOperator, SageMakerEndpointConfigOperator, and SageMakerEndpointOperator from airflow.providers.amazon.aws.operators.sagemaker. The current implementation uses generic Python tasks with boto3, which is not idiomatic Airflow.",0,"The code references 'KNN_IMAGES_BY_REGION' which is not defined in the provided snippet and appears to be from a test harness or system test file (as evidenced by the error message mentioning 'system testfile'). Additionally, the code is incomplete (cuts off at 'endpoint_name = en'), and there are missing imports (boto3, json, TriggerRule, task decorator). The reference to test file directions in the error message suggests this was copied from test infrastructure rather than being production-ready code.",1,"The code does address the core requirements of the instruction: it handles CSV training data (TRAIN_DATA), trains a classification model using SageMaker KNN, creates model infrastructure (model, endpoint config, endpoint), makes predictions via call_endpoint, and includes automated resource cleanup (delete_endpoint, delete_endpoint_config). Despite implementation issues, the business logic flow matches the requested workflow of transforming raw data into a deployable ML model with prediction capabilities and resource management.",0.3333333333333333
153,Ground Truth (HuggingFace),181,1,"The DAG uses native Airflow providers and operators specifically designed for Google Cloud DLP tasks. It properly imports and uses CloudDLPCreateJobTriggerOperator, CloudDLPListJobTriggersOperator, CloudDLPGetDLPJobTriggerOperator, CloudDLPUpdateJobTriggerOperator, and CloudDLPDeleteJobTriggerOperator from airflow.providers.google.cloud.operators.dlp. No generic PythonOperator wrappers are used where native operators are available.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. It also imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be a testing utility. These are not standard Airflow libraries and should not be present in production-ready code.",1,"The DAG fully implements the requested functionality: it creates a DLP job trigger, lists triggers, retrieves a specific trigger, updates the trigger (changing the recurrence period), and deletes the trigger. The tasks are properly sequenced in the correct order (create >> list >> get >> update >> delete), demonstrating sequential task management as requested. The DAG serves as a technical reference for managing data protection tasks through a structured, automated pipeline.",0.6666666666666666
154,Ground Truth (HuggingFace),182,1,"The code uses native Azure-specific operators (LocalFilesystemToADLSOperator and ADLSDeleteOperator) from the airflow.providers.microsoft.azure package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)'. It also adds a watcher() task to the DAG with 'list(dag.tasks) >> watcher()'. This is not production-ready code and includes testing infrastructure that should not be present in a production DAG.",1,"The DAG fulfills the user instruction by demonstrating interaction between local file systems and cloud storage using Azure-specific operators. It shows file transfer (LocalFilesystemToADLSOperator for upload) and file deletion (ADLSDeleteOperator for removal), which illustrates how data can be moved and managed programmatically in Azure cloud infrastructure.",0.6666666666666666
155,Ground Truth (HuggingFace),183,1,"The code correctly uses native Airflow providers and operators specifically designed for Alibaba Cloud OSS interactions. It imports and uses OSSCreateBucketOperator and OSSDeleteBucketOperator from airflow.providers.alibaba.cloud.operators.oss, which are the appropriate idiomatic operators for this task rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes test execution code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code.",1,"The code fulfills the user instruction to create an Airflow DAG that illustrates sequential task execution for Alibaba OSS bucket management. It demonstrates task dependencies (create_bucket >> delete_bucket), shows cloud service interactions through OSS operators, and provides a simple workflow pattern as requested. The DAG properly shows how to programmatically interact with Alibaba OSS bucket resources.",0.6666666666666666
156,Ground Truth (HuggingFace),184,1,"The code uses native Airflow operators designed for Oracle database operations: SQLExecuteQueryOperator for SQL queries and OracleStoredProcedureOperator for stored procedures. It does not wrap database logic in PythonOperator with hooks, which would be non-idiomatic. The operators are imported from the appropriate provider packages (airflow.providers.oracle and airflow.providers.common.sql).",1,The code is clean and production-ready. It only imports standard Airflow libraries from official provider packages. There are no imports from internal testing modules (like tests_common.test_utils.system_tests) or test harness boilerplate code. All imports and usage are legitimate Airflow 2.9.3 components.,1,"The DAG fully addresses the user instruction. It demonstrates: (1) executing database transactions using Oracle connections via SQLExecuteQueryOperator, (2) different parameter input strategies for stored procedures using both list-based parameters (positional with input/output types) and dict-based parameters (named with input/output types), and (3) various methods of executing database operations with input and output parameter handling through the two OracleStoredProcedureOperator examples. The workflow clearly highlights the different approaches as requested.",1.0
157,Ground Truth (HuggingFace),185,1,"The code uses native Airflow operators throughout: LocalFilesystemToGCSOperator for file upload, BigQueryCreateEmptyDatasetOperator and BigQueryCreateEmptyTableOperator for BigQuery setup, BigQueryCreateDataTransferOperator for creating the transfer configuration, and BigQueryDataTransferServiceStartTransferRunsOperator for triggering the transfer. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages provider-specific operators.",0,"The code contains clear test harness boilerplate and references to internal testing infrastructure. Specifically: ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), and the DAG_ID includes 'example_gcp_bigquery_dts'. The comments and structure indicate this is from Airflow's system tests rather than production-ready code. Additionally, the code is incomplete (cuts off at 'gcp_run_sensor = BigQuer').",1,"The DAG fulfills the user instruction to develop a data ingestion pipeline that transforms a local CSV file containing US state information into a structured BigQuery table using Google Cloud data transfer services. It implements: (1) uploading local CSV to GCS, (2) creating BigQuery dataset and table with appropriate schema for US states (name, post_abbr), (3) creating a BigQuery Data Transfer Service configuration, and (4) starting the transfer run. This is a systematic approach to moving structured data from local storage to cloud-based analytical platforms as requested.",0.6666666666666666
158,Ground Truth (HuggingFace),186,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Memorystore Memcached operations. It imports and uses CloudMemorystoreMemcachedCreateInstanceOperator, CloudMemorystoreMemcachedDeleteInstanceOperator, CloudMemorystoreMemcachedUpdateInstanceOperator, and other specialized operators from airflow.providers.google.cloud.operators.cloud_memorystore. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",1,"The code uses only standard Airflow libraries and Google Cloud provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The imports are clean and production-appropriate, using airflow.models.dag, airflow.providers.google.cloud.operators, and airflow.providers.standard.operators.bash.",0,"While the code demonstrates creating a distributed caching service (Memcached) with specific node and memory configurations (node_count: 1, cpu_count: 1, memory_size_mb: 1024), the DAG is incomplete. The code is truncated at the end with 'delete_memcached_instance = C', leaving the workflow unfinished. A complete infrastructure-as-code workflow should include the full lifecycle: creation, configuration/update operations, and cleanup/deletion. The DAG does not fully illustrate a complete workflow for 'dynamically creating and managing' the service as requested, and the incomplete code prevents proper execution.",0.6666666666666666
159,Ground Truth (HuggingFace),187,1,"The code uses appropriate Airflow providers and operators for GCS operations. It correctly uses GCSCreateBucketOperator, LocalFilesystemToGCSOperator, GCSToLocalFilesystemOperator, and GCSDeleteBucketOperator from the airflow.providers.google.cloud package. These are the idiomatic, native operators for GCS file operations rather than wrapping hooks in PythonOperators.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 1) Imports from 'system.google' and 'system.openlineage.operator' which appear to be internal test modules, 2) Uses OpenLineageTestOperator for checking test events, 3) Imports 'tests_common.test_utils.watcher' and 'tests_common.test_utils.system_tests', 4) Includes test_run = get_test_run(dag) at the bottom. These are clear indicators of test harness code that has leaked into what should be a production DAG example.",1,"The DAG fulfills all the requirements from the user instruction: 1) Creates a GCS bucket (create_bucket task), 2) Uploads a local file to GCS (upload_file task), 3) Downloads the file from GCS (download_file task), 4) Performs cleanup operations (delete_bucket task with ALL_DONE trigger rule). The workflow demonstrates inter-system file transfer between local filesystem and GCS using appropriate Airflow operators, and implements proper task dependencies to show the complete lifecycle of cloud file operations.",0.6666666666666666
160,Ground Truth (HuggingFace),188,1,"The code uses native Kafka provider operators (ProduceToTopicOperator and AwaitMessageTriggerFunctionSensor) from airflow.providers.apache.kafka, which are the idiomatic Airflow way to interact with Kafka. It does not wrap Kafka logic in generic PythonOperators with hooks, demonstrating proper use of specialized operators for the task.",0,"The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production code. Additionally, the code references 'example_dag_event_listener.await_function' as a string, which appears to be example/test code rather than production-ready implementation.",1,"The code successfully implements the requested event-driven pipeline with Kafka message production and consumption. It demonstrates: (1) message production via ProduceToTopicOperator producing numbers 0-49, (2) asynchronous message consumption via AwaitMessageTriggerFunctionSensor, (3) conditional event handling based on number characteristics (divisibility by 3, 5, and 15 for FizzBuzz logic), and (4) sensor-based triggering of specific actions. The implementation showcases the core requirements of event-driven architecture with Kafka in Airflow.",0.6666666666666666
161,Ground Truth (HuggingFace),189,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator for bucket creation, CloudTextToSpeechSynthesizeOperator for text-to-speech conversion, and GCSDeleteBucketOperator for cleanup. These are the appropriate provider-specific operators rather than generic PythonOperator wrappers around hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes test setup comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN'. It also calls 'watcher()' and 'get_test_run(dag)' which are test utilities, not production code patterns.",1,"The DAG fulfills all requirements from the user instruction: it creates a GCS bucket, converts text to speech using Google Cloud Text-to-Speech service with sample text ('Sample text for demo purposes'), stores the audio file in the bucket with a specified filename, and cleans up by deleting the bucket. The workflow demonstrates the complete lifecycle as requested.",0.6666666666666666
162,Ground Truth (HuggingFace),190,1,"The code uses native BigQuery operators from airflow.providers.google.cloud.operators.bigquery throughout, including BigQueryCreateEmptyTableOperator, BigQueryUpdateTableOperator, BigQueryDeleteTableOperator, etc. It does not wrap BigQuery logic in PythonOperator + Hook patterns. This is the correct, idiomatic approach for Airflow.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from environment variables with 'SYSTEM_TESTS_ENV_ID', references 'SYSTEM_TESTS_GCP_PROJECT', and includes naming patterns like 'bucket_{DAG_ID}_{ENV_ID}' and 'dataset_{DAG_ID}_{ENV_ID}' which are typical of Airflow's internal system tests. The file also references a test resources directory with 'Path(__file__).parent / ""resources"" / ""update_table_schema.json""'. While it doesn't explicitly import from tests_common, the structure and naming conventions strongly suggest this is adapted from Airflow's test suite rather than being production-ready code.",1,"The DAG successfully demonstrates dynamic table creation and transformation in BigQuery with multiple data object types as requested. It creates a standard table (test_table), a view (test_view), and a materialized view (test_materialized_view). It also shows management operations including updating tables (update_table) and deleting views (delete_view). The code is incomplete (cuts off at 'upsert_table = B'), but the core requirements of creating, modifying, and managing different table representations are fulfilled.",0.6666666666666666
163,Ground Truth (HuggingFace),191,1,"The code uses appropriate Airflow providers and operators for the task. It leverages GoogleSheetsToGCSOperator, GCSToGoogleSheetsOperator, GoogleSheetsCreateSpreadsheetOperator, and GCS bucket operators from the official Google provider packages. These are the idiomatic, native operators designed for Google Sheets and GCS interactions, rather than wrapping hooks in PythonOperators.",0,"The code contains clear indicators of test harness boilerplate and system test patterns. Variables like ENV_ID with 'SYSTEM_TESTS_ENV_ID', PROJECT_ID with 'SYSTEM_TESTS_GCP_PROJECT', comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN', and the overall structure of creating/deleting temporary resources (bucket, connection) are characteristic of Airflow's system test examples rather than production-ready code. Additionally, the code is incomplete (truncated at 'delete_bucke' instead of 'delete_bucket') and includes placeholder comments like '# Override to match your needs'.",1,"The DAG fulfills the user instruction to create a data migration pipeline between Google Sheets and GCS. It systematically: (1) creates a spreadsheet, (2) extracts/uploads spreadsheet data to GCS using GoogleSheetsToGCSOperator, and (3) reimports the data from GCS back to a spreadsheet using GCSToGoogleSheetsOperator. The pipeline demonstrates the complete cycle of extraction, transfer to cloud storage, and reimport as requested.",0.6666666666666666
164,Ground Truth (HuggingFace),193,1,"The code uses the appropriate Airflow providers and operators for the task. It uses MySQLToGCSOperator from airflow.providers.google.cloud.transfers.mysql_to_gcs for transferring data from MySQL to GCS, and GCSCreateBucketOperator/GCSDeleteBucketOperator for bucket management. These are the idiomatic, native operators designed specifically for these operations rather than wrapping hooks in PythonOperators.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 1) imports pytest, 2) imports from tests.system.utils.watcher, 3) imports from tests.system.utils (get_test_run), 4) includes test_run = get_test_run(dag) at the bottom, 5) adds watcher() to the DAG tasks, and 6) includes comments like 'TEST SETUP', 'TEST BODY', 'TEST TEARDOWN'. These are all testing artifacts that indicate this is test/example code, not production-ready code.",1,"The DAG fulfills the user instruction requirements: 1) Transfers data from MySQL database to Google Cloud Storage using MySQLToGCSOperator, 2) Implements an automated, one-time batch process (schedule='@once'), 3) Demonstrates cloud data extraction and storage, and 4) Includes built-in bucket management (creates bucket before transfer, deletes after with trigger rule ALL_DONE). The core business logic matches what was requested.",0.6666666666666666
165,Ground Truth (HuggingFace),194,1,"The code uses the native BigQueryToPostgresOperator from airflow.providers.google.cloud.transfers.bigquery_to_postgres, which is the idiomatic Airflow approach for transferring data between BigQuery and PostgreSQL. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'import pytest', 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are internal testing modules and test infrastructure that indicate this is example/test code, not production-ready code.",0,"The instruction requested 'configurable parameters', 'robust error handling', and 'resource management'. The code lacks all three: (1) Parameters are hardcoded (DATASET_NAME, TABLE, destination_table) rather than being configurable via Variables or params; (2) No error handling mechanisms like retries, on_failure_callback, or SLAs are implemented; (3) No resource management features like pools, task concurrency limits, or proper connection configuration are present. Additionally, the code includes test setup/teardown tasks (create_dataset, create_table, delete_dataset) which are not part of the requested data movement process for production use.",0.3333333333333333
166,Ground Truth (HuggingFace),196,1,The code uses idiomatic Airflow patterns appropriately. It leverages the TaskFlow API with @task decorators for Python tasks and uses BashOperator for bash commands. The params mechanism and templating with Jinja2 ({{ params.foo }}) are standard Airflow features. The code does not wrap operators unnecessarily in PythonOperator when native operators are available.,1,"The code is clean and uses only standard Airflow libraries (airflow.decorators, airflow.models.dag, airflow.operators.bash) along with standard Python libraries (datetime, os, textwrap, pendulum). There are no imports from internal testing modules like 'tests_common.test_utils' or test harness boilerplate like 'get_test_run(dag)'. The code is production-ready.",1,"The DAG fully adheres to the instruction. It demonstrates parameter passing through multiple methods: (1) params passed to @task decorated functions, (2) params with templating in BashOperator using env variables, (3) environment variable testing via the env_var_test_task. The workflow showcases dynamic parameter injection across both Python tasks (my_py_command, print_env_vars) and Bash tasks (also_run_this), illustrating different methods of runtime configuration as requested.",1.0
167,Ground Truth (HuggingFace),197,1,"The code uses native Airflow operators designed specifically for the task: LocalFilesystemToADLSOperator for uploading files to Azure Data Lake Storage and ADLSDeleteOperator for deleting files. These are the appropriate provider-specific operators from airflow.providers.microsoft.azure, not generic PythonOperator wrappers around hooks.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', uses 'list(dag.tasks) >> watcher()', imports 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)'. These are internal testing modules and patterns that indicate this is test/example code, not production-ready code.",1,"The DAG fulfills the user instruction completely. It uploads a local file to Azure Data Lake Storage using LocalFilesystemToADLSOperator, then immediately deletes the uploaded file using ADLSDeleteOperator. The task dependency (upload_file >> delete_file) ensures the deletion happens after the upload, demonstrating file movement and cleanup in a cloud storage environment as requested.",0.6666666666666666
168,Ground Truth (HuggingFace),198,1,"The code uses native Airflow operators designed for the specific tasks: DataflowCreatePipelineOperator, DataflowRunPipelineOperator, DataflowDeletePipelineOperator for Dataflow operations, and GCSCreateBucketOperator, GCSSynchronizeBucketsOperator, GCSDeleteBucketOperator for GCS operations. It does not wrap logic in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains test harness boilerplate at the end: 'from tests_common.test_utils.watcher import watcher' and includes a comment about needing a watcher for teardown tasks. This is internal testing infrastructure that should not be present in production-ready code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are testing conventions, and includes comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' that indicate this is test code rather than clean production code.",1,"The DAG fulfills the user instruction completely. It demonstrates a Google Cloud Dataflow workflow for batch text analysis using a word count example (kinglear.txt file). It manages the entire lifecycle: creates a GCS bucket, moves sample text files, creates a Dataflow pipeline, runs the pipeline, and then cleans up by deleting the pipeline and bucket. The pipeline configuration includes proper parameters for input file and output location for the word count analysis.",0.6666666666666666
169,Ground Truth (HuggingFace),199,0,"The code uses `@asset` decorator which does not exist in Airflow 3.0.6. The correct approach is to use `@dag` with asset-based scheduling and `@task` decorators with inlet/outlet parameters. The `@asset` decorator appears to be a hallucinated API. Idiomatic Airflow 3.x uses Dataset (or Asset class) objects passed to schedule parameters and task inlets/outlets, not as function decorators for defining producer DAGs.",0,"The `@asset` decorator does not exist in Airflow's standard library. The correct imports for Airflow 3.0.6 would be from `airflow.datasets` or similar. The code imports `asset` as a decorator from `airflow.sdk` which is not a valid Airflow module structure. Additionally, the `asset` decorator syntax with `schedule=None` and the signature `def asset2_producer(self, context, asset1_producer)` appears to be fabricated functionality.",0,"While the code attempts to show producer-consumer relationships with S3-based assets, it fails to create a working pipeline. The instruction asks for a workflow that 'illustrates how different data assets can be managed and scheduled' with 'clear processing steps'. The code has non-functional `@asset` decorators with empty/minimal logic, and the consumer DAG (`consumes_asset_decorator`) has a task that does nothing meaningful. There's no actual data processing, transformation, or clear demonstration of asset management - just placeholder functions.",0.0
170,Ground Truth (HuggingFace),200,1,"The code uses native Airflow operators appropriately: BashOperator for command execution and SingularityOperator from airflow.providers.singularity for container orchestration. It does not wrap these in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,The code includes test harness boilerplate at the bottom: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production-ready code examples.,1,"The DAG successfully demonstrates orchestration of system-level tasks as requested: (1) time-based scheduling via 'schedule=timedelta(minutes=10)', (2) command execution through BashOperator and SingularityOperator tasks, (3) basic error handling with 'retries: 1' in default_args and 'retries=3' on the sleep task, and (4) distributed computing environment illustration through the use of containerized execution (SingularityOperator with Docker image). All key requirements are met.",0.6666666666666666
171,Ground Truth (HuggingFace),201,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, OracleToGCSOperator for data transfer, and GCSDeleteBucketOperator for cleanup. These are all provider-specific operators from airflow.providers.google.cloud, which is the idiomatic way to interact with GCS in Airflow. No PythonOperator wrappers around hooks are used.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities not appropriate for production code. Additionally, comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' indicate this is test code rather than production-ready code.",0,"While the DAG implements the sequential flow of creating a bucket, transferring data, and cleaning up the bucket as requested, it uses OracleToGCSOperator which transfers data from Oracle database to GCS. The instruction asked for 'transferring data' in the context of cloud infrastructure management, which typically implies moving data between cloud storage locations or uploading local/sample data, not extracting from an Oracle database. This introduces an external dependency (Oracle database) that wasn't part of the instruction and doesn't align with the 'cloud infrastructure management' showcase theme.",0.3333333333333333
172,Ground Truth (HuggingFace),202,0,"The provided code is incomplete and does not contain any Airflow DAG structure, operators, or tasks. It only shows configuration variables and constants (CONTENTS, TEXT_EMBEDDING_MODEL, MULTIMODAL_MODEL, etc.) without any DAG definition, task definitions, or use of Airflow operators. There is no evidence of using Google Cloud Vertex AI operators (e.g., VertexAITextEmbeddingOperator, VertexAIGenerativeModelOperator) or any other Airflow operators that would be appropriate for this use case.",1,"The code snippet, while incomplete, does not contain any imports from internal testing modules or test harness boilerplate. It only shows configuration constants using what appears to be Google Cloud Vertex AI SDK classes (Tool, grounding, HarmCategory, HarmBlockThreshold). There are no hallucinated test utilities or non-standard Airflow imports visible in the provided code.",0,"The instruction requires building an AI-powered content analysis pipeline using Airflow that processes text and image data with multimodal ML models on Google Cloud, performing embedding, generation, and safety-filtered content evaluation. The provided code only contains configuration variables and constants without any actual pipeline implementation, DAG definition, task orchestration, or workflow logic. It does not fulfill the requirement of building a functional Airflow pipeline that orchestrates these operations.",0.3333333333333333
173,Ground Truth (HuggingFace),203,1,"The code uses the native `GoogleApiToS3Operator` from `airflow.providers.amazon.aws.transfers.google_api_to_s3`, which is the idiomatic Airflow approach for transferring Google Sheets data to S3. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built transfer operator designed for this exact use case.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: (1) `from system.amazon.aws.utils import SystemTestContextBuilder` - internal testing utility, (2) `from tests_common.test_utils.watcher import watcher` - test watcher import, (3) `from tests_common.test_utils.system_tests import get_test_run` - test harness import, (4) `test_run = get_test_run(dag)` - test execution boilerplate, and (5) `sys_test_context_task` and `test_context` usage throughout the DAG. These are all testing artifacts that leak into what should be production code.",1,"The code fulfills the user instruction requirements: (1) Extracts tabular data from Google Spreadsheet using `GoogleApiToS3Operator` with configurable `GOOGLE_SHEET_ID` and `GOOGLE_SHEET_RANGE`, (2) Securely transfers to cloud storage (S3), (3) Implements dynamic environment-based naming using `env_id` variable for bucket and key names (`s3_bucket = f'{env_id}-google-api-sheets'`), and (4) Creates a repeatable template with parameterized environment variables. Despite the test harness code, the core business logic matches the instruction.",0.6666666666666666
174,Ground Truth (HuggingFace),204,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator for bucket creation, AzureBlobStorageToS3Operator for the transfer operation, and S3DeleteBucketOperator for cleanup. These are the correct provider-specific operators from airflow.providers.amazon.aws and airflow.providers.amazon.aws.transfers packages, rather than wrapping logic in PythonOperators with hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test setup logic (sys_test_context_task, test_context) and a watcher() call that are part of Airflow's internal testing framework, not production-ready code.",1,"The DAG fulfills all requirements from the instruction: it creates a temporary S3 bucket (create_s3_bucket task), transfers data from Azure Blob Storage to S3 (azure_blob_to_s3 task), and deletes the bucket after completion (delete_s3_bucket task with TriggerRule.ALL_DONE). The workflow demonstrates clear setup, execution, and teardown stages as requested.",0.6666666666666666
175,Ground Truth (HuggingFace),205,1,"The code uses native Airflow providers and operators appropriately. It leverages GCS operators (GCSCreateBucketOperator, LocalFilesystemToGCSOperator), Vertex AI operators (CreateDatasetOperator, CreateCustomPythonPackageTrainingJobOperator), and BashOperator for file operations. No generic PythonOperator wrapping is used where specific operators exist. This is idiomatic Airflow usage.",0,"The code contains clear indicators of test harness boilerplate. It includes comments like 'Example Airflow DAG for Google Vertex AI service testing Custom Jobs operations' and uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, which are typical of internal testing frameworks. The naming conventions (e.g., ENV_ID suffixes on resource names) and structure suggest this is adapted from Airflow's system tests rather than production-ready code.",0,"The DAG is incomplete - it cuts off mid-definition of the CreateCustomPythonPackageTrainingJobOperator task. The instruction requires a complete pipeline with 'standardized dataset preparation and model training steps', but the code ends abruptly without showing the full training job configuration, model training execution, or any downstream tasks. Additionally, there's no clear housing price prediction logic visible, and the pipeline lacks completion with proper task dependencies and cleanup operations that would make it truly reproducible.",0.3333333333333333
176,Ground Truth (HuggingFace),207,1,"The code uses native Airflow providers and operators throughout. It leverages GCS operators (GCSCreateBucketOperator, GCSSynchronizeBucketsOperator), Vertex AI dataset operators (CreateDatasetOperator), and Vertex AI pipeline job operators (RunPipelineJobOperator, GetPipelineJobOperator, DeletePipelineJobOperator). No generic PythonOperator wrappers are used where specific operators exist. This is idiomatic Airflow usage.",1,The code imports only standard Airflow libraries and Google Cloud provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears production-ready with proper imports from airflow.providers.google.cloud.operators and airflow.models.dag.,0,"The code is incomplete - it cuts off mid-line at 'trigger' in the delete_pipeline_job task definition. More critically, while it includes most required components (synchronizes housing data via GCSSynchronizeBucketsOperator, creates tabular dataset via CreateDatasetOperator, runs pipeline via RunPipelineJobOperator), it's missing the ListPipelineJobOperator which is imported but never used. The instruction asks to 'manage the pipeline job lifecycle' which typically includes listing jobs. Additionally, the incomplete delete_pipeline_job task and missing task dependencies/cleanup operations (delete bucket, delete objects tasks are imported but not instantiated) mean the full lifecycle management is not demonstrated.",0.6666666666666666
177,Ground Truth (HuggingFace),209,1,"The code correctly uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic and recommended approach for executing SQL queries in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native SQL operator designed for this purpose.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that represent hallucination/leakage from test code into what should be production DAG code.",1,"The DAG fulfills the user instruction by: (1) illustrating task sequencing with the '>>' operator connecting two tasks, (2) using SQLExecuteQueryOperator for SQL query execution, (3) connecting to a Presto database via the conn_id parameter, and (4) demonstrating fundamental data manipulation tasks (SELECT and DROP TABLE operations). The core business logic requested is present and functional.",0.6666666666666666
178,Ground Truth (HuggingFace),210,1,"The code uses native Airflow Weaviate operators (WeaviateIngestOperator, WeaviateDocumentIngestOperator) and the WeaviateHook appropriately. It does not wrap Weaviate logic in generic PythonOperators unnecessarily. The use of @task decorators for setup/preparation tasks is idiomatic for Airflow 2.x TaskFlow API, and the actual Weaviate operations use the provider-specific operators as intended.",1,"The code imports only standard Airflow libraries from the official providers package (airflow.providers.weaviate.operators.weaviate and airflow.providers.weaviate.hooks.weaviate). There are no imports from test utilities, internal testing modules, or test harness boilerplate. The code appears production-ready without test scaffolding.",0,"The code is incomplete - it cuts off mid-comment ('# Example tasks to create class with OpenAI vectorizer, store data without vectors in XCOM, and call Weavi'). While it demonstrates creating a Weaviate class and ingesting data with custom vectors using different input methods (XCOM and callable), the DAG appears truncated and doesn't show the complete workflow. The instruction asks for a demonstration of 'different data input methods' and the code starts to show this but is incomplete. Additionally, the code defines sample_data_without_vector and get_data_without_vectors() but never uses them in any task, suggesting missing implementation for the vectorizer-based ingestion pattern that was likely intended.",0.6666666666666666
179,Ground Truth (HuggingFace),211,1,"The code uses native BigQuery operators from airflow.providers.google.cloud.operators.bigquery throughout, including BigQueryCreateEmptyDatasetOperator, BigQueryCreateEmptyTableOperator, BigQueryInsertJobOperator, and various check operators. It does not wrap BigQuery logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and references to system testing infrastructure. Specifically: ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), and the variable naming pattern with SYSTEM_TESTS prefix indicates this is test code. Additionally, the DAGS_LIST variable and the loop creating multiple DAGs based on test scenarios (locations = [None, LOCATION]) are typical test harness patterns, not production-ready code.",1,"The code fulfills the user instruction comprehensively. It demonstrates: (1) creating datasets via BigQueryCreateEmptyDatasetOperator, (2) creating tables via BigQueryCreateEmptyTableOperator with schema definitions, (3) executing SQL queries for data insertion using BigQueryInsertJobOperator with INSERT statements, and (4) data retrieval through SELECT queries. The workflow showcases multiple BigQuery operators and demonstrates basic data management techniques in a cloud data warehouse environment as requested.",0.6666666666666666
180,Ground Truth (HuggingFace),213,1,"The code uses the native TriggerDagRunOperator from airflow.providers.standard.operators.trigger_dagrun, which is the idiomatic and correct way to trigger another DAG in Airflow. It does not wrap this functionality in a PythonOperator or use hooks unnecessarily.",0,The code imports 'from airflow.sdk import DAG' which is not a standard Airflow 3.0.0 import path. The correct import should be 'from airflow import DAG' or 'from airflow.models import DAG'. The 'airflow.sdk' module appears to be either a hallucination or confusion with experimental/internal APIs that are not part of the standard production Airflow library.,0,"The instruction asks to 'Design a workflow that demonstrates how to trigger a secondary DAG from a primary DAG' and mentions 'This example holds 2 DAGs'. However, the provided code only contains ONE DAG (example_trigger_controller_dag). The second DAG (example_trigger_target_dag) that should be triggered is completely missing from the code. The instruction explicitly requires creating a scenario with both DAGs, but only the controller DAG is present.",0.3333333333333333
181,Ground Truth (HuggingFace),214,1,"The code uses native Dataproc operators from airflow.providers.google.cloud.operators.dataproc (DataprocCreateWorkflowTemplateOperator, DataprocInstantiateWorkflowTemplateOperator, DataprocInstantiateInlineWorkflowTemplateOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Google Cloud Dataproc operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production DAG code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are test-specific.",1,"The DAG fulfills the user instruction by demonstrating: (1) Creating a Dataproc workflow template with DataprocCreateWorkflowTemplateOperator, (2) Setting up a Hadoop/Spark cluster configuration (CLUSTER_CONFIG with master and worker nodes), (3) Running a Pig job (PIG_JOB with query), and (4) Executing the workflow template with DataprocInstantiateWorkflowTemplateOperator. The workflow illustrates the complete lifecycle from template creation to execution as requested.",0.6666666666666666
182,Ground Truth (HuggingFace),215,0,"The code uses TaskFlow API (@task decorator) with boto3 client calls wrapped in Python functions instead of using native Airflow operators. For DynamoDB table creation, point-in-time recovery enablement, and table deletion, there are appropriate operators available (e.g., DynamoDBCreateTableOperator, DynamoDBDeleteTableOperator) that should be used instead of wrapping boto3 calls in @task decorated functions. While DynamoDBToS3Operator is correctly used for the transfer, the resource management tasks (create_table, enable_point_in_time_recovery, delete_dynamodb_table) are not idiomatic.",0,"The code imports and uses 'tests.system.providers.amazon.aws.utils' which is an internal testing module (line: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder'). It also includes test harness boilerplate with 'sys_test_context_task = SystemTestContextBuilder().build()' and uses 'test_context = sys_test_context_task()' within the DAG. This is clearly test infrastructure code that should not be in production DAG code.",0,"The code is incomplete - it cuts off mid-comment at the end ('# Segmenting allows the transfer to be parallelized into {segment} number of parall'). The instruction specifically requests 'a segmented export to S3', and while there's a comment indicating segmented transfer was intended ([START howto_transfer_dynamodb_to_s3_segmented]), the actual implementation is missing. The DAG also lacks proper task dependencies and orchestration flow (no chain() call or dependency setup visible for all tasks). While it does create a DynamoDB table, enable point-in-time recovery, and has a basic export operator, the incomplete nature and missing segmented export implementation means it doesn't fully fulfill the requirements.",0.0
183,Ground Truth (HuggingFace),217,1,"The code uses native Airflow SQL operators (SQLColumnCheckOperator and SQLTableCheckOperator) from airflow.providers.common.sql.operators.sql, which are the idiomatic and recommended operators for data quality checks. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operators designed for this exact use case.",0,"The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fulfills the user instruction completely. It performs systematic checks on database table structure and content using SQL operators: (1) SQLColumnCheckOperator validates column-level constraints (null checks, distinct checks on the 'id' column), and (2) SQLTableCheckOperator confirms the expected number of rows (row count check). Both required components of the instruction are implemented correctly.",0.6666666666666666
184,Ground Truth (HuggingFace),218,1,"The code uses native Airflow operators designed specifically for Google Display Video 360 and BigQuery operations. It imports and uses GoogleDisplayVideo360DownloadLineItemsOperator, GoogleDisplayVideo360UploadLineItemsOperator, GCSToBigQueryOperator, and other specialized operators from airflow.providers.google.marketing_platform and airflow.providers.google.cloud packages. This is the idiomatic approach rather than wrapping hooks in PythonOperators.",1,The code imports only standard Airflow libraries from official providers (airflow.providers.google.marketing_platform and airflow.providers.google.cloud). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. All imports are production-ready and appropriate for a real DAG.,0,"The instruction requests orchestrating complex marketing data operations with tasks for downloading line items, uploading files, AND performing data transformations. While the code includes download_line_items, upload_line_items, and upload_erf_to_bq tasks, there are no explicit data transformation tasks implemented. The GCSToBigQueryOperator performs a data transfer but not a transformation. Additionally, the DAG lacks task dependencies (no >> or << operators) to actually orchestrate the workflow, and several imported operators (CreateQueryOperator, RunQueryOperator, DownloadReportV2Operator, etc.) are not used despite being relevant to 'extracting advertising performance metrics' mentioned in the instruction. The DAG is incomplete and doesn't fully fulfill the business logic requested.",0.6666666666666666
185,Ground Truth (HuggingFace),219,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Catalog operations. It imports and uses CloudDataCatalogCreateTagTemplateOperator, CloudDataCatalogGetTagTemplateOperator, CloudDataCatalogCreateTagTemplateFieldOperator, CloudDataCatalogRenameTagTemplateFieldOperator, and other specialized operators from airflow.providers.google.cloud.operators.datacatalog. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains test harness boilerplate that indicates it was copied from internal testing modules. Specifically, it uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables (ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT')), which are patterns from Airflow's system tests rather than production-ready code. While it doesn't directly import test utilities, the naming conventions and structure clearly leak testing infrastructure into what should be production code.",1,"The DAG successfully demonstrates the lifecycle of a tag template management workflow as requested. It includes: (1) Creating a tag template with CloudDataCatalogCreateTagTemplateOperator, (2) Retrieving the template with CloudDataCatalogGetTagTemplateOperator, (3) Modifying by creating additional fields with CloudDataCatalogCreateTagTemplateFieldOperator, and (4) Renaming fields with CloudDataCatalogRenameTagTemplateFieldOperator. The code showcases different operational steps for data governance metadata templates, fulfilling the instruction to demonstrate creating, retrieving, and modifying templates. Although the code is truncated, the visible portions clearly show the requested lifecycle operations.",0.6666666666666666
186,Ground Truth (HuggingFace),221,1,"The code uses native Airflow operators designed for GCS operations: GCSCreateBucketOperator, GCSDeleteBucketOperator, GCSDeleteObjectsOperator, GCSListObjectsOperator, and GCSToGCSOperator. These are the appropriate provider-specific operators from airflow.providers.google.cloud rather than wrapping logic in PythonOperator with hooks. This is idiomatic Airflow usage.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from system.openlineage.operator import OpenLineageTestOperator', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. The code also includes OpenLineageTestOperator task and watcher() calls which are testing utilities. This is clearly test infrastructure code, not production-ready.",1,"The DAG fulfills the user instruction to create an automated data movement and management pipeline. It demonstrates: (1) transferring files between cloud storage buckets using GCSToGCSOperator, (2) systematic tracking via GCSListObjectsOperator to list bucket contents, and (3) cleanup procedures using GCSDeleteObjectsOperator and GCSDeleteBucketOperator. The workflow shows safe data movement with proper orchestration of list, copy, and delete operations as requested.",0.6666666666666666
187,Ground Truth (HuggingFace),223,0,"The code uses PythonOperator (via @task decorator) with boto3 client calls for create_dataset, create_job, delete_dataset, and delete_job instead of using native Airflow providers. While GlueDataBrewStartJobOperator is used correctly, the dataset and job creation/deletion should use operators like GlueDataBrewCreateDatasetOperator or similar if available in the provider package. The pattern of wrapping boto3 calls in @task decorators is the anti-pattern described in the scoring criteria.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()', 'test_context = sys_test_context_task()', and 'from tests_common.test_utils.watcher import watcher' at the end. These are internal testing utilities that should not appear in production-ready code.",1,"The DAG fulfills the user instruction to create an automated data profiling pipeline using AWS Glue DataBrew. It demonstrates full lifecycle management including: creating S3 buckets for input/output, uploading JSON data, creating a DataBrew dataset, creating a profile job, starting the job to analyze the data, and cleaning up all resources (deleting job, dataset, and buckets). The workflow shows programmatic data profiling with dynamic resource management as requested.",0.3333333333333333
188,Ground Truth (HuggingFace),224,1,"The code uses the specific GCSToPrestoOperator from airflow.providers.presto.transfers.gcs_to_presto, which is the idiomatic Airflow approach for transferring data from GCS to Presto. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG code.,1,"The code fulfills the user instruction to design a data migration workflow that transfers CSV files from Google Cloud Storage to a Presto table. The GCSToPrestoOperator is configured with the necessary parameters (source_bucket, source_object, presto_table) to accomplish this task, enabling seamless data movement between cloud storage and analytics platforms as requested.",0.6666666666666666
189,Ground Truth (HuggingFace),225,1,"The code uses the native ShortCircuitOperator from airflow.operators.python, which is the idiomatic Airflow way to implement short-circuit logic. It does not wrap functionality in PythonOperator + Hook patterns. The code also properly uses EmptyOperator for demonstration purposes and leverages TriggerRule.ALL_DONE appropriately. All operators and patterns are standard Airflow constructs.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.operators.python.ShortCircuitOperator, airflow.operators.empty.EmptyOperator, airflow.models.baseoperator.chain, airflow.utils.trigger_rule.TriggerRule). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or any test harness boilerplate code.",1,"The DAG fully demonstrates conditional task execution using short-circuit logic as requested. It shows multiple scenarios: (1) a condition that returns True allowing downstream tasks to execute, (2) a condition that returns False causing downstream tasks to be skipped, and (3) a more complex chain demonstrating how short-circuit affects parallel branches with trigger rules. The code effectively showcases how tasks can be dynamically skipped or executed based on predefined boolean conditions (lambda: True and lambda: False), fulfilling the instruction to 'demonstrate conditional task execution using short-circuit logic' and 'showcase how tasks can be dynamically skipped or executed depending on specific criteria.'",1.0
190,Ground Truth (HuggingFace),226,1,"The code correctly uses the DockerSwarmOperator from airflow.providers.docker.operators.docker_swarm, which is the idiomatic and native Airflow operator for Docker Swarm tasks. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator with appropriate parameters.",0,"The code contains test harness boilerplate that should not be present in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. These lines indicate this is example/test code rather than clean, production-ready code.",1,"The code fully satisfies the user instruction. It creates an Airflow pipeline using the DockerSwarmOperator for container-based task scheduling, launches a CentOS container (image='centos:latest'), executes a brief sleep operation (command='/bin/sleep 10'), and automatically removes the container after completion (auto_remove=True). All required elements are present and correctly implemented.",0.6666666666666666
191,Ground Truth (HuggingFace),227,1,"The code uses appropriate Airflow operators for the task: DockerOperator from airflow.providers.docker.operators.docker for containerized operations, BashOperator for file detection, and ShortCircuitOperator for validation logic. These are the idiomatic, provider-specific operators designed for these purposes rather than wrapping everything in PythonOperators with hooks.",0,The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing imports that should not be present in production DAG code. This is a hallucination/leakage issue that makes the code non-production-ready.,1,"The DAG fulfills all requirements from the instruction: (1) monitors a specified input directory for new files using BashOperator with find command, (2) validates file presence using ShortCircuitOperator to check if data is available, (3) performs controlled file transfer using DockerOperator with containerized mv operation, (4) implements error handling through ShortCircuitOperator that stops execution if no file is found, and (5) includes logging capabilities through do_xcom_push and the print task. The workflow correctly chains these operations together.",0.6666666666666666
192,Ground Truth (HuggingFace),228,0,"The DAG uses only EmptyOperator placeholders throughout, which is not idiomatic for a production data quality workflow. A proper implementation should use specific operators like PythonOperator for data analysis, BranchPythonOperator or BranchSQLOperator for the integrity check branching logic, and EmailOperator or SlackOperator for notifications. The current code provides no actual functionality - it's merely a skeleton with labels showing flow direction.",1,"The code uses only standard Airflow libraries (airflow.models.dag, airflow.operators.empty, airflow.utils.edgemodifier) with no test harness imports or internal testing modules. It is clean from a hallucination perspective, though functionally incomplete.",0,"The instruction requires a workflow that 'ingests raw data, performs initial analysis, and checks data integrity' with branching that 'either saves clean data and generates a report, or handles errors by describing issues and sending notifications.' While the DAG structure and labels suggest the correct flow, it fails to implement any actual business logic. There is no data ingestion, no analysis logic, no integrity checking logic, no branching operator (like BranchPythonOperator), no data saving mechanism, no report generation, and no notification sending. The use of EmptyOperators means none of the required functionality is actually implemented.",0.3333333333333333
193,Ground Truth (HuggingFace),229,1,"The code uses the native OpenAITriggerBatchOperator from airflow.providers.openai.operators.openai for triggering batch operations, and OpenAIHook from airflow.providers.openai.hooks.openai for file operations. It leverages Airflow's task expansion with .expand() and uses the @task decorator for custom logic. This is idiomatic Airflow - using specific providers and operators designed for OpenAI integration rather than wrapping everything in generic PythonOperators.",0,The code contains test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG definition.,1,"The DAG fulfills the instruction requirements: (1) It builds an automated data enrichment pipeline using OpenAI to generate descriptive information about a static list of items (POKEMONS list), (2) It leverages Airflow's task expansion capabilities (generate_messages.expand(pokemon=POKEMONS)), and (3) It uses deferred execution capabilities (deferrable=True in OpenAITriggerBatchOperator). The pipeline generates messages for each Pokemon, batches them, triggers OpenAI batch processing with deferrable execution, and includes cleanup logic.",0.6666666666666666
194,Ground Truth (HuggingFace),230,1,"The code uses appropriate Airflow providers and operators for the task. It leverages CloudFirestoreExportDatabaseOperator for Firestore export, GCSCreateBucketOperator for Cloud Storage operations, BigQueryCreateTableOperator and BigQueryInsertJobOperator for BigQuery operations, and CloudDatastoreCommitOperator for Datastore operations. All operators are native to their respective providers rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear indicators of being a test/example file rather than production code. The docstring explicitly states 'Example Airflow DAG', uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, includes 'example' in the tags, and has START/END comment markers typical of documentation examples (e.g., '# [START howto_operator_export_database_to_gcs]'). While it doesn't import test harness modules directly, it's clearly test/example boilerplate rather than clean production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It: (1) Captures data from a NoSQL database (Firestore/Datastore) via CloudDatastoreCommitOperator, (2) Exports it to neutral storage format (Cloud Storage) using CloudFirestoreExportDatabaseOperator, (3) Prepares it for analytical querying by creating an external table in BigQuery from the exported data, and (4) Demonstrates querying the structured data. The workflow successfully transforms unstructured NoSQL data into a structured, analyzable format across Google Cloud platforms (Firestore → GCS → BigQuery).",0.6666666666666666
195,Ground Truth (HuggingFace),231,1,"The code correctly uses the Neo4jOperator from airflow.providers.neo4j.operators.neo4j, which is the idiomatic and native operator for executing Cypher queries against Neo4j. It does not wrap logic in a PythonOperator with a Hook, but instead uses the purpose-built operator for this task.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready DAG code.,0,"The instruction requires a daily schedule ('on a daily basis'), but the DAG is configured with 'schedule=None', which means it will not run automatically on a daily basis. While the code does use the current date as a dynamic parameter via '{{ds}}' template, the missing daily schedule means it fails to meet the core requirement of executing daily.",0.3333333333333333
196,Ground Truth (HuggingFace),233,1,"The code uses native Databricks providers and operators (DatabricksSqlOperator and DatabricksCopyIntoOperator) from airflow.providers.databricks.operators.databricks_sql. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operators for Databricks SQL operations. This is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific code like 'list(dag.tasks) >> watcher()' and references to system tests. This is not production-ready code and includes testing infrastructure that should not be present in a production DAG.",1,"The DAG fully demonstrates the requested functionality: (1) creates tables using DatabricksSqlOperator with CREATE TABLE statements, (2) inserts data using INSERT statements, (3) performs data transformations and selections with SELECT queries, (4) exports data to files using output_path and output_format parameters, (5) imports data using DatabricksCopyIntoOperator for loading from external file locations. All requested SQL operations (table creation, data selection, file export, and data import) are showcased using appropriate Databricks operators.",0.6666666666666666
197,Ground Truth (HuggingFace),234,1,"The code uses the native CloudSQLExecuteQueryOperator from airflow.providers.google.cloud.operators.cloud_sql, which is the idiomatic Airflow approach for executing queries on Cloud SQL instances. It does not wrap logic in PythonOperator with hooks, but instead relies on the purpose-built operator for this task.",1,"The code is clean and uses only standard Airflow libraries (airflow.models, airflow.providers.google.cloud.operators.cloud_sql) along with standard Python libraries (os, subprocess, datetime, pathlib, urllib.parse). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate code.",0,"The code is incomplete - it cuts off mid-definition in the postgres_kwargs dictionary and does not show the actual DAG instantiation or task definitions. While the SQL list demonstrates the required operations (CREATE TABLE, INSERT, DROP TABLE) and the setup suggests cross-database interaction between PostgreSQL and MySQL, the actual workflow/pipeline with task dependencies is not present. The instruction requires a complete 'automated database management workflow' that 'demonstrates cross-database interaction and query execution', but the provided code is only a partial configuration snippet without the DAG structure or operators instantiated.",0.6666666666666666
198,Ground Truth (HuggingFace),235,1,"The DAG uses appropriate Airflow providers and operators for the tasks: GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket operations, LocalFilesystemToGCSOperator for file uploads, and DataflowTemplatedJobStartOperator and DataflowStartFlexTemplateOperator for template-based data processing. No generic PythonOperator wrappers are used where native operators are available.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes a watcher() call at the end. These are testing utilities not appropriate for production code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, which are test-specific.",1,"The DAG fulfills all requirements from the instruction: it creates a bucket (GCSCreateBucketOperator), uploads files (LocalFilesystemToGCSOperator for both CSV and schema files), performs template-based data processing (DataflowTemplatedJobStartOperator and DataflowStartFlexTemplateOperator), and includes automated cleanup of temporary resources (GCSDeleteBucketOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of task success/failure).",0.6666666666666666
199,Ground Truth (HuggingFace),236,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Vision tasks. It imports and uses CloudVisionCreateProductSetOperator, CloudVisionCreateProductOperator, CloudVisionCreateReferenceImageOperator, and other Vision-specific operators from airflow.providers.google.cloud.operators.vision. It also uses GCSCreateBucketOperator and GCSToGCSOperator for GCS operations. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from SYSTEM_TESTS_ENV_ID environment variable, SYSTEM_TESTS_GCP_PROJECT, and includes comments like '[START howto_operator_vision_*]' and '[END howto_operator_vision_*]' which are documentation/testing markers. The DAG_ID includes 'example_gcp_vision_autogenerated_id' and tags include 'example', suggesting this is test/example code rather than production-ready code. Additionally, the code is incomplete (cuts off at 'task_id' without completion).",0,"The code is incomplete and cuts off mid-operator definition ('task_id' without value or closing). While it demonstrates creating product sets, products, and reference images which aligns with 'creating, managing, and analyzing image resources', it does not show the complete end-to-end workflow. The 'analyzing' component is not clearly implemented (though annotate_image_request is defined, no operator uses it). The workflow is not fully demonstrated due to the incomplete code, and there's no clear chain of operations showing the systematic workflow from creation through management to analysis.",0.3333333333333333
200,Ground Truth (HuggingFace),237,1,"The code uses native Airflow operators appropriately. It leverages DynamoDBToS3Operator from airflow.providers.amazon.aws.transfers.dynamodb_to_s3 for the data transfer, S3CreateBucketOperator and S3DeleteBucketOperator for S3 operations. While it does use @task decorators for setup/teardown operations (which is acceptable for orchestration logic), the core data transfer functionality uses the idiomatic DynamoDBToS3Operator rather than wrapping hooks in PythonOperators.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing module, not a standard Airflow library. Additionally, it uses 'sys_test_context_task = SystemTestContextBuilder().build()' which is test harness boilerplate. These are clear indicators of test code leakage into what should be production-ready code.",1,"The code fulfills the user instruction requirements: (1) It transfers data from DynamoDB to S3 using DynamoDBToS3Operator, (2) It implements point-in-time recovery capabilities via enable_point_in_time_recovery() function with proper retry logic, (3) It demonstrates incremental data export through the incremental_export task group that uses start_time and end_time parameters, and (4) It shows backup strategies for cloud-based database management with proper setup, export, and teardown operations.",0.6666666666666666
201,Ground Truth (HuggingFace),238,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Catalog tasks. It imports and uses CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, CloudDataCatalogCreateTagOperator, CloudDataCatalogCreateTagTemplateOperator, and GCSCreateBucketOperator from airflow.providers.google.cloud.operators. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID' and PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT' environment variables, which are typical patterns from Airflow's internal system tests. The DAG_ID is set to 'datacatalog_search_catalog' and resource names include patterns like 'SYSTEM_TESTS' references. Additionally, the code includes START/END comment markers (e.g., '# [START howto_operator_gcp_datacatalog_create_entry_group]') which are documentation generation markers used in Airflow's example/test code, not production code.",1,"The code fulfills the user instruction to build a Google Cloud Data Catalog workflow that demonstrates creating and managing metadata for a GCS bucket. It creates a GCS bucket, creates an entry group, creates an entry for the GCS bucket with file patterns, creates a tag template, and creates tags. The workflow showcases programmatic registration, tagging, and cataloging of cloud storage resources for data governance and discoverability. Although the code is truncated, the visible portions demonstrate all the key requested components.",0.6666666666666666
202,Ground Truth (HuggingFace),239,1,"The code uses idiomatic Airflow patterns throughout. It leverages the TaskFlow API with @task decorators, @task.virtualenv, and @task.external_python decorators which are the recommended Airflow 2.0+ patterns for Python tasks. It does not wrap logic in PythonOperator + Hook patterns but instead uses native task decorators designed for these purposes. The use of templates_dict and templates_exts for SQL rendering is also idiomatic Airflow.",0,The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(example_dag)'. These are internal testing modules that should not appear in production-ready code. This is a hallucination/leakage issue that makes the code unsuitable for production use without modification.,1,"The DAG fully adheres to the instruction to build an educational data processing pipeline exploring Airflow's task flexibility. It implements: (1) a task that prints context (print_context), (2) a task that logs SQL queries (log_sql with template rendering), (3) time-based operations (my_sleeping_function with varying sleep times), and (4) isolated Python environment executions (callable_virtualenv using virtualenv and callable_external_python using external Python). All requested components are present and properly demonstrated.",0.6666666666666666
203,Ground Truth (HuggingFace),241,1,"The code uses native Airflow operators throughout, specifically leveraging Google Cloud provider operators (CloudVisionImageAnnotateOperator, CloudVisionDetectTextOperator, CloudVisionTextDetectOperator, CloudVisionDetectImageLabelsOperator, CloudVisionDetectImageSafeSearchOperator, GCSToGCSOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator). It does not wrap functionality in PythonOperator with hooks, which is the correct idiomatic approach for Airflow.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', and the DAG_ID includes 'example_gcp_vision_annotate_image' which follows the pattern of system test examples. The comments like '[START howto_operator_vision_retry_import]' and '[END howto_operator_vision_retry_import]' are documentation markers typically found in Airflow's internal test/example code, not production-ready code. This is test infrastructure leakage.",1,"The DAG fulfills the instruction requirements: (1) It copies an image file between buckets using GCSToGCSOperator (from cloud-samples-data to a created bucket), (2) It performs multiple types of computer vision analysis using different Google Cloud Vision operators (annotate_image for logo detection, detect_text for text detection, document_detect_text for document text detection), and (3) It demonstrates modular, configurable task chains with proper orchestration. The code is incomplete (cuts off mid-task) but the visible portion shows adherence to the core requirements of advanced cloud service integration with multiple vision analysis types.",0.6666666666666666
204,Ground Truth (HuggingFace),242,1,"The DAG uses native Airflow operators throughout: TrinoToGCSOperator for data extraction, BigQueryCreateEmptyDatasetOperator for dataset creation, BigQueryCreateTableOperator for external table creation, GCSCreateBucketOperator for bucket creation, and appropriate delete operators for cleanup. No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that leverages provider-specific operators.",0,"The code contains clear test harness imports and patterns that should not be in production code: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' and 'from system.openlineage.operator import OpenLineageTestOperator'. These are internal testing modules. Additionally, the code uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are test-specific patterns. The OpenLineageTestOperator is imported but never used in the visible code, suggesting this is test boilerplate.",0,"The code is incomplete (truncated at 'SOURCE_SCHEMA_COLU') and lacks proper task dependencies to orchestrate the complete lifecycle. While it includes operators for bucket creation, dataset creation, data extraction (TrinoToGCSOperator), and external table creation, there are no visible task dependencies defined (no >> or << operators) to create the workflow sequence. The cleanup tasks (delete_dataset, delete_bucket) exist but their placement in the workflow is unclear. The instruction requested a 'robust, idempotent workflow that handles resource provisioning and cleanup automatically', but the orchestration logic is missing or incomplete.",0.3333333333333333
205,Ground Truth (HuggingFace),243,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Vision API tasks. It imports and uses CloudVisionCreateProductSetOperator, CloudVisionCreateProductOperator, CloudVisionCreateReferenceImageOperator, and other Vision-specific operators from airflow.providers.google.cloud.operators.vision. It also uses GCSCreateBucketOperator and GCSToGCSOperator for GCS operations. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear indicators of test harness boilerplate and system test patterns. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', and the DAG_ID includes 'example_gcp' prefix which is typical of Airflow's system tests. The variable naming conventions (e.g., BUCKET_NAME_SRC = 'cloud-samples-data') and the presence of START/END comment markers like '[START howto_operator_vision_retry_import]' are characteristic of Airflow's documentation/testing examples rather than production-ready code. While it doesn't explicitly import from tests_common, it clearly contains test infrastructure patterns.",0,"The code is incomplete - it cuts off mid-statement at 'task_id' in the product_set_create operator definition. More critically, while it sets up infrastructure for image processing (creates buckets, transfers images, creates product sets and products), it does not demonstrate 'automated image analysis' as requested. The annotate_image_request is defined but never used in any operator. The instruction asks for end-to-end image processing and machine learning operations including automated image analysis, but the DAG only shows catalog creation and management without actually performing image analysis operations like logo detection, label detection, or other Vision API analysis features.",0.3333333333333333
206,Ground Truth (HuggingFace),244,1,"The code uses native Airflow AWS provider operators designed specifically for EKS management: EksCreateClusterOperator, EksCreateNodegroupOperator, EksDeleteClusterOperator, EksDeleteNodegroupOperator, EksPodOperator, EksClusterStateSensor, and EksNodegroupStateSensor. It does not wrap these in PythonOperators unnecessarily. The only PythonOperators used (@task decorators) are for EC2 launch template management, which is appropriate since there's no dedicated Airflow operator for launch templates and this is auxiliary infrastructure setup.",0,"The code contains clear test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'from tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator'. It also uses 'sys_test_context_task = SystemTestContextBuilder()...' which is a testing utility pattern. This is test infrastructure leakage that makes the code non-production-ready.",1,"The code fulfills the user instruction to design an automated workflow for provisioning and managing an Amazon EKS cluster with dynamic nodegroup creation. It demonstrates: (1) EKS cluster creation with EksCreateClusterOperator, (2) dynamic nodegroup creation with EksCreateNodegroupOperator, (3) infrastructure-as-code deployment using Airflow's AWS provider operators, (4) proper state management with sensors (EksClusterStateSensor, EksNodegroupStateSensor), and (5) validation through EksPodOperator to run a test pod. The workflow is complete and demonstrates the requested infrastructure-as-code pattern, even though the code is incomplete at the end (truncated).",0.6666666666666666
207,Ground Truth (HuggingFace),245,0,The DAG violates idiomatic Airflow patterns by using a @task decorator with direct Google Cloud Storage client library calls in 'upload_csv_file_to_gcs()' instead of using the appropriate Airflow operator. The correct approach would be to use GCSObjectUpdateOperator or a similar native operator. The pattern 'storage_client = storage.Client()' followed by manual blob operations wrapped in a PythonOperator (via @task) is exactly the anti-pattern described in the scoring criteria - using generic Pythonic patterns instead of native Operators.,0,"The code contains clear indicators of test harness boilerplate that should not appear in production code. Specifically: 1) Environment variables like 'SYSTEM_TESTS_GCP_PROJECT' and 'SYSTEM_TESTS_ENV_ID' are testing artifacts, 2) The RESOURCE_DATA_BUCKET = 'airflow-system-tests-resources' is a test resource reference, 3) Comments like '# Example DAG for AutoML Translation' and the overall structure suggest this is adapted from Airflow's system tests rather than being clean production code. While it doesn't directly import test modules, it contains test infrastructure patterns.",1,"The DAG successfully demonstrates a complete machine learning workflow with Google Cloud services as requested. It includes: 1) Cloud storage interactions (GCS bucket creation, file uploads, file copying), 2) Dataset creation (AutoMLCreateDatasetOperator), 3) Data import (AutoMLImportDataOperator), 4) Model training initiation (AutoMLTrainModelOperator - though truncated), 5) Clear task dependencies showing operational stages, and 6) Specifically implements a translation model workflow. Despite the code being truncated, the visible portions fulfill the instruction's requirements for demonstrating data movement, dataset creation, and automated model training with clear orchestration steps.",0.3333333333333333
208,Ground Truth (HuggingFace),246,1,"The code uses native Airflow operators appropriately: BashOperator for system command execution and DockerOperator for containerized tasks. It does not wrap these in PythonOperator with hooks, but instead uses the provider-specific operators directly (airflow.providers.docker.operators.docker.DockerOperator and airflow.providers.standard.operators.bash.BashOperator).",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and creates 'test_run = get_test_run(dag)'. These are internal testing utilities that represent hallucination/leakage from test examples into what should be production-ready code.",1,"The DAG successfully demonstrates all three requirements from the instruction: (1) task chaining is shown with the dependency pattern 't1 >> [t2, t3] >> t4', (2) system command execution is demonstrated via BashOperator tasks (print_date, sleep, print_hello), and (3) containerized task running is shown with the DockerOperator (docker_op_tester). The pipeline coordinates different types of computational tasks as requested.",0.6666666666666666
209,Ground Truth (HuggingFace),247,1,"The code uses the native KylinCubeOperator from Airflow providers throughout the DAG for all Kylin operations (build, refresh, merge, disable, purge). It does not wrap Kylin functionality in PythonOperator + Hook patterns. The only PythonOperator usage is for generating build times via @dag.task decorator, which is appropriate for simple data generation. The code demonstrates proper use of XComArgs for passing data between tasks, which is idiomatic Airflow 2.0+ pattern.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'test_run = get_test_run(dag)' and adds a watcher task. These are internal testing utilities that indicate this is test/example code rather than clean production-ready code.",1,"The DAG successfully demonstrates data cube transformation and maintenance processes as requested. It showcases systematic processing through: (1) building cubes with different time ranges (build_task1, build_task2, build_task3), (2) updating via refresh operation (refresh_task1), (3) merging cube segments (merge_task), and (4) managing cube lifecycle through disable and purge operations. The pipeline demonstrates a complete workflow for analytical data structure management using Kylin operators, fulfilling the instruction's requirements.",0.6666666666666666
210,Ground Truth (HuggingFace),248,1,The code uses the appropriate Kubernetes-specific operators from airflow.providers.cncf.kubernetes.operators.resource (KubernetesCreateResourceOperator and KubernetesDeleteResourceOperator) rather than wrapping Kubernetes logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Kubernetes resource management.,0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', creates a test_run variable, and adds a watcher() task to the DAG. These are internal testing modules and patterns that indicate this is example/test code, not production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) creates a PVC with 5GB storage allocation (storage: 5Gi), (2) deletes the PVC, (3) runs daily (schedule=timedelta(days=1)), and (4) demonstrates basic Kubernetes resource lifecycle management through the sequential create-then-delete workflow. The business logic is complete and correct.",0.6666666666666666
211,Ground Truth (HuggingFace),249,1,"The code uses native Airflow operators appropriately. It uses BigQueryToGCSOperator from airflow.providers.google.cloud.transfers.bigquery_to_gcs for the core transfer operation, along with other specific operators like BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, GCSCreateBucketOperator, etc. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: (1) imports 'from tests_common.test_utils.watcher import watcher' and uses 'watcher()' in the DAG, (2) imports 'from tests_common.test_utils.system_tests import get_test_run', (3) includes 'test_run = get_test_run(dag)' at the end, (4) imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', and (5) has comments like 'TEST SETUP', 'TEST BODY', 'TEST TEARDOWN'. This is clearly a system test file, not production-ready code.",1,"The code fulfills the user instruction to create a data migration workflow that transfers data from BigQuery to GCS. The BigQueryToGCSOperator successfully demonstrates extracting data from a BigQuery dataset and exporting it to a cloud storage location. While the code includes setup/teardown tasks for testing purposes, the core business logic requested (BigQuery to GCS transfer) is properly implemented.",0.6666666666666666
212,Ground Truth (HuggingFace),250,1,"The code uses native Databricks providers and operators (DatabricksSqlOperator and DatabricksCopyIntoOperator) which are the idiomatic way to interact with Databricks in Airflow. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operators for SQL execution and data import operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_t' (incomplete). These are testing utilities that should not be present in production DAG code.",1,"The DAG fulfills the instruction requirements: it demonstrates a complete data integration process from external file sources to Databricks SQL environment. It includes table creation (create task), data import from external storage (import_csv using DatabricksCopyIntoOperator), transformation/manipulation (select operations), and output generation (select_into_file writing to JSONL). The workflow showcases flexibility across different storage systems (Azure Blob Storage via abfss://) and compute environments (Databricks SQL).",0.6666666666666666
213,Ground Truth (HuggingFace),251,1,"The code uses appropriate Airflow providers and operators for the task. It leverages BeamRunJavaPipelineOperator for running Apache Beam pipelines, PubSubCreateTopicOperator and PubSubDeleteTopicOperator for Pub/Sub management, GCSCreateBucketOperator and GCSDeleteBucketOperator for GCS bucket operations, and DataflowStopJobOperator for Dataflow job management. These are all idiomatic, purpose-built operators rather than generic PythonOperator wrappers around hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes a watcher() call with the comment 'This test needs watcher in order to properly mark success/failure'. These are testing utilities not appropriate for production code. Additionally, the code uses ENV_ID and SYSTEM_TESTS_* environment variables typical of test environments, and includes comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN'.",1,"The code fulfills the user instruction to develop a streaming data processing system. It captures real-time taxi ride events from a public Pub/Sub topic (INPUT_TOPIC = 'projects/pubsub-public-data/topics/taxirides-realtime'), transforms them using Apache Beam (via BeamRunJavaPipelineOperator with a Java pipeline), routes processed data through Google Cloud Pub/Sub topics (creates OUTPUT_TOPIC and configures the pipeline to write to it), and includes automated infrastructure management (creates/deletes GCS buckets, Pub/Sub topics, and manages Dataflow job lifecycle). The solution is scalable and repeatable, demonstrating cloud-native data processing techniques as requested.",0.6666666666666666
214,Ground Truth (HuggingFace),252,1,"The code uses native Airflow providers and operators designed specifically for EKS infrastructure management. It properly uses EksCreateClusterOperator, EksCreateNodegroupOperator, EksClusterStateSensor, EksNodegroupStateSensor, and EksPodOperator from airflow.providers.amazon.aws. The code does include some @task decorated functions for launch template management, but these are appropriate since there are no dedicated Airflow operators for EC2 launch templates, and the boto3 calls are minimal auxiliary operations. The core infrastructure deployment logic follows idiomatic Airflow patterns.",0,"The code contains clear test harness imports and boilerplate that should not be in production code. Specifically, it imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'from tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator'. It also uses SystemTestContextBuilder() which is a test utility pattern. These are internal testing modules that indicate this is example/test code rather than production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It provisions a Kubernetes (EKS) cluster, implements secure metadata handling (IMDSv2 via launch template with HttpTokens required), demonstrates containerized workload execution (EksPodOperator running aws-cli container), creates a scalable and repeatable process (parameterized cluster/nodegroup creation), and includes compliance steps (IMDSv2 enforcement) and verification steps (sensors to await cluster and nodegroup active states). The pipeline is complete for infrastructure deployment with all requested elements.",0.6666666666666666
215,Ground Truth (HuggingFace),253,1,"The code uses native Airflow providers and operators designed specifically for Google Dataprep (DataprepRunFlowOperator, DataprepCopyFlowOperator, DataprepGetJobGroupOperator, etc.) and GCS (GCSCreateBucketOperator, GCSDeleteBucketOperator). It also uses GoogleDataprepHook appropriately within @task decorators for operations that don't have dedicated operators (like create_imported_dataset, create_flow). This is idiomatic Airflow - using specific providers where available and hooks for custom logic, rather than wrapping everything in generic PythonOperators.",0,"The code contains clear hallucination/test harness leakage. It imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is an internal testing module, not a standard Airflow library. The code also includes system test patterns with environment variables like 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_DATAPREP_TOKEN', 'SYSTEM_TESTS_GCP_PROJECT', and references to 'gs://airflow-system-tests-resources/' which are all indicators of test infrastructure rather than production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) Ingests raw data from an external source (DATASET_URI from GCS), (2) Transforms it using a cloud-based data preparation tool (Google Dataprep with flow creation and execution), (3) Generates clean, structured output in specified cloud storage (writes CSV to GCS_BUCKET_PATH), and (4) Creates a reproducible, configurable workflow that can be scaled (uses environment variables, parameterized bucket names, and Dataprep's scalable infrastructure). The code demonstrates a complete data engineering pipeline as requested.",0.6666666666666666
216,Ground Truth (HuggingFace),254,1,"The code uses native Airflow operators designed for Teradata operations. It uses TeradataOperator for SQL operations and TeradataToTeradataOperator for the data transfer, which are specific providers from airflow.providers.teradata. This is idiomatic Airflow - using purpose-built operators rather than wrapping logic in PythonOperator with hooks.",0,"The code contains test harness boilerplate that indicates this is from Airflow's system tests. Specifically: (1) ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID') which is a testing pattern, (2) Multiple START/END comment markers like '[START teradata_to_teradata_transfer_operator_howto_guide]' which are documentation extraction markers used in Airflow's test suite, and (3) The overall structure follows Airflow's system test patterns. This is not clean, production-ready code but rather test/documentation code.",1,"The DAG fulfills the instruction requirements: (1) Source data preparation - creates source table, inserts data, and reads to verify (create_src_table, insert_data_src, read_data_src), (2) Data movement - transfers data from source to destination using TeradataToTeradataOperator (transfer_data), and (3) Destination table validation - creates destination table and reads data to validate (create_dest_table, read_data_dest). The pipeline demonstrates a systematic, traceable approach with clear steps from source to destination, including cleanup tasks.",0.6666666666666666
217,Ground Truth (HuggingFace),255,1,"The code uses native Airflow operators designed for the specific tasks: CloudTextToSpeechSynthesizeOperator for text-to-speech synthesis, CloudTranslateSpeechOperator for speech translation, and GCS operators for bucket management. It does not wrap these operations in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test setup/teardown comments and a watcher() call. The line 'test_run = get_test_run(dag)' is also present. This is not production-ready code and includes testing infrastructure that should not be in a production DAG.",1,"The DAG fulfills the user instruction to create a multi-step cloud-based audio processing workflow that chains together Google Cloud operators for text-to-speech synthesis and speech translation. It demonstrates task dependencies (create_bucket >> text_to_speech_synthesize_task >> translate_speech_task >> translate_speech_task2 >> delete_bucket), cloud service integration (GCS, Text-to-Speech, Translate Speech), and automated workflow management. The workflow logically synthesizes text to speech, stores it in GCS, then translates the speech, meeting the core requirements.",0.6666666666666666
218,Ground Truth (HuggingFace),257,0,"The DAG uses @task decorators with boto3 client calls directly (create_cluster, register_task_definition, delete_task_definition, delete_cluster) instead of using native Airflow AWS providers operators. While EcsRunTaskOperator is used correctly, the cluster creation and task definition registration should use EcsCreateClusterOperator and EcsRegisterTaskDefinitionOperator if available, or at minimum use EcsHook instead of raw boto3 clients. This is a classic anti-pattern of wrapping boto3 logic in PythonOperator-style tasks rather than using idiomatic Airflow operators.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing utility module, not a standard Airflow library. The use of SystemTestContextBuilder() and test_context variables strongly suggests this is test harness boilerplate that has leaked into what should be production code. This is exactly the type of hallucination/leakage that should be avoided.",0,"The instruction asks for creating an ECS cluster, registering a task definition, running a container, and cleaning up resources. While the DAG includes create_cluster and register_task_definition tasks, they are defined but never actually called/chained in the DAG workflow - only create_task_definition (register) is used. The create_cluster task is defined but not executed in the DAG flow. Additionally, the code is incomplete (cuts off at 'task='). The cleanup tasks (delete_task_definition, delete_cluster) are defined but the DAG structure is incomplete, so we cannot verify they are properly chained. The DAG does not fully demonstrate the complete workflow as requested.",0.0
219,Ground Truth (HuggingFace),259,1,"The code uses native Airflow operators designed for the specific tasks: BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryInsertJobOperator for BigQuery operations, and DataflowStartYamlJobOperator for Dataflow. It does not wrap logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a test harness module. The docstring also mentions 'This test requires...' indicating this is test boilerplate. Additionally, the code references system test environment variables like 'SYSTEM_TESTS_GCP_PROJECT' and 'SYSTEM_TESTS_ENV_ID'. These are internal testing patterns that should not appear in production-ready code.",1,"The DAG fulfills the user instruction to create an employee salary data processing pipeline. It: (1) loads employee salary information into BigQuery (create_bq_dataset, create_bq_input_table, insert_data_into_bq_table tasks with employee data including emp_name, salary, country fields), (2) applies transformations using Google Cloud Dataflow (start_dataflow_yaml_job using DataflowStartYamlJobOperator), and (3) writes results to a new BigQuery table (configured via BQ_OUTPUT_TABLE variable). The workflow matches the requested business logic.",0.6666666666666666
220,Ground Truth (HuggingFace),260,1,"The code uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic and recommended approach for executing SQL queries in Airflow. It does not wrap Trino logic in PythonOperator with hooks, but instead uses the native SQL operator designed for this purpose. The operator properly leverages Airflow's connection management system for Trino.",0,"The code contains test harness boilerplate at the end that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules that represent hallucination/leakage from test examples into what should be production-ready code.",1,"The DAG fully adheres to the instruction. It creates a data engineering workflow that: (1) creates a schema (trino_create_schema), (2) generates tables with predefined structures (trino_create_table, and additional tables in trino_multiple_queries), (3) inserts sample city records (trino_insert and multiple inserts in trino_multiple_queries), and (4) uses Trino as the data processing platform via SQLExecuteQueryOperator. The pipeline demonstrates systematic building and populating of database objects with incremental complexity, including templated and parameterized queries.",0.6666666666666666
221,Ground Truth (HuggingFace),261,1,"The code uses appropriate Airflow providers and operators for the task. It leverages GCS-specific operators like GCSCreateBucketOperator, LocalFilesystemToGCSOperator, GCSToGCSOperator, and GCSSynchronizeBucketsOperator from airflow.providers.google.cloud packages. It does not wrap GCS operations in generic PythonOperator + Hook patterns, but instead uses the native operators designed for Google Cloud Storage interactions.",0,"The code contains clear hallucinations and test harness boilerplate. It imports from 'system.google' (DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID) and 'system.openlineage.operator' (OpenLineageTestOperator), which are internal testing modules not part of standard Airflow libraries. The code also includes system test patterns with ENV_ID and SYSTEM_TESTS_ENV_ID environment variables, indicating this is test infrastructure code rather than production-ready code.",0,"The code is incomplete - it cuts off mid-definition with '[START howto_synch_bucke' comment. While it demonstrates file generation (BashOperator creating random binary file), bucket creation, and file uploads, the critical cross-bucket synchronization component appears to be missing or incomplete. The instruction specifically requested 'cross-bucket synchronization' and 'data transfer and replication strategies', but the DAG ends before implementing the GCSSynchronizeBucketsOperator or GCSToGCSOperator tasks that would fulfill this requirement. The _assert_copied_files_exist function suggests there should be copy operations, but they are not present in the provided code.",0.3333333333333333
222,Ground Truth (HuggingFace),262,1,"The code uses native Airflow Vertex AI operators (CreateAutoMLTabularTrainingJobOperator, DeleteAutoMLTrainingJobOperator, CreateDatasetOperator, DeleteDatasetOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",1,"The code uses only standard Airflow libraries and official Google Cloud provider operators. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. While there are comments like '# TEST SETUP' and '# TEST BODY', these are just code comments and don't import or execute test utilities. The code is production-ready.",1,"The DAG fully satisfies the user instruction requirements: (1) Creates a dataset using CreateDatasetOperator, (2) Trains an AutoML model using CreateAutoMLTabularTrainingJobOperator, (3) Automatically cleans up resources after training using DeleteAutoMLTrainingJobOperator and DeleteDatasetOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of training success/failure. The task dependencies are clearly defined with proper sequencing, and it uses cloud-based infrastructure (Google Vertex AI). This demonstrates complete end-to-end ML orchestration.",1.0
223,Ground Truth (HuggingFace),264,0,"While the DAG uses many native Airflow operators (RedshiftCreateClusterOperator, S3CreateBucketOperator, etc.), it wraps the connection creation logic in a custom @task decorator (create_connection function) instead of using idiomatic Airflow patterns. The connection setup should ideally be handled through Airflow's connection management UI or environment variables, not programmatically created within a task. Additionally, the use of S3ToSqlOperator is imported but the code is truncated, so we cannot verify if it's properly used. The manual session management for creating connections is not the idiomatic Airflow approach.",0,"The code explicitly imports and uses internal testing modules that should not appear in production code: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'from tests.system.utils.watcher import watcher'. It also uses SystemTestContextBuilder() which is clearly test harness boilerplate. The 'watcher' import is present but not used in the visible code. These are clear indicators of test code leakage into what should be production-ready code.",0,"The code is incomplete (truncated at 'cluster_identifier=redshift_') and cannot be fully evaluated. However, from what is visible, the DAG attempts to address the core requirements: it provisions cloud resources (Redshift cluster, S3 bucket), loads sample cocktail data to S3, and establishes connections. The instruction asks for 'transfers cocktail recipe data from an S3 bucket to a Redshift database' but the actual S3ToSqlOperator usage is not visible due to truncation. The create_table task using RedshiftDataOperator is incomplete. Without seeing the complete data transfer logic and table creation SQL, we cannot confirm full instruction adherence. The visible code shows setup but not the complete end-to-end data migration pipeline.",0.0
224,Ground Truth (HuggingFace),265,1,"The code uses native SageMaker operators from airflow.providers.amazon.aws.operators.sagemaker (SageMakerCreateNotebookOperator, SageMakerDeleteNotebookOperator, SageMakerStartNoteBookOperator, SageMakerStopNotebookOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for AWS SageMaker operations.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. These are internal testing modules that indicate this is test code, not production-ready DAG code.",1,"The DAG successfully demonstrates step-by-step operational control of a cloud-based machine learning notebook using AWS SageMaker operators. It showcases dynamic instance management with clear, sequential task dependencies: create → stop → start → stop → delete. The workflow illustrates infrastructure management tasks as requested in the instruction.",0.6666666666666666
225,Ground Truth (HuggingFace),266,1,"The code uses native Airflow providers and operators designed specifically for CloudFormation tasks. It properly uses CloudFormationCreateStackOperator, CloudFormationDeleteStackOperator, CloudFormationCreateStackSensor, and CloudFormationDeleteStackSensor from airflow.providers.amazon.aws. No generic PythonOperator wrappers are used where native operators exist.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test/example code rather than clean production-ready code. The sys_test_context_task and watcher() calls are also test-specific patterns.",1,"The DAG fulfills all requirements from the instruction: (1) Creates an AWS SQS queue using CloudFormation, (2) Deletes the stack for teardown, (3) Uses proper Airflow operators and sensors for CloudFormation management, (4) Implements error handling via TriggerRule.ALL_DONE on the delete task and OnFailure='DELETE' in stack parameters, and (5) Establishes sequential task execution through the chain() function. The workflow demonstrates complete infrastructure provisioning and teardown as requested.",0.6666666666666666
226,Ground Truth (HuggingFace),268,1,"The code uses native Airflow operators throughout: GCSCreateBucketOperator, GCSSynchronizeBucketsOperator, CreateDatasetOperator, ImportDataOperator, CreateAutoMLVideoTrainingJobOperator, DeleteAutoMLTrainingJobOperator, DeleteDatasetOperator, and GCSDeleteBucketOperator. It does not wrap functionality in PythonOperators with hooks, but instead uses the specific providers designed for Google Cloud and Vertex AI tasks. This is idiomatic Airflow.",1,"The code imports only standard Airflow libraries and Google Cloud provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears production-ready with proper imports from airflow.models.dag, airflow.providers.google.cloud.operators, and airflow.utils.trigger_rule.",0,"The code is incomplete - it cuts off at 'bucket_name=VIDEO_G' in the delete_bucket task, making it syntactically invalid. More critically, while the DAG includes data ingestion (GCS operations), dataset creation, and model training operations, it lacks explicit task dependencies to orchestrate the workflow sequence. The instruction asks to 'orchestrate a complete machine learning workflow' with a 'systematic task sequence', but no task dependencies (using >> or << operators) are defined to establish the execution order. Without these dependencies, tasks may execute in parallel or random order rather than the systematic sequence required for a proper ML workflow (ingestion → dataset creation → import → training → cleanup).",0.6666666666666666
227,Ground Truth (HuggingFace),270,1,"The code uses native Airflow providers and operators specifically designed for Vertex AI tasks. It properly uses CreateDatasetOperator, ImportDataOperator, CreateAutoMLImageTrainingJobOperator, CreateEndpointOperator, DeleteEndpointOperator, and other Vertex AI-specific operators from airflow.providers.google.cloud.operators.vertex_ai. This is the idiomatic way to interact with Google Cloud Vertex AI services in Airflow, rather than wrapping hooks in PythonOperators.",0,"The code contains clear indicators of test harness boilerplate and internal testing patterns. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', references 'airflow-system-tests-resources' bucket, and includes comments like '[START how_to_cloud_vertex_ai_create_endpoint_operator]' and '[END how_to_cloud_vertex_ai_create_endpoint_operator]' which are documentation generation markers used in Airflow's internal test suite. The DAG_ID 'vertex_ai_endpoint_service_operations' and the overall structure strongly suggest this is copied from Airflow's system tests rather than being production-ready code.",0,"The code is incomplete - it cuts off mid-statement with 'list_endpoint' at the end without completing the operator definition. More critically, while it sets up dataset creation, data import, and model training, it does not complete the full ML lifecycle. The code is missing the actual model deployment (DeployModelOperator is imported but never used), model undeployment, and cleanup tasks. The task dependencies are not properly defined to show the complete end-to-end workflow requested. The instruction asked for a complete ML lifecycle showcasing task dependencies and cloud service interactions, but this incomplete code fails to deliver that.",0.3333333333333333
228,Ground Truth (HuggingFace),271,1,"The code uses the PigOperator, which is the appropriate Airflow operator for executing Pig scripts. It does not wrap Pig logic in a PythonOperator with hooks, but instead uses the native operator designed for this task. This is idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. Additionally, there are unused variables (ENV_ID is defined but never used, and DAG_ID is set to 'example_adf_run_pipeline' but the actual dag_id is 'example_pig_operator').",1,"The code fulfills the user instruction to create a DAG that demonstrates basic Pig script execution in a local Hadoop environment. It runs a simple Pig command ('ls /;') with local execution mode ('-x local') as requested. The DAG is properly configured with appropriate metadata (schedule, start_date, catchup, tags) for a learning example.",0.6666666666666666
229,Ground Truth (HuggingFace),272,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Catalog operations. It imports and uses CloudDataCatalogCreateTagTemplateOperator, CloudDataCatalogCreateTagTemplateFieldOperator, CloudDataCatalogGetTagTemplateOperator, CloudDataCatalogRenameTagTemplateFieldOperator, and other specialized operators from airflow.providers.google.cloud.operators.datacatalog. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code imports from 'system.google' module with 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which appears to be an internal testing module. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, and includes test-oriented naming conventions (e.g., 'template_id_templ_{ENV_ID}'). These are clear indicators of test harness boilerplate that should not be present in production-ready code.",1,"The code fulfills the user instruction to create an automated metadata management pipeline for dynamic configuration and manipulation of data catalog tag templates. It demonstrates: (1) Creating tag templates with CloudDataCatalogCreateTagTemplateOperator, (2) Adding fields dynamically with CloudDataCatalogCreateTagTemplateFieldOperator, (3) Retrieving templates with CloudDataCatalogGetTagTemplateOperator, (4) Renaming fields with CloudDataCatalogRenameTagTemplateFieldOperator, and (5) Updating and deleting operations (visible in imports). The workflow enables systematic tracking and enhancement of metadata across cloud data environments as requested, though the code is truncated.",0.6666666666666666
230,Ground Truth (HuggingFace),273,1,"The code uses native dbt Cloud Airflow providers and operators throughout. It properly uses DbtCloudRunJobOperator for triggering jobs, DbtCloudJobRunSensor and DbtCloudJobRunAsyncSensor for monitoring, and DbtCloudGetJobRunArtifactOperator for retrieving artifacts. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from tests.system.utils import get_test_env_id', 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and usage of 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",1,"The DAG successfully demonstrates the requested functionality: it triggers dbt Cloud jobs using DbtCloudRunJobOperator, monitors execution status using both synchronous (DbtCloudJobRunSensor with deferrable option) and asynchronous (DbtCloudJobRunAsyncSensor) sensors, shows different methods of running jobs (synchronous with wait_for_termination=True by default, and asynchronous with wait_for_termination=False), and includes artifact retrieval. The workflow clearly demonstrates multiple approaches to running and tracking dbt Cloud job runs as requested.",0.6666666666666666
231,Ground Truth (HuggingFace),274,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Compute Engine tasks. It imports and uses ComputeEngineInsertInstanceTemplateOperator, ComputeEngineCopyInstanceTemplateOperator, and ComputeEngineInstanceGroupUpdateManagerTemplateOperator from airflow.providers.google.cloud.operators.compute. These are the appropriate, idiomatic operators for managing GCE instance templates rather than wrapping logic in PythonOperator with hooks.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. This is test harness boilerplate that should not be present in production-ready code. The code also uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are testing-specific patterns, indicating this is test infrastructure code rather than clean production code.",0,"The code is incomplete - it cuts off mid-operator definition (gce_instance_template_copy is not closed with a closing bracket). While it demonstrates creating, copying instance templates, the workflow is incomplete and doesn't show the full end-to-end process including the update of instance group manager templates and proper task dependencies. The instruction asked for a complete automated workflow showcasing best practices, but this truncated code cannot be executed as-is and doesn't demonstrate a complete, working workflow with proper task orchestration.",0.3333333333333333
232,Ground Truth (HuggingFace),275,1,"The code uses the KylinCubeOperator, which is a specific provider operator designed for Kylin cube operations. It does not wrap logic in PythonOperator + Hook patterns. The TaskFlow API usage with @dag.task for generating build times and XComArgs for data passing is idiomatic and appropriate for Airflow 3.0.0.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that indicate this is test code, not production-ready DAG code.",1,"The DAG successfully demonstrates all requested elements: (1) orchestration of data cube operations using KylinCubeOperator, (2) chained complex data processing tasks with build, refresh, merge, disable, and purge commands, (3) time-based segmentation using start_time and end_time parameters, (4) task dependencies clearly established, (5) XCom data passing via TaskFlow API with gen_build_time task, and (6) automated management with is_track_job parameter. All business requirements from the instruction are fulfilled.",0.6666666666666666
233,Ground Truth (HuggingFace),277,1,"The code uses native Airflow operators designed specifically for this use case. It employs LatestOnlyOperator, which is the idiomatic Airflow solution for ensuring only the most recent DAG run executes downstream tasks. It also properly uses EmptyOperator for demonstration and TriggerRule enum for controlling task execution behavior. No generic PythonOperator wrappers are used where specific operators exist.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.models.dag, airflow.operators.empty, airflow.operators.latest_only, airflow.utils.trigger_rule) and standard Python libraries (datetime, pendulum). There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no non-existent or hallucinated modules.",1,"The code fully addresses the user instruction. It demonstrates a data pipeline that ensures only the most recent workflow run is processed using LatestOnlyOperator, which skips downstream tasks for DAG runs that are not the latest. The DAG structure shows how tasks are skipped (task1, task3 via default trigger rule) versus tasks that run regardless (task4 with ALL_DONE trigger rule, task2 not downstream of latest_only). The catchup=False setting and the use of LatestOnlyOperator together prevent redundant task executions across multiple scheduling intervals, exactly as requested.",1.0
234,Ground Truth (HuggingFace),278,1,"The code uses the TaskFlow API with @task decorators and the .expand() method for dynamic task mapping, which is the idiomatic and recommended approach in modern Airflow (2.3+). It does not wrap logic unnecessarily in PythonOperator with hooks, but instead uses native Airflow patterns for dynamic task generation.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.sdk.DAG and airflow.sdk.task). There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no test utilities present.",1,"The first DAG (example_dynamic_task_mapping) perfectly fulfills the instruction: it takes a list of numbers [1, 2, 3], increments each number using dynamic task mapping (add_one.expand), and calculates their total sum (sum_it). This demonstrates exactly what was requested: dynamic task generation based on input data, incrementing numbers, and computing the sum. The second DAG provides an additional example of chained dynamic mapping but doesn't detract from meeting the core requirement.",1.0
235,Ground Truth (HuggingFace),280,0,"The DAG fails the idiomatic Airflow criterion because it wraps Dataprep Hook operations inside @task decorated functions (PythonOperator pattern) instead of using the native Dataprep Operators that are already imported. For example, 'create_imported_dataset' and 'create_flow' tasks use GoogleDataprepHook inside @task decorators, when the DAG already imports specific operators like DataprepCopyFlowOperator, DataprepDeleteFlowOperator, etc. The idiomatic approach would be to use these native operators directly rather than wrapping hook logic in Python callables.",0,"The code contains clear hallucination/leakage from test harness code. It imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is an internal testing module, uses environment variables like 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_DATAPREP_TOKEN', and 'SYSTEM_TESTS_GCP_PROJECT', and includes comments referencing 'airflow-system-tests-resources'. These are all indicators of test boilerplate that should not appear in production-ready code.",0,"The code is incomplete (truncated at 'body_reque') and cannot be evaluated for full instruction adherence. While it does show elements of cloud resource management (GCS bucket creation), data import (imported dataset), and attempts at transformation/export setup, the incomplete nature means we cannot verify if it fully implements the requested 'complex data processing workflow with multiple interconnected tasks' with 'robust error handling and connection management'. The code also lacks explicit error handling mechanisms beyond basic task structure.",0.0
236,Ground Truth (HuggingFace),281,1,"The code uses SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic and recommended approach for executing SQL queries in Airflow 3.0.0. It properly leverages the SQL operator designed for this task rather than wrapping logic in a PythonOperator with hooks. The conn_id is correctly passed via default_args, making it available to all SQL operators.",0,The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready code. This is a hallucination/leakage issue that makes the code unsuitable for production use without modification.,1,"The DAG fulfills the user instruction to create a MySQL database cleanup workflow that demonstrates dropping tables using SQL operators. It shows two methods: (1) inline SQL with DROP TABLE statement, and (2) external SQL file reference. The tasks are chained sequentially (drop_table_mysql_task >> mysql_task), demonstrating programmatic and sequential table removal as requested. The workflow clearly demonstrates the cleanup pattern requested.",0.6666666666666666
237,Ground Truth (HuggingFace),282,1,"The code uses native Airflow operators (EmptyOperator) and extends BaseOperator to create a custom EmptySkipOperator. It properly uses TriggerRule enums and follows Airflow's operator pattern. The implementation does not wrap logic unnecessarily in PythonOperators but instead creates proper operator classes, which is the idiomatic Airflow approach.",1,"The code imports only standard Airflow libraries (airflow.exceptions, airflow.models, airflow.operators, airflow.utils). There are no imports from test utilities, no test harness boilerplate like get_test_run(), and no internal testing modules. The code is clean and production-ready.",1,"The code fully satisfies the user instruction. It implements: (1) parallel pipeline branches via the create_test_pipeline function that creates two parallel paths (skip_operator and always_true), (2) configurable success conditions through the trigger_rule parameter, (3) demonstrates conditional execution/skipping with EmptySkipOperator that raises AirflowSkipException, and (4) shows different trigger rules (ALL_SUCCESS and ONE_SUCCESS) affecting task execution. The DAG effectively explores different task execution strategies as requested.",1.0
238,Ground Truth (HuggingFace),283,1,The code uses native Airflow operators designed specifically for ADLS operations: LocalFilesystemToADLSOperator for uploading files and ADLSDeleteOperator for deleting files. These are the appropriate provider-specific operators from airflow.providers.microsoft.azure rather than wrapping logic in PythonOperator with hooks.,0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', uses 'list(dag.tasks) >> watcher()', imports 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test infrastructure that indicate this is example/test code rather than clean production-ready code.",1,"The DAG fulfills the user instruction completely. It demonstrates: (1) uploading a local file to ADLS using LocalFilesystemToADLSOperator, (2) immediately deleting it using ADLSDeleteOperator, and (3) proper task dependency (upload_file >> remove_file) to ensure the delete happens after upload. This showcases the requested automated file transfer and cleanup processes in a cloud storage environment.",0.6666666666666666
239,Ground Truth (HuggingFace),284,1,"The DAG uses native Airflow Vertex AI operators throughout: CreateDatasetOperator, ImportDataOperator, CreateAutoMLImageTrainingJobOperator, DeleteAutoMLTrainingJobOperator, and DeleteDatasetOperator. These are the proper provider-specific operators from airflow.providers.google.cloud.operators.vertex_ai rather than wrapping logic in PythonOperator with hooks. This is idiomatic Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and includes a comment '### Everything below this line is not part of example ### Just for system tests purpose ###'. It also calls 'list(dag.tasks) >> watcher()' which is test infrastructure code. The last line appears to be an incomplete import statement 'from tests_common.test_utils.system_te' which is cut off. This is not production-ready code.",1,"The DAG successfully demonstrates the complete lifecycle of an AutoML image training job as requested: (1) Dataset initialization via CreateDatasetOperator, (2) Data import via ImportDataOperator, (3) Model training via CreateAutoMLImageTrainingJobOperator, and (4) Resource deletion via DeleteAutoMLTrainingJobOperator and DeleteDatasetOperator. The workflow properly orchestrates cloud ML infrastructure management from start to finish, fulfilling all aspects of the user instruction.",0.6666666666666666
240,Ground Truth (HuggingFace),285,0,"The code uses the `@asset` decorator which is not a standard Airflow 3.0.1 feature. While Airflow 3.x introduced the Asset/Dataset concept, the correct pattern is to use `@task` decorators with `outlets` and `inlets` parameters, or use the `Asset` class for scheduling. The `@asset` decorator as a task definition wrapper does not exist in Airflow's standard API. The correct idiomatic approach would be to use `@task` decorators with Asset definitions for inlet/outlet relationships.",0,"The code imports and uses `@asset` as a decorator from `airflow.sdk`, which is not a valid import in Airflow 3.0.1. The standard import would be `from airflow import Dataset` (or `Asset` in 3.x). The `@asset` decorator syntax used here (e.g., `@asset(uri=..., schedule=None)` as a task decorator) appears to be hallucinated functionality that doesn't exist in the Airflow API. Additionally, the `asset2_producer` function signature with `self` parameter is incorrect for a standard task definition.",0,"The instruction asks for a data processing workflow that 'captures and transforms data between two sequential assets' and demonstrates 'how one data asset can trigger and inform the processing of a subsequent asset.' However, the code does not implement any actual data capture or transformation logic. The `asset1_producer` does nothing (just `pass`), `asset2_producer` only prints context information without transforming data, and `process_nothing` explicitly does nothing. The DAG structure also doesn't clearly demonstrate a sequential pipeline where one asset triggers another - instead it has an OR condition in the schedule. The code fails to demonstrate meaningful data processing or transformation as requested.",0.0
241,Ground Truth (HuggingFace),286,1,"The code uses native Airflow GKE operators (GKECreateClusterOperator, GKEDeleteClusterOperator, GKEStartPodOperator) from the Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test_run = get_test_run(dag) and watcher() calls, which are testing artifacts.",1,"The DAG fulfills all requirements from the instruction: (1) Creates a GKE cluster using GKECreateClusterOperator, (2) Runs test pods with different configurations (pod_task with perl image and pod_task_xcom with alpine image demonstrating xcom functionality), and (3) Cleans up the cluster using GKEDeleteClusterOperator with ALL_DONE trigger rule to ensure cleanup happens regardless of pod task success/failure. The workflow demonstrates dynamic Kubernetes infrastructure management with sample workloads.",0.6666666666666666
242,Ground Truth (HuggingFace),287,1,"The code correctly uses native Alibaba Cloud OSS operators from airflow.providers.alibaba.cloud.operators.oss (OSSUploadObjectOperator, OSSDownloadObjectOperator, OSSDeleteObjectOperator, OSSDeleteBatchObjectOperator) instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for working with Alibaba OSS.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production-ready code.",1,"The DAG fulfills the user instruction by demonstrating cloud storage data movement and file lifecycle management with Alibaba OSS. It includes: (1) transferring data between local and cloud storage via OSSUploadObjectOperator and OSSDownloadObjectOperator, and (2) executing precise object deletion strategies using both single object deletion (OSSDeleteObjectOperator) and batch deletion (OSSDeleteBatchObjectOperator). The workflow captures the essential steps requested.",0.6666666666666666
243,Ground Truth (HuggingFace),288,1,The code uses native Airflow operators designed specifically for Azure Blob Storage interactions: LocalFilesystemToWasbOperator for uploading files and WasbDeleteBlobOperator for deleting blobs. These are the appropriate provider-specific operators from airflow.providers.microsoft.azure rather than wrapping logic in PythonOperator with hooks.,0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', creates a 'test_run = get_test_run(dag)' variable, and adds 'list(dag.tasks) >> watcher()' to the DAG. These are internal testing modules and patterns that indicate this is test/example code rather than clean production-ready code.",1,The DAG fulfills the user instruction completely. It demonstrates basic Azure Blob Storage interaction mechanics with sequential upload (LocalFilesystemToWasbOperator) and deletion (WasbDeleteBlobOperator) of a file in a cloud container. The tasks are properly sequenced with 'upload >> delete' to show the sequential workflow as requested.,0.6666666666666666
244,Ground Truth (HuggingFace),289,1,"The code uses native Airflow providers and operators appropriately. It leverages DataprocCreateClusterOperator, DataprocDeleteClusterOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator, and GCSSynchronizeBucketsOperator from the official Google Cloud provider package. The ClusterGenerator utility is used idiomatically to configure the cluster. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' and calls watcher() which is a test utility. This is not production-ready code.",1,"The DAG fulfills the user instruction comprehensively. It provisions a multi-node Dataproc cluster (2 workers + 1 master) with preemptible workers (num_preemptible_workers=1, preemptibility='PREEMPTIBLE'), includes custom initialization scripts (init_actions_uris=[GCS_INIT_FILE] pointing to pip-install.sh), supports metadata for package installations (metadata={'PIP_PACKAGES': 'pyyaml requests pandas openpyxl'}), and manages computational resources automatically through cluster creation and deletion. The orchestration is end-to-end with proper setup, execution, and teardown phases.",0.6666666666666666
245,Ground Truth (HuggingFace),291,1,"The code uses native Airflow operators from the Google Cloud provider package (AutoMLCreateDatasetOperator, AutoMLImportDataOperator, AutoMLTrainModelOperator, AutoMLDeployModelOperator, GCSCreateBucketOperator, GCSSynchronizeBucketsOperator, etc.) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic Airflow approach for working with Google Cloud AutoML services.",0,"The code contains clear indicators of test harness boilerplate and internal testing patterns. Specifically: (1) Uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are internal testing conventions, (2) References 'airflow-system-tests-resources' as a resource bucket, (3) Includes comments like '# TEST SETUP' and '# TEST BODY', (4) The DAG_ID 'example_automl_vision_obj_detect' follows the 'example_' naming pattern typical of test/example code, and (5) Uses tags=['example', 'automl', 'object-detection'] indicating this is example/test code rather than production-ready code.",0,"The DAG is incomplete and does not fulfill the instruction requirements. The code abruptly ends at 'import_dataset' without establishing the complete pipeline flow. Key missing elements include: (1) No task dependencies defined after import_dataset (the chain is incomplete), (2) The create_model, deploy_model, delete_model, delete_dataset, and delete_bucket tasks are defined but never connected in the task dependency graph, (3) The instruction asks for a 'deployable machine learning model' but the deployment task is not properly sequenced, (4) Infrastructure management (cleanup tasks) are defined but not integrated into the workflow. The DAG would fail to execute as intended because the task dependencies are not fully specified.",0.3333333333333333
246,Ground Truth (HuggingFace),292,0,"The code uses task decorators with Hooks instead of proper Operators. Hooks (PinotAdminHook, PinotDbApiHook) are meant to be used within Operators, not instantiated directly in task functions. The tasks instantiate hooks but don't actually execute any operations or return results. This is not idiomatic Airflow - proper implementation would use PinotOperator or similar operator classes if available, or at minimum, the hooks should be used to perform actual operations within the task functions.",0,The code contains clear test harness boilerplate that should not be in production code: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is example/test code rather than production-ready code.,0,"While the DAG attempts to create two tasks using different Pinot connection methods (admin and dbapi) and they are set up to run in parallel by default, the tasks don't actually execute any meaningful Pinot interactions. The hooks are merely instantiated but no operations are performed (no queries executed, no admin commands run, no results returned). The instruction asks for 'distinct Pinot interaction tasks' but the code only instantiates hooks without using them to interact with Pinot. Additionally, the PinotDbApiHook is incorrectly configured with parameters like 'task_id' and 'pinot' that don't match typical hook initialization patterns.",0.0
247,Ground Truth (HuggingFace),293,1,"The code uses the native WeaviateIngestOperator from airflow.providers.weaviate.operators.weaviate for data ingestion, which is the idiomatic Airflow approach. While some tasks use @task decorators with WeaviateHook (create_collection, query, delete), these are appropriate uses since they involve setup/teardown operations and custom logic (assertions) that don't have dedicated operators. The core data ingestion operation correctly uses the specialized operator rather than wrapping a hook in a PythonOperator.",0,The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready code. This is a hallmark of code leaked from Airflow's internal test suite.,1,"The DAG fulfills the user instruction to create an end-to-end pipeline for transforming unstructured trivia data into a semantically searchable vector database. It demonstrates: (1) Loading data from JSON file (get_data_to_ingest), (2) Vectorizing using AI-powered embeddings (text2vec_openai vectorizer), (3) Ingesting into Weaviate vector database (WeaviateIngestOperator), (4) Querying the vector database with semantic search (query_weaviate), and (5) Managing the collection lifecycle (create and delete operations). All key components requested are present.",0.6666666666666666
248,Ground Truth (HuggingFace),294,1,"The code uses native Airflow Dataplex operators from airflow.providers.google.cloud.operators.dataplex, such as DataplexCatalogCreateEntryGroupOperator, DataplexCatalogCreateEntryTypeOperator, and DataplexCatalogCreateAspectTypeOperator. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', which is an internal testing module. This is test harness boilerplate that should not appear in production code. Additionally, the code references ENV_ID and SYSTEM_TESTS_ENV_ID environment variables, which are testing artifacts.",0,"The DAG code is incomplete - it cuts off mid-definition with 'create_aspect_type' not being fully defined. More critically, while it sets up the infrastructure (entry groups, entry types, aspect types), it does not demonstrate a complete 'automated metadata registration pipeline' as requested. The instruction asks for capturing metadata attributes, defining entry types, and creating catalog entries to support data discovery and governance. The code only shows partial setup without demonstrating the actual metadata capture workflow, task dependencies, or a complete end-to-end pipeline that would systematically track and organize data assets.",0.3333333333333333
249,Ground Truth (HuggingFace),295,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator, S3ToSFTPOperator, and S3DeleteBucketOperator from the AWS provider package. The S3ToSFTPOperator is the idiomatic way to transfer files from S3 to SFTP, rather than wrapping hooks in PythonOperators. Task chaining is done using the chain() utility, which is also idiomatic Airflow.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. It also includes test setup/teardown logic (sys_test_context_task, watcher) and test_run assignment at the bottom. This is clearly test infrastructure code, not production-ready DAG code.",0,"While the code demonstrates S3 to SFTP transfer with task chaining, it fails to meet several key requirements from the instruction: (1) No actual file upload to S3 is shown - the s3_key is just a variable with no data being moved, (2) No 'complete lifecycle tracking' is implemented - there's no logging, monitoring, or status tracking of the transfer process, (3) The instruction asks for 'comprehensive task chaining' demonstrating the full migration pipeline, but this only shows bucket creation, transfer, and deletion without validation, error handling, or tracking steps that would constitute a complete lifecycle.",0.3333333333333333
250,Ground Truth (HuggingFace),296,1,"The code correctly uses the TrinoOperator from airflow.providers.trino.operators.trino, which is the idiomatic and native operator for executing Trino queries. It does not wrap Trino logic in PythonOperator with hooks, but instead uses the purpose-built operator throughout the DAG.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the generated DAG code.,1,"The DAG fully satisfies the user instruction. It creates a schema (trino_create_schema), creates multiple tables (city, city1, city2), inserts data into those tables, and demonstrates different query methods including templated queries (using Jinja params) and parameterized queries (using the parameters argument with '?' placeholder). All requested elements are present and correctly implemented.",0.6666666666666666
251,Ground Truth (HuggingFace),297,1,"The code uses the appropriate JenkinsJobTriggerOperator from airflow.providers.jenkins.operators.jenkins_job_trigger for triggering Jenkins jobs, which is the idiomatic Airflow approach. While the artifact retrieval uses a @task decorator with JenkinsHook, this is acceptable because there is no dedicated operator for artifact download in the Jenkins provider, and the implementation properly uses the JenkinsHook to interact with Jenkins rather than making raw API calls.",0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. Additionally, the DAG_ID is set to 'test_jenkins' which suggests this is test code rather than production code.",1,"The code fulfills the user instruction completely. It demonstrates: (1) triggering a Jenkins job for configuration generation using JenkinsJobTriggerOperator with parameters, (2) downloading the generated artifact using the grab_artifact_from_jenkins task, (3) inter-system communication between Airflow and Jenkins via the Jenkins provider, and (4) automated artifact management by retrieving and storing the artifact content in XCom. The workflow properly chains these operations with the artifact retrieval depending on the job trigger output.",0.6666666666666666
252,Ground Truth (HuggingFace),298,1,"The code uses native Airflow Dataprep operators (DataprepCopyFlowOperator, DataprepDeleteFlowOperator, DataprepGetJobGroupOperator, DataprepRunFlowOperator, DataprepRunJobGroupOperator) and sensors (DataprepJobGroupIsFinishedSensor) from the official Google Cloud provider. While it does use @task decorators with GoogleDataprepHook for some operations (create_imported_dataset, create_flow), this is appropriate as these are setup/configuration tasks that may not have dedicated operators. The core workflow orchestration uses proper Dataprep operators, demonstrating idiomatic Airflow patterns.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is an internal testing utility. The code also includes environment variable patterns typical of system tests (SYSTEM_TESTS_ENV_ID, SYSTEM_TESTS_DATAPREP_TOKEN, SYSTEM_TESTS_GCP_PROJECT), and the DAG description explicitly states 'Example Airflow DAG' with references to system tests resources ('gs://airflow-system-tests-resources/dataprep/dataset-00000.parquet'). This is not production-ready code but rather a test/example template.",1,"The code fulfills the user instruction comprehensively. It orchestrates a complex data transformation workflow using Google Dataprep with: (1) Task dependencies shown through chain() and task sequencing, (2) Connection management via programmatic connection creation, (3) Cloud resource provisioning through GCS bucket creation, (4) Systematic approach to creating flows and datasets, (5) Running jobs via DataprepRunFlowOperator/DataprepRunJobGroupOperator, and (6) Monitoring through DataprepJobGroupIsFinishedSensor and get job status operators. Despite being incomplete (code is cut off), the visible portions demonstrate all requested elements of the complex workflow orchestration.",0.6666666666666666
253,Ground Truth (HuggingFace),299,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Stackdriver/Monitoring tasks. It imports and uses StackdriverUpsertAlertOperator, StackdriverDeleteAlertOperator, StackdriverEnableAlertPoliciesOperator, StackdriverDisableAlertPoliciesOperator, StackdriverListAlertPoliciesOperator, and corresponding notification channel operators from airflow.providers.google.cloud.operators.stackdriver. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear evidence of test harness boilerplate and internal testing module imports. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is an internal testing module. It also uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, and includes test-specific naming conventions (TEST_ALERT_POLICY_1, TEST_NOTIFICATION_CHANNEL_1, etc.). The DAG_ID and resource names are constructed with ENV_ID for test isolation. This is not production-ready code but rather example/test code.",1,"The code fulfills the user instruction to design a monitoring and alerting workflow for Google Cloud infrastructure. It demonstrates: (1) Creating alert policies and notification channels via Upsert operators, (2) Managing them through Delete operators, (3) Configuring via enable/disable operators (StackdriverEnableAlertPoliciesOperator, StackdriverDisableAlertPoliciesOperator, StackdriverEnableNotificationChannelsOperator, StackdriverDisableNotificationChannelsOperator), and (4) Tracking infrastructure health through List operators (StackdriverListAlertPoliciesOperator, StackdriverListNotificationChannelsOperator). The alert policies are configured to monitor GCE instance metrics, which aligns with infrastructure health monitoring. Although the code is truncated, the imported operators and defined configurations demonstrate all requested capabilities.",0.6666666666666666
254,Ground Truth (HuggingFace),300,1,"The code uses native Databricks providers and operators (DatabricksSqlOperator and DatabricksCopyIntoOperator) which are the idiomatic way to interact with Databricks SQL environments in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operators for each task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' which is test infrastructure code. This is not production-ready code and should not be present in a clean DAG example.",1,"The DAG fulfills all requirements of the instruction: (1) table creation via DatabricksSqlOperator with CREATE TABLE, (2) data insertion via INSERT INTO statements, (3) transformation via SELECT queries, (4) file-based export using output_path and output_format parameters in DatabricksSqlOperator, and (5) cross-storage data import using DatabricksCopyIntoOperator to load data from external storage into a Delta table. All requested components of an end-to-end Databricks SQL pipeline are present.",0.6666666666666666
255,Ground Truth (HuggingFace),301,1,"The code uses native Airflow operators appropriately: BashOperator for bash commands and SingularityOperator from airflow.providers.singularity for containerized execution. It does not wrap these in PythonOperator with hooks, which would be non-idiomatic. The operators are used directly as intended by the Airflow framework.",0,"The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fulfills the instruction requirements: it includes date printing (t1: print_date), sleep operations (t2: sleep 5 seconds), containerized command execution (t3: SingularityOperator with busybox container sleeping 30 seconds), and demonstrates clear task interdependencies (t1 >> [t2, t3] and t3 >> t4). All requested components for an observability pipeline showcasing system interaction tasks are present.",0.6666666666666666
256,Ground Truth (HuggingFace),302,1,"The code uses native Airflow providers and operators appropriately. It uses SqsPublishOperator for publishing messages, SqsSensor for reading messages (both single and batch modes), and SqsHook for queue management operations. This is idiomatic Airflow - leveraging specific operators rather than wrapping everything in PythonOperators.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test/example code rather than clean production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) publishes messages to SQS using SqsPublishOperator (publish_to_queue_1 and publish_to_queue_2), (2) reads messages in single mode using SqsSensor (read_from_queue), (3) reads messages in batch mode using SqsSensor with max_messages and num_batches parameters (read_from_queue_in_batch), and (4) manages queue lifecycle by creating and deleting the queue. The workflow demonstrates the complete SQS interaction pattern as requested.",0.6666666666666666
257,Ground Truth (HuggingFace),303,1,"The code uses native Airflow constructs appropriately. It leverages Dataset objects, DatasetOrTimeSchedule, CronTriggerTimetable, and BashOperator - all standard Airflow components. The DAGs use dataset-based scheduling through the schedule parameter with Dataset objects, which is the idiomatic way to implement dataset dependencies in Airflow 2.x. No generic PythonOperator wrappers are used where native operators would be more appropriate.",1,"The code imports only standard Airflow libraries (airflow.datasets, airflow.models.dag, airflow.operators.bash, airflow.timetables) and the pendulum library for datetime handling. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or any test harness boilerplate. The code is clean and production-ready.",0,"The code is incomplete and cuts off mid-definition (the last DAG 'dataset_consumes_1_and_2' has an incomplete BashOperator). While the provided portion demonstrates dataset dependencies and multiple scheduling scenarios as requested, the instruction asks for 'a series of DAGs that demonstrate dataset-based scheduling with various dependency scenarios' including 'time schedules, and complex logical conditions.' The docstring references several DAGs that should exist (consume_1_or_2_with_dataset_expressions, consume_1_and_2_with_dataset_expressions, consume_1_or_both_2_and_3_with_dataset_expressions, conditional_dataset_and_time_based_timetable, dataset_consumes_1_never_scheduled, dataset_consumes_unknown_never_scheduled) but these are not present in the truncated code. The incomplete nature means it cannot fulfill the full instruction requirements.",0.6666666666666666
258,Ground Truth (HuggingFace),304,1,"The code uses the native MongoToS3Operator from airflow.providers.amazon.aws.transfers.mongo_to_s3, which is the idiomatic Airflow approach for transferring data from MongoDB to S3. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built transfer operator designed for this exact task.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules. Additionally, the code includes test setup/teardown logic (create_s3_bucket, delete_s3_bucket) and test context management (sys_test_context_task) that are part of a system test framework rather than production pipeline code.",1,"The code fulfills the user instruction completely. It filters documents from MongoDB using a predefined query criteria (mongo_query={'status': 'OK'}), exports the filtered data to S3 storage using MongoToS3Operator, and constructs a workflow that selectively moves data based on a status condition. The core business logic of filtering by status and transferring to S3 is correctly implemented.",0.6666666666666666
259,Ground Truth (HuggingFace),305,1,"The code uses native Airflow operators from the Google Cloud provider package (CloudBuildCreateBuildTriggerOperator, CloudBuildDeleteBuildTriggerOperator, CloudBuildGetBuildTriggerOperator, CloudBuildListBuildTriggersOperator, CloudBuildRunBuildTriggerOperator, CloudBuildUpdateBuildTriggerOperator). While it does use a @task decorator for get_project_number(), this is appropriate as it's a helper function for retrieving project metadata using the Google API client directly, which doesn't have a dedicated operator. The use of XComArg for passing data between tasks is also idiomatic Airflow 2.0+ pattern.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. This is test harness boilerplate that should not be present in production code. The comments also reference 'system test' multiple times (e.g., 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_GCP_PROJECT', 'If you'd like to run this system test locally'), indicating this is test infrastructure code rather than clean production-ready code.",0,"The DAG is incomplete - it cuts off abruptly after defining build_trigger_id without implementing the full workflow. The instruction asks for an 'end-to-end data engineering workflow' that demonstrates 'dynamic trigger creation, project number retrieval, and cloud resource orchestration'. While the code shows project number retrieval (get_project_number task) and begins trigger creation (create_build_trigger), it doesn't complete the workflow. The code appears truncated and doesn't show the full orchestration of cloud resources, trigger execution, or any data engineering pipeline steps. An end-to-end workflow would need to include all the imported operators being used in a complete DAG structure.",0.3333333333333333
260,Ground Truth (HuggingFace),306,1,"The code uses the @task.sensor decorator, which is the idiomatic Airflow 2.x way to create sensors using the TaskFlow API. This is a proper, native Airflow pattern for sensor implementation rather than wrapping sensor logic in a generic PythonOperator. The use of PokeReturnValue and sensor-specific parameters (poke_interval, timeout, mode) demonstrates proper usage of Airflow's sensor framework.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.decorators, airflow.sensors.base) and the pendulum library which is a standard Airflow dependency. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or any test harness boilerplate. The comments with START/END tags are documentation markers, not test code.",0,"While the code demonstrates a sensor mechanism and task dependency, it fails to fulfill the instruction's requirement to 'simulate checking for data availability.' The wait_for_upstream sensor immediately returns PokeReturnValue(is_done=True), meaning it never actually waits or checks anything - it always succeeds on the first poke. A proper demonstration should include some logic that simulates checking for data readiness (e.g., checking if a file exists, querying a database, or at least using a condition that could be false). Additionally, the downstream task (dummy_operator) doesn't execute any 'subsequent operation' - it's completely empty. The instruction asked for a workflow that demonstrates waiting AND executing a subsequent operation, but this implementation is too trivial to be a meaningful demonstration.",0.6666666666666666
261,Ground Truth (HuggingFace),307,1,"The code uses appropriate Airflow operators for the tasks: BashOperator for shell commands to interact with HDFS, and HiveOperator for loading data into Hive tables. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks. The TaskFlow API (@task decorator) is also used appropriately for Python functions.",1,"The code uses standard Airflow libraries and providers (airflow.providers.standard.operators.bash, HiveOperator). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate like 'get_test_run(dag)'. The code is production-oriented (though incomplete/truncated).",0,"The instruction asks for a DAG demonstrating multi-stage data orchestration with task dependencies using BashOperator and HiveOperator, showing data transfer between local storage, HDFS, Hive tables, and MySQL database. While the code shows BashOperator (for HDFS operations) and HiveOperator (for Hive loading), it is missing critical components: 1) The code is incomplete/truncated (the second for loop is cut off), 2) No MySQL operator or interaction is actually implemented - the 'transfer_to_db()' task is just an empty placeholder with no actual MySQL logic, 3) Missing necessary imports (os, datetime, date, timedelta, DAG, task decorator, HiveOperator). The DAG structure is present but the implementation is incomplete and doesn't fully demonstrate the MySQL integration as requested.",0.6666666666666666
262,Ground Truth (HuggingFace),308,1,"The code uses Airflow 3.0.0's native setup/teardown decorators (@setup, @teardown) and the as_teardown() method, which are idiomatic patterns for resource management in modern Airflow. It leverages TaskFlow API with @task decorator and task_group, which are the recommended approaches. No generic PythonOperator wrapping is used where native features exist.",1,"The code imports only standard Airflow 3.0.0 libraries (airflow.sdk modules including DAG, setup, task, task_group, teardown) and pendulum for datetime handling. There are no imports from test utilities, no test harness boilerplate like get_test_run(), and no internal testing modules. The code is clean and production-ready.",1,"The DAG fully satisfies the instruction requirements: (1) Simulates a data processing scenario with cluster initialization (outer_setup/inner_setup returning cluster IDs), (2) Shows data transformation (outer_work, inner_work tasks), (3) Demonstrates cluster teardown (outer_teardown/inner_teardown consuming cluster IDs), (4) Exhibits explicit resource management through setup/teardown decorators and context managers, (5) Demonstrates best practices with coordinated workflow showing proper dependency management and resource lifecycle handling across different task stages. The nested structure with outer and inner setup/teardown patterns showcases sophisticated resource management.",1.0
263,Ground Truth (HuggingFace),309,1,"The code uses the native KylinCubeOperator from Airflow providers for all Kylin operations (build, refresh, merge, disable, purge). It does not wrap these operations in PythonOperator with hooks, which would be non-idiomatic. The use of @dag.task for generating build times is appropriate for simple data generation. The code follows Airflow best practices by using specific operators designed for Kylin cube operations.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that indicate this is example/test code rather than clean production-ready code.",1,"The DAG successfully demonstrates sequential execution of Kylin cube operations with clear task dependencies (build >> build >> refresh >> merge >> disable >> purge >> build). It showcases time-based data segmentation through the use of start_time and end_time parameters with timestamp values, and demonstrates the full progression of data transformation steps from initial build operations through refresh, merge, disable, purge, and final rebuild. The workflow clearly highlights the lifecycle of Kylin cube operations as requested.",0.6666666666666666
264,Ground Truth (HuggingFace),310,1,"The DAG uses native Airflow providers and operators designed specifically for the tasks: GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket operations, CloudTextToSpeechSynthesizeOperator for audio generation, and CloudSpeechToTextRecognizeSpeechOperator for speech recognition. It does not wrap these in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are not production-ready elements and represent test infrastructure leakage.",1,"The DAG fully implements the requested workflow: it creates a GCS bucket, generates audio using text-to-speech (CloudTextToSpeechSynthesizeOperator), performs speech recognition on the generated audio (CloudSpeechToTextRecognizeSpeechOperator), and cleans up by deleting the bucket. The operators are properly chained in sequence, demonstrating a comprehensive speech processing workflow using Google Cloud Speech API interactions as requested.",0.6666666666666666
265,Ground Truth (HuggingFace),311,1,"The code uses native Airflow operators from the Google Cloud provider package (CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, CloudDataCatalogCreateTagOperator, etc.) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for interacting with Google Cloud Data Catalog.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a testing module import. Additionally, the code uses test-related environment variables (SYSTEM_TESTS_ENV_ID, SYSTEM_TESTS_GCP_PROJECT) and naming conventions (DAG_ID includes 'SYSTEM_TESTS'), indicating this is test harness boilerplate rather than production-ready code.",0,"The code is incomplete (truncated at the end with '[START howto_operator_gcp_datacatalog') and does not demonstrate the complete lifecycle as requested. While it shows creating entry groups, entries, and tags, it's missing the tag template creation (referenced but not shown), and the cleanup/deletion operations are imported but not implemented in the visible DAG structure. The instruction asked for a 'complete lifecycle' and 'systematic workflow', but the provided code is fragmented and incomplete.",0.3333333333333333
266,Ground Truth (HuggingFace),313,1,"The DAG uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, GoogleCalendarToGCSOperator for the calendar data transfer, and GCSDeleteBucketOperator for cleanup. The connection management uses TaskFlow API @task decorators which is idiomatic for Airflow 3.0.0. No generic PythonOperator wrapping is used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities not appropriate for production code and indicate this is a system test example rather than clean production-ready code.",1,"The DAG fully implements the requested cloud data transfer pipeline with explicit setup, execution, and teardown stages. Setup: creates GCS bucket and connection. Execution: transfers Google Calendar data to GCS using GoogleCalendarToGCSOperator. Teardown: systematically removes the connection and bucket. The workflow demonstrates complete resource lifecycle management with proper trigger rules (ALL_DONE) to ensure cleanup runs regardless of task success/failure.",0.6666666666666666
267,Ground Truth (HuggingFace),314,1,"The code uses the appropriate Airflow provider operator 'FileTransferOperator' from 'airflow.providers.common.io.operators.file_transfer' for transferring files, which is the idiomatic way to handle file transfers in Airflow. It also uses TaskFlow API decorators (@task) appropriately for custom logic around file creation and cleanup. The code does not wrap provider functionality in PythonOperator unnecessarily.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production-ready code.",1,"The DAG fully addresses the user instruction. It demonstrates: (1) programmatic creation of a temporary file with sample content, (2) transfer of that file from local temporary storage to an AWS S3 bucket using FileTransferOperator, and (3) cleanup of both the temporary file and the S3 bucket. The workflow showcases the complete lifecycle of file creation, movement, and cleanup in a cloud environment as requested.",0.6666666666666666
268,Ground Truth (HuggingFace),315,1,"The code uses native Airflow BigQuery operators appropriately: BigQueryCreateEmptyDatasetOperator, BigQueryCreateEmptyTableOperator, BigQueryToBigQueryOperator, and BigQueryDeleteDatasetOperator. These are the idiomatic, provider-specific operators for BigQuery operations rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. The docstring also explicitly states this is an 'Airflow System Test DAG'. This is not production-ready code.",0,"While the DAG demonstrates table creation, data transfer, and cleanup as requested, it has a critical flaw: the origin table is created but never populated with any employee salary data. The BigQueryToBigQueryOperator will copy from an empty table, which doesn't fulfill the instruction to 'copy employee salary data'. A production HR data migration workflow would need to either insert sample data or assume pre-existing data. The instruction asks for a data migration workflow, but this only demonstrates the structure without actual data to migrate.",0.3333333333333333
269,Ground Truth (HuggingFace),316,1,"The code uses native Airflow providers and operators designed specifically for AWS Bedrock tasks. It properly uses BedrockCustomizeModelOperator, BedrockCreateProvisionedModelThroughputOperator, BedrockInvokeModelOperator, and corresponding sensors (BedrockCustomizeModelCompletedSensor, BedrockProvisionModelThroughputCompletedSensor). It also uses BedrockHook appropriately for cleanup operations. The code follows idiomatic Airflow patterns with task groups, decorators, and proper operator usage rather than wrapping everything in PythonOperators.",0,"The code imports 'from system.amazon.aws.utils import SystemTestContextBuilder' which appears to be an internal testing utility, not a standard Airflow library. This is a clear hallucination/test harness leakage. The line 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' and references to 'test_context' variable further confirm this is test boilerplate that should not be in production code. Additionally, the code includes test-specific environment variables like SKIP_LONG_TASKS and SKIP_PROVISION_THROUGHPUT with comments about 'System Test stack', indicating this is test infrastructure code.",0,"While the code demonstrates model customization and provisioned throughput creation (partial lifecycle management), it is incomplete and does not fulfill the full instruction. The code is truncated (the delete_provision_throughput function is cut off), and critically, it does not demonstrate 'model training, throughput allocation, and model lifecycle management' as a complete end-to-end pipeline. The instruction asks for 'automated monitoring and cleanup' but the monitoring aspect is minimal (only completion sensors), and the cleanup tasks are present but the DAG structure showing how they all connect is missing. Most importantly, there's no actual model invocation/deployment demonstration shown in a complete workflow, and the DAG object instantiation and final task dependencies are not visible due to truncation.",0.3333333333333333
270,Ground Truth (HuggingFace),317,1,"The code uses Airflow 3.0.0's native setup/teardown decorators (@setup, @teardown) and the as_teardown() method, which are the idiomatic way to handle resource management in modern Airflow. It leverages TaskFlow API with @task decorator and uses task_group for logical grouping. The code demonstrates proper use of Airflow-specific patterns rather than wrapping everything in generic PythonOperators.",1,"The code imports only standard Airflow 3.0.0 libraries (airflow.sdk.DAG, setup, task, task_group, teardown) and pendulum for datetime handling. There are no imports from test utilities, no test harness boilerplate like get_test_run(), and no internal testing modules. The code is clean and production-ready.",1,"The DAG fully satisfies the user instruction to demonstrate setup and teardown tasks for resource allocation and cleanup in a multi-stage data processing pipeline. It shows: (1) preparatory tasks (outer_setup, inner_setup), (2) main work tasks (my_first_task, my_second_task, outer_work, inner_work), and (3) cleanup tasks (my_third_task as teardown, outer_teardown, inner_teardown). The code demonstrates multiple patterns including as_teardown() method and decorator-based approaches, simulating a complete operational workflow with proper resource management.",1.0
271,Ground Truth (HuggingFace),319,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocStartClusterOperator, DataprocStopClusterOperator, DataprocDeleteClusterOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also has 'list(dag.tasks) >> watcher()' which is test infrastructure code. These are not production-ready elements and should not be in a production DAG.",0,"While the DAG demonstrates cluster provisioning (create/start/stop/delete), it does not actually perform any big data processing workloads or demonstrate dynamic allocation based on processing requirements. The instruction asks for a pipeline that shows 'dynamic cluster provisioning and management for big data processing workloads' and 'dynamically allocate and deallocate computational resources based on processing requirements.' This DAG only creates, stops, starts, and deletes a cluster without any actual data processing tasks (e.g., Spark jobs, Hive queries) that would justify the cluster operations. It's missing the core business logic of processing data.",0.3333333333333333
272,Ground Truth (HuggingFace),320,1,"The code uses native Airflow operators throughout: GCSToSambaOperator for transfers, GCSCreateBucketOperator/GCSDeleteBucketOperator for bucket management, and LocalFilesystemToGCSOperator for uploads. It does not wrap functionality in PythonOperator + Hook patterns, demonstrating proper use of provider-specific operators.",0,"The code includes test harness boilerplate at the end with 'from tests.system.utils.watcher import watcher' and an incomplete comment 'This test needs watcher in orde'. This is internal testing infrastructure that should not appear in production DAG code, indicating leakage from a test environment.",1,"The DAG successfully demonstrates all requested transfer scenarios: single file copy (copy_file_from_gcs_to_samba), file move (move_file_from_gcs_to_samba with move_object=True), directory copy (copy_dir_from_gcs_to_samba using wildcard pattern), and selective file movement (move_dir_from_gcs_to_samba with keep_directory_structure=False). The workflow showcases flexible file migration strategies between GCS and Samba as requested.",0.6666666666666666
273,Ground Truth (HuggingFace),321,1,"The DAG uses native Airflow operators designed for their specific tasks: CloudDataTransferServiceGCSToGCSOperator for cloud transfer operations, GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket management, and LocalFilesystemToGCSOperator for file uploads. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not appear in production code. Additionally, comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' indicate this is test code rather than production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) demonstrates scalable cloud data movement using Google Cloud Transfer Service with CloudDataTransferServiceGCSToGCSOperator, (2) shows file transfer between cloud storage environments (GCS bucket to GCS bucket), (3) includes robust error management through trigger_rule=TriggerRule.ALL_DONE on cleanup tasks, (4) implements cleanup procedures with GCSDeleteBucketOperator tasks, and (5) demonstrates programmatic cloud storage resource management through bucket creation, file upload, transfer, and deletion operations. The workflow also shows both synchronous and deferrable transfer patterns.",0.6666666666666666
274,Ground Truth (HuggingFace),322,1,"The code uses native Airflow GCS providers and operators appropriately. It leverages GCSUploadSessionCompleteSensor, GCSObjectUpdateSensor, GCSObjectExistenceSensor, and GCSToGCSOperator - all specific operators designed for GCS tasks. No generic PythonOperator wrappers around hooks are used.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', references to 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_GCP_PROJECT', 'airflow-system-tests-resources' bucket, and the 'workaround_in_debug_executor' function which is explicitly for test debugging. The comments like '[START howto_sensor_gcs_upload_session_complete_task]' are documentation markers typical of example/test code, not production code.",0,"The code is incomplete (cuts off mid-line at 'gcs_objec') and lacks the critical component requested: triggering subsequent data processing steps. While it sets up multiple sensors to monitor file uploads and validate conditions (upload completion, object existence, object updates), there are no downstream processing tasks defined. The instruction specifically asks to 'trigger subsequent data processing steps based on file arrival and update conditions', but no such processing tasks exist in the DAG. Additionally, the code appears to be example/test code rather than a production workflow as requested.",0.3333333333333333
275,Ground Truth (HuggingFace),323,1,"The code uses native Airflow operators throughout: GCSCreateBucketOperator, GCSDeleteBucketOperator, LocalFilesystemToGCSOperator, and LifeSciencesRunPipelineOperator from the appropriate provider packages. It does not wrap functionality in PythonOperators with hooks, which would be non-idiomatic. The DAG structure follows Airflow best practices with proper task chaining.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production DAG code. Additionally, comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' indicate this is test code rather than production-ready code.",1,"The DAG fulfills all requirements from the user instruction: (1) uploads a file to a GCS bucket using LocalFilesystemToGCSOperator, (2) runs a simple bash command via the simple_life_science_action_pipeline task that executes 'echo Hello, world', and (3) performs a multi-step pipeline action via the multiple_life_science_action_pipeline task that includes copying files from GCS, running bash commands, and copying files back to GCS. The workflow demonstrates basic orchestration of Google Cloud Life Sciences pipelines as requested.",0.6666666666666666
276,Ground Truth (HuggingFace),324,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Catalog operations. It imports and uses CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, CloudDataCatalogCreateTagOperator, CloudDataCatalogCreateTagTemplateOperator, and other specialized operators from airflow.providers.google.cloud.operators.datacatalog. It does not wrap functionality in PythonOperator with hooks, which would be non-idiomatic. The use of GCSCreateBucketOperator for GCS operations and proper XComArg for task dependencies also demonstrates idiomatic Airflow patterns.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a testing module import. Additionally, the code uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and the DAG structure with naming conventions like 'bucket_{DAG_ID}_{ENV_ID}' strongly suggests this is test harness boilerplate rather than production-ready code. The presence of BashOperator tasks that simply echo XCom values (create_entry_group_result, create_entry_gcs_result, create_tag_result) are typical of system test patterns rather than production pipelines.",1,"The code fulfills the user instruction to construct a data asset registration and tagging pipeline for automated metadata management. It demonstrates: (1) Dynamic catalog entry creation via CloudDataCatalogCreateEntryGroupOperator and CloudDataCatalogCreateEntryOperator, (2) Custom tag assignment through CloudDataCatalogCreateTagOperator with custom fields, (3) Preparation of data assets for discovery by creating GCS bucket and linking it to catalog entries with gcs_fileset_spec. The pipeline shows the complete workflow from creating storage resources, registering them in Data Catalog, and applying metadata tags for governance, which aligns with the requested functionality.",0.6666666666666666
277,Ground Truth (HuggingFace),325,1,"The code uses the native TimeDeltaSensorAsync operator from airflow.providers.standard.sensors.time_delta, which is the idiomatic way to implement asynchronous time-based delays in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built sensor designed for this task.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.providers.standard.operators.empty, airflow.providers.standard.sensors.time_delta, airflow.sdk). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or any test harness boilerplate code.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) a controlled waiting period using TimeDeltaSensorAsync with a 30-second delay, (2) asynchronous behavior that doesn't occupy worker slots during the wait (as stated in the docstring and implementation), (3) time-based synchronization in a data processing workflow context, and (4) a complete pipeline showing the delay before task completion (wait >> finish). The implementation directly addresses all requirements of simulating a controlled waiting period with async sensors.",1.0
278,Ground Truth (HuggingFace),327,1,"The code uses native Airflow operators from the Google Cloud provider package (CloudDatastoreAllocateIdsOperator, CloudDatastoreBeginTransactionOperator, CloudDatastoreRunQueryOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic approach for interacting with Google Cloud Datastore in Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities not appropriate for production code and indicate this is a system test rather than a clean production DAG.",1,"The DAG successfully demonstrates sequential steps of interacting with a cloud-based NoSQL database (Google Cloud Datastore). It showcases transaction management (CloudDatastoreBeginTransactionOperator), query preparation (defining QUERY with transaction context), and database operations (CloudDatastoreAllocateIdsOperator, CloudDatastoreRunQueryOperator). The workflow illustrates the technical mechanics of database interaction in a cloud environment as requested.",0.6666666666666666
279,Ground Truth (HuggingFace),328,1,"The code uses the idiomatic `@task.snowpark` decorator, which is the proper Airflow provider-specific decorator for Snowflake Snowpark integration. It does not wrap Snowpark logic in a generic PythonOperator with hooks, but instead uses the native decorator designed for this purpose. The session is properly injected as an argument, demonstrating correct usage of Airflow's Snowpark integration.",0,"The code includes test harness boilerplate at the end: `from tests_common.test_utils.system_tests import get_test_run` and `test_run = get_test_run(dag)`. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fully adheres to the instruction. It showcases integration between Airflow's task decorators (@task.snowpark) and Snowflake's Snowpark session management, programmatically creates a DataFrame with sample data, writes it to a Snowflake table using `df.write.save_as_table()`, and implements a validation task (`check_num_rows`) that checks data integrity by asserting the row count equals the expected value of 12. All required components are present and functional.",0.6666666666666666
280,Ground Truth (HuggingFace),329,1,"The code uses the specific GCSToPrestoOperator from airflow.providers.presto.transfers.gcs_to_presto, which is the idiomatic Airflow approach for transferring data from GCS to Presto. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG code.,1,"The DAG fulfills the user instruction to build a single-task data transfer mechanism between GCS (cloud storage) and Presto (distributed query engine). It demonstrates a scheduled job (with schedule='@once' and a note to override as needed) that moves data from GCS to Presto with minimal configuration. The implementation is straightforward and reliable, using environment variables for configuration flexibility.",0.6666666666666666
281,Ground Truth (HuggingFace),330,1,"The code uses native Airflow operators appropriately: BashOperator for bash commands and DockerOperator from airflow.providers.docker for Docker integration. It does not wrap these in PythonOperator with hooks, which would be non-idiomatic. The operators are used directly as intended by the Airflow framework.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and creates 'test_run = get_test_run(dag)'. These are internal testing utilities that represent hallucination/leakage from test examples into what should be production-ready code.",1,"The DAG successfully demonstrates basic task orchestration and Docker integration as requested. It shows: (1) different types of tasks (BashOperator and DockerOperator), (2) task sequencing with dependencies (t1 >> [t2, t3] >> t4), and (3) Docker integration via DockerOperator with proper configuration. The workflow clearly showcases how different task types can be orchestrated in Airflow.",0.6666666666666666
282,Ground Truth (HuggingFace),334,1,"The code uses native Airflow providers and operators specifically designed for Azure Data Lake Storage tasks. It imports and uses 'LocalFilesystemToADLSOperator' for uploading files and 'ADLSDeleteOperator' for deleting files, rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach.",0,"The code contains test harness boilerplate and imports internal testing modules that should not be present in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are testing artifacts that make the code non-production-ready.",1,"The DAG fulfills the user instruction requirements: it showcases task sequencing (upload_file >> remove_file), demonstrates cloud storage interactions using Azure Data Lake Storage operators (LocalFilesystemToADLSOperator and ADLSDeleteOperator), and includes explicit dependency management between upload and delete operations. The business logic of uploading and then removing files is correctly implemented.",0.6666666666666666
283,Ground Truth (HuggingFace),335,1,"The code correctly uses the WinRMOperator from airflow.providers.microsoft.winrm.operators.winrm, which is the idiomatic Airflow approach for executing WinRM commands. It does not wrap WinRM logic in a PythonOperator, but instead uses the native operator designed for this purpose. The WinRMHook is also properly instantiated and passed to the operators.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that represent hallucination/leakage from test examples.",0,"The instruction requested a workflow that 'simulates real-world remote system management and monitoring scenarios' with 'inter-task dependencies and parallel processing capabilities'. While the DAG does show parallel processing (t1, t2, t3 run in parallel), it fails on multiple fronts: (1) The commands are Unix/Linux commands (ls -altr, sleep, echo) rather than Windows system-level commands appropriate for WinRM (should use PowerShell or cmd commands like Get-Service, Get-Process, etc.), (2) The tasks don't represent realistic system management scenarios (monitoring, health checks, service management, etc.), (3) There are no meaningful inter-task dependencies beyond a simple fan-in pattern to a final empty operator, and (4) The connection ID is named 'ssh_POC1' which is inappropriate for WinRM (should be a winrm connection).",0.3333333333333333
284,Ground Truth (HuggingFace),336,1,"The code uses idiomatic Airflow patterns throughout. It properly uses the TaskFlow API with @task decorators for Python tasks and BashOperator for bash tasks. It leverages native Airflow XCom mechanisms (ti.xcom_push, ti.xcom_pull, XComArg) rather than wrapping everything in generic PythonOperators. The use of XComArg for implicit XCom passing and task dependencies is particularly idiomatic for modern Airflow.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.models.xcom_arg.XComArg, airflow.providers.standard.operators.bash.BashOperator, airflow.sdk.DAG and task decorator, pendulum). There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no hallucinated or non-existent Airflow components.",1,"The DAG fully adheres to the instruction. It demonstrates cross-task data sharing and XCom communication using both Python (@task decorated functions) and Bash operators (BashOperator). It shows multiple XCom techniques: explicit push with ti.xcom_push(), implicit push by returning values, pulling with ti.xcom_pull(), using XComArg for templating in bash commands, and demonstrates both manual key-based XComs and default return_value XComs. The workflow clearly shows how different tasks push and pull data between each other.",1.0
285,Ground Truth (HuggingFace),338,1,"The code uses the specific GCSToPrestoOperator from airflow.providers.presto.transfers.gcs_to_presto, which is the idiomatic Airflow approach for transferring data from GCS to Presto. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready code. This is a clear example of test infrastructure leaking into what should be clean DAG code.,1,"The DAG fulfills the user instruction to create an automated data migration workflow that moves structured CSV data from cloud storage (GCS) to a distributed query engine (Presto) for analysis. It uses schedule='@once' for one-time data transfer as requested, and implements efficient data transfer using the appropriate operator. The business logic of migrating CSV data from GCS to Presto is correctly implemented.",0.6666666666666666
286,Ground Truth (HuggingFace),340,1,"The code uses native Airflow AWS provider operators throughout: EksCreateClusterOperator, EksDeleteClusterOperator, EksPodOperator, EksClusterStateSensor, and EksFargateProfileStateSensor. It does not wrap functionality in PythonOperator with hooks, but instead leverages the purpose-built operators for EKS cluster management and pod execution. This is idiomatic Airflow.",0,"The code imports from 'system.amazon.aws.utils' which appears to be internal testing infrastructure (SystemTestContextBuilder, get_describe_pod_operator). These are not standard Airflow libraries and suggest this is test harness boilerplate rather than production-ready code. The pattern of using sys_test_context_task and SystemTestContextBuilder is characteristic of Airflow's internal system tests, not production DAGs.",0,"The code is incomplete - the delete_cluster_and_fargate_profile operator definition is cut off mid-parameter. Additionally, while the code demonstrates creating an EKS cluster with Fargate and running a test pod, it's missing the complete cleanup step and the task dependency chain is not fully defined. The instruction requested an 'end-to-end process of creating, testing, and cleaning up' but the cleanup is incomplete and there's no visible chain() call or dependency setup to orchestrate the workflow properly.",0.3333333333333333
287,Ground Truth (HuggingFace),342,0,"While the code does use WinRMOperator (which is appropriate), it unnecessarily creates a WinRMHook instance manually and passes it to each operator via the 'winrm_hook' parameter. The idiomatic Airflow pattern is to use the 'winrm_conn_id' parameter directly in the WinRMOperator, allowing the operator to manage the hook internally. The code shows 'ssh_conn_id=""ssh_POC1""' which is also incorrect naming for a WinRM connection. The proper pattern would be: WinRMOperator(task_id='task', command='...', winrm_conn_id='winrm_connection_id').",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'list(dag.tasks) >> watcher()', and 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is test code, not production-ready DAG code.",1,"The DAG successfully demonstrates the core requirements: parallel remote command execution across multiple Windows servers (t1, t2, t3 run in parallel as indicated by '[t1, t2, t3] >> run_this_last'), executes different shell commands simultaneously ('ls -altr', 'sleep 60', 'echo'), and tracks their completion (all converge to 'run_this_last' task). The workflow structure correctly showcases parallel execution and completion tracking as requested.",0.3333333333333333
288,Ground Truth (HuggingFace),344,1,The code uses native Airflow Kafka providers with specific operators designed for the task: ProduceToTopicOperator for producing messages and AwaitMessageTriggerFunctionSensor for consuming/listening to messages. This is idiomatic Airflow usage rather than wrapping Kafka logic in generic PythonOperators with hooks.,0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. Additionally, the code references 'example_dag_event_listener.await_function' as a string path which appears to be example/test code structure.",1,"The DAG fulfills the instruction requirements: (1) Uses Kafka as message broker with proper connections, (2) Produces sequential numeric data (0-49) via producer_function, (3) Implements a sensor (AwaitMessageTriggerFunctionSensor) that dynamically responds to messages, (4) Detects and reacts to numbers divisible by 3, 5, or 15 with appropriate logic in await_function and wait_for_event. The event-driven architecture with real-time listener is properly demonstrated.",0.6666666666666666
289,Ground Truth (HuggingFace),346,1,"The code correctly uses the AzureContainerInstancesOperator from airflow.providers.microsoft.azure.operators.container_instances, which is the idiomatic and native Airflow operator for Azure Container Instances. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator with proper parameters.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready DAG code. This is a clear example of test infrastructure leakage.,1,"The DAG fulfills the user instruction requirements: (1) Uses Azure Container Instances for ephemeral computational tasks, (2) Demonstrates container-based job triggering via AzureContainerInstancesOperator, (3) Implements standardized resource allocation (memory_in_gb=4.0, cpu=1.0), and (4) Uses dynamic naming conventions with Jinja templating (name='aci-test-{{ ds }}'). The workflow is lightweight and reproducible as requested.",0.6666666666666666
290,Ground Truth (HuggingFace),347,0,"While the DAG uses native operators like GCSToS3Operator, S3CreateBucketOperator, and GCSCreateBucketOperator for most operations, it wraps the GCS file upload logic in a PythonOperator (via @task decorator) using GCSHook instead of using the native GCSCreateObjectOperator or similar operator. The upload_gcs_file function should use an idiomatic operator like GCSCreateObjectOperator or GCSUploadOperator rather than wrapping hook logic in a Python task.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. It also includes test-specific logic like sys_test_context_task, watcher(), and test_run = get_test_run(dag). These are internal testing modules and patterns that indicate this is test code, not production-ready code.",1,"The DAG successfully fulfills the user instruction by demonstrating inter-cloud data transfer between GCS and S3, showcasing task chaining (using the chain() function), cloud service interactions (GCP and AWS), and dynamic environment configuration (using test_context for env_id and project_id). It creates resources (GCS bucket, S3 bucket), transfers data (GCS to S3), and deletes resources (cleanup of both buckets), providing a complete systematic workflow across Google Cloud and AWS infrastructure.",0.3333333333333333
291,Ground Truth (HuggingFace),348,1,"The code uses native Airflow providers and operators specifically designed for Cloud SQL operations (CloudSQLCreateInstanceOperator, CloudSQLInstancePatchOperator, CloudSQLExportInstanceOperator, CloudSQLImportInstanceOperator, etc.) and GCS operations (GCSCreateBucketOperator, GCSBucketCreateAclEntryOperator, etc.). It does not wrap these in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear indicators of test harness boilerplate. It uses environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and the docstring explicitly states this is an 'Example Airflow DAG'. The naming conventions (ENV_ID, SYSTEM_TESTS_*) and the overall structure suggest this was copied from Airflow's system tests rather than being production-ready code. While it doesn't explicitly import test utilities, the test infrastructure patterns are embedded throughout.",1,"The code fulfills the user instruction to construct an automated data infrastructure pipeline demonstrating database provisioning, configuration, and data migration strategies. It includes: (1) Database provisioning via CloudSQLCreateInstanceOperator, (2) Configuration management through CloudSQLInstancePatchOperator with configurable settings, (3) Data migration strategies using CloudSQLExportInstanceOperator and CloudSQLImportInstanceOperator, (4) Database creation/deletion operations, (5) Replica management (failover and read replicas), and (6) Instance cloning. The approach is systematic, reproducible, and configurable as requested.",0.6666666666666666
292,Ground Truth (HuggingFace),350,1,"The DAG uses appropriate Airflow providers and operators for the tasks: GCSCreateBucketOperator, GCSDeleteBucketOperator, LocalFilesystemToGCSOperator, and GCSToGoogleDriveOperator. These are the idiomatic, native operators for GCS and Google Drive operations. While there is a @task decorator used for creating a temporary connection (which wraps logic in a Python function), this is acceptable as there is no dedicated operator for creating Airflow connections programmatically, and this is a setup task rather than the core business logic.",1,The code uses only standard Airflow libraries and official provider packages (airflow.providers.google.cloud and airflow.providers.google.suite). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears to be production-ready with proper imports from official Airflow packages.,0,"The code is incomplete - it cuts off mid-statement at 'source_object=f""{T'. This means the DAG cannot execute successfully. While the visible portions show the requested functionality (creating a temporary GCP connection, uploading files to GCS, and transferring to Google Drive with different configuration options), the incomplete nature means it fails to deliver a working solution. Additionally, there's no cleanup task to delete the temporary connection that was created, which would be expected in a complete showcase example.",0.6666666666666666
293,Ground Truth (HuggingFace),351,1,"The code uses the DrillOperator from airflow.providers.apache.drill.operators.drill, which is the idiomatic and correct way to interact with Apache Drill in Airflow. It does not wrap Drill logic in a PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. This is a hallmark of leaked test code that should be removed for production use.",0,"The instruction specifically requests transforming raw JSON employee data into Parquet format. However, the SQL in the DrillOperator creates a table using 'create table dfs.tmp.employee as select * from cp.`employee.json`' which in Apache Drill's default behavior creates a table in the default storage format (typically JSON or the source format), not explicitly Parquet. To properly convert to Parquet format, the SQL should include 'STORE AS parquet' clause or configure the table to use Parquet format explicitly. The code performs table creation but does not guarantee Parquet format output as requested.",0.3333333333333333
294,Ground Truth (HuggingFace),352,1,"The code uses native Airflow providers and operators specifically designed for Azure Cosmos DB tasks. It properly uses AzureCosmosDocumentSensor for monitoring documents and AzureCosmosInsertDocumentOperator for inserting documents, rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than clean production-ready code.",1,"The DAG fulfills the user instruction completely. It monitors a CosmosDB collection for a specific document using AzureCosmosDocumentSensor (t1), and when that condition is met, it inserts a new document using AzureCosmosInsertDocumentOperator (t2). The workflow demonstrates both document tracking and insertion in a cloud-based NoSQL database environment as requested, with proper task dependencies (t1 >> t2).",0.6666666666666666
295,Ground Truth (HuggingFace),353,1,"The code uses native Airflow Vertex AI operators throughout, including CreateAutoMLForecastingTrainingJobOperator, CreateBatchPredictionJobOperator, CreateDatasetOperator, and GCS operators. It does not wrap functionality in PythonOperators with hooks, but instead leverages the purpose-built operators from airflow.providers.google.cloud.operators.vertex_ai and related provider packages. This is the idiomatic Airflow approach.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', references 'airflow-system-tests-resources' bucket, and includes 'SYSTEM_TESTS' in environment variable names. The comment at the top states 'Example Airflow DAG for Google Vertex AI service testing Batch Prediction operations.' While it doesn't import test utilities directly, the code structure and naming conventions are clearly from Airflow's internal system tests rather than production-ready code.",0,"The DAG is incomplete and does not fulfill the instruction requirements. The code cuts off mid-definition in the CreateAutoMLForecastingTrainingJobOperator (line ends with a comma after 'unavailable_at_forecast_columns'). More critically, while it includes dataset creation and training job setup, it does not show the complete workflow including: (1) the actual batch prediction job creation and execution, (2) task dependencies that chain ingestion → training → prediction, and (3) any output handling for business planning. The instruction asked for a complete pipeline that 'transforms raw sales data into predictive models' and 'generates batch predictions for business planning', but the provided code is truncated and incomplete.",0.3333333333333333
296,Ground Truth (HuggingFace),354,1,"The DAG uses native Airflow Dataplex providers and operators (DataplexCatalogCreateEntryGroupOperator, DataplexCatalogCreateEntryTypeOperator, DataplexCatalogCreateAspectTypeOperator, etc.) instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Google Cloud Dataplex operations.",0,"The code imports from 'system.google' module ('from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID'), which appears to be an internal testing module. It also uses test-related environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and includes test harness patterns. This is not production-ready code and contains test infrastructure leakage.",0,"The DAG is incomplete - it cuts off mid-definition (the 'create_aspect_type' task is not fully defined). While it demonstrates creating entry groups, entry types, and begins to create aspect types, it does not show the full lifecycle including update, get, list, search, lookup, and delete operations that were imported. The instruction requested a 'full lifecycle of metadata catalog objects from creation to potential deletion', but the DAG is truncated and doesn't establish task dependencies or complete the workflow.",0.3333333333333333
297,Ground Truth (HuggingFace),355,0,"The code creates a custom GetRequestOperator that wraps httpx.get() in a BaseOperator. This is not idiomatic Airflow. The proper approach would be to use the SimpleHttpOperator from airflow.providers.http.operators.http, which is specifically designed for making HTTP requests. The code falls into the anti-pattern of creating custom operators for functionality that already exists in Airflow providers.",1,"The code is clean and uses only standard Airflow libraries (airflow.decorators, airflow.models.baseoperator, airflow.operators.email) along with legitimate third-party libraries (httpx, pendulum). There are no imports from internal testing modules or test harness boilerplate. The comments like '[START dag_decorator_usage]' and '[END dag_decorator_usage]' are documentation markers, not test harness code.",1,"The DAG fully implements the requested functionality: it retrieves the server's external IP address (via httpbin.org/get which returns the origin IP), processes that information, and sends an email notification containing the IP address. The workflow successfully tracks and communicates server connection details as requested.",0.6666666666666666
298,Ground Truth (HuggingFace),357,1,"The code uses native Airflow providers and operators throughout. It leverages GCS operators (GCSCreateBucketOperator, GCSSynchronizeBucketsOperator, GCSDeleteBucketOperator), Vertex AI Auto ML operators (CreateAutoMLTextTrainingJobOperator, DeleteAutoMLTrainingJobOperator), and Vertex AI dataset operators (CreateDatasetOperator, ImportDataOperator, DeleteDatasetOperator). No PythonOperator wrappers around hooks are used. This is idiomatic Airflow code that properly utilizes the Google Cloud provider package.",0,"The code contains clear indicators of test harness boilerplate. The docstring states 'Example Airflow DAG for Google Vertex AI service testing Auto ML operations.' The code uses test-specific patterns like ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', and RESOURCE_DATA_BUCKET set to 'airflow-system-tests-resources'. These are internal testing conventions, not production-ready code. Additionally, the code includes START/END comment markers typical of documentation examples: '# [START how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]' and '# [END ...]'.",0,"The DAG is incomplete and does not fulfill the instruction. The last line 'delete_bucket = GCSDeleteBucketOperator' is incomplete - it's missing the operator instantiation with required parameters and is not connected to the DAG workflow. This means the resource cleanup step is not properly implemented. Additionally, there are no task dependencies defined in the code, so the orchestration pipeline is not properly sequenced. A complete implementation would need task dependencies (e.g., create_bucket >> move_dataset_file >> create_text_dataset >> import_text_dataset >> create_auto_ml_text_training_job >> [delete_auto_ml_text_training_job, delete_text_dataset, delete_bucket]).",0.3333333333333333
299,Ground Truth (HuggingFace),359,1,"The code uses the native OpenAITriggerBatchOperator from airflow.providers.openai.operators.openai for batch processing, which is the idiomatic Airflow approach. It also properly uses OpenAIHook from the OpenAI provider package for file operations. The DAG leverages TaskFlow API with @task decorators and dynamic task mapping with .expand(), which are modern Airflow patterns. While some tasks use PythonOperator implicitly via @task decorator, they perform necessary data preparation (generate_messages, batch_upload) that doesn't have dedicated operators, which is acceptable.",0,The code contains test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG code.,1,"The DAG fully implements the requested requirements: (1) Asynchronous batch processing using OpenAI's API with deferrable=True on OpenAITriggerBatchOperator, (2) Dynamic task generation for multiple Pokémon descriptions using .expand(pokemon=POKEMONS), (3) Deferred execution pattern with the deferrable operator, and (4) Scalable pattern for managing batch processing lifecycle including file upload, batch triggering, and cleanup. The workflow demonstrates all key aspects of the instruction.",0.6666666666666666
300,Ground Truth (HuggingFace),360,1,"The code uses native Airflow operators designed for their specific tasks: GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket management, TranslateTextOperator and TranslateTextBatchOperator for translation operations. No generic PythonOperator wrappers are used. All operators are from the official Google Cloud provider package (airflow.providers.google.cloud.operators).",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then adds 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules and test execution patterns that have leaked into what should be production-ready code.",1,"The DAG fully implements the requested functionality: (1) Creates a temporary bucket using GCSCreateBucketOperator, (2) Performs advanced translation orchestration with both single text translation (TranslateTextOperator) and batch translation (TranslateTextBatchOperator) showcasing cloud services, (3) Establishes clear task dependencies with proper sequencing (create_bucket >> translations >> delete_bucket), (4) Implements resource management by cleanly deleting the bucket after processing using trigger_rule=TriggerRule.ALL_DONE to ensure cleanup happens regardless of translation task success/failure.",0.6666666666666666
301,Ground Truth (HuggingFace),362,1,"The code correctly uses the AzureContainerInstancesOperator from airflow.providers.microsoft.azure.operators.container_instances, which is the idiomatic and native operator for deploying Azure Container Instances. It does not wrap logic in PythonOperator with hooks, but uses the provider-specific operator as intended.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leakage.,0,"The instruction explicitly requests 'deploying AND testing containerized applications'. The DAG only demonstrates deployment of containers using AzureContainerInstancesOperator but includes no testing logic whatsoever (no validation tasks, no health checks, no test execution within containers). Additionally, while it shows different registry sources and volume mounting strategies as requested, the lack of any testing component means it fails to fulfill the core requirement of the instruction.",0.3333333333333333
302,Ground Truth (HuggingFace),364,1,"The code uses the native DynamoDBValueSensor from airflow.providers.amazon.aws.sensors.dynamodb, which is the idiomatic Airflow approach for sensing DynamoDB values. It does not wrap sensor logic in PythonOperator. The TaskFlow API (@task decorator) is appropriately used for setup/teardown tasks that require custom boto3 logic where no specific operator exists for table creation/deletion.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: (1) 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' - internal test utility, (2) 'from tests_common.test_utils.watcher import watcher' - test module import, (3) 'from tests_common.test_utils.system_tests import get_test_run' - test module import, (4) 'test_run = get_test_run(dag)' - test harness code, (5) 'sys_test_context_task = SystemTestContextBuilder().build()' - test context builder. This is clearly a system test example, not production-ready code.",1,"The DAG fully implements the requested functionality: (1) Creates a DynamoDB table with proper schema, (2) Demonstrates sensor functionality for tracking data availability in DynamoDB, (3) Implements value checking with different matching strategies (exact match and any-value match from a list), (4) Includes automatic table deletion with proper trigger rules, (5) Uses task chaining to create a systematic workflow. All requirements from the instruction are met.",0.6666666666666666
303,Ground Truth (HuggingFace),367,1,"The code uses native Airflow operators specifically designed for inter-DAG dependencies: ExternalTaskSensor and ExternalTaskMarker. These are the idiomatic, provider-specific operators for establishing cross-DAG dependencies. The code does not wrap logic in PythonOperator with hooks, but instead uses the appropriate sensors and markers as intended by Airflow's design patterns.",1,"The code imports only standard Airflow libraries (airflow.operators.empty.EmptyOperator, airflow.sensors.external_task.ExternalTaskMarker, airflow.sensors.external_task.ExternalTaskSensor). There are no imports from internal testing modules like 'tests_common.test_utils' or test harness boilerplate such as 'get_test_run()'. The code is clean and production-ready.",0,"The instruction requested a 'multi-DAG workflow that demonstrates dependency management between two related data processing pipelines' with parent-child relationship. While the code correctly demonstrates ExternalTaskSensor and ExternalTaskMarker for inter-DAG dependencies, it fails to showcase actual 'data processing pipelines'. The DAGs only contain sensors, markers, and empty operators - there is no data processing logic whatsoever (no data extraction, transformation, loading, validation, etc.). The instruction implies demonstrating how real data pipelines can depend on each other, not just empty placeholder tasks. Additionally, child_task2 references a non-existent 'parent_dag_task_group_id' in the parent DAG, which would cause runtime errors.",0.6666666666666666
304,Ground Truth (HuggingFace),369,1,"The code uses the native `TaskQueueEmptySensor` from `airflow.providers.google.cloud.sensors.tasks`, which is the idiomatic Airflow approach for monitoring Google Cloud Tasks queues. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built sensor operator designed specifically for this task.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (`airflow.models.dag.DAG` and `airflow.providers.google.cloud.sensors.tasks.TaskQueueEmptySensor`). There are no imports from internal testing modules like `tests_common.test_utils.system_tests` or test harness boilerplate. The comments with START/END markers are standard documentation patterns in Airflow examples, not test artifacts.",1,"The code fully adheres to the instruction. It creates an Airflow DAG that monitors a Google Cloud Tasks queue using a sensor to check if the queue is empty (i.e., all tasks have been processed) before proceeding. This serves as the critical checkpoint in a distributed task processing system as requested. The sensor will block downstream operations until the queue is empty, which is exactly what was asked for.",1.0
305,Ground Truth (HuggingFace),370,1,"The code uses native Airflow operators specifically designed for inter-DAG dependencies: ExternalTaskMarker and ExternalTaskSensor from airflow.sensors.external_task. It does not wrap functionality in PythonOperator with hooks, but instead uses the appropriate provider operators for the task. This is the idiomatic way to handle external task dependencies in Airflow.",1,"The code is clean and production-ready, using only standard Airflow libraries (airflow.models.dag, airflow.operators.empty, airflow.sensors.external_task). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate such as 'get_test_run(dag)'. The code includes documentation comments but these are appropriate for production code examples.",1,"The code fully satisfies the user instruction. It creates two separate DAGs (parent and child) that demonstrate inter-DAG synchronization. The parent DAG ('example_external_task_marker_parent') contains an ExternalTaskMarker that signals completion, and the child DAG ('example_external_task_marker_child') contains an ExternalTaskSensor that waits for the parent's specific task to finish before proceeding. The implementation shows the complete workflow with proper configuration including timeout, allowed_states, and failed_states. Additionally, it demonstrates a bonus pattern with task group sensing (child_task2), making it a comprehensive example.",1.0
306,Ground Truth (HuggingFace),371,1,"The code uses native Airflow Vertex AI operators from airflow.providers.google.cloud.operators.vertex_ai, including CreateAutoMLTabularTrainingJobOperator, CreateCustomTrainingJobOperator, CreateEndpointOperator, DeployModelOperator, and CreateBatchPredictionJobOperator. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.",1,The code imports only standard Airflow libraries from airflow.providers.google.cloud.operators.vertex_ai and core Airflow modules. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate like 'get_test_run(dag)'. The code appears to be production-ready template code.,0,"The code is incomplete - it cuts off mid-definition (IMAGE_DATASET dictionary is not closed). More critically, while it imports many Vertex AI operators, it does not actually instantiate a DAG or define any tasks that create an end-to-end ML workflow. The instruction asks for a 'comprehensive data science pipeline that demonstrates training, evaluation, and deployment' but the provided code only shows imports and variable definitions without any actual DAG structure, task definitions, or task dependencies that would constitute a working pipeline.",0.6666666666666666
307,Ground Truth (HuggingFace),372,1,"The code correctly uses the native AzureSynapseRunPipelineOperator from airflow.providers.microsoft.azure.operators.synapse, which is the idiomatic way to trigger Azure Synapse pipelines in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production-ready code.",1,"The DAG successfully demonstrates Azure Synapse integration with Apache Airflow as requested. It shows enterprise data orchestration through automated pipeline triggering using the AzureSynapseRunPipelineOperator, which connects cloud data processing services (Azure Synapse) with workflow management (Airflow). The implementation is simple and clear, fulfilling the user's instruction to create a workflow demonstrating this integration.",0.6666666666666666
308,Ground Truth (HuggingFace),375,1,"The code uses native Asana operators from the official Airflow provider package (airflow.providers.asana.operators.asana_tasks). It properly utilizes AsanaCreateTaskOperator, AsanaFindTaskOperator, AsanaUpdateTaskOperator, and AsanaDeleteTaskOperator instead of wrapping Asana API calls in PythonOperators with hooks. This is the idiomatic Airflow approach for Asana task management.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production-ready DAG code.",1,"The DAG fully demonstrates the complete lifecycle of Asana task management as requested. It includes: (1) Creating a task with AsanaCreateTaskOperator, (2) Finding tasks with AsanaFindTaskOperator, (3) Updating a task with AsanaUpdateTaskOperator, and (4) Deleting a task with AsanaDeleteTaskOperator. The operations are executed sequentially (create >> find >> update >> delete), showcasing the full lifecycle in an automated, sequential manner as specified in the instruction.",0.6666666666666666
309,Ground Truth (HuggingFace),377,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud DLP tasks. It imports from 'airflow.providers.google.cloud.operators.dlp' and uses operators like CloudDLPDeidentifyContentOperator, CloudDLPReidentifyContentOperator, CloudDLPCreateDeidentifyTemplateOperator, etc. This is the idiomatic approach rather than wrapping DLP logic in PythonOperator with hooks.",1,"The code is clean and production-ready. It uses only standard Airflow libraries and Google Cloud DLP provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The use of ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables is a common pattern for parameterization, not test leakage.",1,"The DAG fully addresses the user instruction to create a data privacy workflow that de-identifies sensitive phone numbers using Google Cloud DLP. It demonstrates: (1) masking phone numbers with CloudDLPDeidentifyContentOperator using replace_config, (2) template management (create, list, get, update, delete operations), (3) reversible transformations using crypto_deterministic_config, and (4) reidentification with CloudDLPReidentifyContentOperator. The workflow specifically targets PHONE_NUMBER info types in tabular data as requested.",1.0
310,Ground Truth (HuggingFace),379,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator for bucket creation, LocalFilesystemToS3Operator for file transfer, and S3DeleteBucketOperator for cleanup. Task decorators are used for simple Python functions (file creation/deletion), which is idiomatic for Airflow 2.0+. No unnecessary wrapping of hooks in PythonOperators.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test/example code rather than production-ready code.",1,"The DAG fully implements the requested lifecycle: dynamic environment setup (via test_context), S3 bucket creation (S3CreateBucketOperator), local file generation (create_temp_file task), file upload (LocalFilesystemToS3Operator), and resource cleanup (delete_s3_bucket and delete_temp_file tasks). The systematic approach with proper task chaining demonstrates cloud file transfer best practices including cleanup with ALL_DONE trigger rules.",0.6666666666666666
311,Ground Truth (HuggingFace),380,1,"The code uses native Airflow Vertex AI operators (CreateCustomTrainingJobOperator, CreateDatasetOperator, UploadModelOperator, AddVersionAliasesOnModelOperator, etc.) and GCS operators from the official Google Cloud provider. It does not wrap functionality in PythonOperator + Hook patterns, but instead leverages the purpose-built operators for Vertex AI model service operations. This is idiomatic Airflow.",0,"The code appears to be derived from Airflow's system tests or example DAGs. The presence of 'SYSTEM_TESTS_ENV_ID', 'SYSTEM_TESTS_GCP_PROJECT', 'RESOURCE_DATA_BUCKET = ""airflow-system-tests-resources""', and the comment 'Example Airflow DAG for Google Vertex AI service testing Model Service operations' strongly suggests this is test harness boilerplate. While it doesn't explicitly import from 'tests_common.test_utils', the structure and naming conventions indicate this is test infrastructure code rather than production-ready code.",0,"The instruction requests a housing price prediction pipeline with dataset creation, model training, versioning, deployment, and robust error handling. While the code includes relevant operators for these tasks, it is incomplete (cuts off at 'create_bucket' task definition) and lacks: (1) Complete task definitions and dependencies showing the full workflow, (2) Explicit error handling mechanisms (no error callbacks, retries configuration, or failure handling tasks visible), (3) Deployment operators (no endpoint creation/deployment visible), (4) The workflow orchestration that ties dataset → training → versioning → deployment together. The code is a skeleton/template rather than a complete implementation of the requested pipeline.",0.3333333333333333
312,Ground Truth (HuggingFace),381,1,"The code correctly uses the AzureBatchOperator from airflow.providers.microsoft.azure.operators.batch, which is the idiomatic and native Airflow operator for Azure Batch tasks. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator with appropriate parameters.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fully satisfies the user instruction. It demonstrates remote task scheduling and execution using Azure's cloud infrastructure (Azure Batch), configures a computational job with specific VM specifications (standard_d2s_v3, Ubuntu 22.04, target_dedicated_nodes=1), includes a basic command line task (/bin/bash -c 'echo hello world!'), and illustrates the process of defining a cloud-based computational workflow with precise infrastructure (pool_id, vm_node_agent_sku_id, vm_publisher, vm_offer, vm_sku) and execution parameters (batch_job_id, batch_task_id, batch_task_command_line).",0.6666666666666666
313,Ground Truth (HuggingFace),382,1,"The code uses native Airflow providers and operators specifically designed for Microsoft Graph/Power BI operations. It leverages MSGraphAsyncOperator for API calls and MSGraphSensor for monitoring status, which are the appropriate idiomatic Airflow patterns. The code does not wrap logic in PythonOperator + Hook combinations, but instead uses the dedicated operators from airflow.providers.microsoft.azure.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production DAG code.",1,"The DAG fulfills all requirements from the user instruction: (1) retrieves workspace information via the workspaces_task and workspaces_info_task, (2) refreshes datasets automatically through refresh_dataset_task, (3) scans workspace modifications using the modified workspaces endpoint, (4) tracks dataset refresh status with refresh_dataset_history_task sensor, and (5) handles administrative Power BI tasks through the admin API endpoints. All requested business logic is implemented.",0.6666666666666666
314,Ground Truth (HuggingFace),383,1,"The code uses the native PapermillOperator from airflow.providers.papermill.operators.papermill, which is the idiomatic way to execute Jupyter notebooks in Airflow. It does not wrap Papermill logic in a generic PythonOperator, but instead uses the purpose-built operator designed for this task. The verification step uses a TaskFlow @task decorator, which is also idiomatic for Airflow 3.0.0.",0,The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready code. This is a hallucination/leakage issue that makes the code unsuitable for production use without modification.,1,"The code fully adheres to the instruction. It: (1) uses Papermill to dynamically execute a Jupyter notebook via PapermillOperator, (2) uses date-based parameters with templating ('{{ logical_date }}'), (3) generates a new output notebook for each workflow run ('/tmp/out-{{ logical_date }}.ipynb'), and (4) verifies the notebook's execution by checking a specific message ('Ran from Airflow at {logical_date}!') using the scrapbook library in the check_notebook task.",0.6666666666666666
315,Ground Truth (HuggingFace),384,1,"The code uses native AWS SageMaker operators (SageMakerCreateNotebookOperator, SageMakerStartNoteBookOperator, SageMakerStopNotebookOperator, SageMakerDeleteNotebookOperator) from airflow.providers.amazon.aws.operators.sagemaker. It does not wrap logic in PythonOperator with hooks, which is the correct idiomatic approach for Airflow.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder', 'from tests.system.utils.watcher import watcher', and 'from tests.system.utils import get_test_run'. These are internal testing modules that indicate this is test code, not production-ready DAG code.",1,"The DAG successfully demonstrates sequential management of a SageMaker notebook instance through all requested operational states: resource provisioning (create_instance), activation (start_instance), deactivation (stop_instance), and cleanup (delete_instance). The chain() function clearly shows the step-by-step process as requested in the instruction.",0.6666666666666666
316,Ground Truth (HuggingFace),385,1,"The code uses native Airflow operators appropriately. It uses ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator from the Google Cloud provider for VM management, and SSHOperator with ComputeEngineSSHHook for SSH operations. This is the idiomatic way to interact with GCE in Airflow rather than wrapping logic in PythonOperators with hooks.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' and at the end includes 'from tests_common.test_utils.watcher import watcher' with a comment '### Everything below this line is not part of example ### Just for system tests purpose ###'. These are internal testing modules that represent hallucination/leakage from test code.",1,"The DAG fulfills all requirements from the instruction: (1) Creates a VM instance using ComputeEngineInsertInstanceOperator, (2) Runs parallel SSH commands with different authentication techniques (one without IAP tunnel using metadata, one with IAP tunnel), demonstrated by the parallel task execution pattern [metadata_without_iap_tunnel, metadata_with_iap_tunnel], and (3) Cleans up infrastructure using ComputeEngineDeleteInstanceOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of SSH task outcomes.",0.6666666666666666
317,Ground Truth (HuggingFace),386,1,"The code uses native Airflow Cassandra operators and sensors (CassandraTableSensor and CassandraRecordSensor) from the Cassandra provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,The code explicitly imports testing utilities ('from tests_common.test_utils.system_tests import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). These are internal testing modules that should not appear in production-ready code.,0,"While the code creates sensors for monitoring Cassandra data availability (CassandraTableSensor) and record-level conditions (CassandraRecordSensor), it fails to implement the 'gatekeeper' workflow aspect. The instruction specifically requests a sensor-based workflow that acts as a gatekeeper before initiating downstream data transformation or analysis tasks. The current code only defines sensors without any downstream tasks, making it incomplete as a validation pipeline that gates subsequent operations.",0.3333333333333333
318,Ground Truth (HuggingFace),387,1,"The DAG uses appropriate Airflow providers and operators for the tasks: GoogleDriveToLocalOperator, LocalFilesystemToGCSOperator, GCSToGoogleDriveOperator, GoogleDriveFileExistenceSensor, GCSCreateBucketOperator, and GCSDeleteBucketOperator. The only PythonOperator usage (via @task decorator) is for setup/teardown tasks (create_connection, remove_file_from_drive) which is appropriate since these are auxiliary operations without dedicated operators. The core file transfer operations use native operators as intended.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which appears to be an internal testing module. Additionally, the code contains test harness patterns like ENV_ID from environment variables for system tests, CONNECTION_ID generation with test suffixes, and overall structure that resembles Airflow's internal system test examples rather than production-ready code. The DAG is also incomplete (ends abruptly without proper task dependencies defined after remove_file_from_drive).",1,"The DAG successfully demonstrates bidirectional file transfer between Google Drive and Google Cloud Storage as requested. It shows: (1) uploading a file to GCS, (2) copying from GCS to Google Drive (GCSToGoogleDriveOperator), (3) detecting the file in Google Drive (GoogleDriveFileExistenceSensor), (4) downloading from Google Drive to local (GoogleDriveToLocalOperator), and (5) cleanup. This fulfills the requirement to showcase cross-platform file movement and synchronization capabilities between different cloud storage platforms.",0.6666666666666666
319,Ground Truth (HuggingFace),388,0,"The code heavily relies on PythonOperator pattern (via @task decorator) wrapping boto3 client calls instead of using native Airflow operators. For example, create_quicksight_data_source, create_quicksight_dataset, delete_quicksight_data_source, delete_dataset, and delete_ingestion all use boto3.client('quicksight') directly. While QuickSightCreateIngestionOperator and QuickSightSensor are imported (suggesting awareness of proper operators), the majority of QuickSight operations are implemented as Python callables rather than using dedicated operators from airflow.providers.amazon.aws.operators.quicksight.",0,"The code explicitly imports from internal testing modules: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder'. It also includes test harness boilerplate with 'sys_test_context_task = SystemTestContextBuilder().build()' and 'test_context = sys_test_context_task()'. The comments reference test-specific behavior ('If Create Ingestion fails for any reason...', 'changing the template for the ingestion name or the ENV_ID and re-running the test'). This is clearly test infrastructure code, not production-ready DAG code.",0,"The DAG is incomplete (cuts off at 'env_id = t') and does not demonstrate a complete end-to-end ETL workflow. While it includes some components for S3 operations (S3CreateBucketOperator, S3CreateObjectOperator) and QuickSight setup, the actual data transformation logic, systematic ETL flow, and proper resource cleanup orchestration are not fully implemented. The instruction requested 'extract, transfer, and load (ETL) project information with automated provisioning and cleanup', but the code appears to be a partial example/test case rather than a complete business solution.",0.0
320,Ground Truth (HuggingFace),389,1,"The code uses native Azure Service Bus operators from airflow.providers.microsoft.azure.operators.asb throughout. It properly leverages AzureServiceBusCreateQueueOperator, AzureServiceBusSendMessageOperator, AzureServiceBusReceiveMessageOperator, AzureServiceBusTopicCreateOperator, AzureServiceBusSubscriptionCreateOperator, and other specialized operators instead of wrapping logic in PythonOperator with hooks. This is idiomatic Airflow usage.",0,"The code contains test harness boilerplate that should not be in production DAG code. Specifically: (1) 'import pytest' at the top, (2) 'pytest.skip(""Azure Service Bus not available"", allow_module_level=True)' in the exception handler, and (3) references to test-related environment variables like EXECUTION_TIMEOUT with test-oriented defaults. These are testing artifacts that indicate this was copied from a test suite rather than being clean production code.",0,"The code is incomplete - it cuts off mid-comment after the update_service_bus_subscription operator definition. The instruction requested a 'complete lifecycle' demonstration including queue management, topic management, message transmission strategies, and subscription handling. While the code shows creation of queues, topics, and subscriptions, as well as various message sending patterns (single, list, batch), it's missing: (1) the completion of the DAG (no task dependencies defined with chain() despite importing it, no cleanup/deletion tasks executed), (2) subscription message receiving (ASBReceiveSubscriptionMessageOperator is imported but not used), and (3) deletion operators for topics and subscriptions (imported but not instantiated). The truncated nature means the full lifecycle is not demonstrated.",0.3333333333333333
321,Ground Truth (HuggingFace),390,1,"The code uses native Airflow Vertex AI operators (CreateAutoMLForecastingTrainingJobOperator, CreateBatchPredictionJobOperator, CreateDatasetOperator) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic approach for working with Google Cloud Vertex AI in Airflow.",0,"The code contains clear test harness boilerplate and references to system testing infrastructure. Evidence includes: ENV_ID from 'SYSTEM_TESTS_ENV_ID', PROJECT_ID from 'SYSTEM_TESTS_GCP_PROJECT', RESOURCE_DATA_BUCKET = 'airflow-system-tests-resources', BIGQUERY_SOURCE pointing to 'airflow-system-tests-resources.vertex_ai_training_dataset.data', and tags=['example', 'vertex_ai', 'batch_prediction_job']. The docstring also explicitly states 'Example Airflow DAG for Google Vertex AI service testing Batch Prediction operations.' This is clearly test/example code, not production-ready code.",0,"The code is incomplete (cuts off mid-parameter at 'region=') and does not fulfill the complete instruction. While it sets up dataset creation and training job configuration with the required geographic attributes (city, zip_code, county) and 30-day forecast horizon, it's missing critical components: (1) the actual batch prediction job execution is not shown, (2) there's no data transformation pipeline from 'raw sales data', (3) there's no output/insight delivery mechanism. The instruction asks for an 'automated pipeline that transforms raw sales data into predictive insights' but this only shows model training setup without the complete end-to-end pipeline.",0.3333333333333333
322,Ground Truth (HuggingFace),391,1,"The code uses native Airflow AWS provider operators and sensors designed specifically for EKS management: EksCreateClusterOperator, EksCreateNodegroupOperator, EksClusterStateSensor, EksNodegroupStateSensor, and EksPodOperator. It does not wrap these in PythonOperators unnecessarily. The @task decorators for launch template management are appropriate for AWS SDK calls that don't have dedicated operators.",0,"The code explicitly imports from internal testing modules: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'from tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator'. It also uses test harness boilerplate like 'sys_test_context_task = SystemTestContextBuilder()...' and 'test_context = sys_test_context_task()'. This is not production-ready code and contains test infrastructure leakage.",0,"While the code demonstrates EKS cluster creation, validation (via sensors), and pod deployment setup, it is incomplete. The EksPodOperator definition is cut off mid-parameter ('get_logs='), making the DAG non-functional. Additionally, the code is heavily tied to test infrastructure rather than being a clean end-to-end production example as requested. The instruction asked for a 'robust workflow' demonstrating 'complete lifecycle' management, but this appears to be an incomplete test system example rather than a production-ready orchestration DAG.",0.3333333333333333
323,Ground Truth (HuggingFace),392,1,"The code uses native Airflow operators designed for the task: TranslateDocumentOperator and TranslateDocumentBatchOperator from airflow.providers.google.cloud.operators.translate. It also uses GCSCreateBucketOperator and GCSDeleteBucketOperator for GCS operations. No PythonOperator wrappers around hooks are used - all operations leverage provider-specific operators, which is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: (1) imports 'from tests_common.test_utils.watcher import watcher' and uses 'list(dag.tasks) >> watcher()', (2) imports 'from tests_common.test_utils.system_tests import get_test_run', and (3) includes 'test_run = get_test_run(dag)'. These are testing utilities that should not be present in production DAG code. The comments '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' also indicate this is test code rather than production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) Translates multilingual documents using cloud translation services (Google Cloud Translate V3 API), (2) Handles different file formats (XLSX, DOCX, PDF) as specified in the input configurations, (3) Manages input and output configurations through document_input_config and document_output_config parameters, (4) Demonstrates scalable document processing through both single document translation (TranslateDocumentOperator) and batch processing (TranslateDocumentBatchOperator with multiple target languages), and (5) Implements a complete workflow with bucket creation, translation operations, and cleanup.",0.6666666666666666
324,Ground Truth (HuggingFace),393,0,"The code uses TaskFlow API with @task decorators wrapping boto3 client calls instead of using native Airflow AWS providers. Airflow 2.7.2 has dedicated operators like DmsCreateReplicationInstanceOperator, DmsCreateEndpointOperator, EC2CreateSecurityGroupOperator, etc. The code follows the anti-pattern of wrapping SDK calls in PythonOperator-style tasks rather than leveraging the airflow.providers.amazon.aws.operators modules designed specifically for these AWS services.",1,"The code uses standard Airflow decorators (@task, TriggerRule) and boto3 SDK calls. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. While the code is incomplete (cuts off at delete_dms_assets), what is present appears to be production-oriented code without test artifacts.",1,"The code addresses all key requirements from the instruction: creates security groups (create_security_group task), configures database endpoints (create_dms_assets creates both source RDS and target S3 endpoints), and sets up a replication instance (create_replication_instance within create_dms_assets). The orchestration covers cloud infrastructure provisioning and data migration setup across multiple AWS services (EC2, RDS, DMS, S3) as requested.",0.6666666666666666
325,Ground Truth (HuggingFace),394,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocStartClusterOperator, DataprocStopClusterOperator, DataprocDeleteClusterOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also adds a watcher task with 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production code.",1,"The DAG successfully orchestrates the complete lifecycle of a Dataproc cluster as requested. It demonstrates: (1) cluster creation, (2) stopping the cluster, (3) starting the cluster, and (4) deletion of the cluster. The sequential task dependencies are clearly defined using the >> operator, showing the operational states (create -> stop -> start -> delete). This fulfills the requirement to illustrate programmatic management of cloud infrastructure using Airflow operators with clear sequential dependencies.",0.6666666666666666
326,Ground Truth (HuggingFace),395,1,"The code uses specific Airflow providers and operators designed for Kafka and Google Cloud Managed Kafka tasks. It imports and uses ManagedKafkaCreateClusterOperator, ManagedKafkaCreateTopicOperator, and appears to use ProduceToTopicOperator (though the code is cut off). The producer and consumer functions are appropriately defined as callable functions to be passed to Kafka operators, which is the idiomatic pattern for Kafka operators in Airflow. The code does not wrap these in generic PythonOperators but uses the native Kafka operators.",0,"The code contains clear indicators of test harness boilerplate. It uses ENV_ID with 'SYSTEM_TESTS_ENV_ID', PROJECT_ID with 'SYSTEM_TESTS_GCP_PROJECT', and includes patterns typical of Airflow system tests (e.g., dynamic naming with ENV_ID, Session manipulation for connection management). The imports include 'from airflow.settings import Session' which is used to programmatically create connections - a pattern more common in testing than production code. Additionally, the code includes START/END comment markers like '[START how_to_cloud_managed_kafka_produce_to_topic_operator]' which are documentation/example code markers, not production code patterns.",0,"The code is incomplete (cuts off at 'consume_fro') and does not fully demonstrate the requested workflow. While it shows cluster creation and topic creation, and appears to start implementing message production, the consumption part with a specific consumer group is not visible in the provided code. The instruction specifically requested 'producing messages, and consuming those messages with a specific consumer group' - the consumption implementation is missing or incomplete. Additionally, several imported operators (ManagedKafkaDeleteClusterOperator, ManagedKafkaDeleteConsumerGroupOperator, ManagedKafkaGetConsumerGroupOperator, ManagedKafkaListConsumerGroupsOperator, ManagedKafkaUpdateConsumerGroupOperator) are not used in the visible code, suggesting either incomplete implementation or unnecessary imports.",0.3333333333333333
327,Ground Truth (HuggingFace),396,1,The code uses native Airflow operators designed for the specific tasks: WasbBlobSensor for checking Azure blob existence and AzureBlobStorageToGCSOperator for transferring files from Azure to GCS. This is the idiomatic approach rather than wrapping hooks in PythonOperators.,0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher', adds 'list(dag.tasks) >> watcher()' to the DAG, and imports 'from tests_common.test_utils.system_tests import get_test_run' with 'test_run = get_test_run(dag)'. These are internal testing modules and test infrastructure that leak into what should be production-ready code.",1,"The DAG fulfills the user instruction completely. It checks for the existence of a file in an Azure container using WasbBlobSensor, then transfers that file to Google Cloud Storage using AzureBlobStorageToGCSOperator. The tasks are properly chained (wait_for_blob >> transfer_files_to_gcs) to showcase cross-cloud data synchronization and movement capabilities as requested.",0.6666666666666666
328,Ground Truth (HuggingFace),397,1,"The code correctly uses the PostgresOperator from airflow.providers.postgres.operators.postgres for all database operations. It does not wrap PostgreSQL logic in PythonOperator with hooks, but instead uses the native operator designed for PostgreSQL tasks. This is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production DAG code.",1,"The DAG successfully demonstrates task dependencies and sequential PostgreSQL operations as requested. It chains together four database tasks (create_pet_table >> populate_pet_table >> get_all_pets >> get_birth_date) with clear dependency management using the >> operator. The code illustrates how different database operations can be orchestrated in sequence, fulfilling the user's instruction.",0.6666666666666666
329,Ground Truth (HuggingFace),398,0,"The DAG uses only BashOperator with echo commands instead of proper metadata management operators. For a data governance pipeline tracking metadata entries, it should use specific providers like Google Cloud Data Catalog operators (DataCatalogCreateEntryGroupOperator, DataCatalogCreateEntryOperator, etc.) or similar metadata management operators. Using BashOperator to echo task names is not idiomatic for metadata management workflows.",1,"The code uses only standard Airflow libraries (airflow.models.dag, airflow.operators.bash, airflow.models.baseoperator). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective, even though it's functionally inadequate.",0,"The instruction requires an 'end-to-end data governance pipeline that tracks the creation, validation, and cleanup of metadata entries' with 'real-world metadata management processes'. The provided DAG only contains BashOperator tasks that echo strings - there is no actual metadata creation, validation, or cleanup logic. While it has create, get, list, and delete task names, they don't perform any real operations. The 'validation' step explicitly mentioned in the instruction is completely missing. This is a skeleton/mock DAG that doesn't fulfill the business requirements.",0.3333333333333333
330,Ground Truth (HuggingFace),399,1,"The code uses native BigQuery operators from airflow.providers.google.cloud.operators.bigquery throughout, including BigQueryInsertJobOperator, BigQueryCheckOperator, BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, and various quality check operators. It does not wrap BigQuery logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear hallucinations and test harness boilerplate. It imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' and 'from system.openlineage.operator import OpenLineageTestOperator', which are internal testing modules. It also uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, and includes test-specific naming patterns like 'SYSTEM_TESTS'. This is not production-ready code and includes test infrastructure that should not be present in a production DAG example.",0,"While the code demonstrates BigQuery operators and includes some quality check operators (BigQueryCheckOperator, BigQueryColumnCheckOperator, etc.) in the imports, the actual DAG implementation is incomplete (cuts off at 'bigquery_execute_multi_query = BigQueryInsertJobOperator(t'). More critically, the instruction asks for a pipeline that 'transforms raw data, performs quality checks, and demonstrates different query execution strategies.' The code shows data insertion and basic queries, but does not demonstrate meaningful data transformation logic, does not actually implement the quality check operators that were imported, and lacks a complete workflow showing the full pipeline requested. The DAG is more of a test harness example than a production data processing pipeline.",0.3333333333333333
331,Ground Truth (HuggingFace),400,1,"The code uses native Airflow providers and operators specifically designed for Looker integration. It correctly imports and uses LookerStartPdtBuildOperator and LookerCheckPdtBuildSensor from airflow.providers.google.cloud.operators.looker and airflow.providers.google.cloud.sensors.looker respectively. No generic PythonOperator wrappers are used; instead, it leverages the purpose-built Looker operators and sensors, which is the idiomatic Airflow approach.",1,"The code is clean and production-ready, using only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no references to test utilities. The code includes documentation comments with START/END markers which are common in Airflow example code but do not constitute test harness code.",1,The DAG fully adheres to the user instruction. It demonstrates: (1) Different task execution strategies for Looker PDT builds - both blocking (build_pdt_task with default asynchronous=False) and non-blocking (start_pdt_task_async with asynchronous=True); (2) Task chaining - shown by start_pdt_task_async >> check_pdt_task_async_sensor; (3) Sensor-based job monitoring - implemented via LookerCheckPdtBuildSensor that monitors the asynchronous PDT build using the materialization_id output from the async start task. The workflow clearly showcases both approaches as requested.,1.0
332,Ground Truth (HuggingFace),401,1,"The code uses the native TriggerDagRunOperator from airflow.providers.standard.operators.trigger_dagrun, which is the idiomatic and correct way to trigger another DAG in Airflow. It does not wrap this functionality in a PythonOperator or use hooks unnecessarily.",1,The code is clean and production-ready. It uses only standard Airflow libraries (airflow.providers.standard.operators.trigger_dagrun and airflow.sdk). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate code.,0,"The instruction explicitly asks to 'Create a scenario where one workflow initiates another workflow' - this requires BOTH DAGs to be present in the code. The provided code only contains the primary/controller DAG (example_trigger_controller_dag) but is missing the secondary/target DAG (example_trigger_target_dag) that should be triggered. While the docstring mentions both DAGs exist, only one is actually implemented. To fully demonstrate the workflow as requested, both the triggering DAG and the target DAG should be included in the code.",0.6666666666666666
333,Ground Truth (HuggingFace),402,1,"The code uses native Airflow providers and operators specifically designed for GKE and Kubernetes tasks. It imports from 'airflow.providers.google.cloud.operators.kubernetes_engine' and uses specialized operators like GKECreateClusterOperator, GKEStartKueueInsideClusterOperator, GKECreateCustomResourceOperator, and GKEStartKueueJobOperator. This is the idiomatic Airflow approach rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear indicators of test harness boilerplate. It uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables, includes 'ENV_ID' variable naming typical of system tests, and has comments like '[START howto_operator_gke_install_kueue]' and '[END howto_operator_gke_install_kueue]' which are documentation extraction markers used in Airflow's test/example infrastructure. While it doesn't directly import from 'tests_common', the code structure and variable naming clearly indicate this is adapted from Airflow's system test examples rather than being production-ready code.",1,"The DAG successfully demonstrates the infrastructure-as-code approach for setting up a scalable job scheduling environment in Kubernetes. It provisions computational resources (creates GKE cluster with autopilot), defines job queues (creates ResourceFlavor, ClusterQueue, and LocalQueue using Kueue), and prepares the system for parallel workload processing (demonstrates a Kueue job with parallelism=3). All key requirements from the instruction are fulfilled: infrastructure provisioning, queue definition, and parallel workload capability.",0.6666666666666666
334,Ground Truth (HuggingFace),403,1,"The code correctly uses the InfluxDBOperator from airflow.providers.influxdb.operators.influxdb, which is the idiomatic and native operator for InfluxDB operations. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator directly.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready DAG code.,0,"The instruction requires 'dynamic time range relative to the current execution date' for time-bounded data extraction. However, the query uses 'range(start: -10m, stop: {{ds}})' which has issues: (1) {{ds}} is a date string (YYYY-MM-DD) not a valid InfluxDB timestamp format, (2) the time range is not truly dynamic relative to execution_date in a proper way. A correct implementation should use execution_date or logical_date with proper formatting (e.g., using Jinja templating like '{{execution_date}}' or '{{logical_date}}' with appropriate timestamp formatting), and both start and stop should be relative to the execution context.",0.3333333333333333
335,Ground Truth (HuggingFace),404,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocStartClusterOperator, DataprocStopClusterOperator, DataprocDeleteClusterOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes 'test_run = get_test_run(dag)' and uses a watcher() function. These are not standard Airflow libraries and represent test infrastructure that should not be in production code.",1,"The DAG fulfills the user instruction completely. It demonstrates: (1) creating a Dataproc cluster with specific configuration parameters (CLUSTER_CONFIG with master/worker configs), (2) starting a cluster (start_cluster task), (3) stopping a cluster (stop_cluster task), (4) deleting a cluster (delete_cluster task), and (5) handling existing clusters (use_if_exists=True parameter). The workflow provides a complete lifecycle management example for Dataproc cluster infrastructure as requested.",0.6666666666666666
336,Ground Truth (HuggingFace),405,1,"The code uses Airflow's native decorators and patterns appropriately. It leverages @task, @setup, @teardown, and @task_group decorators which are idiomatic Airflow 3.x features. It demonstrates both decorator-based approaches (@setup, @teardown) and direct relationship methods (as_teardown(), >> operators). No generic PythonOperator wrappers are used where native operators would be more appropriate. The code showcases proper TaskFlow API usage with setup/teardown lifecycle management.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.sdk modules including DAG, setup, task, task_group, teardown) and pendulum for datetime handling. There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run(). The code includes proper documentation comments and follows Airflow best practices.",1,"The code fully satisfies all requirements from the user instruction: (1) Showcases task dependencies using >> operators and various linking methods, (2) Demonstrates lifecycle management with explicit setup and teardown operations using both @setup/@teardown decorators and as_teardown() method, (3) Includes nested task groups (section_1 task group nested within outer setup/teardown context), (4) Uses both decorator-based methods (@setup, @teardown, @task) and direct task relationship methods (>>, as_teardown(), context manager with 'with' statement). The DAG comprehensively demonstrates initialization and finalization steps as requested.",1.0
337,Ground Truth (HuggingFace),406,1,"The code uses BashOperator, which is the appropriate and idiomatic Airflow operator for executing bash commands. It does not wrap bash commands in PythonOperator with hooks or use generic patterns. The code properly imports from airflow.operators.bash and uses the operator directly for all shell command executions.",1,"The code is clean and production-ready, using only standard Airflow libraries (airflow.models.dag, airflow.operators.bash, airflow.operators.empty). There are no imports from internal testing modules or test harness boilerplate. The comments with START/END markers are standard Airflow documentation patterns, not test artifacts. The `if __name__ == '__main__': dag.test()` is a standard pattern for local DAG testing.",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) a multi-stage bash script execution environment with multiple BashOperator tasks, (2) sequencing of computational tasks through task dependencies, (3) complex dependency relationships including parallel tasks (loop creating runme_0, runme_1, runme_2), convergence points (run_this and run_this_last), and multiple paths to the final task. The DAG shows how simple shell commands (echo, sleep) can be orchestrated with varied dependency patterns, exactly as requested.",1.0
338,Ground Truth (HuggingFace),407,1,"The code correctly uses the AzureContainerInstancesOperator from airflow.providers.microsoft.azure.operators.container_instances, which is the idiomatic and native Airflow operator for Azure Container Instances. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator with its native parameters.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG code.,1,"The DAG successfully demonstrates the requested functionality: (1) launches containerized workloads using Azure Container Instances, (2) shows configurable resources (memory_in_gb=4.0, cpu=1.0), (3) includes environment_variables parameter for configuration, and (4) demonstrates storage volume integration in task t3 with the volumes parameter using Azure file shares. The three tasks showcase different scenarios: basic container, custom registry, and volume mounting.",0.6666666666666666
339,Ground Truth (HuggingFace),408,1,"The DAG uses native Airflow operators designed for the specific tasks: GoogleAdsToGcsOperator for data transfer, GoogleAdsListAccountsOperator for listing accounts, GCSCreateBucketOperator for bucket creation, and GCSDeleteBucketOperator for cleanup. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities not appropriate for production code.",1,"The DAG fulfills all requirements from the instruction: (1) creates a GCS bucket, (2) extracts data from Google Ads using a custom query, (3) lists advertising accounts, (4) performs automatic bucket cleanup, and (5) demonstrates inter-service orchestration between Google Ads and GCS with proper task dependencies. The workflow showcases the complete data transfer process as requested.",0.6666666666666666
340,Ground Truth (HuggingFace),409,1,"The DAG uses idiomatic Airflow patterns appropriately. It leverages the native `Param` class from `airflow.models.param` to define parameters with validation rules and types. It uses the `@task` decorator for task definition, which is the modern, idiomatic approach in Airflow 2.x. The code does not wrap functionality in unnecessary PythonOperators where native operators would be more appropriate - in this case, the demonstration of parameter configuration is the primary goal, and using decorators/Params is the correct idiomatic approach.",1,"The code uses only standard Airflow libraries and imports. All imports are from official Airflow modules: `airflow.DAG`, `airflow.decorators.task`, `airflow.exceptions.AirflowSkipException`, `airflow.models.dagrun.DagRun`, `airflow.models.param.Param`, and `airflow.models.taskinstance.TaskInstance`. There are no imports from test utilities, internal testing modules, or test harness boilerplate. The code appears production-ready and clean.",0,"While the DAG demonstrates dynamic parameter configuration with various input types (integer, string, boolean, list, dict) and validation rules (enum, examples, descriptions), the code is incomplete - it cuts off mid-comment. More critically, there are no actual tasks defined that USE these parameters to demonstrate the 'interactive' and 'workflow triggering' aspects mentioned in the instruction. The DAG defines parameters but doesn't show how they flow through a workflow or how they customize execution. A complete implementation should include at least one or more tasks that access and utilize these parameters (e.g., using `{{ params.x }}` in templates or accessing via context) to truly demonstrate 'interactive workflow triggering with customizable input parameters'.",0.6666666666666666
341,Ground Truth (HuggingFace),410,1,"The code uses native Airflow operators and patterns appropriately. It uses BashOperator from airflow.providers.standard.operators.bash and the send_opsgenie_notification notifier from airflow.providers.opsgenie.notifications.opsgenie. The on_failure_callback mechanism is the idiomatic way to handle failure notifications in Airflow, rather than wrapping everything in PythonOperators.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",1,"The code fully satisfies the user instruction. It creates an Airflow DAG that demonstrates error handling (bash task with 'fail' command that will trigger failures) and external alerting (integrated failure notifications to Opsgenie, an incident management platform). The on_failure_callback is implemented at both the DAG and task level, showing comprehensive error handling integration.",0.6666666666666666
342,Baseline (Qwen 2.5 1.5B Instruct),0,0,"The code uses PythonOperator with SnowflakeHook instead of the native SnowflakeOperator. This is the anti-pattern described in the scoring criteria. The load and validation logic should use SnowflakeOperator (for SQL execution) or SnowflakeSqlApiOperator, not wrap hooks in Python callables. For Airflow 3.0.1, the idiomatic approach would be to use airflow.providers.snowflake.operators.snowflake.SnowflakeOperator for both the INSERT and SELECT COUNT operations.",1,The code uses only standard Airflow libraries (airflow.operators.python_operator.PythonOperator and airflow.providers.snowflake.hooks.snowflake.SnowflakeHook). There are no imports from internal testing modules or test harness boilerplate. The code is clean and does not contain hallucinated or leaked test utilities.,1,"The code fulfills the user instruction completely. It creates a data pipeline that: (1) loads 12 sample product records into a Snowflake table via the load_sample_products function, and (2) validates the data load by querying the row count and comparing it to the expected count of 12 in the validate_data_load function. The tasks are properly sequenced with load_task >> validate_task dependency.",0.6666666666666666
343,Baseline (Qwen 2.5 1.5B Instruct),2,0,"The code attempts to use a custom timetable but does so incorrectly. The proper idiomatic way to use a custom timetable in Airflow is to pass it to the DAG constructor via the 'timetable' parameter, not by calling a non-existent 'set_upstream' method with timetable parameters on a task. The correct pattern would be: DAG(..., timetable=AfterWorkdayTimetable(...)). Additionally, uses deprecated 'EmptyOperator' import path instead of 'airflow.operators.empty.EmptyOperator' for Airflow 3.0.0.",0,"The code contains multiple hallucinations: (1) 'airflow.timetables.after_workday.AfterWorkdayTimetable' does not exist in standard Airflow - this is a fabricated import path and class; (2) The method 'task.set_upstream(None, AfterWorkdayTimetable(...))' is not a valid Airflow API - set_upstream does not accept timetable parameters; (3) Missing import for 'datetime' class while using it; (4) The timetable constructor parameters shown (start_time, end_time with datetime objects) don't match any real Airflow timetable pattern.",0,"The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable and includes an EmptyOperator, it fails to properly implement the custom timetable mechanism. The timetable should be passed to the DAG constructor, not applied to individual tasks. The code also uses 'schedule_interval=timedelta(days=1)' which would override any timetable setting, showing fundamental misunderstanding of how custom timetables work in Airflow.",0.0
344,Baseline (Qwen 2.5 1.5B Instruct),3,0,"The code uses PythonOperator with hooks instead of native Campaign Manager operators. Airflow 3.0.0 provides google.marketing_platform.operators.campaign_manager that should be used for interacting with Campaign Manager APIs. The code wraps hook calls in Python functions rather than using dedicated operators like CampaignManagerInsertReportOperator, CampaignManagerRunReportOperator, or CampaignManagerDownloadReportOperator.",0,"Multiple issues: (1) Uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.0. (2) Invents non-existent hook methods like 'get_campaign_metrics()', 'generate_performance_report()', and 'track_conversion()' that don't exist in CampaignManagerHook or BigQueryHook APIs. (3) Missing import for 'timedelta'. (4) Uses invalid 'depends_on' parameter in PythonOperator (should use set_upstream/set_downstream or >> operator). (5) 'start_date' should be a datetime object, not a string.",0,"While the DAG attempts to address the three main requirements (retrieve metrics, generate report, track conversions), it fails to demonstrate proper interaction with Campaign Manager's reporting and conversion APIs. The code doesn't show how to create reports, run them, download results, or handle batch conversion uploads using actual Campaign Manager API patterns. It uses fabricated methods that don't reflect real Campaign Manager API operations.",0.0
345,Baseline (Qwen 2.5 1.5B Instruct),4,0,"The code directly calls Hook methods (GCSHook().get_file() and PostgresHook().run_sql()) outside of any operator context. This is not idiomatic Airflow. The code should use proper operators like GCSToLocalFilesystemOperator or PythonOperator wrapping the hook logic, or better yet, use a transfer operator like GCSToBigQueryOperator or similar. Hooks are meant to be used within operators, not as standalone task definitions.",0,"The code has multiple critical errors: (1) Missing import for 'timedelta' from datetime module, (2) The hook methods are being called at DAG parse time rather than at task execution time, which will cause immediate failures, (3) The code attempts to assign hook method return values directly as tasks, which is fundamentally incorrect in Airflow's execution model. While it doesn't import test utilities, the code is not production-ready and contains structural errors that indicate a misunderstanding of Airflow's architecture.",0,"The instruction asks for transfer from 'cloud object storage' to a 'columnar analytics database'. While GCS qualifies as cloud storage, PostgreSQL is not a columnar analytics database (it's a row-based OLTP database). Columnar databases would include BigQuery, Redshift, Snowflake, or ClickHouse. Additionally, the code doesn't create functional tasks at all - it attempts to execute hook methods at parse time rather than creating proper task objects, so it fails to create a working workflow.",0.0
346,Baseline (Qwen 2.5 1.5B Instruct),7,0,"The code uses PythonOperator with undefined python_callable functions (create_table, insert_data, query_data, cleanup_table) instead of using the native VerticaOperator from airflow.providers.vertica.operators.vertica. Idiomatic Airflow would use VerticaOperator to execute SQL statements directly against Vertica, not wrap database operations in Python functions.",0,"The code references undefined functions (create_table, insert_data, query_data, cleanup_table) that are never imported or defined. Additionally, it uses 'timedelta' without importing it from datetime. The code also uses the deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.0. This is incomplete and would fail at runtime.",1,"The DAG structure correctly addresses all required steps from the instruction: creating an employee records table, populating it with data, querying the data, and cleaning up (dropping) the table. The task dependencies are properly set in the correct sequence (create >> insert >> query >> cleanup).",0.3333333333333333
347,Baseline (Qwen 2.5 1.5B Instruct),8,0,The code uses PythonOperator with AzureDataLakeHook instead of using native Azure operators. Airflow providers offer operators like AzureDataLakeStorageListOperator or similar operators that are designed specifically for listing files in ADLS. The pattern of wrapping hook logic inside PythonOperator callables is not idiomatic when dedicated operators exist for the task.,0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.x. In Airflow 3.0.6, it should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used in default_args but never imported. The AzureDataLakeHook methods like 'list_files' and 'get_file_properties' may not match the actual API of the hook, suggesting potential hallucination of method signatures.",0,"While the code attempts to list files and extract metadata, it has several issues: 1) The Hook is instantiated at the DAG level (outside tasks), which is an anti-pattern in Airflow. 2) The code doesn't specifically filter for Parquet files as requested. 3) The metadata extraction task has incorrect XCom pull syntax - it passes a Jinja template string to a Python function parameter, which won't work as intended. 4) Missing import for 'timedelta'. These issues prevent the DAG from functioning correctly for the stated purpose of performing file inventory and metadata extraction for data validation.",0.0
348,Baseline (Qwen 2.5 1.5B Instruct),9,1,"The code correctly uses the GCSToSFTPOperator from airflow.providers.google.cloud.transfers.gcs_to_sftp, which is the appropriate provider-specific operator for transferring files from Google Cloud Storage to SFTP. It does not wrap logic in PythonOperator with hooks, demonstrating idiomatic Airflow usage.",0,"The GCSToSFTPOperator does not support several parameters used in this code. Specifically, 'operation' (with values 'mv' or 'cp'), 'recursive', 'sftp_host', 'sftp_port', 'sftp_username', and 'sftp_password' are not valid parameters for this operator. The actual GCSToSFTPOperator uses 'destination_path' instead of 'sftp_remote_path', requires 'sftp_conn_id' for connection details (not individual host/port/username/password parameters), and does not support operation modes like 'mv' or 'cp', nor a 'recursive' parameter. The code hallucinates non-existent operator parameters and capabilities.",0,"While the code attempts to address the instruction's requirement for multiple transfer strategies (copy, move, transfer directory), it fails because the GCSToSFTPOperator does not actually support these different modes of operation. The operator only supports basic file transfer from GCS to SFTP. The 'operation' parameter with 'mv'/'cp' values and 'recursive' parameter do not exist in the actual operator, so the code cannot fulfill the business requirement of supporting different file transfer modes including moving files and transferring entire directory contents.",0.3333333333333333
349,Baseline (Qwen 2.5 1.5B Instruct),11,0,"While the code uses native Airflow operators (EmrCreateClusterOperator, KubernetesPodOperator), it demonstrates fundamental misunderstanding of EMR on EKS architecture. The code mixes standard EMR cluster creation with KubernetesPodOperator, but EMR on EKS requires specific operators like EmrContainerOperator or EmrEksCreateClusterOperator (depending on version). The EmrCreateClusterOperator creates a traditional EMR cluster (EC2-based), not an EMR on EKS virtual cluster. The task dependencies also don't make architectural sense - using EmrStepSensor to wait for cluster readiness is incorrect, and then running a separate KubernetesPodOperator doesn't actually submit work to EMR on EKS.",0,"The code references 'SecretKeyRef' without importing it (line 62-63). This appears to be borrowed from Kubernetes Python client but is not properly imported or defined. Additionally, the 'secrets' parameter structure in KubernetesPodOperator is malformed - it should use Airflow's Secret class from airflow.kubernetes.secret. The code also uses non-existent parameters like 'emr_release_version' in EmrCreateClusterOperator (redundant with release_label). The 'executor_config' usage is also incorrect for KubernetesPodOperator - service_account_name should be a direct parameter.",0,"The instruction asks for 'deploying and managing an EMR job on an EKS cluster' which specifically refers to EMR on EKS (a service that runs EMR on Kubernetes). However, the code creates a traditional EC2-based EMR cluster (using EmrCreateClusterOperator) and then separately runs a KubernetesPodOperator, which are two disconnected systems. The code does not: (1) create or reference an EKS cluster, (2) create an EMR virtual cluster on EKS, (3) configure the necessary IAM roles for IRSA (IAM Roles for Service Accounts) required for EMR on EKS, or (4) actually submit a job to EMR on EKS using the appropriate operator (EmrContainerOperator). The workflow fundamentally misunderstands the EMR on EKS architecture.",0.0
350,Baseline (Qwen 2.5 1.5B Instruct),12,0,"The code uses the deprecated 'airflow.operators.bash_operator.BashOperator' import path. In Airflow 2.x (specifically 2.7.2), the idiomatic import should be 'from airflow.operators.bash import BashOperator'. While BashOperator itself is the correct operator choice for bash commands, using the deprecated import path is not idiomatic for modern Airflow.",1,"The code is clean and uses only standard Airflow libraries. There are no imports from internal testing modules, no test harness boilerplate, and no hallucinated functions or methods. The code is production-ready in terms of structure (though it has other issues like deprecated imports and incorrect data types).",1,"The DAG fully adheres to the instruction. It creates a data processing workflow with explicit initialization (setup_environment), main processing (main_data_processing), and cleanup (cleanup_environment) stages using bash commands. The tasks are properly sequenced to show setup → processing → teardown flow, which directly addresses the requirement to 'illustrate how setup tasks can prepare an environment and teardown tasks can perform necessary cleanup operations around the main data processing logic.'",0.6666666666666666
351,Baseline (Qwen 2.5 1.5B Instruct),16,0,"The code fails to use Kafka-specific Airflow providers and operators. The instruction explicitly requests a 'Kafka-based FizzBuzz game that demonstrates message streaming and event processing,' but the implementation uses only a generic PythonOperator with no Kafka integration whatsoever. An idiomatic solution should use `airflow.providers.apache.kafka` operators like KafkaProducerOperator or KafkaConsumerOperator to produce/consume messages, demonstrating actual streaming capabilities.",1,"The code is clean and uses only standard Airflow libraries. There are no imports from internal testing modules or test harness boilerplate. While the code uses the deprecated `PythonOperator` import path (should be from airflow.operators.python), this is a version compatibility issue rather than hallucination.",0,"The code completely fails to implement the requested Kafka-based system. The instruction specifically asks for: (1) Kafka message streaming, (2) event processing, (3) a system that generates numbers and checks divisibility, and (4) triggers specific actions based on characteristics. While the code does implement basic FizzBuzz logic with divisibility checks, it lacks all Kafka components - no message production, no streaming, no event processing, and no demonstration of Kafka's pub-sub architecture. This is essentially a standalone FizzBuzz script that doesn't fulfill the core requirement of being 'Kafka-based.'",0.3333333333333333
352,Baseline (Qwen 2.5 1.5B Instruct),17,0,"The code uses deprecated patterns for Airflow 3.0.1. It imports 'PythonOperator' from 'airflow.operators.python_operator' (deprecated path) instead of 'airflow.operators.python'. More critically, it does not use Airflow's native setup/teardown functionality introduced in Airflow 2.7+ via the '.as_setup()' and '.as_teardown()' methods or the 'setup()' and 'teardown()' context managers. Instead, it manually creates tasks and dependencies to simulate setup/teardown behavior, which is not idiomatic for demonstrating setup and teardown in modern Airflow.",1,"The code does not import any internal testing modules or include test harness boilerplate. It uses only standard Airflow libraries (albeit with deprecated import paths), and contains no hallucinated functions or test utilities.",0,"The instruction explicitly asks to 'demonstrate setup and teardown tasks in Airflow, showing how preparatory and cleanup operations can be automatically managed'. While the code creates tasks named 'setup' and 'teardown', it does not use Airflow's actual setup/teardown mechanisms that provide automatic management (e.g., ensuring teardown runs even on failure). Additionally, the dependencies are incorrectly configured: 'setup_task.set_upstream()' should be 'set_downstream()' to make setup run before the tasks. The current code would make task_1, task_2, and task_3 run before setup_task, which is backwards. The teardown is also not properly connected to setup_task, and there's no demonstration of automatic cleanup guarantees.",0.3333333333333333
353,Baseline (Qwen 2.5 1.5B Instruct),19,0,"The code uses several non-existent or incorrect operators. 'PostgresCreateDatabaseOperator' and 'PostgresExecuteQueryOperator' do not exist in 'airflow.providers.google.cloud.operators.postgres' - PostgreSQL operators are in 'airflow.providers.postgres.operators.postgres' (e.g., PostgresOperator). 'GoogleSheetsUploadOperator' does not exist; the correct operator is 'GoogleSheetsCreateSpreadsheetOperator' or using 'GoogleSheetsHook' with appropriate operators. Additionally, GKE (Google Kubernetes Engine) cluster operators are being used instead of Compute Engine instance operators (ComputeEngineInsertInstanceOperator), which doesn't match the instruction to create a Compute Engine instance.",0,"The code imports and uses operators that do not exist in the Airflow provider packages: 'PostgresCreateDatabaseOperator', 'PostgresExecuteQueryOperator' from 'airflow.providers.google.cloud.operators.postgres', and 'GoogleSheetsUploadOperator' from 'airflow.providers.google.cloud.operators.sheets'. These are hallucinated/fabricated operators that are not part of the standard Airflow libraries for version 2.9.3.",0,"The instruction specifically requests creating a 'Compute Engine instance', but the code creates a GKE (Google Kubernetes Engine) cluster instead, which is a different service. While the DAG does attempt to configure PostgreSQL and transfer data to Google Sheets, the infrastructure provisioning step is incorrect. Additionally, the data transfer to Google Sheets uses hardcoded values rather than actually querying and transferring data from the PostgreSQL database, which doesn't demonstrate a true data migration workflow.",0.0
354,Baseline (Qwen 2.5 1.5B Instruct),20,0,"The code uses operators that do not exist in the official Airflow Google Cloud provider package. The actual operators for Airflow 3.0.0 are named differently: `ComputeEngineInsertInstanceTemplateOperator`, `ComputeEngineCopyInstanceTemplateOperator`, `ComputeEngineInsertInstanceGroupManagerOperator`, and `ComputeEngineInstanceGroupUpdateManagerTemplateOperator`. The operator names used in the code (e.g., `GCPComputeInstanceTemplateCreateOperator`, `GCPComputeInstanceTemplateUpdateOperator`) appear to be fabricated or from an outdated/non-existent version. Additionally, there is no `GCPComputeInstanceTemplateUpdateOperator` in Airflow - instance templates are immutable and cannot be updated in place.",0,"The code imports operators that do not exist in the standard Airflow library (`GCPComputeInstanceTemplateCreateOperator`, `GCPComputeInstanceTemplateCopyOperator`, `GCPComputeInstanceTemplateUpdateOperator`, `GCPComputeInstanceGroupManagerCreateOperator`, `GCPComputeInstanceGroupManagerUpdateOperator`). These are hallucinated operator names. Additionally, the code references `timedelta` without importing it from datetime module, which would cause a runtime error.",0,"While the code attempts to address the instruction by creating a workflow for managing instance templates and instance group managers, it fails to properly implement the required functionality due to using non-existent operators. The instruction asks to 'create, copy, and update instance templates with different configurations' - however, the update step is fundamentally flawed as GCP instance templates are immutable and cannot be updated (only replaced). The code structure shows intent to follow the instruction but the implementation is incorrect and would not execute successfully.",0.0
355,Baseline (Qwen 2.5 1.5B Instruct),21,0,"The code imports GCSToGCSOperator but never uses it. There are no actual tasks defined in the DAG, making it impossible to evaluate if idiomatic operators are properly used. The DAG only contains the skeleton structure without any operators instantiated.",1,The code uses only standard Airflow libraries (airflow.providers.google.cloud.transfers.gcs_to_gcs_operator) and does not import any internal testing modules or include test harness boilerplate. The imports are from legitimate Airflow provider packages.,0,"The instruction explicitly asks to 'demonstrate GCS transfer job creation, file upload, and bucket management.' The provided code only defines a DAG skeleton with no tasks. It does not implement: (1) GCS transfer job creation, (2) file upload functionality, or (3) bucket management operations. The DAG is incomplete and non-functional for the stated purpose.",0.3333333333333333
356,Baseline (Qwen 2.5 1.5B Instruct),22,0,"The code uses deprecated 'PythonOperator' from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. More critically, it uses 'dag.create_task_group()' which is not a valid Airflow API. The correct idiomatic approach in Airflow 2.8.4 is to use 'from airflow.utils.task_group import TaskGroup' and instantiate it with 'TaskGroup(group_id=...)' context manager.",0,The code invents a non-existent API method 'dag.create_task_group()'. This method does not exist in Airflow. The correct approach is to use 'TaskGroup' from 'airflow.utils.task_group'. This represents a hallucination of functionality that is not part of the standard Airflow library.,0,"The instruction explicitly asks for 'nested task groups' to demonstrate complex workflow organization. The provided code attempts to create two separate task groups but: (1) uses incorrect syntax that won't execute, (2) does not demonstrate nesting (groups within groups), (3) does not define any sequential processing or dependencies between tasks or groups. The code fails to deliver the core requirements of demonstrating task grouping, nested groups, and sequential processing.",0.0
357,Baseline (Qwen 2.5 1.5B Instruct),23,0,"The code uses PythonOperator with boto3 clients directly instead of using Airflow's native AWS providers. For Airflow 3.0.0, it should use S3Hook with appropriate operators (e.g., S3ToLocalFilesystemOperator or S3Hook) from airflow.providers.amazon.aws package. The code wraps AWS SDK calls in Python functions rather than leveraging idiomatic Airflow operators designed for AWS services.",0,"The code contains multiple hallucinated/non-existent QuickSight API methods that don't exist in the boto3 QuickSight client: 'add_table_to_data_set' and 'upload_csv_to_data_set' are not real boto3 methods. Additionally, the create_data_source API structure is incorrect - QuickSight doesn't support 'CSV' as a Type, and the PhysicalTableMap structure shown is malformed. The code also uses deprecated 'airflow.operators.python_operator' import path instead of 'airflow.operators.python' for Airflow 3.0.0.",0,"While the code attempts to create a pipeline from S3 to QuickSight, it fails to properly implement the QuickSight setup. The instruction asks to 'programmatically set up a data source, dataset, and ingestion process' but the implementation uses non-existent API methods and incorrect API structures. A proper implementation would need to use S3 as the data source type for QuickSight, create a proper manifest file, and use SPICE ingestion APIs. The code doesn't demonstrate a working ingestion process.",0.0
358,Baseline (Qwen 2.5 1.5B Instruct),24,1,"The code uses native Google Cloud DLP operators from airflow.providers.google.cloud.operators.dlp (CreateDlpJobOperator, InspectContentOperator, DeleteDlpJobOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for interacting with Google Cloud DLP.",0,"The code imports 'InspectContentOperator' which does not exist in the standard Airflow Google Cloud DLP provider package. The actual operators available are CloudDLPCreateDLPJobOperator, CloudDLPDeleteDLPJobOperator, CloudDLPGetDLPJobOperator, etc. The InspectContentOperator is a hallucinated/non-existent operator. Additionally, the usage pattern is incorrect - CreateDlpJobOperator already performs the inspection job, and there's no separate 'inspect_content_task' needed that takes the job output.",0,"While the DAG attempts to demonstrate the lifecycle (create, inspect, delete), it fails to properly implement the workflow due to incorrect operator usage. The 'inspect_content' task using a non-existent operator and incorrect parameter (dlp_job_name=create_job_task.output) means the actual business logic of creating, running, and deleting a DLP job is not correctly fulfilled. The task dependencies are also not explicitly set, though they may be implied through XCom references. A proper implementation would create a DLP job, optionally retrieve/monitor its status, and then delete it.",0.3333333333333333
359,Baseline (Qwen 2.5 1.5B Instruct),25,0,"While the code attempts to use SageMaker-specific operators (SageMakerTrainingOperator, SageMakerModelOperator, SageMakerEndpointOperator), the implementation is incorrect. These operators require proper configuration dictionaries (e.g., 'config' parameter) rather than direct parameters like 'instance_count', 'instance_type', 'image_uri', etc. The correct idiomatic approach would use properly structured configuration dictionaries as per the official Airflow SageMaker provider documentation.",0,"The code contains multiple hallucinated/non-existent operators and classes: 'SageMakerRoleOperator' and 'SageMakerMonitoringTask' do not exist in the Airflow AWS provider package. Additionally, 'timedelta' is used but never imported, and 'os' module is referenced without import. The SageMaker operators also use incorrect parameter signatures that don't match the actual Airflow provider implementation.",0,"The code attempts to address the instruction's requirements (provisioning resources, training, model creation, endpoint deployment, and monitoring) but fails in execution. The 'provisioning AWS resources' step uses a non-existent operator rather than properly handling IAM role setup. There is no robust error handling implemented (no on_failure_callback, no sensor checks for job completion). Resource management is not demonstrated beyond basic task sequencing. The workflow structure is present but the implementation is fundamentally broken due to incorrect operator usage and missing imports.",0.0
360,Baseline (Qwen 2.5 1.5B Instruct),26,1,"The code uses native Airflow providers and operators (S3CreateBucketOperator, S3UploadFileOperator, BashOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic approach for Airflow.",0,"The code has several issues: (1) Missing import for 'timedelta' from datetime module, (2) Missing import for 'BashOperator' from airflow.operators.bash, (3) Uses 'airflow.utils.dates.datetime' which is non-standard - should use 'from datetime import datetime', (4) S3UploadFileOperator parameters appear incorrect - 's3_key' and 's3_object_key' are misused (should be 'filename' for local file and 'key' for S3 path). These errors make the code non-functional and not production-ready.",0,"The instruction requires: (1) Generate a temporary file, (2) Transfer it to S3, (3) Clean up temporary resources. The code is missing the critical first step - there is no task to actually CREATE/GENERATE the temporary file. The upload task references a file path but never creates it. The cleanup task attempts to remove '/tmp/temp_file.txt' but this file is never generated in the workflow. Only 2 out of 3 required steps are present.",0.3333333333333333
361,Baseline (Qwen 2.5 1.5B Instruct),27,0,"The code fails to use Campaign Manager 360-specific operators and hooks. It wraps all logic in PythonOperators instead of using native Airflow providers. For Campaign Manager 360 (DoubleClick), Airflow provides 'airflow.providers.google.marketing_platform.operators.campaign_manager' and related hooks. The code also incorrectly uses BigQueryHook to interact with Campaign Manager 360 API, which is not the appropriate tool. Additionally, it uses the deprecated 'PythonOperator' import path instead of 'airflow.operators.python.PythonOperator'.",0,"The code contains several issues: (1) Imports 'ServiceAccountCredentials' from 'google.oauth2.credentials' which is incorrect (should be from 'google.oauth2.service_account'), (2) Uses BigQueryHook incorrectly by passing credentials directly to the constructor which doesn't accept that parameter, (3) References undefined 'hook' variable in 'record_conversion_events' function, (4) The overall approach of querying BigQuery tables to access Campaign Manager 360 data is conceptually flawed - Campaign Manager 360 has its own API that should be accessed via proper hooks/operators, not through BigQuery queries to fictional tables.",0,"The instruction specifically requests integration with 'Campaign Manager 360 APIs' to retrieve digital marketing performance metrics and log conversion events. The code fails to implement this: (1) It doesn't use Campaign Manager 360 API at all, instead querying BigQuery tables, (2) The conversion tracking implementation is a simple SQL UPDATE rather than using Campaign Manager's conversion API, (3) Report generation is a basic CSV write rather than utilizing Campaign Manager's reporting capabilities, (4) There's no dynamic pulling of campaign data from Campaign Manager 360 as requested. The code misses the core requirement of integrating with Campaign Manager 360 APIs.",0.0
362,Baseline (Qwen 2.5 1.5B Instruct),28,0,"The code uses PythonOperator with subprocess calls to execute Pig scripts, which is not idiomatic. Airflow provides airflow.providers.apache.pig.operators.pig.PigOperator specifically for executing Pig scripts. For listing directory contents, BashOperator would be more appropriate than wrapping shell commands in PythonOperator. The code wraps basic operations in Python callables instead of using native Airflow operators.",0,"The code has critical issues: (1) Missing imports for 'datetime' and 'timedelta' which are referenced but not imported, (2) Functions 'execute_pig_script_function' and 'list_directory_contents_function' are defined AFTER they are referenced in the PythonOperator definitions, causing a NameError, (3) The 'os' module is imported inside a function when it should be at the top. While there are no test harness imports, these are fundamental Python errors that make the code non-functional.",0,"The instruction asks to 'execute Pig script AND list directory contents as a simple example of system interaction'. The code has the dependency backwards - it sets execute_pig_script.set_upstream(list_directory_contents), meaning the Pig script runs AFTER listing directories. The logical flow should be: list directories first (as the simple system interaction example), then execute the Pig script. The dependency should be list_directory_contents >> execute_pig_script or execute_pig_script.set_downstream(list_directory_contents).",0.0
363,Baseline (Qwen 2.5 1.5B Instruct),29,0,"The code uses deprecated operators for Airflow 3.0.6. It imports 'PythonOperator' from 'airflow.operators.python_operator' and 'BashOperator' from 'airflow.operators.bash_operator', which are old import paths. In Airflow 2.0+, these should be imported from 'airflow.operators.python' and 'airflow.operators.bash' respectively. Additionally, it uses 'airflow.utils.dates.datetime' instead of the standard 'datetime' module. While the code uses appropriate operators (Bash and Python), the import paths are not idiomatic for the specified Airflow version.",0,"The code has several critical issues: (1) 'timedelta' is used but never imported, (2) 'os' module is used but never imported, (3) The function 'execute_environment_specific_function' is referenced before it's defined, causing a NameError, (4) Setting environment variables at the DAG module level ('os.environ['ENVIRONMENT'] = 'production'') is not production-ready and would affect the entire Airflow process. These are significant code quality issues that would prevent the DAG from running.",1,"The DAG successfully implements all requested features: (1) prints context information via BashOperator, (2) logs SQL queries (simulated with echo), (3) generates sleep-based parallel tasks (3 tasks with sleep commands), (4) executes environment-specific Python functions, (5) demonstrates task chaining with '>>' operator, (6) shows dependency management with sequential and parallel task execution, and (7) includes runtime flexibility through environment variable checking. Despite implementation flaws, it fulfills the business logic requirements.",0.3333333333333333
364,Baseline (Qwen 2.5 1.5B Instruct),30,0,"The code wraps all logic in PythonOperator with Hooks (CalendarHook, BigQueryHook) instead of using native Operators. Idiomatic Airflow would use operators like BigQueryCreateEmptyDatasetOperator, BigQueryInsertJobOperator, or custom operators from the Google provider package. The pattern of defining functions that use hooks and wrapping them in PythonOperator is explicitly the anti-pattern described in the scoring criteria.",0,"The code contains multiple issues: (1) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.1. (2) References non-existent or incorrectly used Hook methods like 'calendar_hook.authenticate()', 'calendar_hook.list_events()', 'calendar_hook.delete_events()', and 'bigquery_hook.create_dataset()' which don't match actual Airflow provider APIs. (3) Functions are defined after they are referenced in the DAG, causing a NameError. (4) The CalendarHook and its methods appear to be fabricated or misunderstood - the actual Google Calendar provider has different interfaces.",1,"The code does attempt to fulfill the instruction requirements: it creates a DAG with explicit setup, execution, and teardown tasks for Google Calendar event synchronization. It demonstrates connection management (hooks), data transfer (calendar to BigQuery), and infrastructure cleanup (deletion). The structure follows the requested pattern of setup -> execution -> teardown, even though the implementation has technical issues.",0.3333333333333333
365,Baseline (Qwen 2.5 1.5B Instruct),31,0,"The code misuses GCSToGCSOperator for bucket creation and file uploads. GCSToGCSOperator is designed for copying objects between GCS buckets, not for creating buckets or uploading from local paths. The correct approach would be to use GCSCreateBucketOperator for bucket creation and LocalFilesystemToGCSOperator (or similar) for uploading local files. Additionally, ImageAnnotateOperator is used, but the parameters and usage appear incorrect (e.g., create_bucket_task.output is not a valid pattern, and source_path pointing to a local path in GCSToGCSOperator is wrong).",0,"The code contains several issues that indicate hallucination or incorrect understanding: (1) Missing import for 'timedelta' from datetime module, (2) Incorrect usage of operators that don't match their actual API (GCSToGCSOperator doesn't accept source_path for local files, doesn't create buckets, and doesn't have an 'output' attribute), (3) The ImageAnnotateOperator parameters appear fabricated - the actual operator may have different parameter names and requirements. The code would fail at runtime.",0,"While the code attempts to address the instruction's requirements (bucket creation, image upload, and analysis), it fails to implement them correctly. The instruction asks for 'a complete image processing workflow' with 'bucket creation to image analysis', but the implementation uses wrong operators for bucket creation and file uploads. The workflow structure is present but the actual functionality would not work as intended. A proper implementation would use GCSCreateBucketOperator, appropriate upload operators, and correctly configured Vision API operators.",0.0
366,Baseline (Qwen 2.5 1.5B Instruct),32,0,"The code fails to use idiomatic Airflow patterns. The load_csv_task directly calls TrinoHook().run_query() outside of an operator context, which is not how Airflow tasks should be defined. The proper approach would be to use a PythonOperator with a callable function that uses the TrinoHook, or better yet, use TrinoOperator to execute the SQL query. The current implementation treats the hook method call as a task definition, which is incorrect.",0,"The code contains several issues: (1) Missing import for 'timedelta' from datetime module, (2) 'start_date' should be a datetime object, not a string, (3) GCSToLocalOperator parameters appear incorrect - 'source_bucket' is not a valid parameter (should use 'bucket' and 'object_name'), (4) The LOAD DATA INPATH syntax is not valid Trino SQL (this is Hive syntax), and (5) The load_csv_task is not properly defined as an Airflow task/operator. These represent fundamental errors that would prevent the DAG from running.",0,"While the code attempts to extract CSV files from GCS and load them into Trino, it fails to properly implement either step. The extraction task uses incorrect parameters, and the loading task uses invalid Trino SQL syntax (LOAD DATA INPATH is Hive, not Trino). Trino doesn't support direct file loading like Hive; it would require reading the CSV data and inserting it via INSERT statements or using a proper connector. The pipeline would not successfully accomplish the stated goal of extracting and loading data.",0.0
367,Baseline (Qwen 2.5 1.5B Instruct),33,0,"The code attempts to use SparkSubmitOperator but misuses it entirely. For database introspection on Alibaba AnalyticDB, the code should use appropriate database operators (e.g., JdbcOperator, or a specific Alibaba AnalyticDB provider if available). Instead, it wraps SparkSubmitOperator inside a Python function called by PythonOperator pattern (though not explicitly instantiated), which is non-idiomatic. The SparkSubmitOperator is also incorrectly configured - it's meant to submit Spark applications, not execute SQL queries directly. For SQL-based introspection, native SQL operators should be used.",0,"The code contains several issues: (1) Uses deprecated 'PythonOperator' import (should be 'from airflow.operators.python import PythonOperator' in Airflow 3.0.0), (2) The SparkSubmitOperator configuration is nonsensical with invalid parameters like 'application=""spark-submit""' and malformed args, (3) References '/path/to/spark-submit.py' as a placeholder that would fail in production, (4) The execute_sql_query function creates operators but doesn't properly instantiate them as tasks - it returns an operator object that's never used, and (5) The loop at the end creates tasks but they're not assigned to variables or properly integrated into the DAG.",0,"The instruction asks for systematic database introspection using sequential SQL queries to discover schemas and table structures in Alibaba AnalyticDB Spark cluster. While the code attempts to define relevant SQL queries (information_schema queries), it fails to: (1) Actually execute them properly - the tasks are created but not linked or executed, (2) Catalog or store the results anywhere for visibility, (3) Create a sequential workflow (no task dependencies defined), and (4) Provide any mechanism to view or output the discovered data schemas and table structures. The code is fundamentally broken and would not accomplish the stated goal.",0.0
368,Baseline (Qwen 2.5 1.5B Instruct),35,0,"The code uses PythonOperator with CohereHook instead of using a native Cohere operator. While Airflow providers may not always have operators for every service method, the pattern here wraps the hook in a PythonOperator, which is the anti-pattern described in the scoring criteria. The idiomatic approach would be to use a dedicated operator if available, or at minimum structure the code more appropriately for the task.",0,"The code has several issues: 1) Uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.6, 2) Missing 'from datetime import timedelta' import, 3) 'start_date' should be a datetime object not a string, 4) The CohereHook constructor parameter 'cohere_api_key' is hardcoded instead of using a connection, 5) The output_results_task references 'embed_text_task.output' which is not a valid Airflow pattern. These are not hallucinations of test code, but represent broken/non-functional code.",0,"The instruction asks to 'process multiple text inputs and generate their corresponding embeddings'. While the code defines multiple input texts, it fails to actually process them. The embed_text function only processes a single text, and the workflow doesn't iterate over the input_texts list. The XCom pull pattern is incorrect, and the output_results_task tries to access a non-existent 'output' attribute. The DAG does not successfully demonstrate processing multiple texts and generating embeddings for each.",0.0
369,Baseline (Qwen 2.5 1.5B Instruct),36,0,"While the code uses Airflow providers (GCSToGCSOperator, LocalToGCSOperator, GCSToLocalOperator), it misuses them entirely. GCSToGCSOperator is designed for copying objects between buckets, not for creating or deleting buckets. For bucket management operations (create/delete buckets), the code should use GCSCreateBucketOperator and GCSDeleteBucketOperator from airflow.providers.google.cloud.operators.gcs. The tasks 'create_source_bucket', 'create_destination_bucket', and 'delete_source_bucket' are all using GCSToGCSOperator incorrectly, which would not achieve the intended bucket management operations.",1,"The code uses only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules, no test harness boilerplate, and no hallucinated functions or classes. All imports are legitimate Airflow 2.7.2 components.",0,"The instruction asks for a DAG that 'demonstrates file transfer and bucket management techniques' and 'showcases different synchronization strategies between source and destination buckets.' While the code attempts to address these requirements, it fails to implement them correctly. The bucket creation/deletion tasks use the wrong operators and wouldn't work. Additionally, there's no actual synchronization happening between buckets - the workflow uploads to source, downloads from destination (which would be empty), but never copies from source to destination. The task dependencies are also not defined, so the execution order is undefined. A proper implementation would need: (1) correct bucket operators, (2) actual GCS-to-GCS sync operations, (3) proper task dependencies, and (4) demonstration of different sync strategies (e.g., overwrite vs. update, prefix-based sync, wildcard patterns).",0.3333333333333333
370,Baseline (Qwen 2.5 1.5B Instruct),37,0,"The DAG relies entirely on generic PythonOperator with print statements instead of using specific Airflow providers and operators designed for data extraction, transformation, and loading. For a BI workbook data refresh scenario, it should use operators like SnowflakeOperator, BigQueryOperator, TableauRefreshWorkbookOperator, or similar provider-specific operators. The code wraps all logic in PythonOperator callables rather than leveraging native Airflow operators for the actual data operations.",0,"The code has a critical error: it imports 'datetime' from 'airflow.utils.dates' but then uses 'timedelta' without importing it. The correct import should be 'from datetime import datetime, timedelta'. Additionally, it uses the deprecated 'PythonOperator' import path ('airflow.operators.python_operator') instead of the modern 'airflow.operators.python.PythonOperator'. While not test harness boilerplate, these issues prevent the code from being production-ready.",0,"The instruction specifically asks for 'data extract refresh processes for a business intelligence workbook' with 'trigger and monitor data updates across different execution modes' and 'different refresh strategies'. The provided code only implements a basic linear ETL pipeline with print statements and no actual BI workbook integration, no different execution modes (sync/async), no real job status tracking (just prints task IDs), and no different refresh strategies (full/incremental). The monitor_job_status_task is also incorrectly placed in the dependency chain (it runs BEFORE the tasks it's supposed to monitor). The code does not fulfill the specific business requirements for BI workbook refresh scenarios.",0.0
371,Baseline (Qwen 2.5 1.5B Instruct),38,0,"The code uses deprecated import path 'airflow.operators.latest_only_operator.LatestOnlyOperator'. In Airflow 3.0.0, this should be imported from 'airflow.operators.latest_only' module. Additionally, BashOperator is used but never imported, which would cause a runtime error.",0,"The code references 'BashOperator' without importing it, which indicates incomplete or hallucinated code. While not importing test utilities, the code is not production-ready due to missing imports that would cause immediate failure.",1,"The DAG correctly implements the requested functionality: it uses LatestOnlyOperator to control conditional task execution based on the most recent scheduling window, and sets up a downstream task that will only execute for the latest scheduled run, preventing redundant processing of older tasks. The business logic and structure align with the instruction despite implementation issues.",0.3333333333333333
372,Baseline (Qwen 2.5 1.5B Instruct),39,0,"The code uses GCSToGCSOperator for all tasks, including bucket creation and deletion, which is incorrect. GCSToGCSOperator is designed for copying objects between GCS buckets, not for bucket management. The correct idiomatic approach would be to use GCSCreateBucketOperator for bucket creation and GCSDeleteBucketOperator for bucket deletion. Additionally, the instruction requires cross-cloud transfer (Azure to GCS), but no Azure-to-GCS transfer operator is used. The correct operator would be AzureBlobStorageToGCSOperator from airflow.providers.google.cloud.transfers.azure_blob_to_gcs.",0,"The code imports BlobToBlobOperator from airflow.providers.microsoft.azure.transfers.blob_to_blob, but this operator is never used in the DAG. Additionally, the datetime import is missing (datetime is used in default_args but not imported). The misuse of GCSToGCSOperator for bucket creation and deletion with non-existent parameters (like destination_bucket_name=None for deletion) suggests hallucinated functionality that doesn't exist in the actual operator.",0,"The instruction explicitly requires 'cross-cloud file transfer' between Azure and Google Cloud storage systems. However, the implemented DAG only uses GCS-to-GCS operators and does not perform any Azure-to-GCS transfer. While it attempts to address bucket creation, file transfer, and cleanup, it fails to implement the core requirement of moving data FROM Azure TO Google Cloud. The BlobToBlobOperator is imported but never used, and no actual Azure source is configured.",0.0
373,Baseline (Qwen 2.5 1.5B Instruct),40,0,"The code uses PythonOperator with hooks (S3Hook and QuickSightHook) instead of using native operators. While hooks are used, they are wrapped in PythonOperator callables rather than using dedicated operators like S3ToQuickSightOperator or similar transfer operators. Additionally, the code uses deprecated operators (PythonOperator and DummyOperator should be imported from airflow.operators.python and airflow.operators.empty respectively in Airflow 2.8.4).",0,"The code contains multiple hallucinations: 1) QuickSightHook does not have an 'upload_csv_to_quick_sight' method - this is a fabricated API. 2) The S3Hook.download_file method signature is incorrect (should use key parameter differently). 3) The functions are defined AFTER they are referenced in the PythonOperator tasks, which would cause a NameError. 4) The 'dag.cli()' at the end is not standard production code. 5) QuickSight doesn't work with direct CSV uploads in this manner - it requires data source and dataset creation through its API.",0,"The instruction explicitly asks to 'programmatically set up a data source, dataset, and ingestion process' for QuickSight. The code fails to demonstrate: 1) Creating a QuickSight data source, 2) Creating a QuickSight dataset with proper schema, 3) Setting up an ingestion process. It only shows a simplistic (and non-functional) file download and upload pattern. The instruction requires demonstrating the full setup workflow for QuickSight integration, which is completely missing.",0.0
374,Baseline (Qwen 2.5 1.5B Instruct),41,0,"The code fails to use idiomatic Airflow patterns. It wraps BigQueryHook and GoogleCloudStorageHook inside PythonOperators instead of using native operators like BigQueryInsertJobOperator, BigQueryToGCSOperator, or GCSToBigQueryOperator. Most critically, it completely ignores the Google Campaign Manager (DCM/DV360) specific providers that Airflow offers (airflow.providers.google.marketing_platform.operators.campaign_manager). The instruction explicitly asks for Google Campaign Manager reporting, but the code uses generic BigQuery queries instead of Campaign Manager API operators to create, run, and process reports.",1,"The code uses only standard Airflow libraries and providers. There are no imports from test modules, no test harness boilerplate, and no internal testing utilities. All imports are from legitimate Airflow provider packages.",0,"The instruction specifically asks to 'Build an automated reporting workflow for Google Campaign Manager that creates, runs, and processes marketing performance reports.' The code fails to interact with Google Campaign Manager API at all. It should use Campaign Manager operators to create a report definition, run the report, wait for completion, and download results. Instead, it queries BigQuery directly (which may not even contain Campaign Manager data) and performs generic data operations. The workflow misses the core requirement of interfacing with Google Campaign Manager's reporting API. Additionally, data passing between tasks is broken - extract_campaign_metrics returns a DataFrame but transform_data and store_report_files have no way to receive it (XCom is not properly utilized).",0.3333333333333333
375,Baseline (Qwen 2.5 1.5B Instruct),42,0,"The code fails to use idiomatic Airflow patterns for Looker PDT operations. It incorrectly uses SparkSubmitOperator for Looker PDT builds, when Airflow provides specialized Looker operators in the apache-airflow-providers-apache-looker package. The proper approach would be to use LookerStartPdtBuildOperator and LookerCheckPdtBuildSensor (or similar operators) designed specifically for starting and monitoring Looker PDT builds. Using SparkSubmitOperator with custom Spark scripts is a non-idiomatic workaround that defeats the purpose of using provider-specific operators.",1,"The code uses standard Airflow libraries (airflow.providers.apache.looker.hooks.looker.LookerHook, airflow.providers.apache.spark.operators.spark_submit.SparkSubmitOperator) and does not import any internal testing modules or include test harness boilerplate. While the operator choice is incorrect for the use case, the imports themselves are legitimate Airflow components.",0,"The code fails to properly implement the requested workflow for starting and checking Looker PDT builds using specialized Airflow operators. While it attempts to demonstrate task dependencies, it does not use the appropriate Looker-specific operators for PDT operations. Instead, it incorrectly uses SparkSubmitOperator, which is designed for Spark job submission, not Looker PDT management. The instruction specifically requested 'specialized Airflow operators for cloud-based data transformation' in the context of Looker PDTs, which this implementation does not deliver. Additionally, the asynchronous job monitoring aspect is not properly implemented using Airflow's sensor pattern.",0.3333333333333333
376,Baseline (Qwen 2.5 1.5B Instruct),43,0,"The code uses PythonOperator with ServiceBusHook for all operations instead of using native Azure Service Bus operators. While Airflow's Azure provider does include operators like ASBReceiveSubscriptionMessageOperator and ASBSendMessageOperator, this implementation wraps all logic in Python callables. The pattern of 'def function() -> hook = ServiceBusHook() -> PythonOperator(python_callable=function)' is exactly the anti-pattern described in the scoring criteria. Idiomatic Airflow would use dedicated operators from airflow.providers.microsoft.azure.operators.service_bus where available.",0,"The code hallucinates methods on ServiceBusHook that don't exist in the actual Airflow Azure provider. Methods like 'create_queue()', 'create_topic()', 'send_message()', and 'subscribe_to_topic()' are not part of the standard ServiceBusHook API. The actual hook provides different methods for interacting with Azure Service Bus. Additionally, 'from airflow.operators.python_operator import PythonOperator' is deprecated in Airflow 3.0.6; it should be 'from airflow.operators.python import PythonOperator'.",1,"The DAG attempts to fulfill the instruction by demonstrating the complete lifecycle: queue creation, topic creation, message transmission (sending), and subscription management. The workflow shows a step-by-step progression through these enterprise messaging operations as requested. Despite implementation issues, the business logic structure aligns with the user's requirements for illustrating Azure Service Bus messaging operations.",0.3333333333333333
377,Baseline (Qwen 2.5 1.5B Instruct),44,0,"The code uses 'FirestoreToGCSOperator' which does not exist in the standard Airflow providers. The actual operator for exporting Firestore is 'CloudFirestoreExportDatabaseOperator' from airflow.providers.google.cloud.operators.cloud_firestore. Additionally, the code is missing the BigQuery external table creation operator entirely. A proper implementation would use CloudFirestoreExportDatabaseOperator for Firestore export and BigQueryCreateExternalTableOperator for creating the external table.",0,"The code imports 'FirestoreToGCSOperator' from 'airflow.providers.google.cloud.transfers.firestore_to_gcs', which is a non-existent module and operator. This is a hallucinated import that does not exist in the Airflow codebase. Additionally, the operator accepts parameters like 'query' with SQL syntax ('SELECT * FROM your_collection') which is not how Firestore queries work - Firestore is a NoSQL database and doesn't use SQL queries.",0,"The instruction explicitly requires three components: (1) export Firestore collection to GCS, (2) create an external BigQuery table from the exported data, and (3) demonstrate cross-project data transfer. The provided code only attempts to address the first component (and does so incorrectly), completely missing the BigQuery external table creation step and any cross-project configuration. The DAG is incomplete and does not fulfill the business requirements.",0.0
378,Baseline (Qwen 2.5 1.5B Instruct),45,0,"The code wraps Pinecone operations inside PythonOperator functions instead of using native Pinecone operators. While it imports PineconeHook, it uses it within a python_callable pattern rather than leveraging dedicated Pinecone operators from airflow.providers.pinecone. This is the anti-pattern of wrapping Hook logic in PythonOperator instead of using purpose-built Operators.",0,"The code contains several issues: (1) Uses deprecated 'airflow.operators.python_operator' import instead of 'airflow.operators.python' for Airflow 3.0.1, (2) References undefined 'pinecone' module in 'pinecone.Index(index_name)' without importing it, (3) The insert_into_pinecone function signature doesn't receive 'df' parameter from the previous task (no XCom usage), making the task dependency non-functional, (4) The upsert API call appears malformed with incorrect parameter structure.",0,"The instruction requires 'flexible configuration of vector data insertion with support for metadata and custom namespaces.' The code has hardcoded values ('your_index_name', 'your_namespace', 'path_to_your_vector_data.csv') with no parameterization or DAG params. There's no mechanism for flexible configuration, no support for different namespaces per run, and the namespace variable is defined but never actually used in the upsert call. The workflow also doesn't properly pass data between tasks.",0.0
379,Baseline (Qwen 2.5 1.5B Instruct),46,1,"The code correctly uses the Azure-specific BlobSensor from airflow.providers.microsoft.azure.sensors.blob, which is the idiomatic Airflow approach for monitoring Azure Blob Storage. It does not wrap a hook in a PythonOperator but uses the native operator designed for this task.",0,"The code contains several issues: (1) 'timedelta' is used but not imported, (2) 'process_file_function' is referenced but never defined, (3) The parameter 'azure_blob_name_pattern' does not exist in BlobSensor - the correct parameter is 'blob_name' for exact match or 'prefix' for prefix matching, (4) 'start_date' should be a datetime object, not a string. These represent hallucinated or incorrect API usage that would prevent the code from running.",0,"While the code attempts to create a monitoring system with sensors and downstream processing, it fails to implement a 'robust file detection mechanism' as requested. The BlobSensor parameter 'azure_blob_name_pattern' is hallucinated and doesn't exist. The instruction asks for 'precise blob or prefix matching' but the implementation uses non-existent parameters. Additionally, the downstream processing task is incomplete (undefined function) and the XCom pull mechanism shown doesn't align with how BlobSensor actually works. A complete, working implementation is not provided.",0.3333333333333333
380,Baseline (Qwen 2.5 1.5B Instruct),48,0,"The code uses PythonOperator with DynamoDBHook and S3Hook instead of leveraging native AWS operators. For DynamoDB to S3 export, AWS provides DynamoDBToS3Operator or at minimum should use S3CreateObjectOperator. The code wraps all logic in Python callables rather than using idiomatic Airflow operators designed for AWS data transfer tasks.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.x (should be 'airflow.operators.python'). Additionally, 'datetime' is used but never imported, and 'json' module is used without import. The code also has logical errors like incorrect paginator usage and malformed S3 paths. These issues indicate the code is not production-ready.",0,"The instruction requires 'precise time-based incremental export capabilities' and 'point-in-time data recovery', but the code performs a full table scan without any incremental logic or timestamp filtering. There's no mechanism for capturing changes since last export, no point-in-time recovery implementation, and the restore task illogically runs after every export in the same DAG execution. The code does not demonstrate 'advanced cloud data transfer techniques' as it uses basic scan/put operations.",0.0
381,Baseline (Qwen 2.5 1.5B Instruct),49,0,"The code uses PythonOperator with simple print statements instead of leveraging appropriate Airflow providers for data protection/privacy management. For managing sensitive data detection configurations (likely referring to Google Cloud DLP, AWS Macie, or similar services), the code should use specific operators like CloudDLPCreateInspectTemplateOperator, CloudDLPGetInspectTemplateOperator, and CloudDLPDeleteInspectTemplateOperator from airflow.providers.google.cloud.operators.dlp, or equivalent operators for other cloud providers. The current implementation is a generic Pythonic wrapper that doesn't actually interact with any data protection service.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.1. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, the code references 'timedelta' without importing it (missing 'from datetime import timedelta'). These are not hallucinations per se, but represent outdated/incorrect library usage for the specified Airflow version.",0,"While the code creates a sequential workflow with create, inspect, and cleanup steps as requested, it fails to demonstrate actual 'programmatic handling of data privacy templates' or 'data protection configurations'. The functions only contain print statements and do not implement any real data protection logic, template management, or interaction with actual privacy/security services. The instruction asks for a pipeline that 'demonstrates how to programmatically handle data privacy templates', but this code only provides placeholder functions with no substantive implementation.",0.0
382,Baseline (Qwen 2.5 1.5B Instruct),50,0,The code uses PythonOperator with Neo4jHook instead of using the native Neo4jOperator. Airflow providers.neo4j includes Neo4jOperator which is designed specifically for executing Cypher queries. The pattern of wrapping hook.run() inside a PythonOperator is not idiomatic when a dedicated operator exists.,0,"The code references 'PythonOperator' but never imports it. The import statement 'from airflow.operators.python import PythonOperator' is missing, which would cause a NameError at runtime. This is a critical code error that makes it non-production-ready.",0,"The instruction specifically asks to 'pull specific node attributes for Tom Hanks on a given date' with 'parameterized Cypher queries'. However, the code has multiple issues: (1) The Cypher query logic is incorrect - it searches for n.name = 'name' (literal string) instead of 'Tom Hanks', (2) It uses n.birthdate instead of querying for a specific date parameter as requested, (3) The query is not properly parameterized (uses f-strings which is SQL injection vulnerable), and (4) The date logic uses datetime.now() instead of accepting a date parameter for flexibility as requested.",0.0
383,Baseline (Qwen 2.5 1.5B Instruct),51,0,"The code fails to use idiomatic Airflow patterns. It wraps all logic in PythonOperators instead of using native AWS provider operators. For S3 operations, it should use S3Hook or operators like S3FileTransformOperator. For AWS Comprehend, while there isn't a dedicated operator for PII detection, the code incorrectly imports 'comprehend_hook' as a function when it should instantiate ComprehendHook class. The tasks use lambda functions and manual hook calls rather than leveraging Airflow's operator ecosystem.",0,"The code contains a hallucinated import: 'from airflow.providers.amazon.aws.hooks.comprehend import comprehend_hook'. The correct import should be 'from airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook' (a class, not a function). Additionally, 'PythonOperator' is imported from the deprecated 'airflow.operators.python_operator' path instead of 'airflow.operators.python'. The code also references 'timedelta' without importing it from datetime.",0,"The code only partially fulfills the instruction. While it attempts to create a PII masking workflow, it has critical gaps: (1) The read_raw_data_task returns a hardcoded string instead of actually reading from a source, (2) The write_redacted_data_task doesn't actually write to S3 or any output file, just returns a string, (3) The PII masking logic only handles 'PERSON' entities and doesn't comprehensively anonymize all PII types (email, phone, SSN, etc.), (4) The Comprehend API call uses detect_entities instead of the more appropriate detect_pii_entities method. The instruction explicitly requires taking raw data, applying masking, and generating a redacted output file - none of which are properly implemented.",0.0
384,Baseline (Qwen 2.5 1.5B Instruct),52,1,"The code uses native Airflow providers for Azure Container Instances (AzureContainerInstanceCreateOperator, AzureContainerInstanceDeleteOperator, AzureContainerInstanceSensor) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic approach for Airflow.",0,"The code contains significant hallucinations. The AzureContainerInstanceCreateOperator does not support parameters like 'image_name', 'image_tag', 'dockerfile_path', 'registry_server', 'registry_username', 'registry_password', 'persistent_volume_name', 'storage_account_name', or 'storage_account_key' in the way they are used. The actual operator expects a container group configuration with different parameter structures. The operator is designed to deploy existing container images, not build them from Dockerfiles. These are invented parameters that don't exist in the actual Airflow 2.8.4 Azure provider API.",0,"While the code attempts to address the instruction's requirements (standard Docker images, custom registries, persistent volumes), it fails to implement them correctly. The instruction asks for 'launching standard Docker images' (deploying existing images), 'using custom container registries' (pulling from ACR), and 'configuring persistent volume attachments' (mounting Azure File shares). The code confuses container deployment with image building (dockerfile_path), uses non-existent parameters, and doesn't properly demonstrate different configuration scenarios as requested. The task dependencies are also illogical (monitoring tasks are disconnected from create tasks, containers are deleted immediately after creation without monitoring).",0.3333333333333333
385,Baseline (Qwen 2.5 1.5B Instruct),54,1,"The code correctly uses the SparkSubmitOperator from airflow.providers.apache.spark.operators.spark_submit_operator, which is the idiomatic and native Airflow operator for submitting Spark jobs. It does not wrap Spark logic in PythonOperator with hooks, demonstrating proper use of Airflow's provider pattern.",1,"The code is clean and uses only standard Airflow libraries. It imports from the official apache.spark provider package and does not include any test harness code, internal testing modules, or non-standard imports. The code is production-ready in structure.",1,"The DAG successfully fulfills the instruction requirements: (1) It illustrates sequential execution of different Spark operators with three distinct tasks (read_csv, transform_data, write_csv), (2) demonstrates task chaining using the >> operator to create dependencies, (3) shows modular task dependencies where each task represents a different Spark processing capability (reading, transforming, writing data), and (4) provides a demonstration pipeline that highlights how complex data workflows can be orchestrated. The DAG structure clearly shows the sequential flow and modular nature requested.",1.0
386,Baseline (Qwen 2.5 1.5B Instruct),55,0,"The code only imports operators but never instantiates them or creates a DAG structure. It imports GCSToGCSOperator which is appropriate, but critically missing are operators for bucket creation (S3CreateBucketOperator, GCSCreateBucketOperator), cross-cloud transfer (S3ToGCSOperator, GCSToS3Operator), and Google Cloud Transfer Service operators (CloudDataTransferServiceCreateJobOperator). The excessive repetitive imports of non-existent S3 sensors (many of these classes don't exist in Airflow) suggest the code is incomplete and not idiomatic.",0,"The code contains numerous hallucinated imports. Many of the S3 sensor classes being imported do not exist in the Airflow codebase (e.g., S3EventBridgeRuleExistsSensor, S3EventBridgeTargetExistsSensor, S3EventBridgePermissionExistsSensor, S3EventBridgePolicyExistsSensor, S3EventBridgeTargetPolicyExistsSensor, S3EventBridgeRulePolicyExistsSensor, S3AclSensor, S3NotificationConfigurationSensor, S3ObjectSizeSensor, S3LastModifiedSensor, S3PrefixExistsSensor). The code also has repetitive duplicate imports of the same non-existent classes, and the import statement is cut off mid-line, indicating the code is malformed.",0,"The instruction requires: (1) bucket creation, (2) file transfer between cloud providers (AWS S3 and GCS), and (3) transfer job management using Google Cloud Transfer Service. The provided code is incomplete - it only contains imports with no DAG definition, no task instantiation, no bucket creation logic, no actual file transfers configured, and no Google Cloud Transfer Service job management. The code does not fulfill any of the requested business logic.",0.0
387,Baseline (Qwen 2.5 1.5B Instruct),56,1,"The code uses native Google Cloud Provider operators (GKEClusterCreateOperator, GKEClusterDeleteOperator, DataFlowJobSubmitOperator, BigQueryCreateDatasetOperator, BigQueryDeleteDatasetOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for interacting with Google Cloud services.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package (airflow.providers.google.cloud.operators.*) and core Airflow modules. There are no test harness imports, internal testing modules, or non-standard libraries present. The code is clean and production-ready in terms of imports.",0,"The instruction specifically requests a DAG that demonstrates 'environment configuration, version listing, and operational state management for cloud data processing platforms' using 'Google Cloud Composer operators'. However, the code does not use any Cloud Composer-specific operators (e.g., CloudComposerCreateEnvironmentOperator, CloudComposerListEnvironmentsOperator, CloudComposerUpdateEnvironmentOperator). Instead, it uses GKE, Dataflow, and BigQuery operators. Additionally, the task 'list_datasets' incorrectly uses BigQueryCreateDatasetOperator instead of a listing operator, and there's no version listing functionality as requested. The DAG does not fulfill the specific business logic of provisioning/managing Cloud Composer environments.",0.6666666666666666
388,Baseline (Qwen 2.5 1.5B Instruct),57,0,"The code uses operators that do not exist in the official Airflow Google Provider package. Specifically, 'GCPComputeInstanceCreateOperator', 'GCPComputeSSHCommandOperator', and 'GCPComputeInstanceDeleteOperator' are not real operators in airflow.providers.google.cloud.operators.compute. The actual operators are 'ComputeEngineInsertInstanceOperator', 'ComputeEngineDeleteInstanceOperator', and there is no native SSH operator for GCE in the standard provider. The correct idiomatic approach would use 'ComputeEngineInsertInstanceOperator' and 'ComputeEngineDeleteInstanceOperator' from 'airflow.providers.google.cloud.operators.compute', and for SSH operations, either 'SSHOperator' from 'airflow.providers.ssh.operators.ssh' or a custom solution.",0,"The code hallucinates non-existent operators. The operators 'GCPComputeInstanceCreateOperator', 'GCPComputeSSHCommandOperator', and 'GCPComputeInstanceDeleteOperator' do not exist in the Airflow 2.9.3 Google Cloud Provider package. This represents a significant hallucination of API components that would cause immediate import failures in a production environment.",1,"Despite using incorrect operator names, the code structure does attempt to fulfill all requirements from the instruction: it creates a GCE instance, executes a remote SSH command, and deletes the instance. The task dependencies are correctly defined (create >> ssh >> delete), and the DAG demonstrates the complete lifecycle management requested. The code also attempts to showcase OS Login authentication through the 'enable_os_login=True' parameter. The logical flow and intent match the user's requirements, even though the implementation uses non-existent operators.",0.3333333333333333
389,Baseline (Qwen 2.5 1.5B Instruct),58,0,The code uses PythonOperator with AzureBatchHook instead of using the native AzureBatchOperator. Airflow provides airflow.providers.microsoft.azure.operators.batch.AzureBatchOperator specifically for submitting Azure Batch jobs. The pattern of wrapping hook logic in a PythonOperator is not idiomatic when a dedicated operator exists.,0,"Multiple issues exist: 1) Missing import for 'timedelta' from datetime module. 2) 'batch_client' is referenced but never defined or imported. 3) The AzureBatchHook initialization appears incorrect - it doesn't take 'batch_client' as a parameter. 4) The hook's 'submit_job' method signature with parameters like 'pool_id', 'command_line', and 'container_image' doesn't match the actual Azure Batch Hook API. 5) BashOperator is used but never imported. These are fabricated/hallucinated APIs that don't exist in the actual Airflow Azure provider.",0,"While the code attempts to trigger an Azure Batch task with a shell command, it fails to properly demonstrate a working implementation due to incorrect API usage and missing imports. The instruction asks to 'demonstrate' and 'showcase' the functionality, which implies working, production-ready code. The code structure is also illogical - it executes the batch job first, then triggers an echo statement, when the dependency should be reversed if the trigger task is meant to initiate the process.",0.0
390,Baseline (Qwen 2.5 1.5B Instruct),59,0,"The code fails to use idiomatic Airflow patterns. It wraps S3 operations in PythonOperator with S3Hook instead of using the native S3 operators. For file upload, it should use LocalFilesystemToS3Operator or S3CreateObjectOperator. For checking file existence, it should use S3KeySensor. The code demonstrates the anti-pattern of using PythonOperator + Hook instead of leveraging Airflow's purpose-built operators from the AWS provider package.",1,"The code uses only standard Airflow libraries (airflow.operators.python_operator, airflow.providers.amazon.aws.hooks.s3) without importing any internal testing modules or test harness boilerplate. No hallucinated imports or test utilities are present.",0,"While the code attempts to address the instruction by creating a file and uploading to S3, it has critical issues: (1) Functions are defined AFTER they are referenced in PythonOperator calls, causing NameError at runtime; (2) The s3_hook is instantiated at module level outside task context, which is problematic for execution; (3) The instruction asks for 'predefined operators' but the code uses generic PythonOperator instead of AWS-specific operators; (4) Error handling is mentioned in comments but not properly implemented (just try-catch with print statements, no proper Airflow error handling mechanisms like on_failure_callback or retries configuration for specific failure scenarios).",0.3333333333333333
391,Baseline (Qwen 2.5 1.5B Instruct),60,0,"The code fails to use the appropriate Airflow operators for different Python runtime environments. For Airflow 3.0.1, it should use: (1) PythonOperator for standard context, (2) PythonVirtualenvOperator for virtual environment execution, and (3) ExternalPythonOperator for external Python interpreter. Instead, it incorrectly uses only PythonOperator for all three tasks and attempts to simulate different environments by setting PYTHONPATH in env variables, which does not actually create isolated virtual or external Python contexts. Additionally, it uses the deprecated 'python_operator' import path instead of 'airflow.operators.python'.",1,"The code uses only standard Airflow imports and does not include any test harness code, internal testing modules, or non-existent libraries. While the implementation is incorrect, there are no hallucinated imports or test boilerplate present.",0,"The instruction explicitly requests a pipeline demonstrating tasks executed in 'standard, virtual, and external Python contexts'. The code attempts to address this but fails to actually implement different runtime environments. Using PythonOperator with modified PYTHONPATH environment variables does not create true virtual or external Python contexts. The code should have used PythonVirtualenvOperator and ExternalPythonOperator to properly fulfill the requirement of showing 'how tasks can be executed in standard, virtual, and external Python contexts'.",0.3333333333333333
392,Baseline (Qwen 2.5 1.5B Instruct),61,0,"The code imports 'GKEPodExecOperator' from 'airflow.providers.google.cloud.operators.gke_kubernetes_engine', which does not exist in the standard Airflow provider structure. The correct approach for Airflow 3.0.6 would be to use 'GKEStartPodOperator' from 'airflow.providers.google.cloud.operators.kubernetes_engine' or use the standard 'KubernetesPodOperator' with GKE configuration. This is a hallucinated/incorrect operator import that makes the code non-functional and non-idiomatic.",0,"The import 'from airflow.providers.google.cloud.operators.gke_kubernetes_engine import GKEPodExecOperator' is a hallucination. This module path and operator do not exist in the official Airflow Google provider package. The correct operators for pod execution on GKE are 'GKEStartPodOperator' or 'KubernetesPodOperator'. Additionally, parameters like 'use_default_container_image' and 'in_cluster' are not valid parameters for any standard GKE pod operators.",0,"While the DAG attempts to address the core requirements (create cluster, deploy workload, delete cluster), it fails on several key instruction points: 1) 'asynchronous task execution' is not demonstrated (no deferrable operators or async patterns shown), 2) 'cross-communication between tasks' is not implemented (no XCom usage or task-to-task data passing), and 3) 'basic pod deployment' uses a non-existent operator that would not actually deploy a functional pod. The workflow structure is correct but the implementation does not fulfill the specific requirements.",0.0
393,Baseline (Qwen 2.5 1.5B Instruct),62,0,"While the code uses AWS provider operators, it uses them incorrectly and inappropriately. The code uses RedshiftDataOperator for training a model on EC2, which is completely wrong - Redshift is a data warehouse, not a compute service for ML training. For document classification on AWS, it should use SageMaker operators (e.g., SageMakerTrainingOperator, SageMakerEndpointOperator) or AWS Comprehend operators. The EC2StartInstanceOperator is used incorrectly (missing instance_id parameter for starting an existing instance). The code also references a non-existent 'S3DeleteBucketOperator' and uses 'create_dag_group' which doesn't exist - it should be TaskGroup from airflow.utils.task_group.",0,"The code contains multiple hallucinated elements: 1) 'S3DeleteBucketOperator' does not exist in Airflow providers (should be S3DeleteBucketOperator from airflow.providers.amazon.aws.operators.s3 if it existed, but the actual operator is different). 2) 'dag.create_dag_group()' is not a valid Airflow method - TaskGroup should be imported and used instead. 3) The EC2StartInstanceOperator parameters are incorrect - it's being used as if creating a new instance but then referenced as if starting an existing one. 4) The workflow logic is nonsensical (terminating an EMR cluster that was never created, using Redshift for ML training).",0,"The instruction asks for a document classification training process using AWS services with proper task groups, monitoring, and error handling. The code fails on multiple fronts: 1) No proper ML service operators (should use SageMaker or Comprehend for document classification, not Redshift). 2) No monitoring tasks implemented. 3) No error handling or resource management (no trigger rules, no try-catch patterns, no cleanup on failure). 4) Task group implementation is incorrect (uses non-existent 'create_dag_group' method). 5) The workflow doesn't make logical sense - it creates an EMR cluster reference without actually creating one, uses Redshift for ML training, and the task dependencies don't represent a real ML pipeline. The code completely misses the core requirement of implementing a document classification training workflow.",0.0
394,Baseline (Qwen 2.5 1.5B Instruct),63,0,"The code fails idiomatic Airflow practices by wrapping all logic in PythonOperators with hooks instead of using native operators. It should use TrinoOperator for querying Trino, GCSToBigQueryOperator or BigQueryCreateExternalTableOperator for BigQuery operations. The pattern of creating hooks at the DAG level (outside tasks) and using PythonOperator with undefined callables (write_data_to_gcs, read_data_from_gcs, create_external_table_in_bigquery) is anti-idiomatic.",0,"The code references undefined functions (write_data_to_gcs, read_data_from_gcs, create_external_table_in_bigquery) that are never implemented. It also uses 'airflow.operators.python_operator.PythonOperator' which is deprecated in Airflow 3.0.0 (should be airflow.operators.python.PythonOperator). Additionally, 'timedelta' is used but never imported, and 'read_data_task.output_schema' references a non-existent attribute. The code is not production-ready.",0,"While the code attempts to address inter-cloud data transfer from Trino to BigQuery, it fails to properly implement the required steps. The instruction asks for 'external table creation' and 'moving metadata', but the code has incomplete/non-functional implementations with missing function definitions. The pipeline structure is outlined but not executable, and it doesn't demonstrate actual cloud data integration techniques as the core logic is missing.",0.0
395,Baseline (Qwen 2.5 1.5B Instruct),64,0,"The code uses PythonOperator with DataplexHook instead of using native Dataplex operators. Airflow providers.google.cloud.dataplex includes dedicated operators like DataplexCreateEntryGroupOperator, DataplexCreateEntryTypeOperator, and DataplexCreateAspectTypeOperator that should be used for idiomatic Airflow code. The pattern of wrapping hook calls in Python functions and using PythonOperator is explicitly the anti-pattern described in the scoring criteria.",1,"The code uses standard Airflow libraries (airflow.providers.google.cloud.hooks.dataplex, airflow.operators.python_operator) without importing any internal testing modules or test harness boilerplate. No hallucinated imports or test utilities are present.",0,"The code has multiple issues with instruction adherence: 1) It lacks task dependencies (create_entry_group_task >> create_entry_type_task >> create_aspect_type_task) to orchestrate the workflow systematically, 2) The parameters are hardcoded rather than being 'clear, configurable parameters' as requested, 3) Variable scoping issues exist (entry_group_name and entry_type_name are referenced in functions where they're not defined), 4) The workflow doesn't demonstrate proper programmatic setup with configurable parameters - it uses hardcoded strings throughout instead of DAG params or variables.",0.3333333333333333
396,Baseline (Qwen 2.5 1.5B Instruct),65,0,"The DAG uses only PythonOperator for all tasks including S3 data ingestion and cloud resource management. For Airflow 3.0.6, idiomatic code should use specific providers like S3Operator/S3Hook from airflow.providers.amazon.aws, SageMakerOperator for model training, or other cloud-native operators. The code wraps everything in generic Python callables instead of leveraging purpose-built operators for S3, ML training platforms, or resource management.",0,"The code references undefined functions (read_images_from_s3, preprocess_images, train_model, evaluate_model) that are never imported or defined. While this could be intentional placeholder code, it makes the DAG non-functional as-is. Additionally, it uses the deprecated 'airflow.operators.python_operator' import path instead of the Airflow 3.x standard 'airflow.operators.python.PythonOperator'. The code is not production-ready.",0,"The instruction explicitly requires 'scalable workflow that supports data ingestion, model training, and automated resource management in a cloud environment.' The DAG provides a basic skeleton but lacks: (1) actual implementation of cloud resource management (no dynamic resource provisioning/cleanup), (2) no configurable training parameters exposed at DAG level (parameters are hardcoded in op_kwargs), (3) no cloud-native scalability features (no use of Kubernetes, SageMaker, Vertex AI, or similar services), and (4) missing actual data ingestion logic. The workflow structure exists but critical requirements are not fulfilled.",0.0
397,Baseline (Qwen 2.5 1.5B Instruct),68,0,"The code uses PythonOperator with RedisHook instead of using the native RedisPublishOperator from airflow.providers.redis.operators.redis_publish. While RedisHook is appropriate for some use cases, Airflow provides specific operators like RedisPublishOperator for publishing messages to Redis. For the waiting/polling logic, a sensor (RedisPubSubSensor or RedisKeySensor) would be more idiomatic than a PythonOperator with manual checking logic.",0,"The code has several issues: (1) Missing import for 'timedelta' from datetime module, (2) Functions are defined AFTER they are referenced in the PythonOperator definitions, which will cause NameError at parse time, (3) The start_date is a string instead of a datetime object, (4) RedisHook().get_connection() returns a Connection object, not a Redis client - should use RedisHook().get_conn() to get the actual Redis connection. These are not hallucinations per se, but significant implementation errors that make the code non-functional.",0,"The instruction asks for a system where 'one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding.' The code attempts this but fails in execution: (1) The wait_for_condition task checks for 'condition_key' but nothing in the pipeline sets this key - it should check for 'start_signal' or there should be another task that sets 'condition_key', (2) The check_redis_key function only returns a boolean but doesn't implement actual waiting/polling behavior - it will execute once and return immediately, not wait for the condition, (3) There's no demonstration of 'different stages of a workflow' with multiple coordination points as implied by the instruction. The pipeline is incomplete and doesn't fulfill the real-time coordination requirement.",0.0
398,Baseline (Qwen 2.5 1.5B Instruct),69,0,"The code fails to use the appropriate Airflow operator for the task. The instruction explicitly asks for 'asynchronous time-based sensors' to 'manage workflow timing' and 'waits for a predetermined time interval'. The correct idiomatic approach would be to use TimeDeltaSensor or TimeSensor from airflow.sensors.time_delta or airflow.sensors.time_sensor. Instead, the code uses generic PythonOperator tasks with no actual delay mechanism, which does not demonstrate sensor-based timing control.",0,"The code has a critical error: it imports 'timedelta' from datetime but uses 'datetime(2023, 1, 1)' without importing 'datetime' class. The code would fail with NameError: name 'datetime' is not defined. While this is a bug rather than hallucination of test utilities, it indicates the code is not production-ready or properly validated.",0,"The code completely fails to implement the core requirement. The instruction specifically asks to 'demonstrate how asynchronous time-based sensors can manage workflow timing' and create a pipeline that 'waits for a predetermined time interval before proceeding to the next task'. The provided code has no sensor, no delay mechanism, and no demonstration of asynchronous waiting. It simply chains two PythonOperator tasks that execute immediately one after another with no controlled delay scenario.",0.0
399,Baseline (Qwen 2.5 1.5B Instruct),70,0,"The code fails to use proper Airflow providers and operators. It imports non-existent operators like 'GetFileOperator' from 'airflow.operators' (should be from a specific provider like airflow.providers.google.cloud.transfers.gcs_to_local). It also imports 'GCPSensor' from hooks instead of sensors. The code uses PythonOperator for transformation when it could potentially use a BashOperator or proper GCS operators. Additionally, 'BashOperator' should be imported from 'airflow.operators.bash' not 'airflow.operators'.",0,"The code contains multiple hallucinated/non-existent imports: 'GetFileOperator' does not exist in 'airflow.operators', and 'GCPSensor' is imported from 'airflow.providers.google.cloud.hooks.gcs' which is incorrect (sensors are in the sensors module, not hooks). The code also references undefined variables like 'datetime', 'timedelta', and 'your_custom_script_function' without importing or defining them. These are clear signs of hallucination and incomplete code generation.",0,"The instruction requires: (1) cloud storage file manipulation, (2) task dependencies, (3) file transfer between buckets, (4) custom transformation script execution, (5) proper error handling, and (6) cleanup. The code attempts task dependencies and cleanup, but fails on key requirements: there is no actual file transfer between buckets (only download from source, no upload to destination), no proper error handling mechanisms (no trigger rules, on_failure_callback, or try-except patterns), and the transformation logic is incomplete (references undefined function). The code does not demonstrate transfer between the defined source and destination buckets.",0.0
400,Baseline (Qwen 2.5 1.5B Instruct),71,0,"The DAG uses DummyOperator for all S3 operations instead of using the appropriate S3 operators from airflow.providers.amazon.aws.operators.s3. While S3Hook is imported, it's not even used. Idiomatic Airflow would use operators like S3CreateBucketOperator, S3CreateObjectOperator, S3ListOperator, and S3PutBucketTaggingOperator. The current implementation with DummyOperator performs no actual S3 operations - it just passes through with params that are never executed.",1,"The code uses only standard Airflow libraries (airflow.operators.dummy_operator and airflow.providers.amazon.aws.hooks.s3). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective, even though it doesn't implement the required functionality.",0,"The instruction explicitly asks to 'demonstrate bucket creation, object manipulation, and metadata tagging' and 'showcases different S3 operations like creating buckets, uploading files, listing contents, and managing bucket tags.' The provided DAG only uses DummyOperator tasks that do nothing - they don't actually create buckets, upload files, list objects, or manage tags. The DAG structure and task names suggest the intent, but no actual S3 operations are performed. This completely fails to fulfill the business logic requested.",0.3333333333333333
401,Baseline (Qwen 2.5 1.5B Instruct),73,0,"The code uses an incorrect import path 'airflow.operators.azure.blob_operator.AzureBlobSensor' which does not exist in Airflow 2.7.2. The correct import should be 'airflow.providers.microsoft.azure.sensors.wasb.WasbBlobSensor' or 'WasbPrefixSensor'. Additionally, the code wraps the sensor instantiation in a function unnecessarily, which is not idiomatic. Sensors should be instantiated directly as task objects in the DAG context.",0,"The code imports from a non-existent module 'airflow.operators.azure.blob_operator'. This is a hallucinated import path that doesn't exist in standard Airflow libraries. The correct provider path for Azure Blob/WASB sensors is 'airflow.providers.microsoft.azure.sensors.wasb'. Additionally, 'timedelta' is used but never imported, and 'DummyOperator' is used but not imported.",0,"While the code attempts to monitor Azure Blob Storage for file patterns, it has critical implementation flaws: (1) The sensors are created inside a function but never actually added to the DAG context, (2) The blob_name parameter doesn't support wildcard patterns in WasbBlobSensor - prefix patterns require WasbPrefixSensor instead, (3) Missing imports prevent the code from running, (4) The downstream tasks (DummyOperator) are referenced but not imported. The code does not fulfill the requirement to create a working workflow that can trigger downstream processes based on file presence.",0.0
402,Baseline (Qwen 2.5 1.5B Instruct),76,0,"The code fails to use idiomatic Airflow operators. It wraps all GCS and SFTP operations in PythonOperator with hooks instead of using native operators like GCSToLocalFilesystemOperator, LocalFilesystemToGCSOperator, SFTPOperator, or GCSToSFTPOperator. The instruction specifically asks for 'GCS and SFTP operators', but the implementation only uses PythonOperator throughout. Additionally, it uses the deprecated PythonOperator instead of the current PythonOperator from airflow.operators.python, and GCSCredentialsHook is not the correct hook for GCS operations (should be GCSHook).",0,"The code contains several issues: (1) Uses 'GCSCredentialsHook' which doesn't exist in standard Airflow - the correct hook is 'GCSHook'. (2) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.6. (3) The SSHHook usage is incorrect - it doesn't have 'put' and 'get' methods directly, and the 'connect' method doesn't accept those parameters. (4) The hook methods like 'get_bucket()' don't exist on GCSCredentialsHook.",0,"The instruction asks for 'advanced cloud storage transfer techniques', 'various file transfer scenarios', 'robust file transfer verification mechanisms', and 'sensor checks'. The code only implements basic upload/download between GCS and local filesystem, completely missing SFTP transfers (the functions are defined but never used in tasks), lacks any sensor operators for verification, has only a stub verification function with no real integrity checks, and doesn't demonstrate multiple transfer scenarios (GCS-to-SFTP, SFTP-to-GCS, etc.). The workflow is overly simplistic and doesn't showcase the advanced techniques requested.",0.0
403,Baseline (Qwen 2.5 1.5B Instruct),77,0,"The code wraps all Weaviate operations in PythonOperator functions using WeaviateHook, rather than using native Weaviate operators. Airflow 3.0.1 with the Weaviate provider includes operators like WeaviateIngestOperator and WeaviateDocumentIngestOperator that should be used for idiomatic Airflow patterns. The code uses the anti-pattern of 'def function() + PythonOperator' instead of leveraging purpose-built operators.",0,"The code contains multiple hallucinations and errors: 1) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.1, 2) The Weaviate client API calls are incorrect (e.g., 'client.vectorizer.text2vector()' and 'client.data_object.create()' don't match actual Weaviate Python client v4 API), 3) SQL-like queries ('DELETE FROM', 'CREATE CLASS') are completely fabricated and not part of Weaviate's API which uses Python methods for schema management, 4) Missing 'timedelta' import, 5) 'start_date' should be a datetime object not a string.",0,"The code attempts to address the instruction but fails on multiple key requirements: 1) Does not properly implement batch import (no actual batching logic shown), 2) Does not demonstrate 'different vectorization approaches' as requested (only shows one flawed approach), 3) Database collection management is completely incorrect with fabricated SQL-like syntax instead of proper Weaviate schema operations, 4) The workflow order is illogical (collection management should happen before data import, not after), 5) No proper error handling or data validation. The code shows a superficial attempt but misses the core technical requirements.",0.0
404,Baseline (Qwen 2.5 1.5B Instruct),78,0,The code uses PythonOperator with custom functions wrapping SQLite operations instead of using Airflow's native SqliteOperator from airflow.providers.sqlite.operators.sqlite. This is the anti-pattern described in the scoring criteria - wrapping database logic in PythonOperator instead of using the provider-specific operator designed for the task.,1,"The code is clean and uses only standard libraries (airflow, sqlite3). There are no imports from internal testing modules, no test harness boilerplate, and no hallucinated functions or modules. The code would run in a production Airflow environment (though it has other issues).",0,"The code has a critical structural flaw: functions are defined AFTER they are referenced in the PythonOperator tasks. In Python, insert_data_to_db and retrieve_data_from_db must be defined before being passed as python_callable arguments. This code would fail with NameError at DAG parsing time. While the intent to demonstrate read/write operations in SQLite (a file-based embedded database) aligns with the instruction, the broken execution order means it cannot actually fulfill the requirement of showcasing 'basic data persistence and retrieval techniques' in a working sequential task flow.",0.3333333333333333
405,Baseline (Qwen 2.5 1.5B Instruct),79,0,"The code uses hallucinated operators that do not exist in the Airflow Databricks provider. Operators like 'DatabricksCreateRepositoryOperator', 'DatabricksUpdateRepositoryOperator', and 'DatabricksDeleteRepositoryOperator' are not real Airflow operators. The correct approach would be to use 'DatabricksReposCreateOperator', 'DatabricksReposUpdateOperator', and 'DatabricksReposDeleteOperator' from 'airflow.providers.databricks.operators.databricks_repos'. Additionally, 'DatabricksRunNowOperator' is used incorrectly for running notebooks - it's meant for triggering existing jobs, not running notebooks directly. The correct operator would be 'DatabricksSubmitRunOperator' or 'DatabricksNotebookOperator'.",0,"The code contains multiple hallucinated operators that do not exist in the standard Airflow Databricks provider library. The operators 'DatabricksCreateRepositoryOperator', 'DatabricksUpdateRepositoryOperator', and 'DatabricksDeleteRepositoryOperator' are fabricated. Additionally, the code is missing the 'timedelta' import from datetime, which would cause a runtime error. The parameters used in 'DatabricksRunNowOperator' (like 'notebook_path', 'parameters', and 'notebook_params') are also incorrect for this operator.",0,"While the code attempts to address the instruction by including tasks for creating, updating, and deleting a repository, and executing a notebook, it fails to implement these correctly due to using non-existent operators. The workflow logic also doesn't make practical sense - it deletes the repository before running the notebook, which would likely cause the notebook execution to fail if it depends on the repository. A proper implementation should create the repo, run the notebook from within that repo, and optionally clean up afterward. The instruction asks for 'automated repository lifecycle management' which implies a working, realistic workflow, not a sequence that would fail in practice.",0.0
406,Baseline (Qwen 2.5 1.5B Instruct),80,1,"The code correctly uses KubernetesPodOperator from airflow.providers.kubernetes.operators.kubernetes_pod_operator, which is the idiomatic Airflow approach for executing workloads in Kubernetes. It does not wrap Kubernetes logic in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",1,The code is clean and production-ready. It only imports standard Airflow libraries (airflow.DAG and airflow.providers.kubernetes.operators.kubernetes_pod_operator). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate code.,0,"The instruction specifically requests 'dynamic cluster provisioning' and 'cleanly tear down infrastructure after task completion'. However, the implementation does not actually provision or tear down any Kubernetes clusters or infrastructure. The 'dynamic_cluster_provisioning_task' just runs an nginx container without any cluster provisioning logic. The 'cleanup_infrastructure_task' attempts to delete a local CSV file that doesn't exist in its container context, rather than tearing down computational resources or infrastructure. The DAG demonstrates ephemeral pod execution and basic data processing, but fails to implement the core infrastructure management aspects (provisioning and teardown of computational resources/clusters) that were explicitly requested.",0.6666666666666666
407,Baseline (Qwen 2.5 1.5B Instruct),81,0,"The code uses deprecated operators 'BashOperator' and 'PythonOperator' from old import paths (airflow.operators.bash_operator and airflow.operators.python_operator). In Airflow 2.7.2, these should be imported from 'airflow.operators.bash' and 'airflow.operators.python'. More critically, for containerized operations, the code should use the DockerOperator from airflow.providers.docker.operators.docker instead of wrapping docker commands in BashOperator. This is not idiomatic Airflow practice.",1,The code does not import any internal testing modules or include test harness boilerplate. It uses standard Airflow libraries (albeit with deprecated import paths) and does not contain hallucinated functions or test utilities like get_test_run().,0,"While the DAG attempts to create a system monitoring pipeline with time validation, wait operations, and containerized processes, it has critical implementation flaws. The 'docker run -it ubuntu /bin/bash' command will fail in a non-interactive context, and the verify_output task references '<container_id>' as a placeholder that will never work. The containerized process doesn't actually execute any meaningful operation, and the task dependency chain, while present, doesn't create a functional pipeline. The instruction asks to 'showcase task dependency and execution strategies' with working containerized operations, which this implementation fails to deliver.",0.3333333333333333
408,Baseline (Qwen 2.5 1.5B Instruct),82,0,"The code imports Hooks (EC2Hook, RDSHook, S3Hook) but wraps all logic in PythonOperator tasks instead of using native AWS Operators. Airflow providers offer specific operators like EC2CreateInstanceOperator, RDSCreateDBInstanceOperator, and S3CreateBucketOperator that should be used for idiomatic Airflow. The pattern of importing hooks and using PythonOperator is the anti-pattern explicitly called out in the scoring criteria.",0,"The code references undefined functions (create_ec2_instance, create_rds_instance, create_s3_bucket, configure_security_group, execute_data_replication) that are never implemented. These functions are called in PythonOperator tasks but don't exist in the code, making this non-functional. Additionally, 'from airflow.operators.python_operator import PythonOperator' is deprecated in Airflow 3.0.1; it should be 'from airflow.operators.python import PythonOperator'.",1,"The DAG structure addresses all key requirements from the instruction: creates cloud resources (EC2, RDS, S3), configures security settings (security group task), and executes data replication. The task dependencies are logically ordered to provision infrastructure before migration. While the implementation is incomplete, the architectural design and task breakdown align with the requested modular pipeline for cloud infrastructure provisioning and data migration.",0.3333333333333333
409,Baseline (Qwen 2.5 1.5B Instruct),83,0,"The code wraps TwitterHook and PostgresHook logic inside PythonOperator functions instead of using native operators. For Twitter operations, it should use TwitterOperator or similar provider-specific operators. For database operations, it should use PostgresOperator or SQLExecuteQueryOperator instead of manually managing connections and cursors within a PythonOperator. This is the anti-pattern of wrapping hooks in Python callables rather than using the designed operators.",0,"The code imports 'airflow.providers.twitter.hooks.twitter.TwitterHook' which does not exist in standard Airflow providers. There is no official Twitter provider in Apache Airflow (Twitter API v1.1 was deprecated and the community provider was archived). Additionally, 'timedelta' is used but never imported. The code also uses deprecated 'PythonOperator' import path ('airflow.operators.python_operator' instead of 'airflow.operators.python').",0,"The instruction requires: (1) collecting tweets from multiple accounts, (2) capturing both incoming and outgoing tweets, (3) performing sentiment analysis, and (4) preparing data for BI. The code only attempts to collect from a single hardcoded account ('example_account'), does not distinguish between incoming/outgoing tweets, has an empty sentiment analysis function (just 'pass'), does not actually pass data between tasks (no XCom usage), and the store_results function would fail as it expects a 'results' parameter that is never provided. The pipeline structure is incomplete and non-functional.",0.0
410,Baseline (Qwen 2.5 1.5B Instruct),85,0,"While the code uses appropriate Operators (BigQueryToBigQueryOperator, BigQueryToGCSOperator, GCSToBigQueryOperator), it instantiates Hooks at the DAG level (bq_hook = BigQueryHook() and gcs_credentials_hook = GCSCredentialsHook()) without using them. This is an anti-pattern - Hooks should be instantiated inside task execution context, not at DAG parse time. Additionally, the instruction explicitly asks to 'programmatically create table schemas' which would require using BigQueryCreateEmptyTableOperator or similar operators for setup, but the code doesn't implement any schema creation logic.",1,"The code uses only standard Airflow 2.8.4 libraries from airflow.providers.google.cloud. There are no imports from test utilities, no test harness boilerplate, and no hallucinated modules. The code is clean from a library perspective.",0,"The instruction requires: (1) programmatically create table schemas (setup stage), (2) copy data between tables (data movement), (3) export data to cloud storage (data movement), and (4) cleanup stages. The code is missing critical components: no table schema creation operators, no cleanup/deletion tasks, and the task dependencies are illogical (copy_data depends on load_data, which depends on export_data - this creates a backwards workflow where data is exported before being copied). The instruction asks for 'setup, data movement, and cleanup stages' but only partial data movement is implemented with incorrect ordering.",0.3333333333333333
411,Baseline (Qwen 2.5 1.5B Instruct),86,0,"The code uses non-existent operators like EC2CreateVPCOperator, EC2CreateSubnetOperator, EC2CreateInstanceOperator, etc. These operators do not exist in the airflow.providers.amazon.aws package. The actual AWS provider for Airflow does not include these specific EC2 infrastructure management operators. The idiomatic approach would be to use existing operators like EcsRunTaskOperator, or use PythonOperator with EC2Hook/boto3 for EC2 management tasks. Since these are hallucinated operators rather than real Airflow operators, this fails the idiomatic test.",0,"The code heavily hallucinates by inventing multiple operators that do not exist in Apache Airflow's AWS provider package: EC2CreateVPCOperator, EC2CreateSubnetOperator, EC2CreateSecurityGroupOperator, EC2AddSecurityGroupRuleOperator, EC2CreateInstanceOperator, EC2DeleteInstanceOperator, EC2DeleteSecurityGroupOperator, EC2DeleteSubnetOperator, and EC2DeleteVPCOperator. None of these are real Airflow operators. Additionally, the code imports EC2Hook and S3Hook but never uses S3Hook. The code also references 'timedelta' without importing it from datetime.",1,"Despite the technical implementation issues, the code does conceptually demonstrate the requested pattern: setup tasks (creating VPC, subnet, security group, EC2 instance) followed by teardown tasks (deleting all resources in reverse order). The DAG structure shows resource allocation and cleanup across multiple task stages as requested, even though the operators used are fictional. The intent and logical flow match the instruction to demonstrate setup and teardown for cloud infrastructure workflow.",0.3333333333333333
412,Baseline (Qwen 2.5 1.5B Instruct),87,0,"The code uses several operators that do not exist in the official Airflow AWS provider package. Specifically, 'S3CreateBucketOperator' and 'EmrSecuritySettingsOperator' are not real Airflow operators. The code imports S3Hook and EmrClientHook but never uses them, suggesting the AI model attempted to use operators but invented non-existent ones. Additionally, the AddStepOperator is used incorrectly with parameters like 'script_path' and 'args' that don't match the actual operator's interface. A proper implementation would use S3CreateBucketOperator from airflow.providers.amazon.aws.operators.s3 (if it exists, otherwise PythonOperator with S3Hook) and properly configure EmrCreateJobFlowOperator with security configurations embedded in the cluster configuration.",0,"The code contains multiple hallucinated operators that do not exist in Apache Airflow 2.8.4: 'S3CreateBucketOperator' is not imported and doesn't exist in the standard AWS provider, 'EmrSecuritySettingsOperator' is completely fabricated, and 'DeleteClusterOperator' does not exist (the correct operator is EmrTerminateJobFlowOperator). The AddStepOperator parameters like 'script_path' and 'args' are also incorrect - the actual operator uses 'step' parameter with a proper step configuration dictionary. These are clear hallucinations of non-existent Airflow components.",0,"While the DAG attempts to address all required components (S3 bucket creation, security configuration, EMR cluster launch, Spark step execution, and cluster lifecycle management), it fails to properly implement any of them due to the use of non-existent operators and incorrect parameter configurations. The instruction requires a working workflow, but this code would fail immediately upon execution. The security configuration step is particularly problematic as it uses a completely fabricated operator rather than embedding security settings in the EMR cluster configuration as is standard practice.",0.0
413,Baseline (Qwen 2.5 1.5B Instruct),89,0,"While the code attempts to use specific operators, it misuses them. The BigQueryToGCSOperator and GCSToBigQueryOperator are for BigQuery operations, not Cloud SQL export/import. For Cloud SQL data export/import, the code should use CloudSQLExportInstanceOperator and CloudSQLImportInstanceOperator from airflow.providers.google.cloud.operators.cloud_sql. Additionally, the 'configure_settings_task' uses DummyOperator instead of the appropriate CloudSQLInstancePatchOperator for database configuration changes. The SqlCreateInstanceOperator and SqlDeleteInstanceOperator usage appears correct, but the overall pattern fails to use the right operators for the Cloud SQL-specific operations requested.",0,"The code imports 'DummyOperator' from 'airflow.operators.dummy_operator', which is deprecated in Airflow 3.x (should be 'airflow.operators.empty.EmptyOperator'). More critically, it's missing required imports: 'datetime' and 'timedelta' are used but not imported. The SqlCreateInstanceOperator and SqlDeleteInstanceOperator import paths appear to be hallucinated - the correct paths in Airflow 3.x should be from 'airflow.providers.google.cloud.operators.cloud_sql'. These are not standard, production-ready imports for the specified Airflow version.",0,"The instruction requires Cloud SQL operations including instance creation, data export/import, database configuration changes, and infrastructure cleanup. While the DAG attempts to address these requirements, it fails on multiple fronts: (1) Uses BigQuery operators instead of Cloud SQL export/import operators, meaning data export/import won't work with Cloud SQL; (2) Database configuration changes are represented by a DummyOperator with no actual implementation; (3) The workflow logic is flawed - it exports data before the instance has databases/data, and imports data that doesn't exist. The DAG does not demonstrate enterprise-grade database infrastructure automation as requested.",0.0
414,Baseline (Qwen 2.5 1.5B Instruct),90,0,"The code uses deprecated operator parameters. The DataprocJobSubmitOperator is being used with parameters like 'main_class' and 'jar_file_uri' which are not the correct parameters for Airflow 2.9.3. The correct approach is to use the 'job' parameter with a properly structured job configuration dictionary. The operator should receive a job configuration dict like {'spark_job': {'main_class': '...', 'jar_file_uris': ['...']}}, not flat parameters. This shows a lack of understanding of the idiomatic way to use the DataprocJobSubmitOperator in modern Airflow versions.",1,"The code imports only standard Airflow providers (airflow.providers.google.cloud.operators.dataproc) and does not include any test harness code, internal testing modules, or non-production imports. All imports are legitimate and production-appropriate.",1,"The DAG successfully implements all required components from the instruction: (1) creates a Dataproc cluster using DataprocClusterCreateOperator, (2) submits a job using DataprocJobSubmitOperator, (3) deletes the cluster using DataprocClusterDeleteOperator, and (4) establishes clear sequential task dependencies (create_cluster_task >> submit_spark_job_task >> delete_cluster_task). The workflow handles infrastructure provisioning and job execution with predictable task progression as requested.",0.6666666666666666
415,Baseline (Qwen 2.5 1.5B Instruct),91,0,"The code fails to use idiomatic Airflow patterns. It uses PythonOperator with raw psycopg2 connections instead of using PostgresOperator or PostgresHook properly. The PostgresHook is imported but never used. For OpenAI, while there may not be a dedicated operator in standard providers, the PostgreSQL interaction should use PostgresOperator for SQL execution or at minimum leverage the imported PostgresHook's get_conn() method rather than hardcoding connection parameters with psycopg2.connect().",1,"The code uses standard Airflow libraries (airflow.operators.python_operator, airflow.providers.postgres.hooks.postgres) and common third-party packages (openai, psycopg2). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",0,"The code has multiple critical issues that prevent it from fulfilling the instruction: (1) Missing 'from datetime import timedelta' import causes a NameError; (2) The workflow logic is fundamentally broken - embeddings are generated at DAG parse time in op_kwargs rather than at task execution time, and the trigger tasks serve no purpose with incorrect callback usage; (3) The pipeline doesn't demonstrate a proper data pipeline pattern - it only processes a single hardcoded text ('Hello, world!') rather than showing how to process text data; (4) No demonstration of vector capabilities setup (e.g., pgvector extension); (5) Task dependencies are illogical with unnecessary trigger tasks. The instruction asks for an automated workflow/data pipeline, but this implementation would fail to execute properly.",0.3333333333333333
416,Baseline (Qwen 2.5 1.5B Instruct),92,0,"The code uses DatabricksSubmitOperator and DatabricksRunNotebookOperator, which appear to be Databricks-specific operators. However, there are critical issues: (1) DatabricksSubmitOperator is being used incorrectly with parameters like 'notebook_path', 'spark_version', 'num_workers', 'driver_memory', 'executor_cores' as direct arguments, which is not the proper API for this operator. The correct approach requires passing a 'json' parameter with a properly structured job configuration including 'new_cluster' and 'notebook_task' specifications. (2) DatabricksRunNotebookOperator may not exist in the standard Airflow Databricks provider - the correct operator for running notebooks is typically DatabricksSubmitRunOperator or using DatabricksSubmitOperator with proper notebook task configuration. This demonstrates a lack of understanding of the actual Databricks provider API.",0,"The code imports 'timedelta' without the proper import statement ('from datetime import timedelta'). Additionally, 'DatabricksRunNotebookOperator' does not exist in the standard airflow.providers.databricks.operators.databricks module as of Airflow 3.0.1. The standard operators are DatabricksSubmitRunOperator, DatabricksRunNowOperator, and DatabricksSubmitOperator. This represents a hallucinated operator that doesn't exist in the actual Airflow Databricks provider.",0,"While the code attempts to address the instruction by showing Databricks job submission and notebook execution, it fails to properly demonstrate: (1) Dynamic job creation with proper cluster configuration structure (the cluster config should be in a nested 'new_cluster' dict within the job JSON), (2) Flexible cluster configuration strategies (only shows hardcoded values without demonstrating flexibility or different configuration patterns), (3) Proper resource management demonstration (the parameters are incorrectly structured). The instruction asks for a workflow demonstrating 'flexible cluster configuration and notebook-based data preparation strategies,' but the implementation uses incorrect operator APIs and doesn't showcase the flexibility or best practices requested.",0.0
417,Baseline (Qwen 2.5 1.5B Instruct),93,0,"The code uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of the modern 'airflow.operators.python.PythonOperator'. In Airflow 3.0.0, the old import path is deprecated. Additionally, while PythonOperator is acceptable for custom logic, the code could be more idiomatic by using the TaskFlow API (@task decorator) which is the recommended pattern in modern Airflow for Python tasks and XCom passing.",1,"The code is clean and uses only standard Airflow libraries. There are no imports from internal testing modules or test harness boilerplate. The deprecated import is still a valid Airflow import, not a hallucination.",0,"The code attempts to demonstrate XCom usage but has a critical flaw: 't1.output' is used in the 'access_translation_result' function, but this variable reference won't work as intended. The function doesn't receive the XCom value properly - it should use 'ti.xcom_pull()' or pass task_instance as a parameter. The translation task returns a value correctly, but the second task cannot actually access it with the current implementation. This fails to properly demonstrate cross-task communication through XCom as requested.",0.3333333333333333
418,Baseline (Qwen 2.5 1.5B Instruct),95,0,"The code imports from 'airflow.providers.google.cloud.operators.spanner_operator' which does not exist in Airflow 2.9.3. The correct import path is 'airflow.providers.google.cloud.operators.spanner'. Additionally, operator names are incorrect - the actual operators are 'SpannerDeployInstanceOperator', 'SpannerDeleteInstanceOperator', 'SpannerDeployDatabaseOperator', 'SpannerUpdateDatabaseOperator', and 'SpannerQueryDatabaseInstanceOperator'. The code also uses non-existent operators like 'CreateInstanceOperator', 'DeleteDatabaseOperator', and 'ExecuteSqlQueryOperator' which are hallucinated.",0,"Multiple hallucinated elements exist: (1) The import path 'airflow.providers.google.cloud.operators.spanner_operator' does not exist - it should be 'spanner' not 'spanner_operator'. (2) Operator names are fabricated - 'CreateInstanceOperator', 'DeleteInstanceOperator', 'CreateDatabaseOperator', 'UpdateDatabaseSchemaOperator', 'ExecuteSqlQueryOperator', and 'DeleteDatabaseOperator' do not exist in the actual Airflow Spanner provider. (3) The 'timedelta' import is missing. (4) Parameter structures like 'configuration' and 'location' for instance creation don't match actual Spanner operator APIs. (5) 'DeleteDatabaseOperator' doesn't exist in the provider.",0,"While the code attempts to address the instruction's requirements (instance deployment, database schema evolution, idempotent modifications, and administrative tasks), it fails because the operators and their configurations are hallucinated and non-functional. The DAG lacks task dependencies (no >> or << operators to chain tasks), making it incomplete. The instruction asks for 'idempotent database modifications' but the code doesn't demonstrate idempotency patterns. Additionally, there's no actual working implementation since all the operators are incorrectly named and imported.",0.0
419,Baseline (Qwen 2.5 1.5B Instruct),96,0,"The provided code is not an Airflow DAG at all. It is a bash command 'airflow initdb' which is used for initializing the Airflow metadata database, not for defining a DAG. There are no imports, no DAG definition, no operators, and no task structure whatsoever.",0,"While the code doesn't contain hallucinated imports or test harness code, it is not valid Airflow DAG code at all. It's a single bash command that appears to be mistakenly provided instead of actual Python DAG code. This represents a fundamental failure to provide appropriate code.",0,"The instruction explicitly requests building an AI-powered content analysis pipeline using Vertex AI with text generation, embedding, and multimodal processing capabilities. The provided code is merely a database initialization command and contains none of the requested functionality: no DAG structure, no Vertex AI integration, no text generation, no embeddings, and no multimodal processing.",0.0
420,Baseline (Qwen 2.5 1.5B Instruct),97,0,"The code uses PythonOperator with AzureBlobStorageHook instead of the native Azure Blob Storage operators. Airflow provides specific operators like LocalFilesystemToWasbOperator, WasbDeleteBlobOperator, or the newer AzureBlobStorageCreateBlobOperator and AzureBlobStorageDeleteBlobOperator that should be used for these operations. The pattern of wrapping hook logic in PythonOperator is not idiomatic Airflow.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.1. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used but never imported. The code also references 'start_date' as a string '2023-04-01' instead of a datetime object, which would cause runtime errors.",1,The code fulfills the core requirements: it creates a DAG that showcases task sequencing (upload_task >> remove_task) and demonstrates cloud file management using Azure operators (uploading and removing files from Azure Blob Storage). Both requested operations are implemented and properly sequenced.,0.3333333333333333
421,Baseline (Qwen 2.5 1.5B Instruct),98,0,"While the code attempts to use SageMaker operators, it misuses them significantly. The SageMakerCreateModelOperator is being used with training job parameters (input_config, output_config, transform parameters) which is incorrect - this operator is for creating model artifacts, not training. For training a KNN model, it should use SageMakerTrainingOperator. Additionally, BashOperator is used for S3 upload instead of the more idiomatic S3CreateObjectOperator or LocalFilesystemToS3Operator. The code also references undefined BashOperator without importing it.",0,"The code contains several issues: (1) BashOperator is used but never imported, (2) SageMakerCreateModelOperator is given parameters that don't exist in its actual API (like input_config, output_config, transform_input, transform_output, container_entry_point, max_runtime_in_seconds, enable_network_isolation, etc.), (3) SageMakerDeployEndpointOperator is given a 'traffic_distribution' parameter that doesn't exist in the standard API, (4) Variables like YOUR_S3_BUCKET are referenced but never defined. The code appears to hallucinate operator parameters that don't exist in the actual Airflow providers.",0,"The instruction requires: (1) data upload - partially addressed with BashOperator but incomplete, (2) model training - NOT properly implemented (SageMakerCreateModelOperator is misused; should use SageMakerTrainingOperator), (3) endpoint deployment - attempted but with incorrect parameters, (4) automatic cleanup - partially addressed but incomplete (cleanup task is cut off and doesn't clean up model/endpoint config). The pipeline is missing the actual training step entirely, which is the core requirement. The code also includes unnecessary tasks like update_endpoint_task that weren't requested and would fail in the defined sequence.",0.0
422,Baseline (Qwen 2.5 1.5B Instruct),99,0,"The code fails to use Airflow's native Pinecone integration. Airflow 2.8.4 includes airflow.providers.pinecone with PineconeHook and potentially operators for Pinecone operations. Instead, the code wraps all Pinecone logic inside PythonOperator callables using the raw pinecone library directly. This is the anti-pattern of using generic PythonOperator instead of purpose-built providers.",0,"The code has multiple critical issues: (1) Missing import for 'datetime' while using datetime(2023, 1, 1), (2) Uses deprecated 'PythonOperator' import path (should be from airflow.operators.python), (3) The trigger_pipeline function attempts to manually execute tasks with .execute(context=None) which is not how Airflow task dependencies work and would fail in practice, (4) The Pinecone API usage appears incorrect (pinecone.upsert doesn't take collection_name parameter in that way, and Index.exists() is not a valid method call pattern), (5) Hard-coded API keys in code rather than using Airflow connections.",0,"While the code attempts to address vector embedding ingestion, it fails on multiple instruction requirements: (1) Does not properly demonstrate 'flexible metadata handling' - no metadata is attached to vectors, (2) Does not implement 'batch processing capabilities' - processes data in a non-batched manner, (3) Does not properly showcase 'Airflow's integration with Pinecone' since it bypasses Airflow's provider system entirely, (4) The pipeline structure is broken with the trigger_pipeline task that doesn't follow Airflow's dependency model (should use >> operators), (5) Data flow between tasks is not implemented (tasks don't pass data via XCom or proper mechanisms).",0.0
423,Baseline (Qwen 2.5 1.5B Instruct),100,0,"The code imports operators that do not exist in the official Airflow Google provider package. The actual Airflow 3.0.1 providers.google.cloud.operators.datafusion module contains CloudDataFusionCreateInstanceOperator, CloudDataFusionDeleteInstanceOperator, CloudDataFusionCreatePipelineOperator, CloudDataFusionStartPipelineOperator, etc. The imported operators 'DeployPipelineOperator' and 'ExecutePipelineOperator' are not real Airflow operators. The correct operators should be CloudDataFusionCreatePipelineOperator and CloudDataFusionStartPipelineOperator respectively.",0,"The code hallucinates non-existent operators. 'CreateDataFusionInstanceOperator', 'DeployPipelineOperator', 'ExecutePipelineOperator', and 'DeleteDataFusionInstanceOperator' are not actual operators in the airflow.providers.google.cloud.operators.datafusion module. The real operators have different names (prefixed with 'CloudDataFusion') and different parameter structures. Additionally, the 'config' parameter structure shown for the create instance task does not match the actual API.",1,"Despite using incorrect operator names, the code does attempt to fulfill all requirements of the instruction: it creates a Data Fusion instance, deploys a pipeline, executes it, and performs cleanup by deleting the instance. The logical flow and task dependencies correctly orchestrate the complete lifecycle as requested.",0.3333333333333333
424,Baseline (Qwen 2.5 1.5B Instruct),101,0,"The code wraps MsSqlHook logic inside PythonOperator functions instead of using native MsSql operators. Airflow provides MsSqlOperator for executing SQL queries directly. The instruction specifically asks to 'demonstrate different methods of database interaction using MsSql operators and hooks', but this implementation only uses hooks wrapped in PythonOperators, not the actual MsSqlOperator. Idiomatic Airflow would use MsSqlOperator for SQL operations and only resort to hooks when operators are insufficient.",0,"The code has several issues: (1) 'timedelta' is used but not imported, (2) 'start_date' should be a datetime object not a string, (3) The insert_data_task uses an invalid syntax '>> PythonOperator(...)' which creates an orphaned operator without proper task assignment, (4) The load_data_task returns values but they are never passed to insert_data_task, making the pipeline non-functional. These are not hallucinations of test modules, but the code is not production-ready due to syntax errors and logical flaws.",0,"The instruction requires: (1) data insertion - partially implemented but broken, (2) parameterized queries - not implemented at all, (3) multiple query scenarios - not demonstrated, (4) different methods of database interaction using both operators AND hooks - only hooks are used, operators are missing. The code only attempts basic data insertion with hooks wrapped in PythonOperators, missing most of the required functionality including query scenarios and proper use of MsSql operators.",0.0
425,Baseline (Qwen 2.5 1.5B Instruct),102,0,"The code fails to use idiomatic Airflow patterns. OracleOperator is designed to execute SQL but not export data to files. The code doesn't actually transfer data from Oracle to GCS - it uses GCSToGCSOperator which only copies between GCS buckets, not from Oracle to GCS. The proper approach would be to use OracleToGCSOperator from airflow.providers.google.cloud.transfers.oracle_to_gcs, which is specifically designed for this Oracle-to-GCS transfer task. Additionally, GCSHook methods (create_bucket, delete_bucket) are being called directly instead of using proper operators like GCSCreateBucketOperator and GCSDeleteBucketOperator.",0,"The code has critical errors that would prevent it from running. It references 'timedelta' without importing it from datetime. More importantly, it incorrectly calls GCSHook.create_bucket() and GCSHook.delete_bucket() as if they were operators - these are Hook methods, not operators, and cannot be used directly in task definitions with task_id parameters. The syntax 'GCSHook.create_bucket(task_id=...)' is not valid Airflow code and represents a fundamental misunderstanding of the Hook vs Operator pattern.",0,"The code fails to meet the core requirements. The instruction asks for: (1) data export from Oracle, (2) transfer to GCS in CSV format, and (3) automated bucket creation and deletion for resource management. The code doesn't actually extract and transfer data from Oracle to GCS - it uses GCSToGCSOperator which only works between GCS buckets. There's no mechanism to export Oracle data to CSV format. The task dependencies are illogical (create_bucket and delete_bucket run after transfer, when the bucket should be created before and deleted after). The workflow doesn't demonstrate a proper ETL pattern for moving Oracle data to cloud storage.",0.0
426,Baseline (Qwen 2.5 1.5B Instruct),103,1,The code uses native SageMaker operators (SageMakerCreateNotebookInstanceOperator and SageMakerDeleteNotebookInstanceOperator) from the official AWS provider package rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for AWS SageMaker operations.,1,"The code imports only standard Airflow libraries from the official providers package (airflow.providers.amazon.aws.operators.sagemaker). There are no test harness imports, internal testing modules, or non-production code patterns present.",0,"The instruction explicitly asks for 'creating, manipulating, and cleaning up' a notebook instance, demonstrating a 'step-by-step orchestration'. The code only includes create and delete operations, but is missing the 'manipulating' step. A complete implementation should include intermediate operations such as starting/stopping the notebook instance (SageMakerStartNotebookInstanceOperator/SageMakerStopNotebookInstanceOperator) or other manipulation tasks to fully demonstrate the sequential management and control of the notebook lifecycle as requested.",0.6666666666666666
427,Baseline (Qwen 2.5 1.5B Instruct),104,0,"The code uses PythonOperator with OracleHook instead of using the native OracleOperator or OracleStoredProcedureOperator. Airflow providers include operators specifically designed for executing stored procedures (e.g., airflow.providers.oracle.operators.oracle.OracleStoredProcedureOperator), which would be the idiomatic approach. Wrapping hook logic in PythonOperator is a non-idiomatic pattern when native operators exist.",1,"The code uses only standard Airflow libraries (airflow.providers.oracle.hooks.oracle.OracleHook and airflow.operators.python_operator.PythonOperator). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective, though it uses deprecated import paths (PythonOperator should be imported from airflow.operators.python in Airflow 3.0.1).",0,"The instruction specifically asks to 'demonstrate calling stored procedures with different parameter passing techniques' and 'showcases input and output parameter handling'. While the code attempts to show different approaches, it has critical flaws: (1) The IN OUT parameter example incorrectly uses fetchone() after callproc instead of properly handling OUT parameters via cursor variables, (2) The keyword parameters example passes a dictionary to callproc which is not valid syntax for cx_Oracle, (3) None of the examples actually demonstrate proper output parameter handling or show how to retrieve values from OUT/IN OUT parameters. The code does not fulfill the business logic of properly demonstrating parameter handling techniques.",0.3333333333333333
428,Baseline (Qwen 2.5 1.5B Instruct),106,0,"The code incorrectly uses GoogleCloudStorageHook methods directly as if they were operators. Hooks are meant to be used within operators or callable functions, not as standalone tasks. The correct approach would be to use GCSCreateBucketOperator, GCSDeleteBucketOperator, and LocalFilesystemToGCSOperator (or PythonOperator with the hook). The code attempts to call hook methods like `GoogleCloudStorageHook.create_bucket()` with a `task_id` parameter, which is not how hooks work in Airflow.",1,The code uses standard Airflow and Google Cloud provider libraries without importing any test utilities or internal testing modules. All imports are from legitimate Airflow packages (airflow.providers.google.cloud).,0,"While the code attempts to address the instruction's requirements (creating buckets, copying files, deleting buckets), it fundamentally fails to implement a working solution. The instruction asks for bucket management including creating, copying, and deleting files and buckets. The code structure shows intent to create source bucket, upload file, copy file, and delete both buckets, but the implementation is broken because hooks are used incorrectly as operators. Additionally, the destination bucket is never created before attempting to copy files to it, which would cause the workflow to fail.",0.3333333333333333
429,Baseline (Qwen 2.5 1.5B Instruct),107,0,"The code uses PythonOperator with GoogleCloudDatastoreHook instead of using native Datastore operators. While Airflow's Google Cloud provider does have DatastoreHook, the pattern here wraps all logic in Python callables rather than using declarative operators. This is the anti-pattern described in the scoring criteria - wrapping hook logic in PythonOperator instead of using purpose-built operators.",0,"The code references two undefined functions: 'start_transaction' and 'rollback_transaction' in the PythonOperator task definitions, but these functions are never defined in the code. This makes the DAG non-functional and indicates hallucination of functionality that doesn't exist. Additionally, 'from airflow.operators.python_operator import PythonOperator' is deprecated syntax in Airflow 2.9.3 (should be 'from airflow.operators.python import PythonOperator').",0,"The instruction specifically asks to 'start a transaction and then safely roll it back without persisting potentially unwanted data modifications.' The code attempts to address this but fails critically: (1) The undefined start_transaction and rollback_transaction functions mean no actual transaction management is implemented, (2) The trigger_rule='all_failed' on rollback means it only runs if previous tasks fail, not as part of a deliberate rollback demonstration, (3) The insert_data and delete_data functions don't operate within a transaction context at all - they make direct, committed changes to Datastore. The code does not demonstrate proper transaction management or safe rollback practices as requested.",0.0
430,Baseline (Qwen 2.5 1.5B Instruct),108,0,"The code wraps all logic in PythonOperators with Hooks instead of using native Airflow operators. For creating a GCS bucket, it should use GCSCreateBucketOperator. For data transfer from PostgreSQL to GCS, it should use PostgresToGCSOperator which is specifically designed for this task. The current implementation is the anti-pattern of wrapping Hook logic in PythonOperators.",0,"The code has critical issues: (1) The function 'create_bucket' is referenced in create_bucket_task but never defined in the code. (2) The variable 'bucket_name' is used in migrate_data() but is not defined within that function's scope. (3) Uses deprecated 'PythonOperator' import path (should be from airflow.operators.python). These are not hallucinations of test code, but represent incomplete/broken code that would fail at runtime.",0,"The instruction explicitly asks for 'full infrastructure provisioning' and to 'highlight how to programmatically create cloud resources'. While the code attempts to create a bucket, the create_bucket function is not implemented. Additionally, the instruction mentions 'secure data migrations' but there's no security configuration, encryption, or secure credential handling demonstrated. The core data extraction and transfer concept is present but incomplete and non-functional due to missing function definitions and variable scope issues.",0.0
431,Baseline (Qwen 2.5 1.5B Instruct),109,0,"The code fails to use idiomatic Airflow patterns in several ways: (1) It uses PythonOperator with a 'file_exists' function instead of using the FileSensor for file checking, which is redundant since FileSensor already exists in the DAG. (2) The HttpSensor is misconfigured - it uses a full URL in the 'endpoint' parameter instead of using an HttpSensor with a proper connection_id and relative endpoint. (3) Most critically, the instruction asks for a 'bash command sensor' but the code uses BashOperator instead of a sensor type (like ExternalTaskSensor or a custom sensor wrapping bash logic). The code mixes operators and sensors without demonstrating proper sensor usage patterns.",0,"The code has multiple issues that prevent it from being production-ready: (1) Missing imports - 'datetime' is used but not imported (only timedelta is imported), 'os' module is used in file_exists function but not imported, and 'BashOperator' is used but not imported. (2) The DAG uses the deprecated 'dag' parameter in task definitions instead of the context manager pattern or proper DAG assignment. (3) HttpSensor endpoint configuration is incorrect - it should use a connection_id, not a full URL. While there are no test harness imports, the code would fail to execute due to these missing dependencies.",0,"The instruction specifically asks for 'time-based, file-based, bash command, and Python function sensors' to demonstrate conditional task execution. The code fails to meet this requirement: (1) No time-based sensor is included (e.g., TimeSensor or TimeDeltaSensor). (2) File-based sensor (FileSensor) is present but used incorrectly alongside a redundant PythonOperator. (3) No bash command SENSOR exists - BashOperator is an operator, not a sensor. The instruction asks for a sensor that waits for a bash command condition, which could be implemented with BashSensor (if available) or a custom sensor. (4) PythonSensor is imported but never used; instead, a PythonOperator is used. The code fundamentally misunderstands the difference between sensors (which wait/poll for conditions) and operators (which execute actions).",0.0
432,Baseline (Qwen 2.5 1.5B Instruct),110,0,"The code fails to use Airflow's native CosmosDB operators. Airflow 2.9.3 includes `airflow.providers.microsoft.azure.operators.cosmos` and `airflow.providers.microsoft.azure.sensors.cosmos` that should be used for CosmosDB operations. Instead, the code wraps CosmosDB logic in a PythonOperator with a custom function, which is a non-idiomatic pattern. Additionally, SparkSubmitOperator is inappropriately used for document insertion when a CosmosDB-specific operator would be more suitable.",0,"The code contains multiple undefined variables and references that would cause runtime errors: 'cosmos_client' is referenced but never defined or imported, 'timedelta' is used but not imported from datetime, and the code references paths and configurations ('/path/to/your/spark_script.py', 'your_database_name', etc.) that are placeholders but treated as if they were actual values. The code is not production-ready and would fail immediately upon execution.",0,"The instruction requires monitoring for document availability and triggering insertion based on dependency (ensuring document existence before write operations). However, the code has a logical flaw: it checks if a document exists and then always proceeds to insert, regardless of the check result. The PythonOperator's return value is not used to conditionally trigger the insertion. Additionally, the instruction implies inserting a document only if certain conditions are met (dependency-based workflow), but the code doesn't implement proper conditional logic using BranchPythonOperator or sensors. The use of SparkSubmitOperator for document insertion is also questionable and not aligned with typical CosmosDB write operations.",0.0
433,Baseline (Qwen 2.5 1.5B Instruct),112,0,The code uses PythonOperator with hooks (PostgresHook and SlackHook) instead of using the native operators designed for these tasks. Airflow 2.8.4 provides SqlToSlackOperator or at minimum PostgresOperator for SQL execution and SlackWebhookOperator for sending messages. The pattern of wrapping hook logic in PythonOperator is not idiomatic Airflow.,0,"While the code doesn't import test harness modules, it has critical structural issues that make it non-functional: (1) Hook instantiation and query execution happens at DAG parse time (module level) rather than at task execution time, (2) 'timedelta' is used but not imported, (3) References undefined task 'extract_data' in the dependency line, (4) Uses deprecated 'python_operator' import path instead of 'airflow.operators.python'. These are severe implementation errors that indicate the code is not production-ready.",0,"The code attempts to address the instruction (extract SQL data and send to Slack) but fails to implement it correctly. The SQL query execution happens at parse time instead of runtime, there's no actual 'extract_data' task defined despite being referenced in dependencies, and the lambda function capturing 'result' would fail since 'result' is evaluated at parse time. The workflow structure doesn't properly separate extraction and sending as distinct executable tasks.",0.0
434,Baseline (Qwen 2.5 1.5B Instruct),113,0,The code uses PythonOperator with S3Hook instead of the idiomatic LocalFilesystemToS3Operator. The pattern of wrapping S3Hook in a PythonOperator (upload_to_s3_function) is exactly the anti-pattern described in the scoring criteria. Airflow provides native operators like LocalFilesystemToS3Operator or S3CreateObjectOperator that should be used for S3 uploads instead of manually implementing the logic with hooks.,0,"The code references undefined functions (generate_text_file_function, upload_to_s3_function, cleanup_temp_file_function) that are never implemented in the provided code. Additionally, 'timedelta' is used but never imported from datetime module. The code is incomplete and would fail immediately if executed, making it non-production-ready.",1,"The DAG structure correctly addresses all three requirements from the instruction: (1) generates a sample text file, (2) uploads it to S3, and (3) performs cleanup operations. The task dependencies are properly defined in the correct sequence (generate >> upload >> cleanup), demonstrating the complete workflow requested.",0.3333333333333333
435,Baseline (Qwen 2.5 1.5B Instruct),114,0,"The code uses deprecated imports and non-idiomatic patterns. It imports 'airflow.operators.python_operator.PythonOperator' and 'airflow.sensors.http_sensor.HttpSensor' which are old-style imports. For Airflow 3.0.0, it should use 'airflow.operators.python.PythonOperator' and 'airflow.providers.http.sensors.http.HttpSensor'. More critically, it wraps HTTP requests in a PythonOperator with the requests library instead of using the native SimpleHttpOperator (or HttpOperator) from airflow.providers.http.operators.http, which is the idiomatic way to make HTTP requests in Airflow.",1,The code does not import any internal testing modules or include test harness boilerplate. It uses standard Airflow libraries (albeit with deprecated import paths) and does not contain hallucinated functions like 'get_test_run' or references to 'tests_common' modules. The code is clean from a hallucination perspective.,0,"The instruction specifies 'sequential HTTP request tasks', but the dependency chain shows 'check_http_service >> [make_http_request_task_1, make_http_request_task_2, make_http_request_task_3]', which means all three HTTP request tasks run in parallel after the sensor check, not sequentially. Sequential execution would require: check_http_service >> make_http_request_task_1 >> make_http_request_task_2 >> make_http_request_task_3. This is a clear deviation from the specified business logic.",0.3333333333333333
436,Baseline (Qwen 2.5 1.5B Instruct),115,0,The code uses PythonOperator with DlpHook instead of using native DLP operators. Airflow provides airflow.providers.google.cloud.operators.dlp with operators like CloudDLPCreateStoredInfoTypeOperator and CloudDLPCreateInspectTemplateOperator that should be used for DLP operations. The pattern of wrapping hook logic in a PythonOperator is not idiomatic when specific operators exist for the task.,0,"The code references a method 'create_custom_dictionary()' on DlpHook that does not exist in the standard Airflow Google provider library. The actual DlpHook does not have this method. Additionally, 'airflow.operators.python_operator' is deprecated in Airflow 3.0.0 - it should be 'airflow.operators.python' with 'PythonOperator' imported from there. The hook and method usage appears to be hallucinated or based on non-existent APIs.",0,"The instruction requires a comprehensive workflow that: (1) uploads custom dictionaries, (2) defines information type patterns, and (3) demonstrates programmatic management of data classification rules. The provided code only attempts to upload a single dictionary (and does so incorrectly). It does not define information type patterns, create inspect templates, or demonstrate a complete data classification rule management workflow. The pipeline is incomplete and missing critical components requested in the instruction.",0.0
437,Baseline (Qwen 2.5 1.5B Instruct),116,0,"The code references 'GoogleCloudDataprocSubmitJobOperator' which is not imported. Additionally, the hive_query_task uses an incorrect job structure with 'query' and 'language' fields that don't match the actual Dataproc Hive job API. The correct approach would be to use DataprocSubmitJobOperator (which is imported correctly in modern versions) with a proper 'hive_job' structure like {'hive_job': {'query_list': {'queries': [...]}}}, or use the specialized DataprocSubmitHiveJobOperator if available. The job definition is not idiomatic for Dataproc Hive jobs.",0,"The code uses 'GoogleCloudDataprocSubmitJobOperator' which is not imported at the top of the file. This operator is referenced but never defined, making the code non-executable. Additionally, 'datetime' is used in default_args but not imported. These are clear errors that would prevent the DAG from running.",0,"While the code attempts to create a multi-node cluster (2 master + 2 worker instances), execute a query, and delete the cluster (demonstrating on-demand infrastructure), it fails to properly implement the Hive query execution. The job structure is incorrect for Dataproc Hive jobs, and the operator is not imported. The instruction specifically asks for 'a simple Hive query' to be executed, but the implementation is broken. Additionally, the master_config specifies 'num_instances': 2, which is incorrect - Dataproc clusters typically have 1 master node, not 2.",0.0
438,Baseline (Qwen 2.5 1.5B Instruct),118,0,"The code imports 'GCSFileUploadCompletedSensor' which does not exist in the official Airflow Google Cloud provider package. The actual available sensors in airflow.providers.google.cloud.sensors.gcs are GCSObjectExistenceSensor, GCSObjectsWithPrefixExistenceSensor, GCSObjectUpdateSensor, and GCSUploadSessionCompleteSensor. This is a hallucinated/non-existent operator, making the code non-functional and not idiomatic.",0,"The code hallucinates a non-existent sensor 'GCSFileUploadCompletedSensor'. This sensor does not exist in Airflow 3.0.1 or any version of the Google Cloud provider. The available sensors are GCSObjectExistenceSensor, GCSObjectsWithPrefixExistenceSensor, GCSObjectUpdateSensor, and GCSUploadSessionCompleteSensor. Additionally, the code uses deprecated 'dag=dag' parameter passing instead of the context manager pattern or TaskFlow API, though this is a minor style issue compared to the hallucinated import.",0,"The instruction asks for 'synchronous and asynchronous methods' for detecting file uploads and object changes, showcasing 'different monitoring strategies'. While the code attempts to show multiple sensors, it fails because: (1) It uses a hallucinated sensor that doesn't exist, (2) It doesn't demonstrate async vs sync patterns (no deferrable=True parameter usage), (3) The 'different monitoring strategies' are just duplicate tasks with the same sensors rather than truly different approaches (e.g., prefix-based monitoring, update time monitoring, or async deferrable sensors), and (4) Missing GCSObjectUpdateSensor which would be appropriate for 'object changes' detection.",0.0
439,Baseline (Qwen 2.5 1.5B Instruct),120,0,"The code uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of the modern 'airflow.operators.python.PythonOperator'. More critically, the file monitoring logic is wrapped in a PythonOperator with an empty pass statement instead of using appropriate sensors like FileSensor from airflow.sensors.filesystem. The workflow should leverage Airflow's built-in file detection capabilities rather than custom Python functions.",1,"The code uses only standard Airflow libraries and providers (airflow.operators, airflow.providers.docker). There are no imports from internal testing modules or test harness boilerplate. While the implementation is incomplete with placeholder functions, it doesn't contain hallucinated or leaked test code.",0,"The code fails to implement the core business logic requested. All three critical functions (check_for_new_files, move_file_to_destination, print_file_contents) contain only 'pass' statements with no actual implementation. Additionally, there's a critical bug: 'file_path' is referenced in move_file_to_destination_task and print_file_contents_task but is never defined or passed between tasks. The workflow cannot detect files, move them, or print their contents as requested. The tasks are structurally connected but non-functional.",0.3333333333333333
440,Baseline (Qwen 2.5 1.5B Instruct),121,0,"The code fails the idiomatic Airflow criterion. It uses PythonOperator with TrinoHook wrapped in Python functions for all database operations, instead of using the native TrinoOperator from airflow.providers.trino.operators.trino. The idiomatic approach would be to use TrinoOperator directly for executing SQL queries, which is specifically designed for this purpose. Example: `TrinoOperator(task_id='create_schema', sql='CREATE SCHEMA...', trino_conn_id='trino_default')`",0,"The code has a hallucination issue: it imports `PythonOperator` from the deprecated path `airflow.operators.python_operator` instead of the correct path `airflow.operators.python` for Airflow 2.8.4. Additionally, the code references `timedelta` without importing it from datetime module. While not a test harness leak, these are incorrect imports that would cause runtime errors.",1,"The code fulfills the instruction requirements: it demonstrates sequential execution of Trino operations (create schema, define tables, insert data), performs both template-based and parameterized queries, shows dependency chaining with the >> operator, and includes error handling through the 'retries' configuration in default_args. All requested business logic components are present.",0.3333333333333333
441,Baseline (Qwen 2.5 1.5B Instruct),122,0,"The DAG fails the idiomatic Airflow criterion. It imports Hooks (S3Hook, GCSHook, AzureBlobStorageHook) and wraps them in PythonOperator tasks instead of using native Operators. Airflow provides specific operators like S3CreateBucketOperator, S3DeleteBucketOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator, and S3ToGCSOperator that should be used for these cloud resource operations. The pattern of using PythonOperator with Hooks is explicitly the anti-pattern described in the scoring criteria.",0,"The code references undefined functions (provision_aws_s3_bucket, data_transfer_from_aws_s3_to_gcp, cleanup_aws_s3_bucket, cleanup_gcp_bucket) that are never implemented or imported. While the imports themselves are valid Airflow libraries, the code is incomplete and would fail immediately upon execution. Additionally, it imports 'python_operator' which is deprecated in Airflow 3.0.1 (should be 'python' module), and imports AzureBlobStorageHook which is never used in the DAG.",1,"The DAG structure does fulfill the instruction requirements: it demonstrates (1) provisioning cloud resources (aws_s3_bucket_provisioner), (2) performing data transfer (aws_s3_to_gcp_data_transfer), and (3) cleaning up resources (aws_s3_bucket_cleanup and gcp_bucket_cleanup). The workflow dependencies are correctly defined to show the end-to-end process of provision → transfer → cleanup. Despite implementation issues, the logical structure matches the requested systematic workflow.",0.3333333333333333
442,Baseline (Qwen 2.5 1.5B Instruct),123,0,"The code uses PythonOperator with CohereHook instead of using a native Cohere operator if available. While CohereHook is appropriate, wrapping it in a PythonOperator is not idiomatic when a dedicated operator exists. The code follows the anti-pattern of 'def function + PythonOperator' instead of using provider-specific operators. Additionally, the task dependency and output passing pattern is incorrect (using embed_text_task.output and calling execute() manually).",0,"The code has several issues: (1) Missing import for 'timedelta' from datetime module, (2) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.1, (3) Hardcoded API key in code ('YOUR_COHERE_API_KEY') is not production-ready - should use Airflow connections, (4) The CohereHook initialization pattern appears incorrect - should use connection_id parameter, (5) Incorrect task output passing with 'embed_text_task.output' which is not how XCom works, (6) Manual execute() call in trigger_embedding_task is not proper Airflow pattern.",0,"While the code attempts to create embeddings using Cohere, it fails to properly implement a production-ready pipeline. The instruction asks to 'transform raw text data' but the code only handles a single hardcoded string ('Your raw text here') rather than processing actual raw text data from a source. There's no proper data ingestion, no batch processing capability, and the storage mechanism is just a print statement. The workflow logic is also broken with the unnecessary trigger_embedding_task that manually calls execute(). The DAG doesn't fulfill the requirement of preparing unstructured text for NLP tasks in any meaningful way.",0.0
443,Baseline (Qwen 2.5 1.5B Instruct),124,0,"The code uses PythonOperator with OracleHook instead of the native OracleOperator. Airflow provides airflow.providers.oracle.operators.oracle.OracleOperator and OracleStoredProcedureOperator specifically designed for executing SQL queries and stored procedures. The current implementation wraps hook logic in Python callables, which is the anti-pattern described in the scoring criteria.",1,"The code uses only standard Airflow libraries (airflow.providers.oracle.hooks.oracle.OracleHook, airflow.operators.python_operator.PythonOperator) and does not import any internal testing modules or include test harness boilerplate. The code is clean from a hallucination perspective.",0,"While the code attempts to demonstrate SQL query execution and stored procedure interactions, it has critical issues: 1) The stored procedure call references undefined variables 'param1' and 'param2' which would cause runtime errors, 2) The instruction asks to 'showcase different methods of executing database operations using Oracle connection parameters' but the code doesn't demonstrate different methods or showcase connection parameter usage, 3) The code is not production-ready due to the undefined variables. The DAG partially fulfills the intent but fails on execution correctness and completeness.",0.3333333333333333
444,Baseline (Qwen 2.5 1.5B Instruct),125,0,"While the DAG uses native GCS operators (GCSCreateBucketOperator, GCSToGCSOperator), the cleanup task incorrectly uses GCSCreateBucketOperator with a non-existent 'delete_objects' parameter to delete resources. The idiomatic approach would be to use GCSDeleteBucketOperator from airflow.providers.google.cloud.operators.gcs for cleanup. This is a misuse of the operator API.",0,"The code hallucinates a 'delete_objects' parameter for GCSCreateBucketOperator in the cleanup_resources_task. This parameter does not exist in the actual Airflow GCS provider API. The GCSCreateBucketOperator is designed to create buckets, not delete them or their contents.",0,"The DAG attempts to fulfill the instruction but fails in the cleanup phase. While it creates a bucket, transfers data using GCSToGCSOperator (specialized operator), it does not properly clean up cloud resources. The cleanup task uses the wrong operator (GCSCreateBucketOperator instead of GCSDeleteBucketOperator) and a non-existent parameter. Additionally, the destination bucket 'another-gcs-bucket' is never created, which would cause the transfer to fail. A complete solution should create both buckets and properly delete them after transfer.",0.0
445,Baseline (Qwen 2.5 1.5B Instruct),126,0,The code uses PythonOperator with PostgresHook wrapped in Python functions instead of using the native PostgresOperator (airflow.providers.postgres.operators.postgres.PostgresOperator) which is specifically designed for executing SQL queries. This is the anti-pattern described in the scoring criteria - wrapping Hook logic in PythonOperator instead of using the appropriate Operator.,0,"The code has several issues: 1) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' (correct for Airflow 3.x), 2) Missing 'from datetime import timedelta' import, 3) Functions (create_table, insert_data, select_data) are defined AFTER they are referenced in the PythonOperator calls, which would cause NameError at parse time, 4) Uses invalid 'depends_on' parameter in PythonOperator (should use 'set_upstream' or '>>' operator for dependencies). These are significant code quality issues that prevent the DAG from running.",1,"Despite implementation issues, the code does attempt to fulfill the instruction requirements: it creates a DAG with sequential SQL tasks (create table, insert data, select data), demonstrates task chaining with dependencies (using >> operator at the end), and shows a complete database operation lifecycle with atomic interactions. The business logic intent matches the instruction.",0.3333333333333333
446,Baseline (Qwen 2.5 1.5B Instruct),127,0,"The code uses PythonOperator with GoogleCloudStorageHook instead of using native GCS operators. Airflow provides specific operators like GCSListObjectsOperator, GCSToGCSOperator, and other GCS-specific operators from airflow.providers.google.cloud.operators.gcs that should be used for idiomatic Airflow code. The pattern of wrapping hook logic in PythonOperator is explicitly non-idiomatic.",0,"The code references undefined functions 'list_files' and 'transfer_files' that are never defined in the code. These functions are called in PythonOperators but don't exist, making this code non-functional. Additionally, 'airflow.operators.python_operator' is deprecated in Airflow 3.x (should be 'airflow.operators.python').",0,"The instruction requires: (1) time-span based file processing, (2) custom transformation script application, (3) automated setup, (4) transformation, (5) cleanup of cloud storage resources. The code fails on multiple counts: no time-span filtering logic is implemented, no actual transformation logic exists (just a print statement), no setup tasks are defined, and critically, no cleanup tasks are included. The hardcoded loop of 10 files doesn't implement dynamic time-span based selection.",0.0
447,Baseline (Qwen 2.5 1.5B Instruct),128,0,"The code uses AzureBlobStorageHook directly and calls its methods as if they were tasks, but Hooks are not Operators. The code should use proper Operators like LocalFilesystemToADLSOperator, AzureDataLakeStorageDeleteOperator, or WasbDeleteBlobOperator. Instead, it incorrectly treats Hook methods (upload_file_to_container, wait_for_blob_availability, delete_blob) as if they return task objects that can be chained with >>. This is not idiomatic Airflow - tasks must be created using Operators, not by calling Hook methods directly.",0,"The code contains multiple hallucinated methods that don't exist in the AzureBlobStorageHook class. Specifically, 'wait_for_blob_availability' is not a real method of AzureBlobStorageHook. Additionally, the code is missing the 'datetime' import which is used in default_args. The usage pattern itself (calling Hook methods directly as tasks) suggests a fundamental misunderstanding of the Airflow architecture.",0,"While the instruction asks for file upload and immediate deletion operations, the code does not properly implement these as executable tasks. The code attempts to create an upload task, a wait task, and a delete task, but none of these are actual Airflow tasks - they're just method calls on a Hook object. The DAG would fail to execute because no actual Operators are instantiated. A proper implementation would use operators like PythonOperator wrapping Hook calls, or native Azure operators for these operations.",0.0
448,Baseline (Qwen 2.5 1.5B Instruct),129,0,"The code does not use Airflow's Redis provider operators. Airflow 3.0.6 has `airflow.providers.redis` with operators like RedisPublishOperator and sensors like RedisKeySensor. Instead, the code wraps Redis logic in PythonOperator with manual Redis client instantiation, which is not idiomatic. Additionally, DummyOperator is incorrectly configured with `python_callable` parameter (which doesn't exist for DummyOperator), showing confusion between operator types.",0,"The code contains multiple errors that would prevent it from running: (1) Uses deprecated `airflow.operators.dummy_operator` and `airflow.operators.python_operator` instead of `airflow.operators.empty` and `airflow.operators.python` for Airflow 3.x, (2) DummyOperator is given a `python_callable` parameter which doesn't exist for that operator type, (3) `timedelta` is used without being imported, (4) `depends_on` parameter doesn't exist in Airflow operators (should use `>>` or `set_upstream`). While not test harness leakage, these are significant implementation errors.",0,"The DAG fails to properly implement the required workflow. The instruction asks to: (1) publish a start signal to Redis, (2) wait for a confirmation key before advancing. However, the implementation is broken: the publish task uses DummyOperator with invalid parameters instead of actually publishing, the wait task doesn't actually poll/wait for Redis confirmation (just a DummyOperator), and the process_confirmation task has incorrect trigger_rule='one_failed' which would only run on failure. There's no actual waiting/polling mechanism (should use RedisSensor or similar) to block until confirmation is received.",0.0
449,Baseline (Qwen 2.5 1.5B Instruct),130,0,"The code wraps Azure Synapse operations in PythonOperator functions using MSSQLHook instead of using native Azure Synapse operators. Airflow provides airflow.providers.microsoft.azure.operators.synapse (e.g., AzureSynapseRunPipelineOperator, AzureSynapseRunSparkBatchOperator) specifically designed for triggering and monitoring Synapse pipelines. The instruction explicitly asks to 'trigger and monitor the pipeline execution' which suggests using pipeline operators, not generic SQL operations wrapped in PythonOperator.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.0. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'datetime' and 'timedelta' are used but never imported. The code also references non-existent methods like 'hook.insert_rows(table=..., dataframe=...)' - MSSQLHook does not have an insert_rows method that accepts a dataframe parameter. These are implementation errors that would prevent the code from running.",0,"The instruction asks to 'trigger and monitor the pipeline execution' of a 'predefined data processing pipeline' in Azure Synapse. The code instead implements a custom ETL workflow using SQL queries against Synapse as a database, not triggering an existing Synapse pipeline. It also lacks proper error handling (no try-except blocks, no on_failure_callback) and logging (no explicit logging statements) as requested. The workflow does not monitor pipeline execution status, which is a key requirement for Synapse pipeline orchestration.",0.0
450,Baseline (Qwen 2.5 1.5B Instruct),131,0,"The code attempts to use WinRMOperator from 'airflow.providers.microsoft.windows.winrm', but this is not the correct import path. The actual operator in Airflow 2.9.3 is WinRMOperator from 'airflow.providers.microsoft.winrm.operators.winrm'. Additionally, the code uses the deprecated 'DummyOperator' instead of 'EmptyOperator'. The import path hallucination makes this non-idiomatic.",0,"The code contains a hallucinated import path 'airflow.providers.microsoft.windows.winrm' which does not exist in standard Airflow. The correct path should be 'airflow.providers.microsoft.winrm.operators.winrm'. Additionally, the code references 'winrm_tasks' list in the dependency setup section, but this list is never created - tasks are created in a loop but not stored. The code also uses non-existent parameters like 'log_cli' and 'catch_exceptions' which are not valid WinRMOperator parameters.",0,"The instruction asks for 'parallel task execution and dependency tracking' simulating 'real-world remote server management scenarios'. However, the code has critical flaws: (1) It uses zip() which pairs hosts with commands 1-to-1, creating only 3 tasks instead of executing all commands on all hosts; (2) The dependency setup references 'winrm_tasks' which is never defined, making the code non-functional; (3) The intended sequential dependencies contradict the requirement for 'parallel task execution'. The code does not fulfill the requirement for distributed task automation with proper parallel execution and dependency tracking.",0.0
451,Baseline (Qwen 2.5 1.5B Instruct),132,0,"While the code attempts to use Airflow providers and operators, it references multiple operators and sensors that do not exist in the official Airflow Google Cloud provider package. Specifically, 'BigtableInstanceOperator', 'BigtableTableCreationOperator', 'BigtableTableDeletionOperator', 'BigtableInstanceDeletionOperator', 'BigtableTableCreationSensor', and 'BigtableTableDeletionSensor' are not real operators/sensors in Airflow 2.9.3. The correct operators should be 'BigtableCreateInstanceOperator', 'BigtableCreateTableOperator', 'BigtableDeleteInstanceOperator', 'BigtableDeleteTableOperator', etc. from 'airflow.providers.google.cloud.operators.bigtable'. Additionally, 'BigQueryToBigtableOperator' does not exist as a standard transfer operator. The code shows intent to be idiomatic but uses incorrect/non-existent operator names.",0,"The code contains multiple hallucinated operators and sensors that do not exist in the Airflow codebase: 'BigtableInstanceOperator', 'BigtableTableCreationOperator', 'BigtableTableDeletionOperator', 'BigtableInstanceDeletionOperator', 'BigtableTableCreationSensor', 'BigtableTableDeletionSensor', and 'BigQueryToBigtableOperator'. Additionally, the code has syntax errors (missing datetime and timedelta imports, duplicate 'column_family_id' parameters in create_table task, invalid 'storage_type' value 'PROFILER' instead of 'SSD' or 'HDD'). These are fabricated components that would cause import and runtime errors.",0,"The instruction requires: (1) dynamic resource allocation, (2) replication monitoring, and (3) systematic teardown with configurable parameters. The code attempts teardown (delete tasks) but fails on the other requirements. There is no dynamic resource allocation (hardcoded values, no parameterization or Variables/Params usage). There is no replication monitoring - the 'monitor_table_health' task just checks if a table exists, not replication status or cluster health. The code lacks configurable parameters (all values are hardcoded strings). The workflow structure is also incomplete as it doesn't define task dependencies, making the execution order undefined.",0.0
452,Baseline (Qwen 2.5 1.5B Instruct),133,0,"The code uses BashOperator with AWS CLI and gsutil commands instead of using Airflow's native cloud provider operators. Idiomatic Airflow would use S3ToGCSOperator, S3CreateObjectOperator, S3DeleteObjectOperator, GCSDeleteObjectOperator, etc. from airflow.providers.amazon.aws and airflow.providers.google.cloud packages. Using bash commands bypasses Airflow's connection management, retry logic, and monitoring capabilities.",0,"The code has multiple issues: 1) Imports 'BashOperator' from 'airflow.operators' which is incorrect syntax for Airflow 3.0.1 (should be 'from airflow.operators.bash import BashOperator'), 2) Uses 'datetime' without importing it, 3) Uses deprecated 'dag' parameter in operator instantiation (Airflow 3.x uses context managers or default_args). These are not hallucinations per se but represent broken/non-functional code that wouldn't run in production.",0,"The instruction requires: 1) Transfer CSV between cloud storage platforms (S3 to GCS), 2) Robust testing mechanisms, 3) Teardown mechanisms, 4) Infrastructure-as-code principles. The code fails on multiple counts: a) Never actually transfers to GCS - only downloads from S3 to local, b) No testing mechanisms implemented, c) Teardown is partial (deletes from S3 and GCS but GCS upload is missing), d) The workflow doesn't complete the transfer pipeline (upload to S3 → download → delete, but no GCS upload step). The delete_from_gcs task is orphaned with no dependencies and references a file that was never uploaded to GCS.",0.0
453,Baseline (Qwen 2.5 1.5B Instruct),134,0,"The code uses BigQuery operators instead of Google Analytics Admin API operators. While it uses specific operators (not PythonOperator wrappers), they are completely wrong for the task. The instruction requires Google Analytics Admin API operators (e.g., GoogleAnalyticsAdminCreatePropertyOperator, GoogleAnalyticsAdminListAccountsOperator, etc.), but the code uses BigQuery operators (BigQueryCreateDatasetOperator, BigQueryDeleteDatasetOperator, etc.). This is a fundamental mismatch.",0,"The code contains multiple issues: (1) Missing imports for 'datetime' and 'timedelta', (2) Uses non-existent operators like 'BigQueryListDatasetsOperator' which doesn't exist in the standard Airflow providers, (3) Incorrect import path 'bigquery_operator' which is deprecated in Airflow 3.0.1 (should be from airflow.providers.google.cloud.operators.bigquery), (4) BigQueryInsertRowsOperator parameters are incorrect (mixes table insert with load job parameters). The code would not run as-is.",0,"The instruction explicitly asks for 'managing Google Analytics admin tasks, including creating, listing, and deleting properties and data streams' using 'Google Analytics Admin API'. The generated code completely ignores this and instead implements BigQuery dataset operations. There is zero alignment with the requested functionality - no Google Analytics properties, no data streams, no Analytics Admin API interaction whatsoever.",0.0
454,Baseline (Qwen 2.5 1.5B Instruct),135,0,"The code fails to use Yandex Cloud-specific operators. The instruction explicitly requests a 'Yandex Cloud data query workflow', but the code uses generic BashOperator with an echo command instead of the appropriate YandexCloudDataprocOperator or similar Yandex-specific provider. Additionally, it references BigQuery (a Google Cloud service) rather than Yandex Cloud services. For the final empty operator, it should use EmptyOperator (or DummyOperator in older versions) instead of BashOperator with an empty command.",0,"The code contains multiple issues: (1) Incorrect import statement 'from airflow.operators import BashOperator' - should be 'from airflow.operators.bash import BashOperator' for Airflow 3.0.6; (2) Missing imports for 'datetime' and 'timedelta'; (3) The bash_command has an unclosed quote in the SQL query string; (4) References BigQuery in comments and dag_id despite the instruction asking for Yandex Cloud. These are not hallucinations of test code, but represent broken/non-functional code that would fail on execution.",0,"The code fails to meet the core instruction requirement. The user explicitly requested a 'Yandex Cloud data query workflow', but the implementation uses BigQuery references and doesn't actually execute any real SQL query (just echoes a string). While it does include a final empty operator as requested, it uses BashOperator incorrectly instead of the proper EmptyOperator. The code does not demonstrate a functional Yandex Cloud integration or execute an actual SQL query against any Yandex service.",0.0
455,Baseline (Qwen 2.5 1.5B Instruct),136,0,"The code uses deprecated operators from Airflow 3.0.1. It imports 'PythonOperator' from 'airflow.operators.python_operator' and 'DummyOperator' from 'airflow.operators.dummy_operator', which were deprecated in Airflow 2.0+ and removed in 3.0. In Airflow 3.0.1, these should be imported from 'airflow.operators.python' and 'airflow.operators.empty' respectively. Additionally, BashOperator and EmailOperator are used without proper imports shown in the code.",0,"The code has multiple issues: (1) Missing imports for 'datetime', 'timedelta', 'BashOperator', and 'EmailOperator'. (2) Attempts to use '.output' attribute on operators (e.g., 'generate_random_number.output'), which doesn't exist in Airflow - XCom should be used instead. (3) The code references operators that aren't properly imported or defined. These are significant errors that would prevent the code from running.",0,"The instruction specifically asks for 'advanced branching techniques', 'dynamically select task paths', 'handle complex workflow routing', 'random selection', 'external Python execution', and 'virtual environment task execution' with 'conditional task routing'. However, the code implements a simple linear pipeline (generate_random_number >> check_even_odd >> print_result >> execute_command >> send_email) with no actual branching logic. There's no use of BranchPythonOperator, no conditional routing, no ExternalPythonOperator, and no PythonVirtualenvOperator. The code completely fails to demonstrate the core requirement of branching and conditional task path selection.",0.0
456,Baseline (Qwen 2.5 1.5B Instruct),137,0,"The code uses BashOperator with gcloud CLI commands instead of the idiomatic Airflow approach. For Google Cloud BigQuery operations, it should use 'airflow.providers.google.cloud.operators.bigquery.BigQueryInsertJobOperator' or similar BigQuery-specific operators from the Google Cloud provider package. Using BashOperator for database operations is not the Airflow best practice when native operators exist.",0,"The code imports 'BashOperator' from 'airflow.operators' which is incorrect for Airflow 3.0.0. The correct import should be 'from airflow.operators.bash import BashOperator'. Additionally, 'datetime' is used but never imported. The code also contains syntax errors (missing closing quote in commit_task bash_command) and would not execute successfully.",0,"The instruction asks for a 'controlled database transaction process with explicit start and cancellation steps' that 'ensures data consistency and prevents unintended persistent changes'. The DAG has a critical flaw: it chains commit_task >> rollback_task, meaning it would commit changes and then attempt to rollback, which makes no logical sense. A proper implementation should use branching logic or trigger rules to either commit OR rollback based on success/failure, not execute both sequentially. The current flow would persist changes (commit) before attempting rollback, failing to 'prevent unintended persistent changes' as requested.",0.0
457,Baseline (Qwen 2.5 1.5B Instruct),139,0,"The code fails to use idiomatic Airflow patterns. It creates a custom ElasticsearchHook class and wraps it in a PythonOperator instead of using the official airflow.providers.elasticsearch.hooks.elasticsearch.ElasticsearchHook and appropriate operators. For Airflow 2.8.4, the proper approach would be to use the Elasticsearch provider package (airflow-providers-elasticsearch) with its native hooks and operators, or at minimum use the existing ElasticsearchHook rather than creating a custom one.",0,"The code has several issues: (1) Missing imports for 'datetime' and 'timedelta' which are referenced but not imported, (2) Uses deprecated 'PythonOperator' import path - should be 'airflow.operators.python.PythonOperator' in Airflow 2.x, (3) The elasticsearch library is imported directly without using Airflow's provider pattern, (4) The code is incomplete and would not run as-is (ellipsis in vector data, incomplete vector dimensions). While not importing test harness code, it contains production-breaking issues.",0,"The instruction specifically asks for: (1) configurable parameters like index name, namespace, and batch size - the code hardcodes these values and doesn't implement batch size at all, (2) dynamically load vector embeddings from environment variables or predefined lists - the code only shows hardcoded vectors with no environment variable loading or dynamic loading mechanism, (3) demonstrates automated vector data insertion - while it attempts insertion, the implementation is incomplete and non-functional. The code misses key requirements around configurability and dynamic loading.",0.0
458,Baseline (Qwen 2.5 1.5B Instruct),140,0,"The code exclusively uses PythonOperator for all tasks (load_data, transform_data, write_data) instead of leveraging specific Airflow providers and operators designed for these operations. For enterprise BI workbook refresh scenarios, it should use providers like Tableau, Power BI, or database-specific operators (e.g., SnowflakeOperator, PostgresOperator). The pattern of wrapping everything in PythonOperator is not idiomatic Airflow.",0,"The code references undefined functions (load_data_from_csv, transform_data_function, write_data_to_database) that are never imported or defined, making it non-executable. Additionally, it uses the deprecated 'airflow.operators.python_operator' import path (should be 'airflow.operators.python' in Airflow 3.0.6). There are also syntax errors in the op_kwargs dictionaries (missing closing braces). While not test harness leakage, this code is not production-ready or clean.",0,"The instruction specifically asks for 'workbook refreshes' and 'monitor their completion status using different execution strategies' for enterprise BI reporting systems. The provided code is a generic ETL pipeline (load CSV → transform → write to database) with no workbook refresh functionality, no BI tool integration (Tableau, Power BI, Looker, etc.), no completion status monitoring, and no demonstration of different execution strategies (async, sync, polling, webhooks, etc.). It completely misses the core requirements.",0.0
459,Baseline (Qwen 2.5 1.5B Instruct),141,0,"The code fails to use idiomatic Airflow patterns. It uses BashOperator with AWS CLI commands instead of the dedicated AWS Glue DataBrew operators available in airflow.providers.amazon.aws.operators.glue_databrew (e.g., GlueDataBrewStartJobRunOperator). While it imports GlueHook and S3Hook, these are never used. The code wraps AWS CLI commands in BashOperator rather than using native Airflow providers designed for AWS Glue DataBrew operations.",0,"The code contains a critical error: it imports 'BashOperator' from 'airflow.operators' which is incorrect syntax for Airflow 3.0.6. The correct import should be 'from airflow.operators.bash import BashOperator'. Additionally, the code confuses AWS Glue (ETL jobs) with AWS Glue DataBrew (data preparation/profiling service). The bash commands use 'aws glue start-job' which is for Glue ETL jobs, not DataBrew profiling jobs. DataBrew requires different commands like 'aws databrew start-job-run'. The imports of GlueHook and S3Hook are never utilized, indicating incomplete or confused implementation.",0,"The instruction specifically requests an 'AWS Glue DataBrew data profiling workflow' with creating, executing, and cleaning up a data processing job. However, the code does not properly implement DataBrew operations. It uses generic Glue commands instead of DataBrew-specific operations (DataBrew datasets, recipes, profile jobs). The workflow lacks proper DataBrew resource creation (dataset, profile job creation), execution (profile job run), and cleanup (deleting DataBrew resources). The code attempts to delete a Glue Catalog table rather than DataBrew resources. It does not demonstrate end-to-end data preparation and profiling as requested.",0.0
460,Baseline (Qwen 2.5 1.5B Instruct),142,0,"The code uses outdated and incorrect operators. It imports 'PythonOperator' from the deprecated 'airflow.operators.python_operator' module (should be 'airflow.operators.python'). More critically, it uses 'MlEngineTrainOperator' which is for the legacy AI Platform (ML Engine), not for Vertex AI. For fine-tuning Gemini models on Vertex AI, it should use operators from 'airflow.providers.google.cloud.operators.vertex_ai' such as 'CreatePipelineJobOperator' or specific tuning operators. The 'GoogleCloudStorageToBigQueryOperator' is also misused - it's for loading data to BigQuery, not uploading to GCS.",0,"The code contains multiple issues: (1) Missing imports for 'datetime' and 'timedelta', (2) References undefined function 'your_final_function' without implementation, (3) Uses non-existent or incorrectly named operators like 'MlEngineTrainOperator' with parameters that don't match actual API (e.g., 'dataset_id', 'train_steps' are not valid parameters for this operator), (4) The 'GoogleCloudStorageToBigQueryOperator' usage doesn't align with its actual purpose or parameters. While not importing test utilities, the code is not production-ready due to these fabricated elements.",0,"The instruction specifically asks for fine-tuning a Google Vertex AI generative model (Gemini) using supervised learning. The code fails to use any Vertex AI operators and instead uses the legacy ML Engine operator which doesn't support Gemini model fine-tuning. It doesn't demonstrate the actual Vertex AI fine-tuning workflow, doesn't prepare training data in the required format for Gemini fine-tuning (JSONL format), and doesn't use the appropriate Vertex AI tuning job operators. The business logic for Vertex AI Gemini fine-tuning is not implemented.",0.0
461,Baseline (Qwen 2.5 1.5B Instruct),143,0,The code uses PythonOperator with TrinoHook for all SQL operations instead of using the native TrinoOperator from airflow.providers.trino.operators.trino. This is the anti-pattern described in the scoring criteria - wrapping hook logic in PythonOperator instead of using the purpose-built operator. The idiomatic approach would be: `from airflow.providers.trino.operators.trino import TrinoOperator` and then use it directly with SQL statements.,0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.1. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'datetime' is used but never imported (line with start_date). The code also instantiates the TrinoHook at the module level outside of task context, which is not a production-ready pattern as it creates the hook at parse time rather than execution time.",1,"The DAG fulfills all requirements from the instruction: (1) executes SQL operations sequentially against Trino, (2) demonstrates task dependencies with the >> operator, (3) showcases progressive data transformation (create schema → create table → insert data → query), (4) creates schemas, (5) generates tables, (6) inserts data, and (7) performs parameterized queries. The business logic requested is complete despite implementation issues.",0.3333333333333333
462,Baseline (Qwen 2.5 1.5B Instruct),144,0,"The code fails to use Airflow's Snowflake integration as requested. It uses a BashOperator with AWS CLI commands instead of native Snowflake operators like SnowflakeOperator or S3ToSnowflakeOperator from airflow.providers.snowflake. The instruction explicitly asks to use 'Airflow's Snowflake integration' for a 'robust data movement strategy', but the code only copies files to /tmp without any actual Snowflake loading logic. Additionally, it uses deprecated import syntax (from airflow.operators import FileSensor) instead of the proper provider-based imports.",0,"The code contains multiple issues: (1) Uses incorrect import paths - 'from airflow.operators import FileSensor, BashOperator' is not valid in Airflow 3.0.1; should be 'from airflow.sensors.filesystem import FileSensor' and 'from airflow.operators.bash import BashOperator'. (2) Missing import for 'datetime'. (3) FileSensor is used incorrectly for S3 paths - it's designed for local filesystem, not S3. (4) The dependency chain is backwards (load_data_to_snowflake >> check_files makes no logical sense). These are not hallucinations of test code, but rather fundamental errors that make the code non-functional.",0,"The code fails to implement the requested ETL process. The instruction asks to 'ingest CSV files from an external data source into a predefined Snowflake table', but the code only copies files to /tmp and never loads them into Snowflake. There is no Snowflake table creation, no COPY INTO command, no use of Snowflake operators, and no actual data loading logic. The task dependency is also reversed (checking files after loading makes no sense). The code is incomplete and does not fulfill the core requirement of loading data into Snowflake.",0.0
463,Baseline (Qwen 2.5 1.5B Instruct),145,0,"This code does not use any Airflow constructs whatsoever. There is no DAG definition, no operators, no tasks, and no task dependencies. Despite the import statement 'from airflow import DAG', the code never instantiates a DAG object or uses any Airflow operators. Instead, it directly calls Azure SDK methods at the module level, which would execute during DAG parsing rather than during task execution. A proper implementation should use Airflow's Azure providers (e.g., AzureDataFactoryRunPipelineOperator) to trigger and monitor Azure Data Factory pipelines as Airflow tasks.",0,"While the code doesn't import test utilities, it has critical issues: (1) It uses Azure Data Factory SDK directly instead of Airflow providers, (2) Contains syntax errors (e.g., 'your_data_factory_name' is not quoted as a string), (3) Uses undefined classes like 'PipelineTriggerRequest' which doesn't exist in the Azure SDK, (4) The code structure suggests it would execute during DAG parsing rather than task execution, which is fundamentally broken. The code appears to be a confused mix of Azure SDK examples rather than production-ready Airflow code.",0,"The instruction asks for an Airflow DAG that demonstrates scheduling and executing a data pipeline in Microsoft Fabric through cloud-native orchestration. This code fails on multiple fronts: (1) No actual DAG is defined or instantiated, (2) No Airflow scheduling is configured (no schedule_interval, start_date, or catchup parameters), (3) No tasks or operators are created, (4) The code references Azure Data Factory, not Microsoft Fabric (which has its own APIs and would typically use different integration patterns), (5) There is no demonstration of triggering data transformation or movement through Airflow's orchestration mechanism. The code is essentially raw Azure SDK calls without any Airflow orchestration layer.",0.0
464,Baseline (Qwen 2.5 1.5B Instruct),146,0,"The code uses DummyOperator throughout, which does nothing by default. It references python_callable and op_kwargs parameters that don't exist on DummyOperator (these belong to PythonOperator). The code should use native AWS operators like S3CreateBucketOperator, S3DeleteBucketOperator, and DataSyncOperators from airflow.providers.amazon.aws. Instead, it attempts to pass callables to DummyOperator, which is fundamentally incorrect and non-idiomatic.",0,"The code has multiple issues: (1) Missing import for 'datetime' while using datetime(2023, 1, 1), (2) References undefined functions (create_buckets, upload_to_source_bucket, sync_data, delete_buckets) that are never defined, (3) Incorrectly uses DummyOperator with python_callable parameter which doesn't exist on that operator. The code would fail immediately upon execution.",0,"The instruction specifically requests AWS DataSync task creation, execution, and cleanup between S3 buckets with complete lifecycle management. The code fails to implement any actual DataSync functionality - it only uses DummyOperators with no real logic. There's no DataSync task creation, no DataSync execution monitoring, and no proper AWS SDK integration. The instruction asks for DataSync (a specific AWS service for data transfer), not simple S3 sync operations.",0.0
465,Baseline (Qwen 2.5 1.5B Instruct),147,0,"The code uses non-existent or incorrect operators. In Airflow 3.0.1, there is no 'airflow.operators.s3_operator' module with operators like S3CreateBucketOperator, S3DownloadOperator, or S3UploadOperator. The correct approach would be to use operators from 'airflow.providers.amazon.aws.operators.s3' such as S3CreateBucketOperator (if it exists) or S3CopyObjectOperator for transfers. The imports are fundamentally wrong for the Airflow version specified.",0,"The code imports from 'airflow.operators.s3_operator' which does not exist in Airflow 3.0.1. The operators S3CreateBucketOperator, S3DeleteObjectOperator, S3DownloadOperator, and S3UploadOperator are either non-existent or incorrectly imported. Additionally, 'datetime' and 'timedelta' are used but never imported. This represents hallucinated/incorrect module paths and operators that are not part of standard Airflow libraries.",0,"The instruction asks for file transfer between cloud storage buckets with explicit error handling and resource management. The code attempts file operations but: (1) does not demonstrate actual transfer BETWEEN buckets (downloads from source but never uploads to destination), (2) lacks explicit error handling mechanisms (no on_failure_callback, no try-except patterns, no sensor validation), (3) does not show resource management beyond basic task dependencies, and (4) the workflow is incomplete as it only works with one bucket despite defining two. The core requirement of moving files between different storage locations is not fulfilled.",0.0
466,Baseline (Qwen 2.5 1.5B Instruct),148,0,"The code fails to use appropriate AWS-specific operators for EC2 provisioning and teardown. It uses DummyOperator for critical infrastructure tasks (start_task and stop_task) instead of using EC2CreateInstanceOperator and EC2TerminateInstanceOperator from airflow.providers.amazon.aws.operators.ec2. The stop_task incorrectly attempts to use 'bash_command' and 'env_vars' parameters on a DummyOperator, which don't exist for that operator. Additionally, it imports from a non-existent provider path 'airflow.providers.amazon.aws.ssh_operator' instead of the correct 'airflow.providers.ssh.operators.ssh'.",0,"The code contains multiple issues: (1) imports SSHOperator from a non-existent path 'airflow.providers.amazon.aws.ssh_operator' - the correct import is 'airflow.providers.ssh.operators.ssh', (2) uses deprecated 'DummyOperator' instead of 'EmptyOperator' for Airflow 2.7.2, (3) attempts to use non-existent parameters 'bash_command' and 'env_vars' on DummyOperator, (4) missing import for 'datetime', and (5) hardcoded placeholder credentials in code which is a security anti-pattern.",0,"The DAG fails to fulfill the core requirements: (1) Does not actually provision a compute resource - uses DummyOperator instead of EC2CreateInstanceOperator, (2) While it includes an SSH command execution step, it cannot work without a provisioned instance, (3) Does not systematically tear down infrastructure - the stop_task is a DummyOperator with invalid parameters that would not execute any teardown logic. The workflow structure is present but none of the actual infrastructure automation is implemented.",0.0
467,Baseline (Qwen 2.5 1.5B Instruct),149,0,The code uses BashOperator to execute a shell command instead of using the native InfluxDBOperator from airflow.providers.influxdb. This is the exact anti-pattern described in the scoring criteria - using a generic approach instead of the specific Provider/Operator designed for InfluxDB interaction.,1,"The code does not import any internal testing modules or include test harness boilerplate. It uses standard Airflow imports (though incorrectly, as BashOperator should be imported from airflow.operators.bash).",0,"The instruction explicitly requests using 'InfluxDBOperator' to execute a 'Flux query' with a 'configurable time window' from a 'predefined bucket'. The code fails on multiple counts: (1) Does not use InfluxDBOperator at all, (2) Uses InfluxQL syntax instead of Flux query language, (3) References 'database' and 'measurement' (InfluxDB 1.x concepts) instead of 'bucket' (InfluxDB 2.x concept), (4) Has a hardcoded time window ('now() - 1h') instead of a configurable parameter, and (5) Does not demonstrate proper connection setup to InfluxDB.",0.3333333333333333
468,Baseline (Qwen 2.5 1.5B Instruct),153,0,"The code attempts to use a specific provider operator (CloudTaskQueueSensor), which would normally be idiomatic. However, 'airflow.providers.google.cloud.sensors.gcp_taskqueue.CloudTaskQueueSensor' does not exist in the Airflow codebase. The correct sensor for Google Cloud Tasks would be 'CloudTasksQueueEmptySensor' from 'airflow.providers.google.cloud.sensors.tasks'. This is a hallucinated/incorrect import that makes the code non-functional.",0,"The code contains multiple issues: (1) Imports a non-existent sensor 'CloudTaskQueueSensor' from 'airflow.providers.google.cloud.sensors.gcp_taskqueue' - this module/class does not exist in Airflow. (2) Uses 'airflow.operators.BashOperator' which is imported but never used. (3) The import should be 'from airflow.operators.bash import BashOperator' for Airflow 2.x. (4) Missing 'datetime' and 'timedelta' imports. (5) Contains procedural logic outside of tasks (the if/else block checking 'past_state') which is not how Airflow DAGs work - this code would execute at DAG parse time, not runtime.",0,"The instruction asks to 'validate the completion status' and 'pause workflow execution until the designated queue is completely empty'. While the code attempts to use a sensor (correct approach), it fails to implement proper validation or any downstream tasks that would execute after the queue is empty. The if/else logic at the bottom is procedural code that runs at parse time, not a task that validates completion. A proper implementation would need the sensor followed by validation/notification tasks that run after the queue empties.",0.0
469,Baseline (Qwen 2.5 1.5B Instruct),154,0,"The code uses BashOperator incorrectly instead of using WinRM-specific operators or PythonOperator with WinRMHook. It attempts to call run_command() at DAG parse time (not task execution time) and passes the result to BashOperator, which is fundamentally wrong. The idiomatic approach would be to use PythonOperator with WinRMHook.run_cmd() method, or create a custom operator. The code also instantiates the hook at module level rather than within task execution context.",0,"The code has multiple issues: (1) Missing imports for datetime, timedelta, and BashOperator. (2) The WinRMHook is instantiated with incorrect parameters - it should use a connection_id, not direct credentials. (3) The run_command function is called at parse time, not execution time, which will fail. (4) BashOperator doesn't have an 'output' attribute that can be accessed as shown in the consolidate_task. (5) The code attempts to execute WinRM commands but passes them to BashOperator which executes bash commands locally, not on Windows machines.",0,"The instruction requires: (1) Remote task scheduling using WinRM hooks to connect to Windows machines, (2) Parallel execution pattern with different command types, and (3) Final consolidation task. The code attempts to address these but fails fundamentally: tasks don't actually execute on remote Windows machines (BashOperator runs locally), there's no proper parallel execution pattern defined (no task dependencies showing parallel execution), and the consolidation task doesn't actually consolidate outputs (it tries to access non-existent .output attributes). The code doesn't demonstrate proper WinRM remote execution at all.",0.0
470,Baseline (Qwen 2.5 1.5B Instruct),155,0,"The code imports and attempts to use 'GCPJobOperator' and 'GCPRunOperator' which do not exist in the standard Airflow Google Cloud provider package. The correct operators for Cloud Run in Airflow 3.0.6 would be from 'airflow.providers.google.cloud.operators.cloud_run' (e.g., CloudRunCreateJobOperator, CloudRunExecuteJobOperator, CloudRunDeleteJobOperator). Additionally, the code imports operators like GCSToGCSOperator, GoogleCloudFunctionOperator, and GoogleCloudStorageToBigQueryOperator that are never used, showing poor understanding of the required operators.",0,"The code hallucinates non-existent operators. 'GCPJobOperator' and 'GCPRunOperator' are not real Airflow operators. The imports reference classes that don't exist in the Airflow provider packages. Additionally, the code is missing required imports (datetime, timedelta) which would cause runtime errors. This represents fabricated API surface area rather than using actual Airflow libraries.",0,"The instruction specifically requests 'containerized jobs using Google Cloud Run operators' with a focus on 'job creation, execution, modification, and cleanup'. Cloud Run has two distinct products: Services and Jobs. The instruction clearly refers to Jobs (containerized batch workloads), but the code attempts to manage Services (long-running HTTP services) with operations like 'create_service', 'deploy_service', and scaling replicas. The code misses the key requirement of job execution entirely and doesn't demonstrate the complete lifecycle of Cloud Run Jobs (create job definition, execute job, update job, delete job) with proper error checking and dependencies.",0.0
471,Baseline (Qwen 2.5 1.5B Instruct),156,0,"The DAG uses only DummyOperator for all tasks instead of using specific Airflow providers and operators designed for the respective cloud services. For a production ML lifecycle DAG, it should use operators like S3ToPostgresOperator, BigQueryCreateEmptyDatasetOperator, SageMakerTrainingOperator, SageMakerEndpointOperator, etc. The code imports hooks (S3Hook, PostgresHook, BigQueryHook) but never uses them, and doesn't implement any actual logic with proper operators. This is the opposite of idiomatic Airflow - it's essentially a skeleton with no real functionality.",1,"The code only imports standard Airflow libraries (airflow.operators.dummy_operator, airflow.providers modules). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'. While the code is non-functional, it doesn't contain hallucinated or leaked test code.",0,"The instruction explicitly asks to 'orchestrate a complete machine learning lifecycle' and 'demonstrate how to manage cloud resources, transfer data, create datasets, and handle model training and deployment' while showcasing 'how to integrate multiple cloud services and operators'. The provided code uses only DummyOperator placeholders with params that are never actually used. It does not demonstrate any actual integration with cloud services, does not transfer data, does not create datasets, and does not train or deploy models. It's merely a skeleton structure that outlines task names but provides no actual implementation of the requested ML lifecycle orchestration.",0.3333333333333333
472,Baseline (Qwen 2.5 1.5B Instruct),157,0,"The code relies entirely on PythonOperator for all tasks (data loading, validation, and reporting). This is not idiomatic Airflow. For file operations, it should use appropriate operators like BashOperator or sensors. For email/reporting, it should use EmailOperator or other notification operators from providers. The pattern of wrapping all logic in PythonOperator callables instead of using purpose-built operators is the anti-pattern described in the scoring criteria.",0,"Multiple critical issues: (1) Uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.6. (2) Functions are defined AFTER they are referenced in the PythonOperator definitions, which will cause NameError at parse time. (3) The code attempts to pass data between tasks using '.output' attribute (e.g., 'load_data.output') which is not how Airflow works - this would fail at runtime. (4) Missing required DAG parameters like 'start_date'. (5) The 'send_error_report_function' is referenced but the actual function is named 'send_error_report', causing another NameError.",0,"The instruction requires: (1) daily data validation - partially addressed with schedule_interval, (2) generate reports with error tracking - only prints to console, no actual report generation, (3) automatically detect data integrity issues - validation logic is superficial (only checks empty columns), (4) trigger appropriate error communication or reporting mechanisms - no actual communication mechanism implemented (email, Slack, etc.), just a print statement. The code is a skeleton that doesn't fulfill the core requirements of automated error detection, reporting, and communication.",0.0
473,Baseline (Qwen 2.5 1.5B Instruct),158,0,"The code uses 'DummyOperator' which is deprecated in Airflow 3.0.1. The correct operator is 'EmptyOperator' from 'airflow.operators.empty'. While it does use HttpSensor (a proper sensor operator), the use of deprecated operators makes this not idiomatic for the specified Airflow version.",0,"The code is missing critical imports: 'datetime' and 'timedelta' are used but not imported. This would cause the DAG to fail at parse time. Additionally, 'DummyOperator' doesn't exist in Airflow 3.0.1 (it was removed in favor of EmptyOperator), which would cause an ImportError.",0,"The instruction specifically asks to 'demonstrate how to conditionally control task execution using Airflow's sensor decorator'. The code does not use the sensor decorator (@task.sensor) at all - it uses the traditional HttpSensor operator class instead. While it does show a sensor checking for data availability before processing, it fails to demonstrate the decorator pattern that was explicitly requested.",0.0
474,Baseline (Qwen 2.5 1.5B Instruct),159,0,"The code uses the deprecated `PostgresOperator` from `airflow.operators.postgres_operator` instead of the modern, idiomatic operators designed for data quality checks. For Airflow 2.7.2, the code should use `SQLColumnCheckOperator` and `SQLTableCheckOperator` from `airflow.providers.common.sql.operators.sql` which are specifically designed for data validation tasks as mentioned in the instruction ('column and table check operators'). The current implementation uses generic SQL execution operators rather than the purpose-built validation operators.",0,"Multiple issues exist: (1) Uses deprecated import path `airflow.operators.postgres_operator` which doesn't exist in Airflow 2.7.2 (should be `airflow.providers.postgres.operators.postgres`), (2) Missing import for `timedelta`, (3) Incorrectly uses a connection string as `postgres_conn_id` value instead of a connection ID reference, (4) SQL syntax error in the first query (misplaced semicolon inside subquery). These are not hallucinations of test code, but the code is not production-ready due to these errors.",0,"The instruction specifically asks to 'demonstrate SQL-based data validation techniques using column and table check operators'. The code fails to use the appropriate `SQLColumnCheckOperator` and `SQLTableCheckOperator` that are designed for data quality validation in Airflow. Instead, it uses generic PostgresOperator for SQL execution. Additionally, the dependency chain is reversed (check_conditions should run first, not last based on the set_upstream calls), which doesn't represent a logical sequential validation flow.",0.0
475,Baseline (Qwen 2.5 1.5B Instruct),160,0,"The code uses GCSHook methods (create_bucket_if_not_exists, delete_bucket) directly as tasks instead of using proper Airflow Operators. These hook methods return values, not Airflow task objects, and cannot be used in task dependencies. The idiomatic approach would be to use GCSCreateBucketOperator and GCSDeleteBucketOperator from airflow.providers.google.cloud.operators.gcs, or wrap hook calls in PythonOperator if no native operator exists. The code fundamentally misunderstands how to create tasks in Airflow.",1,"The code uses only standard Airflow and Google Cloud provider libraries. There are no imports from internal testing modules or test harness boilerplate. All imports (DAG, GCSToGCSOperator, GCSHook, days_ago) are legitimate Airflow components, even though they are used incorrectly.",0,"The instruction requires 'bucket creation, data extraction, and post-transfer cleanup with error handling and observability.' While the code attempts bucket creation, data transfer, and cleanup, it fails on multiple fronts: (1) No error handling mechanisms (try/except, on_failure_callback, trigger rules) are implemented, (2) No observability features (logging, monitoring, alerts, or task sensors) are included, (3) The code is fundamentally broken as hook methods cannot be used as tasks. The instruction's core requirements for error handling and observability are completely missing.",0.3333333333333333
476,Baseline (Qwen 2.5 1.5B Instruct),161,0,"The code uses deprecated 'PythonOperator' from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. More critically, it uses 'dag.create_task_group()' which is not a valid Airflow API - the correct approach is 'from airflow.utils.task_group import TaskGroup' and using context managers. For Airflow 3.0.0, the code should use the @task decorator pattern or proper TaskGroup imports, not an invented 'create_task_group' method.",0,"The code invents a non-existent API method 'dag.create_task_group()' which does not exist in any version of Airflow. The correct API is 'TaskGroup' from 'airflow.utils.task_group'. Additionally, the setup() and teardown() functions are called directly as regular Python functions during DAG parsing, not as Airflow tasks with the @setup and @teardown decorators as requested in the instruction.",0,"The instruction explicitly requested 'explicit setup/teardown decorators' to demonstrate lifecycle management and automatic resource management. The code does not use Airflow's @setup and @teardown decorators (available in Airflow 2.7+/3.0.0) at all. Instead, it calls regular Python functions during DAG parsing that print messages but don't actually participate in the task execution lifecycle. The code also fails to demonstrate 'nested task groups' properly - it creates two separate task groups at the same level rather than nesting one inside another. The task dependencies are also backwards (using set_upstream incorrectly).",0.0
477,Baseline (Qwen 2.5 1.5B Instruct),162,0,"While the code does use GCSToGCSOperator (which is good), it uses the deprecated 'PythonOperator' instead of the modern 'PythonOperator' from the correct import path. More critically, the preprocessing logic wraps functionality in a PythonOperator instead of using appropriate operators for image processing tasks. For ML preprocessing on GCS, idiomatic Airflow would use operators like DataprocSubmitJobOperator, DataflowTemplatedJobStartOperator, or CloudRunExecuteJobOperator rather than a generic PythonOperator.",0,"The code has several critical issues: (1) Uses incorrect import path 'airflow.providers.google.cloud.transfers.gcs_to_gcs_operator' instead of 'airflow.providers.google.cloud.transfers.gcs_to_gcs', (2) Imports 'gcs_hook' but never uses it, (3) Uses 'timedelta' without importing it from datetime, (4) The syntax 'preprocess_task >> [PythonOperator(...) for image in transfer_task.output]' is invalid - 'preprocess_task' is undefined and 'transfer_task.output' doesn't exist in this context. (5) Uses deprecated 'python_operator' module. The code would fail immediately upon execution.",0,"The instruction requires: (1) transfer image files between cloud storage buckets, (2) prepare them for ML-based product recognition, (3) manage image datasets AND product metadata. The code only partially addresses image transfer and has a placeholder for preprocessing. It completely misses product metadata management, which is a key requirement. The preprocessing logic is non-functional (just a print statement) and the dynamic task generation is syntactically broken. There's no actual ML preparation pipeline, no metadata handling, and no reproducible workflow for product recognition.",0.0
478,Baseline (Qwen 2.5 1.5B Instruct),163,0,"The code uses completely fabricated operators and sensors that do not exist in Airflow 2.7.2. There is no 'AwsSQSServiceOperator' or 'AwsSQSSensor' with the parameters shown (state='active'/'inactive'). The correct approach would be to use CloudFormationCreateStackOperator and CloudFormationDeleteStackOperator from airflow.providers.amazon.aws.operators.cloudformation, or for direct SQS management, use SqsCreateQueueOperator and SqsDeleteQueueOperator from airflow.providers.amazon.aws.operators.sqs (which exist but have different parameters). The imports themselves are incorrect paths.",0,"The code hallucinates non-existent operators and sensors. 'AwsSQSServiceOperator' and 'AwsSQSSensor' with 'state' parameters ('active'/'inactive') do not exist in the Airflow AWS provider package. The import paths 'airflow.providers.amazon.aws.operators.sqs_operator' and 'airflow.providers.amazon.aws.sensors.sqs_sensor' are also incorrect. Real operators like SqsCreateQueueOperator, SqsDeleteQueueOperator, and SqsSensor exist but have different interfaces and import paths.",0,"The instruction specifically asks for 'CloudFormation stack management workflow' to demonstrate 'infrastructure provisioning and teardown'. The code attempts to use SQS operators directly rather than CloudFormation operators (CloudFormationCreateStackOperator, CloudFormationDeleteStackOperator, CloudFormationCreateStackSensor). While it does show create/delete sequential steps for SQS, it completely misses the CloudFormation requirement, which is the core of the instruction.",0.0
479,Baseline (Qwen 2.5 1.5B Instruct),164,0,"The code uses FileSensor correctly, but imports and uses 'ExternalTimeSensor' from 'airflow.sensors.time_based', which does not exist in Airflow 2.8.4. The correct sensor for time-based waiting would be TimeSensor or TimeDeltaSensor from airflow.sensors.time_sensor. Additionally, PythonOperator is used but never imported, which would cause a runtime error. The code attempts to use idiomatic operators but fails due to incorrect imports and missing imports.",0,"The code hallucinates a non-existent module 'airflow.sensors.time_based' and a non-existent sensor 'ExternalTimeSensor'. In Airflow 2.8.4, there is no such module or sensor. The correct import would be from airflow.sensors.time_sensor or airflow.sensors.date_time. Additionally, PythonOperator is used without being imported, indicating incomplete or hallucinated code.",0,"While the code attempts to demonstrate various sensor types (file checks, time-based conditions) and shows conditional triggering of downstream tasks, it fails to execute properly due to incorrect imports and missing operator definitions. The instruction asks for 'various sensor types' and 'different sensor behaviors', but the code only shows FileSensor (used twice) and a hallucinated ExternalTimeSensor. It lacks diversity in sensor types (e.g., no DateTimeSensor, TimeDeltaSensor, or other valid sensors like HttpSensor, SqlSensor). The code also doesn't demonstrate different sensor behaviors like poke vs reschedule mode comprehensively.",0.0
480,Baseline (Qwen 2.5 1.5B Instruct),165,0,"The code does not use the appropriate operators for workflow template management. The instruction specifically asks for 'creating and executing a Google Dataproc workflow template' and 'asynchronous job instantiation and template management', but the code uses CreateDataprocClusterOperator and SubmitJobToDataprocOperator instead. The idiomatic approach would be to use DataprocInstantiateWorkflowTemplateOperator or DataprocCreateWorkflowTemplateOperator from airflow.providers.google.cloud.operators.dataproc for workflow template operations, which is the specific feature requested.",1,"The code uses only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules, no test harness boilerplate, and no hallucinated functions or classes. All imports and operators are legitimate Airflow 2.7.2 components.",0,"The instruction explicitly requests a workflow that demonstrates 'creating and executing a Google Dataproc workflow template' with 'asynchronous job instantiation and template management'. The provided code creates a cluster and submits a job directly, but does not create or use a workflow template at all. Workflow templates are a specific Dataproc feature that allows reusable job configurations, and the code completely misses this requirement. Additionally, the pig_job_config is incorrectly structured (uses mainJarFileUri instead of queryFileUri for Pig jobs).",0.3333333333333333
481,Baseline (Qwen 2.5 1.5B Instruct),166,0,"The code imports GCS-specific operators (GCSToGCSOperator, LocalFilesystemToGCSOperator, GCSToFilesystemOperator) but never uses them. Instead, it uses BashOperator (which is not even imported) with raw gsutil and shell commands. This is the opposite of idiomatic Airflow - it should use the native GCS operators that were imported for bucket creation, file transfers, and cleanup operations.",0,"The code references BashOperator three times but never imports it from airflow.operators.bash. This will cause an immediate NameError at runtime. While not a test harness hallucination, this is a critical code error that makes it non-production-ready.",0,"The instruction explicitly requires 'ensures complete resource cleanup regardless of task outcomes' which implies the use of trigger rules (e.g., trigger_rule='all_done') or try-finally patterns to guarantee cleanup even on failure. The current linear dependency (init >> transfer >> cleanup) means cleanup will NOT run if transfer fails, violating the core requirement of guaranteed cleanup. Additionally, the instruction asks for 'cross-platform file transfer' between cloud storage systems, but the implementation only uses GCS commands without demonstrating transfer between different cloud platforms (e.g., GCS to S3, or Azure to GCS).",0.0
482,Baseline (Qwen 2.5 1.5B Instruct),167,1,"The code correctly uses the SQLExecuteQueryOperator from airflow.providers.postgres.operators.sql, which is the idiomatic and recommended operator for executing SQL queries in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.DAG and airflow.providers.postgres.operators.sql.SQLExecuteQueryOperator). There are no imports of internal testing modules, test harness boilerplate, or any hallucinated/non-existent modules.",1,"The DAG fully adheres to the instruction. It demonstrates basic SQL query execution against a Postgres database (which can be the Airflow metadata database), shows how to connect using postgres_conn_id, runs multiple SQL statements (CREATE TABLE, INSERT, SELECT), and showcases the SQLExecuteQueryOperator's capabilities for executing database queries. The workflow has proper task dependencies and demonstrates a complete use case.",1.0
483,Baseline (Qwen 2.5 1.5B Instruct),168,0,"The DAG fails idiomatic Airflow practices. While it correctly uses SnowflakeOperator for the Snowflake task, it wraps the S3 and GCS operations in PythonOperators with hooks instead of using native operators. The code should use S3ToGCSOperator or similar transfer operators from airflow.providers.amazon.aws.transfers or airflow.providers.google.cloud.transfers. Additionally, for NoSQL export to S3, it should use specific operators like MongoToS3Operator or similar depending on the NoSQL database type, rather than a generic PythonOperator.",1,"The code uses only standard Airflow libraries and providers (airflow.operators.python_operator, airflow.providers.amazon.aws.hooks.s3, airflow.providers.google.cloud.hooks.gcs, airflow.providers.snowflake.operators.snowflake). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective.",0,"The DAG has critical implementation gaps that prevent it from fulfilling the instruction. The python_callable functions (export_data_to_s3 and upload_data_to_gcs) are defined after they are referenced in the operators, which will cause a NameError. More importantly, these functions only contain 'pass' statements with no actual implementation logic. The SnowflakeOperator uses a hardcoded INSERT statement with placeholder values rather than loading data from GCS. The instruction requires a 'complete data pipeline' with 'seamless data processing,' but this code provides only a skeleton without functional data movement between NoSQL → S3 → GCS → Snowflake.",0.3333333333333333
484,Baseline (Qwen 2.5 1.5B Instruct),169,0,"The code uses S3UploadFileOperator and S3ListObjectsOperator which do not exist in the airflow.providers.amazon.aws.operators.s3 module. The correct operators are S3CreateObjectOperator for uploading and S3ListOperator for listing. Additionally, S3UploadFileOperator is not a standard operator - the idiomatic approach would be to use S3CreateObjectOperator or S3FileTransformOperator. The code attempts to use specific operators but references non-existent ones, which violates idiomatic Airflow practices.",0,"The code imports S3UploadFileOperator and S3ListObjectsOperator which are not real operators in the Airflow AWS provider package. These are hallucinated/non-existent classes. The actual operators available are S3CreateObjectOperator, S3CopyObjectOperator, S3DeleteObjectsOperator, and S3ListOperator (not S3ListObjectsOperator). This constitutes hallucination of non-existent Airflow components.",1,"Despite the technical implementation issues, the DAG structure does attempt to fulfill the instruction requirements: it creates a bucket (S3CreateBucketOperator), uploads sample data (upload_file_task), and performs metadata operations (list_objects_task). The logical flow and intent align with the instruction to demonstrate S3 file processing, storage management, bucket creation, data upload, and metadata operations as a learning exercise for programmatic cloud storage interaction.",0.3333333333333333
485,Baseline (Qwen 2.5 1.5B Instruct),171,0,"The code uses PythonOperator with PinotHook instead of using native Pinot operators. This is the anti-pattern described in the scoring criteria - wrapping hook logic in PythonOperator rather than using specialized operators. While Airflow's Pinot provider may have limited operators, the idiomatic approach would be to use any available operators (like PinotAdminOperator or similar) or at least structure the code more appropriately if operators don't exist for these specific use cases.",0,"The code has several issues: (1) Missing import for 'timedelta' from datetime module, (2) Functions 'execute_admin_command_function' and 'retrieve_database_info_function' are defined AFTER they are referenced in the PythonOperator calls, which would cause a NameError, (3) The PinotHook methods used (run_query, get_table_metadata) may not match the actual API of the PinotHook in Airflow 2.9.3, (4) The 'start_date' is a string instead of a datetime object. These are not hallucinations of test code, but represent broken/non-production-ready code.",0,"The instruction asks for 'administrative commands and retrieves database information using specialized Pinot hooks' for 'data exploration and management'. While the code attempts to execute admin commands and retrieve database info, the implementation is fundamentally broken (functions defined after use, incorrect method calls, wrong data types). The code structure suggests intent to follow instructions, but the execution is so flawed that it would not run and therefore does not fulfill the requirement of demonstrating 'system-level and query-level interactions with Apache Pinot'.",0.0
486,Baseline (Qwen 2.5 1.5B Instruct),172,0,"The code imports 'DataFlowTemplateOperator' from 'airflow.providers.google.cloud.templates.dataflow', which is not a valid import path. The correct operator for Airflow 3.0.6 is 'DataflowStartFlexTemplateOperator' or 'DataflowTemplatedJobStartOperator' from 'airflow.providers.google.cloud.operators.dataflow'. This is a critical error that prevents the code from running.",0,The import statement 'from airflow.providers.google.cloud.templates.dataflow import DataFlowTemplateOperator' is hallucinated. This module path and operator do not exist in the standard Airflow providers package. The correct imports should be from 'airflow.providers.google.cloud.operators.dataflow'.,0,"While the code attempts to address the user instruction by uploading files and executing a Dataflow job, it has several issues: (1) The GCSToGCSOperator is used incorrectly - it moves files between buckets rather than uploading from local, which doesn't align with 'uploads source files'; (2) The template_path parameter points to a Python file ('/path/to/dataflow/template.py') when it should point to a GCS path for a Dataflow template; (3) Missing bucket management tasks as requested; (4) No task dependencies are defined. The code does not fully implement the requested workflow.",0.0
487,Baseline (Qwen 2.5 1.5B Instruct),173,1,"The code uses native Airflow operators from official providers: GKECreateClusterOperator, GKEDeleteClusterOperator, DataprocCreateClusterOperator, and DataprocDeleteClusterOperator. It does not wrap logic in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'datetime' in default_args (start_date: datetime(2023, 4, 1)) but does not include 'from datetime import datetime' in the imports section. Only 'from datetime import timedelta' is imported, which will cause a NameError at runtime. This is a code quality issue that prevents production readiness.",0,"The instruction explicitly asks to 'spin up a Dataproc cluster within that Kubernetes environment' (GKE), demonstrating infrastructure-as-code with coordinated workflow. However, the DAG creates two independent workflows: (1) GKE create->delete and (2) Dataproc create->delete, with no dependency between them. The Dataproc cluster is not configured to run on GKE (Dataproc-on-GKE), and there's no orchestration showing the Dataproc cluster being created within/after the GKE cluster. The task dependencies should be: create_gke_cluster >> create_dataproc_cluster >> delete_dataproc_cluster >> delete_gke_cluster to show coordinated lifecycle management.",0.3333333333333333
488,Baseline (Qwen 2.5 1.5B Instruct),174,0,"The DAG fails to use the appropriate operator for exporting Firestore data to GCS. The instruction requires 'captures a specific Firestore collection, exports it to cloud storage' but the code uses GCSToBigQueryOperator for the 'export_firestore_to_gcs' task, which is incorrect. The proper idiomatic approach would be to use CloudFirestoreExportDatabaseOperator or a similar Firestore export operator to actually export Firestore data to GCS. The code assumes data already exists in GCS and only performs GCS-to-BigQuery transfers.",1,"The code uses only standard Airflow providers and operators (airflow.providers.google.cloud). There are no imports from internal testing modules or test harness boilerplate. While the operators are misused, they are legitimate Airflow components.",0,"The DAG fails to implement the core requirement of the instruction. The user requested a process that 'captures a specific Firestore collection, exports it to cloud storage' but the code never actually exports Firestore data to GCS. The 'export_firestore_to_gcs' task is misnamed and actually uses GCSToBigQueryOperator, which assumes data already exists in GCS. Additionally, there are two identical GCSToBigQueryOperator tasks (gcs_export_task and bq_load_task) which is redundant. The DAG is missing the critical Firestore export step entirely.",0.3333333333333333
489,Baseline (Qwen 2.5 1.5B Instruct),175,0,"The code uses PythonOperator for all tasks instead of leveraging Airflow's provider ecosystem. While there isn't a native Twitter provider in modern Airflow (Twitter API v1.1 was deprecated), the code should use XCom for data passing between tasks, and could use more idiomatic patterns like TaskFlow API (@task decorator) available in Airflow 2.x. The code also uses deprecated 'python_operator' import path instead of 'airflow.operators.python'. Most critically, it doesn't use any specific operators or hooks - everything is wrapped in generic PythonOperators, which is anti-pattern for Airflow.",1,The code is clean and doesn't import any internal testing modules or include test harness boilerplate. It uses standard Airflow libraries (though with deprecated import paths). No hallucinated functions like 'get_test_run' or test utilities are present.,0,"The code has several critical issues with instruction adherence: 1) Data is not passed between tasks - fetch_tweets returns data but clean_and_analyze doesn't receive it (no XCom usage), 2) The clean_and_analyze task expects 'tweets' parameter but PythonOperator doesn't pass it, 3) The store_insights_task just prints a message instead of actually storing data, 4) The 'depends_on' parameter is not a valid PythonOperator parameter (should use >> or set_upstream/downstream). The pipeline structure exists but the actual data flow is broken, failing to deliver a working data engineering pipeline.",0.3333333333333333
490,Baseline (Qwen 2.5 1.5B Instruct),176,0,"The code uses PythonOperator for all tasks, which is acceptable for LevelDB operations since there is no native LevelDB operator in Airflow. However, the code uses the deprecated 'airflow.operators.python_operator.PythonOperator' instead of the modern 'airflow.operators.python.PythonOperator'. This is not idiomatic for Airflow 2.9.3.",0,"The code has multiple critical issues: (1) Missing import for 'timedelta' from datetime module, (2) Functions are defined AFTER they are referenced in PythonOperator calls, which will cause NameError at DAG parsing time, (3) 'start_date' is incorrectly set as a string '2023-01-01' instead of a datetime object, (4) Uses deprecated import path 'airflow.operators.python_operator' instead of 'airflow.operators.python'. These are significant code quality issues that prevent the DAG from being production-ready.",1,"The DAG fulfills the user instruction by demonstrating basic LevelDB key-value operations including initialization, insertion (put), retrieval (get), and cleanup. It shows a complete workflow for interacting with a lightweight embedded key-value database as requested.",0.3333333333333333
491,Baseline (Qwen 2.5 1.5B Instruct),177,0,"While the code attempts to use a specific operator (DisplayVideo360ToBigQueryOperator), this operator does not exist in the standard Airflow providers. The actual Airflow Google provider does not have a 'display_video360_to_bigquery' transfer operator. The correct approach would be to use GoogleDisplayVideo360CreateQueryOperator, GoogleDisplayVideo360RunQueryOperator, and GoogleDisplayVideo360DownloadReportV2Operator in combination with BigQuery operators. The code also references 'BigQueryOperator' without importing it, and uses a Hook import that isn't utilized.",0,"The code imports and uses 'DisplayVideo360ToBigQueryOperator' from 'airflow.providers.google.cloud.transfers.display_video360_to_bigquery', which does not exist in the Airflow codebase. This is a hallucinated/non-existent operator. Additionally, 'BigQueryOperator' is used without being imported, and 'timedelta' is used without importing it from datetime. The code also contains placeholder values and malformed SQL that wouldn't work in production.",0,"The instruction requires collecting, downloading, and transferring DV360 data to a centralized data warehouse. While the intent is present, the implementation is fundamentally broken due to using non-existent operators. The second task (load_data_task) also doesn't make sense as it attempts to INSERT data that should already be loaded by the first task, and the SQL query references a non-existent source table format. The pipeline doesn't properly implement the three-step process (collect, download, transfer) required for DV360 data extraction.",0.0
492,Baseline (Qwen 2.5 1.5B Instruct),178,0,"The code uses deprecated 'PythonOperator' imported from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. More critically, it fails to use idiomatic XCom patterns. The code references a non-existent 'xcom' object directly (xcom.set(), xcom.get()) which is not valid Airflow syntax. Proper XCom usage requires either: (1) returning values from python_callable functions, (2) using task_instance.xcom_push()/xcom_pull() with the ti parameter, or (3) using the TaskFlow API with @task decorator.",0,The code hallucinates a non-existent 'xcom' object/module that can be called directly with xcom.set() and xcom.get() methods. This is not part of the Airflow API. XCom operations must be performed through the TaskInstance object (ti.xcom_push/ti.xcom_pull) or by returning values from functions. This code would fail immediately at runtime with NameError: name 'xcom' is not defined.,0,"While the DAG attempts to demonstrate XCom data sharing between tasks as requested, it fails to implement working code. The instruction asked for 'various methods like direct pushing, returning values, and bash operator interactions' but the code: (1) uses invalid XCom syntax that won't work, (2) doesn't demonstrate returning values from functions (the idiomatic way), (3) doesn't include any BashOperator interactions as requested. The code shows intent but doesn't deliver functional examples of the requested patterns.",0.0
493,Baseline (Qwen 2.5 1.5B Instruct),179,0,"The code fails to use idiomatic Airflow patterns. It wraps all SQS operations in PythonOperator with SQSHook instead of using native AWS SQS operators and sensors. Airflow 2.8.4 provides airflow.providers.amazon.aws.operators.sqs (SqsPublishOperator) and airflow.providers.amazon.aws.sensors.sqs (SqsSensor) that should be used. The code references undefined functions (create_sqs_queue, publish_messages, receive_messages_sensor, delete_sqs_queue) that would presumably use SQSHook, which is the anti-pattern described in the scoring criteria.",0,"The code references four Python callable functions (create_sqs_queue, publish_messages, receive_messages_sensor, delete_sqs_queue) that are never defined in the provided code. This makes the DAG non-functional and incomplete. While it imports SQSHook, it never actually uses it. The code also uses the deprecated 'PythonOperator' import path (airflow.operators.python_operator) instead of the current airflow.operators.python for Airflow 2.8.4.",0,"The instruction explicitly asks to 'demonstrate different message retrieval strategies using sensors' (plural), but the code only shows one sensor task. Additionally, the instruction requires a 'complete workflow' but the code is incomplete with undefined functions. The task structure attempts to follow the workflow (create queue → publish → sense → delete), but without implementation details and only one sensing strategy, it fails to fully adhere to the requirements of demonstrating 'different message retrieval strategies' and providing a working implementation.",0.0
494,Baseline (Qwen 2.5 1.5B Instruct),180,0,"The DAG uses PythonOperator with undefined functions (preprocess_data_function, train_model_function) instead of leveraging idiomatic Airflow patterns. While KubernetesPodOperator is appropriate for deployment, the data preprocessing and model training tasks should ideally use specific operators or at least have the callable functions defined. More critically, it uses the deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' (the modern import path in Airflow 2.0+/3.0+). The approach of wrapping core ML logic in undefined PythonOperator callables without showing the implementation is not idiomatic.",1,"The code uses only standard Airflow libraries and providers (kubernetes, http). There are no imports from internal testing modules or test harness boilerplate. While the functions referenced are undefined, this is a completeness issue rather than hallucination of non-existent Airflow features.",1,"The DAG addresses all key requirements from the instruction: (1) transforms raw training data (download_raw_data, preprocess_data), (2) trains a classification model (train_model), (3) exposes it as a real-time prediction service (deploy_model using KubernetesPodOperator), and (4) demonstrates automatic resource management (resource limits/requests in KubernetesPodOperator). The workflow follows the complete pipeline from CSV data ingestion through model deployment as a service.",0.6666666666666666
495,Baseline (Qwen 2.5 1.5B Instruct),181,0,"The code fails the idiomatic Airflow criterion. It uses PythonOperator with DlpHook for all operations instead of using the native Google Cloud DLP Operators provided by Airflow. The airflow.providers.google.cloud.operators.dlp module contains dedicated operators like CloudDLPCreateJobTriggerOperator, CloudDLPUpdateJobTriggerOperator, CloudDLPDeleteJobTriggerOperator, CloudDLPGetJobTriggerOperator, and CloudDLPListJobTriggersOperator. The code wraps Hook logic in Python callables rather than leveraging these purpose-built operators, which is the anti-pattern described in the scoring criteria.",0,"The code contains hallucinated/non-existent methods on the DlpHook class. Methods like 'create_data_inspection_trigger', 'list_data_inspection_triggers', 'get_data_inspection_trigger', 'update_data_inspection_trigger', and 'delete_data_inspection_trigger' do not exist in the actual DlpHook implementation. The actual DlpHook methods have different names (e.g., create_job_trigger, list_job_triggers, etc.). Additionally, the import statement uses 'from airflow.operators.python_operator import PythonOperator' which is deprecated in Airflow 3.x; it should be 'from airflow.operators.python import PythonOperator'.",1,"The code does fulfill the instruction requirements from a structural perspective. It creates a sequential workflow that demonstrates creating, listing, retrieving, updating, and deleting a DLP job trigger in the correct order. The task dependencies are properly defined to show sequential task management. Despite the implementation issues (wrong operators and hallucinated methods), the business logic flow matches what was requested: a technical reference for managing data protection tasks through a structured, automated pipeline.",0.3333333333333333
496,Baseline (Qwen 2.5 1.5B Instruct),182,0,"The code creates an AzureBlobStorageHook instance and passes it to operators via a 'hook' parameter. This is not idiomatic Airflow usage. Azure operators should accept 'azure_conn_id' (or similar connection parameters) directly, not pre-instantiated hooks. The operators should internally create and manage their own hooks. Additionally, the operator names used (AzureBlobFileUploadOperator, AzureBlobFileDownloadOperator, AzureBlobDeleteOperator) do not match the actual Airflow 2.9.3 provider operators, which are typically named LocalFilesystemToWasbOperator, WasbBlobSensor, WasbDeleteBlobOperator, etc. The correct pattern is to pass connection IDs to operators, not hook instances.",0,"The operators imported (AzureBlobFileDownloadOperator, AzureBlobFileUploadOperator, AzureBlobDeleteOperator) do not exist in the standard Airflow Azure provider package for version 2.9.3. The actual operators in airflow.providers.microsoft.azure.operators have different names and interfaces. Additionally, the 'hook' parameter being passed to operators is not a standard parameter for Azure operators. The AzureBlobStorageHook constructor also does not accept 'resource_group_name' and 'account_name' as shown - it typically only needs a connection ID.",1,"Despite the implementation issues, the DAG does attempt to fulfill the instruction requirements: it demonstrates file transfer between local filesystem and Azure cloud storage (upload and download operations), includes deletion for cloud resource management, uses Azure-specific operators (even if incorrectly), and shows a programmatic workflow for data movement in cloud infrastructure. The logical flow (upload -> download -> delete) illustrates the interaction requested.",0.3333333333333333
497,Baseline (Qwen 2.5 1.5B Instruct),183,0,"The code imports OssHook but then uses PythonOperator with undefined python_callable functions (create_bucket, upload_file, delete_bucket). This is the anti-pattern of wrapping Hook logic in PythonOperator instead of using native Operators. The Alibaba Cloud provider includes operators like OSSCreateObjectOperator, OSSDeleteBatchObjectsOperator, etc. that should be used directly for idiomatic Airflow code.",0,"The code references undefined functions (create_bucket, upload_file, delete_bucket) that are never implemented in the provided code. Additionally, 'timedelta' is used but never imported. The code is incomplete and would fail immediately upon execution. While it doesn't import test harness code, it's not production-ready or clean.",1,"The code does fulfill the instruction requirements: it creates a DAG that demonstrates sequential task execution (create_bucket >> upload_file >> delete_bucket), shows task dependencies clearly, and attempts to interact with Alibaba OSS bucket resources for cloud infrastructure management. The logical flow matches what was requested, even though the implementation has technical issues.",0.3333333333333333
498,Baseline (Qwen 2.5 1.5B Instruct),184,0,The code uses PythonOperator with OracleHook instead of using the native OracleOperator from airflow.providers.oracle.operators.oracle. This is the anti-pattern described in the scoring criteria - wrapping hook logic in PythonOperator instead of using the purpose-built operator for Oracle database operations.,0,"The code has several issues: (1) Missing import for 'timedelta' from datetime module, (2) Functions 'insert_data' and 'select_data' are defined AFTER they are referenced in the PythonOperator tasks, which would cause a NameError at DAG parse time, (3) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 2.9.3. While not test harness leakage, these are significant code quality issues that prevent production readiness.",0,"The instruction asks to 'explore various methods of executing database operations with input and output parameter handling' and 'highlight different parameter input strategies for stored procedures'. The code only shows one basic method (callproc with input params) and does not demonstrate: (1) Output parameter handling (OUT or IN/OUT parameters), (2) Different parameter input strategies (named vs positional, different data types, complex objects), (3) Various methods of database operations (the code only uses callproc, not direct SQL execution, bulk operations, etc.). The task dependency is also incorrect (insert should come before select, not after).",0.0
499,Baseline (Qwen 2.5 1.5B Instruct),185,0,"The code uses non-existent or incorrect operators. 'GCSFileTransferOperator' and 'BigQueryCreateTableOperator' are not standard Airflow operators. The correct operators should be 'LocalFilesystemToGCSOperator' (or similar) and 'BigQueryCreateEmptyTableOperator'. Additionally, 'BashOperator' is used but never imported. While GCSToBigQueryOperator is correct, the overall approach fails to use proper idiomatic Airflow operators for the local-to-GCS transfer step.",0,"The code references operators that do not exist in the standard Airflow provider packages: 'GCSFileTransferOperator' and 'BigQueryCreateTableOperator' are hallucinated/incorrect operator names. Additionally, 'BashOperator' is used without being imported. These are fabricated or incorrectly named components that would cause the DAG to fail.",0,"The instruction specifically asks to 'transform a local CSV file' into BigQuery, but the code downloads from a URL (https://example.com/us_states.csv) rather than working with an existing local file. The instruction emphasizes 'moving structured data from local storage to cloud-based analytical platforms', but the implementation assumes downloading from the internet. This is a fundamental misunderstanding of the requirement.",0.0
500,Baseline (Qwen 2.5 1.5B Instruct),186,1,"The code uses the native DockerOperator from airflow.providers.docker.operators.docker, which is the idiomatic way to manage Docker containers in Airflow. It does not wrap Docker logic in PythonOperator with hooks, but instead uses the purpose-built operator for Docker operations.",1,"The code imports only standard Airflow libraries (airflow.DAG and airflow.providers.docker.operators.docker.DockerOperator). There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run(). The code is clean and production-ready from an import perspective.",0,"The instruction asks for an infrastructure-as-code workflow to 'dynamically create and manage a distributed caching service' with 'precise configuration controls' for a 'scalable in-memory data storage solution' (clearly referring to systems like Redis, Memcached, or similar). However, the implementation has critical flaws: (1) It uses a generic 'cache-service-image' that doesn't exist, rather than a real caching solution like Redis or Memcached; (2) The DockerOperator usage is incorrect - pulling and removing images should use different operators or methods, not DockerOperator with just an image parameter; (3) The workflow doesn't actually demonstrate infrastructure-as-code principles for creating a distributed cache cluster with multiple nodes; (4) The configuration controls (NODE_COUNT, MEMORY_SIZE) are passed as environment variables but there's no actual orchestration of multiple cache nodes or cluster setup. The DAG shows a superficial understanding but fails to deliver a functional distributed caching infrastructure.",0.6666666666666666
501,Baseline (Qwen 2.5 1.5B Instruct),187,0,"The code uses operators that do not exist in the standard Airflow GCS provider package. Specifically, 'GCSUploadFileOperator' and 'GCSDownloadFileOperator' are not real operators in airflow.providers.google.cloud.operators.gcs. The correct operators should be 'LocalFilesystemToGCSOperator' (or 'GCSFileTransformOperator') for upload and 'GCSToLocalFilesystemOperator' for download. Alternatively, 'GCSHook' methods could be wrapped in PythonOperator, but the native approach would use operators like 'LocalFilesystemToGCSOperator' and 'GCSToLocalFilesystemOperator' from the transfer module.",0,"The code imports and uses non-existent operators ('GCSUploadFileOperator' and 'GCSDownloadFileOperator') that are not part of the Airflow GCS provider library. These are hallucinated operator names that do not exist in the actual Airflow codebase, making this code non-functional and not production-ready.",1,"Despite using incorrect operator names, the DAG structure and logic correctly addresses all requirements from the instruction: creating a GCS bucket, uploading a local file, downloading the file, and performing cleanup operations (deleting the bucket). The workflow demonstrates inter-system file transfer and management with proper task dependencies.",0.3333333333333333
502,Baseline (Qwen 2.5 1.5B Instruct),188,0,"The code fails to use Airflow's native Kafka providers and operators. It wraps PySpark Kafka logic inside PythonOperator instead of using `airflow.providers.apache.kafka` operators like KafkaProducerOperator or KafkaConsumerOperator. Additionally, it uses a PythonOperator with a lambda for the 'sensor' instead of using proper Airflow sensors (e.g., KafkaConsumerSensor). This is the anti-pattern of wrapping external logic in PythonOperator rather than using idiomatic Airflow operators.",1,"The code does not import any internal testing modules or include test harness boilerplate. It uses standard libraries (airflow, pyspark) without hallucinated or non-existent imports. While the implementation approach is not idiomatic, the imports themselves are legitimate.",0,"The instruction explicitly requests: (1) a sensor that triggers specific actions based on number characteristics, (2) conditional event handling, and (3) asynchronous message processing. The code fails on multiple fronts: the 'sensor' is just a PythonOperator with a lambda returning True (no actual sensing logic or number characteristic checking), there's no real conditional event handling based on message content characteristics, and the PySpark batch read/write operations don't demonstrate true asynchronous message processing or event-driven architecture. The consume function uses batch processing (spark.read) rather than streaming consumption, which doesn't align with event-driven patterns.",0.3333333333333333
503,Baseline (Qwen 2.5 1.5B Instruct),189,0,"The code uses operators that do not exist in the standard Airflow providers. Specifically, 'GoogleCloudStorageBucketCreateOperator' and 'GoogleCloudStorageBucketDeleteOperator' are not imported, and these exact class names do not exist in airflow.providers.google.cloud.operators.gcs. The correct operators are 'GCSCreateBucketOperator' and 'GCSDeleteBucketOperator'. Additionally, 'GoogleCloudTextToSpeechSynthesizeOperator' does not have an 'output_uri' parameter in the standard implementation - it typically uses 'target_bucket_name' and 'target_filename'. The code also references 'create_bucket_operator.output_bucket' which is not a valid attribute.",0,The code contains hallucinated operators and parameters. 'GoogleCloudStorageBucketCreateOperator' and 'GoogleCloudStorageBucketDeleteOperator' are not real Airflow operators (correct names are 'GCSCreateBucketOperator' and 'GCSDeleteBucketOperator'). The 'output_uri' parameter for GoogleCloudTextToSpeechSynthesizeOperator and the 'output_bucket' attribute on the create_bucket_operator are also fabricated. These are not part of the standard Airflow 2.8.4 Google Cloud provider.,1,"Despite the implementation issues, the code does attempt to fulfill all the requirements from the instruction: creating a bucket, generating an audio file from sample text using Google Cloud Text-to-Speech, and cleaning up the bucket. The logical flow and task dependencies align with the requested business logic, even though the specific operators and parameters are incorrect.",0.3333333333333333
504,Baseline (Qwen 2.5 1.5B Instruct),190,0,"The code uses PythonOperator with BigQueryHook instead of native BigQuery operators. Airflow provides specific operators like BigQueryCreateEmptyTableOperator, BigQueryInsertJobOperator, and BigQueryExecuteQueryOperator that should be used for these operations. The pattern of wrapping hook logic in PythonOperator is not idiomatic Airflow and defeats the purpose of using a workflow orchestration tool with purpose-built operators.",0,"The code references Python functions (create_standard_table, create_view, create_materialized_view, drop_table) that are never defined in the provided code. These functions are called in the python_callable parameter but don't exist, making this code non-functional. Additionally, it uses the deprecated 'airflow.operators.python_operator' import path instead of 'airflow.operators.python'.",0,"While the DAG structure attempts to address the instruction by including tasks for creating standard tables, views, and materialized views, the implementation is incomplete and non-functional. The functions referenced don't exist, so no actual table creation, modification, or management occurs. The instruction asks for a working pipeline that demonstrates dynamic table creation and transformation, but this code cannot execute successfully.",0.0
505,Baseline (Qwen 2.5 1.5B Instruct),191,0,"The code does not use any Airflow Operators at all. It attempts to execute Hook methods directly in the DAG definition file, which is fundamentally incorrect. Hooks should be called within task contexts (e.g., inside PythonOperator callables or via dedicated Operators). The code should use operators like GoogleSheetsToGCSOperator, GCSToGoogleSheetsOperator, or at minimum PythonOperator with proper task definitions. The current implementation has no actual task definitions - just direct Hook method calls that would execute during DAG parsing, not during task execution.",0,"The code uses 'GCSCredentialsHook' which does not exist in Airflow providers. The correct class is 'GCSHook' from airflow.providers.google.cloud.hooks.gcs. Additionally, methods like 'upload_object_from_string' and 'delete_objects_in_bucket' are not accurate - GCSHook uses methods like 'upload' and proper object deletion methods. The code also references BigQueryHook.load_table_from_dataframe which exists but is used incorrectly in this context.",0,"The instruction asks for: (1) Extract spreadsheet data from Google Sheets, (2) Transfer to GCS, (3) Reimport data into a NEW spreadsheet environment. The code attempts steps 1 and 2 but then loads data into BigQuery instead of reimporting back to Google Sheets as requested. The instruction specifically mentions 'reimporting the data into a new spreadsheet environment', which means back to Google Sheets, not BigQuery. The cleanup task also doesn't align with the migration goal of preserving data in the new location.",0.0
506,Baseline (Qwen 2.5 1.5B Instruct),193,0,"The code uses PythonOperator with MySqlHook and GCSHook instead of using native Airflow operators. For this use case, the idiomatic approach would be to use MySqlToGCSOperator from airflow.providers.google.cloud.transfers.mysql_to_gcs, which is specifically designed for transferring data from MySQL to GCS. The current implementation wraps hook logic in Python callables, which is the anti-pattern described in the scoring criteria.",1,"The code uses only standard Airflow libraries and providers (airflow.providers.mysql.hooks.mysql.MySqlHook, airflow.providers.google.cloud.hooks.gcs.GCSHook, airflow.operators.python_operator.PythonOperator). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective.",0,"The instruction specifically requests 'built-in bucket management' as part of the workflow. The provided code does not include any bucket management functionality such as creating the bucket if it doesn't exist, checking bucket existence, or any bucket configuration. The code only performs data extraction and upload, missing the bucket management requirement. Additionally, while the instruction mentions 'one-time batch process' (which is correctly implemented with schedule_interval=None), the lack of bucket management is a critical omission from the specified requirements.",0.3333333333333333
507,Baseline (Qwen 2.5 1.5B Instruct),194,0,"The code uses PythonOperator with Hooks (GoogleCloudBigQueryHook and PostgresHook) instead of native operators. For BigQuery to Postgres transfer, idiomatic Airflow would use BigQueryToPostgresOperator or at minimum BigQueryOperator for extraction. The pattern of wrapping hook logic in PythonOperator is explicitly non-idiomatic.",0,"The code imports 'airflow.operators.python_operator.PythonOperator' which is deprecated in Airflow 2.x. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, there are critical bugs: the extract task returns a DataFrame but the load task doesn't receive it (no XCom handling), and PostgresHook.get_connection() returns a Connection object, not a database connection with cursor() method. The code would fail in production.",0,"The instruction explicitly requests 'configurable parameters' and 'robust error handling and resource management'. The code has hardcoded values (query, table names), no parameterization, no error handling (try/except blocks), and poor resource management (connections and cursors not properly closed with context managers). The data passing between tasks is also broken, making the DAG non-functional.",0.0
508,Baseline (Qwen 2.5 1.5B Instruct),196,0,"The code uses deprecated operators 'PythonOperator' and 'BashOperator' from old import paths. For Airflow 2.8.4, it should use 'from airflow.operators.python import PythonOperator' and 'from airflow.operators.bash import BashOperator'. While these are still operators (not wrapping with hooks), the import paths are outdated and not idiomatic for the specified Airflow version.",0,"The code has multiple critical errors: (1) The function 'print_params' is referenced before it's defined, causing a NameError. (2) The variable 'params' is used in 'access_params_in_bash' before it's defined. (3) The logic in 'print_params' incorrectly tries to access 'params' from kwargs when it should access 'param1' and 'param2' directly from op_kwargs. (4) The environment variable approach in 'set_env_vars' won't work as expected since each BashOperator runs in its own shell session. This code would fail in production.",0,"While the DAG attempts to demonstrate parameter passing and environment variable testing, the implementation is fundamentally broken. The environment variable example doesn't work (export in one task won't persist to another), the parameter passing has ordering errors, and the methods shown don't properly illustrate dynamic runtime configuration injection. The instruction asks for showcasing 'different methods' and 'dynamically passed and accessed' parameters, but the implementation fails to execute and doesn't demonstrate working patterns like templating, XCom, or proper env var injection.",0.0
509,Baseline (Qwen 2.5 1.5B Instruct),197,0,The code uses PythonOperator with AzureDataLakeHook instead of using native Azure operators. Airflow providers offer operators like LocalFilesystemToADLSOperator or AzureDataLakeStorageDeleteOperator that should be used for idiomatic Airflow. The pattern of wrapping hook logic in a PythonOperator is explicitly non-idiomatic.,1,"The code uses standard Airflow libraries (airflow.providers.microsoft.azure.hooks.adls, airflow.operators.python_operator) without any test harness imports or internal testing modules. It is clean production-style code without hallucinated elements.",0,"The code has a critical logic error that violates the instruction. It uploads a LOCAL file to ADLS, but then attempts to delete it FROM ADLS using the same local path. The instruction asks to 'upload a local file to Azure Data Lake Storage and then immediately deletes the uploaded file' - meaning delete from ADLS, not the local file. However, the delete_file method is being called with the local filename parameter, which is incorrect. The uploaded file in ADLS would have a different path/name in the cloud storage. Additionally, the instruction implies two separate tasks (upload THEN delete), but the code combines both in a single task, losing the ability to handle failures independently and demonstrate proper workflow orchestration.",0.3333333333333333
510,Baseline (Qwen 2.5 1.5B Instruct),198,0,"The code uses incorrect operators for the task. It imports 'BigQueryCreateDatasetOperator' and 'BigQueryLoadTableFromTextOperator' which are not related to Dataflow batch text processing. For a Dataflow word count pipeline, it should use operators like 'DataflowCreatePythonJobOperator' or 'DataflowTemplatedJobStartOperator'. The 'DataflowJobStartOperator' and 'DataflowJobDeleteOperator' imports appear to be non-existent or incorrect operator names in Airflow 3.0.0. The correct operators would be from 'airflow.providers.google.cloud.operators.dataflow' such as 'DataflowStartFlexTemplateOperator' or similar.",0,"The code imports operators that do not exist in the standard Airflow library: 'DataflowJobStartOperator' and 'DataflowJobDeleteOperator' are not valid operators in airflow.providers.google.cloud.operators.dataflow. The actual operators available are 'DataflowStartFlexTemplateOperator', 'DataflowTemplatedJobStartOperator', 'DataflowCreatePythonJobOperator', etc. Additionally, 'BigQueryLoadTableFromTextOperator' is not a standard operator; the correct one would be 'BigQueryCreateExternalTableOperator' or similar. This represents hallucinated/non-existent imports.",0,"The instruction asks for a DAG that manages the lifecycle of creating, running, and deleting a Dataflow pipeline while handling GCS operations for a word count example. The code fails to properly implement this: (1) It incorrectly focuses on BigQuery operations instead of GCS operations, (2) It doesn't create/upload a sample text file to GCS, (3) It doesn't properly implement a Dataflow word count pipeline (the template_path reference is generic and doesn't demonstrate the word count logic), (4) The job deletion using 'job_id=start_dataflow_job_task.output['jobId']' is incorrect syntax for XCom references in Airflow. The code misses key GCS file handling operations and doesn't properly demonstrate the Dataflow word count workflow.",0.0
511,Baseline (Qwen 2.5 1.5B Instruct),199,0,"The code uses S3Hook wrapped in PythonOperator (via @task decorator) instead of using the native S3CreateObjectOperator for uploads. For Airflow 3.0.6, idiomatic usage would leverage S3CreateObjectOperator and S3FileTransformOperator or similar operators from airflow.providers.amazon.aws.operators.s3. Additionally, the code does not use modern asset-based decorators (like @asset or Dataset APIs introduced in Airflow 2.4+) as requested in the instruction, which would be the idiomatic way to manage data lineage and scheduling based on data assets.",0,"The code has multiple critical issues: (1) Missing imports for 'datetime' and 'timedelta', (2) Invalid syntax with 'depends_on' parameter in @task decorator - this parameter does not exist in Airflow's task decorator, (3) S3Hook is instantiated at module level without proper connection configuration, (4) Tasks are defined but never called or connected in the DAG context. The code would fail immediately upon execution due to these errors.",0,The instruction explicitly requests 'modern asset-based decorators' to illustrate data asset management and producer-consumer relationships. The code completely misses this requirement - it does not use Dataset APIs (@asset decorator or Dataset objects) which are the modern Airflow 2.4+ feature for asset-based scheduling and lineage. The code attempts to show producer-consumer patterns but does so incorrectly with invalid syntax and without leveraging Airflow's native data-aware scheduling capabilities.,0.0
512,Baseline (Qwen 2.5 1.5B Instruct),200,0,"The code uses PythonOperator to wrap a subprocess command execution ('ls -l'). Airflow provides the BashOperator specifically designed for executing shell commands, which is the idiomatic approach. The correct pattern would be: `from airflow.operators.bash import BashOperator` and `BashOperator(task_id='run_command', bash_command='ls -l')`. Using PythonOperator + subprocess is a non-idiomatic 'Pythonic' workaround.",0,"The code has a critical bug: it uses 'subprocess.run()' without importing the subprocess module. This would cause a NameError at runtime. Additionally, it uses deprecated imports: 'airflow.operators.python_operator.PythonOperator' is deprecated in Airflow 2.x (should be 'airflow.operators.python.PythonOperator'), and 'airflow.utils.dates.days_ago' is also deprecated in favor of using datetime directly or pendulum.",0,"The instruction asks for orchestration of system-level tasks with time-based scheduling, command execution, and basic error handling. While the DAG includes scheduling (daily) and attempts command execution, the error handling implementation is fundamentally flawed. The 'handle_error_task' runs BEFORE 'run_command_task' (due to set_upstream), which doesn't demonstrate error handling. Proper error handling would use trigger rules (e.g., trigger_rule='one_failed'), on_failure_callback, or task dependencies that execute error handlers after failures occur. The current implementation doesn't illustrate error handling in any meaningful way.",0.0
513,Baseline (Qwen 2.5 1.5B Instruct),201,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Storage operations: CreateBucketOperator, LocalToGCSOperator, and DeleteBucketOperator from airflow.providers.google.cloud. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code has a critical error: it uses 'timedelta' without importing it. The line 'retry_delay': timedelta(minutes=5)' will cause a NameError at runtime. The import statement 'from datetime import timedelta' is missing. While not a test harness hallucination, this is a code completeness issue that prevents production readiness.",0,"The instruction asks for 'transferring data' which implies moving data between cloud resources or demonstrating data movement operations. The code only uploads from local to GCS (LocalToGCSOperator) but imports GCSToLocalOperator without using it. A complete demonstration of 'data transfer' in a cloud infrastructure context would typically include GCS-to-GCS transfer or a round-trip operation (upload then download). The current implementation only shows a one-way local upload, which doesn't fully showcase 'data movement' capabilities as requested. Additionally, the DeleteBucketOperator will fail because it attempts to delete a non-empty bucket (the uploaded file would still be present), which is not handled in the code.",0.3333333333333333
514,Baseline (Qwen 2.5 1.5B Instruct),202,0,"While the DAG uses native Airflow operators (BigQueryCreateDatasetOperator, BigQueryInsertJobOperator, KubernetesPodOperator), it fails to leverage Google Cloud's Vertex AI providers for multimodal ML tasks. For an AI-powered content analysis pipeline using embeddings, generation, and safety filtering, the idiomatic approach would be to use airflow.providers.google.cloud.operators.vertex_ai operators (like VertexAIBatchPredictionJobOperator, VertexAIModelServiceOperator, or VertexAIEndpointServiceOperator) rather than wrapping everything in generic KubernetesPodOperators. The code treats ML inference as a black-box container execution instead of using purpose-built Vertex AI operators.",0,"The code has a critical import error: it uses 'timedelta' in default_args without importing it (missing 'from datetime import timedelta'). Additionally, it imports KubernetesPodHook and BigQueryHook but never uses them, which suggests incomplete or copy-pasted code. While these aren't test harness hallucinations, the missing import makes this non-functional production code.",0,"The instruction specifically requests a pipeline that 'uses multimodal machine learning models to perform embedding, generation, and safety-filtered content evaluation.' The DAG provides only a skeleton with generic KubernetesPodOperators that reference undefined container images. It doesn't demonstrate actual integration with Google Cloud's multimodal ML services (Vertex AI, Gemini API, Vision API, etc.), nor does it show how embeddings are generated, how text/image content is processed through ML models, or how generation tasks are performed. The pipeline structure (load data → run analysis → run safety filtering) is overly simplistic and doesn't reflect the multimodal, multi-step nature of embedding + generation + evaluation workflows.",0.0
515,Baseline (Qwen 2.5 1.5B Instruct),203,0,"The code uses PythonOperator with hooks (GoogleSheetsHook and S3Hook) instead of using native operators. For this use case, GoogleSheetsToS3Operator or a transfer operator would be more idiomatic. The code wraps hook logic in Python functions rather than leveraging Airflow's built-in operators designed for data transfer tasks.",0,"The code has multiple issues: (1) Missing imports for 'datetime' and 'timedelta', (2) Uses deprecated 'PythonOperator' import path - should be 'from airflow.operators.python import PythonOperator' for Airflow 3.0.6, (3) The GoogleSheetsHook.get_pandas_dataframe() method doesn't exist in the actual API - this is a hallucinated method. (4) Data passing between tasks is broken - extract_data_task returns a dataframe but transfer_data_task doesn't receive it as a parameter.",0,"The instruction specifically requires 'dynamic environment-based naming' for the cloud storage destination, but the code uses hardcoded values ('your-bucket-name', 'your-project-id', 'your-sheet-id') without any environment-based logic. While it attempts to extract from Google Sheets and transfer to cloud storage (S3), it lacks the dynamic environment configuration that was explicitly requested. Additionally, the data passing between tasks is broken, so the pipeline wouldn't actually work.",0.0
516,Baseline (Qwen 2.5 1.5B Instruct),204,0,"The code wraps all logic in PythonOperator functions instead of using native Airflow operators. For S3 bucket operations, it should use S3CreateBucketOperator and S3DeleteBucketOperator. For data transfer, it should use transfer operators like AzureBlobStorageToS3Operator or at minimum use the operators designed for these cloud services rather than manually calling hooks within Python functions.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.0. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, it imports SFTPHook which is never used in the code, indicating unnecessary or confused imports.",0,"While the code attempts to follow the three-stage structure (setup, transfer, teardown), it has critical implementation gaps. The set_up_s3_bucket function only prints a message without actually creating a bucket. The transfer_data function references hardcoded paths and bucket names that don't align with the temporary bucket created in setup. The delete_s3_bucket references 'temporary_bucket' which is never created or passed between tasks. The workflow structure is present but the actual implementation is incomplete and non-functional.",0.0
517,Baseline (Qwen 2.5 1.5B Instruct),205,0,"The DAG heavily relies on PythonOperator with undefined python_callable functions (download_housing_data, preprocess_housing_data, train_model, evaluate_model, upload_results) instead of using native Airflow operators. For S3 operations, it should use S3CreateObjectOperator, S3FileTransformOperator, or similar operators from airflow.providers.amazon.aws.operators.s3. For ML training on cloud infrastructure (as mentioned in the instruction), it should use SageMakerTrainingOperator or similar cloud ML operators. The code wraps S3Hook and other hooks in PythonOperators rather than using the purpose-built operators, which is the anti-pattern described in the scoring criteria.",0,"The code references undefined functions (download_housing_data, preprocess_housing_data, train_model, evaluate_model, upload_results) and an undefined variable (load_data_sql) that are never imported or defined. While these aren't test harness imports, the code is incomplete and not production-ready as it would fail immediately upon execution. Additionally, RedshiftOperator is imported but the sql_statement parameter references an undefined 'load_data_sql' variable, and the 'parameters' argument is not a valid parameter for RedshiftOperator in Airflow 2.7.2.",0,"The instruction specifically requests 'a cloud-based training job' for machine learning, but the DAG uses a generic PythonOperator for model training without leveraging any cloud ML infrastructure (e.g., AWS SageMaker, Google AI Platform, Azure ML). The instruction emphasizes 'cloud infrastructure' and 'automated workflow management' for ML training, but the implementation doesn't use any cloud-native ML training operators. Additionally, the use of Redshift for ML training data storage seems misaligned with typical ML workflows where data would be stored in S3 and fed directly to cloud ML services.",0.0
518,Baseline (Qwen 2.5 1.5B Instruct),207,1,"The code uses native Vertex AI operators from airflow.providers.google.cloud.operators.vertex_ai (CreateDatasetOperator, RunPipelineJobOperator, DeleteDatasetOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Google Cloud Vertex AI operations.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules, no test harness boilerplate like get_test_run(), and no non-existent or hallucinated modules. The code is clean and production-ready (aside from placeholder values).",0,"The instruction explicitly requests a workflow that 'synchronizes housing data' as the first step, but the DAG does not include any data synchronization task. The workflow jumps directly to dataset creation without showing how housing data is synchronized/loaded into GCS or the dataset. Additionally, 'manages the pipeline job lifecycle' typically implies monitoring, checking status, or conditional cleanup, but the code only performs a simple delete operation. The DAG is missing the critical data synchronization component and lacks comprehensive pipeline job lifecycle management (e.g., checking job status, handling failures).",0.6666666666666666
519,Baseline (Qwen 2.5 1.5B Instruct),209,0,"The code uses 'SQLExecuteQueryOperator' from 'airflow.providers.presto.operators.sql_execute_query', but this import path is incorrect. The correct import for Airflow 3.x should be 'from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator'. Additionally, the parameter 'presto_conn_id' is not a valid parameter for SQLExecuteQueryOperator - it should use 'conn_id' instead. The operator also doesn't exist in the presto-specific path as imported.",0,"The import statement 'from airflow.providers.presto.operators.sql_execute_query import SQLExecuteQueryOperator' appears to be a hallucinated/non-existent module path. In Airflow 3.x, SQLExecuteQueryOperator is located in 'airflow.providers.common.sql.operators.sql', not in a presto-specific operators module. This represents a hallucination of a non-standard library path.",0,"The instruction asks to 'illustrate the core principles of task sequencing and SQL query execution' and 'demonstrates fundamental data manipulation tasks' (plural). The provided code only contains a single SELECT query with no task sequencing demonstrated (no dependencies between multiple tasks), and no data manipulation (INSERT, UPDATE, DELETE, CREATE TABLE, etc.). It fails to demonstrate task sequencing principles and only shows a basic query execution, not multiple fundamental data manipulation tasks.",0.0
520,Baseline (Qwen 2.5 1.5B Instruct),210,0,"The code fails to use idiomatic Airflow patterns. It imports WeaviateHook but never uses it, instead wrapping logic in PythonOperator with undefined functions (create_weaviate_class, prepare_data). For loading data, it uses a generic HttpOperator instead of the dedicated WeaviateIngestOperator available in airflow.providers.weaviate.operators.weaviate. Idiomatic Airflow would use WeaviateIngestOperator or similar native operators designed specifically for Weaviate operations.",0,"The code has multiple issues: (1) It imports 'timedelta' without the required 'from datetime import timedelta' statement, (2) It references undefined functions 'create_weaviate_class' and 'prepare_data' that are never implemented, (3) It uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator', and (4) The HttpOperator endpoint and JSON structure don't match actual Weaviate API specifications. The code would fail immediately upon execution.",0,"While the code attempts to address the three main requirements (create class, prepare data, load data), it fails to demonstrate a working implementation. The instruction asks to 'demonstrate how to ingest data' and show 'different data input methods' using 'Airflow operators' - but the code provides only skeleton structure with undefined functions and doesn't actually demonstrate multiple input methods or use proper Weaviate operators. The workflow structure is present but non-functional.",0.0
521,Baseline (Qwen 2.5 1.5B Instruct),211,1,"The code uses native BigQuery operators from airflow.providers.google.cloud.operators.bigquery including BigQueryCreateDatasetOperator, BigQueryCreateTableOperator, BigQueryInsertJobOperator, and BigQueryExecuteQueryOperator. It does not wrap BigQuery functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package. There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run(). The code is clean and production-ready (aside from placeholder values).",1,"The DAG fully satisfies the user instruction. It demonstrates: (1) creating datasets via BigQueryCreateDatasetOperator, (2) creating tables via BigQueryCreateTableOperator, (3) executing SQL for data insertion via BigQueryInsertJobOperator with a load configuration, and (4) executing SQL queries for data retrieval via BigQueryExecuteQueryOperator. The workflow showcases different BigQuery operators and demonstrates basic data management techniques (create, insert, query) in a cloud data warehouse environment as requested.",1.0
522,Baseline (Qwen 2.5 1.5B Instruct),213,0,"The code uses TriggerDagRunOperator correctly, which is idiomatic. However, it includes a non-idiomatic pattern with the send_message_to_secondary() function that uses BaseHook in a way that doesn't align with standard Airflow patterns. The function is called directly in the DAG context (not as a task), which is anti-pattern. Additionally, the import path 'airflow.operators.trigger_dagrun_operator' is incorrect for Airflow 3.0.0 - it should be 'airflow.operators.trigger_dagrun' or use TriggerDagRunOperator from airflow.operators.trigger.",0,"The code contains several issues: (1) Missing import for 'datetime' module, (2) Incorrect import path for TriggerDagRunOperator in Airflow 3.0.0, (3) The BaseHook.get_hook('my_base_hook') usage is fabricated - there's no 'my_base_hook' connection defined, and BaseHook doesn't have a generic 'send_message' method. (4) The send_message_to_secondary() function is called directly in DAG context rather than being wrapped in a task, which would cause execution during DAG parsing rather than runtime.",0,"The instruction asks for a workflow demonstrating how to trigger a secondary DAG with a predefined configuration message. While the code does include a TriggerDagRunOperator with a conf parameter containing a message, it only defines the primary DAG. The instruction implies creating 'a scenario' which should include both the primary and secondary DAG to demonstrate the complete workflow. Additionally, the send_message_to_secondary() function is extraneous and not properly integrated as a task in the workflow.",0.0
523,Baseline (Qwen 2.5 1.5B Instruct),214,0,"While the code uses Dataproc operators, it does not use the workflow template operators as requested. The instruction specifically asks to 'demonstrate creating and executing a Dataproc workflow template', but the code uses direct cluster creation and job submission operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator) instead of the appropriate workflow template operators like DataprocCreateWorkflowTemplateOperator and DataprocInstantiateWorkflowTemplateOperator. The idiomatic approach for workflow templates would be to use the dedicated workflow template operators.",0,"The DataprocSubmitJobOperator is incorrectly configured for a Pig job. Pig jobs don't use 'main_class' and 'jar_file_uri' parameters - these are for Spark/Hadoop JAR jobs. For Pig jobs, the job configuration should use a 'pig_job' dictionary with 'query_file_uri' or 'query_list'. The current configuration appears to be a confused mix of Spark job parameters applied to what should be a Pig job, which would not work in production.",0,"The instruction explicitly requests 'creating and executing a Dataproc workflow template' and mentions 'illustrate the lifecycle of a cloud data processing job from template creation to execution'. However, the code does not create or use workflow templates at all. Instead, it directly creates a cluster, submits a job, and deletes the cluster. Workflow templates are a specific Dataproc feature that allows reusable job configurations, and the code completely misses this requirement. Additionally, the Pig job configuration is incorrect (using Spark/JAR parameters instead of Pig-specific parameters).",0.0
524,Baseline (Qwen 2.5 1.5B Instruct),215,0,"The code attempts to use Airflow operators but misuses them significantly. The UpdateTableOperator is incorrectly used with DynamoDB update_expression syntax (which is for item updates, not table configuration). Enabling point-in-time recovery requires using update_table() method with PointInTimeRecoverySpecification parameter, not an update expression. The CopyObjectOperator is also misused - it's designed for copying objects within S3, not for exporting DynamoDB data to S3. A proper implementation would require using DynamoDB export functionality (likely through a custom operator or PythonOperator with boto3) since no native DynamoDB export operator exists in Airflow providers. The 'segmented export' requirement is not addressed at all - the code just copies a CSV file.",1,"The code uses only standard Airflow libraries and AWS provider imports. There are no test harness imports, internal testing modules, or non-existent operators. While the operators are misused, they are real operators from the airflow.providers.amazon.aws package.",0,"The DAG fails to properly implement the required workflow. While it attempts to create a DynamoDB table (correct), it incorrectly implements point-in-time recovery enablement using wrong operator parameters. Most critically, it completely fails to perform a 'segmented export' from DynamoDB to S3 - instead it just copies a static CSV file from a local path to S3, which has nothing to do with DynamoDB data export. The instruction specifically asks for exporting DynamoDB data to S3 in segments, which would require using DynamoDB's export-to-S3 functionality or scanning the table in segments, neither of which is implemented.",0.3333333333333333
525,Baseline (Qwen 2.5 1.5B Instruct),217,1,"The code uses SQLOperator, which is the appropriate Airflow operator for executing SQL queries. It does not wrap SQL logic in PythonOperator with hooks. Note: There is an unused PostgresHook instantiation at the module level, but the actual tasks correctly use SQLOperator with the connection_id parameter, which is the idiomatic approach.",1,The code uses only standard Airflow libraries (airflow.providers.postgres.hooks.postgres.PostgresHook and airflow.operators.sql.SQLOperator). There are no imports from test modules or test harness boilerplate like get_test_run().,0,"The DAG has a critical logic flaw in task dependencies. The instruction asks to 'validate column-level constraints AND confirm the expected number of rows', implying both checks should run. However, the code sets check_column_constraints_task.set_upstream(count_rows_task), meaning count_rows runs first, then column constraints. More importantly, the count_rows_task only counts rows but does NOT validate against an expected number - it just returns a count without any assertion or validation logic. The instruction explicitly requires 'confirms the expected number of rows', which means comparing against a threshold or expected value, not just counting. Additionally, the column constraints query may not work as written (information_schema.constraint_columns structure varies by database), and there's no actual validation logic - just data retrieval.",0.6666666666666666
526,Baseline (Qwen 2.5 1.5B Instruct),218,0,"The DAG fails to use Display Video 360 (DV360) specific operators. The instruction explicitly mentions 'extracting advertising performance metrics' and 'managing data transfer between Google Display Video 360 and BigQuery', but the code uses GCSDownloadObjectOperator to download line items, which assumes data is already in GCS. Airflow has providers.google.marketing_platform.operators.display_video that should be used for DV360 operations (e.g., GoogleDisplayVideo360DownloadLineItemsOperator, GoogleDisplayVideo360UploadLineItemsOperator, or GoogleDisplayVideo360CreateQueryOperator). Additionally, the code uses BashOperator (which isn't even imported) to call a Python script for processing, when it should use native operators or at minimum a PythonOperator with proper callable functions.",0,"The code references BashOperator in the process_file_task but never imports it from airflow.operators.bash. This will cause a NameError at runtime. While this isn't a test harness hallucination, it represents incomplete/broken code that wouldn't pass basic validation.",0,"The instruction specifically asks to 'orchestrate complex marketing data operations across Google Cloud Platform services, focusing on extracting advertising performance metrics and managing data transfer between Google Display Video 360 and BigQuery' with tasks for 'downloading line items, uploading files, and performing data transformations'. The DAG only implements downloading from GCS (not from DV360 API), processing via external script, and loading to BigQuery. It completely misses: (1) actual DV360 API integration for extracting advertising metrics, (2) uploading files back to DV360 (only downloads are present), and (3) proper data transformations (delegated to an external undefined Python script). The DAG treats this as a simple GCS-to-BigQuery pipeline rather than a DV360-centric marketing data orchestration.",0.0
527,Baseline (Qwen 2.5 1.5B Instruct),219,0,"The code incorrectly uses DataCatalogClient (a Hook/Client class) directly as if it were an Operator. The correct approach would be to use DataCatalogCreateTagTemplateOperator, DataCatalogGetTagTemplateOperator, and DataCatalogUpdateTagTemplateOperator from airflow.providers.google.cloud.operators.datacatalog. The code attempts to call Hook methods directly (create_tag_template, get_tag_template, update_tag_template) and pass 'dag=dag' to them, which is not how Hooks work in Airflow. This is a fundamental misunderstanding of the Operator vs Hook pattern.",0,"The code imports DataCatalogClient from airflow.providers.google.cloud.hooks.datacatalog, but then uses it incorrectly as if it were an Operator class. While DataCatalogClient may exist as a Hook, the way it's being invoked (calling methods directly with dag=dag parameter and expecting them to return task objects) is not valid Airflow syntax. This represents either hallucinated API usage or a fundamental misunderstanding of Airflow's architecture. Additionally, GCSToBigQueryOperator and BigQueryHook are imported but never used, indicating unnecessary imports.",1,"Despite the implementation issues, the code does attempt to address the user instruction by including the three key lifecycle operations requested: creating a tag template (create_tag_template_task), retrieving a tag template (retrieve_tag_template_task), and modifying a tag template (update_tag_template_task). The business logic intent aligns with demonstrating tag template lifecycle management for data governance, even though the technical implementation is incorrect.",0.3333333333333333
528,Baseline (Qwen 2.5 1.5B Instruct),221,0,"While the code uses S3ToS3Operator for the copy operation (which is good), it falls back to BashOperator with AWS CLI commands for deletion instead of using the native S3DeleteObjectsOperator from airflow.providers.amazon.aws.operators.s3. The logging task also uses BashOperator inappropriately. For listing/tracking files, it should use S3ListOperator. The code should leverage Airflow's native AWS operators throughout rather than shell commands.",0,"The code has multiple issues: (1) Missing import for 'timedelta' from datetime module, (2) Missing import for 'BashOperator' from airflow.operators.bash, (3) The S3ToS3Operator import path is incorrect for Airflow 3.0.6 - it should be 'airflow.providers.amazon.aws.operators.s3' not 'transfers.s3_to_s3_operator', (4) The operator name should be S3CopyObjectOperator in newer versions. The code would fail to execute due to these import errors.",0,"The instruction requires 'transfers files between cloud storage buckets while implementing systematic tracking and cleanup procedures' with capabilities to 'list, move, and delete'. The code attempts transfer and deletion but: (1) Does not implement proper listing/tracking of files before or after operations, (2) The logging task incorrectly tries to access 'copy_task.output' which doesn't work this way in Airflow, (3) Missing systematic tracking mechanisms to record what was moved, (4) No validation or verification that files were successfully copied before deletion. The workflow lacks the comprehensive tracking and safety measures requested.",0.0
529,Baseline (Qwen 2.5 1.5B Instruct),223,0,The code fails the idiomatic Airflow criterion. It imports GlueDataBrewHook but then wraps all logic in PythonOperator tasks instead of using native AWS Glue DataBrew operators. Airflow providers.amazon.aws package includes operators like GlueDataBrewStartJobRunOperator that should be used directly. The pattern of 'PythonOperator(python_callable=...)' with hooks is explicitly non-idiomatic when dedicated operators exist.,0,"The code references undefined functions (set_up_glue_data_brew, run_glue_data_brew_job, cleanup_resources) as python_callable parameters without implementing them. These functions are called but never defined in the code, making this incomplete and non-executable. Additionally, 'airflow.operators.python_operator' is deprecated in Airflow 3.x (should be 'airflow.operators.python'). While not test harness leakage, this represents incomplete/broken code generation.",0,"The instruction requires a complete automated data profiling pipeline with full lifecycle management that 'shows how to programmatically profile data'. The provided code only shows a skeleton structure with task dependencies but no actual implementation. It lacks: (1) Dataset/connection configuration for S3 JSON data, (2) DataBrew dataset creation, (3) Profile job configuration and execution, (4) Insight generation/retrieval logic, (5) Dynamic resource management details. The code structure suggests the right flow but provides no working implementation of the business logic requested.",0.0
530,Baseline (Qwen 2.5 1.5B Instruct),224,0,"The code attempts to use a 'GCSToPrestoOperator' which does not exist in Apache Airflow's standard providers. There is no airflow.providers.google.cloud.transfers.gcs_to_presto module. The correct idiomatic approach would be to use GCSToLocalFilesystemOperator or read from GCS and then use PrestoOperator to load the data, or use a combination of existing operators. The code also misuses PrestoHook by calling it directly in task definitions (check_table_exists_task) rather than within an operator context.",0,"The code imports and uses 'GCSToPrestoOperator' from 'airflow.providers.google.cloud.transfers.gcs_to_presto', which is a hallucinated/non-existent operator. This module does not exist in Airflow 2.7.2 or any standard Airflow distribution. Additionally, the code references 'BashOperator' without importing it, and uses 'timedelta' without importing it from datetime. The SQL syntax used (SELECT FROM @bucket) is also not valid Presto syntax.",0,"While the code attempts to address the user instruction of transferring CSV files from GCS to Presto, it fails to implement a working solution. The use of non-existent operators and invalid SQL syntax means this DAG would not execute successfully. The instruction requires a functional data migration workflow, but this code would fail immediately upon execution. Additionally, the table existence check and logging tasks don't contribute meaningfully to the migration workflow as requested.",0.0
531,Baseline (Qwen 2.5 1.5B Instruct),225,0,"The code fails to use the appropriate Airflow operator for conditional/short-circuit logic. The instruction explicitly asks for 'short-circuit logic' and 'tasks dynamically skipped or executed depending on specific criteria'. Airflow provides the ShortCircuitOperator specifically for this purpose, which should be used instead of evaluating conditions at DAG parse time with Python if statements. The current implementation uses PythonOperator with parse-time conditionals, which is not idiomatic for runtime conditional execution in Airflow.",1,"The code uses only standard Airflow libraries (airflow.operators.python_operator.PythonOperator) and does not import any internal testing modules or include test harness boilerplate. While the code has logical issues, it doesn't contain hallucinated or leaked test code.",0,"The code fundamentally misunderstands the requirement. The instruction asks for 'conditional task execution using short-circuit logic' where tasks are 'dynamically skipped or executed' at runtime. However, this implementation evaluates conditions at DAG parse time (using Python if statements outside of task execution), not at runtime. This means the DAG structure is fixed when parsed, not dynamic during execution. Additionally, the logic is broken (check_condition() is called at parse time, task functions shadow the task objects, and the condition 'check_condition() and not check_condition()' is always False). A proper implementation would use ShortCircuitOperator or BranchPythonOperator to make runtime decisions about task execution.",0.3333333333333333
532,Baseline (Qwen 2.5 1.5B Instruct),226,1,"The code correctly uses the DockerOperator from airflow.providers.docker.operators.docker, which is the appropriate provider-specific operator for Docker tasks. It does not wrap Docker logic in a PythonOperator with hooks, demonstrating idiomatic Airflow usage.",1,The code uses only standard Airflow libraries (airflow.providers.docker.operators.docker) and does not import any internal testing modules or include test harness boilerplate. The imports and code structure are clean and production-ready.,0,"The instruction asks for a Docker Swarm operator demonstration, but the code uses DockerOperator (which runs containers on a single Docker host) rather than DockerSwarmOperator (which deploys services to a Docker Swarm cluster). Additionally, the 'automatic removal' requirement is misunderstood - the second task attempts to manually remove containers with an incorrect command, when DockerOperator has an 'auto_remove=True' parameter that handles this automatically. The correct approach would be to use DockerOperator with auto_remove=True (or DockerSwarmOperator if truly targeting Swarm), eliminating the need for a separate removal task.",0.6666666666666666
533,Baseline (Qwen 2.5 1.5B Instruct),227,1,"The code uses appropriate Airflow providers and operators for the task. It uses FileSensor from airflow.sensors.filesystem for file monitoring and DockerOperator from airflow.providers.docker.operators.docker for containerized operations. These are the idiomatic, native operators designed for these specific purposes rather than wrapping logic in PythonOperator with hooks.",0,"The code references 'LogSensor' which is not a standard Airflow operator or sensor. This appears to be a hallucinated component that doesn't exist in the Airflow library. The import statement for LogSensor is also missing, which would cause an immediate NameError at runtime. This is a clear hallucination that makes the code non-functional.",1,"The DAG fulfills all the key requirements from the instruction: (1) monitors a specified input directory for new files using FileSensor, (2) validates file presence through the sensor, (3) performs controlled file transfer using containerized operations via DockerOperator, (4) implements file detection and processing mechanism, and (5) includes built-in error handling through retries configuration and logging capabilities. The structure appropriately sequences these operations with task dependencies.",0.6666666666666666
534,Baseline (Qwen 2.5 1.5B Instruct),228,0,"The code uses PythonOperator with lambda functions that only print statements, which is not idiomatic for a production data quality workflow. For Airflow 2.8.4, this should use appropriate operators like BranchPythonOperator for branching logic, and actual data quality operators or sensors. The lambda functions with print statements are placeholder code, not production-ready implementations. Additionally, it imports the deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python'.",1,The code uses only standard Airflow libraries and does not import any internal testing modules or include test harness boilerplate. There are no hallucinated imports like 'tests_common.test_utils.system_tests' or similar testing artifacts.,0,"The instruction requires a 'branching process' that conditionally routes to either clean data path or error handling path based on data integrity checks. However, the code does not implement actual branching logic - it simply sets both tasks as downstream dependencies using the list syntax [save_clean_data_and_report_task, handle_errors_task], which means both tasks will execute in parallel rather than conditionally. A proper implementation would require BranchPythonOperator or similar to evaluate the data integrity results and choose one path. Additionally, all tasks are placeholder lambdas with print statements rather than actual data ingestion, analysis, integrity checking, or notification logic.",0.3333333333333333
535,Baseline (Qwen 2.5 1.5B Instruct),229,0,"The code uses PythonOperator to wrap OpenAI API calls instead of using the native OpenAIOperator from airflow.providers.openai. For Airflow 3.0.6, the idiomatic approach would be to use OpenAI provider operators. Additionally, the instruction specifically mentions 'leveraging Airflow's task expansion' which suggests using dynamic task mapping (expand() method), but the code uses a Python list comprehension instead. The instruction also mentions 'deferred execution capabilities' which implies using deferrable operators (async/await pattern), but standard synchronous operators are used.",0,"The code imports 'from airflow.operators.python_operator import PythonOperator' which is deprecated syntax. In Airflow 3.0.6, it should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used but never imported, which would cause a runtime error. The code also references 'start_date' as a string '2023-01-01' instead of a datetime object, which is incorrect.",0,"The instruction asks for a pipeline that 'leverages Airflow's task expansion and deferred execution capabilities'. The code does not use dynamic task mapping (task expansion via .expand()) which is a key feature for this use case in modern Airflow. It also does not use deferrable/async operators for deferred execution. The task dependencies are also incorrectly set up with set_upstream in a chain that doesn't make logical sense for parallel enrichment tasks. The business logic of enriching items with OpenAI is present, but the specific Airflow capabilities mentioned are not utilized.",0.0
536,Baseline (Qwen 2.5 1.5B Instruct),230,0,"The DAG fails to use idiomatic Airflow operators. It wraps all logic in PythonOperators with Hooks instead of using native operators like GCSToBigQueryOperator, BigQueryInsertJobOperator, or FirestoreExportDatabaseOperator. For example, the load_data_to_bigquery function manually uses BigQueryHook when a GCSToBigQueryOperator would be the proper approach. The extract and transform tasks similarly use Hooks within Python functions rather than leveraging purpose-built operators from Airflow providers.",1,The code uses only standard Airflow libraries and provider packages. There are no imports from internal testing modules or test harness boilerplate. All imports are from legitimate airflow.providers packages and core Airflow modules.,0,"The instruction asks for extraction from a NoSQL database (implying Google Cloud NoSQL like Firestore or Bigtable), but the code incorrectly uses SnowflakeHook, which is a SQL data warehouse, not a NoSQL database. The instruction specifically mentions 'different Google Cloud platforms' and transforming 'unstructured data into structured format', but Snowflake is not a Google Cloud service. The DAG should extract from a Google Cloud NoSQL service (Firestore, Bigtable, or Datastore), export to GCS, and load to BigQuery for analytical querying. The current implementation misses the core requirement of using Google Cloud NoSQL databases.",0.3333333333333333
537,Baseline (Qwen 2.5 1.5B Instruct),231,0,The code uses PythonOperator with Neo4jHook instead of using the native Neo4jOperator from airflow.providers.neo4j.operators.neo4j. This is the anti-pattern described in the scoring criteria - wrapping hook logic in a PythonOperator rather than using the purpose-built operator for Neo4j queries.,0,"The code references an undefined variable 'tom_hanks_query' that is never declared or defined in the code. This would cause a NameError at runtime. Additionally, the code uses deprecated 'airflow.operators.python_operator' import path instead of 'airflow.operators.python' which is the correct path for Airflow 3.0.6.",0,"The instruction explicitly requires using 'the current date as a dynamic parameter' in the Cypher query. The code completely fails to implement this requirement - there is no date parameter being passed to the query, no use of Airflow's templating or execution_date, and the undefined 'tom_hanks_query' variable shows no evidence of parameterization. Additionally, while the DAG is scheduled daily, the core requirement of dynamically passing the current date to the query is missing.",0.0
538,Baseline (Qwen 2.5 1.5B Instruct),233,1,"The code correctly uses the DatabricksSQLOperator from airflow.providers.databricks.operators.databricks_sql, which is the idiomatic and native operator for executing SQL against Databricks SQL endpoints. It does not wrap logic in PythonOperator with hooks, demonstrating proper use of Airflow providers.",0,"The DatabricksSQLOperator does not support parameters like 'output_mode', 'output_format', and 'output_file_path' as used in the export_to_csv_task. These parameters are hallucinated and do not exist in the actual operator interface. The DatabricksSQLOperator is designed to execute SQL statements and optionally return results, but does not have built-in file export functionality with these specific parameters.",1,"The DAG fulfills the user instruction by demonstrating interaction with Databricks SQL endpoints through multiple SQL operations: table creation (CREATE TABLE), data insertion (INSERT INTO), data selection (SELECT), and attempted file operations (export/import). While the implementation of file operations has technical issues, the intent to showcase different SQL operations including table creation, data selection, file export, and data import is present and aligns with the instruction requirements.",0.6666666666666666
539,Baseline (Qwen 2.5 1.5B Instruct),234,0,"The code imports PostgresOperator and MySqlOperator from 'airflow.providers.google.cloud.operators.sql', which is incorrect. These operators do not exist in the Google Cloud SQL provider. The correct imports should be 'airflow.providers.postgres.operators.postgres.PostgresOperator' and 'airflow.providers.mysql.operators.mysql.MySqlOperator'. This is a fundamental error in using the appropriate Airflow providers and operators.",0,"The code hallucinates non-existent operators by importing PostgresOperator and MySqlOperator from the wrong provider package (airflow.providers.google.cloud.operators.sql). These operators do not exist in that module. While the code doesn't include test harness boilerplate, the incorrect imports represent a form of hallucination where the AI invented incorrect import paths.",0,"While the code attempts to fulfill the requirements of table creation, data insertion, and table deletion across PostgreSQL and MySQL, the task dependencies are logically broken. The dependency chain creates a circular dependency (create_postgres_table depends on create_mysql_table, which depends on delete_mysql_table, which depends on insert_data_mysql, which depends on create_mysql_table). This makes the DAG unexecutable. Additionally, the instruction asks for 'Cloud SQL environment' interaction, but the code uses generic postgres/mysql connections rather than Google Cloud SQL specific operators or connections, missing the cloud-specific context.",0.0
540,Baseline (Qwen 2.5 1.5B Instruct),235,1,"The code uses appropriate GCP-specific operators from airflow.providers.google.cloud including GoogleCloudStorageBucketCreateOperator, GoogleCloudStorageFileUploadOperator, DataFlowTemplateOperator, BigQueryInsertJobOperator, and GCSToGCSOperator. It does not wrap functionality in PythonOperator with hooks, but instead uses the native operators designed for these tasks.",1,"The code imports only standard Airflow libraries from the official Google Cloud provider package. There are no imports from test utilities, internal testing modules, or test harness boilerplate. All imports are production-ready and appropriate for Airflow 2.7.2.",0,"While the code implements bucket creation, file uploads, and template-based data processing, it fails to properly implement 'automated cleanup of temporary resources'. The cleanup_task uses GCSToGCSOperator which is designed for copying/moving files between buckets, not for deleting temporary resources. The proper operator for cleanup would be GoogleCloudStorageDeleteOperator or similar deletion operator. Additionally, the source_objects parameter format is incorrect (should not include 'gs://' prefix). The cleanup logic does not actually delete the temporary files as requested.",0.6666666666666666
541,Baseline (Qwen 2.5 1.5B Instruct),236,0,"The DAG fails to use idiomatic Airflow patterns. It wraps Vision API and BigQuery operations in PythonOperator with Hooks (VisionHook, BigQueryHook) instead of using native Operators. For example, it should use CloudVisionAnnotateImageOperator for image annotation and BigQueryInsertJobOperator for BigQuery operations. The process_images and store_results functions are defined after they're referenced, and they perform complex operations that should be handled by dedicated operators.",0,"The code contains several issues: (1) VisionHook methods like 'list_blobs', 'copy_blob', and 'annotate_image' don't exist in the standard Airflow VisionHook API - these appear to be hallucinated methods. (2) The function definitions are placed after the DAG definition where they're referenced, which would cause NameError at parse time. (3) Uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python'. (4) BigQueryHook.dataset_table_ref() is not a standard method.",0,"The instruction asks for an end-to-end workflow showcasing 'creating, managing, and analyzing image resources' with Google Cloud Vision's 'product and image management capabilities'. The code only attempts basic image annotation and doesn't demonstrate: (1) Creating product sets or products (key Vision Product Search features), (2) Managing reference images, (3) Product search functionality, (4) Proper analysis workflow. It only shows a basic copy-annotate-store pattern that doesn't fulfill the comprehensive product and image management requirements.",0.0
542,Baseline (Qwen 2.5 1.5B Instruct),237,0,"The code uses PythonOperator with hooks instead of native AWS operators. For DynamoDB to S3 export, it should use DynamoDBToS3Operator or similar AWS-specific operators from airflow.providers.amazon.aws.transfers or airflow.providers.amazon.aws.operators.dynamodb. The pattern of wrapping S3Hook calls in PythonOperator is not idiomatic when dedicated operators exist for AWS service interactions.",1,"The code uses standard Airflow libraries (airflow.operators.python_operator.PythonOperator and airflow.providers.amazon.aws.hooks.s3.S3Hook). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective, though it has logical issues.",0,"The code fails to implement the requested functionality. The instruction asks for DynamoDB to S3 backup with point-in-time recovery and incremental export capabilities. However, the export_to_s3 function only lists and downloads existing S3 objects (doesn't export from DynamoDB at all), and the restore_from_s3 function uploads a CSV file that doesn't exist. The code doesn't demonstrate: (1) actual DynamoDB data export, (2) incremental backup strategies, (3) proper point-in-time recovery from DynamoDB, or (4) any DynamoDB interaction whatsoever. The logic is fundamentally incorrect and doesn't fulfill the business requirements.",0.3333333333333333
543,Baseline (Qwen 2.5 1.5B Instruct),238,1,"The code uses native Airflow providers and operators from airflow.providers.google.cloud.operators.datacatalog. It properly leverages GoogleCloudDataCatalogCreateEntryOperator, GoogleCloudDataCatalogCreateTagTemplateOperator, and other specialized operators instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for GCP Data Catalog operations.",0,"The code imports GoogleCloudDataCatalogHook but never uses it, which is minor. More critically, several operators used do not exist or have incorrect signatures in Airflow 2.7.2. Specifically: GoogleCloudDataCatalogUpdateTagTemplateOperator, GoogleCloudDataCatalogCreateTagBindingOperator are not standard operators in the official provider package. The CreateEntryOperator also has incorrect parameter usage (tags parameter and schema parameter are not valid for this operator in the actual API). The CreateTagOperator has incorrect parent parameter - tags are created under entries, not tag templates. These represent hallucinated APIs that don't match the actual Airflow GCP provider implementation.",0,"The instruction specifically asks to 'register, tag, and catalog cloud storage resources' for a 'Google Cloud Storage bucket'. However, the code does not reference any actual GCS bucket. The create_entry task should specify a GCS bucket as the resource being cataloged (using gcs_fileset_spec or linked_resource pointing to a GCS bucket URI). Instead, it creates generic entries with arbitrary schemas that are not connected to any real GCS resource. The workflow demonstrates Data Catalog operations in abstract but fails to fulfill the core requirement of cataloging an actual GCS bucket for data governance and discoverability.",0.3333333333333333
544,Baseline (Qwen 2.5 1.5B Instruct),239,0,"The code uses deprecated operators that are not idiomatic for Airflow 3.0.1. It imports 'PythonOperator' from 'airflow.operators.python_operator', 'PostgresOperator' from 'airflow.operators.postgres_operator', and 'BashOperator' from 'airflow.operators.bash_operator'. In Airflow 2.0+, these should be imported from 'airflow.operators.python', 'airflow.providers.postgres.operators.postgres', and 'airflow.operators.bash' respectively. The deprecated import paths are not idiomatic for modern Airflow versions.",1,"The code does not import any internal testing modules or include test harness boilerplate. While it uses deprecated import paths, these are still standard Airflow libraries (albeit outdated). There are no hallucinated functions like 'get_test_run' or imports from 'tests_common'. The code is clean from a hallucination perspective, though it has a missing import for 'datetime'.",1,"The DAG fulfills the instruction requirements: (1) prints context via PythonOperator with a lambda that prints a message, (2) logs SQL queries using BashOperator to echo SQL statements, (3) performs time-based operations with a sleep command, and (4) demonstrates isolated Python environment execution by running a Python command that prints the executable path. All four requested components are present and sequenced appropriately.",0.6666666666666666
545,Baseline (Qwen 2.5 1.5B Instruct),241,0,"While the code uses Google Cloud provider operators (GCSToGCSOperator and Vision operators), there are critical issues with operator usage. The operators GoogleCloudVisionAnnotateImageOperator, GoogleCloudVisionDetectTextOperator, GoogleCloudVisionLabelDetectionOperator, and GoogleCloudVisionObjectLocalizationOperator do not exist in the standard Airflow Google provider package for version 2.8.4. The actual operator available is CloudVisionCreateProductSetOperator, CloudVisionAnnotateImageOperator (with different parameters), or users should use CloudVisionImageAnnotateOperator. Additionally, the code attempts to use copy_image_task.output incorrectly - GCSToGCSOperator doesn't return the destination path in a format that can be used with f-string interpolation like this. The correct approach would be to hardcode the destination path or use XCom properly.",0,"The code imports operators that do not exist in the standard Airflow Google Cloud provider library: GoogleCloudVisionAnnotateImageOperator, GoogleCloudVisionDetectTextOperator, GoogleCloudVisionLabelDetectionOperator, and GoogleCloudVisionObjectLocalizationOperator. These are hallucinated operator names. The actual Vision API integration in Airflow uses different operator names like CloudVisionImageAnnotateOperator or requires using the generic operator with proper request formatting.",1,"Despite the technical implementation issues, the code does attempt to fulfill the user's instruction. It demonstrates: (1) copying an image file between GCS buckets using GCSToGCSOperator, (2) performing multiple types of computer vision analysis (label detection, text detection, object localization), (3) orchestrating these as modular task chains with proper dependencies, and (4) showcasing cloud service integration. The logical structure and intent align with the requirements for advanced cloud service integration and complex data processing workflows, even though the specific operators used are incorrect.",0.3333333333333333
546,Baseline (Qwen 2.5 1.5B Instruct),242,0,"The DAG uses PythonOperator with hooks (TrinoHook, BigQueryHook) instead of native operators. For the tasks described, Airflow provides specific operators like GCSCreateBucketOperator, BigQueryCreateEmptyDatasetOperator, TrinoOperator, and BigQueryCreateExternalTableOperator. The code wraps all logic in PythonOperator callables rather than using these idiomatic, purpose-built operators.",0,"The code references undefined functions (create_bucket, create_dataset, extract_data_from_trino, generate_external_table_in_bigquery) that are never implemented. It also imports 'timedelta' without the proper import statement (missing 'from datetime import timedelta'). Additionally, it uses the deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 3.0.6. The code is incomplete and not production-ready.",0,"While the DAG structure attempts to address the instruction (bucket creation, dataset creation, data extraction from Trino, external table in BigQuery), it fails to implement any actual logic. The instruction requires 'a robust, idempotent workflow that handles resource provisioning and cleanup automatically,' but there is no cleanup logic, no idempotency checks, and no actual implementation of the core functionality. The functions are referenced but not defined, making this a skeleton rather than a working solution.",0.0
547,Baseline (Qwen 2.5 1.5B Instruct),243,0,"The code attempts to use Vision API operators, but these operators do not exist in the standard Airflow providers package. The imports `GoogleCloudVisionCreateProductCatalogOperator` and `GoogleCloudVisionCreateProductOperator` are not real operators in airflow.providers.google.cloud.operators.vision. The actual Vision provider only contains operators like `CloudVisionCreateProductSetOperator`, `CloudVisionCreateProductOperator`, `CloudVisionCreateReferenceImageOperator`, and `CloudVisionDetectImageLabelsOperator` (or the generic `CloudVisionAnnotateImageOperator`). The code invents non-existent operators rather than using the correct ones from the provider.",0,"The code hallucinates multiple non-existent operators. `GoogleCloudVisionCreateProductCatalogOperator` does not exist in Airflow's Google Cloud Vision provider. The actual provider uses `CloudVisionCreateProductSetOperator` for creating product sets (not catalogs). Additionally, the code references `.output` attributes on operators (e.g., `create_product_catalog_task.output`, `transfer_images_task.output`) in a way that suggests direct output passing, which is not how these operators work without proper XCom handling. The operator names and their usage patterns are fabricated.",1,"Despite the technical implementation issues, the DAG structure does attempt to fulfill the user's requirements: it transfers images (GCSToGCSOperator), creates product catalogs/products (attempted with Vision operators), and performs automated image analysis (annotate_image_task). The logical flow demonstrates an end-to-end pipeline for image processing and ML operations using Google Cloud Vision APIs as requested, even though the specific operators used are incorrect.",0.3333333333333333
548,Baseline (Qwen 2.5 1.5B Instruct),244,0,"The code does not use EKS-specific operators. Airflow 2.9.3 provides `airflow.providers.amazon.aws.operators.eks` module with operators like `EksCreateClusterOperator`, `EksCreateNodegroupOperator`, `EksDeleteClusterOperator`, etc. Instead, the code incorrectly uses EC2 operators (EC2CreateInstanceOperator, EC2TerminateInstanceOperator) which are not the idiomatic way to provision EKS clusters. EKS is a managed Kubernetes service that should be provisioned via EKS-specific operators, not by manually creating EC2 instances.",0,The code imports operators that either don't exist or are incorrectly named in Airflow 2.9.3. Specifically: (1) `EC2WaitConditionOperator` does not exist in the standard AWS provider package - the correct operator would be `EC2StartInstanceOperator` with wait functionality or using sensors. (2) The `resource_id` parameter format and usage in EC2WaitConditionOperator appears fabricated. (3) Accessing `create_control_plane_instance.instance_id` as an attribute in task dependencies is not how XCom/task outputs work in Airflow - this would need proper XCom pulling or templating.,0,"The instruction explicitly asks for 'provisioning and managing an Amazon EKS cluster with dynamic nodegroup creation' and 'infrastructure-as-code deployment of Kubernetes resources using Airflow's AWS provider operators'. The provided code: (1) Does not create an EKS cluster at all - it only creates standalone EC2 instances. (2) Does not create nodegroups, which are EKS-specific constructs. (3) Does not deploy any Kubernetes resources. (4) Does not demonstrate EKS management capabilities. (5) Simply creates and immediately terminates EC2 instances without any EKS or Kubernetes integration. The code fundamentally misunderstands the requirement and delivers EC2 instance management instead of EKS cluster provisioning.",0.0
549,Baseline (Qwen 2.5 1.5B Instruct),245,0,"The DAG uses PythonOperator for the core ML training task ('train_translation_model') with a reference to 'train_translation_model_function' which is not defined. For a proper ML workflow on Google Cloud, it should use native operators like VertexAICreateCustomTrainingJobOperator or similar from airflow.providers.google.cloud.operators.vertex_ai. The use of GoogleCloudEndpointsDeployServiceOperator is also questionable for model deployment - modern GCP ML workflows typically use Vertex AI Model/Endpoint operators instead.",0,"The code references 'train_translation_model_function' which is never defined, making this non-executable. Additionally, GoogleCloudEndpointsDeployServiceOperator is used incorrectly - this operator is for API Gateway/Cloud Endpoints service deployment, not ML model deployment. The operator also references parameters that don't match the actual operator signature (e.g., 'version_name', 'endpoint_config_path' are not valid parameters for this operator in Airflow 2.8.4).",0,"While the DAG attempts to show a workflow with data movement (GCS to BigQuery), dataset creation, and model training/deployment, it fails to deliver a working solution. The instruction asks for 'automated model training processes' but the training logic is completely missing (undefined function). The use of Cloud Endpoints for model deployment doesn't align with typical ML model serving patterns. The DAG structure shows dependencies but lacks the actual implementation of the translation model training workflow requested.",0.0
550,Baseline (Qwen 2.5 1.5B Instruct),246,0,"The code fails idiomatic Airflow practices in multiple ways: (1) Uses deprecated 'PythonOperator' and 'DockerOperator' imports instead of the correct Airflow 3.0.1 paths (should be 'airflow.operators.python.PythonOperator' and 'airflow.providers.docker.operators.docker.DockerOperator'). (2) For system command execution, it wraps a print statement in PythonOperator instead of using the native BashOperator which is specifically designed for executing system commands. (3) Uses 'python_operator' module name which doesn't exist in Airflow 3.x - it should be 'airflow.operators.python'.",0,The code uses incorrect/non-existent import paths for Airflow 3.0.1. The imports 'from airflow.operators.python_operator import PythonOperator' and 'from airflow.operators.docker_operator import DockerOperator' are outdated and will cause ImportError in Airflow 3.0.1. These modules were restructured in Airflow 2.0+. The correct imports should be 'from airflow.operators.python import PythonOperator' and 'from airflow.providers.docker.operators.docker import DockerOperator'.,0,"The instruction asks for 'system command execution' but the code uses PythonOperator with print() instead of actually executing system commands (e.g., using BashOperator with shell commands like 'echo', 'ls', etc.). While task chaining is demonstrated (>>), and there's an attempt at containerized task running with DockerOperator, the core requirement of demonstrating actual system command execution is not fulfilled. The code only prints messages rather than executing real system commands.",0.0
551,Baseline (Qwen 2.5 1.5B Instruct),247,0,"The code attempts to use Kylin-specific operators, which would be idiomatic if they existed. However, the imports reference operators that do not exist in the actual Apache Airflow Kylin provider. The real airflow.providers.apache.kylin package (as of Airflow 3.0.6) contains KylinCubeOperator for building cubes, not operators like KylinLoadOperator, KylinTransformOperator, KylinUpdateOperator, or KylinDeleteOperator. These are fabricated/hallucinated operators.",0,"The code imports and uses multiple non-existent operators (KylinLoadOperator, KylinTransformOperator, KylinUpdateOperator, KylinDeleteOperator) that are not part of the standard Airflow Kylin provider library. The actual provider only includes KylinCubeOperator. Additionally, 'timedelta' is used but not imported. The operator parameters (sql_file_path, warehouse_name, database_name, table_name) do not match the actual Kylin operator API, which typically works with project names and cube names.",0,"While the code attempts to demonstrate data cube transformation and maintenance processes with a pipeline structure (load -> transform -> update -> delete), it fails to use actual Kylin operators correctly. The instruction asks for showcasing data cube operations using Kylin operators, but the code uses fabricated operators rather than the real KylinCubeOperator which is designed for building and refreshing Kylin cubes. The business logic of systematic processing, updating, and managing analytical data structures is conceptually present but not implementable with the hallucinated operators.",0.0
552,Baseline (Qwen 2.5 1.5B Instruct),248,0,"The code uses KubernetesPodOperator to run kubectl commands inside busybox containers, which is a generic 'Pythonic' workaround. Airflow 3.0.0 provides native Kubernetes resource management operators like ResourceOperator or KubernetesCreateResourceOperator and KubernetesDeleteResourceOperator from airflow.providers.cncf.kubernetes.operators.resource that are specifically designed for managing Kubernetes resources (PVCs, ConfigMaps, etc.). Using kubectl commands in a pod is not idiomatic when native operators exist for resource lifecycle management.",1,"The code uses only standard Airflow libraries (airflow.providers.kubernetes.operators.kubernetes_pod_operator) and does not import any internal testing modules or include test harness boilerplate. The imports and code structure are clean and production-appropriate, even though the approach is not optimal.",0,"The instruction requires creating a PVC with 5GB storage allocation, but the code does not define or include the PVC specification (pvc.yaml is referenced but not provided). The kubectl commands assume external files and tools that won't exist in a busybox container without additional configuration. Additionally, the busybox image doesn't contain kubectl by default, making the commands non-functional. The workflow structure (create then delete) is present, but the actual implementation cannot fulfill the requirement of managing a 5GB PVC.",0.3333333333333333
553,Baseline (Qwen 2.5 1.5B Instruct),249,0,"The code uses PythonOperator with BigQueryHook and GoogleCloudStorageHook instead of the native operators designed for this task. Airflow provides BigQueryToGCSOperator specifically for exporting BigQuery data to GCS. The idiomatic approach would be: `from airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator`. Instead, the code wraps hook logic in Python callables, which is the anti-pattern described in the scoring criteria.",1,"The code uses only standard Airflow libraries and providers. There are no imports from internal testing modules or test harness boilerplate. All imports (BigQueryHook, GoogleCloudStorageHook, PythonOperator) are legitimate Airflow components, even though they're not the optimal choice for this use case.",0,"While the code attempts to fulfill the instruction of transferring data from BigQuery to GCS, it has critical implementation flaws that prevent it from working correctly. The XCom mechanism used to pass the DataFrame between tasks won't work as intended because DataFrames cannot be serialized through XCom in this manner. Additionally, the load_data_to_gcs function receives a string template instead of the actual DataFrame, and the GCS upload logic has errors (incorrect bucket path format, wrong content_type parameter). The workflow would fail in execution, thus not properly demonstrating a working data migration as requested.",0.3333333333333333
554,Baseline (Qwen 2.5 1.5B Instruct),250,1,"The code uses appropriate Airflow providers and operators for the task. It uses SparkSubmitOperator from airflow.providers.apache.spark and DatabricksSubmitOperator from airflow.providers.databricks, which are the idiomatic operators for Spark and Databricks operations. It does not wrap logic in PythonOperator with hooks, but instead uses the native operators designed for these integrations.",1,The code imports only standard Airflow libraries from official providers (apache.spark and databricks). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code is clean and uses production-appropriate imports.,0,"The code fails to fulfill the specific requirements of the instruction. The instruction asks for 'table creation, data import, transformation, and output generation' with 'flexibility of connecting and manipulating data across different storage systems.' However, the code has critical issues: (1) The SparkSubmitOperator tasks point to '/usr/local/spark/examples/src/main/python/README.md' which is a markdown file, not executable Spark code; (2) There is no explicit table creation step; (3) The tasks are placeholders without actual data processing logic; (4) The code doesn't demonstrate connecting to external file sources or different storage systems in a meaningful way. While the structure suggests a pipeline, the implementation is incomplete and non-functional for the stated business requirements.",0.6666666666666666
555,Baseline (Qwen 2.5 1.5B Instruct),251,0,"This code is not an Airflow DAG at all. It is a standalone Apache Beam pipeline script with no Airflow imports, no DAG definition, no operators, and no task orchestration. There is no `from airflow import DAG`, no task definitions, and no use of Airflow operators like BeamRunPythonPipelineOperator, DataflowCreatePythonJobOperator, or PubSubCreateTopicOperator. This appears to be raw Beam code that would need to be wrapped in Airflow operators to be considered idiomatic Airflow.",1,"While the code is not an Airflow DAG, it does not contain any hallucinated imports from internal testing modules or test harness boilerplate. The imports and code structure are standard Apache Beam libraries (though incomplete and with errors like missing 'import beam' and 'import json'). There are no references to test utilities or non-standard internal modules.",0,"The instruction explicitly asks for an 'Airflow DAG' that demonstrates a streaming data processing system with automated infrastructure management and a scalable, repeatable solution. The provided code is a raw Apache Beam pipeline with no Airflow orchestration, no DAG definition, no task scheduling, no infrastructure management operators (like creating Pub/Sub topics/subscriptions), and no demonstration of Airflow's orchestration capabilities. It completely fails to deliver an Airflow-based solution as requested.",0.3333333333333333
556,Baseline (Qwen 2.5 1.5B Instruct),252,1,"The code uses native Airflow providers and operators appropriately: GKECreateClusterOperator for cluster provisioning, KubernetesPodOperator for running containerized workloads, and GKEDeleteClusterOperator for cleanup. It does not wrap these operations in PythonOperator with hooks, which would be non-idiomatic. The operators are from the official airflow.providers.google.cloud and airflow.providers.kubernetes packages.",0,"The code imports 'GCSToGCSOperator' from airflow.providers.google.cloud.transfers.gcs_to_gcs but never uses it in the DAG. This is unnecessary and suggests potential code generation artifacts. Additionally, the import 'from airflow.utils.dates import datetime' is incorrect - datetime should be imported from the standard library, not from airflow.utils.dates (which doesn't export datetime in modern Airflow versions). While not a test harness leak, these are hallucinated/incorrect imports that would cause issues in production.",0,"The instruction explicitly requires 'built-in compliance and verification steps' as part of the infrastructure deployment pipeline. The generated code provisions a cluster and runs a workload, but includes no verification or compliance checking steps. There are no tasks to verify cluster health, validate security configurations, check compliance policies, or confirm successful workload execution. The instruction emphasizes 'secure metadata handling' and 'compliance and verification steps' which are not demonstrated beyond basic metadata parameters in the cluster creation.",0.3333333333333333
557,Baseline (Qwen 2.5 1.5B Instruct),253,0,"The DAG uses SnowflakeOperator and GCSToBigQueryOperator which are idiomatic, but the transform_data_task uses BashOperator to call a Python script instead of using appropriate cloud-based transformation operators. The instruction specifically mentions 'cloud-based data preparation tool' but the code wraps transformation logic in a generic BashOperator calling an external script. For a proper idiomatic approach, it should use operators like DataprocSubmitJobOperator, DataflowTemplatedJobStartOperator, or similar cloud-native transformation operators. Additionally, BashOperator is not imported, causing a runtime error.",0,"The code imports 'GCSEndpointHook' which is never used in the DAG, indicating unnecessary or confused imports. More critically, 'BashOperator' is used in the code but never imported, which would cause the DAG to fail at parse time. The import statement 'from airflow.providers.apache.snowflake.operators.snowflake import SnowflakeOperator' is also incorrect for Airflow 3.0.0 - the correct path should be 'airflow.providers.snowflake.operators.snowflake'. These issues indicate hallucinated or incorrect module paths.",0,"The instruction asks for ingesting from an external source, transforming using a cloud-based data preparation tool, and outputting to cloud storage. However, the implementation has several misalignments: (1) It uses Snowflake for ingestion from S3, then loads to BigQuery - mixing two different cloud data warehouses unnecessarily; (2) The transformation step uses a local Python script via BashOperator rather than a cloud-based preparation tool (e.g., Dataprep, Dataflow, Glue, Databricks); (3) The workflow is not easily configurable (hardcoded credentials, paths) or scalable. The solution lacks cohesion and doesn't demonstrate a proper cloud-native data engineering pattern as requested.",0.0
558,Baseline (Qwen 2.5 1.5B Instruct),254,0,"The code fails to use idiomatic Airflow patterns. For a database-to-database transfer operation, it should use specific database operators (e.g., SqlOperator, PostgresOperator, MySqlOperator) or transfer operators (e.g., GenericTransfer, specific provider transfer operators). Instead, it wraps all logic in PythonOperators with empty placeholder functions that just print messages. This is the anti-pattern of using generic PythonOperators instead of purpose-built database operators from Airflow providers.",0,"The code contains an error: it imports 'PythonOperator' from 'airflow.operators.python_operator' (deprecated path in Airflow 3.x) and uses 'timedelta' without importing it from datetime. In Airflow 3.0.1, the correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is referenced but not imported. While not a hallucination of test utilities, these are significant code quality issues that would prevent the DAG from running.",0,"The instruction explicitly requests a 'complete data transfer operation between two database tables' with 'source data preparation, data movement, and destination table validation.' The provided code only creates a skeleton structure with empty placeholder functions that print messages but contain no actual implementation. There is no database connection, no actual data extraction, no data loading, and no validation logic. The code does not demonstrate any actual data transfer capability or systematic approach - it's merely a template with the correct task names but no functional business logic.",0.0
559,Baseline (Qwen 2.5 1.5B Instruct),255,0,"The code attempts to use specific Google Cloud operators, which is good practice. However, it imports 'SpeechTranslateOperator' from 'airflow.providers.google.cloud.operators.speech', which does not exist in the Airflow provider ecosystem. There is no native Google Cloud Speech Translation operator in Airflow. The correct approach would be to use the Google Cloud Translation API operator or implement translation using the appropriate Google Cloud service (Cloud Translation API, not Speech API). The code also uses non-existent operators with incorrect parameters.",0,"The code hallucinates the 'SpeechTranslateOperator' which does not exist in airflow.providers.google.cloud.operators.speech. Additionally, the TextToSpeechSynthesizeOperator parameters shown (like 'text', 'voice_name', 'output_uri') do not match the actual operator's API in Airflow 3.0.6. The actual operator requires different parameters such as 'input_data' with proper configuration objects. This represents hallucination of non-existent operators and incorrect API usage.",0,"While the code attempts to create a multi-step workflow with task dependencies and Google Cloud integration as requested, it fails to implement a working solution. The instruction asks for 'text-to-speech synthesis and speech translation', but the code uses non-existent operators and incorrect logic flow. The storage upload/download tasks (Steps 3-4) don't logically fit the audio processing workflow described. The workflow should synthesize text to speech, then translate that speech (or the text), but the implementation is fundamentally broken with hallucinated operators and illogical task sequencing.",0.0
560,Baseline (Qwen 2.5 1.5B Instruct),257,1,"The code uses native AWS ECS operators from airflow.providers.amazon.aws.operators.ecs (EcsCreateClusterOperator, EcsRegisterTaskDefinitionOperator, EcsRunTaskOperator, EcsDeleteClusterOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for ECS operations.",0,"The code contains several issues: (1) EcsCreateClusterOperator is being passed 'task_definition_family' parameter which is not a valid parameter for cluster creation - this appears to be a hallucinated parameter. (2) The task_id parameter is missing from all operators, which is required in Airflow. (3) The 'dag=dag' pattern is deprecated in Airflow 3.0.0 in favor of context managers. While not hallucination per se, these represent non-production-ready code quality issues that suggest the code wasn't properly validated against actual Airflow 3.0.0 APIs.",1,"The DAG fulfills all requirements from the instruction: (1) Creates an ECS cluster, (2) Registers a task definition with a container and basic command ('echo Hello from ECS!'), (3) Runs the container on Fargate, and (4) Cleans up resources by deleting the cluster. The workflow demonstrates the complete lifecycle as requested, with proper task dependencies.",0.6666666666666666
561,Baseline (Qwen 2.5 1.5B Instruct),259,0,"The code uses 'DataFlowTemplateOperator' which does not exist in Airflow providers. The correct operator for Airflow 3.0.0 is 'DataflowTemplatedJobStartOperator' from airflow.providers.google.cloud.operators.dataflow. This is a critical error that shows lack of knowledge of the actual Airflow provider APIs. Additionally, the 'template_path' parameter is incorrect - Dataflow templates use GCS paths (gs://) for template locations, not local file paths.",0,The code imports and uses 'DataFlowTemplateOperator' which is a hallucinated/non-existent class. The actual operator in airflow.providers.google.cloud.operators.dataflow is 'DataflowTemplatedJobStartOperator' or 'DataflowStartFlexTemplateOperator'. This represents a fabricated API that doesn't exist in the Airflow codebase.,0,"While the DAG attempts to address the instruction by including a Dataflow operation and BigQuery transfer, it fails on multiple fronts: (1) The Dataflow operator is incorrectly implemented with a non-existent class, (2) The template_path points to a Python file rather than a proper Dataflow template location, (3) The BigQueryToBigQueryOperator at the end is redundant since the Dataflow job should already write to the final BigQuery table directly, and (4) Missing task dependencies definition (no >> or set_upstream/downstream). The workflow doesn't properly implement the requested 'load -> transform with Dataflow -> write to new table' pattern.",0.0
562,Baseline (Qwen 2.5 1.5B Instruct),260,0,"The code uses PythonOperator with TrinoHook instead of the native TrinoOperator. This is the anti-pattern described in the scoring criteria. The idiomatic approach would be to use `from airflow.providers.trino.operators.trino import TrinoOperator` and pass SQL queries directly to it, rather than wrapping TrinoHook calls in Python functions.",0,"The code imports `PythonOperator` from the deprecated module path `airflow.operators.python_operator`. In Airflow 3.0.6, this should be `from airflow.operators.python import PythonOperator`. Additionally, `timedelta` is used in default_args but never imported from datetime module, which would cause a runtime error.",1,"The code fulfills all requirements from the instruction: it creates a schema (city_data), generates a table with predefined structure (city table with id, name, population, area, latitude, longitude), inserts sample city records (New York, Los Angeles, Chicago), uses Trino as the processing platform, and demonstrates incremental complexity through proper task dependencies (schema -> table -> data).",0.3333333333333333
563,Baseline (Qwen 2.5 1.5B Instruct),261,1,"The code uses appropriate GCS-specific operators from airflow.providers.google.cloud, including GCSBucketCreateOperator and GCSToGCSOperator. These are the idiomatic Airflow operators for Google Cloud Storage operations rather than wrapping logic in PythonOperator with hooks.",1,"The code imports only standard Airflow libraries from the official providers package. There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run().",0,"The instruction explicitly requires 'generates random binary files' as a key component of the pipeline. However, the code does NOT generate any files - it only attempts to transfer files that are assumed to already exist. The upload_file_task uses GCSToGCSOperator which copies from one GCS bucket to another, but there's no task that actually creates/generates the random binary files. To fulfill the instruction, the DAG should include a task (likely using PythonOperator or BashOperator with LocalFilesystemToGCSOperator) that generates random binary files and uploads them to GCS. The current implementation misses this critical step of file generation.",0.6666666666666666
564,Baseline (Qwen 2.5 1.5B Instruct),262,0,"The code uses deprecated and incorrect operators. It imports 'PythonOperator' from 'airflow.operators.python_operator' which is deprecated in Airflow 3.x (should be 'airflow.operators.python'). More critically, it uses 'MLEngineDeployModelOperator' and 'MLEngineDeleteModelOperator' from 'airflow.providers.google.cloud.operators.mlengine', but these are for AI Platform (legacy ML Engine), not AutoML. For AutoML orchestration, it should use operators from 'airflow.providers.google.cloud.operators.vertex_ai' or 'airflow.providers.google.cloud.operators.automl'. The 'BigQueryInsertJobOperator' is also misused - it's for running jobs, not creating tables with schema. The code conflates different GCP ML services and doesn't use the appropriate AutoML operators.",1,"The code does not import any internal testing modules or include test harness boilerplate. All imports are from standard Airflow libraries, even though some are incorrectly used or deprecated. No evidence of 'tests_common' or 'get_test_run' patterns.",0,"The instruction requires 'AutoML model training' and 'automatic cleanup of resources after model training'. While the code attempts to create a dataset, train a model, and delete a model, it fails on multiple fronts: (1) It doesn't actually train an AutoML model - MLEngineDeployModelOperator is for deploying pre-trained models, not training; (2) The cleanup is incomplete - it only deletes the model but not the BigQuery dataset created earlier; (3) The BigQueryInsertJobOperator usage is incorrect for data insertion. The workflow doesn't demonstrate proper AutoML orchestration as requested.",0.3333333333333333
565,Baseline (Qwen 2.5 1.5B Instruct),264,0,"The code fails to use idiomatic Airflow operators. It imports hooks (S3Hook, RedshiftHook, PostgresHook) but wraps all logic in PythonOperator tasks instead of using native operators like S3ToRedshiftOperator, RedshiftDataOperator, or RedshiftSQLOperator. This is the anti-pattern of wrapping hooks in PythonOperators rather than using purpose-built operators from Airflow providers.",0,"The code references undefined functions (provision_redshift, load_sample_data, establish_connection, transfer_data, cleanup_resources) that are never implemented. These function references make the code non-executable and incomplete. While it doesn't import test harness modules, the code is not production-ready as it contains placeholder function calls without implementations.",0,"While the DAG structure attempts to address the instruction's requirements (provision resources, load data, establish connections, transfer data), it fails to provide actual implementation. The instruction asks for a working pipeline that 'transfers cocktail recipe data from S3 to Redshift' and 'demonstrates end-to-end cloud data migration', but the code only provides empty task shells without any actual data transfer logic, S3 bucket references, Redshift table definitions, or COPY commands.",0.0
566,Baseline (Qwen 2.5 1.5B Instruct),265,1,"The code correctly uses native SageMaker operators from airflow.providers.amazon.aws.operators.sagemaker (SageMakerCreateNotebookInstanceOperator, SageMakerStartNotebookInstanceOperator, SageMakerStopNotebookInstanceOperator, SageMakerDeleteNotebookInstanceOperator) instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for AWS SageMaker operations.",1,"The code uses only standard Airflow libraries and AWS provider packages. There are no imports from internal testing modules (like tests_common.test_utils.system_tests) or test harness boilerplate. The code is clean and production-ready in structure, though it contains placeholder values for AWS resources.",1,"The DAG successfully fulfills the instruction requirements: (1) illustrates step-by-step operational control of a cloud-based ML notebook using AWS SageMaker operators, (2) demonstrates dynamic instance management through create/start/stop/delete operations, and (3) showcases clear, sequential task dependencies (create >> start >> stop >> delete). The workflow comprehensively demonstrates infrastructure management tasks for SageMaker notebooks.",1.0
567,Baseline (Qwen 2.5 1.5B Instruct),266,0,"The code attempts to use specific AWS SQS operators (AwsSQSCreateQueueOperator, AwsSQSDeleteQueueOperator, AwsSQSSensor), which appears idiomatic. However, these operators do not exist in the standard Airflow AWS provider package. The actual AWS provider does not include dedicated SQS create/delete operators. The correct idiomatic approach would be to use CloudFormationCreateStackOperator and CloudFormationDeleteStackOperator (as mentioned in the instruction) or use PythonOperator with SqsHook. Since the code uses non-existent operators rather than the actual available Airflow operators, this fails the idiomatic test.",0,"The code imports operators that do not exist in the standard Airflow providers package: AwsSQSCreateQueueOperator, AwsSQSDeleteQueueOperator, and AwsSQSSensor are not real operators in airflow.providers.amazon.aws. This constitutes hallucination of non-existent Airflow components. The actual AWS provider (as of version 3.0.6) does not include these specific SQS operators.",0,"The instruction specifically requests 'CloudFormation stack management workflow' to create and delete an AWS SQS queue, but the code does not use any CloudFormation operators (CloudFormationCreateStackOperator, CloudFormationDeleteStackOperator, CloudFormationCreateStackSensor, etc.). Instead, it attempts to use direct SQS operators. The instruction emphasizes 'infrastructure provisioning and teardown' through CloudFormation, which is the proper IaC approach, but the implementation completely bypasses CloudFormation in favor of direct SQS operations.",0.0
568,Baseline (Qwen 2.5 1.5B Instruct),268,0,"The code attempts to use native operators but contains critical errors. It references 'GoogleCloudStorageUploadFileOperator' which doesn't exist in the imports (missing import statement). Additionally, it uses non-existent operators 'GoogleCloudMLEngineTrainOperator' and 'GoogleCloudMLEngineDeployModelOperator' which are not valid in Airflow 3.0.0. The correct operators for Vertex AI in modern Airflow would be from 'airflow.providers.google.cloud.operators.vertex_ai' package. The ML Engine operators are deprecated and the code shows incorrect usage patterns like accessing '.output' and '.outputs' attributes that don't work as shown.",0,"The code hallucinates several non-existent operators and APIs. 'GoogleCloudMLEngineTrainOperator' and 'GoogleCloudMLEngineDeployModelOperator' are not real operators in Airflow 3.0.0. The code also invents attributes like 'create_bucket_task.output' and 'train_model_task.outputs['model_name']' which are not valid Airflow patterns. Additionally, 'GoogleCloudStorageUploadFileOperator' is used but never imported, and the actual operator name in the GCS provider is 'GCSFileTransformOperator' or 'LocalFilesystemToGCSOperator' depending on the use case.",0,"While the code attempts to address the instruction's requirements (data ingestion via GCS, model training, and cleanup), it fails to implement a working solution. The instruction asks for 'dataset creation' which is not properly addressed - uploading a CSV file is not the same as creating a proper ML dataset. The model training and deployment operators are non-functional hallucinations. The cleanup step (delete bucket) would likely fail in practice as it attempts to delete a bucket that may still contain objects. The overall workflow structure is present but the implementation is fundamentally broken.",0.0
569,Baseline (Qwen 2.5 1.5B Instruct),270,0,"The code fails idiomatic Airflow practices. While it imports ML Engine operators, it wraps core ML workflow steps (preprocess_data, train_model, evaluate_model) in generic PythonOperators with placeholder print statements instead of using appropriate cloud-native operators. For a GCP ML workflow, it should use operators like DataprocSubmitJobOperator, VertexAITrainingJobOperator, or custom operators that actually interact with cloud services. The train_model function especially should use MlEngineTrainModelOperator (which is imported but never used) or similar cloud-based training operators rather than a hollow PythonOperator.",0,"The code imports 'airflow.operators.python_operator.PythonOperator' which is deprecated in Airflow 3.0.0. The correct import should be 'airflow.operators.python.PythonOperator'. Additionally, the imported operators 'MlEngineTrainModelOperator' and 'MlEngineListModelsOperator' are never used, and 'MlEngineHook' is instantiated at module level but never utilized. The 'MlEngineDeployModelOperator' parameters like 'package_path' and 'framework_version' appear to be mixing different deployment paradigms (AI Platform Legacy vs Vertex AI), suggesting confusion about actual API requirements.",0,"The instruction explicitly requests a DAG that 'orchestrates a complete machine learning lifecycle for image classification' and 'showcases how to programmatically manage cloud machine learning infrastructure.' The provided code only contains placeholder print statements for preprocessing, training, and evaluation—no actual ML logic or cloud infrastructure management. It fails to demonstrate real cloud service interactions for an end-to-end ML workflow. The only concrete operator (MlEngineDeployModelOperator) has placeholder values and doesn't represent a working deployment configuration. The DAG structure exists but lacks substantive implementation of the requested ML lifecycle.",0.0
570,Baseline (Qwen 2.5 1.5B Instruct),271,0,The code imports 'airflow.operators.pig_operator.PigOperator' which does not exist in Airflow 3.0.1 or any recent version. The correct import path should be 'airflow.providers.apache.pig.operators.pig.PigOperator' from the apache-airflow-providers-apache-pig package. This demonstrates a fundamental misunderstanding of Airflow's provider architecture and proper operator usage.,0,"The code contains multiple issues: (1) Uses a non-existent import path 'airflow.operators.pig_operator', (2) References 'timedelta' without importing it from datetime, (3) Uses 'DummyOperator' without importing it, (4) Uses string '2023-04-01' for start_date instead of a datetime object. These are hallucinated or incorrect patterns that would cause immediate runtime failures.",0,"While the code attempts to create a DAG that runs a Pig command to list directory contents as requested, the implementation is fundamentally broken due to incorrect imports and syntax errors. The code would not execute at all, failing to demonstrate 'basic Pig script execution' or serve as a 'learning example' since it cannot run. Additionally, 'ls /path/to/directory' is a shell command, not a valid Pig Latin script command.",0.0
571,Baseline (Qwen 2.5 1.5B Instruct),272,0,"The code uses only PythonOperator with placeholder functions instead of leveraging specific Airflow providers for metadata management. For a metadata management pipeline working with 'cloud data environments' and 'data catalog tag templates', it should use providers like Google Cloud Data Catalog (airflow.providers.google.cloud.operators.datacatalog), AWS Glue Data Catalog, or similar native operators. The current implementation wraps all logic in generic Python callables, which is the anti-pattern described in the scoring criteria.",1,"The code is clean and uses only standard Airflow libraries. There are no imports from internal testing modules or test harness boilerplate. While it uses the deprecated 'airflow.operators.python_operator' import path (should be 'airflow.operators.python'), this is a deprecation issue rather than hallucination.",0,"The instruction requests a pipeline for 'dynamic configuration and manipulation of data catalog tag templates' with 'systematic tracking and enhancement of metadata across cloud data environments'. The generated code only provides empty placeholder functions with print statements and no actual implementation. It lacks: (1) any actual data catalog integration, (2) dynamic configuration capabilities, (3) real metadata manipulation logic, (4) connection to cloud data environments, and (5) meaningful tracking or enhancement mechanisms. The task structure is reasonable but contains no business logic to fulfill the requirements.",0.3333333333333333
572,Baseline (Qwen 2.5 1.5B Instruct),273,0,"While the code attempts to use dbt Cloud providers, it contains critical errors. The import 'from airflow.sensors.dbt_cloud import DbtCloudJobStatusSensor' is incorrect - the proper import should be 'from airflow.providers.dbt.cloud.sensors.dbt import DbtCloudJobRunSensor'. Additionally, the DbtCloudRunJobOperator parameters are incorrect - it should use 'job_id' or 'account_id' and 'job_id', not 'project_name', 'profile_name', and 'job_name' which don't exist in the actual operator API. The code shows an attempt at idiomatic Airflow but fails due to incorrect API usage.",0,"The code contains hallucinated/non-existent parameters and import paths. The sensor import path 'airflow.sensors.dbt_cloud' does not exist in Airflow 2.9.3. The parameters 'project_name', 'profile_name', and 'job_name' are not valid parameters for DbtCloudRunJobOperator - the actual operator requires 'dbt_cloud_conn_id', 'job_id', and 'account_id'. These are fabricated APIs that don't exist in the actual Airflow dbt Cloud provider.",0,"The instruction asks for 'different methods of running and tracking dbt Cloud job runs with synchronous and asynchronous sensors.' While the code attempts to create operators and sensors, it fails to demonstrate the key requirement: the tasks are not connected in any workflow, there's no demonstration of synchronous vs asynchronous execution patterns (e.g., wait_for_completion parameter), and the sensors are created but never linked to the run operators. The code creates isolated tasks without establishing dependencies or demonstrating different monitoring approaches.",0.0
573,Baseline (Qwen 2.5 1.5B Instruct),274,0,The code uses non-existent operators. The correct operators in Airflow 3.0.6 for Google Cloud Compute Engine are 'ComputeEngineInsertInstanceTemplateOperator' and 'ComputeEngineCopyInstanceTemplateOperator'. There is no 'GCPComputeInstanceTemplateUpdateOperator' or 'GCPComputeInstanceTemplateCreateOperator' in the official Airflow providers. The import path and operator names are hallucinated and do not match the actual Airflow Google provider API.,0,"The code imports operators that do not exist in the Airflow codebase: 'GCPComputeInstanceTemplateCreateOperator', 'GCPComputeInstanceTemplateCopyOperator', and 'GCPComputeInstanceTemplateUpdateOperator'. These are fabricated operator names. The actual operators available are 'ComputeEngineInsertInstanceTemplateOperator' and 'ComputeEngineCopyInstanceTemplateOperator' from 'airflow.providers.google.cloud.operators.compute'. Additionally, instance templates in GCP are immutable and cannot be updated in-place, so the concept of an 'update' operator is fundamentally flawed.",0,"While the code attempts to demonstrate create, copy, and update operations as requested, it fails because the operators used are non-existent. Furthermore, the instruction asks to 'update instance templates', but GCP instance templates are immutable resources that cannot be updated after creation - they can only be versioned or replaced. The workflow does not reflect best practices for managing compute resources programmatically, as it uses invalid operators and demonstrates an impossible operation (updating an immutable template).",0.0
574,Baseline (Qwen 2.5 1.5B Instruct),275,0,"The code uses PythonOperator for the 'segment_by_date' task where the function 'segment_data_cubes_by_date' is defined AFTER it is referenced, which is non-idiomatic. Additionally, the import statement 'from airflow.operators.python_operator import PythonOperator' is deprecated in Airflow 3.0.0 (should be 'from airflow.operators.python import PythonOperator'). The segmentation logic is wrapped in a PythonOperator with just a print statement rather than using proper Kylin operators or demonstrating actual time-based segmentation through the KylinCubeOperator parameters.",0,The import 'from airflow.providers.apache.kylin.operators.kylin_cube_operator import KylinCubeOperator' appears to be fabricated. The actual Apache Kylin provider for Airflow does not have a 'kylin_cube_operator' module with a 'KylinCubeOperator' class. The correct import path and operator name do not match standard Airflow provider patterns. This is a hallucinated/non-existent operator.,0,"While the DAG attempts to demonstrate build, refresh, merge, disable, and purge operations as requested, it fails on several key requirements: (1) No demonstration of 'precise time-based segmentation' - the segment_by_date task only prints a message without actual implementation, (2) No XCom data passing is demonstrated between tasks as explicitly requested, (3) The function definition appears after its usage which would cause a runtime error, (4) The time-based segmentation is not integrated with the cube operations themselves. The instruction asked for a technical orchestration demonstrating these concepts, but the implementation is incomplete and non-functional.",0.0
575,Baseline (Qwen 2.5 1.5B Instruct),277,0,"The code uses PythonOperator for task skipping logic instead of idiomatic Airflow patterns. Airflow 2.8.4 provides native mechanisms like LatestOnlyOperator, max_active_runs=1, or BranchPythonOperator for controlling concurrent runs and skipping tasks. The approach of manually calling .skip() on task objects is not idiomatic and won't work as intended in Airflow's execution model.",0,"The code contains multiple hallucinated/non-existent Airflow API methods: 1) dag.get_current_dagrun() does not exist, 2) dag.get_active_runs(key=...) does not exist with this signature, 3) task.skip() is not a valid method to skip tasks from another task's context, 4) Missing import for 'datetime', 5) Functions process_data() and validate_data() are referenced but never defined. These are fabricated APIs that don't exist in Airflow.",0,"While the code attempts to address the requirement of preventing redundant task executions, it fails to implement a working solution. The instruction asks for a mechanism to ensure only the most recent workflow run is processed and to skip unnecessary runs when a newer instance is in progress. The code's approach is fundamentally flawed (non-existent APIs, incorrect skip logic) and would not execute successfully. A correct implementation would use max_active_runs=1, LatestOnlyOperator, or proper branching logic with task dependencies.",0.0
576,Baseline (Qwen 2.5 1.5B Instruct),278,0,"The code does not use Airflow 3.x's dynamic task mapping feature. Instead, it manually generates tasks in a loop using a custom function, which is an anti-pattern for Airflow 2.3+. The idiomatic approach would be to use the `.expand()` method with the `@task` decorator or `PythonOperator.partial().expand()` to leverage native dynamic task mapping. Additionally, it uses the deprecated `PythonOperator` import path instead of `airflow.operators.python.PythonOperator`.",1,"The code uses only standard Airflow libraries and does not import any internal testing modules or include test harness boilerplate. While it uses a deprecated import path, this is not a hallucination issue.",0,"The instruction asks for incrementing each number and then calculating their total sum. The code increments numbers but then squares them (incremented_number * incremented_number) instead of summing them. Additionally, there is no final task that aggregates/sums all the incremented values together. The workflow should have: (1) tasks that increment each number, and (2) a final task that sums all incremented values. The current implementation fails to meet both requirements.",0.3333333333333333
577,Baseline (Qwen 2.5 1.5B Instruct),280,1,"The code uses native Airflow operators designed for specific tasks: BigQueryCreateDatasetOperator for dataset creation, BigQueryInsertJobOperator for data transformation, SparkSubmitOperator for data import, and PostgresOperator for database export. It does not wrap logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator' but never uses it in the DAG. In Airflow 3.0.0, this import path is deprecated (should be 'airflow.operators.python'). Additionally, 'timedelta' is used in default_args but never imported. The code also has configuration issues: BigQueryInsertJobOperator doesn't have 'destination_dataset_id' and 'destination_table_id' parameters at the top level - these should be inside the configuration dict. These issues indicate incomplete or incorrect code generation.",1,"The DAG fulfills the instruction requirements: it orchestrates a complex data processing workflow with multiple interconnected tasks (4 tasks with clear dependencies), includes cloud resource management (BigQuery dataset creation), data import (Spark CSV import), transformation (BigQuery query), and export (PostgreSQL insert). The tasks are properly chained with dependencies. While error handling could be more robust, the basic retry mechanism is present in default_args, and connection management is handled through Airflow's connection system (postgres_conn_id, project_id references).",0.6666666666666666
578,Baseline (Qwen 2.5 1.5B Instruct),281,0,"The code mixes patterns inappropriately. It defines a `drop_table()` function that uses MySQLHook (a Pythonic pattern), but then tries to use it with SQLOperator. The SQLOperator expects a SQL string, not a function call. The correct idiomatic approach would be to use MySqlOperator (from airflow.providers.mysql.operators.mysql) directly with SQL strings, not wrap Hook logic in a function. Additionally, `SQLOperator` is deprecated; the provider-specific `MySqlOperator` should be used.",0,"The code imports `SQLOperator` from `airflow.operators.sql_operator`, which does not exist in Airflow 3.0.0. The correct import path would be `airflow.providers.common.sql.operators.sql.SQLOperator` or preferably `airflow.providers.mysql.operators.mysql.MySqlOperator`. This represents either hallucination of non-existent imports or use of severely outdated/incorrect module paths.",0,"The instruction explicitly asks to 'demonstrate dropping tables using Airflow's SQL operators' and to drop tables 'sequentially'. While the code attempts to drop multiple tables, it has critical flaws: (1) The SQLOperator is being passed a function call `drop_table(table)` which executes immediately during DAG parsing, not during task execution, (2) There is no sequential dependency chain established between tasks (no `>>` or `set_downstream()`), so tables would drop in parallel, not sequentially as requested. The code fundamentally misunderstands how to use SQL operators.",0.0
579,Baseline (Qwen 2.5 1.5B Instruct),282,0,"The code uses deprecated 'PythonOperator' imported from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. Additionally, it references 'BranchPythonOperator' without importing it, and when it should be imported from 'airflow.operators.python'. The code also uses non-idiomatic patterns like 'depends_on' parameter which doesn't exist in Airflow - dependencies should be set using bit-shift operators (>> or <<) or set_upstream/set_downstream methods.",0,"The code contains multiple critical errors: (1) 'BranchPythonOperator' is used but never imported, (2) 'depends_on' parameter does not exist in Airflow operators, (3) The lambda function in branch_task tries to access 'check_even_task.output' which is not a valid way to access XCom values in task definitions, (4) The branching logic is fundamentally broken as it's defined after the branch targets and doesn't properly set up task dependencies. These are not standard Airflow patterns and represent hallucinated or incorrect API usage.",0,"The instruction asks for 'parallel pipeline branches with configurable success conditions' and tasks 'conditionally executed or skipped based on predefined trigger rules'. While the code attempts branching logic and mentions trigger_rule='all_failed', the implementation is fundamentally broken: (1) The branching logic doesn't work as written, (2) There are no actual parallel branches demonstrated, (3) The trigger rules are not properly showcased with multiple upstream tasks, (4) The 'configurable success conditions' aspect is not implemented. The code shows intent but fails to deliver a working demonstration of the requested concepts.",0.0
580,Baseline (Qwen 2.5 1.5B Instruct),283,0,"The code uses PythonOperator with AzureDataLakeHook instead of using the native ADLS operators. For Airflow 3.0.0, the idiomatic approach would be to use operators from airflow.providers.microsoft.azure.operators.adls (e.g., ADLSDeleteOperator) or the newer Azure Blob Storage operators. The pattern of wrapping hook methods in PythonOperator is explicitly non-idiomatic.",1,"The code uses standard Airflow libraries (airflow.providers.microsoft.azure.hooks.adls, airflow.operators.python_operator) without any test harness imports or internal testing modules. While the import path may be outdated for Airflow 3.0.0, it doesn't constitute hallucination - it's a legitimate (though possibly deprecated) import path.",0,The instruction explicitly requires TWO operations: 'uploading a local file and then immediately deleting it' to demonstrate 'automated file transfer and cleanup processes'. The provided code only implements the upload task and completely omits the deletion task. This is a critical missing component that fails to fulfill the core requirement of the instruction.,0.3333333333333333
581,Baseline (Qwen 2.5 1.5B Instruct),284,0,"The code imports operators that do not exist in the official Airflow providers. The imports 'GCloudAutoMLImageTrainingOperator' and 'GCloudAutoMLImageDeployModelOperator' from 'airflow.providers.google.cloud.operators.gcloud_aiplatform' are hallucinated. Similarly, 'GCSUploadOperator' and 'GCSDownloadOperator' from 'airflow.providers.google.cloud.operators.gcloud_storage' do not exist. The correct operators would be from 'airflow.providers.google.cloud.operators.vertex_ai' (e.g., CreateAutoMLImageTrainingJobOperator) and 'airflow.providers.google.cloud.transfers.local_to_gcs' (e.g., LocalFilesystemToGCSOperator). Since these are not real operators, this fails the idiomatic test.",0,"The code contains multiple hallucinated imports. The operators 'GCloudAutoMLImageTrainingOperator', 'GCloudAutoMLImageDeployModelOperator', 'GCSUploadOperator', and 'GCSDownloadOperator' do not exist in the standard Airflow provider libraries for Google Cloud. The correct module paths and operator names are different (e.g., airflow.providers.google.cloud.operators.vertex_ai.auto_ml contains CreateAutoMLImageTrainingJobOperator). This code would fail immediately upon execution due to import errors.",0,"While the DAG attempts to follow the instruction's lifecycle (dataset initialization, model training, and resource deletion), it fails in execution due to hallucinated operators. Additionally, the logic is flawed: 'init_auto_ml_training_task' and 'train_auto_ml_model_task' appear to be duplicates using the same operator with the same parameters, and 'delete_resources_task' incorrectly uses a training operator with a 'delete=True' parameter that doesn't exist. The instruction asks for dataset initialization, training, and resource deletion, but the implementation doesn't properly demonstrate these distinct phases with correct operators. The deployment step is also not part of the core requirement but was added, and resource deletion is not properly implemented.",0.0
582,Baseline (Qwen 2.5 1.5B Instruct),285,0,"The code uses PythonOperator for simple data generation and transformation tasks, which is acceptable for basic workflows. However, it imports from the deprecated 'airflow.operators.python_operator' module instead of the modern 'airflow.operators.python' module (PythonOperator should be imported from airflow.operators.python in Airflow 3.0.1). While PythonOperator is appropriate for custom logic, the deprecated import path makes this non-idiomatic for the specified Airflow version.",0,"The code has a critical bug: 'timedelta' is used in default_args but never imported. The code would fail immediately with a NameError. Additionally, 'python_callable=transformed_data' references a variable (the transformed data string) instead of the function 'transform_source_data', which is another critical error that would prevent execution.",0,"The code attempts to create a data processing workflow with two sequential tasks, but it has the dependency backwards. The instruction asks for one asset to 'trigger and inform' a subsequent asset, meaning source_data_task should run first, then transformed_data_task. However, the code uses 'source_data_task.set_upstream(transformed_data_task)', which makes the transformation run BEFORE the source data generation. The correct dependency should be 'source_data_task >> transformed_data_task' or 'transformed_data_task.set_upstream(source_data_task)'.",0.0
583,Baseline (Qwen 2.5 1.5B Instruct),286,0,"The code uses completely hallucinated operators that do not exist in Airflow. 'KubeletStartPodOperator' and 'KubeletStopPodOperator' are not real Airflow operators. The correct operator for running pods on Kubernetes/GKE is 'GKEStartPodOperator' or 'KubernetesPodOperator' from airflow.providers.google.cloud.operators.kubernetes_engine or airflow.providers.cncf.kubernetes.operators.pod. Kubelet is a Kubernetes node agent, not an Airflow operator concept. This demonstrates a fundamental misunderstanding of Airflow's Kubernetes integration.",0,"The code imports and uses non-existent operators: 'KubeletStartPodOperator' and 'KubeletStopPodOperator' from 'airflow.providers.google.cloud.operators.kubernetes_kubelet'. This module path does not exist in Airflow. Additionally, the code is missing the 'datetime' import statement (uses 'datetime(2023, 1, 1)' without importing it), which would cause a runtime error. These are clear hallucinations of non-existent Airflow components.",0,"While the code attempts to follow the instruction's structure (create cluster, run test pods, cleanup), it fails to implement it correctly due to using non-existent operators. The workflow logic is present but non-functional. Additionally, there's a logical error in dependencies: 'stop_test_pod_1 >> stop_test_pod_2' appears twice, and the parallel stop operations should converge before deletion, not be sequential. The instruction asks for 'running test pods with different configurations' which is attempted but not properly executed with valid operators.",0.0
584,Baseline (Qwen 2.5 1.5B Instruct),287,0,"The code fails to use Alibaba OSS-specific operators. It imports OssHook but never uses it, and instead incorrectly uses HDFS operators (HdfsUploadFileOperator, HdfsDownloadFileOperator, HdfsDeleteDirOperator) for Alibaba OSS operations. The idiomatic approach would be to use operators from airflow.providers.alibaba.cloud.operators.oss such as OSSCreateObjectOperator, OSSDeleteObjectOperator, or OSSDownloadObjectOperator, or to use native OSS operators designed for this cloud provider.",0,"The code contains multiple issues: (1) It imports 'timedelta' but never defines the import statement (missing 'from datetime import timedelta'), (2) It uses non-existent operator 'HdfsDeleteDirOperator' which is not imported and doesn't exist in the standard Airflow HDFS provider, (3) The HDFS operators used don't have 'bucket' parameters as shown in the code - this appears to be a conflation of HDFS and OSS APIs that don't actually exist in Airflow.",0,"The instruction explicitly requests using 'Alibaba OSS operators' for cloud storage data movement and file lifecycle management (transfer between local and cloud, object deletion). However, the code uses HDFS operators instead of Alibaba OSS operators, completely missing the core requirement. While it attempts to demonstrate upload, download, and delete operations (the lifecycle steps), it does so with the wrong technology stack.",0.0
585,Baseline (Qwen 2.5 1.5B Instruct),288,0,The code uses PythonOperator with AzureBlobStorageHook instead of the native Azure Blob Storage operators. Airflow provides specific operators like `WasbUploadBlobOperator` and `WasbDeleteBlobOperator` (or their newer equivalents in the microsoft.azure provider) that should be used for these operations. The pattern of wrapping hook logic in Python functions and calling them via PythonOperator is not idiomatic when dedicated operators exist.,0,"The code has several issues: (1) Uses deprecated `PythonOperator` import path (`airflow.operators.python_operator` instead of `airflow.operators.python`), (2) References undefined `timedelta` without importing it from datetime, (3) Defines functions after they are referenced in the PythonOperator tasks, which would cause a NameError, (4) The hook is instantiated at the module level outside of task context, which is an anti-pattern in Airflow. While not test harness leakage, these are significant code quality issues that prevent the DAG from running.",1,"The code does fulfill the core instruction: it creates a workflow that demonstrates Azure Blob Storage interaction with sequential upload and deletion of a file in a cloud container. The task dependencies are correctly set (upload_file_task >> delete_file_task), showing the sequential nature requested. Despite implementation issues, the business logic matches the requirement.",0.3333333333333333
586,Baseline (Qwen 2.5 1.5B Instruct),289,0,"The code uses DataprocCreateClusterOperator but with incorrect parameters. The operator does not support 'num_workers', 'worker_machine_type', 'master_machine_type', 'preemptible', 'init_actions_uris', or 'metadata' as direct parameters. Instead, it requires a 'cluster_config' dictionary following the Dataproc API structure. The code also references 'DataprocDeleteClusterOperator' which is imported but not shown in the imports section. This is not idiomatic usage of the Dataproc operators.",0,"The code invents non-existent parameters for DataprocCreateClusterOperator. The actual operator requires a structured 'cluster_config' parameter, not flat parameters like 'num_workers', 'preemptible', etc. Additionally, 'DataprocDeleteClusterOperator' is used but not imported. The 'main_class' and 'jar_file_uri' parameters in DataprocSubmitJobOperator should be nested within a 'job' configuration dictionary, not passed as direct parameters.",0,"While the DAG attempts to address the instruction by creating a cluster with preemptible workers and initialization scripts, it fails to implement these correctly due to incorrect operator usage. The instruction asks for 'multi-node cluster with preemptible workers and custom initialization scripts' and 'flexible metadata and package installations', but the implementation uses non-existent parameters that would not work in practice. The DAG also lacks proper cluster deletion and error handling that would be expected in a production 'end-to-end' orchestration.",0.0
587,Baseline (Qwen 2.5 1.5B Instruct),291,1,"The code uses the native GoogleCloudStorageDownloadOperator from airflow.providers.google.cloud.operators.storage, which is the idiomatic Airflow approach for GCS operations. It does not wrap functionality in PythonOperator with hooks.",1,"The code uses only standard Airflow libraries (airflow.providers.google.cloud.operators.storage) without any test harness imports, internal testing modules, or non-production boilerplate code. It is clean and production-ready from an import perspective.",0,"The instruction requires a complete automated data processing and machine learning pipeline including: (1) raw image data ingestion from GCS, (2) data preparation for object detection training, (3) model creation, and (4) infrastructure management. The provided code only implements a single task to download one image from GCS. It is missing critical components such as: data preparation/preprocessing tasks, model training operators (e.g., Vertex AI operators for AutoML or custom training), model deployment tasks, dataset creation for object detection, and the overall pipeline orchestration with multiple tasks and dependencies. This represents a minimal fragment rather than the comprehensive ML pipeline requested.",0.6666666666666666
588,Baseline (Qwen 2.5 1.5B Instruct),292,0,"The code uses PythonOperator pattern (via @task decorator) wrapping Hook logic instead of using native Pinot Operators. While task decorators are idiomatic for custom Python logic, the instruction asks for 'Pinot interaction tasks' which should ideally use dedicated Operators like PinotOperator or similar if available in airflow.providers.apache.pinot.operators. The code manually instantiates hooks (PinotAdminHook, PinotDatabaseHook) inside Python functions rather than using purpose-built operators designed for Pinot interactions.",0,"The code contains several issues: (1) 'start_date' is defined as a string '2023-01-01' instead of a datetime object, which will cause runtime errors. (2) 'retry_delay' is set to an integer (5 * 60 = 300) instead of a timedelta object. (3) The hook methods used (hook.get_tables(), hook.query_data()) may not exist in the actual Pinot hook implementations - these appear to be assumed/hallucinated API methods without verification against the actual provider library. (4) The query_data method signature with two parameters ('your_table_name', 'SELECT...') is non-standard and likely incorrect.",0,"The instruction explicitly requests 'parallel task execution' but the code implements sequential execution with the dependency operator '>>' (pinot_admin_task() >> pinot_database_task()). This creates a serial dependency where the database task waits for the admin task to complete, contradicting the requirement for parallel execution. To achieve parallel execution, both tasks should be called without dependency chaining.",0.0
589,Baseline (Qwen 2.5 1.5B Instruct),293,0,"The DAG heavily relies on PythonOperator with undefined python_callable functions (load_trivia_data, query_trivia_data, manage_knowledge_data). For AI-powered embedding and vector database operations, idiomatic Airflow would use specific providers like OpenAIEmbeddingOperator, WeaviateOperator, PineconeOperator, or similar vector database operators. The code imports OpenAIHook but never uses it. The SparkSubmitOperator usage for vectorization is questionable when the instruction specifically mentions 'AI-powered embedding techniques' which typically implies using embedding model providers (OpenAI, Cohere, etc.) rather than Spark. This is a generic Pythonic pattern wrapped in PythonOperators instead of using native operators for vector databases and embedding services.",0,"The code references undefined functions (load_trivia_data, query_trivia_data, manage_knowledge_data) that are never implemented, making this non-executable. It imports 'datetime' but doesn't include the import statement (missing 'from datetime import datetime'). The code also references an external file 'vectorize_trivia_data.py' that doesn't exist. While it doesn't import test harness code, the undefined references and missing implementations make this incomplete and not production-ready.",0,"The instruction requires an 'end-to-end pipeline that demonstrates how to load, vectorize, query, and manage structured knowledge data' with 'AI-powered embedding techniques' for a 'semantically searchable vector database'. While the DAG has task placeholders for these steps, it provides no actual implementation. There's no integration with any vector database (Pinecone, Weaviate, Chroma, etc.), no embedding generation logic using AI models (OpenAI, Cohere, etc.), and no demonstration of semantic search capabilities. The SparkSubmitOperator suggests batch processing rather than AI embeddings. The code is essentially a skeleton without the core functionality requested.",0.0
590,Baseline (Qwen 2.5 1.5B Instruct),294,0,"The code uses operators from 'airflow.providers.google.cloud.operators.dataplex' (CreateAssetOperator, UpdateAssetOperator, DeleteAssetOperator), but these operators are for managing Dataplex Lake/Zone Assets, not for metadata registration and catalog entries. The instruction specifically asks for 'captures metadata attributes, defines entry types, and creates catalog entries' which requires Data Catalog or Dataplex Catalog operators (e.g., DataplexCreateEntryOperator, DataplexCreateEntryTypeOperator, or DataCatalogCreateEntryOperator). Using asset management operators instead of catalog/metadata operators is not idiomatic for the stated use case of metadata registration and data discovery.",0,"The code imports 'datetime' class but never imports it (missing 'from datetime import datetime'). The operators used (CreateAssetOperator, UpdateAssetOperator, DeleteAssetOperator) do not exist in the standard Airflow Google provider package with these exact names for Airflow 3.0.6. The actual operators are named differently (e.g., DataplexCreateAssetOperator). Additionally, the 'dag=dag' parameter pattern is deprecated in Airflow 2.0+ and should not be used in 3.0.6.",0,"The instruction requires building a metadata registration pipeline that 'captures metadata attributes, defines entry types, and creates catalog entries to support comprehensive data discovery and governance'. The provided code only creates/updates/deletes Dataplex assets (infrastructure resources like lakes and zones), which is fundamentally different from metadata catalog registration. It does not define entry types, create catalog entries, or implement any metadata capture logic. The workflow also lacks any systematic tracking across different systems and repositories as requested.",0.0
591,Baseline (Qwen 2.5 1.5B Instruct),295,0,"While the code uses transfer operators (GCSToGCSOperator and S3ToS3Operator), they are misused. The instruction requires moving files from cloud object storage to a secure file transfer protocol (SFTP) destination, but the code only uses GCS-to-GCS and S3-to-S3 transfers. The proper idiomatic approach would be to use operators like S3ToSFTPOperator or GCSToSFTPOperator (or similar transfer operators to SFTP). Additionally, the operators are used incorrectly - GCSToGCSOperator is used for listing files and S3ToS3Operator for downloading, which doesn't make logical sense for a cross-cloud migration.",1,"The code uses only standard Airflow providers and operators (airflow.providers.google.cloud.transfers and airflow.providers.amazon.aws.transfers). There are no imports from test modules or test harness boilerplate. The code is clean from a hallucination perspective, even though it's functionally incorrect.",0,"The instruction explicitly requests: (1) moving files from cloud object storage to a secure file transfer protocol (SFTP) destination, (2) complete lifecycle tracking, and (3) comprehensive task chaining. The code fails on multiple counts: it doesn't transfer to SFTP at all (only between S3 and GCS), there's no lifecycle tracking implementation (no logging, monitoring, or status tracking tasks), the task chaining is illogical (tasks move data back and forth between buckets without clear purpose), and the 'expiration_days' parameter used in GCSToGCSOperator doesn't exist in the actual operator. The code does not fulfill the core business requirement of SFTP transfer or demonstrate proper lifecycle tracking.",0.3333333333333333
592,Baseline (Qwen 2.5 1.5B Instruct),296,0,The code uses PythonOperator with TrinoHook instead of the native TrinoOperator. Airflow provides airflow.providers.trino.operators.trino.TrinoOperator specifically for executing Trino queries. The pattern of wrapping hook.run() calls inside Python functions and using PythonOperator is not idiomatic when a dedicated operator exists for the task.,0,"The code imports 'from airflow.operators.python_operator import PythonOperator' which is deprecated in Airflow 2.x. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used in default_args but never imported from datetime module, which would cause a runtime error.",0,"The instruction explicitly requests 'data insertion techniques' and 'demonstrating different query methods including templated and parameterized queries'. The code only creates schema and tables but does not insert any city data. It also fails to showcase templated queries (using Jinja templates) or parameterized queries (using query parameters). The instruction asks for a complete data pipeline with insertion, but the code stops at DDL operations.",0.0
593,Baseline (Qwen 2.5 1.5B Instruct),297,0,"The code uses PythonOperator with an empty function stub for downloading artifacts instead of using the idiomatic JenkinsJobTriggerOperator (which can retrieve artifacts) or a dedicated artifact retrieval operator. The download_artifact_from_jenkins function is defined after it's referenced, and wraps what should be native operator functionality in a generic PythonOperator pattern.",1,The code uses only standard Airflow libraries (airflow.operators.python_operator.PythonOperator and airflow.providers.jenkins.operators.jenkins.JenkinsOperator). There are no imports from internal testing modules or test harness boilerplate.,0,"While the code attempts to trigger a Jenkins job, it fails to implement the artifact retrieval functionality - the download_artifact_from_jenkins function only contains a 'pass' statement with a comment. The instruction explicitly requires 'downloads the generated artifact' and 'automated artifact management', but no actual implementation is provided. The workflow structure is present but incomplete.",0.3333333333333333
594,Baseline (Qwen 2.5 1.5B Instruct),298,0,"The code attempts to use specific operators, which is good practice. However, the operators used (DataprepCreateProjectOperator, DataprepImportDataOperator, DataprepTransformDataOperator, DataprepExportDataOperator, DataprepDeleteProjectOperator) do not exist in the official Airflow Google provider package. The actual Google Cloud Dataprep integration in Airflow uses different operators like DataprepGetJobsForJobGroupOperator and DataprepRunJobGroupOperator. This represents a fundamental misunderstanding of the available Airflow operators.",0,"The code imports and uses completely fabricated operators that do not exist in Apache Airflow or its provider packages. The operators DataprepCreateProjectOperator, DataprepImportDataOperator, DataprepTransformDataOperator, DataprepExportDataOperator, and DataprepDeleteProjectOperator are hallucinated and not part of airflow.providers.google.cloud.operators.dataprep or any other Airflow module. The actual Dataprep operators available in Airflow 3.0.1 are limited to job execution and monitoring, not project creation or data import/export operations.",0,"While the code attempts to create a workflow with task dependencies as requested, it fails to implement a functional Google Dataprep workflow because it uses non-existent operators. The instruction asks for orchestrating data transformation using Google Dataprep with task dependencies, connection management, and monitoring. The code shows task dependencies but cannot actually execute Dataprep jobs, manage connections properly, or monitor real cloud resources since the operators are fictional. A correct implementation would use DataprepRunJobGroupOperator to execute actual Dataprep recipes/flows and DataprepGetJobsForJobGroupOperator for monitoring.",0.0
595,Baseline (Qwen 2.5 1.5B Instruct),299,0,"The code attempts to use operators but imports non-existent operators. The correct operators for Google Cloud Monitoring are from 'airflow.providers.google.cloud.operators.stackdriver' (or 'cloud_monitoring' in newer versions), not 'gcp_alertmanager'. Operators like 'GCPAlertManagerCreatePolicyOperator', 'GCPAlertManagerCreateChannelOperator', etc. do not exist in Airflow's Google provider. The correct operators would be 'StackdriverUpsertAlertOperator', 'StackdriverUpsertNotificationChannelOperator', or similar. Additionally, sensors are misused - GCEInstanceSensor and GKEClusterSensor are for waiting/checking status, not for enabling/disabling resources.",0,"The code hallucinates multiple non-existent operators and modules. Specifically: 'airflow.providers.google.cloud.operators.gcp_alertmanager' does not exist, and all operators imported from it (GCPAlertManagerCreatePolicyOperator, GCPAlertManagerCreateChannelOperator, GCPAlertManagerUpdatePolicyOperator, GCPAlertManagerUpdateChannelOperator) are fabricated. Additionally, 'GCENetworkInterfaceOperator' and 'GKENodePoolOperator' do not exist as standalone operators in the standard Airflow Google provider. The code also has incomplete syntax (cuts off at the end) and missing imports (datetime is not imported).",0,"The instruction asks for a monitoring and alerting workflow that creates, manages, and configures alert policies and notification channels, with the ability to dynamically enable, disable, and track infrastructure health alerts. While the code attempts to create alert policies and channels, it fails because it uses non-existent operators. Furthermore, the code creates GCE instances, GKE clusters, and GCS buckets, which are infrastructure provisioning tasks unrelated to the monitoring/alerting requirement. The 'enable/disable' tasks incorrectly use sensors instead of operators that would actually enable/disable alert policies. The code does not demonstrate tracking infrastructure health or dynamic management of alerts as requested.",0.0
596,Baseline (Qwen 2.5 1.5B Instruct),300,0,"The code imports several operators that do not exist in the official Airflow Databricks provider: DatabricksSqlCreateTableOperator, DatabricksSqlInsertIntoTableOperator, DatabricksSqlQueryOperator, DatabricksSqlExportTableToParquetOperator, and DatabricksSqlImportTableFromParquetOperator. The actual Databricks provider (airflow.providers.databricks) provides DatabricksSqlOperator for executing SQL queries, but not these specialized operators. The idiomatic approach would be to use DatabricksSqlOperator with appropriate SQL statements or PythonOperator with DatabricksSqlHook for more complex operations.",0,"The code hallucinates multiple non-existent operators from the Databricks provider. These operators (DatabricksSqlCreateTableOperator, DatabricksSqlInsertIntoTableOperator, DatabricksSqlQueryOperator, DatabricksSqlExportTableToParquetOperator, DatabricksSqlImportTableFromParquetOperator) are not part of the standard Airflow Databricks provider library. Additionally, the code imports GCSToGCSOperator but never uses it. The code also has a missing import for 'datetime' while using datetime(2023, 4, 1).",1,"Despite using non-existent operators, the code structure does attempt to fulfill all the requirements from the instruction: table creation, data insertion, transformation (query), file-based export (to Parquet), and cross-storage data import (from GCS). The logical flow and task dependencies align with building an end-to-end data pipeline as requested.",0.3333333333333333
597,Baseline (Qwen 2.5 1.5B Instruct),301,0,"The code uses PythonOperator with lambda functions for simple tasks like printing dates and sleeping, which is not idiomatic. For containerized command execution, it should use DockerOperator from airflow.providers.docker.operators.docker instead of BashOperator with a docker run command. The 'containerized command execution' requirement suggests using proper Docker integration, not shell commands.",0,"The code has multiple issues: (1) Missing imports for 'datetime', 'time', and 'BashOperator' - these are referenced but not imported at the top of the file. (2) Uses deprecated 'airflow.operators.python_operator.PythonOperator' instead of 'airflow.operators.python.PythonOperator' for Airflow 2.9.3. (3) The code would fail immediately upon execution due to NameError exceptions from missing imports.",1,"The DAG does fulfill the core requirements: it includes date printing (print_date_task), sleep operations (sleep_5_seconds_task), containerized command execution (execute_container_task), and establishes clear task interdependencies with the >> operator creating a linear pipeline. Despite implementation issues, the business logic requested is present.",0.3333333333333333
598,Baseline (Qwen 2.5 1.5B Instruct),302,0,The code uses PythonOperator with SQSHook for all SQS operations instead of using the native SQS operators available in airflow.providers.amazon.aws.operators.sqs. Airflow 3.0.0 provides operators like SqsPublishOperator and SqsSensor that should be used for idiomatic Airflow code. The pattern of wrapping hook logic in PythonOperator callables is explicitly non-idiomatic.,0,"The code references 'read_messages_task' in the dependencies line at the bottom (publish_task >> read_messages_task >> read_batch_task >> delete_queue_task), but this task is never defined. Only 'read_batch_task' is defined. This is a critical error that would cause the DAG to fail at parse time. Additionally, 'from airflow.operators.python_operator import PythonOperator' is deprecated in Airflow 3.0.0; it should be 'from airflow.operators.python import PythonOperator'.",0,"The instruction asks for 'sending messages, reading messages in single and batch modes, and managing queue lifecycle.' The code only implements batch reading (read_batch_task) but does not implement single message reading as a separate task. Additionally, the instruction mentions 'multiple tasks' for publishing and consuming, but only one publish task is created. The queue lifecycle management is partially addressed (delete only, no create), and the undefined 'read_messages_task' reference suggests incomplete implementation of the requested functionality.",0.0
599,Baseline (Qwen 2.5 1.5B Instruct),303,0,"The code uses the deprecated 'PythonOperator' import path ('airflow.operators.python_operator') instead of the modern 'airflow.operators.python.PythonOperator'. More critically, for Airflow 2.9.3, the DAG instantiation pattern is outdated - it should use the context manager pattern (@dag decorator or 'with DAG()' syntax) which is the idiomatic approach in modern Airflow. The code also doesn't leverage any dataset-specific operators or patterns available in Airflow 2.x.",0,"The code has a critical error: it imports 'timedelta' from datetime but never imports 'datetime' itself, yet uses 'datetime(2023, 1, 1)' and 'datetime.now()'. This would cause an immediate NameError at runtime. While not a test harness hallucination, this is a fundamental code error that makes it non-functional and not production-ready.",0,"The instruction explicitly requests: (1) advanced scheduling mechanisms using dataset dependencies, (2) tasks triggered based on dataset updates, (3) time schedules AND complex logical conditions, and (4) a series of DAGs demonstrating various dependency scenarios. The provided code contains only a single basic DAG with simple time-based scheduling (hourly interval) and no dataset dependencies whatsoever. It completely fails to demonstrate Dataset objects, dataset-based triggers, or any of the advanced scheduling patterns requested. This is a fundamental miss of the core requirement.",0.0
600,Baseline (Qwen 2.5 1.5B Instruct),304,0,"The code uses PythonOperator with MongoDBHook and S3Hook instead of using native operators. Airflow 3.x provides MongoToS3Operator (airflow.providers.amazon.aws.transfers.mongo_to_s3) specifically designed for this MongoDB-to-S3 transfer use case. The current implementation wraps hook logic in a Python callable, which is the anti-pattern described in the scoring criteria.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.x. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used but never imported, which would cause a runtime error. These issues indicate the code is not production-ready.",1,"The code fulfills the core business requirements: it filters MongoDB documents based on a status condition (query = {'status': 'relevant'}), and exports/transfers those documents to S3 storage. The workflow selectively moves data based on predefined query criteria as requested, ensuring only relevant documents are transferred between the two storage platforms.",0.3333333333333333
601,Baseline (Qwen 2.5 1.5B Instruct),305,0,"The code misuses GKEPodOperator for tasks that should use dedicated operators. For Google Cloud Build operations, it should use CloudBuildCreateBuildOperator from airflow.providers.google.cloud.operators.cloud_build. The GKEPodOperator is being used to run docker and kubectl commands directly, which is not idiomatic. Additionally, the instruction specifically mentions 'Google Cloud Build for automated software deployment' but the code doesn't use any Cloud Build operators at all.",1,The code uses only standard Airflow libraries and Google Cloud provider operators. There are no imports from test modules or test harness boilerplate. All imports are from legitimate airflow.providers.google.cloud packages.,0,"The instruction explicitly requests 'Google Cloud Build for automated software deployment', 'dynamic trigger creation', and 'project number retrieval'. The code fails to implement any of these key requirements: (1) No Cloud Build operators are used (CloudBuildCreateBuildOperator, CloudBuildCreateBuildTriggerOperator), (2) No dynamic trigger creation is implemented, (3) No project number retrieval logic is present (should use CloudProjectGetOperator or similar). The code instead focuses on GKE pod operations and BigQuery data loading, which don't align with the core instruction.",0.3333333333333333
602,Baseline (Qwen 2.5 1.5B Instruct),306,1,The code uses native Airflow operators appropriately. It uses FileSensor (a specific sensor operator) for checking data availability and PythonOperator for simple task execution. This is idiomatic Airflow - using the right operators for the right tasks. The FileSensor is the correct choice for demonstrating a sensor mechanism as requested.,0,"The code has a critical error: it uses 'timedelta' without importing it. The code references 'from datetime import timedelta' which is missing. Additionally, 'start_date' should be a datetime object, not a string '2023-04-01'. While these are bugs rather than hallucinations of test frameworks, they prevent the code from being production-ready or even executable.",1,"The code fulfills the instruction requirements: (1) demonstrates a basic sensor mechanism using FileSensor to wait for data readiness, (2) simulates checking for data availability with the sensor, and (3) executes a subsequent downstream operation after the sensor passes. The workflow correctly chains: generate_data >> check_data_availability (sensor) >> execute_downstream_operations. This matches the requested pattern of waiting on upstream data readiness before proceeding with downstream tasks.",0.6666666666666666
603,Baseline (Qwen 2.5 1.5B Instruct),307,0,"The code uses deprecated import paths and misses idiomatic operators for MySQL transfer. Specifically: (1) Uses 'airflow.operators.bash_operator.BashOperator' instead of 'airflow.operators.bash.BashOperator' (deprecated in Airflow 2.0+), (2) For MySQL transfer, uses a generic BashOperator with mysql CLI command instead of the idiomatic MySQLOperator or transfer operators like HiveToMySqlOperator/HiveToMySqlTransferOperator from airflow.providers.mysql.transfers, (3) The dag parameter is passed explicitly instead of using context manager pattern (though this is acceptable, it's less idiomatic for modern Airflow).",1,The code does not import any internal testing modules or include test harness boilerplate. All imports are from standard Airflow libraries (airflow core and apache.hive provider). No hallucinated functions like get_test_run() or test utilities are present.,1,"The DAG fulfills the instruction requirements: (1) Demonstrates multi-stage data orchestration with clear task dependencies, (2) Uses different operators as requested (BashOperator and HiveOperator), (3) Implements a workflow showing data transfer between local storage → HDFS → Hive tables → MySQL database, (4) Includes proper task dependency chain (local_to_hdfs >> hive_create_table >> hive_insert_data >> mysql_transfer >> cleanup). All requested components are present and logically connected.",0.6666666666666666
604,Baseline (Qwen 2.5 1.5B Instruct),308,0,"The code uses 'ClusterManagerOperator' from 'airflow.providers.apache.spark.operators.cluster_manager', which does not exist in Apache Airflow. This is a hallucinated operator. For Spark cluster management, idiomatic Airflow would use operators like DataprocCreateClusterOperator (GCP), EmrCreateJobFlowOperator (AWS), or SparkSubmitOperator for job submission. The approach of using spark-submit commands within a non-existent operator is not idiomatic.",0,"The code imports 'ClusterManagerOperator' from 'airflow.providers.apache.spark.operators.cluster_manager', which is a completely fabricated/hallucinated operator that does not exist in any version of Apache Airflow. The correct Spark-related operators in Airflow include SparkSubmitOperator, SparkSqlOperator, etc., but not ClusterManagerOperator. This is a critical hallucination that would cause immediate import failures.",0,"While the DAG attempts to address the instruction's requirements of cluster initialization, data transformation, and cluster teardown with proper task dependencies, it fails to implement these correctly. The instruction asks for 'explicit resource management' and 'best practices for managing computational resources', but the code uses a non-existent operator and doesn't demonstrate actual cluster lifecycle management (create cluster, run jobs, destroy cluster). A proper implementation would use real operators like EmrCreateJobFlowOperator/EmrTerminateJobFlowOperator or DataprocCreateClusterOperator/DataprocDeleteClusterOperator with SparkSubmitOperator for the transformation step.",0.0
605,Baseline (Qwen 2.5 1.5B Instruct),309,0,"The code uses PythonOperator with undefined functions (build_cube_function, segment_data_function, purge_old_data_function) instead of using the native KylinCubeOperator from airflow.providers.apache.kylin.operators.kylin_cube. Airflow has a dedicated Kylin provider that should be used for Kylin cube operations. This is a classic anti-pattern of wrapping logic in PythonOperator instead of using the purpose-built operator.",0,"The code references three undefined functions (build_cube_function, segment_data_function, purge_old_data_function) that are never imported or defined. This makes the code non-functional and incomplete. Additionally, it imports 'PythonOperator' from the deprecated 'airflow.operators.python_operator' module instead of 'airflow.operators.python' (the correct import path for Airflow 3.0.1).",0,"The task dependencies are set up incorrectly - the code has build_cube depending on segment_data, which depends on purge_old_data (reverse order). The instruction asks for 'progression of data transformation steps from build to purge', implying the sequence should be: build → segment → purge. The current implementation executes in the opposite order: purge → segment → build. Additionally, the code lacks any implementation of time-based data segmentation as requested in the instruction.",0.0
606,Baseline (Qwen 2.5 1.5B Instruct),310,0,"The code imports and uses operators that do not exist in the official Airflow Google Cloud provider package. Specifically, 'GoogleCloudSpeechCreateAudioOperator' and 'GoogleCloudStorageDeleteBucketOperator' are not real operators. The actual Google Cloud provider has 'CloudSpeechToTextRecognizeSpeechOperator' for speech recognition, 'GCSCreateBucketOperator' for bucket creation, and 'GCSDeleteBucketOperator' for bucket deletion. The code fabricates operators rather than using the actual idiomatic Airflow operators available in airflow.providers.google.cloud.operators.",0,"The code hallucinates multiple non-existent operators: 'GoogleCloudSpeechCreateAudioOperator', 'GoogleCloudSpeechRecognizeOperator', and 'GoogleCloudStorageDeleteBucketOperator' are not part of the standard Airflow library or Google Cloud provider package. These are fabricated imports that would fail at runtime. The actual operators have different names and import paths (e.g., CloudSpeechToTextRecognizeSpeechOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator).",1,"Despite using incorrect/non-existent operators, the code does attempt to fulfill the instruction's requirements: it creates a DAG that orchestrates bucket creation, audio generation, speech recognition, and bucket cleanup in sequence. The workflow structure and task dependencies align with the requested 'systematic demonstration of Google Cloud Speech API interactions' and shows how operators are chained together for a comprehensive speech processing workflow.",0.3333333333333333
607,Baseline (Qwen 2.5 1.5B Instruct),311,0,"The code attempts to use specific Data Catalog operators, which is good practice. However, the operators used (RegisterFileEntriesOperator, AttachCustomMetadataTagsOperator) do not exist in the actual Airflow Google Cloud provider library. The real operators available are CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, CloudDataCatalogCreateTagOperator, etc. The code invents non-existent operators rather than using the actual idiomatic Airflow operators for Data Catalog operations.",0,"The code hallucinates operators that do not exist in Apache Airflow's Google Cloud provider. Specifically, 'RegisterFileEntriesOperator' and 'AttachCustomMetadataTagsOperator' are not real operators in airflow.providers.google.cloud.operators.datacatalog. The actual module contains operators like CloudDataCatalogCreateEntryOperator, CloudDataCatalogCreateTagOperator, CloudDataCatalogCreateTagTemplateOperator, etc. This represents a significant hallucination of non-existent APIs.",1,"Despite using non-existent operators, the code does attempt to fulfill all aspects of the instruction: it creates an entry group, registers file entries, and attaches custom metadata tags in a systematic workflow. The logical structure and dependencies demonstrate the complete lifecycle of data catalog operations as requested, even though the implementation uses hallucinated operators.",0.3333333333333333
608,Baseline (Qwen 2.5 1.5B Instruct),313,0,"The DAG uses PythonOperator for all tasks with undefined python_callable functions (create_google_calendar_service_account_key, authenticate_with_google_calendar_api, etc.). While it imports GoogleCalendarHook, it never uses it. This is the anti-pattern of wrapping logic in PythonOperators instead of using native operators. For a production DAG, it should use appropriate Google Cloud operators (e.g., GCSCreateBucketOperator, GCSDeleteBucketOperator) for resource management and either use existing Calendar operators or properly implement the Hook within operators rather than undefined Python functions.",0,"The code references multiple undefined functions (create_google_calendar_service_account_key, authenticate_with_google_calendar_api, list_calendars, transfer_events_to_csv, cleanup_temporary_files) that are never defined or imported. This makes the code non-functional and incomplete. Additionally, 'airflow.operators.python_operator' is deprecated in Airflow 3.0.0 - it should be 'airflow.operators.python.PythonOperator'. The GoogleCalendarHook is imported but never used, suggesting incomplete or placeholder code.",0,"The instruction explicitly requests 'explicit setup, execution, and teardown stages' with 'creates necessary cloud resources' and 'systematically removes all temporary infrastructure components'. While the DAG structure suggests these stages (create_key, authenticate, transfer, cleanup), none of the actual implementation exists - all functions are undefined. The instruction asks for cloud resource creation/teardown (like GCS buckets, service accounts, etc.), but the code only has placeholder task names without any actual cloud infrastructure management logic. The DAG is a skeleton that doesn't fulfill the business requirements.",0.0
609,Baseline (Qwen 2.5 1.5B Instruct),314,0,"The code attempts to use Airflow operators but contains multiple non-existent operators. 'S3UploadOperator' does not exist in the standard Airflow providers (the correct operator is 'S3CreateObjectOperator' or 'LocalFilesystemToS3Operator'). 'LocalFileRemoveOperator' and 'LocalFileSensor' from 'airflow.providers.amazon.aws.operators.local_filesystem' do not exist. The code also uses 'S3MoveObjectOperator' incorrectly (moving from the same bucket to the same key is redundant). While it attempts to use operators, the hallucinated operators and incorrect usage patterns indicate a lack of idiomatic Airflow practices.",0,"The code contains multiple hallucinated/non-existent operators and imports: 'LocalFileSensor' from 'airflow.providers.amazon.aws.operators.local_filesystem' does not exist, 'S3UploadOperator' does not exist, and 'LocalFileRemoveOperator' does not exist. The import statement 'from airflow.providers.amazon.aws.operators.local_filesystem import LocalFileSensor' is completely fabricated. Additionally, 'timedelta' is used but never imported. These are clear hallucinations of non-existent Airflow components.",0,"The instruction asks to 'demonstrate file transfer from local temporary storage to AWS S3 bucket, showcasing how to programmatically create, move, and clean up temporary files.' While the DAG attempts to address these requirements, it fails in execution: (1) The file upload mechanism uses a non-existent operator, (2) The 'move' operation is redundant and incorrectly configured (moving within the same bucket to the same key), (3) Local file cleanup uses a non-existent operator, (4) The workflow logic is flawed - it should upload from local to S3, not 'move' within S3. The DAG does not successfully demonstrate a working file transfer workflow as requested.",0.0
610,Baseline (Qwen 2.5 1.5B Instruct),315,0,"The code references operators like 'BigQueryCreateTableOperator' and 'BigQueryDeleteTablesOperator' but fails to import them. Additionally, it instantiates 'BigQueryHook' at the module level (line 25) without using it, which is not idiomatic. While it does use the proper 'BigQueryToBigQueryOperator' for data transfer, the missing imports and unused hook instantiation demonstrate non-idiomatic patterns.",0,The code references 'BigQueryCreateTableOperator' and 'BigQueryDeleteTablesOperator' which do not exist in the standard Airflow providers for BigQuery in version 2.9.3. The actual operators are 'BigQueryCreateEmptyTableOperator' and 'BigQueryDeleteTableOperator'. These are hallucinated/incorrect operator names that would cause import and runtime errors.,1,"The DAG fulfills the user instruction requirements: it demonstrates table creation (source and destination), data transfer (using BigQueryToBigQueryOperator), and cleanup (deleting temporary tables). The workflow logically addresses the HR data management scenario for copying employee salary data from one BigQuery table to another within the same dataset, even though the implementation has technical issues.",0.3333333333333333
611,Baseline (Qwen 2.5 1.5B Instruct),316,0,"The code wraps S3Hook and BedrockModelHook logic inside Python functions and uses PythonOperator instead of using native Airflow operators. Should use S3CreateObjectOperator, BedrockInvokeModelOperator, or similar provider-specific operators. This is the anti-pattern of wrapping hooks in PythonOperators rather than using the idiomatic operator-based approach.",0,"Multiple issues: (1) Missing imports for 'datetime', 'timedelta', and 'PythonOperator'. (2) BedrockModelHook does not exist in standard Airflow providers - the correct hook would be BedrockHook or BedrockRuntimeHook. (3) The model_id 'amazon.titanic' appears to be fabricated. (4) Methods like 'invoke(input_data=input_data)' and 'output_data.save()' are not real Bedrock API patterns. The code contains hallucinated classes and methods that don't exist in the actual Airflow AWS provider.",0,"The instruction requires an end-to-end ML pipeline with: model training, throughput allocation, model lifecycle management, systematic preparation/training/deployment, automated monitoring, and cleanup. The provided code only has basic data download and a non-functional model invocation. It completely misses: (1) actual model training/fine-tuning, (2) throughput/provisioned throughput allocation, (3) model deployment, (4) lifecycle management (versioning, updates), (5) monitoring tasks, and (6) cleanup tasks. The code does not demonstrate Bedrock's custom model training capabilities at all.",0.0
612,Baseline (Qwen 2.5 1.5B Instruct),317,0,"The code uses the deprecated 'airflow.operators.python_operator.PythonOperator' import path. In Airflow 3.0.0, this should be 'airflow.operators.python.PythonOperator'. Additionally, the code does not demonstrate the use of Airflow's native setup/teardown task features introduced in Airflow 2.7+ (setup() and teardown() methods or task decorators with setup/teardown semantics), which would be the idiomatic way to handle resource allocation and cleanup in modern Airflow.",0,"The code references undefined functions (setup_function, main_work_function, cleanup_function) and an undefined import (timedelta is used but not imported from datetime). While these could be considered incomplete rather than hallucinated, the code is not production-ready and would fail immediately upon execution. It lacks the actual implementation details that would make it functional.",0,"The instruction specifically asks to 'demonstrate setup and teardown tasks in Airflow' and 'manage resource allocation and cleanup for a multi-stage data processing pipeline'. The code provides only a basic skeleton with three sequential tasks but does not actually demonstrate: (1) Airflow's native setup/teardown task patterns, (2) any actual resource allocation/cleanup logic, (3) a multi-stage pipeline (only has single main work task), or (4) how teardown tasks execute even when upstream tasks fail (a key feature of teardown tasks). The code is a basic sequential pipeline that doesn't showcase the setup/teardown functionality that Airflow provides.",0.0
613,Baseline (Qwen 2.5 1.5B Instruct),319,0,"The code wraps AWS Batch Hook operations inside PythonOperator functions instead of using native Airflow operators. For AWS Batch operations, the code should use operators like BatchOperator or BatchCreateComputeEnvironmentOperator from airflow.providers.amazon.aws.operators.batch. The pattern of 'hook = AwsBatchHook()' inside a python_callable is the anti-pattern explicitly mentioned in the scoring criteria.",0,"The code has a critical import error - it uses 'datetime' without importing it (missing 'from datetime import datetime'). Additionally, it uses 'PythonOperator' from the deprecated 'airflow.operators.python_operator' path instead of the modern 'airflow.operators.python.PythonOperator'. The AwsBatchHook methods used (create_job_definition, describe_jobs, delete_job_definition) do not match the actual AWS Batch Hook API, which uses different method signatures and parameters. This represents hallucinated/incorrect API usage.",0,"The instruction asks for dynamic cluster provisioning and management for big data processing workloads with dynamic allocation/deallocation based on processing requirements. The code attempts AWS Batch job definitions but: (1) AWS Batch is not a cluster provisioning service - EMR, Databricks, or Dataproc would be appropriate for big data clusters, (2) There is no actual data processing workload demonstrated, (3) The 'dynamic' aspect based on processing requirements is missing - it just creates and potentially deletes a job definition without any logic to scale based on workload, (4) The workflow doesn't demonstrate flexibility of cloud-based infrastructure as requested.",0.0
614,Baseline (Qwen 2.5 1.5B Instruct),320,0,"The code wraps all logic in PythonOperator with custom functions instead of using native Airflow operators. It should use GCSToSambaOperator or similar transfer operators if available, or at minimum use GCSHook properly with dedicated operators. The code also imports GCSCredentialsHook but never uses it, and the upload_file_to_samba function doesn't actually download from GCS - it expects local file paths. This is a classic anti-pattern of wrapping hooks in PythonOperator instead of using purpose-built operators.",1,"The code uses standard Airflow libraries (airflow.providers.google.cloud.hooks.gcs, airflow.providers.samba.hooks.samba, airflow.operators.python_operator) without importing any test utilities or internal testing modules. No test harness boilerplate is present.",0,"The code fails to implement the core requirement of transferring files FROM Google Cloud Storage TO Samba. The upload_file_to_samba function expects local file paths, not GCS paths (gs://). It never downloads files from GCS before uploading to Samba. The tasks reference GCS paths like 'gs://your-bucket-name/your-file.txt' but the function cannot handle these. Additionally, the delete tasks are empty lambda functions that do nothing. The code does not demonstrate working single file copy, file move, directory copy, or selective file movement between GCS and Samba as requested.",0.3333333333333333
615,Baseline (Qwen 2.5 1.5B Instruct),321,0,"The code uses GCSToGCSOperator for all tasks, but this operator is designed for simple GCS-to-GCS file transfers, not for using the Google Cloud Transfer Service as requested. The instruction specifically asks for 'Google Cloud Transfer Service', which requires using CloudDataTransferServiceCreateJobOperator or similar operators from airflow.providers.google.cloud.operators.cloud_storage_transfer_service. The code also invents a 'config' parameter that doesn't exist in GCSToGCSOperator and misuses the operator for cleanup, success, and failure tasks where it doesn't make sense.",0,"The code hallucinates parameters and functionality. GCSToGCSOperator does not accept a 'config' parameter with the structure shown (transfer_config dictionary with fields like 'description', 'job_name_prefix', 'status_quo', etc.). It also doesn't have a 'job_name_prefix' parameter. The code also has a missing import for 'timedelta'. These are fabricated APIs that don't exist in the actual Airflow provider.",0,"The instruction explicitly requests using 'Google Cloud Transfer Service' for file transfers, but the code uses GCSToGCSOperator which is a simple copy operator, not the Transfer Service. The instruction asks for 'robust error management and cleanup procedures', but the implementation creates nonsensical tasks (cleanup_task, success_task, failure_task) that all use the same GCSToGCSOperator with identical parameters and are chained linearly, which doesn't provide actual error handling or cleanup. There's no demonstration of 'programmatically managing cloud storage resources' (like creating/deleting buckets or transfer jobs) or proper error handling patterns (like trigger rules, on_failure_callback, or branching).",0.0
616,Baseline (Qwen 2.5 1.5B Instruct),322,0,The code uses PythonOperator with GoogleCloudStorageHook instead of using native GCS operators. Airflow provides GCSObjectExistenceSensor for monitoring file uploads and other GCS-specific operators. The pattern of wrapping hook logic in PythonOperator is not idiomatic when dedicated operators exist for these tasks.,0,"The code imports 'airflow.operators.python_operator.PythonOperator' which is deprecated in Airflow 3.0.0. The correct import should be 'airflow.operators.python.PythonOperator'. Additionally, the hook method 'get_object(Bucket=bucket_name, Name=object_name)' uses incorrect parameter names - it should use 'bucket_name' and 'object_name' as positional arguments or the correct keyword arguments for the GCS hook API.",0,"The instruction asks for monitoring file uploads, checking for completed transfers, and triggering processing based on file arrival and update conditions. The code attempts validation but doesn't properly monitor for file arrival (should use sensors), doesn't check update conditions (timestamps, file modifications), and the validation logic is flawed (catches exceptions but doesn't fail the task). The workflow doesn't truly 'monitor' - it just checks once per run without proper sensor-based waiting for file arrival.",0.0
617,Baseline (Qwen 2.5 1.5B Instruct),323,0,"The code fails idiomatic Airflow practices in multiple ways: (1) Uses GCSToLocalOperator instead of LocalFilesystemToGCSOperator for uploading to GCS - the operator name and direction are backwards for the stated goal. (2) Uses PythonOperator with a lambda to print BigQuery results instead of using BigQueryOperator or BigQueryInsertJobOperator for running queries. (3) Instantiates BigQueryHook and runs query at DAG parse time (module level) rather than at task execution time, which is a critical anti-pattern. (4) BashOperator is referenced but never imported.",1,The code uses standard Airflow libraries and providers (airflow.providers.google.cloud). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.,0,"The instruction asks to: (1) upload a file to GCS bucket, (2) run a simple bash command, and (3) perform a multi-step pipeline action. The code fails on multiple counts: (1) GCSToLocalOperator downloads FROM GCS, not uploads TO GCS - this is the opposite of what was requested. (2) BashOperator is used but never imported, causing a runtime error. (3) The instruction mentions 'Google Cloud Life Sciences pipeline' which typically involves LifeSciencesRunPipelineOperator, but this is not used at all. The BigQuery query execution is also incorrectly implemented at parse time rather than execution time.",0.3333333333333333
618,Baseline (Qwen 2.5 1.5B Instruct),324,0,"While the code uses native operators (GCSCreateBucketOperator, BigQueryInsertJobOperator, etc.), it uses incorrect and non-existent operators. Specifically: (1) 'airflow.providers.apache.snowflake.operators.snowflake' should be 'airflow.providers.snowflake.operators.snowflake' - there is no 'apache' in the Snowflake provider path. (2) GCSUploadFileOperator does not exist; the correct operator is GCSFileTransformOperator or LocalFilesystemToGCSOperator. (3) HDFSUploadOperator is imported but never used. (4) The code mixes unrelated technologies (Snowflake for metadata management when the instruction asks for cloud storage metadata management, typically handled by services like Google Data Catalog, AWS Glue, or Azure Purview). The operator usage shows fundamental misunderstanding of Airflow provider structure.",0,"The code contains multiple issues: (1) Incorrect import path 'airflow.providers.apache.snowflake' - Snowflake provider does not have 'apache' in its path. (2) 'GCSUploadFileOperator' does not exist in the GCS provider. (3) 'HDFSUploadOperator' is imported but never used. (4) Missing import for 'timedelta' which is used in default_args. (5) The operators are missing required parameters like 'dag=dag' or need to be used within a DAG context manager. While not test harness code, these are hallucinated/incorrect operator names and paths that would fail in production.",0,"The instruction asks for 'automated metadata management for cloud storage resources' with 'dynamically create catalog entries, assign custom tags, and prepare data assets for discovery and governance.' The code fails to address this properly: (1) It uses Snowflake (a data warehouse) for metadata management instead of proper cloud catalog services like Google Data Catalog, AWS Glue Data Catalog, or similar. (2) There is no actual tagging mechanism for cloud storage resources - the code just inserts JSON strings into a Snowflake table, which doesn't enable discovery or governance. (3) No integration with actual metadata/catalog services that would enable data discovery and governance. (4) The pipeline focuses on data movement (GCS upload, Spark processing, BigQuery loading) rather than metadata registration and tagging. The core requirement of automated metadata management for cloud storage is not fulfilled.",0.0
619,Baseline (Qwen 2.5 1.5B Instruct),325,0,"The code uses deprecated 'PythonOperator' imported from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. In Airflow 3.0.6, the old import path is deprecated. Additionally, the first task 'wait_for_duration_task' is unnecessary - it's a PythonOperator that just prints a message, which doesn't serve the purpose of demonstrating asynchronous waiting. The idiomatic approach would be to use the TimeDeltaSensor directly without a redundant PythonOperator wrapper.",0,"The code has critical errors: 1) Missing import for 'timedelta' from datetime module, which is used in 'retry_delay' and 'delta' parameters. 2) The 'start_date' is set as a string '2023-10-01' instead of a datetime object. 3) Uses deprecated import path 'airflow.operators.python_operator' which may not exist in Airflow 3.0.6. These are not hallucinations per se, but represent broken/non-functional code that wouldn't run in production.",0,"The instruction specifically asks for an 'asynchronous time delta sensor' that doesn't occupy worker slots during the waiting phase. However, the TimeDeltaSensor in the code does NOT have 'mode=""reschedule""' set, which means it runs in the default 'poke' mode. In poke mode, the sensor DOES occupy a worker slot during the entire waiting period, which directly contradicts the requirement. To achieve asynchronous behavior without occupying worker slots, the sensor must use 'mode=""reschedule""'. Additionally, the first PythonOperator task serves no purpose in demonstrating the time-based synchronization concept.",0.0
620,Baseline (Qwen 2.5 1.5B Instruct),327,0,"The code uses PythonOperator with pymongo directly instead of using the idiomatic Airflow approach. Airflow provides MongoHook (airflow.providers.mongo.hooks.mongo.MongoHook) for MongoDB interactions. The proper pattern would be to use MongoHook within operators or create custom operators that leverage the hook. Additionally, the code attempts to pass connection objects between tasks via .output, which is not how Airflow task dependencies work - connections should be re-established in each task using hooks and connection IDs stored in Airflow's connection management.",0,"While the code doesn't import test harness modules, it contains several critical errors that make it non-functional: 1) Missing import for 'timedelta' from datetime module, 2) The pattern of passing task output (connect_task.output) as op_kwargs is fundamentally broken - PythonOperator doesn't work this way in Airflow, and connection objects cannot be serialized/passed between tasks, 3) The approach of trying to share a MongoDB client connection across tasks violates Airflow's execution model where tasks run in separate processes/workers. This code would fail immediately in production.",1,"The code does fulfill the user instruction to create a sequential workflow demonstrating cloud-based NoSQL database interactions with transaction management and query preparation. It shows: connection establishment, insert (transaction), read (query), update (transaction), delete (transaction), and connection closure in a sequential pipeline. While the implementation is flawed, it does conceptually address the requested business logic of showcasing database interaction mechanics in a workflow.",0.3333333333333333
621,Baseline (Qwen 2.5 1.5B Instruct),328,0,"The code fails to use idiomatic Airflow patterns. While it correctly uses task decorators, it wraps all Snowflake operations in PythonOperators (via @task decorator) with SnowflakeHook instead of using the native SnowflakeOperator for SQL operations. The instruction specifically asks to 'showcase integration between task decorators and Snowflake's Snowpark session management' and 'programmatically create a DataFrame', but the code doesn't use Snowpark at all - it just executes raw SQL. For validation, a SnowflakeCheckOperator or SQLCheckOperator would be more idiomatic than a custom Python function. The code should leverage Snowpark's DataFrame API (session.create_dataframe(), df.write.save_as_table()) rather than raw SQL execution.",0,"The code has several issues: 1) Missing import for 'timedelta' which is used in default_args. 2) The SnowflakeHook.get_session() method doesn't exist in standard Airflow Snowflake provider - this appears to be a hallucinated API. The correct method would be get_conn() or for Snowpark specifically, get_snowpark_session(). 3) The session.execute() pattern shown doesn't align with actual Snowpark session API. This suggests the code mixes concepts without using actual working Airflow/Snowflake provider APIs.",0,"The instruction specifically asks to 'showcase integration between Airflow's task decorators and Snowflake's Snowpark session management' and 'programmatically create a DataFrame'. The code fails on multiple fronts: 1) It doesn't use Snowpark at all - no Snowpark DataFrame creation, no Snowpark API usage. 2) While it uses task decorators, it doesn't demonstrate proper Snowpark session management. 3) The 'create_dataframe' task doesn't actually create a DataFrame - it just creates an empty table. 4) Data is inserted via raw SQL INSERT statements rather than programmatically creating and writing a DataFrame as requested. 5) The validation task exists but uses assertions rather than proper data quality checks. The code misses the core requirement of demonstrating Snowpark DataFrame operations.",0.0
622,Baseline (Qwen 2.5 1.5B Instruct),329,0,"The code attempts to use a specific operator 'GoogleCloudStorageToPrestoOperator', which would be idiomatic if it existed. However, this operator does not exist in the official Airflow providers. The airflow.providers.google.cloud.transfers module does not contain a gcs_to_presto transfer operator. This is a hallucinated/non-existent operator. To properly implement this, one would need to use either a PythonOperator with hooks or chain existing operators (e.g., GCSToLocalFilesystemOperator + a Presto load operation), which would not be as idiomatic as a dedicated transfer operator.",0,"The code imports 'GoogleCloudStorageToPrestoOperator' from 'airflow.providers.google.cloud.transfers.gcs_to_presto', which does not exist in Apache Airflow 2.9.3 or any standard Airflow distribution. This is a hallucinated import and operator. The code would fail immediately upon execution with an ImportError. While it doesn't include test harness code, the use of a non-existent operator is a critical hallucination issue.",0,"While the code attempts to address the instruction to 'move data from GCS to Presto' with a 'single-task data transfer mechanism' and includes scheduling, it fails to deliver a working solution because it uses a non-existent operator. The instruction requires a functional DAG that demonstrates the data transfer, but this code cannot execute. A proper implementation would require using actual Airflow operators/hooks that exist in the ecosystem.",0.0
623,Baseline (Qwen 2.5 1.5B Instruct),330,1,"The code uses the native DockerOperator from airflow.providers.docker.operators.docker_operator, which is the idiomatic way to run Docker containers in Airflow. It does not wrap Docker logic in a PythonOperator with hooks, but instead uses the purpose-built operator for Docker integration.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.DAG and airflow.providers.docker.operators.docker_operator.DockerOperator) and does not include any test harness boilerplate, internal testing modules, or non-standard imports.",0,"The instruction asks for a demonstration DAG that showcases 'basic task orchestration and Docker integration, showing how different types of tasks can be sequenced.' However, the DAG only uses DockerOperator tasks - it does not demonstrate 'different types of tasks' as requested. A proper demonstration should include a mix of task types (e.g., BashOperator, PythonOperator, DockerOperator) to showcase how different types of tasks can work together in a workflow. Additionally, the task dependencies are set up backwards (task1 depends on task2, which depends on task3, meaning execution order is task3 -> task2 -> task1), which may not be the intended demonstration of task sequencing.",0.6666666666666666
624,Baseline (Qwen 2.5 1.5B Instruct),334,0,The code uses PythonOperator with AzureDataLakeStorageHook instead of using the native Azure Data Lake Storage operators. Airflow providers for Azure include operators like AzureDataLakeStorageUploadOperator and AzureDataLakeStorageDeleteOperator (or similar) that should be used instead of wrapping hook logic in PythonOperator. This is the exact anti-pattern described in the scoring criteria.,0,"The code references undefined functions 'upload_file_to_adls' and 'remove_file_from_adls' that are never defined in the code. Additionally, 'timedelta' is used but never imported. The code also uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.6. While not test harness boilerplate, these are significant code quality issues that would prevent the DAG from running.",0,"The dependency management is incorrect. The instruction asks to 'upload and remove files' which logically means upload first, then remove. However, the code sets 'upload_file_task.set_upstream(remove_file_task)', meaning remove happens before upload, which is backwards. The correct dependency should be 'remove_file_task.set_downstream(upload_file_task)' or 'upload_file_task >> remove_file_task'. This fails to properly demonstrate the requested task sequencing.",0.0
625,Baseline (Qwen 2.5 1.5B Instruct),335,0,The code uses PythonOperator with WinRMHook instead of using the native WinRMOperator. Airflow provides airflow.providers.microsoft.winrm.operators.winrm.WinRMOperator specifically designed for executing WinRM commands. The pattern of wrapping hook logic in a PythonOperator is not idiomatic when a dedicated operator exists for the task.,0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.0.1. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'timedelta' is used but never imported. The code also references 'start_date' as a string '2023-04-01' instead of a datetime object, which would cause runtime errors.",0,"The instruction explicitly requests 'inter-task dependencies and parallel processing capabilities' as well as simulating 'real-world remote system management and monitoring scenarios'. The generated code creates independent tasks with no dependencies defined between them, failing to demonstrate inter-task dependencies. It also doesn't showcase parallel processing patterns (like task groups or dynamic task mapping). The workflow is overly simplistic with only basic command execution and doesn't simulate a realistic system management scenario with monitoring, validation, or multi-stage operations.",0.0
626,Baseline (Qwen 2.5 1.5B Instruct),336,0,"The code uses deprecated operators. In Airflow 3.0.1, `PythonOperator` and `BashOperator` should be imported from `airflow.operators.python` and `airflow.operators.bash` respectively, not from `airflow.operators.python_operator` and `airflow.operators.bash_operator`. These old import paths are deprecated and removed in Airflow 3.x.",0,"The code contains multiple hallucinated functions and patterns that don't exist in Airflow. The functions `xcom_push()` and `xcom_pull()` are called as standalone functions without proper context (they should be methods on the task instance context). Additionally, the BashOperator uses a non-existent `xcom_pull` bash command. The proper way to push XCom data is to return a value from the Python callable or use `ti.xcom_push()`, and to pull data use `ti.xcom_pull()` with the task instance from context. The bash command `xcom_pull -k my_key` is completely fabricated and not a valid Airflow feature.",0,"While the DAG attempts to demonstrate XCom communication between Python and Bash operators as requested, it fails to implement working code. The XCom push/pull mechanisms are incorrectly implemented and would not function. A proper implementation would require using task instance context (`ti.xcom_push()` and `ti.xcom_pull()`), and for Bash operators, using Jinja templating to access XCom values. The code does not demonstrate functional cross-task data sharing.",0.0
627,Baseline (Qwen 2.5 1.5B Instruct),338,0,"The code wraps cloud storage download and Presto data loading logic inside PythonOperator functions instead of using native Airflow operators. For cloud storage, it should use providers like GCSToLocalFilesystemOperator, S3Hook with operators, or similar. For Presto, while PrestoHook is used, it's wrapped in a PythonOperator rather than using a native operator pattern. The idiomatic approach would leverage specific operators from airflow.providers packages (e.g., GCSToBigQueryOperator pattern, or cloud-to-Presto transfer operators if available).",0,"The code contains several issues: (1) Functions 'download_csv' and 'load_to_presto' are defined AFTER they are referenced in the PythonOperator declarations, which would cause a NameError. (2) The variable 'csv_file_path' used in load_to_presto() is not defined in that scope - it's only defined locally in download_csv(). (3) The code includes 'if __name__ == ""__main__"": dag.cli()' which is test/debug boilerplate not appropriate for production DAGs. (4) Uses deprecated 'PythonOperator' import path (should be from airflow.operators.python).",0,"The instruction requests a 'one-time data transfer' but the DAG is configured with 'schedule_interval=timedelta(days=1)', making it a daily recurring job rather than one-time. Additionally, the download_csv function is just a placeholder with a print statement and no actual implementation for downloading from cloud storage. The workflow structure is present but incomplete and misconfigured for the stated requirement.",0.0
628,Baseline (Qwen 2.5 1.5B Instruct),340,0,"The code uses completely fabricated operators that do not exist in the Airflow AWS provider package. The correct operators for EKS operations are EKSCreateClusterOperator, EKSDeleteClusterOperator, EKSCreateFargateProfileOperator, and EKSPodOperator (not EKSDeployPodOperator or EKSCleanupPodOperator). Additionally, EC2CreateClusterOperator and EC2DeleteClusterOperator do not exist - these should be EKS operators. The imports reference non-existent modules and operators.",0,"The code heavily hallucinates non-existent operators and imports. Specifically: EC2CreateClusterOperator, EC2DeleteClusterOperator, EKSCreateFargateProfileOperator, EKSDeployPodOperator, and EKSCleanupPodOperator do not exist in airflow.providers.amazon.aws.operators. The actual operators available are EKSCreateClusterOperator, EKSDeleteClusterOperator, EKSCreateFargateProfileOperator, and EKSPodOperator. The imports from airflow.providers.amazon.aws.operators.ec2 and airflow.providers.amazon.aws.operators.eks are incorrect for the operators being used.",0,"While the code attempts to follow the instruction's structure (create EKS cluster, create Fargate profile, run test pod, cleanup), it fails to properly implement these steps due to using non-existent operators. The instruction requires provisioning an EKS cluster with Fargate compute resources and running a test pod, but the implementation uses fabricated operators that would not execute. Additionally, the Fargate profile creation is incorrectly embedded as a parameter in the cluster creation rather than being a separate task, and there's no explicit Fargate profile creation step shown in the task flow.",0.0
629,Baseline (Qwen 2.5 1.5B Instruct),342,0,"The code uses PythonOperator with WinRMHook instead of using the native WinRMOperator. Airflow provides airflow.providers.microsoft.winrm.operators.winrm.WinRMOperator specifically designed for executing WinRM commands. The current implementation wraps the hook in a Python function, which is a non-idiomatic pattern when a dedicated operator exists.",1,The code uses only standard Airflow libraries (airflow.providers.microsoft.winrm.hooks.winrm.WinRMHook and airflow.operators.python_operator.PythonOperator). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective.,0,"The instruction explicitly asks for 'parallel remote command execution' and 'executing different shell commands simultaneously'. However, the code sets up sequential dependencies with 'tasks[i].set_upstream(tasks[i + 1])', which chains the tasks in series rather than running them in parallel. Additionally, all tasks execute the same command ('echo Hello from {}!') rather than 'different shell commands' as requested. The workflow does not demonstrate parallel execution or command diversity as specified.",0.3333333333333333
630,Baseline (Qwen 2.5 1.5B Instruct),344,0,"The code uses PythonOperator for the producer task without actually producing messages to Kafka. A proper implementation should use KafkaProducerOperator or similar from airflow.providers.apache.kafka. The producer function just prints and sleeps, not actually sending messages to Kafka. The event_listener also uses PythonOperator with an infinite while loop, which is anti-pattern in Airflow - tasks should be finite and idempotent.",0,"The code has several issues: (1) Missing imports for 'timedelta' and 'time' modules, (2) KafkaSensor is imported from 'airflow.sensors.kafka' but the correct import path in Airflow 2.8.4 should be 'airflow.providers.apache.kafka.sensors.kafka', (3) The sensor_task.get() method call in event_listener is not a valid Airflow pattern - sensors don't have a get() method that can be called this way, (4) The overall architecture misunderstands how Airflow sensors work - they don't return values to be consumed by downstream tasks in this manner.",0,"While the code attempts to address the requirements (Kafka message processing, sequential numeric data, sensor for numeric conditions, event listener for numbers divisible by 3, 5, or 15), it fails to implement them correctly. The producer doesn't actually produce to Kafka, the sensor isn't properly configured to consume messages, and the event listener uses an infinite loop which is incompatible with Airflow's execution model. The code shows understanding of the requirements but doesn't deliver a functional implementation.",0.0
631,Baseline (Qwen 2.5 1.5B Instruct),346,0,The code uses a PythonOperator wrapping AzureContainerInstanceHook instead of using the native AzureContainerInstancesOperator from airflow.providers.microsoft.azure.operators.container_instances. This is the anti-pattern explicitly called out in the scoring criteria - wrapping hook logic in a PythonOperator rather than using the dedicated operator designed for ACI tasks.,1,"The code uses only standard Airflow libraries (airflow.providers.microsoft.azure.hooks.aci, airflow.operators.python_operator) without any test harness imports or internal testing modules. The imports and structure are clean and production-appropriate.",1,"The DAG fulfills the instruction requirements: it demonstrates triggering a container-based job on Azure Container Instances with standardized resource allocation (CPU: 0.5-1, Memory: 256-512 MiB) and dynamic naming conventions (timestamp-based naming pattern 'task_YYYYMMDDHHMMSS'). The workflow is lightweight, reproducible, and represents an ephemeral computational task execution pattern.",0.6666666666666666
632,Baseline (Qwen 2.5 1.5B Instruct),347,1,"The code imports appropriate transfer operators from Airflow providers (GCSToGCSOperator, S3ToS3Operator, GCSBucketToS3Operator, S3ObjectToGCSOperator, S3BucketToGCSOperator) which are the idiomatic way to handle inter-cloud data transfers. It does not wrap logic in PythonOperator + Hook patterns.",0,"The code contains severe issues: (1) Massive duplication with the same imports repeated 40+ times (S3ObjectToGCSOperator and S3BucketToGCSOperator imported repeatedly), which suggests a generation error or hallucination. (2) The code is incomplete - it only contains imports with no DAG definition, no tasks, no task chaining, and no actual implementation. This is not production-ready code.",0,"The instruction requires: (1) A complete DAG showcasing inter-cloud data transfer, (2) Task chaining demonstration, (3) Cloud service interactions (GCP and AWS), (4) Dynamic environment configuration, (5) Creating, transferring, and deleting resources. The provided code only contains import statements with no DAG object, no tasks, no task dependencies/chaining, no resource creation/deletion operators, and no dynamic configuration. It fails to fulfill any of the core requirements.",0.3333333333333333
633,Baseline (Qwen 2.5 1.5B Instruct),348,1,"The code uses native Airflow providers and operators appropriately (RDSCreateDBInstanceOperator, RDSModifyDBInstanceOperator, S3CreateBucketOperator, RedshiftCopyTableOperator, S3KeySensor). It does not wrap hooks in PythonOperators, which demonstrates idiomatic Airflow usage.",0,"The code contains multiple issues: (1) Missing import for 'timedelta' from datetime module, (2) Uses 'S3DeleteBucketOperator' which doesn't exist in the standard AWS provider (should be S3DeleteBucketOperator from a different path or use a different approach), (3) The 'start_date' is a string instead of a datetime object, (4) Imports EC2Hook but never uses it. These indicate hallucinated or incorrect imports and usage patterns that would fail in production.",0,"The DAG does not properly fulfill the instruction requirements. Key issues: (1) Uses S3KeySensor to wait for RDS instance availability instead of RDSInstanceAvailableSensor or similar RDS-specific sensor, (2) RedshiftCopyTableOperator is used incorrectly - it's meant to copy data INTO Redshift, not from a source database to S3, (3) The data migration strategy is illogical (copying to S3 but never loading into the provisioned RDS instance), (4) The workflow doesn't demonstrate actual data migration to the newly created RDS instance, (5) RDSModifyDBInstanceOperator uses incorrect parameters like 'new_db_cluster_identifier' which doesn't exist for this operator. The pipeline doesn't achieve a coherent database provisioning, configuration, and migration workflow as requested.",0.3333333333333333
634,Baseline (Qwen 2.5 1.5B Instruct),350,0,"The code imports two different operators for the same task: 'GCSToGoogleDriveOperator' and 'GoogleCloudStorageToGoogleDriveOperator'. Only one of these is actually used. More critically, the 'create_connection_task' uses a BashOperator (which is not imported) to run a gcloud command, which is not idiomatic for managing Airflow connections. Airflow connections should be managed through the UI, CLI, or programmatically via the Airflow API/metadata database, not through bash commands. The BashOperator is also not imported, causing a NameError.",0,"The code imports 'GoogleCloudStorageToGoogleDriveOperator' from 'airflow.providers.google.cloud.transfers.gcs_to_drive', but this operator does not exist in standard Airflow providers. The correct operator is 'GCSToGoogleDriveOperator' from 'airflow.providers.google.suite.transfers.gcs_to_gdrive'. Additionally, 'LocalFilesystemToGCSOperator' does not have parameters like 'src_dir' and 'dst_path_prefix' - it uses 'src' and 'dst' instead. The 'GoogleCloudStorageToGoogleDriveOperator' parameters like 'drive_file_name_template' and 'impersonate_user' appear to be fabricated or incorrect.",0,"The instruction asks to 'create a temporary GCP connection' which should involve creating an Airflow connection object, not running a gcloud config command. The code attempts to upload files to GCS and transfer to Google Drive, but uses incorrect operators and parameters. The workflow structure is present but the implementation is flawed. The instruction mentions 'different configuration options' for file transfers, but only one transfer task is implemented without demonstrating multiple configuration approaches.",0.0
635,Baseline (Qwen 2.5 1.5B Instruct),351,1,"The code correctly uses the DrillOperator from airflow.providers.apache.drill.operators.drill, which is the idiomatic way to interact with Apache Drill in Airflow. It does not wrap Drill logic in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",1,"The code uses only standard Airflow libraries and the official Apache Drill provider. There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run(). The imports are clean and production-ready.",0,"The code does not correctly implement Apache Drill functionality for JSON to Parquet conversion. The SQL statements used are not valid Drill syntax - they appear to be a mix of Spark SQL and Hive syntax. Apache Drill does not support 'CREATE TABLE...USING json' or 'ALTER TABLE...SET TBLPROPERTIES' commands. The correct approach would be to use Drill's CTAS (CREATE TABLE AS SELECT) with format specifications in the storage plugin configuration, or use CTAS with sys.options to write Parquet files. The pipeline structure is present (load, convert, verify), but the actual SQL implementation does not accomplish the stated goal of transforming JSON to Parquet using Apache Drill's actual capabilities.",0.6666666666666666
636,Baseline (Qwen 2.5 1.5B Instruct),352,0,The code uses PythonOperator with hooks instead of native CosmosDB operators. Airflow has airflow.providers.microsoft.azure.operators.cosmos.AzureCosmosDocumentSensor and related operators for CosmosDB operations. The code wraps logic in Python callables rather than using the idiomatic CosmosDB-specific operators and sensors available in the Azure provider package.,0,"The code contains multiple critical errors: (1) Uses MsSqlHook for CosmosDB operations, which is incorrect - MsSqlHook is for SQL Server, not CosmosDB; (2) Attempts to use SQL queries on CosmosDB which uses a different query language; (3) Calls non-existent method 'create_item' on MsSqlHook; (4) Missing imports for 'datetime' and 'timedelta'; (5) Uses deprecated 'PythonOperator' import path (should be from airflow.operators.python). These represent hallucinated/incorrect API usage.",0,"The instruction requires monitoring for a specific document and inserting a NEW document WHEN the condition is met (i.e., when the document is found). However, the code logic is flawed: (1) It always executes both tasks in sequence without conditional branching; (2) The insert_task attempts to insert a document with the SAME id as the one being searched for, which would cause a conflict; (3) There's no proper conditional logic to only insert when the monitored document exists. The workflow doesn't properly demonstrate document tracking with conditional insertion as requested.",0.0
637,Baseline (Qwen 2.5 1.5B Instruct),353,0,"The code uses deprecated operators (GoogleCloudMLTrainingOperator and GoogleCloudMLPredictionOperator from airflow.providers.google.cloud.operators.ml_engine_operator) which are for AI Platform (formerly ML Engine), not Vertex AI as requested. For Vertex AI in Airflow 2.9.3, it should use operators from airflow.providers.google.cloud.operators.vertex_ai such as CreateCustomTrainingJobOperator or CreateBatchPredictionJobOperator. Additionally, it uses PythonOperator with an undefined load_sales_data function instead of using GCS operators like GCSToLocalFilesystemOperator or appropriate data transfer operators.",0,"The code has several issues: (1) Missing imports for 'datetime' and 'timedelta', (2) References an undefined function 'load_sales_data' that is never implemented, (3) Uses deprecated/incorrect operator imports that don't align with modern Airflow provider packages for Vertex AI. While it doesn't import test harness code, these undefined references and missing imports make it non-functional and not production-ready.",0,"The instruction specifically requests a pipeline using Google Vertex AI for sales forecasting with time-series data ingestion, model training, and batch predictions. The code fails on multiple fronts: (1) Uses AI Platform (ML Engine) operators instead of Vertex AI operators, (2) Does not implement proper time-series data ingestion (load_sales_data is undefined), (3) Configures a generic linear regression training job rather than a time-series forecasting model appropriate for sales data, (4) The prediction step uses hardcoded instances instead of batch prediction on actual data for business planning. The workflow structure is present but the implementation does not fulfill the specific requirements.",0.0
638,Baseline (Qwen 2.5 1.5B Instruct),354,0,"The code uses PythonOperator with undefined python_callable functions (create_data_catalog, add_entries, etc.) instead of using native Airflow providers for Google Cloud Dataplex. Airflow 3.0.0 has google-cloud-airflow-providers with DataplexCreateEntryOperator, DataplexUpdateEntryOperator, DataplexDeleteEntryOperator, etc. This is a classic anti-pattern of wrapping logic in PythonOperator instead of using purpose-built operators.",0,"The code references undefined functions (create_data_catalog, add_entries, update_entries, list_entries, delete_entries) that are never imported or defined. Additionally, it imports 'datetime' but never includes the import statement. The code also uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.0. This is incomplete and non-functional code.",0,"While the DAG attempts to show a lifecycle (create, add, update, list, delete), it fails to demonstrate the specific requirements: creating and managing metadata catalog entries, entry types, and aspect types in Google Cloud Dataplex. The instruction specifically mentions 'entry types' and 'aspect types' which are key Dataplex Catalog concepts, but the code only references generic catalog operations without proper implementation. The code structure suggests the right flow but lacks actual working implementation.",0.0
639,Baseline (Qwen 2.5 1.5B Instruct),355,0,"The code uses deprecated operators that don't exist in Airflow 2.9.3. 'SimpleHttpOperator' was deprecated and replaced by 'HttpOperator' from 'airflow.providers.http.operators.http'. 'EmailOperator' should be imported from 'airflow.operators.email' but the parameters used (smtp_host, smtp_port, smtp_username, smtp_password) are not valid for EmailOperator - these should be configured in an Airflow connection, not passed directly to the operator.",0,"The code imports 'SimpleHttpOperator' from 'airflow.operators.http_operator' which doesn't exist in Airflow 2.9.3. It also uses non-existent parameters for EmailOperator (smtp_host, smtp_port, smtp_username, smtp_password). Additionally, 'datetime' is used but not imported. The endpoint '/api/v1/server/ip' appears to be a made-up internal API that doesn't exist.",0,"While the DAG attempts to retrieve an IP address and send an email, it fails to properly implement the solution. The instruction asks to retrieve the 'current server's external IP address', which typically means using a public service like 'https://api.ipify.org' or 'https://ifconfig.me', not a custom internal endpoint. The email content uses 'get_server_ip.output' which is not valid syntax for accessing XCom values in Airflow - it should use Jinja templating or XComArg. The workflow structure is correct but the implementation is fundamentally broken.",0.0
640,Baseline (Qwen 2.5 1.5B Instruct),357,0,"The code uses deprecated 'PythonOperator' imported from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. Additionally, the cleanup_function is defined AFTER it's referenced in the PythonOperator, which would cause a NameError. While the code does use native operators for GCS and ML Engine tasks (which is good), the improper use of PythonOperator and the deprecated import path fails idiomatic standards.",1,The code uses only standard Airflow libraries and Google Cloud providers. There are no imports from internal testing modules or test harness boilerplate. All imports are from legitimate Airflow 2.x packages.,0,"The DAG has critical logical errors that prevent it from fulfilling the instruction. The task dependencies are reversed: 'import_data_task >> sync_data_task' means dataset creation happens before data sync, and 'train_model_task >> import_data_task' means training happens before import, and 'cleanup_task >> train_model_task' means cleanup happens before training. The correct flow should be: sync_data -> import_data -> train_model -> cleanup. Additionally, the BigQueryCreateDatasetOperator only creates an empty dataset but doesn't actually import/load the data into BigQuery tables. The instruction requires 'data synchronization, dataset import, model training, and resource cleanup' in a systematic workflow, but the implementation has the steps in reverse order and missing actual data import logic.",0.3333333333333333
641,Baseline (Qwen 2.5 1.5B Instruct),359,0,"The code uses PythonOperator with manual API calls instead of using the native OpenAI provider. Airflow 3.0.0 has `airflow.providers.openai` with operators like `OpenAIEmbeddingOperator` and hooks. For asynchronous batch processing with deferred execution, it should use deferrable operators (async operators with triggers) but instead uses synchronous PythonOperator. The instruction explicitly asks for 'deferred execution' which in Airflow means using deferrable/async operators, not standard synchronous operators.",0,"The code imports 'airflow.operators.python_operator.PythonOperator' which is deprecated in Airflow 3.0.0. The correct import should be 'airflow.operators.python.PythonOperator'. Additionally, the OpenAI API usage shows 'openai.Completion.create' with 'engine=""text-davinci-instruct""' which is an outdated API pattern (legacy Completions API). The modern OpenAI API uses ChatCompletion with model names like 'gpt-4' or 'gpt-3.5-turbo'.",0,"The instruction asks for 'asynchronous batch processing' with 'deferred execution' and a 'scalable pattern for managing the batch processing lifecycle'. The code fails on multiple fronts: (1) It doesn't implement true async/deferred execution (no deferrable operators or triggers), (2) It doesn't showcase batch processing - it processes items sequentially with incorrect upstream dependencies (tasks[i].set_upstream(tasks[i+1]) creates reverse sequential dependencies), (3) No batch lifecycle management is implemented (no batch creation, status checking, or result retrieval pattern), (4) The tasks should run in parallel for scalability but are chained sequentially. The code misses the core requirements of the instruction.",0.0
642,Baseline (Qwen 2.5 1.5B Instruct),360,0,"The code uses DummyOperator with python_callable parameter, which is not valid - DummyOperator doesn't accept python_callable. For bucket operations, it should use GCSCreateBucketOperator and GCSDeleteBucketOperator from airflow.providers.google.cloud.operators.gcs. The translation operator import path is also incorrect - it should be from airflow.providers.google.cloud.operators.translate, not 'translations'.",0,"The code contains multiple hallucinated elements: (1) GoogleCloudTranslateOperator is imported from a non-existent path 'airflow.providers.google.cloud.translations' - the correct path is 'airflow.providers.google.cloud.operators.translate'. (2) The operator parameters appear to be fabricated - parameters like 'destination_language', 'output_format', 'model', and 'credentials_path' don't match the actual CloudTranslateTextOperator API. (3) DummyOperator is used with python_callable which doesn't exist on that operator. (4) Functions create_bucket and delete_bucket are referenced but never defined.",0,"The code attempts to address the instruction but fails in execution. While it tries to create a bucket, perform translations, and delete the bucket, the implementation is fundamentally broken: (1) Bucket creation and deletion use DummyOperator incorrectly instead of actual GCS operators. (2) The task dependencies are incomplete - only the first translation task is linked to create_bucket due to the loop structure, and delete_bucket has no upstream dependencies defined. (3) Missing required imports (datetime, timedelta). The workflow structure doesn't properly orchestrate the required sequence of create → translate → delete.",0.0
643,Baseline (Qwen 2.5 1.5B Instruct),362,0,"The code uses AzureContainerInstancesHook directly and calls methods on it (list_running_containers, start_container, stop_container, delete_container) without wrapping them in proper Airflow Operators or Tasks. This is not idiomatic Airflow. The code should use AzureContainerInstancesOperator or create proper task definitions using @task decorator or PythonOperator. The hook is being used as if it returns tasks, but hooks are meant to be used within operators, not as standalone task creators.",0,"The code hallucinates non-existent methods on AzureContainerInstancesHook. Methods like 'list_running_containers()', 'start_container()', 'stop_container()', and 'delete_container()' do not exist in the standard Airflow Azure provider's AzureContainerInstancesHook. The actual hook provides methods like 'get_conn()' to interact with Azure SDK, but not these high-level task-creating methods. Additionally, 'datetime' is used but not imported.",0,"The instruction asks for a DAG that demonstrates launching different container configurations with varying registry sources and volume mounting strategies, and includes testing containerized applications. The code attempts to start containers but: (1) does not demonstrate multiple different container configurations, (2) does not show varying registry sources (e.g., Docker Hub, Azure Container Registry, private registries), (3) does not implement any testing logic for the deployed containers, (4) the volume mounting is incomplete and incorrect (missing proper Azure File Share or other volume configurations), and (5) lacks task dependencies to create a proper workflow.",0.0
644,Baseline (Qwen 2.5 1.5B Instruct),364,0,"The code uses non-existent or incorrectly named operators. In Airflow 3.0.1, there is no 'airflow.operators.dynamodb_operator' module with 'CreateTableOperator' or 'DeleteTableOperator'. The correct approach would use operators from 'airflow.providers.amazon.aws.operators.dynamodb' or similar AWS provider modules. Additionally, 'DDBPutItemOperator' is referenced but not imported, and 'DynamodbKeyExistsSensor' from 'airflow.sensors.dynamodb_sensor' does not exist in standard Airflow providers. The correct sensor would be from 'airflow.providers.amazon.aws.sensors.dynamodb'.",0,"The code contains multiple hallucinated imports and operators that do not exist in standard Airflow libraries. Specifically: 'airflow.operators.dynamodb_operator.CreateTableOperator', 'airflow.operators.dynamodb_operator.DeleteTableOperator', 'airflow.sensors.dynamodb_sensor.DynamodbKeyExistsSensor', and the undefined 'DDBPutItemOperator'. These are fabricated module paths and class names not present in Airflow 3.0.1 or its AWS provider packages. Additionally, 'datetime' and 'timedelta' are used but not imported.",0,"The instruction requires 'value checking with different matching strategies' and testing 'DynamoDB item value detection', but the code only checks for table/key existence using 'DynamodbKeyExistsSensor' (which itself is hallucinated). There is no implementation of value checking or different matching strategies for item values. The instruction emphasizes 'value detection' and 'different matching strategies', which are completely missing from the implementation. While table creation and deletion are present, the core requirement of value checking with various strategies is not fulfilled.",0.0
645,Baseline (Qwen 2.5 1.5B Instruct),367,0,"The code uses deprecated PythonOperator import ('airflow.operators.python_operator' instead of 'airflow.operators.python'). More critically, for a multi-DAG workflow with parent-child relationships, the code should use idiomatic Airflow operators like TriggerDagRunOperator and ExternalTaskSensor to establish cross-DAG dependencies, but instead only defines a single DAG with basic PythonOperators.",0,"The code has multiple issues: (1) Missing import for 'datetime' module, (2) Functions 'generate_dummy_data' and 'process_data' are referenced before they are defined, (3) Invalid syntax 'depends_on=[generate_data]' - should use set_upstream/set_downstream or >> operator, (4) The process_data function expects a parameter but PythonOperator won't pass the return value automatically without proper task context usage.",0,"The instruction explicitly asks for a 'multi-DAG workflow' with 'parent-child DAG relationship' that demonstrates 'dependency management between two related data processing pipelines' where 'one DAG triggers and monitors the execution of tasks in another DAG'. The provided code only contains a single DAG with two tasks in sequence, completely missing the core requirement of having multiple DAGs with cross-DAG dependencies using operators like TriggerDagRunOperator and ExternalTaskSensor.",0.0
646,Baseline (Qwen 2.5 1.5B Instruct),369,0,"The code uses BashOperator with gcloud CLI commands instead of the idiomatic Airflow approach. Airflow provides google.cloud.operators.tasks sensors and operators specifically for Google Cloud Tasks. The proper approach would be to use CloudTasksQueueEmptyOperator or a sensor from airflow.providers.google.cloud.sensors.tasks to monitor queue status, rather than wrapping shell commands in BashOperator.",0,"The code imports 'BashOperator' from 'airflow.operators' which is incorrect for Airflow 2.9.3. The correct import should be 'from airflow.operators.bash import BashOperator'. Additionally, 'datetime' is used but never imported. These are not hallucinations of test modules, but they are import errors that would prevent the code from running.",0,"The instruction requires monitoring a Google Cloud Tasks queue to ensure all tasks have been processed before proceeding with downstream operations. The provided code has critical flaws: (1) It only counts lines with 'State: DONE' but doesn't verify if the queue is empty or compare against total tasks, (2) There's no conditional logic or sensor to actually wait/block until the queue is empty - the downstream task will execute regardless of queue status, (3) The bash command doesn't properly check queue emptiness or task completion status. A proper implementation would use a sensor to poll and wait until the queue meets the completion criteria before allowing downstream tasks to proceed.",0.0
647,Baseline (Qwen 2.5 1.5B Instruct),370,0,"The code fails to use the idiomatic Airflow approach for inter-DAG dependencies. It should use ExternalTaskSensor and ExternalTaskMarker (or at minimum ExternalTaskSensor) from airflow.sensors.external_task, but instead uses a generic PythonOperator with no actual signaling mechanism. The instruction specifically mentions 'external task markers and sensors' which are specific Airflow constructs that were not utilized.",1,"The code uses only standard Airflow imports and does not include any test harness code, internal testing modules, or non-existent libraries. However, it is missing the datetime and timedelta imports which would cause runtime errors, but this is an incompleteness issue rather than hallucination.",0,"The instruction explicitly requires: (1) two separate DAGs - only one parent DAG is provided, the child DAG is completely missing, (2) external task markers and sensors for synchronization - neither are implemented, (3) a child DAG that waits for the parent's specific task - this is entirely absent. The code only provides a basic parent DAG with a simple PythonOperator and does not demonstrate any inter-DAG dependency workflow or synchronization mechanism.",0.3333333333333333
648,Baseline (Qwen 2.5 1.5B Instruct),371,0,"This code is not an Airflow DAG at all. It contains no DAG definition, no operators, no tasks, and no task dependencies. It's a plain Python script using Google Cloud client libraries directly. For Airflow 2.7.2 with Vertex AI, it should use providers like 'airflow.providers.google.cloud.operators.vertex_ai' with operators such as VertexAICreateCustomTrainingJobOperator, VertexAIModelServiceOperator, or similar native Airflow operators for Vertex AI operations.",0,"The code imports 'automl_v1beta1' which is an outdated AutoML library, not Vertex AI. Vertex AI uses different APIs (aiplatform). The code also references non-existent methods like 'client.train_model()', 'client.evaluate_model()', and 'client.deploy_model()' with incorrect parameter structures that don't match actual Google Cloud API signatures. Additionally, there are no Airflow imports whatsoever, making this fundamentally not Airflow code.",0,"The instruction explicitly asks for an Airflow DAG workflow for Vertex AI with training, evaluation, and deployment. While the script attempts to cover these three phases conceptually, it fails to deliver: (1) No Airflow DAG structure, (2) Uses deprecated AutoML API instead of Vertex AI, (3) No demonstration of 'comprehensive data science pipeline' with different data types and use cases, (4) No actual workflow orchestration or task dependencies. The code is a non-functional script that doesn't meet the core requirement of being an Airflow workflow.",0.0
649,Baseline (Qwen 2.5 1.5B Instruct),372,0,"The code fails to use idiomatic Airflow patterns. It creates multiple PythonOperator tasks (insert_data_to_synapse, copy_blob_from_storage, trigger_synapse_job, monitor_synapse_job_status, cleanup_temp_files) without defining the actual callable functions, and more critically, it doesn't use the appropriate Azure Synapse operators available in the airflow.providers.microsoft.azure package. For Airflow 2.9.3, operators like AzureSynapseRunPipelineOperator or AzureSynapseRunSparkBatchOperator should be used instead of wrapping logic in PythonOperators. The code also instantiates a SynapseHook directly at the DAG level rather than using it within an operator, which is not the correct pattern.",0,"The code references multiple undefined Python functions (insert_data_to_synapse, copy_blob_from_storage, trigger_synapse_job, monitor_synapse_job_status, cleanup_temp_files) that are never implemented. Additionally, it uses 'datetime' without importing it (missing 'from datetime import datetime'). The SynapseHook is instantiated with parameters that don't match the actual Airflow provider API (e.g., 'auth_type', 'use_encryption', 'encryption_algorithm' are not valid parameters for SynapseHook). The HttpSensor usage for checking blob existence is also incorrect - it uses 'endpoint_url' which is not a valid parameter for HttpSensor (should be 'endpoint').",0,"While the code attempts to demonstrate Azure Synapse integration with Airflow, it fails to provide a working implementation. The instruction asks for 'automated pipeline triggering' and 'enterprise data orchestration', but the code only provides skeleton tasks without actual implementation. The callable functions are referenced but never defined, making this a non-functional DAG. A proper implementation should use AzureSynapseRunPipelineOperator or similar operators to actually trigger Synapse pipelines, and should include complete, working code that demonstrates the integration rather than placeholder functions.",0.0
650,Baseline (Qwen 2.5 1.5B Instruct),375,0,"The code uses PythonOperator with custom callable functions (create_task_in_asana, find_task_in_asana, etc.) instead of using the native Asana operators available in Airflow. The idiomatic approach would be to use operators from airflow.providers.asana.operators.asana (e.g., AsanaCreateTaskOperator, AsanaUpdateTaskOperator, AsanaDeleteTaskOperator) which are specifically designed for Asana task management operations.",0,"The code references undefined functions (create_task_in_asana, find_task_in_asana, update_task_in_asana, delete_task_from_asana) that are never imported or defined. Additionally, it imports PythonOperator from the deprecated 'airflow.operators.python_operator' module (should be 'airflow.operators.python' in Airflow 3.0.6), and the 'datetime' and 'timedelta' imports are missing entirely. The code is not production-ready and would fail immediately upon execution.",1,"The DAG structure correctly demonstrates the full lifecycle of Asana task management as requested: creating, finding, updating, and deleting tasks in a sequential manner with proper task dependencies (create_task >> find_task >> update_task >> delete_task). The business logic flow matches the instruction requirements despite implementation issues.",0.3333333333333333
651,Baseline (Qwen 2.5 1.5B Instruct),377,0,"This code is not an Airflow DAG at all. It contains no DAG definition, no operators, no tasks, and no Airflow imports. It's just a standalone Python function using the Google Cloud DLP client library directly. A proper implementation should use CloudDLPDeidentifyContentOperator or similar operators from airflow.providers.google.cloud.operators.dlp, not raw Python functions.",1,"The code uses standard Google Cloud DLP library (google.cloud.dlp_v2) without any test harness imports or internal testing modules. While it's not Airflow code, it doesn't contain hallucinated or test-specific imports.",0,"The instruction explicitly asks for an 'Airflow pipeline' that demonstrates masking, templating, and optionally reversing sensitive information transformations. This code: (1) is not an Airflow DAG/pipeline at all, (2) only performs inspection/detection of phone numbers, not de-identification/masking as requested, (3) does not demonstrate templating or reversible transformations, and (4) lacks the workflow orchestration aspect entirely. It fundamentally fails to deliver the requested data privacy workflow in Airflow.",0.3333333333333333
652,Baseline (Qwen 2.5 1.5B Instruct),379,0,"The DAG uses only DummyOperator for all tasks instead of leveraging appropriate Airflow providers and operators. For S3 operations, it should use S3CreateBucketOperator, LocalFilesystemToS3Operator, S3DeleteBucketOperator, etc. from airflow.providers.amazon.aws.operators.s3. While S3Hook is imported, it's never used. The code represents a skeleton/placeholder rather than idiomatic Airflow implementation that uses task-specific operators.",0,"The code imports DummyOperator from 'airflow.operators.dummy_operator', which is deprecated in Airflow 3.0.1. The correct import should be 'from airflow.operators.empty import EmptyOperator'. Additionally, 'timedelta' is used in default_args but never imported from datetime module, which would cause a runtime error. These issues indicate the code is not production-ready for the specified Airflow version.",0,"The instruction explicitly requests implementation of 'dynamic environment setup, S3 bucket creation, local file generation, file upload, and subsequent resource cleanup' with a 'systematic approach to demonstrate cloud file transfer best practices'. The provided code only creates a skeleton with DummyOperators that perform no actual operations. None of the required functionality is implemented - no actual bucket creation, no file generation logic, no upload mechanism, and no cleanup implementation. This is a placeholder DAG structure, not a working implementation.",0.0
653,Baseline (Qwen 2.5 1.5B Instruct),380,0,"The DAG exclusively uses PythonOperator for all tasks instead of leveraging Airflow's Google Cloud provider operators. For a Vertex AI ML pipeline, it should use operators like CreateCustomTrainingJobOperator, CreateDatasetOperator, DeployModelOperator, etc. from airflow.providers.google.cloud.operators.vertex_ai. The code also imports PythonOperator from the deprecated 'airflow.operators.python_operator' path instead of 'airflow.operators.python'. Additionally, it imports sklearn and pandas which suggests local ML training rather than using Vertex AI's managed training services as requested.",1,"The code does not import any internal testing modules or include test harness boilerplate. All imports are from standard libraries (airflow, google.cloud, sklearn, pandas). While the implementation is not correct for the use case, there are no hallucinated testing utilities or non-existent modules.",0,"The instruction explicitly requests 'using Google Vertex AI' with 'dataset creation, model training, versioning, and deployment with robust error handling and cloud infrastructure management.' The provided code does not use Vertex AI at all - it uses sklearn for local model training, has placeholder functions with only print statements, lacks any Vertex AI integration, has no model versioning logic, no robust error handling beyond basic retries, and doesn't leverage cloud infrastructure management. The code is a generic skeleton that doesn't fulfill the specific requirement of building a Vertex AI-based ML pipeline.",0.3333333333333333
654,Baseline (Qwen 2.5 1.5B Instruct),381,0,"The code fails to use proper Airflow operators. It incorrectly calls `AzureComputeHook().run_command()` directly and assigns it to a task variable, which is not how Airflow tasks work. Hooks are meant to be used within operators, not called directly at the DAG definition level. Additionally, it uses a `PythonOperator` with an undefined `run_command` function instead of using appropriate Azure operators like `AzureContainerInstancesOperator` or `AzureBatchOperator` for remote execution. The proper idiomatic approach would be to use operators from `airflow.providers.microsoft.azure.operators.*` packages.",0,"The code contains several hallucinated elements: (1) `AzureComputeHook().run_command()` method does not exist in the actual Airflow Azure provider - this is a fabricated API; (2) The parameters like 'cluster_name', 'vm_size', 'image', 'admin_username', 'ssh_private_key_path' are not valid for any standard Azure hook method; (3) The `run_command` function referenced in the PythonOperator is never defined, making this code non-functional; (4) The way the hook is being called directly and assigned to a task variable is not valid Airflow syntax.",0,"The instruction asks for a DAG that demonstrates remote task scheduling and execution on Azure with specific VM specifications and command line task execution. While the code attempts to address these requirements conceptually (VM creation with specifications, command execution), it fails in execution: (1) The implementation is fundamentally broken and non-functional; (2) It doesn't actually demonstrate a working cloud-based computational workflow; (3) The VM creation and command execution are not properly implemented using valid Airflow/Azure constructs. A proper implementation would use operators like AzureContainerInstancesOperator, AzureBatchOperator, or similar to actually schedule and execute remote tasks on Azure infrastructure.",0.0
655,Baseline (Qwen 2.5 1.5B Instruct),382,0,"The code fails to use idiomatic Airflow patterns. It wraps all Power BI operations in PythonOperator with custom functions instead of using the native PowerBIOperators available in airflow.providers.microsoft.powerbi. Specifically, it should use PowerBIRefreshDatasetOperator for dataset refresh operations and PowerBIHook methods directly through operators. Additionally, it incorrectly uses MsSqlHook to query fictional 'powerbi.workspaces' tables instead of using PowerBIHook methods to interact with the Power BI REST API.",0,"The code contains significant hallucinations. It assumes the existence of SQL tables like 'powerbi.workspaces' and 'powerbi.datasets' that don't exist in standard Power BI architecture. Power BI data is accessed via REST API, not SQL queries. The code also uses deprecated 'PythonOperator' import path (should be from airflow.operators.python) and includes placeholder functions with 'pass' statements that would fail in production. Missing imports for 'datetime' and 'timedelta' would cause runtime errors.",0,"The code fails to properly implement the requested functionality. The instruction asks for: (1) retrieving workspace information, (2) refreshing datasets automatically, (3) scanning workspace modifications, (4) tracking dataset refresh status, and (5) handling administrative tasks. While the code attempts to create tasks for some of these, the implementations are either empty (pass statements) or fundamentally incorrect (using SQL queries instead of Power BI API calls). It doesn't implement workspace modification scanning or proper dataset refresh status tracking. The workflow lacks proper task dependencies and doesn't create a coherent pipeline.",0.0
656,Baseline (Qwen 2.5 1.5B Instruct),383,0,The code uses PythonOperator with a custom function wrapping papermill.execute_notebook() instead of using the native PapermillOperator from airflow.providers.papermill.operators.papermill. This is the exact anti-pattern described in the scoring criteria - wrapping functionality in a PythonOperator instead of using the purpose-built operator.,1,"The code is clean and does not import any internal testing modules or include test harness boilerplate. It uses standard libraries (airflow, papermill) without any hallucinated or leaked test code.",0,"The instruction requires three key elements: (1) dynamically execute notebook with date-based parameters - partially met but parameters are not actually passed to papermill, (2) generate new output notebook for each run - met, (3) verify notebook execution by checking a specific message - NOT met. The code does not include any verification step to check for a specific message generated during notebook execution. Additionally, the code is missing the 'datetime' import and doesn't pass date parameters to the notebook execution, which was explicitly requested.",0.3333333333333333
657,Baseline (Qwen 2.5 1.5B Instruct),384,0,"The code violates idiomatic Airflow principles by directly calling Hook methods (SageMakerHook().run_notebook_instance()) instead of using appropriate Operators. For activation (start) and deactivation (stop) operations, the code should use SageMakerStartNotebookInstanceOperator and SageMakerStopNotebookInstanceOperator respectively. Additionally, the Hook methods are being called at DAG parse time rather than at task execution time, and the return values are not valid Airflow tasks. The 'run_notebook_instance' method also doesn't exist in the SageMakerHook API.",0,"The code hallucinates a non-existent method 'run_notebook_instance' on SageMakerHook. This method does not exist in the standard Airflow SageMaker provider. The parameters like 'state', 'wait_for_completion', and 'max_wait_time' in this context are fabricated. Additionally, the 'image_name' parameter in SageMakerCreateNotebookInstanceOperator is not a valid parameter for this operator.",0,"While the code attempts to address the instruction's requirement for sequential management (provisioning, activation, deactivation, cleanup), it fails to properly implement the activation and deactivation steps. The tasks for starting and stopping the notebook instance are not valid Airflow tasks and would cause the DAG to fail at parse time. The code also lacks proper task dependencies to ensure sequential execution. No dependency chain (e.g., create_notebook_instance_task >> activate_notebook_instance_task >> ...) is defined.",0.0
658,Baseline (Qwen 2.5 1.5B Instruct),385,0,"The code uses GKE (Google Kubernetes Engine) operators instead of Compute Engine operators. The instruction specifically requests 'Airflow operators for Google Cloud Compute Engine' to create a VM instance and run SSH commands. The correct operators should be ComputeEngineInsertInstanceOperator, ComputeEngineSSHOperator, and ComputeEngineDeleteInstanceOperator from airflow.providers.google.cloud.operators.compute. Instead, the code uses GKEClusterCreateOperator, GKEPodExecOperator, etc., which are for Kubernetes clusters, not Compute Engine VMs.",0,"The GKEPodExecOperator includes a 'use_sudo' parameter that does not exist in the actual Airflow provider. This is a hallucinated parameter. Additionally, the import statement references 'airflow.providers.google.cloud.operators.compute_engine' but imports GKE operators which should come from 'airflow.providers.google.cloud.operators.kubernetes_engine'. The operators themselves exist but are incorrectly imported from the wrong module path.",0,"The instruction asks for a workflow that creates a VM instance on Compute Engine, runs parallel SSH commands with different authentication techniques, and cleans up the infrastructure. The provided code creates a GKE cluster (Kubernetes) instead of a Compute Engine VM instance, uses GKEPodExecOperator to execute commands in pods rather than SSH commands on VMs, and does not demonstrate different authentication techniques as requested. The fundamental infrastructure type is wrong (GKE vs Compute Engine VM).",0.0
659,Baseline (Qwen 2.5 1.5B Instruct),386,0,"The code fails idiomatic Airflow practices. While it attempts to use CassandraRowSensor (which is appropriate), the validate_data() function wraps CassandraHook logic inside a PythonOperator instead of using native Cassandra operators. The validation logic should ideally use CassandraOperator or similar native operators for executing queries and validations, rather than wrapping hook calls in Python functions.",0,The code contains multiple issues: (1) It imports 'airflow.sensors.cassandra' which doesn't exist in standard Airflow - the correct import should be from 'airflow.providers.apache.cassandra.sensors.record' or 'airflow.providers.apache.cassandra.sensors.table'. (2) CassandraRowSensor is not a standard sensor in Airflow 3.0.6. (3) The sensor queries reference 'information_schema.columns' which is not valid for Cassandra (this is SQL/MySQL syntax). (4) Uses deprecated 'PythonOperator' instead of the Airflow 3.x standard 'PythonOperator' from airflow.operators.python.,1,"The code does fulfill the core instruction requirements: it creates a sensor-based workflow that monitors a Cassandra keyspace and table for data availability, implements record-level validation conditions (checking for empty tables and column existence), and establishes a gatekeeper pattern before downstream tasks. The DAG structure with sensors followed by validation tasks aligns with the requested 'sensor-based workflow that acts as a gatekeeper' pattern.",0.3333333333333333
660,Baseline (Qwen 2.5 1.5B Instruct),387,0,"The code uses GCSToGoogleDriveOperator incorrectly. This operator transfers FROM GCS TO Google Drive, not bidirectionally. The code attempts to use it for both upload (GDrive to GCS) and download (GCS to GDrive) operations, but the operator only supports GCS to GDrive transfers. For the reverse direction (GDrive to GCS), the code should use LocalFilesystemToGCSOperator combined with GoogleDriveToLocalOperator, or use the appropriate GoogleDriveToGCSOperator if available. The misuse of the operator shows a fundamental misunderstanding of the available Airflow operators.",0,"The code has multiple issues: (1) Missing import for 'datetime' - uses datetime(2023, 1, 1) without importing it, (2) The GCSToGoogleDriveOperator parameters are incorrect - this operator doesn't use 'source_bucket' and 'destination_bucket' parameters as shown. The actual operator transfers from GCS to Google Drive and uses different parameter names like 'source_bucket', 'source_object', 'destination_object' (Drive file ID or path), but not 'destination_bucket' since Google Drive doesn't use buckets. The code hallucinates an API that doesn't match the actual Airflow provider implementation.",0,"The instruction asks for bidirectional file transfer between Google Drive and GCS (both directions to showcase cross-platform movement). However, the code only uses GCSToGoogleDriveOperator for both tasks, which only supports one direction (GCS to GDrive). The 'upload_task' is mislabeled and incorrectly implemented - it cannot upload from Google Drive to GCS using this operator. The DAG fails to demonstrate the complete bidirectional transfer capability requested in the instruction.",0.0
661,Baseline (Qwen 2.5 1.5B Instruct),388,0,"The DAG fails to use idiomatic Airflow operators. It imports S3Hook and PostgresHook but wraps all logic in PythonOperator tasks instead of using native operators like S3ToLocalFilesystemOperator, LocalFilesystemToS3Operator, or PostgresOperator. The code references undefined functions (extract_data_from_s3, transform_data, load_data_into_postgresql, create_quicksight_dataset) that would presumably use hooks inside PythonOperators, which is the anti-pattern described in the scoring criteria. For AWS QuickSight operations, it should use airflow.providers.amazon.aws.operators.quicksight or similar native operators rather than wrapping SDK calls in PythonOperators.",1,"The code uses only standard Airflow imports from official providers (airflow.providers.amazon.aws, airflow.providers.postgres, airflow.operators.python_operator). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'. While the code references undefined functions, this is an incompleteness issue rather than hallucination of non-existent Airflow modules.",0,"The instruction explicitly requests 'automated provisioning and cleanup of AWS resources' as part of the ETL workflow. The provided DAG only implements the core ETL steps (extract, transform, load, create dataset) but completely omits any resource provisioning tasks (e.g., creating RDS instances, setting up IAM roles, provisioning QuickSight data sources) and cleanup tasks (e.g., teardown operators, resource deletion). A complete implementation should include setup and teardown tasks to fulfill the 'automated provisioning and cleanup' requirement.",0.3333333333333333
662,Baseline (Qwen 2.5 1.5B Instruct),389,0,"The code wraps ServiceBusHook calls inside Python functions and uses PythonOperator for all tasks. This is not idiomatic Airflow. The airflow.providers.microsoft.azure package provides dedicated operators like ASBSendMessageOperator, ASBReceiveMessageOperator, etc. that should be used instead of wrapping hook logic in PythonOperator. This is the exact anti-pattern described in the scoring criteria.",0,"The code contains multiple hallucinated methods on ServiceBusHook that do not exist in the actual Airflow Azure provider. Methods like create_queue(), delete_queue(), create_topic(), delete_topic(), publish_message(), subscribe_to_topic(), and unsubscribe_from_topic() are not part of the ServiceBusHook API. Additionally, 'pandas' is imported but never used. The code also uses deprecated 'airflow.operators.python_operator' instead of 'airflow.operators.python' for Airflow 3.0.0.",0,"While the code attempts to address the instruction by including queue management, topic management, message transmission, and subscription handling, it fails to implement a working solution due to hallucinated methods. The DAG is also incomplete (cuts off mid-definition). Additionally, it doesn't demonstrate a 'complete lifecycle' with proper task dependencies showing the flow of operations, and lacks proper error handling or demonstration of different messaging strategies as requested.",0.0
663,Baseline (Qwen 2.5 1.5B Instruct),390,0,The code wraps PostgreSQL data loading in a PythonOperator with PostgresHook instead of using the native PostgresOperator or SQLExecuteQueryOperator. All tasks use PythonOperator for operations that could leverage Airflow's built-in operators. The pattern of `hook = PostgresHook(...); df = hook.get_pandas_df(query)` inside a PythonOperator is explicitly non-idiomatic when PostgresOperator exists for database operations.,1,"The code uses only standard Airflow libraries (airflow.operators.python_operator, airflow.providers.postgres.hooks.postgres) and legitimate third-party packages (Prophet). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",0,"The code has critical implementation flaws that prevent it from fulfilling the instruction. The tasks don't properly pass data between them - load_data returns a DataFrame but preprocess_data doesn't receive it as input. The functions are defined but not properly connected via XCom or other data passing mechanisms. The geographic attributes (city, zip_code, county) are queried but never used in the forecasting logic. The 30-day forecast horizon is mentioned in make_predictions but the overall pipeline is broken and would fail at runtime. The instruction requires a working automated pipeline that transforms raw sales data into predictive insights, but this implementation cannot execute successfully.",0.3333333333333333
664,Baseline (Qwen 2.5 1.5B Instruct),391,0,"The code uses completely hallucinated operators that do not exist in Airflow. There are no operators called 'EC2CreateClusterOperator' or 'EC2WaitForClusterStateOperator' in airflow.providers.amazon.aws.operators.ec2. For EKS cluster management, the correct operators would be from airflow.providers.amazon.aws.operators.eks (e.g., EksCreateClusterOperator, EksDeleteClusterOperator). Additionally, the KubernetesPodOperator import path is incorrect - it should be from airflow.providers.cncf.kubernetes.operators.pod, not from airflow.providers.amazon.aws.operators.kubernetes.",0,"The code heavily hallucinates non-existent operators and import paths. EC2CreateClusterOperator and EC2WaitForClusterStateOperator do not exist in the Airflow codebase. The import 'from airflow.providers.amazon.aws.operators.kubernetes import KubernetesPodOperator' is also incorrect. Furthermore, the usage of KubernetesPodOperator is malformed - the 'image' parameter is set to a concatenation of apiVersion and kind from the pod_spec dictionary, which makes no sense. The operator also incorrectly uses 'aws_conn_id' parameter which is not valid for KubernetesPodOperator.",0,"While the code attempts to address the instruction's requirements of creating an EKS cluster lifecycle (creation, validation, deployment), it fails to implement any of these steps correctly due to using non-existent operators. The instruction asks for EKS cluster management, but the code uses hallucinated EC2 operators instead of the proper EKS operators. The validation step is attempted but with a non-existent operator. The pod deployment is also incorrectly implemented with wrong parameters and import paths. The overall structure shows intent to follow the instruction, but the implementation is fundamentally broken.",0.0
665,Baseline (Qwen 2.5 1.5B Instruct),392,0,"The code fails to use proper Airflow operators and task structure. It attempts to execute translation logic at DAG definition time (in the for loop) rather than creating tasks that execute at runtime. The code uses a custom function `translate_file()` that would need to be wrapped in a PythonOperator, but even that is not done. Additionally, it references `GoogleCloudTranslationOperator` in imports but never uses it. The code also uses `GCSEndpointHook` incorrectly (this hook doesn't have a `translate_text` method). Proper idiomatic Airflow would use `CloudTranslateTextOperator` or `CloudTranslateDocumentOperator` from the Google Cloud provider, along with `GCSToLocalFilesystemOperator` and `LocalFilesystemToGCSOperator` for file handling.",0,"The code contains multiple hallucinations and errors: 1) `GoogleCloudTranslationOperator` is imported but doesn't exist in Airflow providers (the correct operator is `CloudTranslateTextOperator`), 2) `GCSEndpointHook` is not a valid hook (should be `GCSHook`), 3) The `gcs_hook.translate_text()` method doesn't exist on any GCS hook, 4) `BashOperator` is used but never imported, 5) The code attempts to use hooks and execute logic at DAG parse time rather than task execution time. These are fundamental misunderstandings of Airflow's architecture and available components.",0,"The code fails to implement the requested functionality. The instruction requires: 1) Translation of multilingual documents across XLSX, DOCX, and PDF formats, 2) Use of cloud translation services, 3) Proper input/output configuration management, 4) Scalable document processing. The code attempts to address translation but fails because: a) It doesn't actually create Airflow tasks (logic runs at parse time), b) It doesn't handle different file formats appropriately (XLSX, DOCX, PDF require different extraction methods before translation), c) It doesn't demonstrate scalable patterns (no dynamic task generation, no proper task dependencies), d) The translation logic is fundamentally broken (wrong hooks/methods), e) File handling is incorrect (tries to open GCS paths as local files). A proper implementation would need tasks to download files from GCS, extract text based on format, translate using Translation API, and upload results.",0.0
666,Baseline (Qwen 2.5 1.5B Instruct),393,0,"The code uses PythonOperator with Hooks (EC2Hook, RDSHook) instead of using native AWS Operators. Airflow 2.7.2 provides specific operators like EcsCreateClusterOperator, RdsCreateDbInstanceOperator, etc. from airflow.providers.amazon.aws.operators that should be used for idiomatic Airflow code. The pattern of wrapping hook logic in PythonOperator is explicitly the anti-pattern described in the scoring criteria.",0,"The code contains several hallucinations: 1) EC2Hook.create_security_group() method does not exist in the standard Airflow AWS provider - the EC2Hook doesn't have this convenience method. 2) RDSHook.create_db_instance() and RDSHook.create_replication_instance() methods do not exist - RDSHook is primarily for connections, not for creating resources. 3) The function definitions are placed after they are referenced in the PythonOperator calls, which would cause a NameError. 4) The replication instance creation uses incorrect parameters - AWS DMS replication instances are not created via RDS and don't use 'SourceDBInstanceIdentifier' parameter.",1,"The code attempts to fulfill the instruction by creating three sequential tasks: creating security groups, configuring database endpoints, and setting up a replication instance for data transfer. The overall structure addresses the requirement to orchestrate cloud infrastructure provisioning and data migration tasks across AWS services, even though the implementation has technical issues.",0.3333333333333333
667,Baseline (Qwen 2.5 1.5B Instruct),394,1,"The code correctly uses native Airflow providers and operators specifically designed for Dataproc cluster management (DataprocClusterCreateOperator and DataprocClusterDeleteOperator from airflow.providers.google.cloud.operators.dataproc). It does not wrap functionality in PythonOperator with hooks, which would be non-idiomatic.",0,"The code has a critical import error: it uses 'datetime' in the default_args without importing it. The code imports 'timedelta' from datetime but not 'datetime' itself. The line 'start_date': datetime(2023, 1, 1) will fail at runtime. This is a production-readiness issue that prevents the code from executing properly.",0,"The instruction asks to 'orchestrate the complete lifecycle of a cloud-based distributed computing cluster, highlighting the sequential task dependencies and operational states.' While the code creates and deletes a cluster with proper dependencies, it does not demonstrate the 'complete lifecycle' or 'operational states' of a Dataproc cluster. A complete lifecycle would typically include: create cluster → submit job(s)/perform computation → monitor/validate → delete cluster. The current implementation only creates and immediately deletes the cluster without any actual workload execution, which doesn't fulfill the business requirement of orchestrating a meaningful cluster lifecycle.",0.3333333333333333
668,Baseline (Qwen 2.5 1.5B Instruct),395,0,"The code imports operators that do not exist in the official Airflow providers. The imports 'from airflow.providers.google.cloud.transfers.kafka import KafkaProducerOperator, KafkaConsumerOperator' and 'from airflow.providers.google.cloud.sensors.kafka import KafkaSensor' are hallucinated - these are not real Airflow operators. The Google Cloud provider does not have Kafka-specific operators in this path. For Kafka operations, the correct approach would be to use 'airflow.providers.apache.kafka' operators (e.g., ProduceToTopicOperator, ConsumeFromTopicOperator) or use Google Cloud operators for cluster management (e.g., DataprocCreateClusterOperator for Kafka on Dataproc, or appropriate GCP Managed Kafka operators if they exist).",0,"The code contains multiple hallucinated imports and operators that do not exist in standard Airflow libraries. Specifically: 1) KafkaProducerOperator and KafkaConsumerOperator do not exist in 'airflow.providers.google.cloud.transfers.kafka', 2) KafkaSensor does not exist in 'airflow.providers.google.cloud.sensors.kafka', 3) The parameters used (num_partitions, num_replicas, check_mode) are not valid for any real Kafka operators in Airflow. Additionally, KafkaProducerOperator is incorrectly used to 'create a Kafka cluster' which is not its purpose - cluster creation would require GCP-specific infrastructure operators.",0,"The instruction requires three key components: 1) Creating a Kafka cluster (infrastructure provisioning), 2) Producing messages, and 3) Consuming messages with a consumer group. While the code attempts to address all three, it fails on implementation: The 'create_kafka_cluster' task incorrectly uses KafkaProducerOperator instead of a proper GCP cluster creation operator (e.g., operators for Google Cloud Managed Service for Apache Kafka). The code does not actually create infrastructure - it just attempts to produce to a topic. The overall workflow structure is present but the implementation does not fulfill the actual requirements for managing a Google Cloud Managed Kafka cluster.",0.0
669,Baseline (Qwen 2.5 1.5B Instruct),396,0,"The code fails to use idiomatic Airflow patterns. It instantiates hooks at the DAG level (azure_storage_hook, gcs_hook) and attempts to call hook methods directly as tasks (azure_storage_hook.check_for_blob). This is not how Airflow operators work. The code should use AzureBlobStorageSensor or a PythonOperator with proper task definition, and most critically, it uses GCSToGCSOperator which transfers files within GCS, not from Azure to GCS. The correct operator would be AzureBlobStorageToGCSOperator or similar cross-cloud transfer operator.",0,"The code has multiple issues: (1) Missing import for 'timedelta' from datetime, (2) Missing import for 'GoogleCloudStorageHook', (3) The method 'check_for_blob' is called incorrectly - it's a hook method that returns a boolean, not a task, and the syntax used doesn't create a valid Airflow task, (4) GCSToGCSOperator is used incorrectly for cross-cloud transfer when it only works within GCS. While these aren't test harness imports, they represent broken/non-functional code that wouldn't run in production.",0,"The instruction requires checking for file existence in Azure and transferring from Azure to GCS. While the code attempts to check for blob existence, it does so incorrectly (not as a proper task). More critically, it completely fails the transfer requirement by using GCSToGCSOperator, which only transfers between GCS buckets, not from Azure to GCS. The correct approach would use AzureBlobStorageToGCSOperator or a combination of operators that actually perform cross-cloud transfer.",0.0
670,Baseline (Qwen 2.5 1.5B Instruct),397,0,The code uses PythonOperator with PostgresHook instead of the native PostgresOperator. Airflow provides airflow.providers.postgres.operators.postgres.PostgresOperator specifically for executing SQL queries against PostgreSQL databases. The pattern of wrapping hook.run() calls inside Python functions and using PythonOperator is not idiomatic when a dedicated operator exists for the task.,1,The code uses only standard Airflow libraries (airflow.providers.postgres.hooks.postgres.PostgresHook and airflow.operators.python_operator.PythonOperator). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective.,0,"While the code does demonstrate sequential PostgreSQL operations (create table, insert data, select data) with dependencies, it has a critical structural flaw: the Python functions (create_table, insert_data, select_data) are defined AFTER they are referenced in the PythonOperator declarations. This would cause a NameError at DAG parse time. Additionally, the dependency management uses the deprecated set_upstream() method instead of the more modern bitshift operators (>> or <<) or set_downstream(). The code would not execute successfully as written, failing to properly showcase the requested functionality.",0.3333333333333333
671,Baseline (Qwen 2.5 1.5B Instruct),398,0,"The code uses deprecated 'PythonOperator' imported from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. In Airflow 2.x, the correct import path is 'from airflow.operators.python import PythonOperator'. While PythonOperator is appropriate for custom logic simulation here, the import path is outdated and non-idiomatic for Airflow 2.8.4.",1,The code does not import any internal testing modules or include test harness boilerplate. It uses only standard Airflow libraries (albeit with deprecated import paths). No hallucinated functions like 'get_test_run' or test utilities are present.,0,"The code has critical implementation flaws that prevent it from fulfilling the instruction. The functions are defined AFTER they are referenced in the PythonOperator tasks, which will cause NameError at DAG parsing time. Additionally, 'get_metadata' and 'delete_metadata' functions expect parameters (metadata_id) but the PythonOperator tasks don't pass any arguments via 'op_kwargs' or 'op_args'. The instruction asks for a systematic workflow that 'simulates real-world metadata management processes' with validation, but the implementation only has print statements with no actual metadata tracking, validation logic, or state management between tasks. The pipeline lacks any mechanism to pass metadata IDs between create and subsequent operations.",0.3333333333333333
672,Baseline (Qwen 2.5 1.5B Instruct),399,1,"The code uses native BigQuery operators from airflow.providers.google.cloud.operators.bigquery including BigQueryCreateDatasetOperator, BigQueryInsertJobOperator, BigQueryDeleteTableOperator, and BigQueryExecuteQueryOperator. It does not wrap BigQuery logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code references 'BashOperator' in the log_result task but never imports it. This will cause a NameError at runtime. Additionally, 'from airflow.utils.dates import datetime' is incorrect for Airflow 3.0.1 - datetime should be imported from the standard library. The code also uses 'BigQueryExecuteQueryOperator' which was deprecated and removed in favor of 'BigQueryInsertJobOperator' in recent Airflow versions.",0,"The instruction asks for a pipeline that 'transforms raw data, performs quality checks, and demonstrates different query execution strategies.' While the code includes quality checks (check_count) and query execution, it has critical logical flaws: (1) it deletes the table immediately after inserting data and before executing queries, making subsequent operations fail; (2) it doesn't demonstrate actual data transformation - just basic insert/query operations; (3) the dependency chain is illogical (delete_table >> execute_query would fail since the table no longer exists); (4) log_result has incorrect dependencies (should depend on check_count, not be parallel to it). The workflow does not represent a functional data processing pipeline.",0.3333333333333333
673,Baseline (Qwen 2.5 1.5B Instruct),400,0,"The code fails to use idiomatic Airflow patterns. It wraps curl commands in PythonOperator functions instead of using the native LookerStartPdtBuildOperator from airflow.providers.google.cloud.operators.looker or similar Looker provider operators. The code uses subprocess.run with curl commands, which is a generic Pythonic pattern rather than leveraging Airflow's Looker provider operators designed specifically for PDT builds.",0,"The code references 'LookerJobSensor' from 'airflow.sensors.looker' which does not exist in Airflow 2.7.2. The correct import would be from airflow.providers.google.cloud.sensors.looker. Additionally, the code has critical errors: it references 'dag' variable before it's defined, missing 'subprocess' import, and the DAG instantiation is completely missing. The LookerJobSensor usage also appears incorrect as it's mixed with a custom monitor function that's never used.",0,"While the code attempts to address the requirements (blocking/non-blocking PDT builds, task chaining, sensor-based monitoring), it fails to properly implement them. The blocking vs non-blocking distinction is only shown via a URL parameter difference without proper implementation. The sensor monitoring is incorrectly configured and doesn't properly demonstrate sensor-based job monitoring. The 'chain_tasks' function doesn't demonstrate actual Airflow task chaining but rather makes another API call. The non-blocking task is never integrated into the workflow dependencies, missing a key aspect of showcasing different execution strategies.",0.0
674,Baseline (Qwen 2.5 1.5B Instruct),401,0,"The code does not use the proper TriggerDagRunOperator from airflow.operators.trigger_dagrun. Instead, it references a non-existent 'SecondaryDAGRunOperator' and wraps the logic inside a Python function called by trigger_secondary_dag(). The idiomatic approach would be to directly instantiate TriggerDagRunOperator as a task in the DAG, not wrap it in a Python function.",0,"The code references 'SecondaryDAGRunOperator' which does not exist in Airflow's standard library. This is a hallucinated/invented class name. Additionally, the code uses 'PythonOperator' import but never actually uses it, and the pattern of calling trigger_secondary_dag() as a function outside of task definition is incorrect and non-functional.",0,"The instruction explicitly asks to use TriggerDagRunOperator to trigger a secondary DAG with a predefined configuration message. The code fails to properly implement this: (1) it doesn't import or use the actual TriggerDagRunOperator, (2) it doesn't define a secondary DAG, (3) the task structure is fundamentally broken as it tries to instantiate an operator inside a function rather than as a DAG task, and (4) the code would not execute successfully as written.",0.0
675,Baseline (Qwen 2.5 1.5B Instruct),402,1,"The code uses the appropriate KubernetesPodOperator from airflow.providers.kubernetes.operators.kubernetes_pod_operator, which is the idiomatic way to run Kubernetes pods in Airflow. It does not wrap logic in PythonOperator with hooks, but instead uses the native Kubernetes operator designed for this purpose.",1,"The code is clean and uses only standard Airflow libraries (airflow.providers.kubernetes). There are no imports from internal testing modules, no test harness boilerplate, and no references to test utilities like get_test_run(). The code is production-ready in structure.",0,"While the DAG demonstrates the conceptual workflow with tasks for provisioning, defining queues, and preparing for parallel processing, it only uses echo commands that print messages rather than actually implementing infrastructure-as-code. The instruction asks to 'illustrate' and 'demonstrate' provisioning computational resources, defining job queues, and preparing for parallel workload processing. The current implementation is purely symbolic (echo statements) and doesn't actually provision resources, create job queues, or demonstrate parallel processing capabilities. A proper implementation would use actual Kubernetes resources (ConfigMaps, Jobs, Deployments) or at least kubectl commands to create these resources, and demonstrate parallel execution through task groups or dynamic task mapping.",0.6666666666666666
676,Baseline (Qwen 2.5 1.5B Instruct),403,0,"The code uses PythonOperator with InfluxDBHook instead of using the native InfluxDBOperator. This is the exact anti-pattern described in the scoring criteria. The instruction explicitly asks to 'demonstrate integration with InfluxDB using the InfluxDBOperator', but the implementation wraps the hook logic in a PythonOperator instead of using the dedicated operator from airflow.providers.influxdb.operators.influxdb.",0,"The code imports 'PythonOperator' from 'airflow.operators.python_operator', which is deprecated in Airflow 3.x. The correct import should be 'from airflow.operators.python import PythonOperator'. Additionally, 'datetime' is used without being imported (missing 'from datetime import datetime'). The code also references 'fetch_data_task.output' which is not a valid way to pass data between tasks in Airflow - this would not work as written.",0,"The instruction specifically requests using 'InfluxDBOperator for time-bounded data extraction' and building a task that uses 'a dynamic time range relative to the current execution date'. While the code does implement time-bounded extraction, it fails on two key points: (1) it does not use InfluxDBOperator as explicitly requested, and (2) it uses datetime.now() instead of the execution_date/logical_date context variable, which means it's not properly using Airflow's execution context for dynamic time ranges.",0.0
677,Baseline (Qwen 2.5 1.5B Instruct),404,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocDeleteClusterOperator, DataprocSubmitJobOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code has a critical import error: it uses 'datetime' without importing it (line 'start_date': datetime(2023, 1, 1)). The import statement 'from datetime import timedelta' is present but 'from datetime import datetime' is missing. Additionally, the DataprocSubmitJobOperator usage appears incorrect - the parameters 'main_class', 'jar_file_uri', and 'args' are not direct parameters of this operator; instead, it expects a 'job' parameter with a properly structured job configuration dictionary. This suggests either hallucinated API usage or outdated/incorrect operator knowledge.",0,"The instruction explicitly requests a workflow demonstrating 'creating, starting, stopping, and deleting' a Dataproc cluster with handling of existing clusters. The provided code only includes create and delete operations, missing the start and stop operations entirely. There is no demonstration of handling existing clusters (e.g., using use_if_exists parameter or checking cluster state). The workflow shows create -> submit job -> delete, but does not fulfill the complete lifecycle management requirement of start/stop operations.",0.3333333333333333
678,Baseline (Qwen 2.5 1.5B Instruct),405,0,"The code uses deprecated 'PythonOperator' from 'airflow.operators.python_operator' instead of the modern 'airflow.operators.python.PythonOperator'. For Airflow 3.0.6, it should use the current import path. Additionally, the code uses the old-style DAG context manager pattern with 'dag.create_task_group()' which doesn't exist - the correct approach is 'from airflow.utils.task_group import TaskGroup'. The code also doesn't use the modern decorator-based approach (@task, @task_group, @setup, @teardown) which are idiomatic for Airflow 2.x+ and especially relevant for the instruction's requirement to demonstrate 'decorator-based and direct task relationship methods'.",0,The code contains a hallucinated method 'dag.create_task_group()' which does not exist in Airflow. The correct approach is to use 'TaskGroup' from 'airflow.utils.task_group'. This is a significant error that would cause the DAG to fail on import.,0,"The instruction explicitly requests: (1) nested task groups with explicit setup and teardown operations, (2) both decorator-based and direct task relationship methods, and (3) lifecycle management with initialization and finalization steps. The code fails on multiple counts: it doesn't use Airflow's explicit setup/teardown decorators (@setup/@teardown) introduced for lifecycle management, it doesn't demonstrate decorator-based task creation (@task decorator), the task groups are not nested (they're siblings), and the task dependency is incorrect (setup_task.set_upstream(main_task) makes main_task run BEFORE setup, which is backwards). The code only partially demonstrates direct task relationships but misses the core requirements.",0.0
679,Baseline (Qwen 2.5 1.5B Instruct),406,0,"The code uses the deprecated 'airflow.operators.bash_operator.BashOperator' import path. In Airflow 2.x (especially 2.8.4), the idiomatic import should be 'from airflow.operators.bash import BashOperator'. Additionally, 'timedelta' is used but never imported, and the 'start_date' should be a datetime object, not a string. The 'dag' parameter in operators is also deprecated in favor of context manager or decorator patterns in Airflow 2.x, though still functional.",1,"The code does not import any internal testing modules or include test harness boilerplate. It uses standard Airflow libraries (albeit with deprecated import paths), and there are no hallucinated functions or non-existent APIs.",0,"The instruction asks for a workflow demonstrating 'complex dependency relationships' between tasks. However, the dependencies are defined incorrectly: 'task_1.set_upstream(task_2)' makes task_2 run BEFORE task_1, and 'task_2.set_upstream(task_3)' makes task_3 run BEFORE task_2. This creates a reverse flow (task_3 → task_2 → task_1) instead of the logical forward progression suggested by the task names and echo statements. The correct approach should use 'set_downstream()' or the '>>' operator to create task_1 → task_2 → task_3. This fundamental error in dependency definition means the DAG does not properly demonstrate task sequencing as requested.",0.3333333333333333
680,Baseline (Qwen 2.5 1.5B Instruct),407,0,"The code imports 'ACIOperator' from 'airflow.providers.microsoft.azure.operators.aci_operator', but this is not the correct import path for Airflow 3.0.1. The actual operator is 'AzureContainerInstancesOperator' from 'airflow.providers.microsoft.azure.operators.container_instances'. The code uses a non-existent operator, which means it's not using the proper idiomatic Airflow approach for Azure Container Instances.",0,"The code hallucinates the 'ACIOperator' class which does not exist in the Airflow providers package. The correct operator is 'AzureContainerInstancesOperator'. Additionally, the parameters used (resource_requests, resource_limits, volumes, acr_credentials) do not match the actual API of the real operator. The second task usage with 'container_name' parameter also doesn't align with how the actual operator works.",0,"While the code attempts to address the instruction by creating a DAG that launches containerized workloads with configurable resources, environment variables, and storage volumes, it fails to do so correctly due to using non-existent operators and incorrect parameter structures. The instruction asks for a working pipeline demonstrating Azure Container Instances integration, but this code would not execute successfully. Additionally, the second task's purpose (checking status) is unclear and doesn't demonstrate a meaningful data processing pipeline.",0.0
681,Baseline (Qwen 2.5 1.5B Instruct),408,0,"The code fails to use idiomatic Airflow patterns. It wraps Google Ads and GCS operations in PythonOperators with Hooks instead of using native Operators. For example, it uses `PythonOperator(python_callable=extract_data_from_google_ads)` with GoogleAdsHook instead of using `GoogleAdsToGcsOperator` or similar transfer operators. Additionally, `GoogleCloudStorageHook.create_bucket_task()` and `GoogleCloudStorageHook.cleanup_bucket_task()` are not real methods - the correct approach would be to use `GCSCreateBucketOperator` and `GCSDeleteBucketOperator` from `airflow.providers.google.cloud.operators.gcs`.",0,"The code contains hallucinated/non-existent methods. `GoogleCloudStorageHook.create_bucket_task()` and `GoogleCloudStorageHook.cleanup_bucket_task()` do not exist in the Airflow codebase. Hooks are meant to be used within operators or callables, not to generate tasks directly. The code also imports `PythonOperator` from the deprecated `airflow.operators.python_operator` module instead of `airflow.operators.python`. Additionally, the functions `extract_data_from_google_ads` and `list_advertising_accounts` are defined after they are referenced, which would cause a NameError.",0,"The code does not fulfill the instruction requirements. While it attempts to create a workflow with bucket creation, data extraction, account listing, and cleanup, the implementation is incomplete and non-functional. The extract_data_from_google_ads and list_advertising_accounts functions are empty stubs with only 'pass' statements. Most critically, there is no actual data transfer to GCS - the extracted data is never uploaded to the bucket. The instruction specifically asks for a 'complete data transfer process' and 'cloud data migration', but no data is actually written to GCS. The workflow structure exists but lacks the core functionality requested.",0.0
682,Baseline (Qwen 2.5 1.5B Instruct),409,0,"The DAG fails to use idiomatic Airflow 2.7.2 patterns for dynamic parameter configuration. It should use DAG params with the Params class to define input types and validation rules, which is the native Airflow way to handle user inputs and validation. Instead, it uses generic PythonOperators with hardcoded values in functions. The correct approach would be to use `from airflow.models.param import Param` and define params in the DAG constructor with validation schemas.",1,"The code uses only standard Airflow libraries and does not import any internal testing modules or include test harness boilerplate. All imports are from legitimate Airflow packages (airflow.operators.python_operator, airflow.utils.dates, airflow core).",0,"The instruction asks for an 'interactive' DAG that demonstrates 'dynamic parameter configuration for workflow triggering' with 'various input types and validation rules' that users can customize. The provided code does not implement true dynamic parameters - it hardcodes values in the get_user_inputs function rather than accepting them from DAG trigger configuration. It doesn't showcase multiple input types (string, int, boolean, etc.) or demonstrate how users can actually customize these parameters when triggering the workflow. A proper implementation would use DAG params with different types and validation rules that users can modify via the UI or API when triggering the DAG.",0.3333333333333333
683,Baseline (Qwen 2.5 1.5B Instruct),410,0,"The code uses deprecated 'BashOperator' import path. In Airflow 3.0.6, it should be 'from airflow.operators.bash import BashOperator' not 'from airflow.operators.bash_operator import BashOperator'. The old import path was deprecated and removed in Airflow 2.0+. Additionally, the DAG uses the deprecated context manager pattern (dag=dag parameter) instead of the modern context manager or decorator approach recommended for Airflow 2.0+/3.0+.",0,"The code has a critical error: 'timedelta' is used in default_args but never imported. The import statement 'from datetime import timedelta' is missing. Also, 'datetime' is imported from 'airflow.utils.dates' which is non-standard - it should be 'from datetime import datetime'. While not test harness code, these are broken imports that would prevent the DAG from loading.",0,"The implementation does not properly demonstrate error handling with external alerting. The 'trigger_rule' is set to 'all_failed' which will never trigger because it requires ALL upstream tasks to fail, but there's only one upstream task. The correct trigger_rule should be 'one_failed' or the DAG should use on_failure_callback for proper failure notifications. The current setup will not send alerts when bash_task fails, failing to meet the requirement of 'integrated failure notifications to an incident management platform'.",0.0
684,Fine-tuned (Qwen 2.5 1.5B Airflow),0,1,"The code correctly uses the SnowflakeOperator from airflow.providers.snowflake.operators.snowflake, which is the idiomatic way to execute SQL statements in Snowflake. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing modules. Additionally, the line 'snowflake_insert_many >> TriggerRule.ALL_DONE' is incorrect syntax and appears to be malformed code.",0,"The instruction requires two key components: (1) insert 12 product records, and (2) validate the data load by verifying the row count. The code inserts 20 records instead of 12, and completely lacks any validation task to verify the total number of rows matches the expected count. There is no SELECT COUNT(*) query or any mechanism to check that the data was loaded correctly.",0.3333333333333333
685,Fine-tuned (Qwen 2.5 1.5B Airflow),2,1,"The code uses the appropriate Airflow components idiomatically. It imports and uses the AfterWorkdayTimetable class directly, uses the EmptyOperator from the standard providers package, and follows proper DAG construction patterns. No generic PythonOperator wrappers are used where native operators exist.",0,"The code contains a significant error that suggests hallucination or confusion about Airflow 3.0.0 API. The 'timetables' parameter as a dictionary is not a valid DAG parameter in Airflow. The correct approach is to pass the timetable instance directly to the 'schedule' parameter (e.g., schedule=AfterWorkdayTimetable(...)). The code shows 'schedule=""@daily""' alongside a 'timetables' dict, which is not standard Airflow API and appears to be fabricated syntax.",0,"The instruction asks to 'showcase the implementation of a custom timetable scheduling mechanism using the AfterWorkdayTimetable class' and 'triggers an empty operator after standard work hours'. While the code attempts to use AfterWorkdayTimetable, it does so incorrectly (wrong syntax with the timetables dict). More critically, it uses 'schedule=""@daily""' which would override any custom timetable behavior. The code does not properly implement the custom timetable as the primary scheduling mechanism, failing to demonstrate the requested scheduling flexibility.",0.3333333333333333
686,Fine-tuned (Qwen 2.5 1.5B Airflow),3,1,"The code uses native Airflow operators from the Google Marketing Platform provider (GoogleCampaignManagerBatchInsertOperator, GoogleCampaignManagerGetOperator, GoogleCampaignManagerUpdateOperator, GoogleCampaignManagerListOperator, GoogleCampaignManagerDeleteOperator) rather than wrapping logic in PythonOperators with hooks. This is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be in production code.",0,"The instruction asks for a workflow that: (1) retrieves Campaign Manager metrics, (2) generates a standard performance report across campaigns, and (3) demonstrates batch conversion tracking and reporting capabilities. The provided code performs CRUD operations (batch insert, get, update, list, delete) on Campaign Manager resources but does NOT retrieve metrics, generate performance reports, or demonstrate conversion tracking/reporting. The code appears to be a generic example of Campaign Manager API operations rather than a metrics and reporting workflow as requested.",0.3333333333333333
687,Fine-tuned (Qwen 2.5 1.5B Airflow),4,1,"The code uses the native GCSToTrinoOperator from airflow.providers.trino.transfers.gcs_to_trino, which is the idiomatic Airflow approach for transferring data from GCS to Trino. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator designed for this specific task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)' at the bottom. These are internal testing utilities that represent hallucination/leakage from test examples into production code.",0,"The instruction requests a workflow for transferring CSV data from cloud object storage to a columnar analytics database. While the code does use a transfer operator, there are several issues: (1) The GCS_BUCKET variable is incorrectly set to 's3a://bucket-name/' which is an S3 path, not a GCS bucket name, showing confusion between cloud providers. (2) The instruction mentions CSV data specifically, but there's no configuration for CSV parsing or schema handling. (3) The code includes TODO comments ('TODO: Update this example') indicating it's incomplete. (4) Trino is used as the target, which is appropriate for a columnar analytics database, but the implementation lacks production-ready configuration for actual data migration.",0.3333333333333333
688,Fine-tuned (Qwen 2.5 1.5B Airflow),7,1,"The code correctly uses the VerticaOperator from airflow.providers.vertica.operators.vertica, which is the idiomatic way to interact with Vertica databases in Airflow. It does not wrap database operations in PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,The code contains problematic elements that suggest test harness boilerplate. The load_connections() function imports from 'examples.connections' and uses 'import_loader' which appears to be internal testing infrastructure. The conn_id parameter is also incorrectly set to the result of connect_info (which would be connection data) rather than a string connection ID. This is not production-ready code.,0,"The instruction explicitly requests a workflow that demonstrates: (1) creating an employee records table, (2) populating it with data, (3) querying it, and (4) cleaning up (dropping the table). The provided code only performs SELECT queries on an already-existing 'employees' table. It does not include CREATE TABLE, INSERT/COPY operations, or DROP TABLE statements. The workflow is incomplete and missing critical steps for a data management demonstration.",0.3333333333333333
689,Fine-tuned (Qwen 2.5 1.5B Airflow),8,1,"The code uses the native ADLSListOperator from airflow.providers.microsoft.azure.operators.adls, which is the idiomatic Airflow approach for listing files in Azure Data Lake Storage. It does not wrap functionality in a PythonOperator with hooks.",0,The code explicitly imports testing utilities ('from tests_common.test_utils.system_tests import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). This is not production-ready code and represents test infrastructure leakage.,0,"The instruction specifically requests: (1) listing Parquet-formatted files in a specific directory, (2) file metadata extraction for data validation and tracking. The code uses a generic '/path' placeholder instead of a specific directory, includes an unnecessary ADLSToSFTPTempFileOperator that doesn't serve the stated purpose, and does not filter for Parquet files or extract/process file metadata. The DAG lacks any validation or tracking logic as requested.",0.3333333333333333
690,Fine-tuned (Qwen 2.5 1.5B Airflow),9,1,"The code uses the native FTPToSFTPServerOperator from airflow.providers.ftp.transfers.ftp_to_sftp, which is the idiomatic Airflow approach for transferring files between FTP and SFTP servers. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operator designed for this task.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it contains a load_env_variables() function that uses 'from airflow.utils.env import SystemEnvironmentVariable' and references test-related paths with comments like 'This should match the path to the folder holding this test case' and 'Absolute test folder containing tmp file'. The function manipulates environment variables for testing purposes using os.path.dirname(os.path.realpath(__file__)). While this function is defined but not called, its presence indicates this is example/test code rather than clean production-ready code.",0,"The instruction asks for transferring files 'from a cloud storage bucket to an SFTP destination', but the code transfers files from FTP to SFTP instead. Cloud storage buckets typically refer to services like AWS S3, Google Cloud Storage, or Azure Blob Storage, not FTP servers. The correct operators would be S3ToSFTPOperator, GCSToSFTPOperator, or similar cloud-to-SFTP operators. While the code does demonstrate multiple transfer strategies (copy individual files, move files, transfer directories) as requested, it fundamentally misses the core requirement of transferring FROM cloud storage.",0.3333333333333333
691,Fine-tuned (Qwen 2.5 1.5B Airflow),11,0,"The code uses a @task decorator with a custom Python function 'create_eks_cluster' that manually instantiates ClusterHook, which is not idiomatic. Airflow 2.8.4 provides EksCreateClusterOperator from airflow.providers.amazon.aws.operators.eks that should be used instead. The pattern of wrapping hook logic in a PythonOperator/@task is exactly what should be avoided when native operators exist.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is clearly from internal testing modules. It also uses 'SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN).build()' which is test harness boilerplate, not production code. Additionally, the code references 'test_context = sys_test_context_task()' and 'env_id = test_context[ENV_ID_KEY]', further confirming this is test infrastructure leakage.",0,"The instruction asks for 'deploying and managing an EMR job on an EKS cluster' - this is EMR on EKS, not traditional EMR. However, the code uses EmrCreateJobFlowOperator which creates a traditional EMR cluster (EC2-based), not EMR on EKS. For EMR on EKS, it should use EmrContainerOperator or EmrEksCreateClusterOperator. The code also appears incomplete (cuts off mid-definition in create_eks_cluster function), and doesn't demonstrate a complete pipeline with proper IAM role configuration and a sample Python computation job as requested.",0.0
692,Fine-tuned (Qwen 2.5 1.5B Airflow),12,0,"The code imports SingularityOperator but never uses it. Instead, it only defines a simple PythonOperator via the @task decorator that prints 'Hello World'. The instruction asks for bash commands to represent different phases, which should use BashOperator, not a Python function. This is not idiomatic for the requested use case.",0,"The code explicitly imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities and test harness boilerplate. This is a clear hallucination/leakage of test code into what should be production DAG code.",0,"The instruction explicitly requests: (1) a data processing workflow with initialization and cleanup stages, (2) using bash commands for different phases (preparation, processing, finalization), and (3) setup/teardown tasks around main processing logic. The provided code does none of this - it only contains a single Python task that prints 'Hello World' with no initialization, cleanup, bash commands, or multi-phase workflow structure.",0.0
693,Fine-tuned (Qwen 2.5 1.5B Airflow),16,0,"The code does not use any Airflow operators, tasks, or DAG structure. It imports BashOperator but never uses it. The code is just a plain Python script with a for loop, not an Airflow DAG with tasks and dependencies. There are no Kafka providers/operators used despite the instruction requiring Kafka-based streaming.",0,"While the code doesn't import test harness modules, it contains a syntax error ('for loop = range(1,11):' should be 'for loop in range(1,11):') and is not valid Python/Airflow code. Additionally, it's not actually a DAG - there's no DAG object instantiation, no tasks defined, making it fundamentally broken as Airflow code.",0,"The instruction explicitly asks for a 'Kafka-based FizzBuzz game that demonstrates message streaming and event processing' with a system that 'generates numbers, checks for divisibility by 3 and 5, and triggers specific actions.' The provided code: (1) has no Kafka integration whatsoever, (2) has no message streaming or event processing, (3) is not an Airflow DAG with tasks/operators, (4) just prints to console instead of triggering actions, and (5) has incorrect FizzBuzz logic (missing the case for numbers divisible by both 3 and 5, which should print 'FizzBuzz').",0.0
694,Fine-tuned (Qwen 2.5 1.5B Airflow),17,1,"The code uses appropriate Airflow operators and patterns. It uses BashOperator from airflow.providers.standard.operators.bash and the @task decorator for TaskFlow API, which are both idiomatic Airflow approaches. It does not wrap logic unnecessarily in PythonOperator when better alternatives exist.",0,"The code has a critical error in the schedule parameter: 'pendulum.timeperiod(datetime(2020, 1, 1), datetime(2020, 1, 2))' is not valid. Pendulum does not have a 'timeperiod' function, and 'datetime' is used without being imported. This appears to be hallucinated/incorrect code that would fail at runtime. The correct approach would be to use a cron expression, timedelta, or None for the schedule parameter.",0,"The instruction explicitly asks for a DAG that 'demonstrates setup and teardown tasks' with 'preparatory and cleanup operations' and 'clear initialization and finalization steps'. The provided code does not use any setup or teardown task patterns (such as .as_setup() or .as_teardown() methods available in Airflow 2.7+/3.0+). It only shows basic task definitions without any setup/teardown semantics, failing to demonstrate the core requirement of the instruction.",0.3333333333333333
695,Fine-tuned (Qwen 2.5 1.5B Airflow),19,0,"The DAG uses a PythonOperator (via @task decorator) with ComputeEngineHook to create a Compute Engine instance instead of using the native ComputeEngineInsertInstanceOperator or ComputeEngineInsertInstanceFromTemplateOperator that are already imported but never used. The create_instance task wraps hook logic in a Python function, which is the anti-pattern described in the scoring criteria. The proper idiomatic approach would be to use ComputeEngineInsertInstanceFromTemplateOperator directly.",0,"The code has multiple critical issues: (1) PostgresToGoogleSheetsOperator is used incorrectly - this operator is designed to transfer PostgreSQL query results to Google Sheets, not to execute SQL DDL/DML commands or configure Google Sheets properties. The parameters like 'google_sheets_config' with 'target_sheets_property' don't exist in the actual operator. (2) The code has syntax errors including unclosed parentheses in the insert_data task. (3) The POSTGRES_TEMPLATE has duplicate 'disks' keys and invalid 'source_image_uri' values. (4) The zone parameter format 'LOCATION + "":"" + REGION' is incorrect for GCP APIs. This appears to be hallucinated/fabricated API usage that doesn't match actual Airflow provider implementations.",0,"The instruction requires: (1) creating a Compute Engine instance, (2) configuring a PostgreSQL database, and (3) transferring data to Google Sheets. The DAG attempts to create a Compute Engine instance but does so incorrectly. It completely fails to configure a PostgreSQL database - there's no database creation, schema setup, or initial data loading. The PostgresToGoogleSheetsOperator usage is fundamentally wrong and wouldn't actually transfer data from PostgreSQL to Sheets. The DAG is missing critical steps like waiting for the instance to be ready, installing/configuring PostgreSQL on the instance, creating database schemas, and properly extracting and loading data to Sheets. The multi-step workflow is incomplete and non-functional.",0.0
696,Fine-tuned (Qwen 2.5 1.5B Airflow),20,1,"The DAG uses native Airflow providers and operators from airflow.providers.google.cloud.operators.compute, including ComputeEngineInsertInstanceOperator, ComputeEngineCopyInstanceOperator, ComputeEngineInsertInstanceGroupManagerOperator, and ComputeEngineInstanceGroupManagerFromCopyOperator. It does not wrap logic in PythonOperator + Hook patterns, which is the correct idiomatic approach for Airflow.",0,"The code includes a test harness import at the bottom: 'from tests_common.test_utils.system_tests import get_test_run'. This is internal testing boilerplate that should not be present in production DAG code. Additionally, there are several syntax errors and typos in the code (e.g., '.INSTANCE' instead of 'INSTANCE', 'IMAGRANK' instead of likely 'INSTANCE_TEMPLATE', duplicate 'body=' parameters, inconsistent indentation), suggesting code quality issues.",0,"The instruction asks for a workflow to manage instance templates and instance group managers, specifically to 'create, copy, and update instance templates'. However, the code creates and copies instances (not instance templates), and the instance group manager operations are incomplete and contain errors (duplicate 'body' parameters, incorrect body structure, mixing region/zone parameters). The DAG does not properly demonstrate instance template management as requested, and the logic flow doesn't align with typical instance template and IGM workflows.",0.3333333333333333
697,Fine-tuned (Qwen 2.5 1.5B Airflow),21,1,"The code uses native Airflow GCS operators (GCSCreateBucketOperator, GCSToGCSOperator, GCSDeleteBucketOperator, GCSSynchronizeBucketsOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test setup/teardown comments and a watcher() call. The line 'test_run = get_test_run(dag)' is also test-specific code that has no place in a production DAG.",0,"The instruction requested a workflow demonstrating 'GCS transfer job creation, file upload, and bucket management'. While the code demonstrates bucket management (create/delete) and file transfer (GCSToGCSOperator), it does NOT demonstrate file upload to the source bucket. The DAG attempts to copy 'folder/file.txt' from the source bucket, but never uploads any files to that bucket first, meaning the copy operation would fail. A complete solution should include a task to upload sample files to the source bucket before attempting the transfer.",0.3333333333333333
698,Fine-tuned (Qwen 2.5 1.5B Airflow),22,1,"The code uses native Airflow operators (EmptyOperator, BashOperator) and the TaskGroup utility for organizing tasks. It does not wrap logic in PythonOperator unnecessarily and follows idiomatic Airflow patterns for task grouping and dependencies.",1,"The code uses only standard Airflow libraries (airflow.models.dag, airflow.operators.bash, airflow.operators.empty, airflow.utils.task_group) and pendulum for datetime handling. There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",0,"The instruction asks for a multi-stage workflow demonstrating task grouping with nested task groups and sequential processing. While the code does show task grouping with TaskGroup, it does NOT demonstrate nested task groups (TaskGroups within TaskGroups). The workflow is relatively simple with only one TaskGroup level. Additionally, there's a logical error: 'include_tasks' is defined as a list of operators outside the TaskGroup, but then a TaskGroup with the same name is created, and the dependency 'run_this_first >> include_tasks' references the list, not the TaskGroup, which would cause confusion or errors. The code fails to fully demonstrate the complexity and nested structure requested in the instruction.",0.6666666666666666
699,Fine-tuned (Qwen 2.5 1.5B Airflow),23,0,"The code uses TaskFlow API (@task decorators) with hooks inside Python functions instead of using native Airflow operators where available. For example, 'create_dataset' and 'create_ingestion' wrap QuickSightHook calls in @task functions rather than using dedicated operators. While QuickSightCreateIngestionOperator is imported, it's not actually used. The pattern of wrapping hook logic in PythonOperator-style tasks is not idiomatic when native operators exist.",0,"The code imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which appears to be an internal testing module (system.amazon.aws.utils). It also uses 'sys_test_context_task = SystemTestContextBuilder()' which is test harness boilerplate. Additionally, there are syntax errors like 'quick SightHook()' (space in name), undefined 'self.role_arn', malformed string escapes in 'response[\""DatasetId\""]', and nonsensical subprocess commands ('pg_dump', 'aws s3 put-data-file' which doesn't exist). The code is not production-ready.",0,"The instruction asks to transfer sample project data from S3 to QuickSight and demonstrate programmatic setup of data source, dataset, and ingestion. While the code attempts to create a dataset and ingestion, it has critical issues: (1) The data source creation is missing entirely - only dataset creation is attempted, (2) The sample data writing logic uses nonsensical commands (pg_dump, non-existent AWS CLI commands) that won't actually write JSON data to S3, (3) The code is incomplete (cuts off mid-line), and (4) Multiple syntax and logic errors prevent it from functioning. The core business logic of transferring data from S3 to QuickSight is not properly implemented.",0.0
700,Fine-tuned (Qwen 2.5 1.5B Airflow),24,0,"The code imports multiple non-existent or incorrectly named operators such as 'CloudDLP infiltration_test_operator', 'CloudDLPinspect_job_operator', 'CloudDLP_list_infiltration_tests_operator', 'CloudDLP_list_jobs_operator', and 'CloudDLP_update_job_operator' with inconsistent naming conventions (spaces, underscores). It also uses undefined functions like 'root_logger_name()', 'print_logger_output()', 'wait_for_task_status()', 'get_last_record()', and 'print_json()' which are not standard Airflow operators or sensors. While it does use some legitimate operators (CloudDLPCreateJobOperator, CloudDLPDeleteJobOperator), the overall approach is not idiomatic as it mixes hallucinated operators with real ones and uses non-existent helper functions instead of proper Airflow patterns.",0,"The code contains multiple hallucinations: (1) It imports 'from tests_common.test_utils.system_tests import get_test_run' which is internal testing infrastructure, not production code. (2) It uses 'test_run = get_test_run(dag)' which is test harness boilerplate. (3) It invents non-existent operators and functions like 'infiltration_test_operator', 'CloudDLPinspect_job_operator', 'root_logger_name()', 'print_logger_output()', 'wait_for_task_status()', 'get_last_record()', and 'print_json()'. (4) It references nonsensical configuration keys like 'inflationConfiguration', 'dataBreachIncident', and 'infiltration' which are not part of the Google Cloud DLP API. (5) The operator 'CloudDLPUpdateJobOperator' does not exist in the standard Airflow Google provider package.",0,"The instruction asks for a DLP workflow demonstrating the complete lifecycle from creation to deletion for sensitive data inspection tasks. While the code attempts to show creation (CloudDLPCreateJobOperator) and deletion (CloudDLPDeleteJobOperator), it fails to properly implement the core DLP inspection functionality. The code uses fictional concepts like 'infiltration', 'inflation', and 'insulation' instead of actual DLP inspection jobs. A proper implementation should create a DLP inspection job with valid inspect configuration (info types, storage config), potentially get/list the job status, and then delete it. The current code is too broken with hallucinated operators and nonsensical configurations to fulfill the business requirement of demonstrating a real DLP inspection workflow.",0.0
701,Fine-tuned (Qwen 2.5 1.5B Airflow),25,1,"The code uses native Airflow SageMaker operators and sensors from airflow.providers.amazon.aws.operators.sagemaker and airflow.providers.amazon.aws.sensors.sagemaker. It imports SageMakerTrainingOperator, SageMakerModelOperator, SageMakerProcessingOperator, and related sensors, which are the idiomatic way to interact with SageMaker in Airflow. While there are some @task decorators for cleanup operations (which is acceptable for auxiliary tasks), the core ML workflow operations use proper operators rather than wrapping hooks in PythonOperators.",0,"The code contains clear hallucinations and test harness boilerplate. Specifically, it imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs' which appears to be internal testing infrastructure. The line 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' is test harness code. Additionally, there are suspicious tokens like 'RANDOMIZATION_START_TOKEN = ""$RGXbf786�3b9""' and 'RANDOMIZATION_END_TOKEN = ""$3c9b9e7rXfbGF""' that appear to be test artifacts. The code also references undefined variables like 'bucket_name' in the experiments_config list comprehension without proper initialization.",0,"The code is incomplete (cuts off mid-function in clean_endpoint) and does not demonstrate a complete, working end-to-end ML lifecycle workflow. While it imports relevant operators for training, model creation, and endpoints, there is no actual DAG definition showing the orchestration of these components. The instruction requested provisioning AWS resources, configuring training jobs, creating model artifacts, and setting up inference endpoints with robust error handling - but the code only shows imports, helper functions, and incomplete cleanup tasks. There's no visible DAG instantiation or task chaining that would demonstrate the complete workflow requested.",0.3333333333333333
702,Fine-tuned (Qwen 2.5 1.5B Airflow),26,1,"The code uses native Airflow operators appropriately: LocalFilesystemToS3Operator for file transfer, S3CreateBucketOperator for bucket creation, and S3DeleteBucketOperator for cleanup. These are the correct, idiomatic operators from the AWS provider package rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'watcher()' and 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is test/example code, not production-ready code.",0,"The instruction asks to 'generate a temporary file, transfer it to cloud storage, and then clean up the temporary resources.' The code transfers a file (/tmp/world-small.txt) but does NOT generate it - it assumes the file already exists. There is no task that creates the temporary file before the upload. While it does transfer to S3 and clean up the bucket, it fails to implement the file generation step that was explicitly requested.",0.3333333333333333
703,Fine-tuned (Qwen 2.5 1.5B Airflow),27,0,The DAG uses a TaskFlow API @task decorator with a Hook (GoogleMarkingPlatformHook) wrapped in a Python function instead of using native Campaign Manager 360 Operators. Airflow 3.0.1 with the Google Marketing Platform provider should have dedicated operators like GoogleCampaignManagerReportOperator or similar for retrieving campaign data and conversions. The code follows a 'Pythonic' pattern rather than using the idiomatic Operator-based approach.,0,The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test harness boilerplate that should not appear in production DAG code. This is a clear hallucination/leakage of test infrastructure into production code.,0,"The instruction requires: (1) retrieve digital marketing performance metrics, (2) generate reports, and (3) record conversion activities. The provided DAG only retrieves a single conversion ID using a basic API call. It does not generate any reports, does not retrieve comprehensive campaign performance metrics, does not log/record conversion events, and lacks the 'dynamically pull campaign data' functionality requested. Additionally, there's a syntax error ('api_call converting' is undefined) that would prevent execution. The DAG fails to implement the core business logic requested.",0.0
704,Fine-tuned (Qwen 2.5 1.5B Airflow),28,1,"The code uses the PigOperator, which is the appropriate Airflow operator for executing Pig scripts. This is idiomatic Airflow as it leverages the specific operator designed for Pig tasks rather than wrapping logic in a PythonOperator with hooks.",0,The code explicitly imports testing utilities ('from tests.system.utils import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). These are internal testing modules that should not appear in production-ready DAG code.,0,"The instruction asks for a Pig script execution that lists directory contents. However, the pig parameter contains 'pig -x local2 ls /tmp', which is a shell command syntax rather than a valid Pig Latin script. The correct approach would be to either: (1) use a Pig Latin script with proper syntax (e.g., 'fs -ls /tmp' in Pig), or (2) use BashOperator for shell commands. Additionally, 'local2' is not a valid Pig execution mode (should be 'local' or 'mapreduce'). The DAG_ID 'example_adf_run_pipeline' also suggests Azure Data Factory rather than Pig/Hadoop, indicating potential confusion in the implementation.",0.3333333333333333
705,Fine-tuned (Qwen 2.5 1.5B Airflow),29,1,The code uses native Airflow operators appropriately. It uses SqlExecuteQueryOperator from airflow.providers.common.sql.operators.sql for SQL operations and BashOperator for bash commands. The TaskFlow API is used correctly with the @task decorator for the connection creation function. No generic PythonOperator wrapping is used where specific operators would be more appropriate.,0,"The code explicitly imports testing utilities at the end: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production DAG code, representing clear hallucination/leakage from test examples.",0,"The instruction asks for a multi-stage system monitoring and logging workflow that prints context information, logs SQL queries, generates sleep-based parallel tasks, and executes environment-specific Python functions with task chaining and runtime flexibility. The provided code only demonstrates SQL query execution against a SQL Server database. It is missing: (1) context information printing tasks, (2) sleep-based parallel tasks, (3) environment-specific Python functions, (4) meaningful task chaining/dependency management beyond simple sequential execution, and (5) runtime flexibility demonstrations. The code appears to be a basic SQL operator example rather than the requested monitoring/logging workflow.",0.3333333333333333
706,Fine-tuned (Qwen 2.5 1.5B Airflow),30,0,"The DAG violates idiomatic Airflow patterns by mixing approaches. It uses a @task decorator with DataJoinHook (create_datajoin_job()) instead of consistently using native operators. This function wraps hook logic in a Python callable, which is then called directly in the task dependency chain. The proper approach would be to use only the CloudDataJoinCreateJobOperator consistently. Additionally, there's a reference to 'ConnectionDeleteOperator' which is not imported and appears to be non-standard.",0,"The code contains significant hallucination and test harness leakage. It imports and uses testing modules that should not be in production code: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', and 'from tests_common.test_utils.system.utils import get_last_records_of_table'. There's also a syntax error ('list watchers([delete_connection])' should be 'list_watchers'). The bottom section explicitly states '### Everything below this line is not part of test ###' but includes test boilerplate. Additionally, 'ConnectionDeleteOperator' is used but never imported, and the DataJoinHook/operators appear to be non-standard or fabricated Airflow components.",0,"The instruction asks for a 'Google Calendar event synchronization' pipeline, but the DAG implements a DataJoin to Google Sheets transfer workflow instead. There is no Calendar API integration, no calendar event handling, and no calendar-related operators. While it does demonstrate setup/execution/teardown patterns with GCP operators, it completely misses the core requirement of Calendar event synchronization. The DAG appears to be repurposed from a different use case (GCS to Sheets via DataJoin) rather than built for the specified Calendar synchronization requirement.",0.0
707,Fine-tuned (Qwen 2.5 1.5B Airflow),31,0,"While the code imports proper Airflow operators (GCSCreateBucketOperator, CloudVisionCreateProductSetOperator, etc.), it never actually uses them to create tasks. Instead, it defines Python functions (_detect_image_with_gcp_sftp, _detect_document_from_gcp_sftp, _create_product_set, _create_image_url_product) that would need to be wrapped in PythonOperators. The code imports the correct operators but fails to use them idiomatically. A proper implementation would instantiate these operators directly as tasks rather than writing custom Python functions with service clients.",0,"The code contains multiple hallucinations and non-existent APIs: (1) References 'vision.VisionService()' which is not a standard Airflow or Google Cloud library class, (2) Uses 'sftpUri' in Vision API requests which is not a valid Vision API parameter, (3) References 'service_dlp_detectDocument' which appears to be a made-up variable, (4) Contains malformed Unicode characters ('s撮ureUri' instead of 'sourceUri'), (5) Uses non-existent Vision API methods and response structures. The code also references undefined variables like 'gcs_to_gcs_output_path', 'bucket', 'key', and 'service_dlp_detectDocument' without defining them.",0,"The instruction asks for a complete DAG that orchestrates an image processing workflow through interconnected tasks from bucket creation to image analysis. However, the provided code: (1) Never instantiates a DAG object or defines any actual tasks, (2) Only imports operators but never uses them, (3) Defines helper functions but doesn't create a workflow, (4) Has no task dependencies or orchestration logic, (5) The code is incomplete (cuts off mid-function). There is no executable DAG that would run an end-to-end image processing workflow as requested.",0.0
708,Fine-tuned (Qwen 2.5 1.5B Airflow),32,1,"The code uses the specific GCSToTrinoOperator from airflow.providers.trino.transfers.gcs_to_trino, which is the idiomatic Airflow approach for transferring data from GCS to Trino. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this task.",0,"The code contains clear hallucinations and test harness boilerplate. Specifically: (1) It imports 'from tests_common.test_utils.system_tests import get_test_run' which is an internal testing module, (2) It includes 'test_run = get_test_run(dag)' which is test harness code, (3) It references an undefined 'TrinoConn()' class that doesn't exist in standard Airflow libraries, (4) It includes an irrelevant 'get_s3_bucket_url' function with AWS S3 documentation references when the task is about GCS, and (5) The function 'example_gcs_to_trino()' is defined but doesn't return the operator or properly integrate it into the DAG context.",0,"The code does not fulfill the instruction properly. While it attempts to use GCSToTrinoOperator, the implementation is broken: (1) The operator is created inside a function that doesn't return it or properly add it to the DAG, (2) The 'TrinoConn()' class is undefined and should use a string connection ID instead, (3) The task is never actually added to the DAG context properly - calling 'example_gcs_to_trino()' inside the DAG context doesn't register the operator because it's not returned or yielded. The instruction requires a working automated pipeline, but this code would fail to execute.",0.3333333333333333
709,Fine-tuned (Qwen 2.5 1.5B Airflow),33,1,"The code uses the specific Alibaba Cloud provider operator 'AAzureDBSparkSQLOperator' designed for AnalyticDB Spark SQL operations, rather than wrapping logic in a PythonOperator with hooks. This is the idiomatic Airflow approach for interacting with external systems.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready DAG code.,0,"The instruction asks for 'systematic database introspection' with 'sequential SQL queries' to 'discover and catalog data resources' providing 'visibility into available data schemas and table structures'. However, the DAG has critical issues: (1) No task dependencies are defined, so queries won't execute sequentially as required; (2) The 'delete_table' task contradicts the diagnostic/introspection purpose - it's a destructive operation that shouldn't be in a read-only diagnostic workflow; (3) Some tasks query specific hardcoded tables ('test_db', 'test_table') rather than discovering what exists; (4) The logic doesn't build a comprehensive catalog - it runs disconnected queries without aggregating results.",0.3333333333333333
710,Fine-tuned (Qwen 2.5 1.5B Airflow),35,1,"The code uses the specific CohereEmbeddingOperator from airflow.providers.cohere.operators.embedding, which is the idiomatic way to interact with Cohere AI services in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the native operator designed for this task.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction explicitly asks to 'process multiple text inputs and generate their corresponding embeddings'. However, the code only processes a single text sample (SAMPLE_TEXT) with one operator instance. It does not demonstrate processing multiple text inputs as requested. A proper implementation would include multiple CohereEmbeddingOperator tasks or use dynamic task mapping to handle multiple text inputs.",0.3333333333333333
711,Fine-tuned (Qwen 2.5 1.5B Airflow),36,1,"The DAG uses native Airflow operators designed for GCS operations: GCSCreateBucketOperator, GCSDeleteBucketOperator, GCSSynchronizeBucketsOperator, GCSToGCSOperator, and S3ToGCSOperator from the Google Cloud provider package. It does not wrap hooks in PythonOperators but instead uses the appropriate operators for each task.",0,The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a hallmark of leaked test code.,0,"While the DAG demonstrates file transfer operations (GCSToGCSOperator, S3ToGCSOperator) and bucket management (create/delete buckets), it has several issues: 1) The create_temp_gcs_bucket() task function has incorrect logic - it tries to use TriggerRule as a callable/task which is not valid Airflow syntax. 2) The S3ToGCSOperator is used with BUCKET_S3 = 's3://airflow-system-tests-resources/gcs' which appears to be a test resource, not a proper demonstration. 3) The 'extra_files' parameter used in S3ToGCSOperator doesn't exist in the actual operator signature. 4) The overall structure is more of a test setup/teardown pattern rather than a clean demonstration of synchronization strategies as requested.",0.3333333333333333
712,Fine-tuned (Qwen 2.5 1.5B Airflow),37,0,"The code uses multiple non-existent or hallucinated operators. 'ExcelToBlobOperator' does not exist in airflow.providers.google.cloud.transfers, and 'DataJoinOperator' does not exist in airflow.providers.google.cloud.operators.datajoin. These appear to be fabricated operators rather than actual Airflow providers. While the code attempts to use specific operators (which would be idiomatic), they are not real Airflow operators, making this non-idiomatic by default.",0,"The code contains multiple hallucinations: (1) It imports 'from tests.system.utils import get_test_run' which is internal testing infrastructure, (2) It uses 'test_run = get_test_run(dag)' which is test harness boilerplate, (3) It imports non-existent operators like 'ExcelToBlobOperator' and 'DataJoinOperator' that don't exist in the Airflow provider packages, (4) Uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables typical of Airflow's internal test suite.",0,"The instruction asks for a DAG demonstrating 'data extract refresh processes for a business intelligence workbook' with 'trigger and monitor data updates across different execution modes' and 'explicit job status tracking and different refresh strategies'. The provided code is about copying Excel files to blob storage, not about BI workbook refresh processes. It doesn't demonstrate monitoring refresh jobs, different execution modes, or status tracking. The code appears to be a generic file transfer example rather than a BI data refresh workflow with the requested features.",0.0
713,Fine-tuned (Qwen 2.5 1.5B Airflow),38,0,"The code uses LatestOnlySensor instead of the correct LatestOnlyOperator as requested in the instruction. LatestOnlySensor is a sensor that waits for a specific time condition, while LatestOnlyOperator is designed to skip downstream tasks if the DAG run is not the latest scheduled run. The instruction specifically asks for 'LatestOnlyOperator to control task execution and prevent redundant processing of older scheduled tasks', which is the core use case for LatestOnlyOperator, not LatestOnlySensor.",1,"The code uses only standard Airflow libraries and providers (airflow.providers.standard.*). There are no imports from internal testing modules or test harness boilerplate. The code is clean from a hallucination perspective, even though it uses the wrong operator for the task.",0,"The instruction explicitly asks to 'use LatestOnlyOperator to control task execution and prevent redundant processing of older scheduled tasks'. However, the code uses LatestOnlySensor instead, which has completely different behavior. LatestOnlyOperator skips downstream tasks when the DAG run is not the latest scheduled run (preventing backfill redundancy), while LatestOnlySensor waits until a target time is reached. Additionally, the DAG has 'catchup=False', which contradicts the purpose of demonstrating LatestOnlyOperator (which is most useful when catchup=True to show how it skips tasks in older runs). The code also includes unused functions (my_task, my_task_for_sensor) and a confusing 'drop()' method call that doesn't serve the demonstration purpose.",0.3333333333333333
714,Fine-tuned (Qwen 2.5 1.5B Airflow),39,0,"The code heavily misuses Airflow patterns. It wraps operators inside @task decorators (e.g., GCSCreateBucketOperator inside create_bucket() task), which is incorrect - operators should be instantiated directly in the DAG context, not inside task functions. The move_file_to_gcs() function incorrectly instantiates GCSCreateBucketOperator and calls non-existent methods like expand_bucket_ref() and scale_view(). The code also uses Hook methods directly in the DAG body (S3Hook().copy_file()) instead of using proper operators like S3ToGCSOperator. This demonstrates a fundamental misunderstanding of Airflow's operator vs. task decorator patterns.",0,"The code explicitly imports and uses test harness code at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. Additionally, it invents non-existent methods like expand_bucket_ref() and scale_view() on GCSCreateBucketOperator. The code also references undefined variables like 'boto3' in remove_connection() without importing it. There's also a syntax error with 'from airflow.providers.google.cloud.hooks.gcs GCSHook' (missing 'import').",0,"The instruction asks for Azure to Google Cloud transfer, but the code implements AWS S3 to GCS transfer instead (uses S3Hook, references S3 buckets and AWS connections). The code also has multiple structural issues: duplicate DAG definitions, syntax errors ('with DAG(...) is not None' without a body), and the workflow logic is broken. While it attempts to create a bucket, transfer files, and delete the bucket, the implementation is fundamentally flawed and doesn't demonstrate a working cross-cloud transfer between the requested platforms (Azure and GCP).",0.0
715,Fine-tuned (Qwen 2.5 1.5B Airflow),40,0,"While the code does use some native Airflow operators like QuickSightCreateIngestionOperator and QuickSightSensor, it heavily relies on non-idiomatic patterns. The create_connection task uses raw boto3 calls with fabricated DynamoDB operations instead of using proper Airflow connection management or AWS operators. The cleanup tasks (delete_sensor, delete_operator, delete_ingestion) wrap boto3 client calls in PythonOperators (@task decorator) instead of using native QuickSight operators. The code also invents non-existent boto3 APIs like 'boto3.session.DDBConnection()' and 'client.delete_sensor()' which don't exist in AWS SDK.",0,"The code contains multiple hallucinations and test harness leakage. It imports 'from tests.system.providers.amazon.aws.utils import SystemAmazonAirflowTestBase' which is an internal testing module. It uses a decorator '@TestAmazonAirflowSetup' that doesn't exist. It references a class 'TestQuicksightSystemSettings(SystemAmazonAirflowTestBase)' which is test boilerplate. The code invents non-existent boto3 APIs like 'boto3.session.DDBConnection()', 'create_table()' with invalid parameters, 'delete_sensor()', and 'get_waiter(""resource_not_exists"")'. It also references undefined variables like 'ENV_ID', '@dag' decorator without proper import, and 'QuickSightDeleteOperator' which doesn't exist in Airflow providers. The code is incomplete and cuts off mid-statement.",0,"The code fails to fulfill the user instruction in multiple ways. The instruction asks to transfer sample project data from S3 to AWS QuickSight, but the code doesn't demonstrate actual data transfer from S3 - instead it fabricates a DynamoDB connection. It doesn't show how to programmatically set up a QuickSight data source properly (the create_connection task is nonsensical). The code is incomplete (cuts off mid-line), wrapped in test harness boilerplate instead of being a standalone DAG, and doesn't demonstrate a working end-to-end pipeline. Key steps like creating a QuickSight data source, creating a dataset with proper S3 configuration, and setting up visualization are either missing or incorrectly implemented.",0.0
716,Fine-tuned (Qwen 2.5 1.5B Airflow),41,0,"This code does not use any Airflow operators, tasks, or DAG structure whatsoever. It only contains a helper function for creating a report request using a Google Cloud client. There are no imports from airflow.operators, airflow.providers, no DAG definition, and no tasks. For Google Campaign Manager (formerly DoubleClick Campaign Manager), Airflow provides operators in airflow.providers.google.marketing_platform.operators.campaign_manager that should be used instead of raw API client code wrapped in functions.",0,"The code contains a critical error: 'from google.cloud.marketing_platform_v孺1' includes a non-ASCII character '孺' in the version string, which appears to be a corruption or hallucination. The correct import should be something like 'google.cloud.marketing_platform_admin_v1alpha' or use the appropriate Airflow provider. Additionally, the comment tags '[START howto_campaign_manager_async_api_client_body]' suggest this may be copied from documentation or test examples rather than being production-ready code. The CampaignManagerAsyncServiceClient is also not a standard class in typical Google Cloud libraries for Campaign Manager.",0,"The instruction requires building an automated reporting workflow that: (1) creates reports, (2) runs reports, (3) processes/extracts campaign metrics, (4) transforms data, and (5) prepares it for BI analysis. The provided code only contains a single helper function to create a report request dictionary - it does not implement any workflow, DAG, tasks, data extraction, transformation, or processing pipeline. There is no orchestration, no task dependencies, and no actual execution logic for running or processing reports.",0.0
717,Fine-tuned (Qwen 2.5 1.5B Airflow),42,0,"The DAG uses ComputeEngineHook methods directly in the DAG body (start_instance, stop_instance) instead of using proper Airflow Operators. Lines like 'start_pdt_instance = ComputeEngineHook().start_instance(...)' are not idiomatic - these should be wrapped in appropriate Operators (e.g., ComputeEngineStartInstanceOperator) to properly integrate with Airflow's task execution model. Additionally, the 'clean_files' task uses a Hook directly in a PythonOperator pattern rather than using a native operator.",0,"The code contains hallucinated/invalid API calls that don't exist in the Airflow or Google Cloud libraries. Specifically, 'service.global_->instances().getPreconfiguredPdtOperation()' is not a valid method - the syntax 'global_->' is nonsensical. The code also references non-existent methods like 'operation_client.waiter()' and 'waiters.wait_for_operation_to_complete()'. Additionally, the code is incomplete (truncated at 'trigger_r'). Most critically, this appears to be about Google Compute Engine instances, not Looker PDT builds as requested.",0,"The instruction explicitly asks for a workflow demonstrating 'starting and checking Looker PDT builds using specialized Airflow operators'. However, the provided code is about Google Compute Engine instance management (creating, starting, stopping, deleting GCE instances), not Looker PDT (Persistent Derived Tables) builds. Looker PDT operations would require LookerOperator or similar from airflow.providers.google.cloud.operators.looker, not ComputeEngine operators. The DAG completely misses the core requirement of working with Looker PDT builds and asynchronous job monitoring for data transformation.",0.0
718,Fine-tuned (Qwen 2.5 1.5B Airflow),43,0,"The code uses operators that do not exist in the official Apache Airflow providers. The imports reference 'airflow.providers.microsoft.azure.operators.asb_queue.AzureServiceBusQueueCreateOperator', 'airflow.providers.microsoft.azure.operators.asb_topic.AzureServiceBusTopicCreateOperator', and corresponding sensors that are not part of the standard Azure provider package. The actual Azure provider for Airflow does not have these specific operators with these exact names and parameters. This appears to be fabricated/hallucinated operators rather than real Airflow components.",0,"The code contains clear hallucination issues: (1) It imports 'from tests_common.test_utils.system_tests import get_test_run' which is internal testing infrastructure, and (2) It uses 'test_run = get_test_run(dag)' which is test harness boilerplate that should not be in production code. Additionally, the operators and sensors used (AzureServiceBusQueueCreateOperator, AzureServiceBusTopicCreateOperator, AzureServiceBusQueueSensor, AzureServiceBusTopicSensor) do not exist in the actual Airflow Azure provider package, indicating fabricated/hallucinated components.",0,"While the code attempts to demonstrate Azure Service Bus operations including topic creation, message watching, queue operations, and cleanup, it fails to properly implement the requested functionality because: (1) The operators used are non-existent/hallucinated, so the code would not run, (2) The logic is confused - using 'AzureServiceBusQueueCreateOperator' to send messages to a topic is semantically incorrect, (3) Missing proper subscription management demonstration as requested, and (4) The parameters used (like 'location', 'administrative_state', 'security_quorum_connections') do not align with actual Azure Service Bus concepts. The code does not fulfill the instruction to create a working demonstration of enterprise messaging infrastructure.",0.0
719,Fine-tuned (Qwen 2.5 1.5B Airflow),44,0,"The code uses completely hallucinated operators that do not exist in Airflow 2.7.2. Operators like 'FirestoreTaskOperator', 'FirestoreToBigQueryOperator', and 'GCSToFirestoreOperator' are not real Airflow operators. The correct approach would be to use 'ExportDocumentsOperator' from airflow.providers.google.cloud.operators.firestore to export Firestore to GCS, then 'BigQueryCreateExternalTableOperator' or 'GCSToBigQueryOperator' to create the BigQuery table. This is not idiomatic Airflow as it doesn't use actual provider operators.",0,"The code contains multiple hallucinations: (1) It imports non-existent operators (FirestoreTaskOperator, FirestoreToBigQueryOperator, GCSToFirestoreOperator), (2) It includes test harness boilerplate with 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and (3) It uses 'test_run = get_test_run(dag)' which is testing infrastructure, not production code. This is clearly test/example code that has leaked into what should be production DAG code.",0,"The instruction requires: (1) Export a Firestore collection to GCS, (2) Create an external BigQuery table from the exported data, and (3) Demonstrate cross-project data transfer. The provided code does not follow this workflow correctly. It attempts to use a non-existent 'FirestoreToBigQueryOperator' that supposedly goes directly from Firestore to BigQuery, skipping the GCS intermediate step entirely. The correct workflow should be: Firestore → GCS (export) → BigQuery (create external table or load). The code also creates collections and documents as test setup rather than working with an existing collection as specified.",0.0
720,Fine-tuned (Qwen 2.5 1.5B Airflow),45,1,"The code uses the native PineconeIngestOperator from airflow.providers.pinecone.operators.pinecone, which is the idiomatic way to interact with Pinecone in Airflow. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator for the task.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready DAG code.,1,"The DAG fulfills the user instruction to design a vector data ingestion workflow for Pinecone. It demonstrates: (1) populating a Pinecone vector database with predefined embedding vectors, (2) flexible configuration via environment variables for index_name and namespace, (3) support for metadata (shown in the input tuples with metadata_1), and (4) custom namespace support. The DAG also shows batch_size configuration and xcom integration for additional flexibility.",0.6666666666666666
721,Fine-tuned (Qwen 2.5 1.5B Airflow),46,1,"The code correctly uses the WasbSensor from airflow.providers.microsoft.azure.sensors.wasb, which is the idiomatic Airflow approach for monitoring Azure Blob Storage. It does not wrap functionality in PythonOperator + Hook patterns, but instead uses the native operator designed for this purpose. Multiple sensor configurations are demonstrated (standard, with inlets, and deferrable mode), all using the proper provider-specific operator.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production-ready code. The ENV_ID and DAG_ID naming conventions also suggest this is example/test code rather than production code.",0,"The instruction requests a monitoring system that 'checks for specific file conditions before initiating downstream data processing tasks.' While the code implements sensors for file detection, it completely lacks any downstream data processing tasks. The DAG only chains sensors together without any actual processing logic (e.g., data transformation, loading, validation tasks). The instruction explicitly mentions 'initiating downstream data processing tasks,' which is not fulfilled. A complete solution would include operators that perform actual data processing after the sensor detects the file.",0.3333333333333333
722,Fine-tuned (Qwen 2.5 1.5B Airflow),48,0,"The code does not use appropriate DynamoDB-specific operators for the stated task. The instruction requires capturing and exporting DynamoDB table data to S3 with incremental export capabilities, but the code uses EMR operators (EmrStartJobFlowOperator, EmrAddStepsOperator) which are not idiomatic for DynamoDB-to-S3 export. Airflow provides DynamoDB-specific operators like DynamoDBToS3Operator or AWS Data Pipeline operators that would be more appropriate. Additionally, the code wraps custom logic in task decorators rather than using native operators designed for DynamoDB export operations.",0,"The code contains multiple hallucinations and test harness artifacts: (1) Imports from 'tests_common.test_utils.watcher' which is an internal testing module, (2) Uses non-existent boto3 methods like 'boto3.connect_to_service(""emrswf"")' and 'createMonitoringGroup', (3) Contains syntax errors and incomplete code (e.g., 'boto3.client(""emr-swf"").deleteStep StepId=event_log' is malformed), (4) References non-standard attributes like 'sys._MEIPASS', (5) Uses fabricated EMR-SWF service that doesn't exist in AWS, and (6) The code is incomplete and not production-ready with syntax errors throughout.",0,"The code completely fails to address the user instruction. The task requires: (1) DynamoDB table data capture and export to S3, (2) Time-based incremental export capabilities, (3) Point-in-time data recovery functionality. Instead, the code implements an EMR job flow with Hive/Presto steps that have no relation to DynamoDB export. There is no DynamoDB interaction, no incremental export logic, no point-in-time recovery mechanism, and no demonstration of DynamoDB-to-S3 data transfer techniques. The code appears to be a generic EMR example that doesn't fulfill any aspect of the specified business logic.",0.0
723,Fine-tuned (Qwen 2.5 1.5B Airflow),49,0,"The code uses 'DataJoinOperator' from 'airflow.providers.google.cloud.operators.datajoin', which does not exist in the official Airflow providers. This appears to be a hallucinated operator. For sensitive data detection configurations in GCP, the correct approach would be to use operators from 'airflow.providers.google.cloud.operators.dlp' (Data Loss Prevention), such as CreateDLPJobOperator, GetDLPJobOperator, and DeleteDLPJobOperator.",0,"The code contains multiple hallucinations: (1) It imports 'DataJoinOperator' from a non-existent module 'airflow.providers.google.cloud.operators.datajoin'. (2) It includes test harness boilerplate at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)', which are internal testing utilities that should not appear in production code. (3) The use of 'SFTPToGCSOperator' for creating a bucket is semantically incorrect - this operator transfers files from SFTP to GCS, not creates buckets.",0,"The instruction asks for an automated pipeline for managing sensitive data detection configurations, demonstrating how to programmatically handle data privacy templates (create, inspect, and clean up data protection configurations). The provided code does not address this requirement at all. Instead, it attempts to transfer SFTP files to GCS and uses a non-existent DataJoinOperator. The correct implementation should use Google Cloud DLP (Data Loss Prevention) operators to create inspect templates, storage configs, or DLP jobs for sensitive data detection, inspect/retrieve those configurations, and then clean them up. The current code completely misses the core business logic of data privacy/sensitive data detection.",0.0
724,Fine-tuned (Qwen 2.5 1.5B Airflow),50,0,"The code uses 'CloudGraphiteInsertQueryOperator' and 'GraphiteHook' from 'airflow.providers.google.cloud', which are completely incorrect for graph database operations. Graph databases (like Neo4j) require Cypher queries and specific graph database providers (e.g., Neo4jOperator from airflow.providers.neo4j). Graphite is a time-series monitoring tool, not a graph database. This demonstrates a fundamental misunderstanding of the technology stack required.",0,"The code contains multiple issues: (1) Imports test harness code with 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)', which is test boilerplate that should not be in production code. (2) Uses non-existent providers - 'airflow.providers.google.cloud.hooks.graphite' and 'airflow.providers.google.cloud.operators.graphite' do not exist in standard Airflow. (3) Contains syntax errors ('is:' instead of ':' and 'QUERYBODY' instead of 'QUERY_BODY').",0,"The instruction requires: (1) Graph database query for Tom Hanks node attributes, (2) Parameterized Cypher queries, (3) Date-based filtering. The generated code fails completely: it uses Graphite (monitoring tool) instead of a graph database, contains no Cypher queries, has no reference to Tom Hanks or any node attributes, uses a generic time-series query instead of graph traversal, and doesn't implement parameterization for flexible querying. The code is entirely unrelated to the requested graph database extraction workflow.",0.0
725,Fine-tuned (Qwen 2.5 1.5B Airflow),51,0,"While the code does use Airflow providers (ComprehendIngestDataOperator and ComprehendIngestDataSensor), these operators do not exist in the official Airflow AWS provider package. The actual AWS Comprehend provider does not have 'ComprehendIngestDataOperator' or 'ComprehendIngestDataSensor'. For PII detection and redaction with AWS Comprehend, the idiomatic approach would be to use ComprehendHook with appropriate operators or use the actual available operators like ComprehendStartPiiEntitiesDetectionJobOperator. The code appears to use fabricated/non-existent operators.",0,"The code contains multiple hallucinations and test harness boilerplate that should not be in production code: (1) Imports from 'tests_common.test_utils.watcher' and 'tests_common.test_utils.system_tests', (2) Uses 'watcher()' function in the DAG, (3) Includes 'get_test_run(dag)' and 'get_test_runer()' calls, (4) Has test-related comments like '# Needed to run the example DAG with pytest'. Additionally, the operators themselves (ComprehendIngestDataOperator, ComprehendIngestDataSensor) appear to be fabricated as they don't exist in the standard Airflow AWS provider library.",0,"The instruction requires a pipeline that: (1) takes raw data, (2) applies intelligent PII masking using AWS Comprehend, and (3) generates a redacted output file. This code fails to meet these requirements: (1) It uses non-existent operators that don't perform actual PII detection/redaction, (2) There is no actual data ingestion from a source, (3) There is no PII masking or anonymization logic, (4) There is no output file generation with redacted content. The code appears to be a skeleton example that doesn't implement the actual business logic of identifying and anonymizing PII in unstructured text data.",0.0
726,Fine-tuned (Qwen 2.5 1.5B Airflow),52,0,"The DAG fails to use the imported Azure Container Instances Operators idiomatically. Instead of using operators like ContainerInstancesCreateFromTemplateOperator, ContainerInstancesGetContainerOperator, ContainerInstancesUpdateContainerImageOperator, and ContainerInstancesDeleteContainerOperator that were imported, the code wraps everything in @task decorated Python functions that only contain 'pass' statements or return mock data. For example, 'create_container_instance_from_template()' just has 'pass', 'get_container()' returns a dictionary instead of using ContainerInstancesGetContainerOperator, and 'delete_container()' attempts to instantiate an operator inside a task function rather than using it directly in the DAG definition.",0,"The code contains several issues indicating it's incomplete or test-derived: (1) The DAG is incomplete - it cuts off mid-line with 'ContainerInstancesDel', (2) Uses test-like variable names (T1='t1smsmm', T2='t2smsmm') that are defined but never used, (3) References ENV_ID from environment variables suggesting test harness usage, (4) The DAG_ID is 'ci_test' which appears to be a test identifier, (5) Contains placeholder/mock implementations with 'pass' statements and hardcoded return values rather than actual operational code, (6) Missing import for 'json' module which is used in create_connection() function.",0,"The instruction requires a pipeline that showcases: (1) launching standard Docker images, (2) using custom container registries, and (3) configuring persistent volume attachments. The provided code fails on all counts: it contains only stub functions with 'pass' statements or mock returns, no actual container deployment occurs, there's no configuration for persistent volumes, and no demonstration of custom registry usage beyond placeholder variables. The code structure suggests an outline but provides no working implementation of the requested deployment patterns or systematic approach to container deployment scenarios.",0.0
727,Fine-tuned (Qwen 2.5 1.5B Airflow),54,1,"The code uses the SparkSubmitOperator from airflow.providers.apache.spark.operators.spark_submit, which is the idiomatic and correct way to submit Spark jobs in Airflow. It does not wrap Spark logic in a PythonOperator with hooks, but uses the native operator designed for this purpose.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run', creates a test_run object with 'test_run = get_test_run(dag)', and includes a main block that calls 'tests_common.test_run(test_run)'. These are internal testing utilities that indicate this is test/example code, not production-ready code.",0,"The instruction explicitly asks for a DAG that 'illustrates the sequential execution of different Spark operators' (plural), 'showing how complex data workflows can be orchestrated using modular task dependencies' and 'highlights task chaining and different Spark processing capabilities'. The provided code only contains a single SparkSubmitOperator (t1) with no task dependencies, no chaining, and no demonstration of multiple Spark operators or complex workflows. It fails to meet the core requirements of demonstrating sequential execution, task chaining, and multiple Spark processing capabilities.",0.3333333333333333
728,Fine-tuned (Qwen 2.5 1.5B Airflow),55,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator and S3DeleteBucketOperator from the AWS provider, and GCSToS3Operator for transferring files between cloud providers. It does not wrap functionality in PythonOperator + Hook patterns where native operators exist.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready DAG code.,0,"The instruction specifically requests 'Google Cloud Transfer Service and AWS S3 operators' for demonstrating cloud service integration and transfer job management. However, the code uses GCSToS3Operator (a direct transfer operator) instead of Google Cloud Transfer Service operators (like CloudDataTransferServiceCreateJobOperator). The instruction asks for 'transfer job management' which implies using the Transfer Service API to create and manage transfer jobs, not just a simple file copy. Additionally, the code includes an unrelated BashOperator that just prints 'hello world', which doesn't contribute to the requested cloud storage interaction demonstration.",0.3333333333333333
729,Fine-tuned (Qwen 2.5 1.5B Airflow),56,1,"The code uses native Airflow providers and operators appropriately. It imports and uses specific operators like DataflowCreateApplicationJobOperator, DataflowRunApplicationJobOperator, DataflowDeleteApplicationJobOperator, GCSCreateBucketOperator, and GCSDeleteBucketOperator from airflow.providers.google.cloud.operators. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear hallucinations and test harness artifacts. It includes references to non-existent components like 'AsyncGCSBakeInFlightOperator' and 'EasyTestHook' which are not standard Airflow libraries. There are comments like '### Everything below this line is not part of test ###' and 'EasyTestHook will bind the right values to these variables before running the test' that indicate test harness boilerplate. Additionally, the code has syntax errors (incomplete line at the end with 'location=l') and broken logic in the OBJECTS loop that references undefined variables and methods.",0,"The instruction specifically asks for a DAG demonstrating Google Cloud Composer operators with environment configuration, version listing, and operational state management. However, the code uses Dataflow operators instead of Cloud Composer operators (which would be ComposerEnvironmentOperator, ComposerListEnvironmentsOperator, etc.). Cloud Composer and Dataflow are different GCP services. The DAG does not demonstrate Composer environment provisioning, version listing, or state management as requested. Instead, it focuses on Dataflow application job management, which is not what was asked for.",0.3333333333333333
730,Fine-tuned (Qwen 2.5 1.5B Airflow),57,1,"The code uses native Airflow providers and operators specifically designed for Google Compute Engine tasks. It imports and uses ComputeEngineInsertInstanceOperator, ComputeEngineInsertInstanceFromTemplateOperator, ComputeEngineInsertInstanceIntoRegionOperator, ComputeEngineDeleteInstanceOperator, and ComputeEngineRunCommandOnInstanceOperator from airflow.providers.google.cloud.operators.compute. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains several issues indicating it may be from a test harness or example system: 1) Uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are typical of Airflow's internal testing framework, 2) Contains suspicious/malformed URLs like 'https://{NETWORK}.preprereq.cloudcompose.com' with non-standard domains, 3) Has typos and inconsistencies like 'PREPREREQ' instead of 'PREREQ', 4) The code is incomplete (cuts off mid-comment), and 5) Contains configuration that appears to be test boilerplate rather than production-ready code.",0,"The DAG is incomplete and does not fully demonstrate the requested lifecycle management. While it shows instance creation operators, the code cuts off before showing: 1) The actual execution of remote commands via SSH (ComputeEngineRunCommandOnInstanceOperator is imported but not used in the visible DAG definition), 2) Instance deletion (ComputeEngineDeleteInstanceOperator is imported but not instantiated), and 3) The proper task dependencies to orchestrate the complete lifecycle (create → execute SSH command → delete). The instruction specifically requested a complete demonstration of instance creation, remote command execution via SSH, and instance deletion with OS Login authentication, but the provided code is truncated and incomplete.",0.3333333333333333
731,Fine-tuned (Qwen 2.5 1.5B Airflow),58,0,"The code uses a PythonOperator (via @task decorator) to create a connection and manually interact with Azure Batch clients instead of relying on native Airflow operators. The 'create_connection' task wraps low-level client logic that should be handled by Airflow's connection management and native operators. Additionally, the operators used (BatchCreateJobQueueOperator, BatchTriggerJobOperator) appear to be non-standard or incorrectly used - these are not standard operators in the official airflow.providers.microsoft.azure.operators.batch module as of Airflow 3.0.6.",0,"The code explicitly imports and uses testing modules that should not be in production code: 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test harness boilerplate like 'test_run = get_test_run(dag)', 'watcher()', and comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN'. These are clear indicators of test code leakage into what should be production DAG code.",0,"The instruction asks to 'trigger a computational task on Azure Batch' and 'execute a simple shell command'. However, the code does not define or configure any actual shell command to be executed. The BatchTriggerJobOperator is called without specifying what command or task should run. Additionally, the task order is illogical (create_connection >> trigger_job >> create_job_queue), as you would typically create a job queue before triggering a job. The code focuses on connection setup and queue management but fails to demonstrate the core requirement of executing a simple shell command on Azure Batch.",0.0
732,Fine-tuned (Qwen 2.5 1.5B Airflow),59,0,"The code wraps the LocalFilesystemToS3Operator inside a TaskFlow @task decorator (uploadToLocalFileSystem function), which is not idiomatic. The LocalFilesystemToS3Operator should be instantiated directly as a task in the DAG, not returned from a Python function decorated with @task. This creates unnecessary complexity and defeats the purpose of using the specific operator. The correct approach would be: `upload_to_s3 = LocalFilesystemToS3Operator(task_id='upload-to-s3', ...)` directly in the DAG context.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, lines `from tests.system.utils import get_test_run` and `test_run = get_test_run(dag)` are imports and usage of internal testing modules. These are hallucinated testing artifacts that have no place in a production DAG example.",0,"The instruction explicitly asks for 'dynamically generating a local file' as part of the end-to-end process. The provided code does not include any task that creates or generates the local file (/tmp/this-is-my-file.tar.gz). It only attempts to upload a file that is assumed to already exist. A proper implementation would include a @task or operator that creates the file before the upload step. Additionally, while error handling is mentioned in the requirements, there is no explicit error handling demonstrated beyond the basic trigger_rule on the cleanup task.",0.0
733,Fine-tuned (Qwen 2.5 1.5B Airflow),60,0,"The code fails to use appropriate Airflow operators for the task. The instruction requires demonstrating 'Python operators with different runtime environments' (standard, virtual, and external Python contexts), but the code only shows a TaskFlow API task decorator and attempts to create BashOperator instances incorrectly. It should use PythonOperator, PythonVirtualenvOperator, and ExternalPythonOperator from airflow.operators.python to properly demonstrate different Python runtime environments.",0,The code contains a critical syntax error in the list comprehension: `[task(bash_command=...) for task in range(5)]`. This shadows the `task` decorator with the loop variable and attempts to call an integer. The BashOperator is imported but never properly instantiated. This appears to be malformed/broken code rather than clean production-ready code.,0,"The instruction explicitly asks for a multi-stage data processing pipeline using Python operators with different runtime environments (standard, virtual, and external Python contexts) with clear task dependencies. The provided code: (1) does not demonstrate multiple Python runtime environments, (2) has no clear multi-stage data processing scenario, (3) lacks proper task dependency structure, and (4) incorrectly uses BashOperator instead of the various Python operators requested. The code fundamentally fails to meet the core requirements.",0.0
734,Fine-tuned (Qwen 2.5 1.5B Airflow),61,0,"While the code uses GKE-specific operators (GKECreateClusterOperator, GKEDeleteClusterOperator), it incorrectly imports and uses GCE (Compute Engine) operators for what should be Kubernetes workload deployment. The instruction asks for 'pod deployment' and 'running sample workloads' on GKE, which should use KubernetesPodOperator or similar Kubernetes-native operators, not GCECreateInstanceFromTemplateOperator which is for VM instances. Additionally, KubernetesHook is imported but never used. The code conflates GKE (managed Kubernetes) with GCE (VM instances), which is not idiomatic for Kubernetes workload management.",0,"The code contains several hallucinated/invalid configurations: 1) The CLUSTER definition includes non-existent GKE API fields like 'endpoint_config', 'control_plane_pvc_size', 'node_pool_configs' with 'pod_pool' and 'service_account_file' which are not valid GKE cluster configuration parameters. 2) GCEResourceListOperator is imported but never used. 3) GCECreateInstanceFromTemplateOperator and GCEDeleteInstanceOperator are imported but used incorrectly in a GKE context. 4) The code is incomplete (truncated at 'cluster_na'). 5) The cluster configuration structure does not match actual GKE API specifications.",0,"The code fails to meet several key requirements: 1) 'Basic pod deployment' - No pods are deployed; instead, it attempts to create GCE VM instances which is incorrect for Kubernetes workloads. 2) 'Cross-communication between tasks' - No XCom usage or task dependencies demonstrating data/state sharing. 3) 'Asynchronous task execution' - While deferrable=True could be used for async execution, it's not implemented. 4) 'Using the cluster for sample workloads' - The cluster is created but no actual Kubernetes workloads (pods, deployments, jobs) are run on it. 5) Task dependencies are not defined, so the workflow sequence is unclear. The code is also incomplete/truncated.",0.0
735,Fine-tuned (Qwen 2.5 1.5B Airflow),62,0,"The code uses SageMaker operators (SagemakerCreateTrainingJobOperator, SagemakerDeleteTrainingJobOperator) and sensors which is good, but the core logic in create_processing_job() function is not idiomatic. It constructs shell commands with git operations and subprocess calls rather than using proper SageMaker Processing Job operators like SageMakerProcessingOperator. The function creates a malformed configuration dictionary that doesn't match actual SageMaker API specifications. Additionally, there's a @task decorator for patch_s3_sensor that appears to be a workaround rather than proper operator usage.",0,"The code contains multiple hallucinations and errors: 1) References undefined 'PythonCodeLocation.uri' and 'PythonCodeLocation.name' without importing or defining them. 2) Uses a fake/invalid Docker image URI '7fd2895d-registry.gitlab.com/sagemaker-ti/training:DQN-current-0'. 3) Creates a malformed processing job configuration with non-existent API parameters like 'Targets', 'CustomDataConfig', 'CustomTrainingParameters' that don't exist in SageMaker Processing Job API. 4) Includes nonsensical shell commands with Chinese characters ('branch名稱') and invalid syntax. 5) The code is incomplete (cuts off mid-function). This is not production-ready code.",0,"The instruction asks for a document classification training process for medical documents with task groups, proper error handling, and resource management. The code fails to deliver: 1) No document classification logic or medical document categorization is implemented. 2) No task groups are created despite being explicitly requested. 3) No proper error handling is implemented. 4) The workflow is incomplete and doesn't show the full orchestration of creating, monitoring, and cleaning up a model. 5) The code focuses on a malformed processing job rather than a coherent training workflow. The DAG_ID suggests 'processing_job' but the instruction clearly asks for a 'training process' with classifier creation.",0.0
736,Fine-tuned (Qwen 2.5 1.5B Airflow),63,1,"The DAG uses native Airflow operators throughout: TrinoToGCSOperator for data transfer, BigQueryCreateEmptyDatasetOperator, BigQueryCreateExternalTableOperator, BigQueryDeleteDatasetOperator for BigQuery operations, and GCSCreateBucketOperator/GCSDeleteBucketOperator for GCS operations. No PythonOperator wrappers with hooks are used. This is idiomatic Airflow code that leverages provider-specific operators.",0,"The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test harness boilerplate that should not be present in production DAG code. The code also includes comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' which are test-oriented rather than production-oriented.",0,"The instruction asks for 'inter-cloud data transfer and external table creation' between Trino and BigQuery. While the DAG includes TrinoToGCSOperator for data transfer from Trino to GCS, it does NOT actually create an external table in BigQuery. The QUERY_CREATE_EXTERNAL_TABLE variable contains a CREATE EXTERNAL TABLE statement, but it's incorrectly passed to TrinoToGCSOperator as SQL (which would execute it on Trino, not BigQuery). The BigQueryCreateExternalTableOperator is imported but never used in the DAG. The pipeline is incomplete and doesn't fulfill the core requirement of creating an external table in BigQuery that references the transferred data.",0.3333333333333333
737,Fine-tuned (Qwen 2.5 1.5B Airflow),64,1,"The code uses native Airflow Dataplex operators (DataplexCreateEntryGroupOperator, DataplexCreateLakeOperator, DataplexCreateWorkflowOperator, etc.) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear indicators of test harness boilerplate with ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), and references to 'gs://airflow-system-tests-resources/dataplex/spark/sample-spark-file.txt'. Additionally, the code is incomplete/malformed with syntax errors (|# instead of proper comment closure, incomplete closing parenthesis on create_workflow, and the DAG code is cut off). This is not production-ready code.",0,"The instruction specifically requests orchestration of 'entry groups, entry types, and aspect types' for metadata management and data governance. While the code creates entry groups and includes some workflow operations, it does not demonstrate creation or configuration of entry types or aspect types as distinct components. The code also appears incomplete (cut off mid-definition) and focuses more on workflow and lake creation rather than the comprehensive metadata management setup requested. The instruction emphasizes 'systematically creating and configuring' these three specific components, but the code only partially addresses entry groups.",0.3333333333333333
738,Fine-tuned (Qwen 2.5 1.5B Airflow),65,0,"The code imports proper Airflow operators (CloudVisionDetectImageLabelsOperator, etc.) but never uses them. Instead, it defines Python functions that wrap logic with BashOperator and call non-existent 'VisionAnnotator' methods. This is the anti-pattern of wrapping functionality in Python callables instead of using the native operators that were imported. The BashOperator usage with bash_command for vision API calls is completely non-idiomatic.",0,"The code contains multiple hallucinations: (1) References undefined 'VisionAnnotator' class that doesn't exist in Airflow, (2) Uses undefined variables 'dag' and 'sc' that are never declared, (3) BashOperator parameters like 'bash_lines' and 'output_columns' don't exist in the actual API, (4) The bash commands appear to be fabricated and don't correspond to real GCP CLI commands, (5) Code is incomplete (cuts off mid-comment). This is not production-ready code.",0,"The instruction asks for an 'image object detection training system' with data ingestion, model training, and resource management. The provided code only attempts to use Google Cloud Vision API for image annotation/detection (inference), not training. There is no model training workflow, no configurable training parameters, no data ingestion pipeline, and no proper resource management beyond creating/deleting a bucket (which isn't even used). The code completely misses the core requirement of training a machine learning model.",0.0
739,Fine-tuned (Qwen 2.5 1.5B Airflow),68,0,"While the code uses RedisCoordinationOperator which appears to be a specific operator, it calls `.hook()` method directly on the operator instances (wait_for_start.hook() and combine_tasks.hook()), which is not idiomatic Airflow usage. Operators should be used declaratively in the DAG structure, not by calling their internal methods. Additionally, the create_connection task uses a @task decorator to programmatically create connections, which is not the standard way to manage connections in production Airflow (connections should be managed via UI, CLI, or environment variables).",0,"The code explicitly imports test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)'. This is internal testing infrastructure that should not appear in production DAG code, indicating hallucination/leakage from test examples.",0,"The instruction asks for a system where 'one task publishes a start signal, and subsequent tasks wait for specific conditions before proceeding.' However, the code does not show any task explicitly publishing/setting a signal to Redis. The RedisCoordinationOperator usage appears incomplete - it waits for keys but there's no corresponding task that sets/publishes those keys. The workflow shows BashOperator tasks (start_job, finish_job) but they don't interact with Redis to publish signals. The coordination mechanism is not properly implemented as requested.",0.0
740,Fine-tuned (Qwen 2.5 1.5B Airflow),69,1,"The code uses the appropriate TimeDeltaSensor from airflow.providers.standard.sensors.time_delta, which is the idiomatic Airflow way to handle time-based delays. It does not wrap sensor logic in a PythonOperator, but uses the native sensor operator designed for this purpose.",1,"The code uses only standard Airflow libraries (airflow.decorators, airflow.providers.standard.sensors.time_delta) and does not import any testing modules or include test harness boilerplate. The code is clean from a production perspective, though it has syntax errors.",0,"The code has critical implementation errors that prevent it from fulfilling the instruction. First, the task dependency is backwards - the sensor should come BEFORE the task, not after (wait_for_3_minutes >> TimeDeltaSensor creates the sensor after calling check()). Second, the delta parameter has an invalid string value '3 * 60 * 1' instead of a timedelta object. Third, there's a syntax error in xcom_pull with 'call_function': '' instead of proper key access. The instruction asks for a pipeline that 'waits for a predetermined time interval before proceeding to the next task', but this implementation would execute the task first, then create the sensor, which is backwards and non-functional.",0.6666666666666666
741,Fine-tuned (Qwen 2.5 1.5B Airflow),70,0,"The DAG uses PythonOperator (via @task decorator) with GCSHook for file operations instead of using idiomatic GCS operators. For example, 'write_to_gcs()' uses GCSHook.upload() wrapped in a @task, when it should use GCSToGCSOperator or similar transfer operators. The 'write_to_local_file()' task also wraps hook logic in a Python function instead of using appropriate operators like GCSToLocalFilesystemOperator.",0,"The code contains clear test harness boilerplate and imports from internal testing modules: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. Additionally, the code has syntax errors (e.g., 'bucket=name=bucket_name', 'project_id(project_id=GCP_PROJECT_ID)') and references undefined variables ('bucket_name', 'prefix', 'Path'), making it non-production-ready.",0,"The instruction requires: (1) file transfer between buckets, (2) custom transformation script execution, (3) proper error handling, and (4) cleanup. The DAG fails on multiple counts: it doesn't transfer files between different buckets (only creates one bucket), doesn't execute any custom transformation script, has no error handling mechanisms (try/except blocks or error callbacks), and while it has cleanup (delete_bucket), the overall implementation is broken with syntax errors and doesn't demonstrate the requested file manipulation workflow.",0.0
742,Fine-tuned (Qwen 2.5 1.5B Airflow),71,0,"While the DAG uses many native S3 operators (S3CreateBucketOperator, S3PutObjectOperator, etc.), the create_bucket_tags task is implemented as a TaskFlow @task decorator that directly uses boto3 client instead of using the available S3PutBucketTaggingOperator. This is a non-idiomatic pattern - the code should use S3PutBucketTaggingOperator which is already imported but not properly utilized for the tagging operation.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. Additionally, the watcher() function is called at the end. These are testing artifacts that should not be in production code.",0,"The DAG has multiple critical issues that prevent it from fulfilling the instruction: 1) The create_bucket_tags function has broken logic - it calls ACM (Certificate Manager) client instead of S3 client, and the logic doesn't make sense for S3 tagging. 2) The chain() call has invalid parameters (force_delete, destroy_downstream) that don't exist in the chain function. 3) The DAG is missing S3ListBucketOperator to list bucket contents as requested. 4) The task ordering is illogical - it tries to create tags and get tags before putting an object, and the bucket creation happens after other operations. 5) The overall workflow is broken and would fail at runtime.",0.0
743,Fine-tuned (Qwen 2.5 1.5B Airflow),73,0,"The code uses ADLSListOperator and ADLSToSFTPTempFileOperator, which are not appropriate for the task. The instruction asks for monitoring Azure Blob Storage for file arrivals with specific patterns and triggering downstream processes. The idiomatic approach would be to use AzureBlobStorageSensor (e.g., WasbBlobSensor or WasbPrefixSensor) from airflow.providers.microsoft.azure.sensors.wasb to wait for files, not ADLS operators. Additionally, the code uses ADLS (Azure Data Lake Storage) operators instead of Azure Blob Storage operators, which are different services.",0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. It also imports from 'system.azure import SYSTEM_TEST_CONFIG_DIR', which appears to be internal testing infrastructure. These are not production-ready imports and indicate the code was copied from Airflow's test suite rather than being clean, production code.",0,"The code fails to meet the core requirements of the instruction. The user requested: (1) monitoring Azure Blob Storage for file arrivals, (2) checking for specific file patterns/prefixes, and (3) triggering downstream processes based on file presence. The provided code uses ADLS operators instead of Azure Blob Storage, does not implement any monitoring/sensing mechanism (uses list and transfer operators instead of sensors), does not check for file patterns, and does not demonstrate triggering downstream processes. The DAG appears to be a copy-paste from an ADLS-to-SFTP example rather than a blob storage monitoring workflow.",0.0
744,Fine-tuned (Qwen 2.5 1.5B Airflow),76,0,"The code violates idiomatic Airflow principles by instantiating an operator (GCSToGCSOperator) inside a PythonOperator decorated function (upload_file_to_gcs). The GCSToGCSOperator is created but never executed properly within the task context. This is a clear anti-pattern - operators should be instantiated at the DAG level, not inside Python callables. Additionally, the 'remove_file' task incorrectly uses GCSCreateBucketOperator instead of a proper file deletion operator like GCSDeleteObjectsOperator.",0,"The code explicitly imports and uses test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production DAG code. The code also uses test-specific environment variables (SYSTEM_TESTS_ENV_ID, SYSTEM_TESTS_GCP_PROJECT) and includes START/END comment markers typical of documentation examples rather than production code.",0,"The instruction requires: (1) showcasing advanced cloud storage transfer techniques using GCS and SFTP operators, (2) handling various file transfer scenarios, (3) implementing robust file transfer verification mechanisms, and (4) ensuring data integrity through sensor checks. The code fails on multiple counts: it lacks any sensor checks for file transfer verification, has no data integrity validation mechanisms, the upload_file_to_gcs task is broken and doesn't actually upload files, and it only demonstrates basic SFTP-to-GCS transfers without showcasing the requested GCS-to-SFTP direction (despite the DAG name). The code is incomplete and non-functional for demonstrating advanced transfer techniques.",0.0
745,Fine-tuned (Qwen 2.5 1.5B Airflow),77,0,"The code fails to use idiomatic Airflow patterns. The tasks 'create_collection_with_vectorizer' and 'create_new_collection_without_vectorizer' are decorated with @task but contain only Collection instantiation without any actual execution logic or connection to Weaviate. They don't use proper Weaviate operators or hooks. The Collection objects are instantiated but never used to perform any operations like creating collections in Weaviate, importing data, or managing the database. This is not how Airflow should interact with external systems - it should use providers' operators or hooks with proper connection management.",0,The code explicitly imports testing modules and includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. This is internal testing code that should not appear in production DAG examples.,0,"The instruction requires: (1) transforming JSON-based Jeopardy question data, (2) importing into vector-enabled database collections, (3) batch import data handling, (4) different vectorization approaches, and (5) programmatic collection management. The code fails on multiple fronts: the WeaviateToWeaviateOperator is used incorrectly (it's meant for transferring data between Weaviate instances, not for initial data import), the two collection creation tasks don't actually create collections in Weaviate (they just instantiate Python objects), there's no actual data transformation pipeline, no demonstration of batch import, and no proper handling of vectorization. The code structure suggests an example but doesn't implement a functional data processing pipeline as requested.",0.0
746,Fine-tuned (Qwen 2.5 1.5B Airflow),78,0,"The code wraps SqlExecuteQueryOperator calls inside a PythonOperator (via @task decorator) and manually calls .execute() on them. This is anti-idiomatic. The proper approach is to instantiate SqlExecuteQueryOperator directly in the DAG context as independent tasks, not wrap them in a Python function. The operators should be declared at the DAG level to create proper task dependencies, not executed imperatively within a single task function.",0,"The code explicitly imports 'from tests.system.utils import get_test_run' and uses 'test_run = get_test_run(dag)', which is test harness boilerplate. This is internal testing infrastructure that should not appear in production DAG code. Additionally, the code contains START/END comment markers (e.g., '# [START create_table]') that are typical of documentation examples but not production code.",1,"The DAG does fulfill the core instruction: it demonstrates basic data persistence and retrieval using SQLite (an embedded, file-based database). It creates a sequential flow showing write operations (CREATE TABLE, INSERT) and read operations (SELECT queries). The instruction requirements for 'lightweight, file-based key-value store' and 'reading and writing operations' are met, even though the implementation approach is not idiomatic.",0.3333333333333333
747,Fine-tuned (Qwen 2.5 1.5B Airflow),79,0,"The DAG uses a generic @task decorator with a print statement instead of using Databricks-specific operators. For repository and notebook management, it should use operators from airflow.providers.databricks such as DatabricksReposCreateOperator, DatabricksReposUpdateOperator, DatabricksReposDeleteOperator, and DatabricksRunNowOperator or DatabricksNotebookOperator. The current implementation is not idiomatic Airflow for Databricks operations.",0,The code explicitly imports testing modules (from tests_common.test_utils.system_tests import get_test_run) and includes test harness boilerplate (test_run = get_test_run(dag)). This is internal testing infrastructure that should not appear in production DAG code.,0,"The instruction requires a workflow that: (1) creates a Databricks repository, (2) updates the repository, (3) deletes the repository, and (4) executes a data preparation notebook. The provided code only contains a placeholder task that prints a message about cluster creation. It does not implement any repository lifecycle management (create/update/delete) or notebook execution functionality. The DAG completely fails to fulfill the requested business logic.",0.0
748,Fine-tuned (Qwen 2.5 1.5B Airflow),80,0,"The DAG heavily relies on PythonOperator tasks (via @task decorator) that wrap Hook calls instead of using native Operators. For example, create_spark_file(), create_kbucket(), create_queue(), delete_kbucket(), and delete_queue() all use KueeyHook within Python callables. While one QueueCreateOperator is used (queue_create_task2), the majority of the workflow uses the anti-pattern of wrapping Hooks in Python tasks rather than leveraging purpose-built Operators. The delete_instances task also uses a raw client call wrapped in a Python task instead of using a proper GCE operator.",0,"The code imports and uses 'KueeyHook', 'QueueCreateOperator', and 'QueueDeleteOperator' from airflow.providers.google.cloud, but these do not exist in standard Airflow 2.9.3. The actual GCP provider does not have a 'kueue' module or 'Kueue' operators/hooks. Additionally, the code references 'gcp_compute.InstancesClient()' without importing it. The entire 'Kueue' integration appears to be fabricated/hallucinated, as Kueue is a Kubernetes-native job queueing system, not a GCP service with Airflow operators.",0,"The instruction asks for a Kubernetes-native pipeline demonstrating dynamic cluster provisioning, ephemeral pod execution, and infrastructure teardown. The generated code instead attempts to use non-existent GCP 'Kueue' operators and creates GCS buckets, which doesn't align with Kubernetes pod execution. The proper approach would use KubernetesPodOperator or GKEStartPodOperator for ephemeral pod execution, and potentially GKECreateClusterOperator/GKEDeleteClusterOperator for dynamic cluster provisioning. The DAG also has logical errors (DeleteBucketOperator runs before create tasks, circular dependencies) and doesn't demonstrate a coherent data processing workflow.",0.0
749,Fine-tuned (Qwen 2.5 1.5B Airflow),81,1,"The code uses the SingularityOperator from airflow.providers.singularity.operators.singularity, which is the appropriate provider-specific operator for containerized operations. It also uses the @task decorator for Python tasks, which is idiomatic for Airflow 2.x. The code does not wrap operators in PythonOperator unnecessarily.",0,The code explicitly imports from 'tests.system.utils' (line: 'from tests.system.utils import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). These are internal testing modules that should not appear in production DAG code. This is a clear example of test leakage into production code.,0,"The instruction requested a workflow that 'validates system time, performs wait operations, and executes a containerized process' to showcase task dependency and execution strategies. While the code does include containerized operations (SingularityOperator), it does not validate system time or perform explicit wait operations. Additionally, there are critical errors: 'some_task_defense' is referenced but never defined (should be 'some_task_external_connection_def'), and the DAG context manager uses 'is not None' instead of 'as dag'. The code would fail to execute and does not fulfill the specific requirements of the instruction.",0.3333333333333333
750,Fine-tuned (Qwen 2.5 1.5B Airflow),82,1,"The code uses native Airflow providers and operators appropriately. It imports and uses RdsCreateDbInstanceOperator, RdsDeleteDbInstanceOperator, RdsCreateEventSubscriptionOperator, S3CreateBucketOperator, and S3DeleteBucketOperator from airflow.providers.amazon.aws.operators. While there are some @task decorated functions with boto3/hooks, the primary infrastructure operations use the proper operators rather than wrapping everything in PythonOperators.",0,"The code contains a clear hallucination/test harness import: 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder'. This is an internal testing module that should not appear in production code. Additionally, the code references 'ENV_ID' variable that is never defined in the visible code, suggesting this is copied from a test suite. The presence of SystemTestContextBuilder is a strong indicator this is test boilerplate.",0,"The code does not fulfill the user instruction adequately. The instruction asks for orchestration across AWS services (RDS, EC2, and S3) with infrastructure provisioning, security configuration, and data replication. However: 1) The code is incomplete (cuts off mid-definition), 2) There is no EC2 provisioning whatsoever, 3) No security group configuration is shown, 4) No actual data migration/replication logic is implemented - only infrastructure creation/deletion, 5) The S3 operations are defined as helper functions but not integrated into the DAG flow. The code appears to be a fragment from an RDS event example rather than a complete solution to the stated requirements.",0.3333333333333333
751,Fine-tuned (Qwen 2.5 1.5B Airflow),83,0,"While the code does use TwitterToGCSOperator (a provider-specific operator), it wraps the GCSDeleteBucketOperator inside a @task decorator unnecessarily, which is not idiomatic. The delete operation should be a direct operator instantiation, not wrapped in a TaskFlow function. Additionally, the create_connection task uses @task decorator to create connections programmatically, which is not the standard Airflow pattern for connection management.",0,"The code contains multiple hallucinations and test harness artifacts: (1) imports from 'tests_common.test_utils.watcher' and 'tests_common.test_utils.system_tests', (2) uses 'get_test_run(dag)' and 'get_test_run_results(test_run)', (3) imports 'django.conf.settings' which is not part of standard Airflow, (4) references non-existent 'settings.Connection.TWITTER', 'settings.TWITTER_CONSUMER_KEY', 'settings.BEARER_TOKEN', and 'settings.DATABASES' which appear to be Django-specific patterns. This is clearly test/example code, not production-ready.",0,"The instruction requires: (1) collecting tweets from multiple accounts, (2) processing for insights, (3) sentiment analysis, (4) storing results in a database, and (5) preparing data for business intelligence. The provided code only collects tweets from a single Twitter URL and stores them in GCS. It does not perform sentiment analysis, does not process multiple accounts, does not store results in a database, and does not prepare data for BI. The code is missing the core business logic requested.",0.0
752,Fine-tuned (Qwen 2.5 1.5B Airflow),85,0,"The code defines multiple Python functions (dataset_create_hook, table_create_hook, etc.) that wrap BigQueryHook methods, which would typically be called from PythonOperators. This is a non-idiomatic pattern. While the imports show proper operators like BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryToBigQueryOperator, and BigQueryToGCSOperator are available, the code implements custom hook wrapper functions instead of using these native operators directly in the DAG definition. This violates the idiomatic Airflow principle of using specific Providers and Operators designed for the task.",0,"The code contains clear hallucination/test harness leakage. It imports 'pytest' and includes a pytest.skip() statement with test-related logic ('pytest.skip(""bigquery operators are disabled because of broken dependencies"", allow_module_level=True)'). Additionally, it references system test environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', which are internal testing patterns. The try-except block around imports with pytest.skip is typical test harness boilerplate that should not appear in production DAG code.",0,"The code is incomplete (cuts off mid-definition with 'sched') and does not implement a complete data workflow as requested. While it imports the necessary operators (BigQueryToBigQueryOperator for copying data between tables, BigQueryToGCSOperator for exporting to cloud storage), the actual DAG definition is incomplete. The instruction requires a complete pipeline with setup, data movement, and cleanup stages, but the code only shows function definitions for hooks and an incomplete DAG declaration. No actual task definitions or workflow orchestration is present in the provided code.",0.0
753,Fine-tuned (Qwen 2.5 1.5B Airflow),86,1,"The code uses native Airflow providers and operators appropriately. It uses SFTPFileSensor and SFTPFolderSensor from airflow.providers.sftp, which are the idiomatic operators for SFTP operations. The setup and teardown tasks use @task decorators, which is acceptable for connection management logic that doesn't have a dedicated operator.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. These are testing utilities that should not be present in production DAG code.",0,"The instruction asks for a cloud infrastructure workflow demonstrating setup and teardown tasks for resource allocation and cleanup. However, this DAG is focused on SFTP sensor operations, not cloud infrastructure. Additionally, the setup/teardown implementation has syntax errors (invalid trigger_rule assignment syntax) and logical issues (create_conn_task appears twice in dependencies, circular reference). The DAG doesn't demonstrate a proper cloud resource lifecycle with allocation, usage, and cleanup stages as requested.",0.3333333333333333
754,Fine-tuned (Qwen 2.5 1.5B Airflow),87,0,"The code heavily violates idiomatic Airflow patterns. It wraps Operators inside @task decorated functions (e.g., add_step, modify_cluster, get_step_id) instead of using them directly in the DAG definition. For example, 'add_step' creates an EmrStartJobFlowOperator inside a TaskFlow function, and 'get_step_id' creates an EmrStepSensor inside a function. These operators should be instantiated directly in the DAG context, not wrapped in PythonOperators via @task decorators. The 'setup_bucket' task uses boto3 client directly instead of using S3CreateBucketOperator which is already imported. The 'delete_amzn_dataset' task uses boto3 client instead of EmrTerminateJobFlowOperator.",0,"The code contains multiple issues indicating hallucination: 1) Invalid boto3 API calls like 'boto3.client(""emr"").get_job_flow_role()' and 'get_job_flow_service_role()' which don't exist in the boto3 EMR client API. 2) Invalid method 'get_location(CurrentRegion=""us-east-1"")' on S3 client. 3) Invalid parameter 'waitForCompletion=True' in terminate_job_flow. 4) The code is incomplete (cuts off mid-import statement). 5) Inconsistent variable naming (SHORTcNAME, FILE_path with random capitalization). 6) References to non-existent attributes like 'poke_interval' being set on operators that don't have this attribute in that context.",0,"The code fails to fulfill the instruction requirements. While it imports relevant operators (S3CreateBucketOperator, EmrCreateJobFlowOperator, EmrAddStepsOperator, EmrTerminateJobFlowOperator), it never properly uses them to create a functional DAG workflow. There's no actual DAG instantiation with proper task dependencies showing: S3 bucket creation → security configuration → EMR cluster launch → Spark step execution → cluster termination. The @task decorated functions don't form a coherent workflow and misuse operators. Security configuration is completely missing. The code appears to be a collection of disconnected examples rather than a working orchestration pipeline.",0.0
755,Fine-tuned (Qwen 2.5 1.5B Airflow),89,1,"The DAG correctly uses native Cloud SQL operators from airflow.providers.google.cloud.operators.cloud_sql including CloudSQLCreateInstanceOperator, CloudSQLCreateInstanceDatabaseOperator, CloudSQLExportDatabaseOperator, CloudSQLImportDatabaseOperator, and CloudSQLDeleteInstanceOperator. It does not wrap these operations in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains test harness boilerplate at the end with 'from tests_common.test_utils.watcher import watcher' which is an internal testing module. This is clearly test infrastructure code that should not be present in production DAG code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are testing conventions, not production patterns.",1,"The DAG fulfills the user instruction to orchestrate complex Cloud SQL operations. It includes: (1) instance creation via CloudSQLCreateInstanceOperator, (2) database creation via CloudSQLCreateInstanceDatabaseOperator, (3) data export via CloudSQLExportDatabaseOperator, (4) data import via CloudSQLImportDatabaseOperator, (5) database configuration changes through the instance body settings, and (6) infrastructure cleanup via CloudSQLDeleteInstanceDatabaseOperator and CloudSQLDeleteInstanceOperator. The workflow demonstrates enterprise-grade database infrastructure automation with proper task dependencies and trigger rules.",0.6666666666666666
756,Fine-tuned (Qwen 2.5 1.5B Airflow),90,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code, indicating hallucination/leakage from test examples.",1,"The DAG fully implements the requested workflow: (1) creates a Dataproc cluster, (2) submits a job to the cluster, and (3) deletes the cluster. The sequential task dependencies are clearly defined with 'create_cluster >> run_job >> delete_cluster'. The use of TriggerRule.ALL_DONE on the delete operation ensures robust cleanup even if the job fails, demonstrating proper infrastructure management and clear task progression as requested.",0.6666666666666666
757,Fine-tuned (Qwen 2.5 1.5B Airflow),91,1,The code uses native Airflow operators designed for the specific tasks: OpenAIEmbeddingOperator for generating embeddings and PgVectorIngestOperator for storing vectors in PostgreSQL. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.,0,"The code contains multiple hallucinations and test harness boilerplate: (1) Imports from 'tests.system.providers.openai.utils' (SystemTestContextBuilder), (2) Imports from 'tests.system.utils.watcher', (3) Uses test_context object throughout, (4) Contains bizarre HuggingFace API calls that don't exist (HfApi().create_table_from_original_file, HfApi().insert_into_original_file), (5) Has nonsensical version checking logic at the top using HfApi().search_repo_token() for version comparison, (6) Includes TEST SETUP/BODY/TEARDOWN comments indicating this is test code, (7) Has invalid method calls like embed_task.drop() and pgvector_ingest_task.drop() which don't exist on operators. This is clearly not production-ready code.",0,"While the code attempts to use OpenAI embeddings and PostgreSQL vector storage, it fails to implement a working pipeline. The instruction asks for a workflow that generates embeddings for text data and stores them in PostgreSQL. However, the code has critical issues: (1) The embed_task references 'd[""text""]' but embedding_data is a list of tuples, not dictionaries, (2) The pgvector_ingest_task references 'd[""vector""]' but the vectors haven't been generated yet - they're empty lists in the data, (3) The create_table and insert_embeddings tasks use completely fabricated HuggingFace API methods that don't exist, (4) The overall flow is broken and would not execute successfully. The code does not demonstrate a functional data pipeline as requested.",0.3333333333333333
758,Fine-tuned (Qwen 2.5 1.5B Airflow),92,1,"The code uses native Airflow operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. It does not wrap logic in PythonOperator with hooks, which demonstrates idiomatic Airflow usage.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction explicitly requests a Databricks pipeline with Databricks-specific features (dynamically create jobs, run notebooks, manage computational resources). However, the provided code uses Google Cloud Dataproc with SparkR jobs instead. This is a completely different cloud platform and technology stack. The code does not use any Databricks operators (e.g., DatabricksRunNowOperator, DatabricksSubmitRunOperator) or demonstrate notebook-based data preparation strategies on Databricks.",0.3333333333333333
759,Fine-tuned (Qwen 2.5 1.5B Airflow),93,1,"The code uses Airflow's TaskFlow API with @task decorators and native XCom functionality (task.xcom_push, task.xcom_pull), which are idiomatic patterns for Airflow 3.0.0. It does not wrap logic unnecessarily in PythonOperator and uses the appropriate Airflow SDK constructs.",0,The code contains clear hallucination/test harness leakage. It imports 'from tests_common.test_utils.watcher import watcher' and includes test boilerplate code with 'list(dag.tasks) >> watcher()'. This is internal testing infrastructure that should not appear in production DAG code.,0,The instruction specifically requested a DAG that translates text and demonstrates cross-task communication by accessing the translation result through XCom. The provided code does not perform any text translation whatsoever - it only pushes/pulls a string 'Hello World' and a datetime object. The core business logic requirement (text translation) is completely missing.,0.3333333333333333
760,Fine-tuned (Qwen 2.5 1.5B Airflow),95,0,"The DAG uses a mix of idiomatic and non-idiomatic patterns. While it correctly uses Spanner-specific operators like SpannerInstanceCreateOperator, SpannerInstanceUpdateOperator, and SpannerAdminDatabaseOperator, it also includes a custom @task decorated function 'generate_arms_file' that wraps SpannerHook logic in a PythonOperator pattern. This is exactly the anti-pattern described in the scoring criteria - using a Hook inside a Python function instead of leveraging native operators. The proper approach would be to use SpannerArmsPrefixGeneratorOperator (which is imported but never used) or another native operator for this functionality.",0,"The code contains several hallucinations and errors: 1) References undefined 'spanner_instance_config.Size.TINY/MEDIUM/LARGE' without importing this module, 2) Uses non-existent operators like 'SpannerArmsPrefixGeneratorOperator' and 'SpannerInstanceGetOperationMonitoringOperator' which are not standard Airflow 2.9.3 operators, 3) References a non-existent 'create_spanner_arms_file' method on SpannerHook, 4) The SQL commands shown are not valid Spanner DDL/DML syntax (transactional triggers don't exist in Spanner with that syntax), 5) The code is incomplete (cuts off mid-line with '{{ task_instance.xco'), and 6) SpannerSqlApplyChangesOperator is imported but appears to be a hallucinated operator name.",0,"The DAG partially addresses the instruction but has significant gaps. It attempts to show instance deployment (create/update/delete operations) which aligns with 'instance deployment' and 'administrative tasks', but fails to properly demonstrate 'database schema evolution' and 'idempotent database modifications'. The SpannerAdminDatabaseOperator is imported but never used to create a database. The SQL commands provided are invalid Spanner syntax and don't demonstrate real schema evolution. The code is also incomplete (cuts off abruptly), meaning it cannot fulfill the requirement of being a 'comprehensive example'. A complete implementation would need actual database creation, valid DDL statements for schema changes, and proper demonstration of idempotent operations.",0.0
761,Fine-tuned (Qwen 2.5 1.5B Airflow),96,0,"The code does not use standard Airflow Vertex AI operators from airflow.providers.google.cloud.operators.vertex_ai. Instead, it uses non-standard imports like 'system.google.vertex_ai_text_embedding_model' and 'system.google.third_party_config' which are not part of the official Airflow providers. Additionally, it uses a TaskFlow API @dag.task() for one task but then attempts to chain non-operator objects (ModelServiceClient methods) with bitshift operators, which is not idiomatic Airflow. Proper implementation should use operators like GenerativeModelGenerateContentOperator, TextEmbeddingModelGetEmbeddingsOperator, etc.",0,"The code contains multiple hallucinated/non-existent imports and modules. 'system.google.third_party_config', 'system.google.workspace_config', and 'system.google.vertex_ai_text_embedding_model' are not standard Airflow libraries or Google Cloud provider modules. These appear to be fabricated internal testing modules. The code also has syntax errors (line with 'WORKSPACE_NAME PROJECT_ID' is malformed). The pattern resembles test harness code rather than production DAG code.",0,"The instruction asks for an AI-powered content analysis pipeline demonstrating multiple generative AI model interactions including text generation, embedding, and multimodal processing capabilities. The provided code only attempts to create a text embedding model (and does so incorrectly with hallucinated modules). It does not demonstrate: (1) text generation capabilities, (2) multimodal processing (image/video + text), or (3) a content analysis pipeline workflow. The code is focused on model service management rather than actual AI content processing tasks.",0.0
762,Fine-tuned (Qwen 2.5 1.5B Airflow),97,1,"The code uses native Azure-specific operators from the official Airflow providers package. It uses LocalFilesystemToADLSOperator for uploading files and ADLSDeleteOperator for deleting files from Azure Data Lake Storage. These are the appropriate, idiomatic operators for the task rather than wrapping logic in PythonOperator with hooks.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. This is a clear example of test infrastructure leaking into the DAG definition.,1,"The DAG fulfills the user instruction completely. It showcases task sequencing (upload followed by delete operations), demonstrates cloud file management using Azure operators, and illustrates how to programmatically upload (LocalFilesystemToADLSOperator) and remove (ADLSDeleteOperator) files from a remote storage system (Azure Data Lake Storage). The business logic requested is fully implemented.",0.6666666666666666
763,Fine-tuned (Qwen 2.5 1.5B Airflow),98,0,"The code uses TaskFlow API (@task decorators) with boto3 client directly instead of using native SageMaker operators. For example, 'delete_knn_endpoint' and 'delete_knn_endpoint_config' use boto3.client('sagemaker') wrapped in @task decorators instead of using SageMakerDeleteEndpointOperator and SageMakerDeleteEndpointConfigOperator. The 'create_knn_endpoint_config' task just returns a dictionary instead of using SageMakerEndpointConfigOperator. While the imports show awareness of SageMaker operators (SageMakerNotebookProcessorOperator, SageMakerProcessingJobSensor), the actual implementation relies on PythonOperator pattern (via @task) + boto3 instead of native operators.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is clearly from internal Airflow testing modules. It also uses 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' which is test harness boilerplate. Additionally, there are hallucinated/non-existent imports like 'SageMarPredictionsSensor' (typo - should be SageMakerEndpointSensor or similar) and 'SageMakerNotebookProcessorOperator' which doesn't exist in standard Airflow providers. The code is incomplete and appears to be a fragment from a system test rather than production-ready code.",0,"The instruction requires a complete ML pipeline with: (1) data upload, (2) model training, (3) endpoint deployment, and (4) automatic cleanup. The provided code is incomplete - it only shows bucket creation and some cleanup tasks. Critical components are missing: no SageMaker training job operator, no model creation, no endpoint deployment operator, no actual data upload to S3 for training. The 'sample_rapids_script' function appears to be unrelated boilerplate and is never used. The DAG definition is cut off mid-task. The code does not fulfill the core requirement of training a KNN model on SageMaker and deploying it as a real-time endpoint.",0.0
764,Fine-tuned (Qwen 2.5 1.5B Airflow),99,1,"The code uses the native PineconeIngestOperator from airflow.providers.pinecone.operators.pinecone, which is the idiomatic way to interact with Pinecone in Airflow. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator for vector ingestion.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The code fulfills the user instruction to create a data pipeline that showcases vector embedding ingestion with flexible metadata handling and batch processing capabilities. It demonstrates: (1) programmatic insertion of vector data into Pinecone using Airflow's integration, (2) batch processing with configurable batch_size parameter, (3) multiple examples showing different ingestion patterns (single insert and batch insert), and (4) flexible metadata handling through namespace and ref_id parameters.",0.6666666666666666
765,Fine-tuned (Qwen 2.5 1.5B Airflow),100,0,The DAG violates idiomatic Airflow patterns by mixing PythonOperator-style tasks (using @task decorator with CloudDataFusionHook directly) alongside native operators. The tasks 'create_pipleline_with_no_bucket' and 'create_pipleline_with_gcs_bucket' wrap Hook logic in Python callables instead of using the CloudDataFusionCreatePipelineOperator that is already imported and available. This is exactly the anti-pattern described in the scoring criteria.,0,"The code contains multiple hallucinations and errors: (1) Import statement has a syntax error 'from airflow.providers.google.cloud.hooks.data fusion' (space in 'data fusion'), (2) References to operators like CloudDataFusionCreateTaskOperator, CloudDataFusionDeleteTaskOperator, CloudDataFusionGetTaskOperator, CloudDataFusionListTasksOperator that don't exist in standard Airflow providers for Data Fusion, (3) Uses non-existent methods like 'hook.create_pipeline()' and 'hook.create_pipeline_from_resource()' which are not part of CloudDataFusionHook API, (4) References undefined variable 'pipeline' in create_pipeline_from_resource operator, (5) Code is incomplete (cuts off at 'get_pipeline = CloudDat'), (6) Uses fabricated API patterns like 'JobTrigger.builder().for_amount(10).build()' that don't match Google Cloud Data Fusion API.",0,"The instruction requires orchestrating the 'complete lifecycle' including: (1) instance creation, (2) pipeline deployment, (3) execution, and (4) cleanup. The provided code is incomplete and missing critical components: no Data Fusion instance creation/deletion operators, no pipeline execution logic, incomplete cleanup (only bucket deletion, no pipeline or instance cleanup), and the code cuts off mid-statement. The DAG does not demonstrate a complete lifecycle management as requested.",0.0
766,Fine-tuned (Qwen 2.5 1.5B Airflow),101,1,The code uses native Airflow operators appropriately: MsSqlOperator for SQL execution and MSSqlToMSSqlOperator for data transfer between MsSQL instances. It imports from the correct provider packages (airflow.providers.microsoft.mssql) and does not wrap database operations in PythonOperator + Hook patterns unnecessarily. The code demonstrates proper use of Airflow's provider-based architecture.,0,"The code contains test harness boilerplate that should not be in production code. Specifically: 1) ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID') suggests this is from a system test, 2) The DAG_ID includes 'example_mssql_to_mssql' which is test/example naming, 3) The try-except block with the print statement about installing providers is test documentation code, not production code. While it doesn't import from tests_common directly, it clearly contains system test artifacts.",0,"The instruction requires: 1) Transform raw geographical and user data into structured database tables, 2) Handle data insertion, 3) Support parameterized queries, 4) Demonstrate different methods of database interaction using MsSql operators and hooks. The provided code only executes 'SELECT 1' statements and does not: create or populate any tables with geographical/user data, perform any data transformations, demonstrate parameterized queries, or use MsSqlHook to show different interaction methods. It's a basic connectivity example, not a data processing pipeline as requested.",0.3333333333333333
767,Fine-tuned (Qwen 2.5 1.5B Airflow),102,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator for bucket creation, OracleToGCSOperator for transferring data from Oracle to GCS, and GCSDeleteBucketOperator for bucket deletion. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)'. The comments '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' and the watcher call 'list(watcher(dag))' are also test-related artifacts that should not appear in production code.",1,"The DAG fulfills all requirements from the instruction: it extracts data from Oracle database using OracleToGCSOperator, transfers the data to Google Cloud Storage in CSV format (export_format='csv'), and ensures proper resource management through automated bucket creation (GCSCreateBucketOperator) and deletion (GCSDeleteBucketOperator with TriggerRule.ALL_DONE). The workflow demonstrates enterprise-level best practices for moving relational database content to cloud storage.",0.6666666666666666
768,Fine-tuned (Qwen 2.5 1.5B Airflow),103,0,"The code violates idiomatic Airflow principles by using subprocess calls to AWS CLI commands wrapped in @task decorators (e.g., create_notebook(), create_notebook_instance()) instead of using native Airflow operators. While it does import proper SageMaker operators like SageMakerCreateNotebookInstanceOperator, it doesn't actually use them correctly in the DAG definition. The code also references non-existent operators like 'SageMakerNotebookInstanceOperator' and 'SageMakerNotebookStarterSensor' which don't exist in the standard Airflow provider package.",0,"The code contains multiple hallucinated/non-existent Airflow components: 'SageMakerNotebookInstanceOperator', 'SageMakerNotebookStarterSensor', 'SageMakerNotebookStatusSensor', and 'SageMakerHook.get_task_function()' method which doesn't exist. It also references undefined variables like 'key_pair_name' and 'create_key_pair' function that was never defined. The code is incomplete (cuts off mid-line) and contains invalid AWS CLI commands that don't exist (e.g., 'aws sagemaker register notebook'). This is not production-ready code.",0,"The instruction asks for sequential management of SageMaker notebook resources demonstrating create, manipulate, and cleanup operations with proper task dependencies. While the code attempts to show this pattern, it fails to implement it correctly. The DAG structure is broken with undefined functions, hallucinated operators, and incomplete task definitions. The actual SageMaker operators that were imported (SageMakerCreateNotebookInstanceOperator, SageMakerStartNoteBookOperator, SageMakerStopNotebookOperator, SageMakerDeleteNotebookInstanceOperator) are not properly used. The code doesn't demonstrate a working sequential workflow as requested.",0.0
769,Fine-tuned (Qwen 2.5 1.5B Airflow),104,1,"The code correctly uses the OracleStoredProcedureOperator from airflow.providers.oracle.operators.oracle, which is the idiomatic and native Airflow operator for calling Oracle stored procedures. It does not wrap logic in PythonOperator with hooks, demonstrating proper use of Airflow's provider-specific operators.",0,The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready DAG code.,0,"While the DAG attempts to demonstrate different parameter passing techniques, it has critical issues: 1) The first task has a malformed procedure string with syntax errors (unclosed quote, invalid Oracle syntax with '::varchar2'), 2) The dict-based examples use placeholder/dummy values ('test': 'test', 't': 'est') that don't represent realistic stored procedure calls, 3) The output_columns parameter is used but not properly explained or demonstrated with meaningful procedure names. The code does not provide a working demonstration of input and output parameter handling as requested.",0.3333333333333333
770,Fine-tuned (Qwen 2.5 1.5B Airflow),106,1,"The code uses native Airflow GCS operators (GCSCreateBucketOperator, GCSDeleteBucketOperator, GCSToGCSOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leakage.,0,"The instruction asks for a workflow that demonstrates 'creating, copying, and deleting files and buckets'. While the DAG creates buckets, copies files between buckets, and deletes buckets, it is missing a critical component: there is no task to actually CREATE/upload the source file ('test_copy.txt') to the source bucket before attempting to copy it. The copy operation would fail in practice because the source file doesn't exist. A complete workflow should include a file creation/upload step (e.g., using LocalFilesystemToGCSOperator or GCSCreateObjectOperator) before the copy operation.",0.3333333333333333
771,Fine-tuned (Qwen 2.5 1.5B Airflow),107,1,"The code uses native Airflow operators from the Google Cloud provider package (CloudDatastoreBeginTransactionOperator, CloudDatastoreDeleteOperationOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic approach for Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. These are testing utilities that should not be present in production-ready code.",0,"The instruction explicitly asks for a workflow that 'shows how to start a transaction and then safely roll it back without persisting potentially unwanted data modifications.' While the code does start a transaction using CloudDatastoreBeginTransactionOperator, it does not implement any rollback mechanism. There is no operator or task that demonstrates rolling back the transaction. The CloudDatastoreDeleteOperationOperator is used for deleting operations, not for rolling back transactions. The DAG is incomplete and does not fulfill the core requirement of demonstrating transaction rollback.",0.3333333333333333
772,Fine-tuned (Qwen 2.5 1.5B Airflow),108,0,"The code fails to use the appropriate operator for the main task. The instruction requires extracting data from PostgreSQL and moving it to GCS, but the code uses SQLExecuteQueryOperator instead of the idiomatic PostgresToGCSOperator (which is imported but never used). The SQLExecuteQueryOperator only executes a query without transferring data to GCS. Additionally, the custom @task function 'create_pg_conn' attempts to create a PostgreSQL connection using ComputeEngineHook in a non-standard way, rather than using proper Airflow connection management or appropriate operators.",0,The code contains multiple hallucinated elements: 1) Imports from non-existent modules like 'airflow.providers.google.cloud.hooks.compute._variables' and 'airflow.providers.google.cloud.operators.datajoin' (GCSDatasetCreateOperator doesn't exist). 2) The ComputeEngineHook is misused with fabricated methods like 'build_postgres_instance_request()' that don't exist in the actual Airflow API. 3) References to non-existent fields and methods like 'request.set_body_field()' and 'postgres_connection_profile'. 4) The GCE_BODY structure contains invalid/nonsensical configurations mixing compute instance specs with fictional dataset synchronization specs. The code appears to be a mashup of different concepts that don't exist in standard Airflow 2.8.4.,0,"The code fails to fulfill the core instruction requirements: 1) While PostgresToGCSOperator is imported, it's never actually used - instead SQLExecuteQueryOperator is used which doesn't transfer data to GCS. 2) The infrastructure provisioning is attempted but uses hallucinated/non-existent APIs rather than proper GCP operators. 3) Database connection configuration is attempted through fabricated methods rather than standard Airflow connection management. 4) No actual secure data migration occurs - the SQLExecuteQueryOperator just runs a query without moving data anywhere. 5) The DAG is incomplete (cuts off mid-definition). The solution does not demonstrate a working data extraction and transfer system as requested.",0.0
773,Fine-tuned (Qwen 2.5 1.5B Airflow),109,0,"The code fails to use the actual Sensor operators that are imported at the top of the file (FileSensor, PythonSensor, ShellSensor, TimeSensor, WeekDaySensor). Instead, it attempts to use decorator patterns like @task.sensor, @task.time, @task.weekday which are not valid Airflow sensor patterns. The idiomatic approach would be to instantiate the imported sensor classes (e.g., FileSensor(task_id='wait_for_file', filepath='/tmp/file.txt')) rather than wrapping logic in task decorators. The sensors are imported but never used, and the decorator syntax shown does not represent actual Airflow sensor functionality.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' at the bottom. The __main__ block also references 'watcher()' which is test infrastructure. This is not production-ready code and includes testing artifacts that should not be present in a clean example DAG.",0,"The instruction asks for a workflow demonstrating various sensor types (time-based, file-based, bash command, and Python function sensors) that wait for specific conditions before triggering downstream tasks. While the code attempts to show multiple sensor types, it fails in execution: (1) the sensor implementations are incorrect and non-functional (using invalid decorator patterns), (2) there are no downstream tasks defined to demonstrate the conditional triggering behavior that sensors enable, and (3) the sensors don't actually demonstrate waiting for real conditions (most just return True/False without meaningful logic). The code structure suggests an attempt to fulfill the requirement but lacks proper implementation and the critical element of showing how sensors trigger downstream tasks.",0.0
774,Fine-tuned (Qwen 2.5 1.5B Airflow),110,0,"The code wraps Airflow Operators inside @task decorated functions (PythonOperator pattern) instead of using the Operators directly. For example, `AzureCosmosInsertDocumentOperator` is instantiated inside a `@task` function rather than being used as a standalone task. The operators should be instantiated directly in the DAG context, not wrapped in TaskFlow API functions. This is anti-idiomatic as it creates unnecessary Python callable wrappers around native operators.",1,The code uses only standard Airflow libraries from the official providers package (airflow.providers.microsoft.azure.operators.cosmos). There are no imports from internal testing modules or test harness boilerplate. The code is clean and uses production-ready imports.,0,"The instruction requires monitoring a CosmosDB collection for document availability and triggering insertion based on document existence (dependency-based workflow ensuring document existence before write operations). However, the code creates a collection, inserts a document, deletes it, then queries - which is the opposite flow. There is no monitoring/sensor for document availability, and the workflow doesn't check for document existence before performing write operations. The query task runs AFTER deletion, not before insertion as a validation step. The implementation does not fulfill the core requirement of monitoring for availability and conditional execution.",0.3333333333333333
775,Fine-tuned (Qwen 2.5 1.5B Airflow),112,1,"The code uses the native SqlToSlackOperator from airflow.providers.slack.transfers.sql_to_slack, which is the idiomatic Airflow approach for transferring SQL query results to Slack. It does not wrap logic in a PythonOperator with hooks, but instead uses the purpose-built operator designed for this exact task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing utilities that indicate this is example/test code rather than clean production-ready code.",0,"The code has a critical bug that prevents it from fulfilling the instruction. The slack_conn_id parameter is incorrectly set to SQL_CONN_ID (which is 'presto_default', a SQL connection), when it should be set to a Slack connection ID. This would cause the DAG to fail at runtime. The correct implementation should have sql_conn_id=SQL_CONN_ID and slack_conn_id pointing to an actual Slack connection. While the overall structure attempts to extract SQL data and send to Slack as requested, this configuration error means it cannot successfully execute the monitoring workflow.",0.3333333333333333
776,Fine-tuned (Qwen 2.5 1.5B Airflow),113,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator, S3DeleteBucketOperator, and LocalFilesystemToS3Operator for S3 operations. The file creation uses a TaskFlow @task decorator which is idiomatic for simple Python operations. No unnecessary wrapping of hooks in PythonOperators is present.",0,The code explicitly imports testing modules at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate with 'test_run = get_test_run(dag)'. This is internal testing infrastructure that should not be in production DAG code.,0,"The instruction asks for cleanup operations after uploading the file. However, the 'delete_file' task is misnamed and misconfigured - it uses LocalFilesystemToS3Operator which uploads files, not deletes them. There is no actual cleanup of the local file (/tmp/short.txt) or the uploaded S3 object. The DAG should either delete the local file using a proper cleanup task or delete the S3 object, but instead it attempts to upload again. This fails to properly demonstrate the requested cleanup operations.",0.3333333333333333
777,Fine-tuned (Qwen 2.5 1.5B Airflow),114,1,"The code uses native Airflow HTTP providers and operators appropriately: HttpSensor for checking service availability, HttpPostOperator for POST requests, and HttpGetOperator for GET requests. These are the idiomatic, provider-specific operators rather than wrapping logic in PythonOperator with hooks.",0,The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test harness boilerplate that should not appear in production DAG code.,0,"While the code does implement sensor checks (HttpSensor) and sequential HTTP request tasks (HttpPostOperator, HttpGetOperator), there are critical issues: 1) The HttpSensor is incorrectly configured with 'task_ids' parameter instead of 'task_id', 2) The final task 'task.delete_task_instance' is malformed and not a valid Airflow operator, 3) The workflow logic appears broken with incorrect XCom references. The code does not fulfill the instruction to create a working pipeline that validates and explores HTTP service connectivity through proper sequential execution.",0.3333333333333333
778,Fine-tuned (Qwen 2.5 1.5B Airflow),115,1,"The code uses native Airflow providers and operators designed for Google Cloud DLP tasks (CloudDLPCreateDatasetOperator, CloudDLPCreateInspectionJobOperator, CloudDLPGetInspectionJobOperator, etc.) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach.",0,"The code contains multiple hallucinated elements: 1) 'GCSSensorOperator' does not exist (should be GCSObjectExistsAsyncSensor or similar), 2) 'GCUSensorOperator' is completely fabricated, 3) CloudDLPCreateDatasetOperator and CloudDLPDeleteDatasetOperator are not real operators in the Airflow DLP provider, 4) The DAG is incomplete with unclosed syntax. These are clear hallucinations of non-existent Airflow components.",0,"The instruction requires: 1) uploading custom dictionaries, 2) defining information type patterns, and 3) demonstrating programmatic management of data classification rules. The code only shows inspection job creation/deletion but does NOT include operators for creating custom info types (CloudDLPCreateStoredInfoTypeOperator), creating custom dictionaries, or defining custom detection rules. It focuses on inspection jobs rather than the dynamic configuration of sensitive data detection rules as requested.",0.3333333333333333
779,Fine-tuned (Qwen 2.5 1.5B Airflow),116,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. It does not wrap functionality in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,1,"The DAG fulfills the user instruction completely. It creates a multi-node Dataproc cluster (1 master + 2 workers) dynamically, executes a simple Hive query ('SHOW DATABASES;'), and tears down the cluster afterward using trigger_rule=ALL_DONE to ensure cleanup. This demonstrates scalable, on-demand data infrastructure for lightweight analytical tasks as requested.",0.6666666666666666
780,Fine-tuned (Qwen 2.5 1.5B Airflow),118,0,"The DAG violates idiomatic Airflow patterns by wrapping Operators inside @task decorated functions instead of using them directly. For example, create_file_in_gcs(), create_bucket(), and delete_bucket() instantiate operators (LocalFileSystemToGCSOperator, GCSCreateBucketOperator, GCSDeleteBucketOperator) inside Python functions but never execute them. These operators should be instantiated directly in the DAG context, not wrapped in TaskFlow functions. The correct pattern is to use the operators directly as tasks, not to create them inside @task functions.",1,The code uses only standard Airflow 3.0.1 libraries and Google Cloud provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. All imports are from legitimate airflow.providers.google.cloud packages and core Airflow modules.,0,"The DAG is incomplete (ends with '>> u' suggesting truncation) and has critical logical errors that prevent it from demonstrating the requested functionality. The @task decorated functions create operators but don't execute them, the delete_bucket() function incorrectly creates a bucket instead of deleting it, and the workflow dependencies are incomplete. While it attempts to showcase various GCS sensors (synchronous and asynchronous with deferrable=True), the broken implementation means it cannot actually demonstrate 'file upload completions and object changes' or 'monitoring strategies for cloud storage events' as requested.",0.3333333333333333
781,Fine-tuned (Qwen 2.5 1.5B Airflow),120,0,"The code uses SFTPSensor and references 'LocalFileSystemToSFTPTempSensor' which appears to be incorrectly imported (it's imported as a Sensor but used as a transfer operator). The instruction asks for Docker-based file monitoring between local directories, but the code implements SFTP operations instead. For the actual requirement (local directory monitoring with Docker), the code should use FileSensor for detection and appropriate operators like BashOperator with Docker or DockerOperator for file operations, not SFTP-based operators.",0,"The code explicitly imports and uses test harness modules: 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run'. It also includes test boilerplate like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. Additionally, 'LocalFileSystemToSFTPTempSensor' appears to be a non-existent or hallucinated class (the import path 'airflow.providers.sftp.transfers.local_to_sftp' is incorrect). These are clear signs of test code leakage and hallucination.",0,"The instruction requires: (1) monitoring a source directory for new files, (2) moving files to a destination directory using Docker containers, and (3) printing file contents. The provided code uses SFTP sensors and operations instead of local file system monitoring with Docker. It doesn't implement file moving logic, doesn't use Docker containers as specified, and doesn't include any task to print file contents. The code is essentially an SFTP example that doesn't match the user's requirements at all.",0.0
782,Fine-tuned (Qwen 2.5 1.5B Airflow),121,1,"The code correctly uses the TrinoOperator from airflow.providers.trino.operators.trino, which is the idiomatic and native way to execute Trino queries in Airflow. It does not wrap Trino logic in PythonOperator with hooks, demonstrating proper use of Airflow providers.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code, indicating hallucination/leakage from test examples.",0,"The instruction explicitly requests a DAG that demonstrates: (1) creating schemas, (2) defining tables, (3) inserting data, (4) template-based queries, (5) parameterized queries, (6) dependency chaining, and (7) error handling. The provided code only executes two simple SELECT statements with parameterization. It does NOT create schemas, define tables, insert data, or demonstrate error handling capabilities. It only partially shows dependency chaining (task1 >> task2) and parameterized queries, missing the majority of the requested functionality.",0.3333333333333333
783,Fine-tuned (Qwen 2.5 1.5B Airflow),122,0,"The DAG uses a PythonOperator (via @task decorator) with S3Hook to populate the S3 bucket instead of using the native S3CreateObjectOperator or S3FileTransformOperator. The populate_s3_bucket() function wraps S3Hook logic in a Python callable, which is a non-idiomatic pattern when specific operators exist for S3 file operations.",0,"The code explicitly imports from internal testing modules (from tests_common.test_utils.system_tests import get_test_run) and includes test harness boilerplate (test_run = get_test_run(dag)). Additionally, it contains TEST SETUP/TEST BODY/TEST TEARDOWN comments that are test-specific. The code also references non-existent operator names: S3BucketCreateOperator and S3BucketDeleteOperator instead of the correct S3CreateBucketOperator and S3DeleteBucketOperator (though S3CreateBucketOperator is imported correctly, it's used incorrectly as S3BucketCreateOperator).",1,"The DAG fulfills the user instruction by demonstrating: (1) provisioning cloud resources (S3 bucket creation), (2) performing data transfer (populating S3 bucket and transferring to SQL), and (3) cleaning up resources (S3 bucket deletion with ALL_DONE trigger rule). The workflow showcases the complete end-to-end process of resource creation, data movement, and teardown as requested.",0.3333333333333333
784,Fine-tuned (Qwen 2.5 1.5B Airflow),123,0,"The code wraps CohereEmbeddingOperator inside a PythonOperator's lambda function instead of using CohereEmbeddingOperator directly as a task. The correct idiomatic approach would be to instantiate CohereEmbeddingOperator directly in the DAG context (e.g., `CohereEmbeddingOperator(task_id='cohere_embed_task', input_text=[...])`) rather than calling it within a python_callable. Additionally, calling `.expand()` on an operator inside a lambda is not standard Airflow usage.",0,The code explicitly imports test harness modules at the bottom: `from tests_common.test_utils.system_tests import get_test_run` and uses `test_run = get_test_run(dag)`. These are internal testing utilities that should not appear in production DAG code. This is a clear example of test boilerplate leakage.,0,"While the code attempts to use Cohere embedding functionality, it does not properly implement a working DAG that transforms text into embeddings. The CohereEmbeddingOperator is incorrectly wrapped in a PythonOperator lambda, and the second task 'pass_by_xcom_cohere_embed_task' references an undefined 'output' variable and doesn't properly retrieve or process the embedding results. The DAG does not demonstrate a functional pipeline for transforming raw text into dense vector representations as requested.",0.0
785,Fine-tuned (Qwen 2.5 1.5B Airflow),124,0,"While the code does use OracleStoredProcedureOperator (which is idiomatic), it then defines a Python function 'execute_stored_procedure_func()' that attempts to manually execute SQL using cursor operations. This function is not wrapped in a PythonOperator and cannot be used as a task. More critically, the instruction asks for 'different methods of executing database operations' but the code only shows one operator (OracleStoredProcedureOperator) and fails to demonstrate other idiomatic approaches like OracleOperator for basic SQL queries. The code should have used multiple Oracle operators to showcase different methods.",0,"The code explicitly imports from 'tests.system.utils' (line: 'from tests.system.utils import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). This is internal testing code that should not appear in production DAG examples. Additionally, the execute_stored_procedure_func() references 'self.conn' which is undefined and would cause runtime errors.",0,"The instruction asks to 'demonstrate basic SQL query execution AND stored procedure interactions' with 'different methods of executing database operations'. The code only attempts to show stored procedure execution (and does so incorrectly). It completely misses basic SQL query execution using OracleOperator. The execute_stored_procedure_func() is not properly implemented as an Airflow task and contains broken code (undefined 'self', 'stored_procedure_sql'). The task dependency 'store_procedure_obj >> execute_stored_procedure_func()' is invalid as the function is not a task object.",0.0
786,Fine-tuned (Qwen 2.5 1.5B Airflow),125,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, SFTPToGCSOperator for data transfer, and GCSDeleteBucketOperator for cleanup. It does not wrap these operations in PythonOperator with hooks, which would be non-idiomatic. The use of specialized transfer operators (SFTPToGCSOperator) is exactly what idiomatic Airflow code should do.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production DAG code. Additionally, comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' indicate this is test code rather than production-ready code.",1,"The DAG fulfills all requirements from the instruction: (1) dynamically creates a GCS bucket using GCSCreateBucketOperator, (2) transfers data using a specialized operator (SFTPToGCSOperator), and (3) cleans up cloud resources after transfer is complete using GCSDeleteBucketOperator with TriggerRule.ALL_DONE to ensure cleanup happens regardless of transfer success/failure. The business logic requested is fully implemented.",0.6666666666666666
787,Fine-tuned (Qwen 2.5 1.5B Airflow),126,1,"The code uses the appropriate SQLExecuteQueryOperator from airflow.providers.common.sql.operators.sql, which is the idiomatic way to execute SQL queries in Airflow. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for SQL execution tasks.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing modules and test execution code. This is a hallucination/leakage issue.",0,"The instruction asks for a DAG that 'sequentially executes SQL tasks' (plural) with 'clear dependency management' showing 'how tasks can be chained together' to perform a 'complete database operation lifecycle' illustrating 'task ordering'. However, the provided code only contains a single SQL task with no dependencies, no chaining, and no demonstration of a complete database operation lifecycle (e.g., create table -> insert data -> validate -> cleanup). It does not fulfill the requirement of showing multiple sequential tasks with dependency management.",0.3333333333333333
788,Fine-tuned (Qwen 2.5 1.5B Airflow),127,1,"The code uses native Airflow operators designed for GCS operations: GCSCreateBucketOperator, GCSDeleteBucketOperator, and GCSTimeSpanFileTransformOperator from airflow.providers.google.cloud.operators.gcs. It does not wrap functionality in PythonOperator with hooks, but instead uses the appropriate provider operators for the task.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test-related code like 'list(dag.tasks) >> watcher()' and 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than clean production-ready code.",1,"The DAG fulfills the user instruction requirements: (1) performs time-span based file processing in GCS using GCSTimeSpanFileTransformOperator with a timespan parameter, (2) applies custom transformation script to selected files from source bucket (using apply_transform with CREATE, OVERWRITE, DELETE actions), (3) transfers results to destination bucket, (4) includes automated setup (create_bucket_src, create_bucket_dst), (5) includes transformation (transform_timespan), and (6) includes cleanup (delete_bucket_src, delete_bucket_dst). All key components requested are present and properly orchestrated.",0.6666666666666666
789,Fine-tuned (Qwen 2.5 1.5B Airflow),128,0,"The code wraps ADLS operators inside @task decorated functions (create_files() and delete_files()), which is anti-idiomatic. The operators should be instantiated directly in the DAG context, not inside PythonOperator tasks. The correct approach would be to use ADLSCreateObjectOperator and ADLSDeleteOperator as standalone tasks in the DAG, not wrapped in Python callables.",0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. This is internal testing infrastructure that should not appear in production DAG code. Additionally, the code references non-existent parameters like 'cmd_cmd_parts', 'task_open_line', and 'wait_for_status' which are not valid parameters for these operators, suggesting hallucinated API signatures.",0,"The instruction asks for a workflow that demonstrates file upload and immediate deletion with basic file management operations. While the code attempts to create and delete files, it does not properly implement the workflow. The operators are instantiated inside task functions but never executed or linked in a proper task dependency chain. There's no actual file upload happening (the code tries to use shell commands inappropriately), and the task dependencies are not established (create_files() and delete_files() are called but not chained with >>). The workflow would not function as intended.",0.0
790,Fine-tuned (Qwen 2.5 1.5B Airflow),129,0,"The code does not use any Redis-specific operators or sensors. The instruction requires using Redis as a message broker and synchronization mechanism, which should be implemented using RedisPublishOperator and RedisSensor from airflow.providers.redis.operators and airflow.providers.redis.sensors. Instead, the code only contains basic PythonOperator tasks (via @task decorator) that print 'Hello' and 'World' without any Redis interaction.",0,"The code imports and uses 'from tests_common.test_utils.watcher import watcher' which is a test harness utility, not production code. This is explicitly a testing module that should not appear in production DAG code. The watcher() task is added at the end for test purposes only.",0,"The code completely fails to implement the requested functionality. The instruction specifically asks for: (1) Redis as a message broker, (2) publishing a start signal, (3) waiting for a confirmation key, and (4) synchronization mechanism before advancing to subsequent tasks. The provided code is a basic example DAG with two simple print tasks and no Redis integration whatsoever. It does not demonstrate inter-service communication or any synchronization mechanism.",0.0
791,Fine-tuned (Qwen 2.5 1.5B Airflow),130,0,"The code wraps SQL execution logic in a Python function `_run_pipeline()` that manually calls `sql_to_sql_task.execute()`, which is anti-pattern in Airflow. The function is called directly at the module level rather than being used as a task in the DAG. Additionally, the SQLToSQLOperator is instantiated and executed inside a Python function rather than being declared as a proper task in the DAG context. This defeats the purpose of using Airflow's task orchestration and is not idiomatic.",0,The code explicitly imports testing utilities (`from tests_common.test_utils.system_tests import get_test_run`) and includes test harness boilerplate (`test_run = get_test_run(dag)`). These are internal testing modules that should not appear in production DAG code. This is a clear hallucination/leakage issue.,0,"The instruction requires: (1) daily schedule - the DAG has `schedule=None`, (2) reliable trigger and monitor pipeline execution - the pipeline_name uses an XCom pull from a non-existent task 'get_pipeline_id', (3) appropriate error handling and logging - no error handling is implemented, (4) the DAG has a broken dependency `run_pipeline >> os.environ['AZURE_SYNAPSE_ACCOUNT']` which attempts to use a string as a task. The code also calls `_run_pipeline()` at module level outside the DAG context, which won't execute as a task. Multiple critical requirements are not met.",0.0
792,Fine-tuned (Qwen 2.5 1.5B Airflow),131,0,"The code uses WinRMOperator which is idiomatic, but it wraps WinRMHook logic inside a Python callable function (winrm_hook_callable) that is then called directly in the DAG context rather than using a proper task decorator or operator. The function creates a WinRMOperator inside itself but never executes it properly, and manually calls hook.connect()/disconnect() which is anti-pattern. The code also has a malformed chain() call with 't2=t1' which is not valid Airflow syntax.",0,"The code explicitly imports and uses test harness code: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. This is test infrastructure leakage and not production-ready code. Additionally, it references 'os.environ.get(""SYSTEM_TESTS_ENV_ID"")' without importing 'os', and includes comment markers like '[START howto_winrm_hook]' that are typical of documentation/test examples.",0,"The instruction asks for a distributed task automation DAG with multiple system commands executed across remote Windows infrastructure, with parallel task execution and dependency tracking simulating real-world scenarios. The provided code only creates a single WinRMOperator task (t1) with no actual commands specified, no parallel execution, no meaningful dependency chain, and no simulation of real-world server management scenarios. The winrm_hook_callable function is not properly integrated as a task and the chain syntax is broken.",0.0
793,Fine-tuned (Qwen 2.5 1.5B Airflow),132,0,The code uses PythonOperator (via @task decorator) wrapping Hook calls (MemoracleMemcachedHook) instead of using native Operators. Idiomatic Airflow would use dedicated Operators like MemorystoreMemcachedCreateInstanceOperator and MemorystoreMemcachedDeleteInstanceOperator from airflow.providers.google.cloud.operators.memorystore package.,0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. Additionally, the Hook name 'MemoracleMemcachedHook' appears to be incorrect/hallucinated - the actual Airflow provider uses 'CloudMemorystoreMemcachedHook'. The code also includes test-specific comments like '# TEST SETUP' and '# TEST TEARDOWN'.",0,"The instruction explicitly requests a Cloud Bigtable pipeline with dynamic resource allocation, replication monitoring, and systematic teardown. The provided code implements a Memorystore Memcached solution with BigQuery components instead. This is a completely different Google Cloud service (Memcached vs Bigtable) and does not address the core requirements of Bigtable provisioning, replication monitoring, or configurable parameters for a NoSQL database infrastructure.",0.0
794,Fine-tuned (Qwen 2.5 1.5B Airflow),133,1,"The DAG uses appropriate Airflow providers and operators for the task. It uses GCSCreateBucketOperator, GCSDeleteBucketOperator, and SFTPToGCSOperator from the official Google Cloud and SFTP providers. While there is a @task decorator used for creating the SFTP connection, this is reasonable as there's no dedicated operator for dynamic connection creation. The core data transfer logic uses the native SFTPToGCSOperator rather than wrapping it in a PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also adds a watcher() task to the DAG with 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be in production code.",1,"The DAG fulfills the user instruction to build an automated data transfer pipeline that moves a CSV file between cloud storage platforms (SFTP to GCS) with robust testing and teardown mechanisms. It implements: 1) Infrastructure-as-code by programmatically creating a GCS bucket, 2) Data transfer using SFTPToGCSOperator, 3) Teardown mechanism by deleting the bucket with trigger_rule=TriggerRule.ALL_DONE to ensure cleanup happens regardless of success/failure. The pipeline demonstrates create, transfer, and delete operations as requested.",0.6666666666666666
795,Fine-tuned (Qwen 2.5 1.5B Airflow),134,1,"The code uses specific Google Analytics Admin operators from the official Airflow provider package (airflow.providers.google.marketing_platform.operators.analytics). It leverages native operators like GoogleAnalyticsAdminInsertPropertyOperator, GoogleAnalyticsAdminListPropertiesOperator, GoogleAnalyticsAdminDeleteDataStreamOperator, etc., rather than wrapping API calls in PythonOperators with hooks. This is the idiomatic Airflow approach.",0,"The code contains multiple hallucinated/non-existent components: (1) 'TaskResultSensor' is imported/used but never defined in the imports and doesn't exist in standard Airflow, (2) 'CheckTaskCompletion' is used but not imported and doesn't exist in standard Airflow, (3) The code is incomplete (cuts off at 'GoogleAnalyticsAdmi'), (4) Contains a Chinese comment '但未影响已经存在的数据' which appears unprofessional in production code, (5) The SQL query syntax in check_property_ids is invalid/nonsensical for this context. These are clear signs of hallucination and non-production-ready code.",0,"While the DAG attempts to demonstrate creating, listing, and deleting properties and data streams as requested, it fails to deliver a complete, working solution. The code is incomplete (truncated), uses non-existent sensors/operators that would prevent execution, and lacks proper task dependencies to demonstrate a comprehensive workflow. The delete operations are referenced in imports but never actually implemented in the DAG. A comprehensive demonstration requires complete, executable code with clear task dependencies showing the full lifecycle.",0.3333333333333333
796,Fine-tuned (Qwen 2.5 1.5B Airflow),135,1,"The code uses the specific YCSQLOperator from airflow.providers.yandex.operators.yandex_cloud, which is the idiomatic way to execute SQL queries on Yandex Cloud. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,"The code includes test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing modules and test infrastructure that have no place in a production DAG.",0,"The instruction explicitly requests two components: 1) Execute a trivial SQL query, and 2) Mark completion with a final empty operator. While the code successfully implements the SQL query execution using YCSQLOperator, it completely omits the final empty operator (e.g., EmptyOperator) that should mark the completion of the workflow. This is a clear miss of a key requirement.",0.3333333333333333
797,Fine-tuned (Qwen 2.5 1.5B Airflow),136,1,"The code uses appropriate Airflow operators and patterns. It leverages the @task.branch() decorator for branching logic, EmptyOperator for placeholder tasks, and proper trigger rules. The branching is implemented idiomatically using Airflow's native decorators rather than wrapping everything in generic PythonOperators.",1,"The code uses only standard Airflow libraries and operators (airflow.decorators, airflow.models.dag, airflow.providers.standard.operators). There are no imports from internal testing modules or test harness boilerplate. The code is clean and production-ready.",0,"The instruction specifically requested 'advanced branching techniques using different Python execution contexts' including 'random selection, external Python execution, and virtual environment task execution'. While the code demonstrates branching with @task.branch(), it fails to implement: (1) Random selection logic (the branch_func references task.args['RANDOM_STRING'] which is undefined and would cause runtime errors), (2) External Python execution (no ExternalPythonOperator or @task.external_python), and (3) Virtual environment task execution (no @task.virtualenv or VirtualenvOperator). The code only shows basic branching patterns and does not fulfill the key requirements of demonstrating different Python execution contexts.",0.6666666666666666
798,Fine-tuned (Qwen 2.5 1.5B Airflow),137,1,"The code uses native Airflow providers and operators appropriately for BigQuery and GCS operations. It imports and uses BigQueryCancelJobOperator, BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryToBigQueryOperator, GCSCreateBucketOperator, and other specific operators from airflow.providers.google.cloud rather than wrapping logic in PythonOperators with hooks. This is idiomatic Airflow usage.",0,"The code contains multiple issues indicating hallucination/test leakage: (1) Imports pytest which is a testing framework not needed in production DAGs, (2) Uses pytest.skip() which is test harness code, (3) Contains syntax errors like 'TABLE_MYSQL SOURCE' and 'TABLE_BIGQUERY TARGET' (invalid Python variable names with spaces), (4) References undefined variables like 'SOURCE' and uses malformed SQL syntax 'FROM STAGING.BUCKET', (5) Has incomplete code with 'show_jobs' list defined but never used, (6) Uses templated '{{ job_id }}' without defining where job_id comes from. These issues suggest this is test/example code that was not properly cleaned for production use.",0,"The instruction asks for a pipeline that 'simulates a controlled database transaction process with explicit start and cancellation steps' to 'demonstrate transaction management that ensures data consistency and prevents unintended persistent changes.' While the code includes BigQueryCancelJobOperator (cancellation) and cleanup operators, it does not implement a coherent transaction simulation workflow. The tasks are not properly connected (no task dependencies defined), the cancel_job task references an undefined job_id template variable, and there's no clear demonstration of starting a transaction, performing operations, and then canceling to prevent persistent changes. The code appears to be a broken example rather than a functional transaction management demonstration.",0.3333333333333333
799,Fine-tuned (Qwen 2.5 1.5B Airflow),139,0,"While the code attempts to use a VectorApplyOperator, it relies on non-standard utilities like 'VectorUtilsTestContext' which appears to be a test harness rather than a production operator pattern. The code does not demonstrate proper use of established Airflow vector providers (e.g., Pinecone, Weaviate, or other vector database operators). A proper implementation would use specific vector database operators with clear connection configurations.",0,"The code imports from 'system_tests_utils.vector_utils' (get_vector_index_name) and uses 'VectorUtilsTestContext' from 'airflow.providers.vector.applications.api.utils', which are clearly test utilities and not production Airflow libraries. The presence of test context objects and system test utilities indicates this is test harness boilerplate that has leaked into the generated code. Additionally, there's an undefined variable 'test_context_task_output' in the task dependencies.",0,"The instruction requested a DAG that demonstrates automated vector data insertion with configurable parameters (index name, namespace, batch size) and shows how to dynamically load vector embeddings from environment variables or predefined lists. The generated code fails to: (1) show actual vector embedding data insertion, (2) implement configurable batch size or namespace parameters, (3) demonstrate loading embeddings from environment variables or lists, (4) provide any actual vector data or embeddings. The code only contains a test context wrapper without real vector operations.",0.0
800,Fine-tuned (Qwen 2.5 1.5B Airflow),140,0,"The code uses 'ExcelToAzureBlobOperator' which does not exist in the official Apache Airflow providers. The actual Azure provider does not have an 'airflow.providers.microsoft.azure.operators.excel' module. This appears to be a hallucinated/non-existent operator. For enterprise BI workbook refresh scenarios (like Tableau, Power BI, or Excel Services), the proper approach would use actual operators like TableauOperator, PowerBIOperator, or custom operators with appropriate hooks.",0,"The code contains multiple hallucinations: (1) It imports from 'tests_common.test_utils.system_tests' which is internal testing infrastructure and should not be in production code, (2) It includes 'test_run = get_test_run(dag)' which is test harness boilerplate, and (3) The 'ExcelToAzureBlobOperator' and the module path 'airflow.providers.microsoft.azure.operators.excel' do not exist in the official Airflow Azure provider.",0,"The instruction asks for 'enterprise-level data extract refresh techniques for business intelligence reporting systems' with 'trigger workbook refreshes and monitor their completion status using different execution strategies.' The provided code only shows a single task that allegedly copies an Excel file to blob storage, with no refresh triggering, no completion monitoring, no different execution strategies, and no actual BI system integration. It completely misses the core requirements of the instruction.",0.0
801,Fine-tuned (Qwen 2.5 1.5B Airflow),141,0,"The code fails to use the appropriate AWS DataBrew operators. It incorrectly uses GlueStartJobOperator for DataBrew jobs when Airflow provides specific DataBrew operators (e.g., GlueDataBrewStartJobRunOperator). The code also uses DatabrewJobSensor correctly but pairs it with the wrong job execution operator. Additionally, it wraps connection creation logic in a custom @task decorator instead of using native Airflow connection management patterns.",0,"The code imports and uses 'system.amazon.aws.utils' which appears to be an internal testing module (SystemTestContextBuilder, prune_logs). It also references undefined variables like 'glue_job_hook' and 'databrew_job_hook' that are never instantiated. The code includes test harness patterns like 'sys_test_context_task' and 'ENV_ID_KEY' which are typical of Airflow system test boilerplate rather than production code.",0,"The instruction asks for an AWS Glue DataBrew data profiling workflow with creating, executing, and cleaning up a data processing job. However, the code: (1) uses Glue operators instead of DataBrew operators for job execution, (2) does not create a DataBrew dataset or profile job as required for profiling, (3) has incomplete/broken task dependencies (the DAG definition is cut off), (4) incorrectly attempts to create connections using custom tasks rather than proper DataBrew resource setup, and (5) does not demonstrate actual data profiling capabilities of DataBrew. The code appears to be a confused mixture of Glue and DataBrew concepts.",0.0
802,Fine-tuned (Qwen 2.5 1.5B Airflow),142,0,"The code violates idiomatic Airflow patterns by calling a Python function `create_generation_training_job()` at DAG parse time to construct the body parameter for the operator. This function should either be removed and the body constructed directly in the operator call, or the entire logic should use proper task dependencies. Additionally, the function uses a Hook (`CloudVertexGenHook`) to create a training job, which is then passed to an Operator - this is redundant and anti-pattern. The Operator itself should handle the job creation.",0,"The code explicitly imports test harness modules at the end: `from tests_common.test_utils.system_tests import get_test_run` and uses `test_run = get_test_run(dag)`. This is test boilerplate that should not be in production code. Additionally, the operators `CreateGenerativeTrainingJobOperator` and `DeleteGenerativeTrainingJobOperator` appear to be fabricated - these are not standard Airflow 3.0.6 operators in the Google Cloud provider package. The actual Vertex AI provider uses different operator names and patterns.",0,"The instruction asks for a DAG that demonstrates fine-tuning a Gemini model using supervised learning with specific training data. The code fails to provide: (1) any actual training dataset configuration or reference, (2) proper Gemini/generative model specification, (3) supervised learning configuration details, (4) any demonstration of how to prepare or use training data. The placeholder values like empty strings for 'image_uri', 'display_name', and 'training_script' make this non-functional. The code also contains nonsensical elements like 'cluster_stack' which is not relevant to Vertex AI model tuning, and the bizarre line `run_this_last = dag.tasks[DAG_ID].as_short_circuit()` which references a non-existent task.",0.0
803,Fine-tuned (Qwen 2.5 1.5B Airflow),143,1,"The code correctly uses TrinoOperator from airflow.providers.trino.operators.trino, which is the idiomatic and native operator for executing Trino queries. It does not wrap logic in PythonOperator with hooks, demonstrating proper use of Airflow providers.",0,"The code is heavily contaminated with test harness boilerplate and imports from internal testing modules. It includes 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'from tests_common.test_utils.system_tests import get_test_run_logger', and test execution logic like 'test_run = next(test_runs)'. Additionally, there's a syntax error with 'DAGunts' instead of 'DAG'. This is clearly not production-ready code.",0,"The instruction asks for a DAG that 'sequentially executes SQL operations' demonstrating 'task dependencies and progressive data transformation steps' including 'create schemas, generate tables, insert data, and perform template-based and parameterized queries'. The provided code creates multiple separate DAGs with single tasks each, showing no sequential execution or task dependencies. It does not create schemas, generate tables, or insert data. It only demonstrates various ways to call TrinoOperator with different parameters across multiple DAGs, which does not fulfill the core requirement of a single workflow with progressive transformation steps.",0.3333333333333333
804,Fine-tuned (Qwen 2.5 1.5B Airflow),144,0,"While the DAG does use SnowflakeOperator and SqlExecuteQueryOperator in several places, the critical 'create_snowflake_external_table' task wraps SnowflakeHook logic inside a @task decorator (essentially a PythonOperator). This should have been implemented using SnowflakeOperator with a SQL statement directly, which is the idiomatic Airflow pattern. The instruction asks for 'Airflow's Snowflake integration' and 'flexible file parsing and consistent data loading', but the code uses a PythonOperator pattern for table creation instead of the native operator.",0,"The code contains several issues indicating it's derived from test/example code: (1) Uses ENV_ID pattern typical of system tests (e.g., 'SYSTEM_TESTS_ENV_ID'), (2) Contains hardcoded credentials (SNOWFLAKE_USER, SNOWFLAKE_PASSWORD) which is not production-ready, (3) Uses 'SnowflakeExternalTableGetId' utility that appears to be a test helper, (4) The 'set_up_connection_snowflake' task attempts to create a connection inline which is not standard practice, (5) Multiple incomplete/broken SQL statements (e.g., 'TriggerRule.ALL_' is truncated, invalid JOIN syntax without FROM clause). The code is not clean or production-ready.",0,"The instruction asks to 'ingest CSV files from an external data source into a predefined Snowflake table' with 'flexible file parsing and consistent data loading'. The DAG fails to meet this requirement: (1) No CSV file ingestion is implemented - the 'load_data_snowflake' task just inserts hardcoded values, not CSV data, (2) No file parsing logic exists, (3) No external data source reading (S3, FTP, etc.) is configured, (4) The EXTERNAL TABLE creation is malformed and uses non-standard Snowflake syntax, (5) The DAG focuses on schema/table management operations (create, drop, delete) rather than actual ETL data movement from CSV files. The core business requirement of CSV ingestion is completely missing.",0.0
805,Fine-tuned (Qwen 2.5 1.5B Airflow),145,0,"While the code does use a specific provider operator (AzureDataFactoryTriggerOperator), it uses the wrong operator for Microsoft Fabric. The code imports from 'airflow.providers.microsoft.azure.operators.data_factory' which is for Azure Data Factory, not Microsoft Fabric. Microsoft Fabric would require different operators or the Fabric-specific provider. The operator usage itself is idiomatic, but it's the wrong service entirely.",0,"The code contains multiple hallucinations: (1) It imports test harness code at the bottom: 'from tests_common.test_utils.system_tests import get_test_run_from_test_group' and uses 'test_run = get_test_run_from_test_group(dag.group)', which is test boilerplate that should not be in production code. (2) It attempts to import 'get_azure_rds_db_name' from a non-existent module path 'airflow.providers.microsoft.azure.translations imports.utils' (note the malformed import path with 'imports' instead of a dot). (3) The 'backup_details' parameter with 'database_name' appears to be fabricated and not a real parameter for AzureDataFactoryTriggerOperator.",0,"The instruction specifically asks for a DAG that demonstrates scheduling and executing a data pipeline in Microsoft Fabric. However, the code uses Azure Data Factory operators instead of Microsoft Fabric operators. Microsoft Fabric and Azure Data Factory are different services with different APIs and operators. The code does not fulfill the core requirement of working with Microsoft Fabric's cloud-native orchestration mechanism.",0.0
806,Fine-tuned (Qwen 2.5 1.5B Airflow),146,1,"The code uses native Airflow providers and operators designed for AWS services. It imports and uses DataSyncOperator, DataSyncSensor, S3CreateBucketOperator, and S3CreateObjectOperator from airflow.providers.amazon.aws, which are the idiomatic way to interact with AWS services in Airflow. The code also uses @task decorators for custom logic, which is the modern TaskFlow API approach. No generic PythonOperator wrapping of hooks is present.",0,The code imports 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder' which is clearly a testing module from Airflow's internal test suite. This is test harness boilerplate that should not appear in production DAG code. The line 'sys_test_context_task = SystemTestContextBuilder().build()' further confirms this is test infrastructure leakage.,0,"While the code attempts to address AWS DataSync integration with S3 buckets, it is incomplete (truncated at '@task(trigger_r'). More critically, the instruction asks for 'creating, executing, and cleaning up a DataSync task' with 'complete lifecycle management workflow that demonstrates task creation, execution, and resource cleanup'. The visible code shows setup tasks but does not show the actual DataSync task execution, sensor monitoring, or cleanup tasks (like deleting the S3 buckets after completion). The DAG structure with chain() or task dependencies is not visible, making it impossible to verify the complete lifecycle is implemented.",0.3333333333333333
807,Fine-tuned (Qwen 2.5 1.5B Airflow),147,0,"The code does not use proper Airflow operators idiomatically. It attempts to use 'CopySFTPFileTransferrer' as if it were an operator by calling it directly with 'sftp_transfer_task()', but this is not the correct pattern. It should use 'SFTPOperator' or similar provider-specific operators. Additionally, 'remove_tmp_sftp_folder_task' is defined as a lambda function without being wrapped in a proper operator like PythonOperator or TaskFlow API decorator. The connection management is also non-idiomatic - connections should be configured in Airflow's connection management system, not instantiated directly in DAG code.",0,The code explicitly imports and uses testing modules that should not be present in production DAG code: 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test harness boilerplate like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are clear indicators of test code leakage into what should be production DAG code.,0,"The instruction requested a data migration DAG demonstrating file transfer between cloud storage buckets with explicit error handling and resource management. However, the code uses SFTP (not cloud storage buckets like S3, GCS, or Azure Blob), lacks explicit error handling mechanisms (no try-catch blocks, error callbacks, or retry logic explicitly shown), and does not demonstrate proper resource management patterns. The instruction specifically mentioned 'cloud storage buckets' and 'different storage locations', but SFTP servers are not cloud storage buckets in the typical sense (S3, GCS, Azure Blob Storage).",0.0
808,Fine-tuned (Qwen 2.5 1.5B Airflow),148,0,"While the DAG does use some native Airflow operators like Ec2CreateInstanceOperator and Ec2DeleteInstanceOperator in the imports, the actual implementation relies heavily on @task decorated functions that wrap boto3 client calls directly (e.g., create_instance() and launch_template_assign_ami() use Ec2Hook().get_boto3_client()). This is the anti-pattern of wrapping Hook logic in PythonOperator-style tasks instead of using the native operators that were imported. The code should use Ec2CreateInstanceOperator directly rather than creating custom task functions with boto3 clients.",0,"The code contains clear hallucinations and test harness boilerplate. It imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, get_test_run' which are internal testing modules. It also includes test-specific code like 'sys_test_context_task = SystemTestContextBuilder()...', 'system_test_before_run()' function, and references to test contexts. This is not production-ready code and includes test infrastructure that should not be present in a production DAG.",0,"The DAG is incomplete and does not fulfill the instruction requirements. The instruction asks for: (1) provision a compute resource, (2) execute predefined remote commands, and (3) systematically tear down infrastructure. The code is truncated and incomplete (cuts off mid-function), lacks any task that executes remote commands on the provisioned instance (no SSH operator, SSM operator, or similar), and while it imports Ec2DeleteInstanceOperator, there's no visible implementation of the teardown logic. The workflow chain is incomplete and doesn't demonstrate the full lifecycle requested.",0.0
809,Fine-tuned (Qwen 2.5 1.5B Airflow),149,0,"The code wraps InfluxDBHook methods inside @task decorated functions (PythonOperator equivalent) instead of using the InfluxDBOperator that was explicitly requested in the instruction. The user asked to 'demonstrate interaction with InfluxDB using the InfluxDBOperator', but the implementation uses InfluxDBHook with TaskFlow API decorators, which is the anti-pattern described in the scoring criteria.",0,"The code contains clear test harness boilerplate and imports from internal testing modules: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production-ready code.",0,"The instruction specifically requested: (1) Use InfluxDBOperator to interact with InfluxDB, (2) Execute a Flux query, (3) Retrieve data from a predefined bucket with a configurable time window. The code fails on multiple counts: it doesn't use InfluxDBOperator, it doesn't execute a proper Flux query (the query syntax appears incorrect), and it doesn't demonstrate retrieving data with a configurable time window as requested. Instead, it focuses on creating buckets and writing data, which wasn't the primary requirement.",0.0
810,Fine-tuned (Qwen 2.5 1.5B Airflow),153,0,"While the code uses native Google Cloud Tasks operators (CloudTasksCreateTaskQueueOperator, CloudTasksGetTaskQueueOperator, etc.), it completely fails to implement the required sensor functionality. The instruction explicitly asks for 'a sensor task that pauses workflow execution until the designated queue is completely empty.' The code should use CloudTasksQueueEmptySensor or a similar sensor operator from airflow.providers.google.cloud.sensors.tasks, but instead only uses standard operators. Additionally, there's an instantiation of CloudTasksHook outside of any operator context (hook = CloudTasksHook(service_acc=default_service_acc)) which is not used, suggesting poor understanding of idiomatic Airflow patterns.",0,"The code contains clear indicators of test harness boilerplate that should not appear in production code. Specifically: ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID'), PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), and comments like '# [START howto_cloud_tasks_queue]' and '# [END howto_cloud_tasks_queue]' which are documentation generation markers typically found in example/test code. The variable naming convention 'SYSTEM_TESTS_*' is a dead giveaway this is test infrastructure code, not production-ready.",0,"The code completely fails to meet the core requirement of the instruction. The user asked for 'an Airflow monitoring DAG that validates the completion status of a specific cloud task queue' with 'a sensor task that pauses workflow execution until the designated queue is completely empty.' Instead, the provided code is a comprehensive example DAG that creates, updates, lists, runs, and deletes task queues - essentially CRUD operations. There is no sensor implementation whatsoever, no monitoring logic, and no validation of queue completion status. The DAG demonstrates various Cloud Tasks operators but does not fulfill the specific business logic requested.",0.0
811,Fine-tuned (Qwen 2.5 1.5B Airflow),154,0,"The code fails to use the WinRMOperator that is imported at the top. Instead, it wraps WinRMHook calls inside PythonOperator-style @task decorators. The instruction asks to 'illustrate remote task scheduling and execution using WinRM hooks', but idiomatic Airflow would use WinRMOperator for executing commands on Windows machines. The code also uses httpx.Client to make HTTP calls instead of actually using WinRM protocol, which is not the proper way to use WinRMHook.",0,"The code contains multiple hallucinations and nonsensical patterns: (1) Uses httpx.Client with HTTP POST to localhost instead of actual WinRM protocol execution, (2) References undefined variable 'ssh_client' in list_remote_commands() function, (3) Uses non-existent hook methods like 'get_command' that don't exist in WinRMHook API, (4) Contains bizarre task decorators with invalid parameters like @task(path='/tmp/sending.txt') and @task(external_files=True), (5) Imports Windows-specific libraries (pywintypes, win32com.client) that wouldn't work in typical Airflow environments, (6) Has duplicate START/END comment markers and malformed documentation blocks, (7) Calls example_winrm_operator() twice at the end. The code appears to be a confused mix of SSH, WinRM, and generic Python concepts.",0,"The instruction asks for: (1) Remote task scheduling using WinRM to connect to Windows machines, (2) Parallel execution pattern with different command types running concurrently, (3) A final consolidation task. The code fails on all counts: It doesn't actually execute WinRM commands properly (uses fake HTTP calls instead), there is no parallel execution pattern (tasks run sequentially: create_task >> list_task >> remove_task), and there is no consolidation task that aggregates results from parallel executions. The code also has multiple disconnected task definitions that aren't part of any coherent DAG structure.",0.0
812,Fine-tuned (Qwen 2.5 1.5B Airflow),155,1,"The code uses native Google Cloud Run operators from airflow.providers.google.cloud.operators.cloud_run, including CloudRunCreateJobOperator, CloudRunExecuteJobOperator, CloudRunUpdateJobOperator, CloudRunDeleteJobOperator, etc. This is the idiomatic Airflow approach rather than wrapping logic in PythonOperator with hooks.",0,The code contains test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction requires 'robust error checking and cross-task dependency management' as well as demonstrating 'cleanup'. While the DAG shows creation, execution, and modification operations, it has critical gaps: (1) No CloudRunDeleteJobOperator is actually used in the task dependencies despite being imported, meaning no cleanup is demonstrated; (2) The dependency structure is overly simplistic with all tasks depending only on create_job in parallel, lacking the systematic workflow showing proper lifecycle sequencing (create → execute → modify → cleanup); (3) No error handling mechanisms like trigger rules (except the import which isn't used), retries, or failure callbacks are implemented; (4) The workflow doesn't demonstrate a realistic lifecycle - operations like remove_stage, set_stage, upsert_job, and update_job all run in parallel after creation, which doesn't showcase proper lifecycle management.",0.3333333333333333
813,Fine-tuned (Qwen 2.5 1.5B Airflow),156,0,"The code heavily relies on @task decorated functions with boto3 clients instead of using native Airflow operators. For example, 'get_execution_role_arn', 'clean_logs', 'get_execution_role_credentials', and 'set_up_system_test_config' all use boto3 clients wrapped in PythonOperator (via @task decorator) instead of using appropriate AWS provider operators. Additionally, there's an incorrect import 'from airflow.providers.amazon.aws.hooks.sagemaker import SagemakerOperator' - SagemakerOperator should be imported from operators, not hooks. The code should use operators like SageMakerProcessingOperator, SageMakerTrainingOperator, etc., instead of wrapping boto3 calls.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is clearly a test harness module from Airflow's internal testing framework. It also uses 'SystemTestContextBuilder().add_variable(test_env_name_key).build()' which is test boilerplate. The DAG ends with an incomplete lambda expression 'override_test_context = lambda' which appears to be test scaffolding code. These are clear indicators of test harness leakage into production code.",0,"The instruction asks for a complete machine learning lifecycle DAG that demonstrates cloud resource management, data transfer, dataset creation, model training, and deployment. However, the provided code is incomplete (ends with 'override_test_context = lambda') and doesn't show the actual DAG structure with tasks chained together. While it imports some relevant operators (SageMakerTrainingJobSensor, SageMakerProcessingJobSensor), it doesn't demonstrate their usage. There's no actual model training operator, no deployment step, no clear data pipeline, and no demonstration of how these components work together in a complete ML lifecycle. The code appears to be setup/configuration code rather than a complete orchestration DAG.",0.0
814,Fine-tuned (Qwen 2.5 1.5B Airflow),157,1,"The code uses native Airflow providers and operators appropriately (ClustersCreateOperator, ClustersDeleteOperator, ClustersListOperator, ClustersUpdateClusterOperator from airflow.providers.google.cloud.operators.dataproc). It does not wrap functionality in PythonOperator + Hook patterns where native operators exist.",0,The code explicitly imports testing modules and includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. This is not production-ready code and represents test infrastructure leakage.,0,"The user requested an automated data processing system that performs daily data validation, generates reports with error tracking, detects data integrity issues, and triggers error communication/reporting mechanisms. The provided code creates and manages Google Cloud Dataproc clusters with no data validation, no reporting, no error tracking, no data integrity checks, and no communication mechanisms. It is a completely unrelated example DAG that does not address any aspect of the user's instruction.",0.3333333333333333
815,Fine-tuned (Qwen 2.5 1.5B Airflow),158,0,"The code does not use any Sensor operator at all. It only uses a BashOperator with a sleep command and calls wait_for_completion() method, which is not the idiomatic way to implement sensor functionality in Airflow. The instruction specifically asks for a sensor task using Airflow's sensor decorator or sensor operators (e.g., FileSensor, HttpSensor, or custom @task.sensor decorator). A proper implementation would use airflow.sensors or the @task.sensor decorator introduced in Airflow 2.x+.",0,The code explicitly imports testing utilities and includes test harness boilerplate: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready DAG code.,0,"The instruction asks to 'use a sensor task to simulate checking for data availability or external system readiness' and 'demonstrate how to conditionally control task execution using Airflow's sensor decorator'. The provided code does neither: (1) it doesn't use any sensor operator or sensor decorator, (2) it doesn't simulate checking for data availability or external system readiness, (3) it doesn't demonstrate conditional task execution based on sensor results, and (4) it doesn't have subsequent processing steps that are triggered after the sensor completes. The code only contains a single BashOperator that sleeps for 30 seconds.",0.0
816,Fine-tuned (Qwen 2.5 1.5B Airflow),159,0,"The code uses Alibaba Cloud Druid operators instead of standard SQL-based data validation operators. For SQL-based data validation as requested, it should use operators like SQLColumnCheckOperator, SQLTableCheckOperator, or SQLCheckOperator from airflow.providers.common.sql.operators.sql, not Druid-specific operators from Alibaba Cloud provider.",0,"The code contains clear test harness boilerplate and imports from internal testing modules: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) | watcher()'. These are not production-ready code and should not be present in a production DAG.",0,"The instruction explicitly asks for 'SQL-based data validation techniques using column and table check operators against a predefined database table' with 'sequential validation tasks that check specific data quality metrics'. The provided code uses Alibaba Cloud Druid and ENI operators instead of SQL check operators, does not demonstrate column/table checks, does not validate against a predefined database table, and does not implement sequential data quality validation tasks. The code is completely unrelated to the requested functionality.",0.0
817,Fine-tuned (Qwen 2.5 1.5B Airflow),160,0,"The DAG violates idiomatic Airflow patterns by wrapping the S3ToSqlOperator inside a @task decorated function (create_table_and_insert_value) instead of using it directly as a task. The S3ToSqlOperator should be instantiated at the DAG level, not inside a Python function. This is a classic anti-pattern of wrapping operators in PythonOperator-style callables rather than using them natively.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test-specific logic like 'sys_test_context_task', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are clear indicators of test harness leakage.",0,"The instruction asks for a complete cloud data transfer process including 'data extraction' and 'observability'. However, the DAG does not demonstrate actual data extraction (no data is uploaded to S3 before transfer), the S3ToSqlOperator is incorrectly implemented inside a function and never actually executed, and the observability is limited to test harness watchers rather than production-grade monitoring. The DAG also lacks proper error handling mechanisms beyond the basic trigger rule. The overall structure is a test scaffold rather than a functional data transfer pipeline.",0.0
818,Fine-tuned (Qwen 2.5 1.5B Airflow),161,1,"The code uses appropriate Airflow operators and patterns. It uses BashOperator from the standard providers package and the @task decorator for Python tasks, which are idiomatic Airflow 3.0.0 patterns. It does not wrap operators in unnecessary PythonOperator calls.",1,"The code uses only standard Airflow libraries (airflow.providers.standard.operators.bash, airflow.sdk). There are no imports from internal testing modules or test harness boilerplate like get_test_run or tests_common.",0,"The code fails to meet the core requirements of the instruction. The user specifically requested: (1) nested task groups, (2) explicit setup/teardown decorators, (3) demonstration of lifecycle management with initialization and finalization steps, and (4) illustration of complex workflow structure with resource management. The provided code has none of these elements - it contains only simple linear task dependencies (t1 >> t2 >> t3) with no task groups, no setup/teardown decorators applied to any tasks, and no demonstration of lifecycle management or resource initialization/finalization patterns.",0.6666666666666666
819,Fine-tuned (Qwen 2.5 1.5B Airflow),162,1,"The code correctly imports and uses native Airflow operators from the Google Cloud Vision provider (CloudVisionCreateProductOperator, CloudVisionCreateProductSetOperator, CloudVisionCreateReferenceImageOperator, etc.) and GCS operators (GCSCreateBucketOperator, GCSToGCSOperator). While there are Python functions defined that use a non-existent 'VisionService' class, the imports demonstrate the intent to use proper Airflow operators rather than wrapping everything in PythonOperators with hooks.",0,"The code contains several hallucinations and errors: 1) References to non-existent 'VisionService' class and undefined 'options' variable in multiple functions. 2) Uses 'str(id)(prefix=""cix_"")' which is invalid Python syntax. 3) The code is incomplete (cuts off mid-function). 4) The Python functions reference methods and patterns that don't exist in standard Airflow libraries. While it doesn't import test harness modules, the code contains fabricated APIs and broken syntax that make it non-production-ready.",0,"The code fails to fulfill the instruction requirements. While it imports relevant operators for Vision API and GCS transfers, it does not actually construct a working DAG with tasks that: 1) Transfer image files between cloud storage buckets, 2) Prepare images for ML-based product recognition, or 3) Demonstrate a reproducible workflow. The DAG object is not instantiated with tasks, and the code is incomplete. The instruction asked for a complete pipeline demonstrating image dataset management and product metadata handling, but only imports and broken helper functions are provided.",0.3333333333333333
820,Fine-tuned (Qwen 2.5 1.5B Airflow),163,0,"The code uses CloudFormationCreateStackOperator and CloudFormationDeleteStackOperator appropriately, but then undermines this by creating custom @task decorated functions like 'delete_stack' that manually use boto3 client instead of using the native CloudFormationDeleteStackOperator. The 'get_stack_status' task uses subprocess to call 'cliz' commands instead of using proper Airflow operators or hooks. Additionally, 'get_status_of_stack' references 'PythonOperator' which is not imported. The mix of proper operators with custom Python functions wrapping AWS SDK calls is not idiomatic.",0,"The code imports 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder' which is clearly a test utility module, not a production library. It also uses 'sys_test_context_task = SystemTestContextBuilder().build()' which is test harness boilerplate. Additionally, the code references undefined variables like 'boto3' (not imported), 'PythonOperator' (not imported), 'MyLambdaFunction' (undefined), and uses a non-existent 'cliz' command. The code is also incomplete (ends with 'if __name__ ==').",0,"The instruction asks for 'creating and deleting an AWS SQS queue using Airflow operators and sensors', but the code attempts to create a CloudFormation stack with a Lambda function reference, not an SQS queue. The get_stack_status function tries to list 'sns-queues' (which is incorrect syntax), not SQS queues. The workflow structure is present (create, wait, delete) but it doesn't demonstrate SQS queue management as requested. The code also lacks a proper CloudFormationDeleteStackOperator usage in the chain and instead uses a custom delete function.",0.0
821,Fine-tuned (Qwen 2.5 1.5B Airflow),164,1,"The code uses native Airflow sensors and operators appropriately: FileSensor, FileSizeSensor, TimeDeltaSensor, TimeSensor, and PythonSensor from the standard providers package. It does not wrap sensor logic in PythonOperators unnecessarily. The sensors are used directly as intended by Airflow's design patterns.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into the generated code.,0,"While the code demonstrates multiple sensor types (FileSensor, TimeDeltaSensor, TimeSensor, PythonSensor), it has several issues: 1) The DAG structure is confusing with duplicate task dependencies that create circular references (check_time_delta appears multiple times in the chain), 2) The PythonSensor is used incorrectly - it uses 'print' as the callable which doesn't return a boolean condition as required by PythonSensor, 3) The workflow logic is unclear and doesn't properly showcase how sensors conditionally trigger downstream tasks in a coherent manner. The instruction asked for a clear demonstration of waiting mechanisms and different sensor behaviors, but the implementation is convoluted and would likely fail at runtime.",0.3333333333333333
822,Fine-tuned (Qwen 2.5 1.5B Airflow),165,1,"The code uses native Airflow Dataproc operators from airflow.providers.google.cloud.operators.dataproc, including DataprocCreateWorkflowTemplateOperator, DataprocDeleteWorkflowTemplateOperator, DataprocInstantiateInlineWorkflowTemplateOperator, and DataprocInstantiateWorkflowTemplateOperator. These are the appropriate, idiomatic operators for managing Dataproc workflow templates rather than wrapping logic in PythonOperator with hooks.",0,The code contains test harness boilerplate at the end: 'from tests.system.utils import get_test_run' which is an internal testing module import. This is followed by a comment about test requirements mentioning cluster setup and JSON file locations. This is not production-ready code and includes test infrastructure leakage.,0,"The instruction specifically requests 'a Pig job on a managed Hadoop cluster' and 'asynchronous job instantiation'. While the code does include a Pig job reference in INLINE_WORKFLOW, the implementation is incomplete and incorrect. The WORKFLOW_TEMPLATE structure uses 'insideJobRunner' (which appears to be invalid) instead of proper Pig job configuration. The Pig job in INLINE_WORKFLOW references WORKFLOW_TEMPLATE incorrectly in the queryList. The code also creates multiple workflows with different patterns but doesn't clearly demonstrate a complete, working Pig job execution as requested. The asynchronous instantiation is present but the core requirement of a properly configured Pig job is not fulfilled.",0.3333333333333333
823,Fine-tuned (Qwen 2.5 1.5B Airflow),166,0,"The code wraps operator logic inside @task decorated functions (e.g., create_tmp_s3_bucket, delete_tmp_s3_bucket) and attempts to instantiate operators within these Python callables. This is anti-idiomatic. Operators should be instantiated at the DAG level, not wrapped in PythonOperator-style task functions. Additionally, the code references non-existent operators like 'CreateTemporaryS3BucketOperator', 'CopySingleFileToS3Operator', 'CopyS3FilesOperator', 'remove_s3_objectsOperator', etc., which are not standard Airflow providers. The proper approach would be to use actual S3 operators (S3CreateBucketOperator, S3CopyObjectOperator, S3DeleteBucketOperator, etc.) directly in the DAG context.",0,"The code contains multiple hallucinated elements: (1) Non-existent operators like 'CreateTemporaryS3BucketOperator', 'CopySingleFileToS3Operator', 'CopyS3FilesOperator', 'remove_s3_objectsOperator', 'remove_bucketOperator', 'remove_objects_operator', 'remove_bucket_operator' that don't exist in Airflow providers. (2) Invalid syntax like 'TriggerStrategyOnceEveryWeekDay Tuesday, Thursday, Saturday' which is not valid Python or Airflow syntax. (3) Hardcoded AWS credentials directly in the code (security anti-pattern). (4) Undefined variables like 'UUID()', 'S3_SOURCE', 'S3_DESTINATION'. (5) Incomplete code (truncated at the end). (6) Invalid use of chain() inside task functions. (7) Syntax errors like 'copy_files Within_S3' with space in variable name.",0,"The instruction requires: (1) Cross-platform file transfer between cloud storage systems, (2) Initialize temporary storage container, (3) Execute file transfer, (4) Ensure cleanup regardless of outcomes. While the code attempts to address these requirements conceptually (creating temp bucket, copying files, cleanup with trigger_rule=ALL_DONE), the implementation is fundamentally broken with syntax errors, hallucinated operators, incomplete code, and incorrect patterns. The code does not demonstrate 'secure' practices (hardcoded credentials), and the 'cross-platform' requirement is not met (only S3-to-S3 is attempted, not between different cloud providers). The DAG would not execute successfully.",0.0
824,Fine-tuned (Qwen 2.5 1.5B Airflow),167,0,"The code uses 'PGDBOperator' from 'airflow.providers.pgdb.operators.pgdb', which is not a standard Airflow operator. The instruction specifically asks to demonstrate SQLExecuteQueryOperator capabilities for executing database queries against a Postgres database. The idiomatic approach for Airflow 3.0.0 would be to use 'SQLExecuteQueryOperator' from 'airflow.providers.common.sql.operators.sql' or the Postgres-specific operator 'PostgresOperator' from 'airflow.providers.postgres.operators.postgres'. The 'PGDBOperator' appears to be either outdated, non-existent, or incorrectly named.",0,"The code contains clear hallucinations and test harness boilerplate. It imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)' at the end, which are internal testing utilities that should not appear in production DAG code. Additionally, the 'PGDBOperator' is not a standard Airflow operator, and the code attempts to pass a BashOperator instance as a SQL parameter (line with 'sql=BashOperator(...)'), which is nonsensical and indicates code generation errors.",0,"The instruction asks to demonstrate basic SQL query execution against an Airflow metadata database using SQLExecuteQueryOperator, but the code uses a non-standard 'PGDBOperator' instead. The code also doesn't demonstrate meaningful SQL queries against the Airflow metadata database (it uses placeholder 'table_name' instead of actual metadata tables like 'dag', 'task_instance', etc.). Additionally, the code contains illogical constructs like passing a BashOperator as a SQL statement, and uses invalid parameters like 'rows' and 'depend_on_last' that don't exist in standard SQL operators. The code fails to showcase proper database query execution capabilities as requested.",0.0
825,Fine-tuned (Qwen 2.5 1.5B Airflow),168,1,The code uses native Airflow operators designed for the specific tasks: MsSqlToGCSOperator for database export to cloud storage and GCSToSnowflakeOperator for loading data from cloud storage to the data warehouse. This is the idiomatic approach rather than wrapping hooks in PythonOperators.,0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', calls 'watcher()' in the DAG, and includes 'test_run = get_test_run(dag)'. These are testing artifacts that should not be present in production-ready code.",0,"The instruction asks for a pipeline from 'NoSQL database export' to cloud storage and then to a data warehouse. However, the code uses MsSqlToGCSOperator which connects to MS SQL Server, a relational (SQL) database, not a NoSQL database. The instruction specifically requested NoSQL database integration (e.g., MongoDB, Cassandra, DynamoDB), so this does not fulfill the core requirement despite implementing the general pipeline pattern correctly.",0.3333333333333333
826,Fine-tuned (Qwen 2.5 1.5B Airflow),169,0,"The code extensively uses PythonOperator patterns (via @task decorator) with boto3 clients directly instead of using native Airflow operators. For example, 'create_queue', 'delete_queue', and 'delete_bucket' tasks use boto3.client() wrapped in @task decorators. While S3CreateBucketOperator is used correctly, the overall pattern violates idiomatic Airflow by not leveraging SQSCreateQueueOperator properly and using raw boto3 calls. Additionally, there's a hallucinated 'S3ToSQSMakeRequestOperator' which doesn't exist in Airflow providers.",0,"Multiple hallucinations and test harness leakage present: (1) Imports 'from system.amazon.aws.utils import SystemTestContextBuilder' which is internal testing infrastructure. (2) Uses 'SystemTestContextBuilder().build()' pattern typical of system tests. (3) References non-existent 'S3ToSQSMakeRequestOperator'. (4) Uses undefined '@bash_operator' decorator and 'CheckThat' construct. (5) Includes test harness code at bottom: 'from tests_common.test_utils.watcher import watcher'. (6) Invalid boto3 API calls like 'boto3.resource(""rds-data"").DataWarehouse.create_connection'. (7) Syntax errors like 'ish' instead of colon after DAG context manager.",0,"The instruction asks for a pipeline demonstrating S3 file processing including creating buckets, uploading sample data, and performing metadata operations. While the code attempts bucket creation, it fails to properly upload sample data to S3 (no actual S3 upload operation using S3CreateObjectOperator or S3Hook), and metadata operations are not demonstrated. The code is also syntactically broken and mixes SQS operations unnecessarily when the focus should be on S3 operations. The learning exercise aspect is compromised by hallucinated operators and test infrastructure that would confuse learners.",0.0
827,Fine-tuned (Qwen 2.5 1.5B Airflow),171,0,"The DAG uses PinotHook wrapped in a PythonOperator (via @task decorator) instead of using native Pinot operators. While Airflow's Pinot provider may have limited operators, the idiomatic approach would be to use PinotOperator if available for executing queries/commands, rather than wrapping hook logic in Python callables. The code follows the anti-pattern of 'def function(): hook = PinotHook()... @task(python_callable=function)' which is explicitly called out as a failure case.",0,"The code contains test harness boilerplate at the bottom: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. This is internal testing infrastructure that should not appear in production DAG code. Additionally, it references 'SYSTEM_TESTS_ENV_ID' environment variable which is test-specific. These are clear indicators of test leakage into production code.",0,"The instruction asks for 'system-level and query-level interactions' that 'executes administrative commands and retrieves database information'. The DAG only implements one narrow function (listing tables filtered by name ending in 'db'), which doesn't demonstrate administrative commands or comprehensive database information retrieval. It lacks query-level interactions (actual data queries) and system-level administrative operations. The implementation is incomplete relative to the stated requirements of demonstrating both administrative commands AND data exploration/management capabilities.",0.0
828,Fine-tuned (Qwen 2.5 1.5B Airflow),172,1,"The code uses native Airflow providers and operators appropriately. It imports and uses GCSCreateBucketOperator, GCSDeleteBucketOperator, DataflowCreateApplicationJobOperator, DataflowRunApplicationJobOperator, and DataflowDeleteApplicationJobOperator from the official Google Cloud provider package. No generic PythonOperator wrappers are used where specific operators exist.",0,The code contains test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction specifically requests a pipeline that 'converts CSV files to AVRO format using Google Dataflow templates' with file upload capabilities. However, the provided code: (1) Does not use Dataflow templates but rather Dataflow Application jobs, (2) Does not include any file upload operators (e.g., LocalFilesystemToGCSOperator), (3) Does not specify CSV to AVRO conversion logic or reference appropriate Dataflow templates for this purpose, (4) References a generic 'in.txt' file rather than CSV files. The code appears to be a generic Dataflow Application example rather than a specific CSV-to-AVRO conversion pipeline as requested.",0.3333333333333333
829,Fine-tuned (Qwen 2.5 1.5B Airflow),173,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocUpdateClusterOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. These are the idiomatic, purpose-built operators for managing Dataproc clusters rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not appear in production-ready code.",0,"The instruction explicitly requests creating a GKE cluster first, then spinning up a Dataproc cluster within that Kubernetes environment (Dataproc on GKE). However, the provided code only creates a standard Dataproc cluster (not on GKE) and completely omits any GKE cluster creation operators (like GKECreateClusterOperator). The code also focuses on updating a Dataproc cluster rather than demonstrating the infrastructure lifecycle coordination between GKE and Dataproc as requested. Additionally, there's a syntax error in the update_cluster operator ('new_cluster=config=' is invalid Python).",0.3333333333333333
830,Fine-tuned (Qwen 2.5 1.5B Airflow),174,1,"The DAG uses appropriate Airflow providers and operators for the task: FirestoreToGCSOperator for exporting Firestore data, BigQueryCreateExternalTableOperator for creating external tables, and GCS operators for bucket management. These are the correct, idiomatic operators rather than wrapping logic in PythonOperators with hooks.",0,The code contains several issues indicating hallucination: 1) The SOURCE_FIRESTOREQUENCE variable references an undefined 'firestore_id' variable. 2) The clean_up_firestore_ref() function contains nonsensical code mixing database settings with Firestore operations that don't align with actual Firestore API. 3) The function is defined but never used in the DAG. 4) The firestore_collection_name parameter uses an incorrect path format mixing BigQuery dataset paths with Firestore collection paths. 5) The code is incomplete (cuts off at '# [END howt'). These indicate the code is not production-ready.,0,"The DAG partially addresses the instruction but has critical gaps: 1) It creates an external table but never actually queries or validates the data in BigQuery, which is required for 'preparing it for analytical querying'. 2) The task dependencies are not defined, so the execution order is unclear. 3) The export_firestore_to_gcs task is defined but not connected to create_bucket or create_external_table, breaking the data flow. 4) There are two export tasks (regular and deferrable) with no clear purpose or dependency chain. The instruction asks for a complete extraction, export, and preparation pipeline, but the implementation is incomplete and non-functional.",0.3333333333333333
831,Fine-tuned (Qwen 2.5 1.5B Airflow),175,0,"The code does not use appropriate Twitter-specific providers or operators. Airflow does not have native Twitter operators in standard providers, so the correct approach would be to use PythonOperator with a Twitter API library (like tweepy) or custom operators. However, the code fabricates non-existent functionality like 'conn.add_tweet()' and uses SQLExecuteQueryOperator inappropriately for Twitter data retrieval. The functions 'create_twitter_tweet' and 'create_csv_file' are defined but never used in the DAG, and they reference non-existent Airflow Connection methods. A proper implementation would use PythonOperator with actual Twitter API calls, not fake SQL queries to retrieve tweets.",0,"The code contains multiple hallucinations: (1) Imports 'from system.openlineage.operator import OpenLineageTestOperator' which appears to be an internal testing module, not a standard Airflow library. (2) References non-existent Connection methods like 'conn.add_tweet()' and 'connection.conn.create_file()'. (3) Uses 'conn_type=""twitter""' which is not a standard Airflow connection type. (4) The SQL queries reference tables like 'twitter_output', 'twitter_join_output', 'csv_input', and 'twitter_users' that don't exist and wouldn't be populated by any actual Twitter data retrieval. (5) The code is incomplete (cuts off mid-statement). This is not production-ready code.",0,"The instruction requires: (1) Retrieve tweets from different Twitter accounts, (2) Transform raw data through cleaning and analysis stages, (3) Prepare aggregated insights for storage, (4) Systematic approach with clear transformation steps. The provided code fails on all counts: It doesn't actually retrieve any tweets from Twitter (just runs fake SQL queries), has no data cleaning or transformation logic, performs no analysis or aggregation, and doesn't store any insights. The DAG structure is disorganized with disconnected tasks that don't form a coherent pipeline. The defined helper functions are never called. This code does not fulfill the business requirements at all.",0.0
832,Fine-tuned (Qwen 2.5 1.5B Airflow),176,0,"The code imports from 'airflow.providers.microsoft.azure.hooks.level_db' and 'airflow.providers.microsoft.azure.operators.level_db', which do not exist in Apache Airflow. LevelDB is not part of the Microsoft Azure provider, and there are no official Airflow providers for LevelDB. This appears to be a hallucinated/fabricated provider. Additionally, the code uses non-existent methods like 'LevelDbHook.get_key_operator()' which is not idiomatic Airflow design.",0,"Multiple issues: (1) The code imports 'from tests.system.utils import get_test_run' and uses 'test_run = get_test_run(multi_run_dag)', which is test harness boilerplate that should not be in production code. (2) The entire LevelDB provider ('airflow.providers.microsoft.azure.hooks.level_db' and 'airflow.providers.microsoft.azure.operators.level_db') is fabricated and does not exist in Airflow 2.9.3 or any version. (3) The code has syntax errors (missing colon after DAG definition, 'odesk' variable appears out of nowhere, TaskGroup is used but not imported).",0,"While the code attempts to demonstrate key-value retrieval and insertion operations conceptually, it fails to provide a working implementation because: (1) LevelDB is not supported by Apache Airflow providers, so the requested functionality cannot be achieved with the fabricated operators shown. (2) The code has multiple syntax errors and would not execute. (3) A proper implementation would require using PythonOperator with a LevelDB Python library (like plyvel), but this is not what was delivered.",0.0
833,Fine-tuned (Qwen 2.5 1.5B Airflow),177,0,"While the code uses specific Display Video 360 operators (GoogleMarketoPlatformVideo360ListAdsOperator and GoogleMarketoPlatformVideo360GetAdSensor), it does not complete the data pipeline as required. The operators used are for listing and checking ads, but there is no operator to actually download performance data or transfer it to a data warehouse. A proper implementation would use operators like GoogleDisplayVideo360DownloadReportV2Operator for downloading reports and a data warehouse operator (e.g., BigQueryOperator, SnowflakeOperator) for loading data. The current implementation is incomplete for the stated purpose.",0,The code contains clear hallucination/leakage issues. It imports internal testing modules at the bottom: 'from tests.system.utils import get_test_run' and 'from tests.system.test_utils.system_tests import get_test_run_from_test_case'. It also includes test harness boilerplate with 'test_run = get_test_run(dag)' and 'test_run_from先生 = get_test_run_from_test_case(test_run)'. These are not production-ready elements and should not be in a production DAG.,0,"The instruction requires building a complete data pipeline that: (1) collects advertising performance data, (2) downloads it, and (3) transfers it to a centralized data warehouse. The provided code only lists ads and checks for a specific ad using a sensor. It does not download any performance/reporting data, nor does it transfer anything to a data warehouse. Critical steps of the instruction are missing, making this implementation incomplete for the stated business requirement of 'comprehensive marketing analytics and reporting'.",0.0
834,Fine-tuned (Qwen 2.5 1.5B Airflow),178,1,"The code uses TaskFlow API with @task decorators, which is the modern, idiomatic way to write Airflow DAGs in version 2.x. It appropriately uses decorators for task definition rather than wrapping everything in PythonOperators.",1,"The code uses only standard Airflow libraries (airflow.decorators, airflow.models.dag, pendulum). There are no imports from test utilities or internal testing modules. The code is clean and production-ready from an import perspective.",0,"The code fails to demonstrate the key requirements: (1) It doesn't show actual cross-task data sharing - tasks are not connected and don't pull XComs from each other, (2) The push_to_xcom task is called with None instead of task_instance, which would cause a runtime error, (3) No demonstration of 'returning values' method for XCom (tasks don't return values), (4) Most critically, despite the DAG being named 'example_xcom_bash' and the instruction specifically mentioning 'bash operator interactions', there is no BashOperator present in the code at all. The print statement references BashOperator but none exists. The code is incomplete and doesn't fulfill the instruction to show various XCom methods and bash operator interactions.",0.6666666666666666
835,Fine-tuned (Qwen 2.5 1.5B Airflow),179,0,"The code fails to use appropriate SQS operators and hooks. It incorrectly uses S3Hook for SQS operations (e.g., 'sqs_hook = S3Hook()' in create_sqs_connection and delete_sqs_queue). While it does use SqsSensor, it should be using SqsHook for queue creation/deletion and SqsPublishOperator for publishing messages. The code wraps connection creation logic in PythonOperators (@task decorators) instead of using proper Airflow connection management.",0,"The code explicitly imports and uses testing modules that should not be in production code: 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run'. These are test harness components that indicate this is test boilerplate, not production-ready code. The comment '# Needed to run the exam' at the end further confirms this is test code.",0,"The code does not fulfill the user instruction. The instruction requires: (1) dynamically creating an SQS queue, (2) publishing multiple messages, (3) demonstrating different message retrieval strategies using sensors, and (4) queue deletion. The code fails on multiple fronts: it doesn't actually create an SQS queue (just creates a connection with a hardcoded queue URL), doesn't publish any messages to SQS, doesn't demonstrate different sensor strategies, and the queue deletion logic uses the wrong hook (S3Hook instead of SqsHook). The code is mostly focused on S3 operations rather than SQS operations as requested.",0.0
836,Fine-tuned (Qwen 2.5 1.5B Airflow),180,0,"The code uses @task decorators with boto3 client directly (e.g., 'boto3.client(""s3"")' in create_sample_data) instead of using native Airflow operators. For S3 operations, it should use S3CreateObjectOperator or similar providers. The create_sample_data task manually creates an S3 bucket and uploads files using boto3 instead of using S3CreateBucketOperator and S3CreateObjectOperator. While SageMakerHook is imported, the actual implementation wraps boto3 logic in Python callables rather than using dedicated SageMaker operators throughout.",0,"The code contains several issues: 1) References to non-existent attributes like 'hook.sagemaker_rds_instance_endpoint_id' which is not a valid SageMakerHook attribute (SageMaker hooks don't have RDS-related attributes). 2) The code is incomplete and cuts off mid-definition ('processing_job_nam' is truncated). 3) Variables like 'processor = ""tmx""' appear arbitrary and unexplained. 4) The INPUT_DATA_S3_URI path construction is malformed ('s3://{{ params.sample_data_url }}/alaska/train/' mixed with hardcoded paths). These suggest hallucinated or incorrectly generated code elements.",0,"The instruction requires: 1) Transform raw training data, 2) Train a classification model, 3) Expose as real-time prediction service, 4) Automated infrastructure provisioning, and 5) Prediction capabilities. The provided code is incomplete and only partially addresses these requirements. It starts infrastructure setup (S3 bucket creation) and begins model creation, but the code cuts off before completing the training job configuration. There is no model training implementation, no endpoint deployment for real-time predictions, and no actual prediction capability demonstrated. The workflow is fundamentally incomplete and does not deliver a deployable ML model with prediction service as requested.",0.0
837,Fine-tuned (Qwen 2.5 1.5B Airflow),181,1,"The code correctly uses native Airflow providers and operators specifically designed for Google Cloud DLP tasks. It imports and uses CloudDLPCreateJobTriggerOperator, CloudDLPGetJobTriggerOperator, CloudDLPListJobTriggersOperator, CloudDLPUpdateJobTriggerOperator, and CloudDLPDeleteJobTriggerOperator from airflow.providers.google.cloud.operators.dlp. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear evidence of test harness boilerplate and internal testing module imports. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and uses 'listtasks = watcher(dag.tasks)' within the DAG context. This is not production-ready code and includes testing infrastructure that should not be present in a clean example DAG.",0,"While the code attempts to demonstrate create, get, list, update, and delete operations for DLP job triggers, it has several critical issues: 1) The code is incomplete (truncated at the update task description), 2) Tasks are defined twice - once outside the DAG context and once inside, creating confusion, 3) There is no clear sequential task dependency chain established (no >> or << operators shown to link the tasks in order), 4) The code contains syntax errors (e.g., 'task_id=""dlp_create_trigger""]' has a closing bracket instead of quote). These issues prevent it from serving as a proper technical reference for sequential task management as requested.",0.3333333333333333
838,Fine-tuned (Qwen 2.5 1.5B Airflow),182,0,The code attempts to use Azure-specific operators but uses incorrect and non-existent operators. 'AzureAdvisorListPatchesOperator' is not a real Airflow operator for file transfer/deletion. 'LocalFilesystemToLocalDirectoryOperator' is not a valid Azure transfer operator. The correct operators for the task would be 'LocalFilesystemToWasbOperator' or 'LocalFilesystemToAzureBlobStorageOperator' for upload and 'AzureBlobStorageDeleteOperator' for deletion. The import statement also has a syntax error ('local_to azure_blob' instead of 'local_to_wasb').,0,"The code explicitly imports test harness modules at the end: 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run'. It also includes test boilerplate code like 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. Additionally, it references non-existent operators like 'AzureAdvisorListPatchesOperator' which appears to be hallucinated. There are also syntax errors like 'delete_patch_from advisor' (space in variable name) and 'is_delete_object(True)' which is not valid syntax.",0,"The instruction asks for a pipeline demonstrating file transfer and deletion between local file systems and Azure cloud storage. While the code attempts to show upload and deletion operations, it fails to properly implement them using correct operators. The 'download_blob_to_local' task uses a non-existent operator and incorrect blob URI format. The deletion task uses 'AzureAdvisorListPatchesOperator' which is not appropriate for blob deletion. The code also includes unnecessary connection creation logic and advisor-related operations that don't align with the simple file transfer/deletion demonstration requested.",0.0
839,Fine-tuned (Qwen 2.5 1.5B Airflow),183,1,"The code uses native Airflow providers and operators specifically designed for Alibaba Cloud services (AliyunOSSCreateBucketOperator, OSSDeleteBucketOperator, AliyunVPCCreateOperator, AliyunVPCDeleteOperator). It does not wrap cloud service logic in PythonOperator with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production DAG code. Additionally, there's a bug: 'bucket=name' should be 'bucket_name=bucket_name' in the delete_bucket task.",1,The DAG successfully demonstrates sequential task execution pattern for cloud infrastructure management with Alibaba Cloud services. It shows clear task dependencies (create_vpc >> create_bucket >> delete_bucket >> delete_vpc) and programmatic interaction with Alibaba OSS bucket resources (create and delete operations). The workflow illustrates both OSS bucket management and VPC interactions as requested.,0.6666666666666666
840,Fine-tuned (Qwen 2.5 1.5B Airflow),184,1,"The code correctly uses the OracleStoredProcedureOperator from airflow.providers.oracle.operators.oracle, which is the idiomatic and native Airflow operator for executing Oracle stored procedures. It does not wrap logic in a PythonOperator with hooks, demonstrating proper use of Airflow providers.",0,"The code contains clear hallucination/leakage issues. It imports 'from tests.system.utils import get_test_run' (appears twice with inconsistent capitalization) and includes 'test_run = get_test_run(dag)', which are internal testing utilities and test harness boilerplate that should not appear in production DAG code. This is explicitly mentioned as a failure case in the scoring criteria.",0,"The instruction asks for a workflow that 'explores various methods of executing database operations with input and output parameter handling' and 'highlights different parameter input strategies for stored procedures.' However, the DAG only shows a single task with one approach to parameter passing (embedded in the procedure string), and critically has a syntax error (unclosed quote in the procedure parameter: 'TEST_PROCEDURE',). It does not demonstrate multiple methods, does not show output parameter handling, and does not create a comprehensive data processing workflow exploring different strategies as requested.",0.3333333333333333
841,Fine-tuned (Qwen 2.5 1.5B Airflow),185,1,"The code correctly uses the native GCSToBigQueryOperator from airflow.providers.google.cloud.transfers.gcs_to_bigquery, which is the idiomatic Airflow approach for transferring data from GCS to BigQuery. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator designed for this task.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it includes 'import pytest', 'pytest.skipif_missing_google_cloud_components()', and most critically 'from tests.system.utils import get_test_run' followed by 'test_run = get_test_run(dag)'. These are testing artifacts that should not appear in production DAG code.",0,"The instruction specifically requests transforming a 'local CSV file containing US state information' into BigQuery. However, the code reads from a GCS bucket ('cloud-samples-data') with an 'employees.csv' file, not from local storage. The instruction emphasizes 'moving structured data from local storage to cloud-based analytical platforms', but the code skips the local-to-GCS upload step entirely and only handles the GCS-to-BigQuery transfer. A complete solution would need a LocalFilesystemToGCSOperator or similar to first upload the local CSV to GCS.",0.3333333333333333
842,Fine-tuned (Qwen 2.5 1.5B Airflow),186,1,"The code uses specific Airflow provider operators (CloudMemorystoreCreateDbOperator, CloudMemorystoreDeleteDbOperator, CloudMemorystoreManagePoolOperator, CloudMemorystoreManageInstancePointsOperator) from airflow.providers.google.cloud.operators.cloud_memorystore rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for GCP Cloud Memorystore operations.",0,"The code contains a clear hallucination at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' which imports internal testing modules. This is test harness boilerplate that should not be in production code. Additionally, there are typos in variable names (QDRANK_INDEX_NAME instead of QDRANT_INDEX_NAME) and the CloudMemorystoreDenseQdrantOperator is imported but never used, suggesting this may be generated from test/example code.",0,"The instruction asks for a workflow to create and manage a 'distributed caching service' with 'specific node and memory configurations'. However, the code focuses on Cloud Memorystore with Qdrant vector database operations (points, pools, indexes) rather than traditional distributed caching configurations. There are no explicit node count, memory size, or cache-specific configurations shown. The code also has incomplete/broken configuration (QDRANK_INDEX_NAME typo, unused DELETE_DB_BODY, incomplete task dependencies) and doesn't demonstrate the scalable in-memory data storage setup with precise configuration controls as requested.",0.3333333333333333
843,Fine-tuned (Qwen 2.5 1.5B Airflow),187,0,The code uses GCSHook wrapped in a PythonOperator (@task decorator) for the delete_bucket function instead of using the native GCSDeleteBucketOperator that is already imported. The GCSDeleteBucketOperator should be used directly as it's the idiomatic Airflow approach for deleting GCS buckets.,0,The code contains test harness boilerplate at the bottom with 'from tests_common.test_utils.watcher import watcher' and 'list(dag.tasks) >> watcher()'. This is internal testing infrastructure that should not appear in production DAG code.,0,"The instruction asks for 'downloading the file' as part of the workflow, but the DAG only uploads files to GCS and uses sensors to wait for them. There is no operator that downloads the file from GCS back to local filesystem (e.g., GCSToLocalFilesystemOperator). Additionally, there are syntax errors ('is' instead of ':' in the with statement, 'LocalFilesystemToGCSOperator' typo) and logical errors (file opened in read mode but attempting to write, tasks not properly chained). The create_file function also has a bug - it opens the file without 'w' mode.",0.0
844,Fine-tuned (Qwen 2.5 1.5B Airflow),188,0,"The code uses BashOperator instead of Kafka-specific operators. For an event-driven Kafka pipeline, it should use providers like 'airflow.providers.apache.kafka.operators' and 'airflow.providers.apache.kafka.sensors' (e.g., KafkaProducerOperator, KafkaConsumerOperator, or custom sensors). The current implementation doesn't demonstrate any Kafka integration whatsoever.",0,"The code references an undefined 'dag' variable that is never instantiated. This would cause a NameError at runtime. Additionally, the task_id 'sleep_{{task_instance}}' uses invalid Jinja templating that won't render properly as a task_id (task_ids should be static strings, not templated). The code is not production-ready.",0,"The instruction explicitly requests: (1) event-driven DAG with Kafka, (2) message production and consumption, (3) a sensor that triggers based on number characteristics, (4) asynchronous message processing, and (5) conditional event handling. The provided code does none of this - it only contains basic BashOperators that echo and sleep, with no Kafka integration, no sensors, no message processing, and no conditional logic based on number characteristics.",0.0
845,Fine-tuned (Qwen 2.5 1.5B Airflow),189,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator for bucket creation, CloudTextToSpeechSynthesizeOperator for text-to-speech conversion, and GCSDeleteBucketOperator for cleanup. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready DAG code. This is a clear example of test infrastructure leakage.,1,"The DAG fulfills all requirements from the user instruction: (1) creates a GCS bucket using GCSCreateBucketOperator, (2) converts text to speech using CloudTextToSpeechSynthesizeOperator and stores the audio file in the bucket (output_uri_template points to the bucket), and (3) cleans up the bucket using GCSDeleteBucketOperator with appropriate trigger rule. The business logic is complete and correctly sequenced.",0.6666666666666666
846,Fine-tuned (Qwen 2.5 1.5B Airflow),190,1,"The code uses appropriate BigQuery-specific operators from airflow.providers.google.cloud.operators.bigquery including BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryInsertJobOperator, BigQuerySelectOperator, and BigQueryDeleteDatasetOperator. These are the idiomatic Airflow operators for BigQuery operations rather than wrapping logic in PythonOperator with hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 1) imports pytest which is a testing library, 2) imports from 'tests.system.utils.watcher' and uses watcher() function, 3) includes 'from tests.syste' at the end (incomplete import), 4) uses ENV_ID from environment variables typically used in system tests, 5) includes START/END comment markers typical of documentation examples, and 6) references TriggerDagRule which appears to be test-related. The code also has syntax errors (missing comma, unclosed string quote in TABLE_2_ID, undefined variables in SQL query).",0,"The instruction asks for a pipeline demonstrating dynamic table creation and transformation in BigQuery with multiple data object types including standard tables, views, and materialized views. The code only creates two standard tables and runs basic queries. It does NOT create or demonstrate: 1) views, 2) materialized views, 3) transformations between different table types, or 4) modifications to existing tables. The code is more of a basic CRUD example rather than a comprehensive demonstration of different BigQuery object types and their transformations as requested.",0.3333333333333333
847,Fine-tuned (Qwen 2.5 1.5B Airflow),191,1,"The code uses native Airflow operators designed specifically for the task: SpreadsheetToGCSOperator for transferring data from Google Sheets to GCS, and GCSToSpreadsheetOperator for transferring data from GCS back to Google Sheets. These are the appropriate provider operators from airflow.providers.google.suite.transfers, not generic PythonOperator wrappers around hooks.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) | watcher()'. These are testing utilities that should not be present in production DAG code.",0,"The task dependency is incorrect. The code has 'get_spreadsheet >> update_spreadsheet', which means it first loads data FROM GCS to a spreadsheet, then exports FROM spreadsheet to GCS. The instruction requires: (1) extract spreadsheet data, (2) transfer to cloud storage, (3) reimport to new spreadsheet. The correct flow should be 'update_spreadsheet >> get_spreadsheet' (export first, then import). The current implementation reverses the logical migration pipeline flow.",0.3333333333333333
848,Fine-tuned (Qwen 2.5 1.5B Airflow),193,1,"The code uses native Airflow operators designed for the specific tasks: MySqlToGCSOperator for transferring data from MySQL to GCS, GCSCreateBucketOperator for bucket creation, and GCSDeleteBucketOperator for bucket deletion. These are all provider-specific operators from airflow.providers.google.cloud, which is the idiomatic approach rather than wrapping hooks in PythonOperators.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)' at the bottom. These are internal testing utilities that indicate this is example/test code rather than clean, production-ready code.",0,"While the DAG demonstrates MySQL to GCS data transfer with bucket management, it does NOT fulfill the instruction requirement for a 'one-time batch process' in a production sense. The DAG includes a delete_bucket task that removes the bucket after data transfer, which contradicts the goal of 'storage' - the data is immediately deleted. For a real data migration workflow, the bucket and data should persist after transfer. The deletion step appears to be test cleanup logic rather than actual migration logic.",0.3333333333333333
849,Fine-tuned (Qwen 2.5 1.5B Airflow),194,0,"The code does not use the appropriate operators for the task. The instruction requires extracting data from BigQuery and loading into PostgreSQL, which should use BigQueryToPostgresOperator or a combination of BigQueryOperator with a transfer operator. Instead, the code uses PGDBOperator with hardcoded INSERT statements, which doesn't perform any data extraction from BigQuery at all.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production DAG code.,0,"The code completely fails to meet the instruction requirements. The instruction asks for extracting salary records from BigQuery and loading them into PostgreSQL with configurable parameters, error handling, and resource management. The generated code: (1) doesn't connect to or query BigQuery at all, (2) uses hardcoded INSERT statements instead of data extraction, (3) has no configurable parameters for the data transfer, (4) lacks error handling mechanisms, (5) has no resource management implementation, and (6) the DAG name and comments reference 'GCS to Postgres' instead of 'BigQuery to Postgres'.",0.0
850,Fine-tuned (Qwen 2.5 1.5B Airflow),196,0,"The DAG does not use Airflow's native parameter passing mechanisms idiomatically. It should use DAG-level 'params' with Param objects and access them via context or templating (e.g., {{ params.vendma }}). Instead, it hardcodes a dictionary directly in the task call. Additionally, the instruction mentions 'Python and Bash tasks' but only a Python task is implemented - a BashOperator demonstrating parameter templating is missing entirely.",1,"The code uses only standard Airflow libraries (airflow.decorators, airflow.models.dag) and common dependencies (pendulum, datetime). No test harness code, internal testing modules, or non-standard imports are present.",0,"The instruction explicitly requests: (1) demonstrating parameter passing methods, (2) environment variable testing, (3) showcasing different methods of injecting runtime configuration, and (4) illustrating parameter access across both Python AND Bash tasks. The provided code only shows a single Python task with hardcoded parameters, lacks any Bash task, demonstrates no environment variable usage, and doesn't showcase Airflow's params feature or multiple configuration injection methods (templating, context variables, etc.).",0.3333333333333333
851,Fine-tuned (Qwen 2.5 1.5B Airflow),197,0,"The code uses a PythonOperator (via @task decorator) for the delete operation that only prints a message and returns True, rather than performing actual work. While it does include ADLSDeleteOperator, it's missing the upload operator entirely. For a proper idiomatic implementation, it should use ADLSUploadOperator (or similar Azure provider operator) for uploading the file and ADLSDeleteOperator for deletion, without wrapping logic in Python callables.",1,"The code uses only standard Airflow libraries and Azure provider operators. There are no imports from test utilities or internal testing modules. The code is clean from a hallucination perspective, though functionally incomplete.",0,"The instruction requires two specific operations: (1) upload a local file to Azure Data Lake Storage, and (2) delete the uploaded file. The provided code only implements the delete operation using ADLSDeleteOperator and includes a dummy Python task that prints but doesn't actually perform any upload. The upload functionality is completely missing, which is a critical omission that fails to meet the core requirement of the instruction.",0.3333333333333333
852,Fine-tuned (Qwen 2.5 1.5B Airflow),198,1,"The DAG uses native Airflow providers and operators designed for Google Cloud services. It properly imports and uses DataflowCreateBatchOperator, DataflowRunBatchOperator, DataflowDeleteBatchOperator, GCSCreateBucketOperator, and GCSDeleteBucketOperator from airflow.providers.google.cloud.operators. No generic PythonOperator wrappers are used where specific operators exist.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also has TEST SETUP/TEST BODY/TEST TEARDOWN comments and calls watcher() which are not production-ready patterns.",0,"The instruction asks for a batch text analysis pipeline using a word count example with a sample text file, but the code does not implement this. There is no text file upload to GCS, no word count Dataflow job specification, and no actual Dataflow pipeline definition. The operators used (DataflowRunBatchOperator, DataflowCreateBatchOperator) appear to be misused or incorrectly configured - they don't specify an actual Dataflow job template or pipeline code. The BATCH_PIPELINE_OPTIONS don't reference any word count logic or input/output files. Additionally, DataflowDeleteBatchOperator is imported but never used, missing the 'deleting' part of the lifecycle management requested.",0.3333333333333333
853,Fine-tuned (Qwen 2.5 1.5B Airflow),199,1,"The code uses native Airflow providers and operators appropriately: S3CreateBucketOperator, S3DeleteBucketOperator, and S3ToSqlOperator from airflow.providers.amazon.aws. These are the idiomatic, purpose-built operators for S3 operations rather than wrapping hooks in PythonOperators.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', then uses 'watcher()' and 'test_run = get_test_run(dag)'. These are testing utilities not appropriate for production code. Additionally, the ENV_ID pattern and comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' indicate this is test infrastructure code.",0,"The instruction specifically requests 'modern asset-based decorators' to illustrate 'relationship between data producers and consumers' in a data workflow. The provided code does not use any asset-based decorators (like @asset or Dataset features in Airflow 2.4+/3.x), nor does it demonstrate producer-consumer relationships through assets. Instead, it's a simple linear pipeline using traditional operators for S3-to-SQL transfer without any asset management or scheduling based on data dependencies.",0.3333333333333333
854,Fine-tuned (Qwen 2.5 1.5B Airflow),200,0,"The code defines task functions decorated with @task but then tries to instantiate them as classes (t1 = TaskCreateSingularityImage()). The SingularityOperator is imported but never used. Instead, the code wraps shell commands (os.system) inside PythonOperator-style task functions. Idiomatic Airflow would use the SingularityOperator directly for Singularity container operations, not wrap os.system calls in Python callables.",0,"The code explicitly imports from 'tests.system.utils' (line: from tests.system.utils import get_test_run) and includes test harness boilerplate (test_run = get_test_run(dag)). This is internal testing infrastructure that should not appear in production DAG code, indicating hallucination/leakage from test examples.",0,"While the instruction asks for orchestrating system-level tasks with time-based scheduling, command execution, and error handling, this code has multiple critical issues: (1) The task definitions are syntactically broken (decorated functions called as class constructors), (2) No actual error handling is implemented beyond a trigger_rule on the cleanup task, (3) The 'zeugman' command appears to be fictional/non-standard, (4) Connection object is created in DAG context but not properly managed, (5) The code would fail to execute as written. The instruction's core requirements are not properly fulfilled.",0.0
855,Fine-tuned (Qwen 2.5 1.5B Airflow),201,0,"The code misuses Airflow patterns by calling `.operator` on Hooks and Operators (e.g., `ComputeEngineHook(...).operator` and `ComputeEngineInsertInstanceOperator(...).operator`), which is not valid Airflow syntax. Operators should be instantiated directly, not derived from Hooks. Additionally, the code uses Compute Engine operators instead of the appropriate GCS operators (GCSCreateBucketOperator, GCSDeleteBucketOperator) for the stated task of managing Cloud Storage buckets.",0,"The code contains multiple hallucinated/undefined variables (ENDPOINT, DISKR_URI, DISKMRI, DISKNETWORK, DISKENTRY, DISKDSTNAME) and references a non-existent operator `ComputeEngineCopyDiskOperator`. It also has syntax errors (e.g., `with DAG(...) is:` instead of `as dag:`), missing imports (timedelta), and the code is incomplete (truncated at `# [START howto_`). The `.operator` attribute calls on Hook and Operator instances are fabricated patterns not present in Airflow.",0,"The instruction explicitly requests creating a GCS bucket, transferring data, and cleaning up the bucket. However, the generated code focuses on Compute Engine instances and disk operations instead of Cloud Storage bucket operations. It does not use GCSCreateBucketOperator, any data transfer operators for GCS, or GCSDeleteBucketOperator. The code completely misses the core requirement of GCS bucket management.",0.0
856,Fine-tuned (Qwen 2.5 1.5B Airflow),202,0,"While the code does use native Airflow operators like CloudNaturalLanguageAnnotateDocumentOperator and CloudNaturalLanguageClassifyTextOperator, it only uses Google Cloud Natural Language API operators. The instruction specifically requests a 'multimodal machine learning pipeline' that processes 'text and image data' with 'embedding, generation, and safety-filtered content evaluation'. This requires operators for Vertex AI (for multimodal models, embeddings, and generation) and potentially Vision API for image processing, not just Natural Language API for text analysis. The code fails to use the appropriate operators for the multimodal AI task described.",0,"The code contains multiple critical issues: (1) Syntax error with 'is it:' instead of a proper context manager, (2) Malformed XCom pull template string with broken quotes and syntax, (3) References to undefined variables like BUCKET_NAME, (4) Imports 'env_id()' and references 'system_tests_system_test_params' which appear to be test harness patterns, (5) References to non-existent local file paths and resources, (6) The code appears to be a corrupted merge of multiple example files with incomplete and broken sections. The DAG would not execute successfully.",0,"The instruction requires an 'AI-powered content analysis pipeline using multimodal machine learning models to process text AND image data' with 'embedding, generation, and safety-filtered content evaluation'. The provided code only implements basic Google Cloud Natural Language API operations (text annotation and classification) on a single text document. It completely misses: (1) Image processing capabilities, (2) Multimodal model integration (e.g., Gemini via Vertex AI), (3) Embedding generation, (4) Content generation, (5) Safety filtering evaluation, (6) Any pipeline structure that processes multiple input types. The code does not fulfill the core requirements of the instruction.",0.0
857,Fine-tuned (Qwen 2.5 1.5B Airflow),203,1,"The code uses the appropriate native Airflow operator 'GoogleCloudSpreadsheetToGCSOperator' from the Google Cloud provider package, which is specifically designed for transferring data from Google Spreadsheets to GCS. It also uses other idiomatic operators like GCSCreateBucketOperator and LocalFilesystemToGCSOperator rather than wrapping logic in PythonOperators with hooks.",0,"The code contains clear test harness boilerplate and imports internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test setup/teardown logic with comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN'. It also calls 'watcher()' and 'get_test_run(dag)' which are testing utilities not appropriate for production code.",1,"The DAG fulfills the user instruction requirements: it extracts tabular data from a Google Spreadsheet using GoogleCloudSpreadsheetToGCSOperator, transfers it to cloud storage (GCS), implements dynamic environment-based naming via ENV_ID variable (BUCKET_NAME = f'bucket_{DAG_ID}_{ENV_ID}'), and provides a repeatable template for extracting structured data from cloud-based spreadsheet services for downstream processing.",0.6666666666666666
858,Fine-tuned (Qwen 2.5 1.5B Airflow),204,0,"The code uses MSGraphTransferOperator which is not the appropriate operator for transferring data from Azure Blob Storage to S3. MSGraphTransferOperator is designed for Microsoft Graph API operations, not for Azure Blob to S3 transfers. The idiomatic approach would be to use operators like S3CreateBucketOperator, AzureBlobStorageToS3Operator (or a combination of AzureBlobStorageHook with S3Hook in a transfer operator), and S3DeleteBucketOperator. The current implementation does not use the correct providers for the stated task.",0,"The code explicitly imports and uses testing modules that should not be in production code: 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. These are test harness components that indicate this is test boilerplate rather than clean production code. Additionally, the code references 'test_run = get_test_run(dag)' which is a clear test artifact.",0,"The instruction explicitly requests: (1) create a temporary S3 bucket, (2) transfer data from Azure Blob Storage to S3, and (3) delete the S3 bucket after completion. The provided code does none of these correctly. It uses MSGraphTransferOperator (wrong operator), references GCP_PROJECT_ID instead of AWS/S3 resources, transfers to 'azure-blob://' instead of S3, and uses a generic BashOperator for cleanup instead of S3DeleteBucketOperator. The code does not implement the requested Azure Blob to S3 transfer workflow with proper setup and teardown stages.",0.0
859,Fine-tuned (Qwen 2.5 1.5B Airflow),205,1,"The code uses native Airflow providers and operators appropriately (GCSCreateBucketOperator, GCSDeleteBucketOperator, CloudNaturalLanguageTextExtractEntitySetCreateOperator, etc.) rather than wrapping logic in PythonOperators with hooks. This is idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(watcher(dag))'. These are testing artifacts that should not appear in production code.",0,"The instruction requests a machine learning data processing pipeline for housing price prediction with dataset preparation and model training steps. However, the provided code implements a Google Cloud Natural Language API workflow for text entity extraction, which is completely unrelated to housing price prediction, data transformation, or ML model training. This appears to be an entirely different use case that does not fulfill any aspect of the user's instruction.",0.3333333333333333
860,Fine-tuned (Qwen 2.5 1.5B Airflow),207,0,"The code uses non-standard, custom operators like 'third_party_config.CreateThirdPartyDatasetOperator' and 'third_party_config.CreateThirdPartyPipelineJobOperator' instead of the official Airflow providers for Google Cloud Vertex AI. Idiomatic Airflow code should use operators from 'airflow.providers.google.cloud.operators.vertex_ai' such as 'CreateDatasetOperator' and 'CreatePipelineJobOperator'. The pattern here appears to wrap functionality in custom operators rather than using the native, purpose-built operators available in the Google Cloud provider package.",0,"The code imports from 'system.google' modules ('from system.google import third_party_config' and 'from system.google import workspace_config') which are not standard Airflow libraries. These appear to be internal testing modules or custom test harness code that should not be present in production DAG code. Additionally, the code is incomplete (truncated at 'CreateThirdPartyPipe'), and references to 'workspace_config' are imported but never used, suggesting this is test/example boilerplate rather than clean production code.",0,"The instruction requests a complete workflow that: (1) synchronizes housing data, (2) creates a tabular dataset, (3) runs a predefined ML pipeline, and (4) manages the pipeline job lifecycle. The provided code only partially addresses these requirements. It attempts to create a dataset and pipeline jobs, but there is no data synchronization step for housing data, no actual pipeline execution or monitoring, and no lifecycle management (e.g., checking status, canceling, or deleting jobs). The code is also incomplete/truncated and contains configuration errors (e.g., referencing 'TABULAR_DATASET[""name""]' before it exists). The workflow does not demonstrate a complete end-to-end ML pipeline as requested.",0.0
861,Fine-tuned (Qwen 2.5 1.5B Airflow),209,0,"The code uses PrestoOperator instead of the SQLExecuteQueryOperator as explicitly requested in the user instruction. The instruction specifically asks to 'illustrates the core principles of task sequencing and SQL query execution using the SQLExecuteQueryOperator' but the implementation uses the older, provider-specific PrestoOperator. For Airflow 3.0.6, the idiomatic approach would be to use airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator with a Presto connection.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready DAG code. This is a clear example of test infrastructure leaking into the generated code.,1,"The DAG successfully demonstrates task sequencing and SQL query execution against a Presto database with fundamental data manipulation tasks (DROP, CREATE, INSERT, SELECT). The workflow logically sequences these operations: drop_table >> create_table >> insert_data >> select_data, which illustrates proper task dependencies and covers the core SQL operations requested. Despite using the wrong operator type, the business logic and sequencing requirements are met.",0.3333333333333333
862,Fine-tuned (Qwen 2.5 1.5B Airflow),210,0,"While the DAG uses native Weaviate operators (WeaviateCreateClassOperator, WeaviateBatchIngestOperator, WeaviateDeleteAllClassesOperator), it also includes two @task decorated functions (add_classes and remove_classes) that wrap WeaviateHook logic in PythonOperators. This is not idiomatic - these operations should use native Weaviate operators if available, or at minimum should be justified. The add_classes task duplicates functionality that could be handled by WeaviateCreateClassOperator, and remove_classes could potentially use WeaviateDeleteAllClassesOperator instead of wrapping hook calls.",0,"The code imports several classes from the weaviate library that appear to be non-standard or potentially hallucinated: 'weaviate.classes.config.DataClass', 'weaviate.components.utils.get_qualified_class_name', 'weaviate.embedders.config.EmbedDim', and 'weaviate.information.sources.config.SourceInfo'. These imports suggest either outdated Weaviate client library usage or hallucinated modules. Additionally, 'WeaviateBatchInferenceOperator' is imported but never used, and 'WeaviateBatchIngestOperator' is used but was not in the import statement (only WeaviateBatchInferenceOperator was imported). The code also has logical issues like executing batch_ingest before creating the class, which would fail in practice.",0,"The DAG has significant logical flow issues that prevent it from fulfilling the instruction properly. The task dependencies show 'batch_ingest >> create_class_with_vector_and_source_info', meaning data ingestion happens BEFORE class creation, which is impossible. There are also duplicate/conflicting dependency chains defined. While the DAG attempts to demonstrate creating a Weaviate class, preparing data, and loading it using operators, the execution order is broken and would fail. The instruction asks for a workflow that 'shows how to create a Weaviate class, prepare data, and load it' in that logical order, but this DAG does not achieve that correctly.",0.0
863,Fine-tuned (Qwen 2.5 1.5B Airflow),211,0,"The code uses 'BigQueryStagingFolderOperator' which is imported from 'airflow.providers.google.cloud.hooks.bigquery'. This is incorrect - operators should be imported from the operators module, not hooks. Additionally, 'BigQueryStagingFolderOperator' does not appear to be a real Airflow operator. The code also lacks proper BigQuery operators for creating datasets, tables, and executing SQL queries as requested in the instruction. A proper implementation should use operators like BigQueryCreateEmptyDatasetOperator, BigQueryCreateEmptyTableOperator, BigQueryInsertJobOperator, or BigQueryExecuteQueryOperator.",0,"The code contains multiple hallucinations and test harness boilerplate. It imports 'pytest' and uses 'pytest.skip()', imports 'from tests.system.utils.watcher import watcher', and includes 'from tests.system.utils import get_test_run' with 'test_run = get_test_run(dag)'. These are all testing utilities that should not be present in production DAG code. Additionally, 'BigQueryStagingFolderOperator' appears to be a non-existent/hallucinated operator.",0,"The instruction requested a pipeline that demonstrates: (1) creating datasets, (2) creating tables, (3) executing SQL queries for data insertion, and (4) executing SQL queries for data retrieval. The provided code only performs table-to-table copy operations using BigQueryToBigQueryOperator and a hallucinated operator. It does not create any datasets, does not create any tables, does not insert data via SQL queries, and does not demonstrate data retrieval. The code fails to fulfill the core requirements of the instruction.",0.0
864,Fine-tuned (Qwen 2.5 1.5B Airflow),213,0,"The code does not use TriggerDagRunOperator at all, which is the idiomatic Airflow operator specifically designed for triggering other DAGs. Instead, it incorrectly attempts to use BashOperator to call a Python function, which is neither idiomatic nor functional for DAG triggering scenarios.",1,"The code uses only standard Airflow libraries (airflow.providers.standard.operators.bash, airflow.sdk) and does not import any internal testing modules or include test harness boilerplate. While the implementation is incorrect, there are no hallucinated or leaked testing artifacts.",0,"The instruction explicitly requests a workflow demonstrating TriggerDagRunOperator to trigger a secondary DAG from a primary DAG with a predefined configuration message. The provided code completely fails to meet this requirement: (1) It does not use TriggerDagRunOperator, (2) It only contains a single DAG instead of primary and secondary DAGs, (3) It does not demonstrate DAG triggering functionality, and (4) It does not pass any configuration message between DAGs. The code appears to be unrelated to the instruction.",0.3333333333333333
865,Fine-tuned (Qwen 2.5 1.5B Airflow),214,1,"The code uses native Dataproc operators from airflow.providers.google.cloud.operators.dataproc (DataprocCreateWorkflowTemplateOperator, DataprocSubmitWorkflowOperator, DataprocDeleteWorkflowTemplateOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Google Cloud Dataproc operations.",0,"The code contains clear test harness boilerplate at the end: 'from tests.system.utils import get_test_run' followed by test assertions ('test_run = get_test_run(dag)' and 'assert test_run.state == ""BATCH""'). This is internal testing code that should not be present in production DAG code. Additionally, there's a comment with mixed English and Chinese characters ('BATCH状态'), suggesting copy-paste from test files.",0,"The instruction asks for a Pig job on a Hadoop/Spark cluster, but the PIG_JOB definition uses 'query_list' with 'sql_statement' which is incorrect for Pig jobs (this is SQL syntax). A proper Pig job should use 'pig_job' with 'query_list' containing Pig Latin scripts, not SQL statements. Additionally, the workflow template's placement configuration uses 'cluster_spec' but doesn't include the actual CLUSTER_CONFIG defined earlier, missing the cluster configuration setup that was requested. The workflow doesn't properly demonstrate setting up a Hadoop/Spark cluster as the cluster configuration is defined but not integrated into the workflow template.",0.3333333333333333
866,Fine-tuned (Qwen 2.5 1.5B Airflow),215,0,"While the code does use some native Airflow operators (EmrCreateJobFlowOperator, EmrAddStepsOperator, S3CreateBucketOperator), it also contains anti-patterns such as the 'get_step_log' task which attempts to create a BashOperator inside a Python function decorated with @task, which is not idiomatic. The task decorator should contain pure Python logic, not operator instantiation. Additionally, the code is focused on EMR operations rather than DynamoDB operations as requested in the instruction.",0,"The code clearly imports and uses test harness infrastructure: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder'. It also uses 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' which is test boilerplate, not production code. Additionally, there are code quality issues like non-ASCII characters (Chinese characters '公钥文件') and incomplete code (ends with '# [STA'), indicating this is not production-ready.",0,"The instruction explicitly requests a DAG that: (1) creates a DynamoDB table, (2) enables point-in-time recovery on DynamoDB, and (3) performs a segmented export to S3. The provided code instead implements an EMR (Elastic MapReduce) data pipeline with Spark jobs. There is no DynamoDB table creation, no point-in-time recovery configuration, and no DynamoDB export functionality. The code completely fails to address the core requirements of the instruction.",0.0
867,Fine-tuned (Qwen 2.5 1.5B Airflow),217,0,"The code uses MySqlOperator for data validation checks, but Airflow provides specialized operators for this purpose. For data integrity verification and checks, the code should use MySqlCheckOperator (for boolean checks), MySqlValueCheckOperator (for value comparisons), or MySqlIntervalCheckOperator. These operators are specifically designed for validation workflows and will fail the task if checks don't pass, whereas MySqlOperator just executes SQL without built-in validation logic.",0,The code contains test harness imports at the bottom: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not appear in production DAG code and represent test boilerplate leakage.,0,"The instruction requires: (1) column-level constraint validation, (2) row count confirmation. The provided code only checks table existence and column existence, but does NOT validate column-level constraints (e.g., NOT NULL, data types, unique constraints) nor does it confirm the expected number of rows in the table. It performs basic structural checks but misses the core requirements of constraint validation and row count verification.",0.0
868,Fine-tuned (Qwen 2.5 1.5B Airflow),218,0,"While the DAG uses native Display Video 360 operators (which is good), it violates idiomatic Airflow patterns in the `wait_for_creation` task. This task uses a `@task` decorator (PythonOperator) that instantiates and executes a `GoogleDisplayVideo360ListLineItemsFilesOperator` inside a polling loop with `time.sleep()`. This is anti-pattern - it should use a Sensor or the operator's native async/deferrable capabilities instead of blocking execution with a sleep loop inside a Python function.",1,"The code uses only standard Airflow libraries from the official Google Marketing Platform provider package. There are no imports from test utilities, no test harness boilerplate like `get_test_run()`, and no internal testing modules. The code is clean and production-ready from an import perspective, despite being labeled as an 'example' DAG.",0,"The instruction specifically requests: (1) extracting advertising performance metrics, (2) managing data transfer between Display Video 360 AND BigQuery, (3) downloading line items, (4) uploading files, and (5) performing data transformations. The provided DAG only covers Display Video 360 operations (creating/deleting line items, uploading files, listing files) but completely omits BigQuery integration, data transformations, and actual performance metrics extraction. There are no BigQuery operators, no data transformation tasks, and no metrics analysis. The DAG is incomplete relative to the requirements.",0.3333333333333333
869,Fine-tuned (Qwen 2.5 1.5B Airflow),219,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Data Catalog operations. It imports and uses CloudDataCatalogCreateTagTemplateOperator, CloudDataCatalogGetTagTemplateOperator, CloudDataCatalogUpdateTagTemplateOperator, and CloudDataCatalogDeleteTagTemplateOperator from airflow.providers.google.cloud.operators.datacatalog. This is the idiomatic approach rather than wrapping logic in PythonOperator with hooks.",0,"The code contains a syntax error that appears to be AI hallucination. In the create_entry_gcs task, there is an invalid line: 'source=GCS_SOURCE=https://storage.googleapis.com/{BUCKET_NAME}/{FILE_PATH_1}'. This uses an invalid double assignment syntax (GCS_SOURCE=) that would cause a Python syntax error. Additionally, the code is incomplete - it cuts off mid-task definition at 'create_tag_template' without completing the operator instantiation or showing the rest of the DAG workflow.",0,"The instruction asks for a tag template management workflow demonstrating creating, retrieving, and modifying metadata templates. While the code imports the necessary operators (CloudDataCatalogCreateTagTemplateOperator, CloudDataCatalogGetTagTemplateOperator, CloudDataCatalogUpdateTagTemplateOperator, CloudDataCatalogDeleteTagTemplateOperator), the DAG code is incomplete and cuts off before actually implementing these tag template operations. The create_tag_template task is not completed, and there are no retrieve (get) or modify (update) operations shown in the actual DAG definition. The code focuses more on entry groups and entries rather than the tag template lifecycle that was requested.",0.3333333333333333
870,Fine-tuned (Qwen 2.5 1.5B Airflow),221,0,"While the code uses GCSToGCSOperator which is idiomatic, it incorrectly uses LocalFilesystemToGCSOperator for GCS-to-GCS operations (list_of_files, create_copy_of_files, and remove_file tasks). LocalFilesystemToGCSOperator is designed to transfer files from local filesystem to GCS, not for GCS-to-GCS operations. The correct operators should be GCSToGCSOperator for copying/moving and GCSDeleteObjectsOperator or GCSListObjectsOperator for listing and deletion operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not appear in production code. Additionally, comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' indicate this is test code, not production-ready.",0,"The instruction requires a pipeline that 'transfers files between cloud storage buckets while implementing systematic tracking and cleanup procedures' with capabilities to 'safely moved, listed, and deleted'. The code attempts file transfer but fails to properly implement: (1) Listing operations - uses wrong operator (LocalFilesystemToGCSOperator instead of GCSListObjectsOperator), (2) Deletion/cleanup - the 'remove_file' task incorrectly uses LocalFilesystemToGCSOperator which uploads rather than deletes, (3) No systematic tracking mechanism is implemented. The code misuses operators throughout and doesn't fulfill the core requirements.",0.0
871,Fine-tuned (Qwen 2.5 1.5B Airflow),223,0,"The code uses boto3 client directly in PythonOperator tasks (via @task decorator) to create and delete Glue DataBrew job runs, instead of using the native Airflow providers. The proper approach would be to use GlueDataBrewStartJobOperator or similar operators from airflow.providers.amazon.aws.operators.glue_databrew. The create_job_run and delete_job_run tasks wrap boto3 calls in Python callables, which is the anti-pattern described in the scoring criteria.",0,"The code imports and uses test harness components that should not be in production code: 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'from tests_common.test_utils.watcher import watcher'. These are internal testing modules that indicate this is test boilerplate rather than production-ready code. The SystemTestContextBuilder and watcher() are clear examples of test harness leakage.",0,"The instruction asks for an AWS Glue DataBrew data profiling pipeline for JSON datasets in S3 with full lifecycle management. However, the code has multiple critical issues: 1) It incorrectly uses boto3.client('glue') instead of 'databrew' client, 2) The workflow structure is malformed with duplicate task definitions and invalid DAG context syntax, 3) It doesn't demonstrate actual data profiling capabilities (creating datasets, recipes, profile jobs), 4) The GlueDatabrewStartOperator is misused to 'stop' a job run which doesn't align with its purpose, 5) Missing key DataBrew components like dataset creation, recipe definition, and profile job configuration. The code appears to be a confused mix of Glue (ETL) and DataBrew (profiling) concepts without properly implementing either.",0.0
872,Fine-tuned (Qwen 2.5 1.5B Airflow),224,0,"The code uses 'GCSToPrestoOperator' which does not exist in the official Apache Airflow providers. The airflow.providers.google.cloud.transfers module does not contain a 'gcs_to_presto' operator. This is a hallucinated/non-existent operator. A proper implementation would require using a combination of actual operators like GCSToLocalFilesystemOperator with PrestoOperator, or a PythonOperator with appropriate hooks.",0,"The code contains multiple issues: (1) It imports 'from tests.system.utils import get_test_run' which is internal testing infrastructure, not production code. (2) It includes 'test_run = get_test_run(dag)' which is test harness boilerplate. (3) Most critically, it uses a non-existent 'GCSToPrestoOperator' that doesn't exist in any official Airflow provider package.",0,"The instruction asks to 'transfer CSV files from Google Cloud Storage to a Presto table'. However, the code has several problems: (1) The 'sql_stmt' parameter is incorrectly set to 'GCS_PATH + SQL' which concatenates a GCS path with an INSERT statement, making no logical sense. (2) The SQL statement shown is a hardcoded INSERT with literal values, not a CSV data transfer operation. (3) The operator itself doesn't exist, so it cannot fulfill the requirement. A proper implementation would read CSV files from GCS and load them into Presto, which this code does not accomplish.",0.0
873,Fine-tuned (Qwen 2.5 1.5B Airflow),225,1,"The code uses the native ShortCircuitOperator from airflow.operators.python, which is the idiomatic Airflow way to implement short-circuit logic. It does not wrap functionality in unnecessary PythonOperators or use hooks where operators are available.",1,"The code is clean and production-ready. It only imports standard Airflow libraries (airflow.models.dag, airflow.operators.empty, airflow.operators.python) and pendulum for date handling. There are no test harness imports or internal testing modules present. The START/END comments are standard Airflow documentation markers, not test boilerplate.",0,"The instruction asks for a workflow that 'demonstrates conditional task execution using short-circuit logic, where certain tasks only run based on predefined boolean conditions' and to 'showcase how tasks can be dynamically skipped or executed depending on specific criteria.' The provided code only has 2 tasks with a ShortCircuitOperator that always returns False, which means downstream tasks would always be skipped. However, there are NO downstream tasks after the ShortCircuitOperator to actually demonstrate the skipping behavior. A proper demonstration would include tasks after the short-circuit operator to show which tasks get skipped and which continue to run. The DAG fails to showcase the actual effect of short-circuit logic on a workflow.",0.6666666666666666
874,Fine-tuned (Qwen 2.5 1.5B Airflow),226,1,"The code correctly uses the DockerSwarmOperator from airflow.providers.docker.operators.docker_swarm, which is the appropriate provider-specific operator for Docker Swarm tasks. It does not wrap Docker logic in a PythonOperator with hooks, demonstrating idiomatic Airflow usage.",0,"The code contains several issues indicating hallucination: (1) Uses undefined variable 'TEST_NAME' in the command parameter, (2) Passes 'dag=DAG_ID' which should be 'dag=dag' or omitted in context manager, (3) Has a syntax error 'with DAG(...) is' instead of 'with DAG(...) as dag:', (4) Includes test-like comments '[START howto_operator_docker_swarm]' and '[END howto_operator_docker_swarm]' suggesting it's copied from test/example code, (5) Defines an unnecessary @task decorator function that doesn't contribute to the workflow.",0,"The code fails to meet key requirements: (1) The command parameter 'f""-n {TEST_NAME} --sleep=10""' references an undefined variable and doesn't clearly execute a sleep operation as requested, (2) The instruction asks for 'automatic removal after completion' but the code doesn't include 'auto_remove=True' or 'remove=True' parameter in DockerSwarmOperator, (3) The workflow has a syntax error preventing execution, (4) The unnecessary test() task doesn't align with the instruction to create a simple container workflow.",0.3333333333333333
875,Fine-tuned (Qwen 2.5 1.5B Airflow),227,0,"The code misuses Airflow operators. It uses 'SFTPSensor' for delete operations (delete_file, delete_file_deferrable) when it should use an operator like SFTPOperator or a dedicated delete operator. Additionally, 'LocalFileSystemToSFTPTempSensor' appears to be incorrectly named/used - sensors are for waiting/monitoring, not for uploading files. The correct operator would be LocalFilesystemToSFTPOperator. The code conflates sensors (which wait for conditions) with operators (which perform actions).",0,"The code contains multiple hallucinations and test harness artifacts: (1) Imports from 'tests.system.providers.sftp.utils' and 'tests.system.utils.watcher', (2) Uses 'get_test_run' from 'tests.system.utils', (3) Includes test_run = get_test_run(dag) at the end, (4) Contains 'LocalFileSystemToSFTPTempSensor' which is not a real Airflow class, (5) Has duplicate import of SFTPSensor, (6) Includes test-specific code like 'need_to_run_in_test', ENV_ID variables, and op.set_visible(False). This is clearly test/example code, not production-ready.",0,"The instruction asks for monitoring an input directory for new files, validating file presence, and performing controlled file transfer using containerized operations with error handling and logging. The code: (1) Does not use containerized operations (no DockerOperator or KubernetesPodOperator), (2) Does not monitor a directory for 'new' files dynamically, (3) Uses hardcoded file paths instead of a flexible directory monitoring mechanism, (4) Does not implement proper error handling or logging beyond default Airflow behavior, (5) Misuses sensors for file deletion instead of actual file transfer/processing operations. The code appears to be a test example rather than a solution to the stated requirements.",0.0
876,Fine-tuned (Qwen 2.5 1.5B Airflow),228,1,"The code uses TrinoToParquetOperator, which is a specific provider operator from airflow.providers.trino. This is idiomatic Airflow usage rather than wrapping logic in a PythonOperator with hooks.",0,"The code contains multiple instances of test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' twice and calls 'test_run = get_test_run(dag)' twice. These are internal testing utilities that indicate hallucination/leakage from example/test code.",0,"The instruction requires a comprehensive data quality workflow with: (1) raw data ingestion, (2) initial analysis, (3) data integrity checks, (4) branching logic, (5) saving clean data, (6) report generation, (7) error handling with issue description, and (8) notifications. The provided code only contains a single TrinoToParquetOperator that selects from a table. It completely fails to implement the branching process, data quality checks, error handling, notifications, or any of the multi-step workflow requirements specified in the instruction.",0.3333333333333333
877,Fine-tuned (Qwen 2.5 1.5B Airflow),229,1,"The code uses the native OpenAITaskOperator from airflow.providers.openai.operators.openai, which is the idiomatic way to interact with OpenAI in Airflow. It does not wrap OpenAI logic in a PythonOperator with hooks.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and should not be in a production DAG.",0,"The instruction asks for an automated data enrichment pipeline that: (1) uses OpenAI to generate descriptive information about a static list of items, (2) leverages task expansion, and (3) uses deferred execution capabilities. The provided code only demonstrates a single OpenAI task with hardcoded input text. It does not process a list of items, does not use dynamic task mapping/expansion (e.g., .expand()), and does not demonstrate deferred execution (e.g., deferrable operators or triggers). The code is a basic example rather than the requested enrichment pipeline.",0.3333333333333333
878,Fine-tuned (Qwen 2.5 1.5B Airflow),230,0,"The code uses PgDBOperator for PostgreSQL operations, which is a relational database operator, not appropriate for NoSQL databases or Google Cloud platforms. For the described use case (NoSQL to GCS to BigQuery pipeline), idiomatic Airflow would use operators like BigtableToGCSOperator, FirestoreExportDatabaseOperator, GCSToBigQueryOperator, or DataflowOperator from airflow.providers.google.cloud packages.",0,The code explicitly imports and uses test harness boilerplate: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production DAG code.,0,"The instruction requires a cloud-based pipeline that: (1) extracts from NoSQL database, (2) exports to neutral storage format, (3) prepares for analytical querying, and (4) uses Google Cloud platforms. The provided code operates on PostgreSQL (a relational database, not NoSQL), does not involve Google Cloud services, performs no data transformation or export to neutral storage, and does not prepare data for analytical querying. It completely fails to address the user's requirements.",0.0
879,Fine-tuned (Qwen 2.5 1.5B Airflow),231,1,"The code correctly uses the Neo4jOperator from airflow.providers.neo4j.operators.neo4j, which is the idiomatic way to interact with Neo4j in Airflow. It does not wrap Neo4jHook in a PythonOperator, but rather uses the native operator designed for this purpose.",0,"The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. This is test harness boilerplate that should not be present in production DAG code. Additionally, there are typos in the print_in_out function ('tome_hanks' instead of 'tom_hanks'), and the function references parameters that don't match the actual data structure returned by Neo4j queries.",0,"The instruction specifically requires using 'the current date as a dynamic parameter' in the Cypher query. The provided code does not incorporate any date parameter or dynamic date logic in the queries. The queries are static ('MATCH (tomHanks {name:'Tom Hanks'}) RETURN tomHanks') and do not use the current execution date. Additionally, the DAG includes extraneous tasks (latest_run_timer with a DELETE query) that are not part of the requirement to retrieve Tom Hanks' information.",0.3333333333333333
880,Fine-tuned (Qwen 2.5 1.5B Airflow),233,0,"The code uses DatabricksSubmitRunOperator which is designed for submitting notebook/jar jobs, not for interacting with Databricks SQL endpoints. For SQL operations (table creation, data insertion, transformations, file export/import), the code should use DatabricksSqlOperator from airflow.providers.databricks.operators.databricks_sql, which is the idiomatic operator specifically designed for executing SQL queries against Databricks SQL endpoints.",0,"The code contains multiple issues: 1) Syntax error on line with 'schedule=models schedules.always()' (invalid syntax), 2) The code is incomplete/truncated at the end, 3) Uses non-standard patterns like '.databricks_conn_id' in default_args (leading dot is unusual), 4) References to 'job_cluster_spec' in default_args which is not a standard default_arg parameter. While it doesn't import test utilities, the code quality issues and syntax errors indicate it's not production-ready.",0,"The instruction explicitly asks for a pipeline demonstrating interaction with Databricks SQL endpoints for creating tables, inserting data, and performing data transformations with specific SQL operations (table creation, data selection, file export, data import). The provided code only uses DatabricksSubmitRunOperator to run notebooks and DatabricksSensor to check job status. It does not demonstrate any SQL operations, table creation, data insertion, selection, file export, or import. The code completely misses the core requirement of showcasing SQL endpoint interactions.",0.0
881,Fine-tuned (Qwen 2.5 1.5B Airflow),234,0,"The code uses PgSQLToSQLiteOperator which is not appropriate for the user instruction. The instruction explicitly asks for 'cross-database interaction and query execution in a Cloud SQL environment' with 'PostgreSQL and MySQL instances', but the code uses PostgreSQL to SQLite transfers instead. For Cloud SQL PostgreSQL and MySQL, the code should use CloudSQLExecuteQueryOperator or similar Cloud SQL-specific operators from airflow.providers.google.cloud.operators.cloud_sql, not SQLite-based operators.",0,"The code contains multiple critical issues: (1) Uses 'nullcontext(open(...))' incorrectly in task dependencies, which is not valid Airflow syntax; (2) The code is incomplete and cuts off mid-statement ('>> postgr'); (3) Contains invalid syntax like 'output_csv=nullcontext(open(""output.csv"", ""w""))' which is not how SqlToCsvOperator works; (4) Attempts to chain file operations like 'open(""failure.txt"", ""a"")' directly in task dependencies, which is nonsensical; (5) Missing closing bracket on csv_to_json_async task definition. This code would not run and appears to be corrupted or hallucinated.",0,"The instruction specifically requests a workflow for Cloud SQL environment demonstrating cross-database interaction between PostgreSQL and MySQL instances. The generated code completely misses this requirement by: (1) Using SQLite instead of MySQL; (2) Not using any Cloud SQL operators or providers; (3) Not demonstrating any cross-database interaction between PostgreSQL and MySQL as requested. The code implements PostgreSQL-to-SQLite transfers, which is entirely different from the requested PostgreSQL and MySQL Cloud SQL workflow.",0.0
882,Fine-tuned (Qwen 2.5 1.5B Airflow),235,0,"The code imports 'CloudDataJoinOperator' from 'airflow.providers.google.cloud.operators.datajoin', which does not exist in the official Airflow providers package. This is a hallucinated/non-existent operator. There is no such operator in the Google Cloud provider for Airflow 2.7.2. The code appears to use proper operators for GCS operations (GCSCreateBucketOperator, LocalFilesystemToGCSOperator, GCSDeleteBucketOperator), but the core data processing operator is fabricated.",0,The code contains multiple hallucinations: (1) It imports 'CloudDataJoinOperator' from a non-existent module 'airflow.providers.google.cloud.operators.datajoin' - this operator does not exist in any official Airflow provider. (2) It imports 'from tests.system.utils import get_test_run' which is internal testing infrastructure and should not be in production code. (3) It includes test harness boilerplate 'test_run = get_test_run(dag)' at the end. These are clear violations of production-ready code standards.,1,"Despite the hallucinated operator, the DAG structure does attempt to fulfill the instruction requirements: it creates a bucket (bucket creation), uploads files (file uploads), performs data processing using a template-based operator approach (template-based data processing via the CloudDataJoinOperator with configured parameters), and includes automated cleanup (delete_bucket with TriggerRule.ALL_DONE). The workflow orchestration and task dependencies are properly defined to match the requested cloud data processing pipeline pattern.",0.3333333333333333
883,Fine-tuned (Qwen 2.5 1.5B Airflow),236,0,"The code uses non-existent operators 'GCSCreateObjectStorageFileOperator' and 'CloudVisionDetectImageOperator' which are not part of the standard Airflow Google Cloud provider package. The correct operators would be 'GCSObjectCreateAclEntryOperator' or using 'BashOperator' with gsutil, and there is no 'CloudVisionDetectImageOperator' in the official providers - image detection would typically use 'CloudVisionDetectImageLabelsOperator', 'CloudVisionDetectTextOperator', or similar specific detection operators. This demonstrates a failure to use the actual idiomatic Airflow operators available in the providers package.",0,"The code imports and attempts to use 'GCSCreateObjectStorageFileOperator' and 'CloudVisionDetectImageOperator' which do not exist in the standard Airflow providers library (airflow.providers.google.cloud version compatible with Airflow 2.7.2). These are hallucinated operators that are not part of the actual Apache Airflow codebase. Additionally, the code is incomplete (cuts off mid-statement at 'task_id'), suggesting it may be generated test code or incomplete boilerplate.",0,"While the DAG attempts to demonstrate product and image management capabilities with Vision API (creating products, product sets, reference images, etc.), it fails to complete the workflow. The code is truncated and incomplete - it cuts off in the middle of defining the 'update_product_labels' task. More critically, the 'detect_image' task uses a non-existent operator, so the core 'analyzing image resources' requirement cannot be fulfilled. The DAG structure shows intent to cover the end-to-end process but the implementation is broken and incomplete, failing to deliver a working demonstration of Google Cloud Vision's capabilities.",0.0
884,Fine-tuned (Qwen 2.5 1.5B Airflow),237,1,"The code uses the native DynamodbToS3Operator from airflow.providers.amazon.aws.transfers.dynamodb_to_s3, which is the idiomatic Airflow approach for transferring data from DynamoDB to S3. It also uses S3CreateBucketOperator and S3DeleteBucketOperator from the AWS provider package, demonstrating proper use of Airflow operators rather than wrapping hooks in PythonOperators.",0,"The code contains multiple hallucinations and test harness artifacts: (1) imports 'from tests_common.test_utils.watcher import watcher' which is a testing module, (2) imports 'from tests_common.test_utils.system_tests import get_test_run' at the bottom, (3) includes 'test_run = get_test_run(dag)' which is test boilerplate, (4) references a non-existent module 'from dynamodb_to_s3_operator_tester import test_operator' with sys.path manipulation, (5) has a syntax error 'with DAG(...) is:' instead of 'with DAG(...) as dag:', and (6) the DynamodbToS3Operator is being misused with non-existent parameters like 'sample_data' and 'sample_data_columns' that don't exist in the actual operator API.",0,"The code fails to meet the instruction requirements: (1) It does not demonstrate actual data backup from an existing DynamoDB table - instead it appears to attempt inserting sample data which is not how DynamodbToS3Operator works, (2) There is no implementation of point-in-time recovery capabilities, (3) There is no demonstration of incremental data export strategies - the operator is called once without any incremental logic, (4) The task naming 'insert_into_dynamodb' is misleading as it should be exporting/backing up from DynamoDB to S3, not inserting. The instruction asked for a backup workflow with incremental export and point-in-time recovery, none of which are properly implemented.",0.3333333333333333
885,Fine-tuned (Qwen 2.5 1.5B Airflow),238,0,"While the code does use specific Data Catalog operators (CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, etc.), several of these operators appear to be non-existent or incorrectly named. Specifically, 'CloudDataCatalogCreateEntrySplitOperator' and 'CloudDataCatalogDeleteEntrySplitOperator' are not standard Airflow Data Catalog operators in version 2.7.2. The standard operators for Data Catalog in Airflow 2.7.2 include CreateEntryGroupOperator, CreateEntryOperator, CreateTagOperator, etc., but not the 'Split' variants used here. This suggests the code is using hallucinated or non-existent operators rather than the actual idiomatic Airflow operators available.",0,"The code contains clear hallucination issues: (1) It imports test harness code at the bottom: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(da' (incomplete line), which is test boilerplate that should not be in production code. (2) It uses operators that don't exist in the standard Airflow providers package (CloudDataCatalogCreateEntrySplitOperator, CloudDataCatalogDeleteEntrySplitOperator). (3) The code is incomplete/truncated at the end. (4) There's an undefined variable 'BUCKET' referenced in CREATE_ENTRY_SPLIT_BODY.",0,"The instruction asks for a workflow that demonstrates 'creating and managing metadata for a Google Cloud Storage bucket' with emphasis on 'register, tag, and catalog cloud storage resources for improved data governance and discoverability.' However, the code: (1) Does not include any tagging operations (no CloudDataCatalogCreateTagOperator or similar), which is a key requirement for data governance. (2) Does not actually reference or interact with a real GCS bucket (the BUCKET variable is undefined). (3) Uses non-existent 'Split' operators that don't align with the instruction. (4) Focuses on entry creation/deletion but misses the critical 'tag' component explicitly mentioned in the requirements. The code demonstrates entry group and entry management but fails to showcase the complete metadata management workflow requested, particularly the tagging aspect.",0.0
886,Fine-tuned (Qwen 2.5 1.5B Airflow),239,0,"The code uses SqlHook wrapped in @task decorated functions (PythonOperator pattern) instead of using the idiomatic SqlExecuteQueryOperator or database-specific operators (e.g., PostgresOperator, MySqlOperator, SnowflakeOperator). The proper Airflow pattern would be to use `from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator` for executing SQL commands directly, rather than wrapping hooks in Python callables.",0,The code contains test harness boilerplate at the bottom with `from tests_common.test_utils.watcher import watcher` and `list(dag.tasks) >> watcher()`. This is internal testing infrastructure that should not appear in production DAG code. The `if __name__ == '__main__'` block with watcher is a clear hallucination/leakage from Airflow's internal test suite.,0,"The instruction asks for a pipeline that 'prints context, logs SQL queries, performs time-based operations, and demonstrates isolated Python environment executions.' The provided code only demonstrates SQL operations (create table, insert, delete) but does NOT: (1) print context information, (2) explicitly log SQL queries in a demonstrative way, (3) perform any time-based operations, or (4) demonstrate isolated Python environment executions (e.g., using @task.virtualenv or @task.docker). The code is a basic SQL example that doesn't fulfill the educational exploration requirements specified.",0.0
887,Fine-tuned (Qwen 2.5 1.5B Airflow),241,1,"The DAG uses native Airflow providers and operators throughout. It imports and uses specific operators from airflow.providers.google.cloud.operators.vision (CloudVisionCreateProductOperator, CloudVisionAddProductToProductSetOperator, etc.) and airflow.providers.google.cloud.operators.gcs (GCSCreateBucketOperator, GCSDeleteBucketOperator) along with GCSToGCSOperator for transfers. No PythonOperator wrappers around hooks are used - all tasks leverage the appropriate native operators for their respective cloud services.",0,"The code contains a clear hallucination/test harness import: 'from system.test_utils.watcher import watcher'. This is an internal testing module that should not appear in production DAG code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are typical of test scaffolding rather than production code. These are clear indicators of test boilerplate leaking into the generated code.",0,"The instruction specifically asks to 'copy an image file between buckets and perform multiple types of computer vision analysis using different Google Cloud Vision operators.' While the DAG does copy files using GCSToGCSOperator, it does NOT perform any actual computer vision analysis tasks. The operators used (CloudVisionCreateProductOperator, CloudVisionAddProductToProductSetOperator, CloudVisionGetProductOperator, etc.) are all product catalog management operators, not vision analysis operators. The instruction requires vision analysis operations like label detection, text detection, face detection, landmark detection, etc., which are completely missing. The DAG focuses on product set management rather than image analysis, failing to meet the core requirement.",0.3333333333333333
888,Fine-tuned (Qwen 2.5 1.5B Airflow),242,1,"The code uses the native TrinoToBigQueryOperator from airflow.providers.trino.transfers, which is the idiomatic way to transfer data from Trino to BigQuery. It does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)'. It also uses 'list(dag.tasks) >> watcher()' which is test infrastructure code. This is not production-ready code.",0,"The instruction explicitly requests a complete data transfer lifecycle including: (1) bucket and dataset creation, (2) data extraction from Trino information schema, (3) external table generation in BigQuery, (4) robust idempotent workflow, and (5) resource provisioning and cleanup. The provided code only contains a single TrinoToBigQueryOperator that transfers data. It does not create buckets or datasets, does not query the Trino information schema specifically, does not handle resource provisioning or cleanup tasks, and lacks the idempotent workflow structure with setup/teardown tasks that the instruction requires.",0.3333333333333333
889,Fine-tuned (Qwen 2.5 1.5B Airflow),243,1,"The code uses native Airflow providers and operators designed for Google Cloud Vision tasks (CloudVisionCreateProductOperator, CloudVisionAddProductToProductSetOperator, etc.) and GCS operations (GCSCreateBucketOperator, GCSToGCSOperator). It does not wrap functionality in PythonOperator + Hook patterns, which demonstrates idiomatic Airflow usage.",0,"The code contains multiple critical syntax errors and hallucinated constructs that indicate AI generation issues: 1) 'CloudVisionRetryWithRetry()' is not a valid Airflow operator or class, 2) Malformed operator instantiation syntax like 'CloudVisionRetryWithRetry().task_id=""detect_labels"")(image_uri=IMAGE)', 3) Incomplete code at the end (truncated comment '# [END howto_operator_vision_dele'), 4) References to non-existent operators like 'CloudVisionListProducts' (should be CloudVisionListProductsOperator), 5) The retry import is present but never properly used. These are clear hallucinations and syntax errors that would prevent the DAG from running.",0,"While the DAG attempts to address the instruction by including image transfer (GCSToGCSOperator), product catalog creation (CloudVisionCreateProductOperator, CloudVisionCreateProductSetOperator), and image analysis operations, the code is fundamentally broken due to syntax errors and hallucinated operators. The detect_labels and detect_signatures tasks use non-existent operators, making the 'automated image analysis' component non-functional. Additionally, the code is incomplete (truncated at the end). A working implementation is required to fulfill the instruction, and this code would fail immediately upon execution.",0.3333333333333333
890,Fine-tuned (Qwen 2.5 1.5B Airflow),244,1,"The code uses native Airflow AWS provider operators (EksCreateClusterOperator, EksCreateNodegroupOperator, EksDeleteClusterOperator, EksDeleteNodegroupOperator) and sensors (EksClusterStateSensor, EksNodegroupStateSensor) appropriately. It does not wrap EKS logic in PythonOperator + Hook patterns. The use of @task decorator for custom logic (start_app_runner) is acceptable for business logic that doesn't have a dedicated operator.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and the usage of 'sys_test_context_task = SystemTestContextBuilder()...' are test framework components, not production-ready code. This is a system test example, not production code.",0,"The instruction asks for 'provisioning and managing an Amazon EKS cluster with dynamic nodegroup creation' demonstrating 'infrastructure-as-code deployment'. While the code creates a cluster and nodegroup, it has several issues: (1) The code is incomplete (truncated at start_app_runner function), (2) It includes serverless application runner operators (EksGetServerlessApplicationRunnerOperator, EksStartServerlessApplicationRunnerOperator) which are not part of standard EKS cluster provisioning, (3) The nodegroup configuration uses 'nodegroup_type=""FARGATE""' and 'min_nodes/max_nodes' parameters that don't align with standard EKS nodegroup creation patterns, (4) Missing actual Kubernetes resource deployment which was requested in the instruction. The code appears to be a test example rather than a complete workflow for the stated business requirement.",0.3333333333333333
891,Fine-tuned (Qwen 2.5 1.5B Airflow),245,0,"The code imports a non-existent operator 'GCSToVertexAITranslateV3Operator' from 'airflow.providers.google.cloud.operators.vertex_ai.text翻译' which contains Chinese characters (翻译) and is not a valid Airflow provider path. This is not idiomatic Airflow usage. Additionally, the DAG is incomplete and doesn't show actual operator instantiation or task definitions that would demonstrate proper use of native operators vs PythonOperator wrappers.",0,"The code contains multiple hallucinations: (1) Imports a non-existent operator with Chinese characters in the module path 'vertex_ai.text翻译', (2) References 'GCSToVertexFlowOperator' in docstring which doesn't exist in standard Airflow, (3) Imports 'pytest' which is a testing library not needed in production DAG code, (4) Uses 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are testing conventions, (5) Contains Chinese text '是正确的吗？' at the end suggesting test/debug code, (6) Imports 'google.cloud.translate_v3.enums' but never uses it.",0,"The instruction asks for a complete ML workflow demonstrating data movement, dataset creation, and model training with clear operational stages and task dependencies. The provided code is incomplete (cuts off mid-DAG definition), doesn't show any tasks being instantiated, contains no model training logic, no dataset creation, no clear task dependencies, and focuses on translation services rather than a general ML workflow. It fails to deliver the requested end-to-end machine learning orchestration.",0.0
892,Fine-tuned (Qwen 2.5 1.5B Airflow),246,0,"The code misuses the @task decorator by wrapping SingularityOperator instantiation inside TaskFlow functions. The functions create_container, create_container_with_network, etc. instantiate SingularityOperator objects but don't execute them properly - they just create operator instances inside Python functions without returning or executing them. This is not idiomatic Airflow. The correct approach would be to directly instantiate SingularityOperator at the DAG level for task chaining, not wrap them in @task decorated functions that don't actually call or return the operators properly.",0,The code explicitly imports test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready code examples.,0,"The instruction asks for a pipeline demonstrating task chaining, system command execution, and containerized task running. While the code attempts containerized tasks (SingularityOperator), it fails to demonstrate proper task chaining (the task dependencies are broken due to incorrect operator usage), and there is no system command execution (no BashOperator or similar). Additionally, the code has multiple bugs: missing 'timedelta' import, incorrect task dependency syntax (create_container_with_volume() is called twice), and the trigger_rule assignment at the end tries to set attributes on function objects rather than task instances.",0.0
893,Fine-tuned (Qwen 2.5 1.5B Airflow),247,0,"The code defines only Python functions (test_gcs_create_cube, test_gcs_delete_cubes, etc.) without any Airflow DAG structure, operators, or tasks. There are no imports of Airflow modules, no DAG definition, and no use of Kylin operators as requested. This appears to be test utility code rather than an Airflow pipeline. A proper implementation should use KylinOperator or similar providers from airflow.providers.apache.kylin if available, or at minimum structure tasks using PythonOperator within a DAG context.",0,"The code contains multiple hallucinations and inconsistencies: (1) Functions are prefixed with 'test_' suggesting test harness code rather than production DAG code, (2) Uses non-existent or incorrectly referenced Google Cloud client libraries (e.g., 'google.cloud.storage.client.StorageClient()' and 'google.cloud.bigtable.client.BigTableClient()' with incorrect API patterns), (3) Mixes GCS (Google Cloud Storage) concepts with Bigtable and YQL (which is not a Google Cloud technology), (4) The instruction mentions Kylin operators but the code references GCS/Bigtable, indicating confusion between Apache Kylin (OLAP engine) and Google Cloud services, (5) Code is incomplete (cuts off at '# [START howto').",0,"The instruction explicitly requests an Airflow pipeline using Kylin operators to demonstrate data cube transformation and maintenance processes. The provided code: (1) Contains no Airflow DAG definition or pipeline structure, (2) Does not use any Kylin operators or interact with Apache Kylin, (3) Instead references Google Cloud Storage and Bigtable operations that are unrelated to Kylin data cubes, (4) Lacks any systematic process flow for cube building, refreshing, or maintenance as would be expected in a Kylin pipeline. The code completely misses the core requirement of demonstrating Kylin-based analytical data structure management.",0.0
894,Fine-tuned (Qwen 2.5 1.5B Airflow),248,0,"The code uses a non-existent 'LOLOperator' from 'airflow.providers.cncf.kubernetes.operators.lol' which is not a real Airflow operator. For Kubernetes PVC management, the code should use actual Kubernetes operators like 'KubernetesPodOperator' with volume configurations, or 'KubernetesCreateResourceOperator' and 'KubernetesDeleteResourceOperator' for resource lifecycle management. Instead, it uses fictional operators and BashOperator with 'lol' commands that have nothing to do with Kubernetes volume management.",0,"The code contains multiple hallucinations: (1) It imports 'LOLOperator' which does not exist in Airflow's Kubernetes provider, (2) It includes test harness boilerplate at the end with 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)', which are internal testing modules that should not appear in production code, (3) The 'lol' bash commands are nonsensical and not related to any real Kubernetes or system utility.",0,"The code completely fails to implement the requested functionality. The instruction asks for: (1) Creating a PVC with 5GB storage, (2) Deleting the PVC, (3) Daily schedule, (4) Kubernetes resource lifecycle management. The generated code: (1) Uses a non-existent 'LOLOperator' instead of Kubernetes operators, (2) Has no PVC creation or deletion logic, (3) Uses '@once' schedule instead of daily, (4) Contains no Kubernetes resource definitions or lifecycle management. The code appears to be a random example DAG unrelated to the user's requirements.",0.0
895,Fine-tuned (Qwen 2.5 1.5B Airflow),249,0,"The code uses GCSToBigQueryOperator, which is the WRONG operator for the task. The instruction asks to transfer data FROM BigQuery TO GCS, but GCSToBigQueryOperator transfers FROM GCS TO BigQuery (opposite direction). The correct operator should be BigQueryToGCSOperator. This is a fundamental misunderstanding of the operator's purpose.",0,"The code contains multiple test harness artifacts: imports pytest, uses pytest decorators (@pytest.mark.parametrize, pytest.skipif), defines test functions (test_gcs_to_bigquery_operator), references undefined variables (tmp_bucket, GCSToBigQueryTaskId, GCSToBigQueryExternalTaskId, GCSToBigQueryDeferrableTaskId), and includes test-specific logic. This is clearly test boilerplate mixed with production DAG code, not clean production-ready code.",0,"The instruction explicitly asks to 'extract data from a BigQuery dataset and export it to a cloud storage location' (BigQuery → GCS). However, the code uses GCSToBigQueryOperator which does the opposite (GCS → BigQuery). The business logic is completely reversed from what was requested. Additionally, the code is incomplete (truncated) and contains test code rather than a functional DAG.",0.0
896,Fine-tuned (Qwen 2.5 1.5B Airflow),250,0,"The code misuses DatabricksSubmitRunOperator by passing a 'sql' parameter, which does not exist in this operator. DatabricksSubmitRunOperator is designed to submit notebook or JAR jobs, not execute SQL. For Databricks SQL operations, the code should use DatabricksSqlOperator from airflow.providers.databricks.operators.databricks_sql. Additionally, DatabricksSensor is used incorrectly with job_id=-1, which is not a valid use case. The code does not demonstrate idiomatic usage of Databricks providers.",0,"The code contains clear hallucinations and test harness leakage. It imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities not meant for production code. Additionally, the SQL statement contains nonsensical content like 'RUN SET DEFAULT DBRI SKINS_DIR TO /databricks/skins/IntelliJ IDEA 2021.2.xsd' which appears to be corrupted or fabricated. The 'sql' and 'new_instance_id' parameters don't exist in DatabricksSubmitRunOperator. There's also a missing import for 'timedelta'.",0,"The instruction requires a complete data integration process including: (1) table creation, (2) data import from external file sources, (3) transformation, and (4) output generation, demonstrating flexibility across different storage systems. The provided code only attempts table creation (CREATE TABLE AS SELECT) and drops a table, but does not show data import from external file sources, meaningful transformations, output generation, or integration across different storage systems. The workflow is incomplete and does not illustrate the requested end-to-end data integration process.",0.0
897,Fine-tuned (Qwen 2.5 1.5B Airflow),251,0,"While the code uses native Dataflow operators (DataflowCreatePipelineOperator, DataflowDeletePipelineOperator), it does not use the appropriate operators for the streaming use case described. For real-time taxi ride events with Pub/Sub and Apache Beam, it should use DataflowStartFlexTemplateOperator or DataflowTemplatedJobStartOperator with a streaming pipeline configuration. The current implementation uses DataflowCreatePipelineOperator with a dummy Java pipeline that has no connection to Pub/Sub topics or streaming data processing. Additionally, there's a syntax error with an 'else' statement without a corresponding 'if', and the task dependency 'delete_dag >> dag' is incorrect.",0,The code explicitly imports testing utilities from 'tests_common.test_utils.system_tests' and includes test harness boilerplate with 'get_test_run_from_test_case(dag)'. This is clearly test infrastructure code that should not be in production DAG code. The comment '# Needed to run the example DAG with pytest' confirms this is test-specific code.,0,"The instruction requires: (1) capturing real-time taxi ride events, (2) transforming them using Apache Beam, (3) routing through Google Cloud Pub/Sub topics, and (4) automated infrastructure management. The provided code fails to meet these requirements. It creates a dummy Java pipeline with placeholder operators (DummyReader, DummyCalculator, DummyWriter) that have no actual business logic. There is no Pub/Sub topic creation, no subscription management, no Apache Beam transformation logic for taxi ride events, and no streaming data processing configuration. The code appears to be a generic Dataflow example template rather than a solution for the specific taxi ride streaming use case.",0.0
898,Fine-tuned (Qwen 2.5 1.5B Airflow),252,1,"The code uses native Airflow AWS EKS providers and operators (EksCreateClusterOperator, EksCreateNodegroupOperator, EksDeployApplicationOperator, EksClusterStateSensor, EksNodegroupStateSensor) rather than wrapping boto3 calls in PythonOperators for the main workflow. While there are some @task decorated functions using boto3 directly (get_issuer_ref, delete_nodegroup, delete_cluster), these appear to be for auxiliary/cleanup operations where specific operators may not exist, which is acceptable. The core infrastructure deployment uses idiomatic Airflow patterns.",0,"The code contains clear test harness imports and boilerplate that should not be in production code. Specifically: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and the usage of 'sys_test_context_task = SystemTestContextBuilder()...' This is test infrastructure leakage. Additionally, there are suspicious/non-existent boto3 API calls like 'boto3.service_name_to_aws_service_name(""root-ca"")' which appears to be hallucinated - this is not a valid boto3 method. The code also references non-standard AWS services and endpoints that don't exist in standard AWS APIs.",0,"While the code attempts to create a Kubernetes (EKS) cluster deployment pipeline, it is incomplete (the code is cut off mid-parameter in create_nodegroup). More critically, the instruction asks for 'secure metadata handling', 'built-in compliance and verification steps', and 'demonstrates the ability to run containerized workloads'. The code shows cluster creation and sensors but lacks explicit compliance/verification steps, security validation tasks, or clear demonstration of running containerized workloads (the EksDeployApplicationOperator is imported but not shown being used in the truncated code). The code is also heavily test-oriented rather than production-ready, which contradicts the 'scalable and repeatable process' requirement.",0.3333333333333333
899,Fine-tuned (Qwen 2.5 1.5B Airflow),253,1,"The code uses native Airflow operators throughout: BigQueryCreateEmptyDatasetOperator, BigQueryCreateTableOperator, BigQueryToGCSOperator, GCSCreateBucketOperator, and GCSDeleteBucketOperator. It does not wrap any logic in PythonOperator with hooks, instead relying on the appropriate provider operators from airflow.providers.google.cloud, which is the idiomatic approach.",0,The code explicitly imports testing utilities at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules and test harness boilerplate that should not appear in production-ready code.,0,"The instruction requires: (1) ingest raw data from an external source, (2) transform it using a cloud-based data preparation tool, and (3) generate clean, structured output in cloud storage. This DAG only exports existing BigQuery data to GCS without any ingestion from an external source or transformation step. It creates an empty table and immediately exports it, missing the core data ingestion and transformation requirements. The workflow is more of a simple export example rather than a complete data engineering solution as requested.",0.3333333333333333
900,Fine-tuned (Qwen 2.5 1.5B Airflow),254,0,"The code uses @task decorators with custom functions (create_table, drop_table) that attempt to use S3Hook methods that don't exist (s3_hook.create_table, s3_hook.drop_table). S3Hook is for S3 operations, not database table operations. For database-to-database transfers as requested, the code should use SQLExecuteQueryOperator for source preparation and validation, and appropriate transfer operators or SQL operators for the destination. The SQLToS3Operator is used but this transfers to S3, not to another database table as instructed. The approach is not idiomatic for a database-to-database transfer pipeline.",0,"The code explicitly imports and uses test harness code at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. This is testing boilerplate that should not be in production code. Additionally, the code invents non-existent methods on S3Hook (create_table, drop_table) and uses a non-existent 'name()' function, which are hallucinated APIs.",0,"The instruction asks for a complete data transfer operation between two DATABASE TABLES with steps for source data preparation, data movement, and destination table validation. The provided code transfers data from SQL to S3 (not database to database), lacks proper source preparation steps, has no destination validation, and the create_table/drop_table functions are incorrectly implemented using S3Hook instead of database operators. The code does not demonstrate a systematic database-to-database transfer as requested.",0.0
901,Fine-tuned (Qwen 2.5 1.5B Airflow),255,1,"The code uses native Airflow operators from Google Cloud providers (GCSCreateBucketOperator, GCSDeleteBucketOperator, CloudTextToSpeechSynthesizeOperator, CloudTranslateSpeechOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for cloud service integration.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be in production code. Additionally, there's a missing import for 'json' module which is used in 'json.dumps(INPUT)'.",1,"The DAG fulfills the user instruction by creating a multi-step cloud-based audio processing workflow that chains together Google Cloud operators for text-to-speech synthesis (CloudTextToSpeechSynthesizeOperator) and speech translation (CloudTranslateSpeechOperator). It demonstrates task dependencies with proper chaining, cloud service integration with GCS bucket management, and automated workflow management. The workflow shows clear orchestration of the requested audio processing pipeline.",0.6666666666666666
902,Fine-tuned (Qwen 2.5 1.5B Airflow),257,0,"The provided code is essentially empty - it only contains a comment and no actual implementation. There are no operators, providers, or any Airflow constructs present. A proper implementation would require using AWS ECS operators from airflow.providers.amazon.aws.operators.ecs (such as EcsCreateClusterOperator, EcsRegisterTaskDefinitionOperator, EcsRunTaskOperator, EcsDeleteClusterOperator) to be considered idiomatic.",1,"While the code is incomplete, it does not contain any hallucinated imports, test harness code, or internal testing modules. The single comment present does not introduce any problematic elements.",0,"The instruction explicitly requests a complete DAG that demonstrates: (1) creating an ECS cluster, (2) registering a task definition, (3) running a container with a basic command, and (4) cleaning up resources. The provided code contains only a comment and fails to implement any of these required components. There is no DAG definition, no tasks, and no workflow logic whatsoever.",0.3333333333333333
903,Fine-tuned (Qwen 2.5 1.5B Airflow),259,1,"The code uses native Airflow providers and operators specifically designed for Google Cloud Dataflow tasks (DataflowCreatePipelineOperator, DataflowTriggerPipelineOperator, DataflowDeletePipelineOperator from airflow.providers.google.cloud.operators.dataflow). It does not wrap functionality in PythonOperator with hooks, which demonstrates idiomatic Airflow usage.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction specifically requests an employee salary data processing pipeline that loads employee salary information from BigQuery, applies transformations, and writes results to a new BigQuery table. However, the provided code is a generic Dataflow pipeline example that reads from a text file ('gs://airflow-system-tests-resources/dataproc/shuffle.txt') with no BigQuery integration, no employee salary data processing logic, and no transformation or loading of results to BigQuery. The code does not fulfill the specific business requirements requested.",0.3333333333333333
904,Fine-tuned (Qwen 2.5 1.5B Airflow),260,1,"The code correctly uses Airflow provider-specific operators (TrinoRunSQLOperator and TrinoRecordSensor) from airflow.providers.trino package. It does not wrap Trino logic in PythonOperator with hooks, but instead uses the native Trino operators designed for the task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production-ready code.",0,"The instruction requires creating a data engineering workflow that: (1) creates schemas, (2) generates tables with predefined structures, and (3) inserts sample city records. The provided code only runs a simple 'SELECT 42' query and checks it with a sensor. It does not create any schemas, does not create any tables, does not insert any city data, and does not demonstrate incremental complexity in building database objects. The code completely fails to address the core requirements of the instruction.",0.3333333333333333
905,Fine-tuned (Qwen 2.5 1.5B Airflow),261,0,"While the DAG uses some appropriate GCS operators (GCSToGCSOperator, GCSSynchronizeBucketsOperator), it severely misuses the TaskFlow API. The @task decorated functions attempt to instantiate operators inside them (e.g., 'create_bucket = GCSCreateBucketOperator(...)' inside a @task function) rather than using them as standalone tasks. Additionally, there are calls to undefined functions like 'local_filesystem_to_gcs(*upload_data.values())' and 'delete_files(bucket_name=...)' which appear to be attempting to use hooks/operators in a non-idiomatic way. The code also has undefined variables like 'local_file_path' being used as a task dependency. This is not idiomatic Airflow usage.",0,"The code contains multiple hallucinations and errors: 1) Undefined function 'local_filesystem_to_gcs()' is called without import or definition, 2) Undefined function 'delete_files()' is called, 3) Variable 'local_file_path' is used as a task dependency but never defined as a task, 4) 'next_step_copy_bucket(copy_task.output)' is passed as an argument to GCSCreateBucketOperator which doesn't accept such parameters, 5) The code structure is broken with operators being instantiated inside @task functions and then referenced outside. While it doesn't import test harness code, it contains significant non-functional hallucinated code that would not run.",0,"The instruction requires: 1) Generate random binary files, 2) Demonstrate cloud storage data transfer, 3) Show bucket interactions, 4) Demonstrate cross-bucket synchronization. The code attempts to address some of these (GCSToGCSOperator for transfer, GCSSynchronizeBucketsOperator for synchronization, bucket creation), but critically fails on the first requirement - it does not actually generate random binary files. The 'create_files()' task creates a filename but never generates actual binary file content. Additionally, the code is so broken (undefined functions, incorrect task dependencies, malformed operator usage) that it would not execute successfully to demonstrate any of the required functionality. The LocalFilesystemToGCSOperator is imported but never properly used to upload generated files.",0.0
906,Fine-tuned (Qwen 2.5 1.5B Airflow),262,0,"The code does not define any Airflow DAG or use any Airflow operators. There is no DAG instantiation, no task definitions using operators like VertexAICreateAutoMLTabularTrainingJobOperator, VertexAICreateDatasetOperator, or VertexAIDeleteDatasetOperator. The code appears to be a configuration/setup script with test utilities but lacks any actual Airflow task orchestration patterns.",0,"The code heavily imports and uses internal testing modules that should not appear in production DAG code. Specifically: 'from system.google import GoogleApiFactory', 'from system.second_party.vertex_ai.utils import default_args, list_functions', and 'from tests_common.test_utils.system_tests import get_test_run_output'. These are clear indicators of test harness boilerplate and internal testing infrastructure that have leaked into the generated code.",0,"The instruction requested an Airflow DAG that: (1) creates a dataset, (2) trains an AutoML model, and (3) cleans up resources after training with clear task dependencies. The provided code does not implement any of these requirements. There is no DAG definition, no tasks for dataset creation, no AutoML training task, no cleanup task, and no task dependencies defined. The code is incomplete (cuts off mid-function) and appears to be test setup code rather than a functional DAG.",0.0
907,Fine-tuned (Qwen 2.5 1.5B Airflow),264,1,"The code uses appropriate Airflow providers and operators for the task. It uses S3CreateBucketOperator, S3CreateObjectOperator, S3DeleteBucketOperator from airflow.providers.amazon.aws.operators.s3, and S3ToRedshiftOperator from airflow.providers.amazon.aws.transfers.s3_to_redshift. These are the idiomatic, native operators for S3 and Redshift operations rather than wrapping logic in PythonOperator with hooks.",0,"The code contains multiple instances of test harness boilerplate and internal testing modules. Specifically: (1) imports 'from system.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is a test utility, (2) imports 'from tests.system.utils.watcher import watcher' at the end, (3) imports 'from tests.system.utils import get_test_run', and (4) includes 'test_run = get_test_run(dag)' which is test execution code. The DAG also uses 'sys_test_context_task = SystemTestContextBuilder().build()' which is clearly test infrastructure, not production code.",0,"The instruction asks for a pipeline that transfers cocktail recipe data from S3 to Redshift. However, the code: (1) does not contain any cocktail recipe data - instead it uses generic AWS account data ('AWSAccountID' and 'FileName'), (2) does not provision a Redshift cluster (only references it via hook), (3) does not establish necessary connections for data transfer (no connection setup is shown), and (4) the S3ToRedshiftOperator configuration appears incomplete/incorrect (it has a sql_statement parameter that queries the table but doesn't properly configure the COPY command parameters). The code is a generic S3-to-Redshift example rather than a cocktail recipe data pipeline as requested.",0.3333333333333333
908,Fine-tuned (Qwen 2.5 1.5B Airflow),265,1,"The code uses native SageMaker operators (SageMakerCreateNotebookOperator, SageMakerDeleteNotebookOperator, SageMakerNotebookSensor) from airflow.providers.amazon.aws.operators.sagemaker and airflow.providers.amazon.aws.sensors.sagemaker. It does not wrap logic in PythonOperator + Hook patterns, which demonstrates proper idiomatic Airflow usage.",0,"The code imports from 'tests.system.providers.amazon.aws.utils' (specifically 'SystemTestContextBuilder' and 'ENV_ID_KEY'), which are internal testing modules. This is test harness boilerplate that should not appear in production DAG code. Additionally, there are commented-out sections with incomplete/nonsensical code, and an assertion statement that appears to be test-related rather than production logic.",0,"The instruction asks to 'illustrate step-by-step operational control of a cloud-based machine learning notebook' with 'dynamic instance management'. However, the code has several critical issues: (1) undefined variables like 's3_output_path' are referenced without definition, (2) the delete_bucket task incorrectly uses SageMakerDeleteNotebookOperator instead of S3DeleteBucketOperator, (3) there's a confusing assertion at the end that doesn't make logical sense, (4) large sections of commented-out unrelated code, and (5) the workflow doesn't demonstrate comprehensive notebook lifecycle management (missing start/stop operations). The code appears incomplete and would not execute successfully.",0.3333333333333333
909,Fine-tuned (Qwen 2.5 1.5B Airflow),266,0,"While the DAG does use CloudFormationCreateStackOperator, CloudFormationDeleteStackOperator, and their corresponding sensors (which is good), it also includes custom @task decorated functions (get_stack_id and wait_for_stack_status) that use subprocess calls to AWS CLI commands. The wait_for_stack_status function in particular reimplements functionality that should be handled by CloudFormationCreateStackSensor and CloudFormationDeleteStackSensor. This is a non-idiomatic pattern - the code wraps CLI commands in Python callables instead of relying purely on the native Airflow operators and sensors provided by the AWS provider package.",1,"The code uses only standard Airflow libraries and AWS provider packages. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. All imports are from legitimate Airflow core and provider packages (airflow.decorators, airflow.models, airflow.providers.amazon.aws, etc.).",0,"The DAG is incomplete - it cuts off mid-comment with '# W' at the end. More critically, while it creates a stack and has a delete_stack operator defined, the DAG does not show the complete sequential execution chain including the deletion sensor (CloudFormationDeleteStackSensor) to wait for stack deletion to complete. The instruction specifically asks for 'infrastructure provisioning and teardown with proper error handling and sequential task execution', but the teardown monitoring is missing. Additionally, the wait_for_stack_status custom task is defined but never used in the DAG, and there's no clear error handling demonstrated beyond the TriggerRule.ALL_DONE on one unused task.",0.3333333333333333
910,Fine-tuned (Qwen 2.5 1.5B Airflow),268,0,"This code does not define an Airflow DAG at all. There are no DAG definitions, no operators (native or otherwise), no task dependencies, and no use of Airflow decorators or patterns. The code appears to be a standalone Python script with functions for Google Cloud operations, but lacks any Airflow constructs like `DAG()`, `@dag`, or any operators (e.g., `GCSCreateBucketOperator`, `CreateDatasetOperator`, `CreateCustomTrainingJobOperator`). It completely fails to use Airflow idioms.",0,"The code contains multiple hallucinated/non-standard imports and references. Specifically: 'from system.google import GoogleCloudApiSession' and 'from system.openai import OpenAIAsyncEmbeddingVectorService' are not standard Airflow or Google Cloud libraries. These appear to be fabricated modules. Additionally, the code references 'openai_embedding_vector_service' which is never properly imported or defined. The code also appears incomplete (cuts off mid-statement) and contains references to non-existent APIs like 'AutoMLDeploymentPredictionClient'.",0,"The instruction explicitly requests an Airflow DAG that orchestrates a machine learning workflow with data ingestion, GCS operations, dataset creation, model training, and cleanup. This code: (1) Does not define any DAG structure, (2) Does not include data ingestion tasks, (3) Does not demonstrate GCS operations using Airflow operators, (4) Does not show dataset creation or model training in an orchestrated workflow, (5) Does not include cleanup/resource management tasks, and (6) Is not even a complete, runnable script. It completely fails to meet the instruction requirements.",0.0
911,Fine-tuned (Qwen 2.5 1.5B Airflow),270,0,"The code uses non-existent operators like 'GCSCreateBucketFileOperator', 'MLEngineCreateArtifactFromBucketOperator', 'MLEngineDeleteArtifactFromBucketOperator', and 'MLEngineGetArtifactFromBucketOperator' which are not part of the standard Airflow Google Cloud provider. The correct idiomatic approach would use 'LocalFilesystemToGCSOperator' or 'GCSObjectCreateAcl' for file uploads. Additionally, the MLEngine operators referenced don't match the actual Airflow 3.0.0 API - the correct operators for Vertex AI (which replaced ML Engine) would be from 'airflow.providers.google.cloud.operators.vertex_ai'.",0,"The code contains multiple hallucinated/non-existent operators and methods: 'GCSCreateBucketFileOperator' does not exist in Airflow providers; 'MLEngineCreateArtifactFromBucketOperator', 'MLEngineDeleteArtifactFromBucketOperator', and 'MLEngineGetArtifactFromBucketOperator' are fabricated operators; 'model_id()' is called as a function without being defined; 'ModelStateSensor' is imported but never used; there's a typo 'ARTIFACTuger_ZIP_FILE_NAME'; and the code is incomplete (ends mid-comment). The DAG also references ML Engine which has been deprecated in favor of Vertex AI.",0,"The instruction asks for a complete machine learning lifecycle for image classification including training, deployment, and orchestration. This code only attempts to create models and versions but is missing critical components: no training job (MLEngineTrainingOperator or equivalent), no actual image classification logic, no data preprocessing, no model evaluation, and no prediction/inference tasks. The DAG is also incomplete (cuts off mid-task definition) and doesn't demonstrate a functional end-to-end ML workflow. It's more of a skeleton for model management rather than a complete ML lifecycle.",0.0
912,Fine-tuned (Qwen 2.5 1.5B Airflow),271,0,"The code uses 'ADFOperator' (Azure Data Factory Operator) to run a Pig script, which is incorrect. For Pig script execution in a local Hadoop environment, the appropriate operator would be 'PigOperator' from 'airflow.providers.apache.pig.operators.pig'. Using ADF for local Hadoop Pig execution is not idiomatic and represents a fundamental misunderstanding of the task requirements.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run', creates a test_run object with 'test_run = get_test_run(dag)', and includes a main block that runs 'tests_common.test_utils.run_test_case'. These are internal testing modules and test infrastructure that indicate this is leaked test code, not production-ready DAG code.",0,"The instruction asks for a Pig script execution workflow in a local Hadoop environment, but the code uses ADFOperator (Azure Data Factory), which is a cloud service operator unrelated to local Hadoop. The correct approach would use PigOperator with appropriate local Hadoop configuration. Additionally, while the pig command 'ls /;' is present, the overall implementation does not demonstrate proper Pig script execution in a local Hadoop context as requested.",0.0
913,Fine-tuned (Qwen 2.5 1.5B Airflow),272,0,"The code wraps Dataproc cluster creation logic inside a @task decorated function ('create_cluster') and uses GCSHook methods like 'fetch_metadata' and 'create_metadata' within a PythonOperator pattern, rather than using native Airflow operators designed for metadata/tag template management. For a metadata management pipeline focused on Data Catalog tag templates, the code should use operators from airflow.providers.google.cloud.operators.datacatalog (e.g., DataCatalogCreateTagTemplateOperator, DataCatalogCreateTagOperator) instead of wrapping hook calls in Python tasks.",0,"The code uses non-existent GCSHook methods 'fetch_metadata' and 'create_metadata' which are not part of the standard Airflow GCSHook API. Additionally, the job definitions reference 'tags' with 'type' fields like 'DIR_LIST' and 'FILE_LIST' which appear to be fabricated constructs not found in standard GCP Data Catalog or GCS tagging APIs. The code also references 'dataProvisioningProposalZones' property which is not a standard GCS bucket metadata field. These are hallucinated APIs that don't exist in production Airflow or GCP libraries.",0,"The instruction requests a metadata management pipeline for Data Catalog tag templates to standardize data asset descriptions and classifications. However, the code focuses on GCS bucket operations and Dataproc cluster creation without actually implementing any Data Catalog tag template operations (create, update, delete tag templates, or apply tags to data assets). The code is incomplete (cuts off mid-operator definition) and doesn't demonstrate dynamic configuration or manipulation of tag templates as requested. It fails to fulfill the core business logic of managing Data Catalog metadata.",0.0
914,Fine-tuned (Qwen 2.5 1.5B Airflow),273,1,"The code uses native dbt Cloud provider operators and sensors (DbtCloudJobRunSensor, DbtCloudModelRunSensor, DbtCloudSnapshotRunSensor) from airflow.providers.dbt.cloud, which is the idiomatic approach. It does not wrap functionality in PythonOperators unnecessarily.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code: 'from tests.system.utils import get_test_run', 'from tests.system.utils import get_test_env_id', and usage of 'test_run = get_test_run(dag)'. These are internal testing modules that indicate this is example/test code rather than clean production-ready code.",0,"The instruction asks for a workflow that 'triggers dbt Cloud jobs' and monitors their execution. However, this DAG only contains sensors for monitoring - it does not include any operators to actually trigger/run dbt Cloud jobs (e.g., DbtCloudRunJobOperator). The DAG assumes a run_id already exists from XCom but never creates it. Additionally, while it demonstrates sensors, it doesn't clearly show the distinction between synchronous and asynchronous sensor patterns as requested.",0.3333333333333333
915,Fine-tuned (Qwen 2.5 1.5B Airflow),274,1,"The code imports and uses specific Google Cloud Compute Engine operators from airflow.providers.google.cloud.operators.compute, including ComputeEngineCopyInstanceOperator, ComputeEngineInsertInstanceOperator, ComputeEngineInsertInstanceFromTemplateOperator, and others. These are the appropriate native operators for Compute Engine operations rather than wrapping logic in PythonOperator with hooks.",0,"The code appears to be incomplete and contains significant issues. The command structures (INSERT_INSTANCE_COMMAND, COPY_INSTANCE_COMMAND) contain malformed, nonsensical pseudo-code that doesn't represent valid Python or any recognizable command syntax. The code includes bizarre string manipulations like 'if isset($projectId)' which is not valid Python, and complex nested string operations that appear to be corrupted or hallucinated. Additionally, the code is truncated and incomplete, cutting off mid-statement. While it doesn't import test harness modules, the presence of this malformed, non-functional code indicates hallucination of invalid syntax patterns.",0,"The instruction requested a workflow to 'create, copy, and update instance templates' for template management. While the imports suggest the right operators are available, the actual DAG implementation is incomplete - there's no DAG definition with tasks showing the workflow, no actual instantiation of the operators, and no demonstration of creating, copying, or updating templates in a coherent workflow. The code only shows imports and malformed command structures without any executable workflow logic that would demonstrate template management best practices.",0.3333333333333333
916,Fine-tuned (Qwen 2.5 1.5B Airflow),275,1,"The code uses specific Airflow providers and operators designed for Kylin cube operations (KylinBuildCubeOperator, KylinRefreshCubeOperator, KylinMergeCubesOperator, KylinDisableCubeOperator, KylinDeleteCubeOperator, etc.) from airflow.providers.alibaba.cloud.operators.kylin. It does not wrap logic in PythonOperator + Hook patterns, but instead uses native operators throughout.",1,"The code imports only standard Airflow libraries from the official Alibaba Cloud provider package. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears production-ready and clean, using only legitimate Airflow 3.0.0 provider operators.",0,"The instruction specifically requested demonstrating 'build, refresh, merge, disable, and purge commands' with 'task dependencies, XCom data passing, and systematic execution' showing 'precise time-based segmentation'. While the code includes build, disable, and delete operations, it is missing: (1) KylinRefreshCubeOperator usage (imported but not instantiated), (2) KylinMergeCubesOperator usage (imported but not instantiated), (3) No task dependencies are defined - tasks are declared but not chained together, (4) No XCom data passing is demonstrated between tasks, (5) No clear time-based segmentation strategy for cube operations. The code appears incomplete (cuts off mid-comment) and lacks the orchestration flow that was central to the request.",0.6666666666666666
917,Fine-tuned (Qwen 2.5 1.5B Airflow),277,0,"The code imports non-standard Airflow utilities (LatestState, LatestStateIfNone, InProgressState) from airflow.utils.state that do not exist in Airflow 2.8.4 or any standard Airflow release. These appear to be fabricated classes. The proper idiomatic approach for ensuring only the latest run is processed would be using max_active_runs=1 DAG parameter, TriggerDagRunOperator with wait_for_completion, or ExternalTaskSensor with proper configuration. The code does not use any legitimate Airflow operators or patterns for controlling concurrent runs.",0,"The code imports several non-existent classes from airflow.utils.state: LatestState, LatestStateIfNone, and InProgressState. These are not part of the standard Airflow library in version 2.8.4 or any other version. The airflow.utils.state module contains TaskInstanceState and DagRunState enums, but not these classes. This represents clear hallucination of API components that don't exist.",0,"The instruction asks for a pipeline that ensures only the most recent workflow run is processed and demonstrates skipping unnecessary task runs when a newer instance is in progress. The generated code does not accomplish this. It creates multiple EmptyOperators with duplicate task_ids (which would cause errors) and attempts to use non-existent state management classes. A proper implementation would use DAG-level concurrency controls (max_active_runs=1), LatestOnlyOperator, or custom logic with XCom/Variables to check for running instances. The code does not demonstrate any working mechanism to prevent redundant executions.",0.0
918,Fine-tuned (Qwen 2.5 1.5B Airflow),278,0,"The code does not use idiomatic Airflow patterns for the requested task. It uses bare 'Task' objects without any actual operators or task decorators (@task). For a data transformation pipeline with dynamic task mapping, it should use @task decorator with map() for dynamic task generation, or use expand() with proper operators. The code appears to be a template example that doesn't implement any actual data processing logic.",0,The code explicitly imports and uses testing modules that should not be in production code: 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test harness boilerplate with 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are clear indicators of test/example code leakage.,0,"The code completely fails to implement the requested functionality. The instruction asks for: (1) taking a list of numbers, (2) incrementing each number using dynamic task mapping, and (3) calculating their total sum. The provided code does none of this - it's a generic task group example with no data processing, no list of numbers, no increment logic, no sum calculation, and no dynamic task mapping based on input data. It appears to be an unrelated example DAG.",0.0
919,Fine-tuned (Qwen 2.5 1.5B Airflow),280,1,"The code uses the native GCSToTrinoOperator from airflow.providers.trino.transfers.gcs_to_trino, which is the idiomatic way to transfer data from GCS to Trino. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for this task.",0,The code explicitly imports testing utilities at the end: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code.,0,"The instruction requests a complex data processing workflow with multiple interconnected tasks, including cloud resource management, data import, transformation, and export, with robust error handling and connection management. The provided code only contains a single task (GCSToTrinoOperator) with no transformations, no export steps, no cloud resource management, no task dependencies, and no error handling mechanisms. It does not fulfill the requirement for a complex, multi-task orchestrated workflow.",0.3333333333333333
920,Fine-tuned (Qwen 2.5 1.5B Airflow),281,0,"The code uses SQLExecuteQueryOperator which is a generic SQL operator, but the instruction specifically asks for a 'MySQL database cleanup workflow'. The code should use the MySQL-specific operator (airflow.providers.mysql.operators.mysql.MySqlOperator) instead of the generic SQLExecuteQueryOperator. Additionally, the conn_id is set to 'my_hive_conn' which is clearly wrong for a MySQL workflow. This demonstrates a failure to use the appropriate provider-specific operator for the task.",0,"The code contains significant test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes test functions like 'test_operator_sql_execute_query()' and test_run initialization code. This is not production-ready code and clearly includes testing infrastructure that should not be present in a production DAG.",0,"The instruction asks for a 'MySQL database cleanup workflow that demonstrates dropping tables' and shows 'how to remove database tables programmatically and sequentially'. While the code does show dropping a table, it fails in multiple ways: (1) It uses a Hive connection ('my_hive_conn') instead of MySQL, (2) The workflow is not a cleanup workflow - it drops, then creates, inserts, and selects data, which is the opposite of cleanup, (3) It only demonstrates dropping a single table, not multiple tables sequentially as would be expected in a cleanup workflow. The business logic does not match the instruction for a MySQL cleanup demonstration.",0.0
921,Fine-tuned (Qwen 2.5 1.5B Airflow),282,1,"The code uses native Airflow decorators (@dag, @task.branch, @task) and proper TaskFlow API patterns. It leverages the @task.branch decorator for conditional execution, which is the idiomatic way to implement branching logic in modern Airflow. The code does not wrap logic unnecessarily in PythonOperator and uses the declarative TaskFlow API approach.",0,"The code contains several issues: 1) Uses non-ASCII characters in function name 'pick哪种路线()' which is unprofessional and potentially problematic. 2) Contains a hallucinated parameter 'return_columns=True' in @task.branch decorator that does not exist in Airflow 2.9.3. 3) Uses invalid syntax '@task.trigger_rule(""all_done"")' - trigger_rule should be a parameter inside @task(), not a separate decorator method. 4) The logic has a bug where 'time.sleep(exit_code)' would sleep for -1 seconds (invalid) or 0 seconds, which doesn't match the print statement claiming to wait. These issues indicate hallucinated/incorrect API usage.",0,"The instruction asks for 'parallel pipeline branches with configurable success conditions' and 'tasks conditionally executed or skipped based on predefined trigger rules.' While the code attempts branching with @task.branch, it fails to demonstrate: 1) True parallel execution (branches should execute in parallel, not sequentially). 2) Proper trigger rule configuration (the trigger_rule syntax is incorrect). 3) Multiple branches with different success conditions that showcase various trigger rules (e.g., one_success, all_success, all_failed, none_failed). 4) The branching logic doesn't actually skip tasks - it returns both routes always. The implementation is incomplete and doesn't properly demonstrate the requested execution strategies.",0.3333333333333333
922,Fine-tuned (Qwen 2.5 1.5B Airflow),283,1,"The code uses native Airflow operators designed for ADLS operations: ADLSCreateObjectOperator, LocalFilesystemToADLSOperator, and ADLSDeleteOperator. These are the appropriate provider-specific operators rather than wrapping logic in PythonOperator with hooks.",0,The code explicitly imports testing modules at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and includes test harness boilerplate 'test_run = get_test_run(dag)'. This is not production-ready code and represents test infrastructure leakage.,0,"The DAG has multiple critical issues that prevent it from fulfilling the instruction: 1) The task dependency chain is incorrect - it ends with 'create_adls()' which calls a task function instead of referencing the task object 'output_path'. 2) The 'upload_data' task references 'create_adl_sas_with_file.output' which doesn't make sense as ADLSCreateObjectOperator creates an object, not a local file to upload. 3) Syntax error in delete_adls: 'allow_trash_true' is not valid Python. 4) The workflow logic is backwards - it should upload THEN delete, but the create task is placed at the end. The instruction asks for 'uploading a local file and then immediately deleting it', but the code doesn't properly demonstrate this flow.",0.3333333333333333
923,Fine-tuned (Qwen 2.5 1.5B Airflow),284,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator, GCSDeleteBucketOperator, CloudAutoMLCreateDatasetOperator, CloudAutoMLCreateImageTrainingJobOperator, CloudAutoMLDeleteDatasetOperator, and CloudAutoMLDeleteImageTrainingJobOperator. It does not wrap logic in PythonOperator with hooks, which is the correct idiomatic approach for Airflow.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. This is a clear example of test infrastructure leakage.,1,"The DAG demonstrates the complete lifecycle of an AutoML image training job as requested: dataset initialization (CloudAutoMLCreateDatasetOperator), model training (CloudAutoMLCreateImageTrainingJobOperator), and resource deletion (CloudAutoMLDeleteDatasetOperator, CloudAutoMLDeleteImageTrainingJobOperator, GCSDeleteBucketOperator). It showcases programmatic management of machine learning infrastructure using Google Cloud services with proper workflow orchestration and task dependencies.",0.6666666666666666
924,Fine-tuned (Qwen 2.5 1.5B Airflow),285,1,"The code uses native Airflow operators appropriately: DataflowStartPipelineJobOperator for Dataflow operations and GCSCreateBucketOperator/GCSDeleteBucketOperator for GCS operations. It does not wrap these in PythonOperators but uses the provider-specific operators directly from airflow.providers.google.cloud.operators, which is the idiomatic approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also adds a watcher() task to the DAG with 'list(dag.tasks) >> watcher()'. These are not production-ready elements and represent test infrastructure leakage.",0,"The instruction asks for a workflow demonstrating data dependencies between two sequential assets where one asset triggers and informs the processing of a subsequent asset. However, the code does not use Airflow's Asset/Dataset feature for triggering or managing data dependencies. Instead, it simply chains two Dataflow operators sequentially using task dependencies (>>). There's no demonstration of how data from the first asset triggers or informs the second - they're just sequential tasks. The code mentions 'assets' in comments and variable names but doesn't actually implement Airflow's asset-based scheduling or data dependency management that would be expected for this instruction.",0.3333333333333333
925,Fine-tuned (Qwen 2.5 1.5B Airflow),286,0,"While the code attempts to use GKE operators from airflow.providers.google.cloud.operators.kubernetes_engine, many of the imported operators appear to be non-existent or incorrectly named (e.g., GKESTartPVCOperator, GKEREportComponentOperator with typos, GKEDeleteComponentOperator, GKEGetComponentsOperator). The standard Airflow GKE provider includes operators like GKECreateClusterOperator and GKEDeleteClusterOperator, but does not include most of the other operators used here. For running pods on GKE, the idiomatic approach would be to use KubernetesPodOperator from airflow.providers.cncf.kubernetes.operators.pod after creating the cluster, not these fabricated operators.",0,"The code contains numerous hallucinated/non-existent operators that are not part of the standard Airflow providers library. Operators like GKESTartPVCOperator, GKEDeleteComponentOperator, GKEGetComponentsOperator, GKEREportComponentOperator, GKEREportComponentsOperator, GKEREportJobsOperator, GKEREportPodsOperator, GKEGetJobHistoryOperator, GKEGetPodHistoryOperator, GKEGetTaskHistoryOperator, and GKESTopComponentOperator do not exist in Airflow 3.0.0 or any standard provider package. Additionally, the import path 'airflow.providers.google.cloud.operators.gke_start_pvc' and 'airflow.providers.google.cloud.operators.gke_stop_component' are fabricated. The code also has typos like 'GKEREport' instead of 'GKEReport' and 'GKESTart' instead of 'GKEStart', further indicating hallucination.",0,"The instruction requires: (1) creating a GKE cluster, (2) running test pods with different configurations, and (3) cleaning up the cluster. While the code includes GKECreateClusterOperator for cluster creation and appears to attempt cleanup, it does not properly implement running test pods with different configurations. The code uses fabricated operators instead of the proper KubernetesPodOperator to run workloads on the cluster. The workflow structure is also incomplete (code is cut off) and doesn't demonstrate a clear flow of create → run pods → cleanup. The CREATE_CLUSTER_BODY configuration also appears malformed with non-standard fields like 'preemptible_node_pool' that don't match GKE API specifications.",0.0
926,Fine-tuned (Qwen 2.5 1.5B Airflow),287,1,"The code uses native Alibaba Cloud OSS operators from airflow.providers.alibaba.cloud.operators.oss (OSSCreateBucketOperator, OSSUploadFileOperator, OSSDownloadFileOperator, OSSDeleteObjectsOperator, OSSDeleteBucketOperator) instead of wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for working with Alibaba OSS.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are internal testing utilities that indicate this is test code, not production-ready code.",1,"The DAG fulfills the user instruction to demonstrate cloud storage data movement and file lifecycle management using Alibaba OSS operators. It includes: (1) transferring data between local and cloud storage (upload_file uploads local file to OSS, get_file downloads from OSS to local), and (2) executing precise object deletion strategies (delete_file removes specific objects, delete_bucket removes the bucket). The workflow captures the essential steps of file lifecycle management as requested.",0.6666666666666666
927,Fine-tuned (Qwen 2.5 1.5B Airflow),288,1,"The DAG uses native Airflow operators designed for Azure Blob Storage tasks: AzureBlobCreateContainerOperator, AzureBlobUploadFileOperator, and AzureBlobDeleteContainerOperator. These are the appropriate provider-specific operators rather than wrapping logic in PythonOperator with hooks, which demonstrates idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'watcher()' is added to the task list. These are not production-ready elements and represent test infrastructure leakage.",0,"The instruction asks for a workflow that illustrates 'sequential upload and deletion of a file' in Azure Blob Storage. While the DAG includes upload and container deletion operations, it has several issues: (1) The task dependencies are not properly defined - tasks are added to a list but no sequential ordering is established using >> or << operators, (2) The 'delete_temp_file' task actually uploads a file with delete_existing_files=True rather than deleting the uploaded blob, (3) The DAG includes extraneous tasks like 'check_connections' and 'get_connection' that don't align with the simple upload-then-delete instruction, (4) The create_temp_file task has broken logic (calling get_free_blobs which doesn't match the intended file creation), and (5) There's a mismatch between the temp file path created and LOCAL_PATH used in upload. The core sequential flow of upload → delete is not clearly implemented.",0.3333333333333333
928,Fine-tuned (Qwen 2.5 1.5B Airflow),289,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production code. The comments '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' further confirm this is test code.",0,"The instruction specifically requested: (1) preemptible workers, (2) custom initialization scripts, (3) flexible metadata and package installations. The provided code does NOT include preemptible workers in the cluster configuration (missing 'preemptibility' setting in worker_config), does NOT include any initialization scripts (missing 'initialization_actions' in CLUSTER_CONFIG), and does NOT demonstrate metadata or package installation capabilities. While it does create a multi-node cluster and submit a job, it fails to meet the core requirements specified in the instruction.",0.3333333333333333
929,Fine-tuned (Qwen 2.5 1.5B Airflow),291,0,The code does not use proper Airflow operators. It references non-existent operators like 'ThirdPartyTestRunMasterTaskOperator' and 'SystemTestSetupOperator' which are not standard Airflow or Google Cloud provider operators. The code also uses a TaskFlow API @task decorator but then calls non-existent modules like 'gcp_vision_aiplatform.datasets.CreateDataset' instead of using proper Vertex AI operators like 'CreateDatasetOperator' from airflow.providers.google.cloud.operators.vertex_ai.dataset. The last line is incomplete and references a non-existent 'CreateAuto' class.,0,"The code contains multiple hallucinated elements: (1) Imports from 'system.google' modules like 'third_party_config' and 'workspace_config' which are internal testing utilities, not production code. (2) Uses fabricated operators 'ThirdPartyTestRunMasterTaskOperator', 'SystemTestSetupOperator', and 'ThirdPartyTestSetupTaskHelperTask' that don't exist in Airflow. (3) References non-existent 'gcp_vision_aiplatform' module instead of proper Airflow provider imports. (4) Contains test harness patterns with ENV_ID and SYSTEM_TESTS_ENV_ID environment variables typical of Airflow's internal system tests.",0,"The instruction asks for a pipeline that: (1) takes raw image data from Google Cloud Storage, (2) prepares it for object detection training, and (3) creates a deployable ML model. The provided code completely fails to address these requirements. It focuses on time-series forecasting with BigQuery data (not images from GCS), uses airline passenger data instead of image data, and doesn't implement any object detection workflow. The code is also incomplete (cuts off mid-line) and non-functional. It does not fulfill the core business logic of image-based object detection pipeline requested.",0.0
930,Fine-tuned (Qwen 2.5 1.5B Airflow),292,0,"The instruction explicitly asks to 'use task decorators' to execute Pinot interaction tasks, but the code uses traditional Operators (PinotAdminOperator and PinotDatabaseToPinotOperator) instead of the @task decorator pattern. While these are valid Airflow operators, they don't fulfill the requirement to use task decorators for implementing the Pinot interactions.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code, representing clear hallucination/leakage from test environments.",0,"The instruction requests: (1) use task decorators, (2) two distinct Pinot interaction tasks, (3) different connection methods, (4) parallel task execution using Pinot admin and database API hooks. The code fails on multiple counts: it doesn't use task decorators as requested, the tasks are executed sequentially (populate_pinot_table >> pinot_to_pinot_example) rather than in parallel, and it doesn't demonstrate using hooks within task decorators as implied by 'using Pinot admin and database API hooks'.",0.0
931,Fine-tuned (Qwen 2.5 1.5B Airflow),293,0,"The code attempts to use Weaviate operators but implements them incorrectly. Operators are instantiated and then called as functions (e.g., `weaviate_document_ingest()` after `WeaviateDocumentIngestOperator()`), which is not idiomatic Airflow. Operators should be instantiated with their parameters directly, not called as callables. Additionally, the code uses non-standard imports like `weaviate.classes.config`, `weaviate.data.reader`, and `weaviate.data.wvchandler` which appear to be custom/non-existent modules rather than standard Airflow provider patterns. The `Reader` and data loading logic wrapped in `@task` decorators suggests a mix of TaskFlow API with incorrect operator usage.",0,"The code contains clear hallucinations and test harness leakage. It imports `from tests_common.test_utils.system_tests import get_test_run` at the bottom, which is explicitly test infrastructure code that should not appear in production DAGs. Additionally, several imports appear to be fabricated: `weaviate.classes.config.UseWeaviateClassConfig`, `weaviate.components.utils.get_class_config_dynamic_loader`, `weaviate.data.reader.Reader`, and `weaviate.data.wvchandler.WeaviateCHandler` are not standard Airflow Weaviate provider modules. The operators `WeaviateVectorizeDataOperator` and the usage patterns shown don't match actual Airflow Weaviate provider APIs.",0,"The instruction asks for an end-to-end pipeline to 'load, vectorize, query, and manage structured knowledge data' for trivia data. The code attempts to load and ingest data but fails to implement: (1) actual vectorization/embedding generation using AI models (it assumes pre-computed vectors exist), (2) any querying/search functionality to demonstrate semantic search capabilities, and (3) proper data transformation from unstructured to structured format. The 'unload_data' task group appears to delete data rather than demonstrate querying. The pipeline is incomplete and doesn't fulfill the core requirement of demonstrating AI-powered embedding techniques for semantic search.",0.0
932,Fine-tuned (Qwen 2.5 1.5B Airflow),294,1,"The code uses native Airflow Dataplex operators from airflow.providers.google.cloud.operators.dataplex and airflow.providers.google.cloud.sensors.dataplex. It properly leverages DataplexCreateCatalogEntryOperator, DataplexCreateEntryGroupOperator, DataplexGetCatalogEntryOperator, and other specialized operators instead of wrapping logic in PythonOperator with hooks. This is idiomatic Airflow usage.",0,"The code contains multiple issues: (1) Syntax error with Chinese comma character '，' instead of ',' in the create_catalog_entry_task (line: description='My Table'，). (2) Invalid attribute access 'Location.region_id' - Location is imported as a class but used incorrectly. (3) Undefined variable 'location' in f-string (resource_name=f'gs://{location}/...'). (4) The code appears incomplete (cuts off at list_catalog_entries comment). (5) Several operators like DataplexCreateLocationOperator and DataplexCreateWorkforceOperator may not exist in standard Airflow providers - these appear to be hallucinated or non-standard operators.",1,"The DAG attempts to fulfill the user instruction by creating a metadata registration pipeline using Google Cloud Dataplex. It includes: (1) Creating entry groups to organize metadata, (2) Creating catalog entries with schema definitions to capture metadata attributes, (3) Getting and listing catalog entries for data discovery, (4) Update and delete operations for governance. The workflow systematically tracks data assets and supports comprehensive data discovery and governance initiatives as requested, despite implementation errors.",0.6666666666666666
933,Fine-tuned (Qwen 2.5 1.5B Airflow),295,0,"The code wraps transfer logic inside PythonOperators using @task decorators (create_file_in_sftp, remove_file_in_sftp) instead of using the native LocalFilesystemToSFTPOperator. The code manually instantiates and calls execute() on transfer objects within Python callables, which is not idiomatic. Proper Airflow usage would leverage the LocalFilesystemToSFTPOperator directly as task operators.",0,"The code contains multiple hallucinations and test harness imports: (1) Imports 'from tests.system.utils.watcher import watcher' and uses 'list(watcher(dag))', (2) Imports 'from tests.system.utils import get_test_run' and calls 'test_run = get_test_run(dag)', (3) Uses non-existent class names 'LocalFileSystemToSFTPTransfert' and 'LocalFilesystemToSFTPTransfert' with typos ('Transfert' instead of 'Transfer'), (4) References 'SFTPSensor' from incorrect import path 'airflow.providers.sftp.sensors.base' instead of the correct path. These are clear signs of hallucinated/test code that should not be in production.",0,"The instruction requires moving files FROM cloud object storage (e.g., S3, GCS, Azure Blob) TO SFTP with complete lifecycle tracking. This code only demonstrates local filesystem to SFTP transfer, not cloud object storage to SFTP. It misses the critical requirement of integrating with cloud storage providers (S3Operator, GCSOperator, etc.). Additionally, the lifecycle tracking is incomplete - the remove_file_in_sftp function appears broken and doesn't properly demonstrate cleanup or comprehensive tracking of the migration lifecycle.",0.0
934,Fine-tuned (Qwen 2.5 1.5B Airflow),296,1,"The code correctly uses the TrinoExecuteQueryOperator from airflow.providers.trino.operators.trino, which is the idiomatic and native operator for executing Trino queries. It does not wrap Trino logic in PythonOperator with hooks, demonstrating proper use of Airflow's provider pattern.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and a comment 'Needed to run the examp' (appears truncated). This is internal testing infrastructure that should not be present in production DAG code, indicating hallucination/leakage from test examples.",0,"The instruction asks for creating 'a schema and multiple tables' but the code only creates one table ('cities') and attempts to create 'stk' as a copy. No schema creation is demonstrated (CREATE SCHEMA statement). Additionally, several tasks have logical errors: sql3 and sql4 define parameters but don't use them in the SQL (hardcoded values instead of parameterized queries), the merge_sql syntax is incorrect, and there are references to non-existent tables ('dummy_table'). The code does not properly demonstrate 'templated and parameterized queries' as requested - the parameters are defined but not actually used in the SQL statements.",0.3333333333333333
935,Fine-tuned (Qwen 2.5 1.5B Airflow),297,0,"While the DAG uses JenkinsJobTriggerOperator correctly for triggering the job, it wraps JenkinsHook in a PythonOperator (@task decorator) for configuration instead of using native Airflow patterns. The create_jenkins_hook task is unnecessary and non-idiomatic - hook configuration should be done via Airflow connections, not in a task. Additionally, the hook manipulation (hook.url = JENKINS_URL, hook.api_token = JENKINS_API_TOKEN) is improper usage.",1,"The code uses only standard Airflow libraries (airflow.decorators, airflow.models.dag, airflow.providers.jenkins). There are no imports from internal testing modules or test harness boilerplate like 'tests_common.test_utils.system_tests'.",0,"The instruction requires two key components: (1) trigger a Jenkins job for creating configuration files, and (2) download the generated artifact. The DAG only implements the first part (triggering the job via JenkinsJobTriggerOperator) but completely misses the artifact retrieval step. There is no task to download or manage the generated artifacts from Jenkins, which is a critical requirement of the workflow.",0.3333333333333333
936,Fine-tuned (Qwen 2.5 1.5B Airflow),298,1,"The DAG uses native Airflow providers and operators specifically designed for Google Cloud Dataprep (DataprepCreateFlowOperator, DataprepCopyFlowOperator, DataprepGetFlowOperator, DataprepListFlowsOperator, DataprepDeleteFlowOperator) and GCS (GCSCreateBucketOperator, GCSDeleteBucketOperator). It does not wrap functionality in PythonOperators with hooks, which would be non-idiomatic. The code follows Airflow best practices by using the appropriate operators from airflow.providers.google.cloud.operators.dataprep.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not be present in production-ready DAG code.",1,"The DAG fulfills the user instruction to orchestrate a complex data transformation workflow using Google Dataprep. It demonstrates: (1) task dependencies through proper chaining of operators, (2) connection management via Dataprep operators that handle authentication, (3) cloud resource provisioning by creating/deleting GCS buckets, and (4) a systematic approach to creating, running, and monitoring data processing jobs through the complete lifecycle of create_flow -> copy_flow -> get_flow -> list_flows -> delete_flow. The workflow showcases the full CRUD operations on Dataprep flows with proper cleanup using trigger rules.",0.6666666666666666
937,Fine-tuned (Qwen 2.5 1.5B Airflow),299,0,"The code fails to use any actual Airflow operators for Google Cloud Monitoring/Stackdriver. Instead, it wraps everything in a generic @task decorator (PythonOperator equivalent) called 'create_service' that doesn't actually implement any real functionality. Proper implementation should use operators from airflow.providers.google.cloud.operators.stackdriver (e.g., StackdriverListAlertPoliciesOperator, StackdriverUpsertAlertOperator, StackdriverDeleteAlertOperator, StackdriverUpsertNotificationChannelOperator).",0,"The code contains multiple hallucinated elements: (1) References to non-existent 'system-tests.utils' module with functions like 'listings()', 'get_listing_item()', 'update_listing_item()', 'delete_listing_item()', and 'service_delete()' which are not part of Airflow. (2) Invalid syntax like 'system-tests.utils' (hyphens in module names). (3) Undefined variables like '_SERVICE_PATH_'. (4) The create_service task is called as a function with parameters it doesn't accept. (5) Incomplete line at the end ('delete_alert_polic'). This appears to be test harness code mixed with fabricated utilities.",0,"While the code attempts to address the instruction by including concepts of creating notification policies, alert policies, listing, getting, updating, enabling/disabling (enabled: true/false), and deleting alerts, it fails to implement any actual working functionality. The tasks don't perform real operations, the configuration structures are incorrect for Google Cloud Monitoring API, and the workflow doesn't create a functional monitoring and alerting system. The code is essentially non-functional pseudocode that cannot create, manage, or configure actual alert policies or notification channels in Google Cloud.",0.0
938,Fine-tuned (Qwen 2.5 1.5B Airflow),300,0,"The code uses DatabricksSubmitRunOperator and DatabricksSensor, but these are not the appropriate operators for 'interacting with Databricks SQL environments' as requested. For Databricks SQL operations (table creation, data insertion, transformation, file-based export), the code should use DatabricksSqlOperator from airflow.providers.databricks.operators.databricks_sql, which is specifically designed for executing SQL statements against Databricks SQL warehouses. The DatabricksSubmitRunOperator is meant for submitting Spark jobs to Databricks clusters, not for SQL operations.",0,"The code contains clear test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run_from_test_group, TestGroup' followed by test_group and test_run instantiation. These are internal testing modules that should not appear in production DAG code. Additionally, the operators used have suspicious/non-standard parameters like 'sql_task_id', 'split_into_chunks', and 'package_layers' that don't match the actual Airflow 3.0.1 Databricks provider API, suggesting hallucinated parameters.",0,"The instruction explicitly requests an end-to-end data pipeline demonstrating: (1) table creation, (2) data insertion, (3) transformation, (4) file-based export, and (5) cross-storage data import using Databricks SQL. The provided code does none of these. It only contains sensor tasks and a generic submit run operator with no actual SQL operations, no table creation, no data manipulation, no transformations, and no file operations. The code completely fails to implement the requested business logic.",0.0
939,Fine-tuned (Qwen 2.5 1.5B Airflow),301,0,"The code imports SingularityOperator but never uses it. Instead, it wraps a simple print statement in a @task decorator (PythonOperator equivalent). The instruction explicitly mentions 'containerized command execution' which should use the SingularityOperator, but the implementation only prints a message rather than executing containerized commands using the imported operator.",0,"The code contains multiple issues: (1) TaskGroup is used but never imported, causing a NameError. (2) The code includes START/END comment markers like '[START howto_operator_singularity]' which are typical of Airflow's internal documentation/testing examples, suggesting this is test harness boilerplate. (3) The dag_id has a space ('example Singularity') which is invalid. (4) The code structure appears to be copied from Airflow's example/test files rather than being production-ready.",0,"The instruction requires an observability pipeline with: (1) date printing, (2) sleep operations, (3) containerized command execution, and (4) clear task interdependencies. The code fails on multiple counts: no date printing task exists, no sleep operations are implemented, containerized execution is not actually performed (SingularityOperator is imported but unused), and there are no task interdependencies defined (no >> or << operators to show task flow). The code only contains a simple print statement wrapped in a task decorator.",0.0
940,Fine-tuned (Qwen 2.5 1.5B Airflow),302,1,"The code correctly uses native Airflow SQS operators from airflow.providers.amazon.aws.operators.sqs including SqsCreateQueueOperator, SqsDeleteQueueOperator, SqsReceiveMessageOperator, SqsSendMessageOperator, and SqsSendMessageBatchOperator. It does not wrap SQS logic in PythonOperator with hooks, which would be non-idiomatic. The operators are used appropriately for their intended purposes.",0,"The code contains multiple critical syntax errors that indicate it's incomplete or corrupted: 1) Multiple DAG context managers use 'is:' instead of 'as dag:' or just ':' (appears 3 times), 2) The variable SQS_QUEUE_URL is referenced but never defined, 3) Multiple lines end with 'is' without any statement (e.g., 'remove_connection() is'), 4) The code is incomplete (cuts off mid-statement with 'get_id = BashOperator('), 5) The remove_connection() function references 'airflow.db.merge_conn' which is not a valid API call. While it doesn't import test harness modules, these errors make it non-production-ready and suggest code generation issues.",0,"The instruction asks for a complete workflow demonstrating: 1) publishing messages, 2) consuming messages in single and batch modes, 3) managing queue lifecycle. While the code attempts to show these elements (send single/batch, receive single/batch, delete queue), it fails to demonstrate queue creation properly (SqsCreateQueueOperator is only partially shown), the tasks lack proper dependencies/sequencing, SQS_QUEUE_URL is undefined making the workflow non-functional, and the code is incomplete. The workflow doesn't properly showcase the full lifecycle from queue creation through message operations to queue deletion in a coherent, executable manner.",0.3333333333333333
941,Fine-tuned (Qwen 2.5 1.5B Airflow),303,0,"The code attempts to use 'DatasetTriggerDagRunOperator' which does not exist in standard Airflow. The correct approach for dataset-based scheduling in Airflow is to use the 'schedule' parameter with Dataset objects and have tasks emit dataset updates using 'outlets' parameter. The code also incorrectly tries to use Dataset objects as task_ids (e.g., 'EmptyOperator(task_id=dataset)' where dataset is a Dataset object, not a string), and attempts non-existent methods like 'Dataset.get_changes()' and 'Dataset.refresh()'. This demonstrates a fundamental misunderstanding of Airflow's dataset scheduling mechanism.",0,"The code contains multiple hallucinations and test harness leakage: (1) Imports from 'tests.system.utils.watcher' and 'tests.system.utils' which are internal testing modules, (2) Uses 'get_test_run(dag)' which is test boilerplate, (3) References a non-existent 'DatasetTriggerDagRunOperator', (4) Uses non-existent Dataset methods like 'Dataset.get_changes()' and 'Dataset.refresh()', (5) Incorrectly imports and uses 'Session' from 'airflow.www.settings' for dataset operations. The code also has a typo 'datsets' instead of 'datasets' in the second operator call.",0,"The instruction asks for 'a series of DAGs that demonstrate dataset-based scheduling with various dependency scenarios' showcasing 'dataset updates, time schedules, and complex logical conditions'. This code only provides a single DAG with broken logic. It does not demonstrate: (1) Multiple DAGs with dataset dependencies between them, (2) How to produce/emit dataset updates (using outlets), (3) Time-based schedules combined with datasets, (4) Complex logical conditions (AND/OR dataset dependencies), or (5) A working example of dataset-triggered workflows. The code fails to fulfill the core requirement of illustrating advanced scheduling mechanisms.",0.0
942,Fine-tuned (Qwen 2.5 1.5B Airflow),304,1,"The code uses the native MongoToS3Operator from airflow.providers.amazon.aws.transfers.mongo_to_s3, which is the idiomatic Airflow approach for transferring data from MongoDB to S3. It also uses S3CreateBucketOperator and S3DeleteBucketOperator appropriately. The code does not wrap logic in PythonOperator with hooks, but instead uses the purpose-built transfer operator.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from system.amazon.aws.utils import SystemTestContextBuilder', 'from tests_common.test_utils.watcher import watcher', and 'from tests_common.test_utils.system_tests import get_test_run'. These are internal testing modules that indicate this is test/example code rather than clean production-ready code. The sys_test_context_task and watcher() calls are also test-specific patterns.",1,"The code fulfills the user instruction to develop a data pipeline that filters and exports specific documents from MongoDB to S3 using predefined query criteria. The MongoToS3Operator is configured with mongo_query={'status': 'completed'}, which filters documents based on a status condition as requested. The workflow selectively moves data between MongoDB and S3 storage platforms as specified.",0.6666666666666666
943,Fine-tuned (Qwen 2.5 1.5B Airflow),305,0,"The DAG heavily relies on @task decorators that wrap manual API calls using googleapiclient.discovery.build() instead of using native Airflow operators. For example, create_gcp_source_repository() manually calls the Source Repo API instead of using a dedicated operator. The copy_to_bucket() task also manually constructs API calls. While copy_source_repo_branch_to_bucket appears to be a function call, it's not a proper operator instantiation. The code should use operators from airflow.providers.google.cloud.operators.* where available.",0,"The code contains several hallucinated/non-existent components: 1) copy_source_repo_branch_to_bucket is imported/called as a function but doesn't exist in standard Airflow providers, 2) The line 'delete_repo = GoogleCloudSourceRepoHook() >> GitHubSourceHook()' is nonsensical - it attempts to chain Hooks (not operators) using the bitshift operator, which is invalid syntax, 3) The API service 'source-repo' with methods like repos().insert() and repos().copy() don't match actual GCP Source Repositories API structure, 4) GCP_SOURCE_REPOSITORY_URL is defined but never used. The code appears to be a mix of real and fabricated Airflow/GCP components.",0,"The instruction asks for a workflow leveraging Google Cloud Build for automated software deployment and infrastructure provisioning, with dynamic trigger creation, project number retrieval, and cloud resource orchestration. However, the provided code focuses entirely on Google Cloud Source Repositories (copying branches to buckets), with no Cloud Build operators, no build triggers, no project number retrieval tasks, and no deployment or infrastructure provisioning logic. The DAG completely misses the core requirement of using Cloud Build.",0.0
944,Fine-tuned (Qwen 2.5 1.5B Airflow),306,0,"The code uses a PythonOperator pattern (via @task decorator) with SqlHook inside the 'create_and_populate_table' function instead of using the native SQLExecuteQueryOperator for table creation. The idiomatic approach would be to use SQLExecuteQueryOperator for all SQL operations, not wrap a hook in a Python callable.",0,"The code appears to be copied from Airflow's system tests or documentation examples. It contains test-related patterns like 'ENV_ID = os.environ.get(""SYSTEM_TESTS_ENV_ID"")', documentation markers like '# [START howto_operator_sql_template]' and '# [END howto_operator_sql_template]', and an absurd dependency chain referencing dag.children[0] through dag.children[73+]. This is clearly test harness boilerplate and not production-ready code.",0,"The instruction explicitly asks for a 'sensor mechanism for waiting on upstream data readiness' and to 'simulate checking for data availability'. The code does not include any Sensor (e.g., SqlSensor, FileSensor, or custom sensor). It only contains SQL execution operators and a table creation task. There is no sensor pattern implemented at all, which is the core requirement of the instruction.",0.0
945,Fine-tuned (Qwen 2.5 1.5B Airflow),307,0,"The code fails to use idiomatic Airflow patterns. It wraps Spark connection creation in a PythonOperator instead of using proper Spark/Hive providers. The instruction asks for HiveOperator usage, but the code uses generic SQLExecuteQueryOperator. For a multi-stage data orchestration with Hive, it should use `airflow.providers.apache.hive.operators.hive.HiveOperator` instead of wrapping everything in PythonOperator and SQLExecuteQueryOperator. Additionally, there's a bizarre construct where `delete_spark_task` is a PythonOperator with `python_callable` set to a BashOperator instance, which is completely wrong.",0,"The code contains multiple instances of test harness boilerplate and internal testing modules that should not be in production code: `from tests_common.test_utils.watcher import watcher` (appears twice), `from tests_common.test_utils.system_tests import get_test_run`, `test_run = get_test_run(dag)`, and test setup/teardown functions (`test_setup()`, `test_teardown()`). The code also has syntax errors like 'FORMAT JSONLSECTED' and incomplete comment tags like '[START howto_operator_adf_trigger_post>' (missing closing bracket). There's also an undefined `SparkSession` import.",0,"The instruction asks for a multi-stage data orchestration demonstrating data transfer between local storage, HDFS, Hive tables, and MySQL database using BashOperator and HiveOperator. The code fails on multiple fronts: (1) No HiveOperator is used despite being explicitly requested, (2) No MySQL interaction is implemented, (3) No clear demonstration of data transfer between local storage and HDFS, (4) The workflow is incomplete and contains broken tasks. The DAG ID is 'example_adf_to_sql' which suggests Azure Data Factory, not the requested Hive/HDFS/MySQL orchestration. The code appears to be a corrupted example from a different use case.",0.0
946,Fine-tuned (Qwen 2.5 1.5B Airflow),308,1,"The code uses native Airflow operators specifically designed for Google Cloud Compute Engine tasks. It properly imports and uses ComputeEngineInsertInstanceOperator and ComputeEngineDeleteInstanceOperator from airflow.providers.google.cloud.operators.compute, rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for GCE resource management.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests_common.test_utils.system_tests import get_test_run' and includes 'test_run = get_test_run(dag)'. These are internal testing modules that represent hallucination/leakage from test examples into what should be production-ready code.",0,"The instruction asks for a pipeline demonstrating 'cluster initialization, data transformation, and cluster teardown' with best practices for resource management. While the code shows cluster initialization (create_instance) and teardown (delete_instance), it completely lacks the data transformation component. There is no actual data processing or transformation task between the create and delete operations. The DAG only demonstrates infrastructure lifecycle management without any actual data processing work, missing a key requirement of the instruction.",0.3333333333333333
947,Fine-tuned (Qwen 2.5 1.5B Airflow),309,1,"The code uses specific Kylin operators (KylinCubeOperator, KylinSwitchOperator, KylinGetCubeStatusOperator, KylinListCubesOperator, KylinDeleteCubeOperator, etc.) which are provider-specific operators designed for Kylin cube operations. This follows idiomatic Airflow patterns by using native operators rather than wrapping logic in PythonOperator with hooks.",0,"The code contains clear hallucination/test harness boilerplate at the end. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and uses 'ListDagsOperator(dag=dag, watcher=watcher)' in the __main__ block. These are internal testing modules that should not appear in production DAG code. Additionally, many of the Kylin operators used (KylinSwitchOperator, KylinPushDataToXComOperator, KylinTaskOperator, KylinCubeDynamicConfigOperator, KylinInstallPluginOperator, KylinLoadCubeOperator, KylinUninstallPluginOperator) do not exist in standard Airflow providers, suggesting these are fabricated/hallucinated operators.",0,"While the code attempts to showcase Kylin cube operations, it fails to properly demonstrate sequential execution with clear task dependencies. The tasks are defined but no dependency chains are established (no >> or << operators connecting tasks). The instruction specifically requested 'sequential execution', 'task dependencies', 'time-based data segmentation', and 'progression from build to purge', but the code shows disconnected tasks without proper orchestration. Additionally, there's no evidence of time-based data segmentation implementation, and the workflow progression is unclear due to missing task dependencies.",0.3333333333333333
948,Fine-tuned (Qwen 2.5 1.5B Airflow),310,1,"The code uses native Airflow operators from the Google Cloud provider package (GCSCreateBucketOperator, GCSDeleteBucketOperator, CloudSpeechToTextRecognizeSpeechOperator, LocalGCSTransferOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for Google Cloud integrations.",0,"The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not be present in production-ready code. Additionally, the code uses SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables which are testing artifacts.",0,"The instruction asks for 'audio generation' as part of the workflow, but the code does not include any audio generation step. The LocalGCSTransferOperator is also incorrectly configured (it appears to be trying to upload a file but has parameters that suggest downloading). The workflow should generate audio, upload it to GCS, perform speech recognition, and then cleanup - but the audio generation step is completely missing. Additionally, the LocalGCSTransferOperator usage is incorrect as it references a local file path that doesn't exist and the parameters (source_bucket, source_object, destination_local_path) suggest a download operation rather than an upload.",0.3333333333333333
949,Fine-tuned (Qwen 2.5 1.5B Airflow),311,1,"The DAG uses native Google Cloud Data Catalog operators from airflow.providers.google.cloud.operators.datacatalog, including CloudDataCatalogCreateEntryGroupOperator, CloudDataCatalogCreateEntryOperator, CloudDataCatalogUpdateEntryGroupOperator, CloudDataCatalogListEntriesOperator, and CloudDataCatalogDeleteEntryOperator. It does not wrap functionality in PythonOperators with hooks, which is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) | watcher()'. These are not production-ready elements and represent test infrastructure leakage.",0,"The instruction specifically requested 'attaching custom metadata tags' as part of the complete lifecycle demonstration. While the DAG successfully demonstrates creating entry groups, registering file entries (via CloudDataCatalogCreateEntryOperator), updating entry groups, and listing entries, it does not include any operators or logic for attaching custom metadata tags to the entries. This is a key missing component of the requested functionality.",0.3333333333333333
950,Fine-tuned (Qwen 2.5 1.5B Airflow),313,0,"The DAG uses a mix of idiomatic and non-idiomatic patterns. While it correctly uses ComputeEngineInsertInstanceOperator for one instance creation, it wraps the other instance creation in a @task decorator with ComputeEngineHook, which is non-idiomatic. The idiomatic approach would be to consistently use ComputeEngineInsertInstanceOperator for all instance creation tasks rather than mixing operators with hook-based task functions.",0,"The code contains several issues: (1) It calls a non-existent method 'create_instance.getInstanceResult()' which is not a valid Airflow pattern, (2) The code is incomplete and cuts off mid-line with 'smb_co', (3) It includes commented code markers like '[START howto_operator_gce_create]' which appear to be from documentation/test examples, (4) The code references Samba/SMB components that are unrelated to Google Calendar data synchronization mentioned in the instruction.",0,"The instruction explicitly requests a Google Calendar data synchronization pipeline, but the code implements a GCP Compute Engine to Samba file transfer workflow. There is no Google Calendar integration, no calendar data handling, and no calendar-related operators or hooks. The code also lacks a complete teardown stage (only partial deletion operators are implied but not shown). The business logic is completely misaligned with the user's request for Calendar data synchronization.",0.0
951,Fine-tuned (Qwen 2.5 1.5B Airflow),314,1,"The code uses native Airflow operators appropriately: S3CreateBucketOperator, LocalFilesystemToS3Operator, and S3DeleteBucketOperator from the AWS provider package. The file creation uses a TaskFlow @task decorator which is idiomatic for simple Python operations. No generic PythonOperator wrapping hooks is present.",0,"The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production DAG code. Additionally, the ENV_ID variable references 'SYSTEM_TESTS_ENV_ID' which is test infrastructure.",1,"The DAG fully demonstrates the requested workflow: (1) creates a temporary file programmatically using the create_sample_file task, (2) transfers it to S3 using LocalFilesystemToS3Operator, and (3) cleans up by deleting the S3 bucket. The workflow showcases file creation, movement to cloud storage, and cleanup as requested.",0.6666666666666666
952,Fine-tuned (Qwen 2.5 1.5B Airflow),315,0,"The code does not contain any Airflow DAG structure, operators, or tasks. It only shows a test function that validates environment variables. There are no BigQuery operators, no task definitions, and no DAG instantiation. This completely fails to demonstrate idiomatic Airflow patterns.",0,"The code snippet appears to be a test harness fragment (test_env_variables function) rather than production DAG code. This suggests test boilerplate leakage. Additionally, it references undefined variables (PROJECT_ID, BUCKET) without proper context or imports, which is not production-ready code.",0,"The instruction explicitly requested a complete Airflow DAG that demonstrates: (1) table creation in BigQuery, (2) data transfer between tables for employee salary data, and (3) cleanup operations. The provided code contains none of these elements - it's merely a test assertion function with no DAG definition, no BigQuery operations, no data migration logic, and no workflow orchestration whatsoever.",0.0
953,Fine-tuned (Qwen 2.5 1.5B Airflow),316,0,"The code uses non-existent or incorrectly named Bedrock operators (BedrockCreateIngestionOperator, BedrockDeleteIngestionOperator, BedrockIngestDataOperator, BedrockRunIngestionOperator, BedrockSensor) that are not part of the standard Airflow AWS provider. Additionally, it wraps custom logic in helper functions like _create_aito_model_from_bucket that execute shell commands via subprocess, which is not idiomatic. The code also uses a @task decorator with BedrockHook in delete_model instead of using proper Bedrock operators. Real Airflow Bedrock operators focus on model invocation, knowledge bases, and provisioned throughput - not these fabricated ingestion operators.",0,"The code explicitly imports 'from tests_common.test_utils.watcher import watcher' which is a test harness module that should not appear in production code. Additionally, the operators used (BedrockCreateIngestionOperator, BedrockIngestDataOperator, BedrockRunIngestionOperator, BedrockSensor with model_arn parameter) do not exist in the actual Airflow AWS provider library. The code also references non-existent AIToModel concepts and fabricated ingestion patterns that are not part of AWS Bedrock's actual API.",0,"The instruction requests an end-to-end ML pipeline demonstrating model training, throughput allocation, and model lifecycle management using AWS Bedrock services. However, the provided code does not implement any of these requirements. It does not show model training (Bedrock uses pre-trained foundation models or fine-tuning jobs), does not configure provisioned throughput allocation, and does not demonstrate proper lifecycle management. Instead, it focuses on a fabricated 'AIToModel ingestion' concept that doesn't align with actual Bedrock capabilities (custom models, fine-tuning, knowledge bases, agents, or guardrails). The code is also incomplete (cuts off mid-parameter) and contains broken logic.",0.0
954,Fine-tuned (Qwen 2.5 1.5B Airflow),317,1,"The code uses the SingularityOperator from airflow.providers.singularity.operators.singularity, which is a specific provider operator designed for the task. It also uses the @task decorator for Python tasks, which is idiomatic Airflow 2.0+ style. No generic PythonOperator wrapping hooks is present.",0,The code explicitly imports and uses testing utilities: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production code.,0,"The instruction asks for a workflow demonstrating setup and teardown tasks with resource allocation and cleanup for a multi-stage data processing pipeline. The provided code does not demonstrate setup/teardown patterns (no use of setup/teardown task groups, no clear preparatory or cleanup tasks), and instead shows a simple Singularity container execution example. It does not fulfill the requirement of showing resource management with preparatory tasks, main work tasks, and cleanup tasks in a complete operational workflow.",0.3333333333333333
955,Fine-tuned (Qwen 2.5 1.5B Airflow),319,0,"The code fails to use idiomatic Airflow patterns. While it imports DataprocCreateClusterOperator and DataprocDeleteClusterOperator, it doesn't actually use them. Instead, it wraps ComputeEngineHook logic inside a @task decorated function (create_cluster), which is essentially a PythonOperator pattern. The code should use the native DataprocCreateClusterOperator, DataprocUpdateClusterOperator, and DataprocDeleteClusterOperator that are already imported but never utilized. Additionally, the create_cluster function attempts to manually create compute instances using ComputeEngineHook rather than using Dataproc operators for cluster management.",0,"The code contains several issues indicating incomplete or hallucinated content. First, the code is truncated mid-statement ('create_cluster(CLUSTER_NA'), suggesting it's incomplete. Second, there's a suspicious line 'ENV_ID = ""{{ env_id() }}""' which appears to be a template variable that doesn't exist in standard Airflow. Third, the create_cluster function uses ComputeEngineHook methods (create_instance, insert_command) in ways that don't align with the actual API - the logic appears confused between creating compute instances and Dataproc clusters. The function body mixes compute instance creation with what seems like attempted Dataproc configuration, which is not how these APIs work.",0,"The instruction asks for a pipeline demonstrating dynamic cluster provisioning and management with allocation/deallocation based on processing requirements. The code fails to deliver this: (1) It doesn't show any actual big data processing workload being executed on the clusters, (2) It doesn't demonstrate dynamic scaling based on processing requirements - the cluster configurations are static, (3) The DataprocUpdateClusterOperator is imported but never used to demonstrate dynamic resource adjustment, (4) The code is incomplete and doesn't show the full workflow of create -> process -> scale -> delete that would demonstrate the flexibility requested, (5) No processing jobs (Spark, Hadoop, etc.) are submitted to show why dynamic provisioning would be beneficial.",0.0
956,Fine-tuned (Qwen 2.5 1.5B Airflow),320,0,"The code attempts to use transfer operators (GCSToSambaOperator, SMBSToGCSOperator) which is idiomatic, but the implementation is severely broken. Most tasks incorrectly use GCSCreateBucketOperator for operations like copy, move, delete, and list - these should use specific operators like GCSToLocalFilesystemOperator, GCSDeleteObjectsOperator, etc. The code also wraps bucket creation in a @task decorator unnecessarily and references non-existent operators like XComPushOperator and GetFileFromSMBOperator.",0,"The code contains multiple hallucinated elements: 1) XComPushOperator does not exist in Airflow, 2) GetFileFromSMBOperator is not a real operator, 3) TaskObj.get_task_by_id() is not a valid Airflow API pattern, 4) The code references 'tests_common' style patterns with SYSTEM_TESTS_ENV_ID and SYSTEM_TESTS_GCP_PROJECT environment variables, suggesting this is test harness boilerplate. The code also has syntax errors (unclosed string in get_example_file task_id) and incomplete implementation.",0,"The instruction requires demonstrating: 1) single file copy from GCS to Samba, 2) file move, 3) directory copy, and 4) selective file movement. The code fails to implement any of these scenarios correctly. Instead of using GCSToSambaOperator for transfers, it misuses GCSCreateBucketOperator throughout. There's one SMBSToGCSOperator (reverse direction) but it's misconfigured. The code does not demonstrate the requested GCS-to-Samba transfer scenarios with different strategies (copy vs move, single file vs directory, selective movement).",0.0
957,Fine-tuned (Qwen 2.5 1.5B Airflow),321,0,"The code misuses operators and shows fundamental misunderstandings of Airflow patterns. GCSCreateBucketOperator is incorrectly assigned to GCS_TO_GCS_TRANSFER_CONFIG with parameters like 'source_bucket' and 'destination_bucket' that don't belong to bucket creation. The GCSToGCSOperator is used with invalid syntax ('gcs_object_name=' appears as a parameter name in the code itself). The code attempts to use Cloud Transfer Service concepts but doesn't actually use the CloudDataTransferServiceOperators (like CloudDataTransferServiceGCSToGCSOperator) that are designed for this purpose. Instead, it mixes GCSToGCSOperator with ImportJob objects incorrectly.",0,"The code contains multiple hallucinated constructs: CCSTransferObject and GCSToGCSTransferConfig are not standard Airflow provider classes. The ImportJob is imported from google.cloud.transfer_v2.types but used incorrectly as a parameter to GCSToGCSOperator. The code has syntax errors (incomplete 'delete_instance' task, invalid 'gcs_object_name=' syntax). The GCSCreateBucketOperator is given non-existent parameters like 'source_bucket', 'destination_bucket', 'overwrite_existing', and 'pre_delete_source'. The code appears to mix actual Google Cloud SDK types with non-existent Airflow constructs.",0,"The instruction asks for a pipeline demonstrating cloud data movement with Google Cloud Transfer Service, robust error management, and cleanup procedures. The code fails on multiple fronts: (1) It doesn't properly implement Cloud Transfer Service operations - it misuses GCSToGCSOperator instead of the appropriate transfer service operators, (2) There is no error management or retry logic implemented, (3) Cleanup procedures are incomplete (delete_instance task is cut off, and there's no proper cleanup orchestration with trigger_rule=TriggerRule.ALL_DONE), (4) The workflow doesn't demonstrate reliable file transfers as the operators are misconfigured with invalid parameters and syntax errors. The code is non-functional and doesn't meet the core requirements.",0.0
958,Fine-tuned (Qwen 2.5 1.5B Airflow),322,1,"The code uses native Airflow operators from the Google Cloud provider package (CloudTransferCreateJobOperator, CloudTransferGetJobOperator, CloudTransferListJobsOperator, CloudTransferUpdateJobOperator, CloudTransferDeleteJobOperator) rather than wrapping logic in PythonOperator with hooks. This is the idiomatic Airflow approach for interacting with Google Cloud Transfer Service.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also calls 'watcher()' in the DAG definition. These are not production-ready elements and represent test infrastructure leakage.",0,"The user instruction asks for a workflow that 'monitors and validates file uploads to a Google Cloud Storage bucket' and 'checks for completed file transfers and triggers subsequent data processing steps based on file arrival and update conditions'. However, the provided code demonstrates Google Cloud Transfer Service job management (create, get, list, update, delete operations) rather than monitoring GCS bucket file uploads. The code does not include any file arrival sensors (like GCSObjectExistenceSensor or GCSObjectUpdateSensor) or validation logic for uploaded files. It also lacks any meaningful data processing steps - only a simple bash echo command. The code appears to be a generic example/tutorial for Cloud Transfer Service operators rather than a solution to the specific monitoring and validation requirements.",0.3333333333333333
959,Fine-tuned (Qwen 2.5 1.5B Airflow),323,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, CloudGenomicsRunPipelineOperator for running the pipeline, and CloudGenomicsDeletePipelineOperator for cleanup. These are the correct provider-specific operators from airflow.providers.google.cloud, not generic PythonOperator wrappers around hooks.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically: (1) imports from 'tests.system.utils.watcher' and uses 'watcher()' function, (2) imports 'get_test_run' from 'tests.system.utils', and (3) includes 'test_run = get_test_run(dag)' at the end. These are internal testing utilities that indicate this is test/example code, not production-ready DAG code.",0,"The instruction requested three specific actions: (1) upload a file to GCS bucket, (2) run a simple bash command, and (3) perform a multi-step pipeline action. The code creates a bucket but does NOT upload any file to it (missing file upload operator), does NOT run any bash command, and while it does run a pipeline operation, it doesn't demonstrate a clear multi-step pipeline workflow as requested. The code only creates a bucket, runs a genomics pipeline referencing a non-existent file, and deletes resources.",0.3333333333333333
960,Fine-tuned (Qwen 2.5 1.5B Airflow),324,0,"While the code does use specific Airflow providers (GCS and SFTP operators), it misuses them entirely. GCSCreateBucketOperator does not have a 'sftp_path' parameter, and GCSDeleteBucketOperator is used incorrectly. The SFTPToGCSOperator is used but with nonsensical paths (gs:// prefix for SFTP source). This demonstrates a fundamental misunderstanding of the operators rather than idiomatic usage.",0,The code explicitly imports and uses test harness boilerplate: 'from tests_common.test_utils.watcher import watcher' and calls 'list(dag.tasks) >> watcher()'. This is internal testing infrastructure that should never appear in production DAG code.,0,"The instruction requests a data asset registration and tagging pipeline for automated metadata management with catalog entries, custom tags, and data governance preparation. The provided code is an SFTP-to-GCS file transfer example that does not implement any metadata management, catalog registration, tagging, or governance features. It completely fails to address the user's requirements.",0.0
961,Fine-tuned (Qwen 2.5 1.5B Airflow),325,0,"The code uses PythonOperator (via @task decorator) with StandardHook instead of using appropriate native operators. For the user's instruction requiring an asynchronous time delta sensor, the code should use TimeDeltaSensor or TimeDeltaSensorAsync from airflow.sensors.time_delta, not BashOperator with 'sleep 5'. The code wraps database queries in Python callables rather than using native operators like PostgresOperator, MySQLOperator, etc.",0,"The code contains documentation markers like '[START howto_standard_hook_kafka]' and '[END howto_standard_hook_kafka]' which are test/documentation boilerplate. Additionally, it references 'StandardHook' which appears to be used incorrectly (StandardHook doesn't have a query method in standard Airflow). The code also has duplicate function definitions (kafka_to_csv defined 4 times) which would cause runtime errors. The ENV_ID variable suggests this is from a system tests template.",0,"The instruction asks for a DAG that simulates a controlled waiting period using an asynchronous time delta sensor to demonstrate time-based synchronization without occupying worker slots. The provided code does not implement this at all - instead it shows various database hook examples with Kafka, S3, Postgres, MySQL, MSSQL, and SQLite connections. There is no TimeDeltaSensor, no asynchronous sensor implementation, and no demonstration of deferrable operators. The code is completely unrelated to the user's request.",0.0
962,Fine-tuned (Qwen 2.5 1.5B Airflow),327,0,"The code uses 'NiuDuExecuteSqlTaskOperator' from 'airflow.providers.alibaba.cloud.operators.niudu', which does not exist in standard Airflow providers. This appears to be a fabricated or non-existent operator. A proper implementation should use legitimate cloud NoSQL database operators such as AWS DynamoDB operators, GCP Bigtable operators, Azure CosmosDB operators, or MongoDB operators from official Airflow providers.",0,"The code contains multiple hallucinations: (1) It imports 'from tests.system.utils import get_test_run' which is internal testing infrastructure, (2) It calls 'test_run = get_test_run(dag)' which is test harness boilerplate, (3) It references an undefined variable 'npc_sql_result' in the task dependency chain, and (4) The operator 'NiuDuExecuteSqlTaskOperator' does not exist in standard Airflow providers. This is not production-ready code.",0,"The instruction asks for a pipeline showcasing 'sequential steps of interacting with a cloud-based NoSQL database, focusing on transaction management and query preparation.' The provided code only shows a single SQL query execution task with no demonstration of: (1) sequential steps/workflow, (2) transaction management concepts, (3) query preparation stages, or (4) proper NoSQL database interaction patterns. Additionally, the SQL syntax used ('select id, name, value from kadmind static_data') appears to be relational database syntax rather than NoSQL query patterns.",0.0
963,Fine-tuned (Qwen 2.5 1.5B Airflow),328,0,"The code uses SnowflakeOperator which is idiomatic, but the instruction specifically asks for 'integration between Airflow's task decorators and Snowflake's Snowpark session management' and to 'programmatically create a DataFrame'. This code does not use task decorators (@task) at all, nor does it use Snowpark for DataFrame operations. It only uses SQL-based operators. The instruction requires using Snowpark's programmatic DataFrame API, not raw SQL statements.",0,"The code contains several issues: 1) The SnowflakeSensor uses invalid SQL syntax 'SCOPE_TABLE(SYSDATE-3)' which is not a standard Snowflake function. 2) The complex procedural SQL blocks with DECLARE, RECORD, and WHILE loops use syntax that appears to be from other databases (PostgreSQL-like) rather than Snowflake's Snowscript. 3) References to 'sys.schemas', 'sys.tables', and 'sys.fn_getrandblob' are not valid Snowflake system objects. This suggests hallucinated or incorrect SQL that would not execute in production.",0,"The code fails to meet the core requirements: 1) Does not use Airflow task decorators (@task) as explicitly requested. 2) Does not use Snowpark session management or programmatically create DataFrames - it only uses raw SQL. 3) While it includes multiple insert operations, there is no clear validation task that checks data integrity of the newly created table (no COUNT checks, row validation, or data quality assertions). The instruction asked for showcasing task decorators + Snowpark integration with validation, but this is a basic SQL-only implementation.",0.0
964,Fine-tuned (Qwen 2.5 1.5B Airflow),329,1,"The code uses the specific GCSToPrestoOperator from airflow.providers.presto.transfers.gcs_to_presto, which is the idiomatic Airflow approach for transferring data from GCS to Presto. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this exact task.",0,"The code contains test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. This is a clear example of test infrastructure leaking into what should be production-ready code.",0,"The instruction requested a 'scheduled job' but the code uses 'schedule=datetime(2021, 5, 4)' which is a one-time execution date, not a recurring schedule. A proper schedule should use a cron expression or timedelta (e.g., '@daily', '0 0 * * *', or timedelta(days=1)). Additionally, there's a syntax error with mismatched parentheses in the DAG definition (extra closing parenthesis after start_date). The code does demonstrate the single-task data transfer mechanism as requested, but fails on the scheduling requirement and contains a critical syntax error.",0.3333333333333333
965,Fine-tuned (Qwen 2.5 1.5B Airflow),330,0,"The code mixes TaskFlow API (@task decorator) with traditional PythonOperator in an inconsistent manner. Functions are decorated with @task but then wrapped again in PythonOperator (print_time_result and print_time_date_result), which is redundant and non-idiomatic. When using @task decorator, you should call the function directly, not wrap it in PythonOperator. Additionally, PythonOperator is referenced but never imported, causing a NameError.",0,"The code contains documentation-style comments like '[START howto_operator_env_var]' and '[END howto_operator_env_var]' which are test/documentation harness boilerplate typically found in Airflow's example tests. Additionally, PythonOperator is used but never imported, indicating incomplete or hallucinated code that would fail at runtime.",0,"The instruction explicitly requests 'Docker integration' to showcase how different types of tasks can be sequenced and executed. The provided code contains only Python tasks with no Docker integration whatsoever (no DockerOperator, no container execution). The DAG also has no task dependencies defined, failing to demonstrate proper task orchestration/sequencing. The code only shows basic Python functions without any meaningful workflow orchestration or Docker components.",0.0
966,Fine-tuned (Qwen 2.5 1.5B Airflow),334,1,"The code uses native Airflow operators specifically designed for Azure Data Lake Storage tasks. It uses LocalFilesystemToADLSOperator for uploading files and ADLSDeleteOperator for deleting files, which are the appropriate provider-specific operators from airflow.providers.microsoft.azure. This is idiomatic Airflow rather than wrapping logic in PythonOperator with hooks.",0,"The code contains multiple test harness imports and boilerplate that should not be in production code. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', the watcher() task addition, and the test_run execution logic at the bottom. These are internal testing modules that indicate this is test/example code rather than clean production-ready code.",1,"The code fulfills the user instruction requirements: (1) showcases task sequencing through the upload >> delete dependency chain, (2) demonstrates cloud storage interactions using Azure Data Lake Storage operators, (3) programmatically uploads files using LocalFilesystemToADLSOperator, (4) removes files using ADLSDeleteOperator, and (5) implements explicit dependency management with the >> operator. All requested business logic is present.",0.6666666666666666
967,Fine-tuned (Qwen 2.5 1.5B Airflow),335,0,"While the code does import WinRMOperator from airflow.providers.microsoft.winrm.operators.winrm, it never actually uses it. Instead, all the logic is wrapped in @task decorated functions that use WinRMHook directly. This is the anti-pattern of using PythonOperator + Hook instead of the native Operator. The instruction asks for 'distributed command execution workflow' which should leverage WinRMOperator for executing commands, not custom Python functions wrapping the hook.",0,"The code contains multiple hallucinated/non-existent methods on WinRMHook that are not part of the standard Airflow WinRM provider: 'create()', 'delete()', 'setup_firewalld()', 'remove_firewalld()', 'create_temp_file()', and 'batch()'. These methods do not exist in the actual WinRMHook implementation. The code also includes documentation markers like '[START howto_operator_winrm_create_connection]' which are test/documentation boilerplate. Additionally, it imports 'from winrm.protocol import Protocol' but never uses it.",0,"The instruction asks for a workflow that 'leverages WinRM to run system-level commands across remote Windows environments, highlighting inter-task dependencies and parallel processing capabilities.' The generated code fails on multiple fronts: (1) There are no inter-task dependencies defined - tasks are listed sequentially without any dependency operators (>>, <<, or set_upstream/downstream), (2) The parallel processing is only mentioned within a single task's 'batch' method rather than using Airflow's native parallel task execution, (3) The workflow doesn't simulate 'real-world remote system management and monitoring scenarios' - it creates/deletes connections repeatedly in an illogical order and includes nonsensical operations like creating tar files with invalid syntax.",0.0
968,Fine-tuned (Qwen 2.5 1.5B Airflow),336,0,"The code attempts to use TaskFlow API decorators but invents non-existent decorators and methods. '@task.xcom_push', '@task.xcom_pull', '@task.xcom_get' are not real Airflow decorators. The code also references 'xcom.push()' and 'channels.pull()' which are not valid Airflow XCom APIs. Idiomatic Airflow 3.0.1 would use TaskFlow API with return values for implicit XCom push, and 'ti.xcom_pull()' or task dependencies for pulling. The instruction requested both Python AND Bash operators, but no BashOperator is present despite the DAG being named 'example_xcom_bash'.",0,"The code contains multiple hallucinated/non-existent Airflow features: '@task.xcom_push', '@task.xcom_pull', '@task.xcom_get' decorators do not exist in Airflow. The 'xcom.push()' method call without context and 'channels.pull()' are fabricated APIs. The 'task_output.output' attribute and the syntax 'channels' parameter are not part of standard Airflow. Additionally, there are syntax errors (missing quotes in 'return ""My own value', missing colon after function definition in 'push_to_xcom', undefined 'value' variable in 'put_one(put_one(value))').",0,"The instruction explicitly requested 'both Python and Bash operators' to demonstrate XCom communication between different task types. The code only contains Python tasks using the @task decorator and completely omits any BashOperator usage. While it attempts to show XCom patterns, it fails to demonstrate cross-operator-type communication (Python <-> Bash) which was the core requirement. The code also has multiple syntax errors that would prevent it from running at all.",0.0
969,Fine-tuned (Qwen 2.5 1.5B Airflow),338,0,"The code uses GCSToS3Operator for transferring data from GCS, which is incorrect for the stated purpose of moving data from cloud storage (GCS) to Presto. There is no actual data loading into Presto - only a SELECT query is executed. The code should use a proper operator like GCSToBigQueryOperator pattern or a dedicated transfer operator, or at minimum use PrestoOperator with INSERT/COPY statements to load the CSV data into Presto tables. The PGDatabaseHook usage for security group creation is also non-idiomatic and appears to be mixing PostgreSQL operations with Presto operations inappropriately.",0,"The code contains clear hallucinations and test harness boilerplate. It imports 'from tests.system.utils import get_test_run' and calls 'test_run = get_test_run(deferral_mode_activated=True)', which are internal testing utilities that should not appear in production code. Additionally, the PGDatabaseHook with methods like 'create_security_group' and 'run_cmd' with sqlcmd commands appear to be fabricated - these are not standard Airflow Hook methods. The provider 'airflow.providers.pgdatabase.hooks.pgdatabase' does not exist in standard Airflow.",0,"The instruction asks for moving structured CSV data from cloud storage (GCS) into Presto for analysis. The code fails to implement this core requirement. It uses GCSToS3Operator which transfers data between GCS and S3, not into Presto. The PrestoOperator only executes a SELECT query, which assumes data already exists in the table - it does not perform any data loading or migration. There is no CREATE TABLE statement, no COPY/INSERT operation to load CSV data into Presto, and the overall workflow does not accomplish the stated goal of data migration from GCS to Presto.",0.0
970,Fine-tuned (Qwen 2.5 1.5B Airflow),340,0,"The DAG creates custom @task functions that wrap Hooks (create_cluster_hook, create_nodegroup_hook, delete_cluster_hook, delete_nodegroup_hook) and manually calls boto3 clients (get_iam_role_arn, delete_iam_role_arn). This is anti-idiomatic. Airflow provides native EKS operators (EksCreateClusterOperator, EksDeleteClusterOperator, etc.) which are used elsewhere in the DAG, but the code unnecessarily mixes these with PythonOperator-style tasks that duplicate functionality. The Hook-wrapping tasks serve no clear purpose and violate the principle of using native operators.",0,"The code contains multiple hallucinations and test harness artifacts: (1) References to 'AWS Systems Test Environment Resources' and 'IAM simulator script' in comments, (2) Uses os.environ['SYSTEM_TEST_IAM_ROLE_NAME'] suggesting test infrastructure, (3) References undefined 'os' module (not imported), (4) Contains syntax errors (space in variable names like 'iam_role Arn', malformed string literals with extra quotes in CLUSTER_NAME and NODEGROUP_NAME), (5) The instruction asks for Fargate compute but the code uses 'nodegroup' compute type, (6) Invalid parameters like 'resource_group_name' which doesn't exist in EksCreateClusterOperator.",0,"The instruction specifically requests 'provisioning an Amazon EKS cluster with Fargate compute resources' but the code uses compute='nodegroup' and creates a nodegroup (EksCreateNodegroupOperator), which is fundamentally different from Fargate. Fargate is serverless compute, while nodegroups are EC2-based. Additionally, the DAG flow is illogical and broken: the chain() at the end creates a nonsensical dependency graph with duplicate tasks, tasks called as functions instead of being referenced, and incorrect ordering (e.g., delete_eks_nodegroup appears twice, get_iam_role_arn() is called multiple times in wrong places). The cleanup logic is also flawed with custom delete hooks instead of relying on the delete operators already defined.",0.0
971,Fine-tuned (Qwen 2.5 1.5B Airflow),342,0,"The code misuses operators entirely. It uses SSHOperator for WinRM tasks (SSH and WinRM are different protocols), and WinRMTransfer (a file transfer operator) is being repurposed to execute commands by setting remote_command attributes directly. The correct approach would be to use airflow.providers.microsoft.winrm.operators.winrm.WinRMOperator for executing remote commands on Windows servers. The code wraps around wrong operators instead of using the native WinRM operator designed for this purpose.",0,The code explicitly imports and uses test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. This is internal testing infrastructure that should not appear in production DAG code.,0,"The instruction asks for parallel remote command execution across multiple Windows servers with tracking of completion. This code fails on multiple fronts: (1) It only targets a single server (CONN_ID), not multiple servers, (2) Tasks are chained sequentially (>>) rather than executed in parallel, (3) It uses incorrect operators (SSH/WinRMTransfer instead of WinRMOperator), (4) The commands include macOS-specific utilities like 'pbcopy' and 'pbpaste' which don't exist on Windows servers, (5) No proper completion tracking mechanism is demonstrated beyond basic task dependencies.",0.0
972,Fine-tuned (Qwen 2.5 1.5B Airflow),344,0,"While the code does use specific Airflow operators (GCSCreateBucketOperator, KMsTGSinkOperator), it wraps KMS operations in PythonOperator-style @task decorators (create_kms_encryption_spec, delete_encryption_spec) instead of using native KMS operators. This is not idiomatic when native operators exist for these operations.",0,"The code contains clear test harness boilerplate and imports from internal testing modules: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', and 'test_run = get_test_run(dag)'. Additionally, it includes test-specific comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN'. This is not production-ready code.",0,"The code completely fails to address the user instruction. The instruction asks for a Kafka-based event-driven DAG with message processing, sequential numeric data production, sensors for numeric conditions (divisible by 3, 5, or 15), and real-time event listeners. Instead, the provided code implements a Google Cloud KMS to GCS transfer workflow with no Kafka components, no numeric data processing, no sensors for divisibility conditions, and no event-driven message processing whatsoever. This appears to be completely unrelated example code.",0.0
973,Fine-tuned (Qwen 2.5 1.5B Airflow),346,0,"The code uses operators that do not exist in the official Airflow Azure provider. The actual Azure Container Instances provider (airflow.providers.microsoft.azure.operators.container_instances) contains AzureContainerInstancesOperator for running containers, not operators like 'ContainerInstancesCreateClusterOperator', 'ContainerInstancesStartContainerOperator', etc. These appear to be hallucinated operator names that don't match the actual Airflow provider API.",0,"The code contains multiple hallucination issues: (1) It imports non-existent operators from the Azure provider that don't exist in Airflow 2.7.2, (2) It includes test harness boilerplate with imports from 'tests.system.utils.watcher' and 'tests.system.utils', (3) It calls 'get_test_run(dag)' and 'watcher()' which are internal testing utilities, and (4) Uses ENV_ID from SYSTEM_TESTS_ENV_ID which is test infrastructure code. This is clearly copied from internal test examples rather than production code.",0,"The instruction asks for executing ephemeral computational tasks using Azure Container Instances with standardized resource allocation and dynamic naming. While the code attempts dynamic naming (CLUSTER_NAME with ENV_ID), it fundamentally misunderstands Azure Container Instances by treating them as clusters that need to be created/started/stopped/deleted. Azure Container Instances are already ephemeral by design - you simply run a container and it executes. The correct approach would use AzureContainerInstancesOperator to run a container task directly, not create a 'cluster'. The workflow doesn't demonstrate the lightweight, ephemeral nature requested.",0.0
974,Fine-tuned (Qwen 2.5 1.5B Airflow),347,0,"The code uses undefined custom operators like 'TransferGCSToS3Task', 'TransferAzureBlobToS3Task', 'TransferMultipleGCSToS3Task', and 'TransferMultipleGCSToS3TaskAsync' which are not standard Airflow operators. These appear to be hallucinated classes. While the code does use legitimate operators like GCSToS3Operator, S3CreateBucketOperator, and GCSCreateBucketOperator, the presence of these non-existent operators and the misuse of @task decorators to create connections (which is not idiomatic) results in a fail.",0,"The code contains multiple hallucinated elements: (1) Undefined operators 'TransferGCSToS3Task', 'TransferAzureBlobToS3Task', 'TransferMultipleGCSToS3Task', and 'TransferMultipleGCSToS3TaskAsync' that don't exist in Airflow providers. (2) Test-related path 'tests_aihow/test_files/file.txt' suggesting test harness code. (3) Unused import 'from airflow._providers import get_provider' accessing private modules. (4) The code is incomplete (cuts off mid-line). (5) Variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' indicate this is test infrastructure code, not production-ready.",0,"While the code attempts to demonstrate inter-cloud data transfer between GCS and S3 with bucket creation and deletion, it fails to deliver a working solution. The workflow is incomplete (code cuts off), uses non-existent operators, and doesn't properly demonstrate the requested systematic workflow. The connection creation tasks are incorrectly implemented and don't serve a real purpose. The code creates duplicate transfer operations (upload_file and delete_file both use GCSToS3Operator with same parameters), and the overall task chaining is unclear and broken. It does not successfully showcase a complete create-transfer-delete workflow as requested.",0.0
975,Fine-tuned (Qwen 2.5 1.5B Airflow),348,1,"The code uses native Airflow provider operators from airflow.providers.alibaba.cloud.operators.drds (DrdsDbCreateOperator, DrdsDbDeleteOperator, DrdsDbDropTableOperator, DrdsDbModifyParamOperator, DrdsDbRenameTableOperator). It does not wrap logic in PythonOperator + Hook patterns, but instead uses the specific operators designed for Alibaba Cloud DRDS database operations. This is idiomatic Airflow usage.",1,"The code imports only standard Airflow libraries from the official providers package (airflow.providers.alibaba.cloud.operators.drds) and core Airflow modules (airflow.models.dag, airflow.utils.trigger_rule). There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate. The code appears to be production-ready without test artifacts.",0,"The instruction requests a comprehensive data infrastructure pipeline demonstrating: (1) database provisioning, (2) configuration, and (3) data migration strategies. While the code shows database provisioning (creating tables with DrdsDbCreateOperator), it has significant issues: (a) The code is incomplete/truncated at the end, (b) There's logical inconsistency - it attempts to rename tables (create_table_first_table) from tables that don't exist yet (temp_table, Employees, it_team), (c) No actual data migration is demonstrated - only table creation, (d) Configuration management (DrdsDbModifyParamOperator) is imported but never used, (e) No task dependencies are defined to show a systematic workflow, (f) Multiple duplicate table creations without clear purpose. The code does not fulfill the requirement for a systematic, reproducible approach to managing database resources with proper migration strategies.",0.6666666666666666
976,Fine-tuned (Qwen 2.5 1.5B Airflow),350,0,"The code uses GCSToDriveOperator which is idiomatic, but the create_connection_to_drive() task is problematic. It creates a Connection object and assigns it to a hook's private _conn attribute within a @task decorator, which is not the proper way to create connections in Airflow. Connections should be created via the Airflow UI, CLI, or environment variables, not programmatically in tasks by manipulating private attributes. The proper idiomatic approach would be to pre-configure the connection or use Airflow's connection management features, not wrap connection manipulation in a PythonOperator-style task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are testing utilities that should not appear in production DAG code. Additionally, comments like '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' indicate this is test code, not production-ready code.",0,"The instruction asks to 'upload files to a Google Cloud Storage bucket' but the code never actually uploads any files to GCS. It creates a bucket (GCSCreateBucketOperator) but there's no task that uploads FILE_NAME/OBJECT_NAME to the bucket before attempting to transfer it to Drive. The GCSToDriveOperator tasks reference OBJECT_NAME and BUCKET_NAME, but the object doesn't exist in the bucket. A complete implementation would need a task using GCSHook or a file upload operator to actually put files in the bucket first. The workflow is incomplete and would fail at runtime.",0.0
977,Fine-tuned (Qwen 2.5 1.5B Airflow),351,1,"The code uses the DrillOperator from airflow.providers.apache.drill.operators.drill, which is the idiomatic and correct way to interact with Apache Drill in Airflow. It does not wrap logic in a PythonOperator with hooks, but instead uses the native operator designed for this purpose.",0,"The code contains clear test harness boilerplate that should not be in production code. Specifically, it imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities. This is a hallmark of leaked test code that should be removed for production use.",0,"The instruction requires transforming raw JSON employee data into Parquet format with specific data conversion and table management operations. However, the provided code only executes a generic shell script ('example_drill.sh') without any visible logic for: (1) reading JSON employee data, (2) converting to Parquet format, (3) table management operations, or (4) any data engineering pipeline structure. The SQL command 'run -f example_drill.sh -c local' is too generic and doesn't demonstrate the required transformation logic. A proper implementation would show explicit SQL queries or commands that handle JSON-to-Parquet conversion.",0.3333333333333333
978,Fine-tuned (Qwen 2.5 1.5B Airflow),352,1,"The code uses the native AzureCosmosInsertDocumentOperator from airflow.providers.microsoft.azure.operators.cosmos, which is the idiomatic Airflow approach for interacting with CosmosDB. It does not wrap logic in PythonOperator with hooks unnecessarily.",0,"The code contains multiple hallucinations and non-existent imports: (1) 'from airflow.www import AppEngineEnvironment' - this is not a real Airflow module, (2) 'from airflow.www.appengine import request_context' and 'from airflow.www.appengine import db' - these are fabricated imports, (3) References to 'db.Neo4jConnectionTag' which has nothing to do with CosmosDB and appears to be hallucinated, (4) 'import pytest' at the top suggests test harness code, (5) Undefined variables like 'cosmon_insert_document_with_xcom' (typo) and 'sincedb_print_tag_alias', (6) The code is incomplete (cuts off mid-line). This code would not run in production.",0,"The instruction requires two key components: (1) monitoring a CosmosDB collection for a specific document (condition checking), and (2) inserting a new document when the condition is met. The provided code only demonstrates document insertion using AzureCosmosInsertDocumentOperator but completely lacks any monitoring/sensor logic to check for document existence or conditions. There is no use of sensors, deferrable operators, or any conditional logic to monitor the collection before insertion. The workflow does not demonstrate the requested 'document tracking' capability.",0.3333333333333333
979,Fine-tuned (Qwen 2.5 1.5B Airflow),353,0,"The code does not use any native Airflow Vertex AI operators from airflow.providers.google.cloud.operators.vertex_ai (e.g., CreateDatasetOperator, CreateBatchPredictionJobOperator). Instead, it wraps everything in a custom test harness (test_utils.DagTestRunner) with decorator-based task creation (@runner.add_task) and custom utility functions (test_utils.create_dataset, test_utils.create_pipeline_item). This is not idiomatic Airflow - it should use proper Vertex AI operators like CreateDatasetOperator, CreateCustomTrainingJobOperator, and CreateBatchPredictionJobOperator.",0,"The code heavily imports and uses internal testing modules that do not exist in standard Airflow: 'from system.test_utils.utils import get_unique_id', 'from system import test_utils', and 'test_utils.DagTestRunner'. These appear to be internal test harness components. Additionally, there are syntax errors and hallucinated function names like 'enable_data_transfer_between_cloude storage_and_vertex_ai_transfer' (typo in 'cloude storage'). The code also contains Chinese characters in comments ('引入自定义模型'), and the overall structure is a test wrapper rather than a production DAG. This is clearly test boilerplate that has leaked into the generated code.",0,"The instruction requires a complete ML pipeline that: (1) ingests time-series sales data, (2) trains a forecasting model, and (3) generates batch predictions. The provided code only attempts to create a dataset and run batch predictions, but completely omits the critical model training step. There is no CreateCustomTrainingJobOperator or AutoMLForecastingTrainingJobOperator to actually train a forecasting model on the sales data. The code references a model but never creates or trains one. Additionally, the configuration is generic test code rather than sales forecasting-specific logic (e.g., no time-series configuration, no forecasting parameters).",0.0
980,Fine-tuned (Qwen 2.5 1.5B Airflow),354,1,"The DAG uses native Airflow Dataplex operators from airflow.providers.google.cloud.operators.dataplex, including DataplexCreateEntryGroupOperator, DataplexCreateEntryOperator, DataplexCreateLakeOperator, DataplexCreateOrUpdateCatalogOperator, DataplexDeleteCatalogOperator, etc. This is the idiomatic approach rather than wrapping hooks in PythonOperators.",0,"The code imports from 'system.google' and 'system.openlineage.operator' which appear to be internal testing modules (e.g., 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' and 'from system.openlineage.operator import OpenLineageTestOperator'). These are test harness imports that should not appear in production code. Additionally, the code is incomplete (cuts off mid-task definition) and contains undefined variables like 'os' (not imported) and typos like 'WRKLFD_ID' vs 'WRKFLW_ID'.",0,"The instruction asks for a data catalog management workflow demonstrating creating and managing metadata catalog entries, entry types, and aspect types with a full lifecycle from creation to deletion. While the code shows creation operators for lakes, entry groups, and entries, it does not demonstrate aspect types or entry types (which are core Dataplex Catalog concepts). The code is also incomplete - it cuts off during the create_entry task definition and doesn't show the deletion tasks that would complete the lifecycle, despite importing delete operators. The focus on workflows (DataplexCreateWorkflowOperator, etc.) seems tangential to the catalog metadata management instruction.",0.3333333333333333
981,Fine-tuned (Qwen 2.5 1.5B Airflow),355,0,"The code uses non-existent providers ('airflow.providers.extralayer') which are not standard Airflow providers. Additionally, the DAG does not address the user instruction at all - it should retrieve external IP and send email, but instead uses fictional SFTP/ExtraLayer operators. Even if these were real, they don't match the requirement.",0,"The code contains clear hallucinations: (1) imports from 'airflow.providers.extralayer' which does not exist in Airflow, (2) uses 'ExtraLayerHook' and 'SFTPToExtraLayerOperator' which are fabricated operators, and (3) includes test harness boilerplate at the end with 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)', which should not be in production code.",0,"The code completely fails to address the user instruction. The requirement was to create a DAG that retrieves the server's external IP address and sends an email notification. Instead, the code implements an unrelated SFTP to ExtraLayer file transfer workflow. There is no IP retrieval logic and no email notification functionality.",0.0
982,Fine-tuned (Qwen 2.5 1.5B Airflow),357,0,"The code heavily relies on PythonOperator tasks (via @task decorator) wrapping boto3 SDK calls instead of using native Airflow operators. For example, 'create_app_role' and 'delete_app_flow_role' use boto3.client('iam') directly rather than using AWS IAM operators from airflow.providers.amazon.aws. While EMR operators are imported, the actual DAG logic shown consists primarily of TaskFlow API functions making raw AWS SDK calls, which is not idiomatic Airflow.",0,"The code imports 'EmrKMZAddStepsOperator', 'EmrKMZCreateJobFlowOperator', 'EmrKMZModifyJobFlowOperator', and 'EmrKMZTerminateJobFlowOperator' from 'airflow.providers.amazon.aws.operators.emr_kmz'. These operators do not exist in the standard Airflow AWS provider package. The 'emr_kmz' module is not a real Airflow provider module, indicating hallucinated imports. Additionally, the code is incomplete and contains syntax errors (unclosed function, malformed JSON string construction).",0,"The user instruction requests a DAG for 'preparing, training, and managing a text classification dataset using cloud-based machine learning infrastructure' with 'data synchronization, dataset import, model training, and resource cleanup'. The provided code focuses on EMR cluster management and IAM role creation/deletion, with no evidence of text classification, dataset handling, model training logic, or ML-specific workflows. The code does not address the core business requirement of text classification ML pipeline automation.",0.0
983,Fine-tuned (Qwen 2.5 1.5B Airflow),359,0,"The code does not use any OpenAI-specific operators or providers. It incorrectly uses SQLExecuteQueryOperator for what should be OpenAI API calls. There are no actual OpenAI operators like OpenAIEmbeddingOperator or any async/batch processing operators for OpenAI. The code also misuses the Connection object by calling .create() on it, which is not how Airflow connections or operators work.",0,The code contains multiple hallucinations: (1) It imports 'from system.openai import OPENAI_CONN_ID' which is not a standard Airflow library path. (2) It includes test harness code at the bottom: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. (3) The SQL queries contain nonsensical Chinese characters '哪里有？' which appears to be corrupted or hallucinated content. (4) The Connection object is misused with a .create() method that doesn't exist in this context.,0,The instruction asks for an Airflow workflow showcasing asynchronous batch processing using OpenAI's API with dynamic task generation for multiple Pokémon descriptions and deferred execution. The generated code: (1) Does not use OpenAI API at all - it only uses SQL operators. (2) Does not implement any asynchronous or deferred execution patterns. (3) Does not generate Pokémon descriptions using AI. (4) Does not implement dynamic task generation. (5) Does not demonstrate batch processing lifecycle management for OpenAI requests. The code appears to be a confused mix of SQL operations that has nothing to do with OpenAI API integration.,0.0
984,Fine-tuned (Qwen 2.5 1.5B Airflow),360,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator for bucket creation, CloudTranslateTextOperator for translation tasks, and GCSDeleteBucketOperator for cleanup. It does not wrap functionality in PythonOperator + Hook patterns. The operators are imported from the correct provider packages (airflow.providers.google.cloud.*) and used directly.",0,The code includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing modules that should not appear in production-ready DAG code. This is a clear example of test infrastructure leakage.,1,"The DAG fulfills all requirements from the instruction: (1) showcases translation task orchestration using cloud services (CloudTranslateTextOperator), (2) has clear task dependencies (create_bucket >> translate_text_csv >> translate_text_text >> delete_bucket), (3) demonstrates resource management by creating a temporary bucket, performing translations, and cleanly deleting the bucket after processing with TriggerRule.ALL_DONE to ensure cleanup happens regardless of translation task success/failure.",0.6666666666666666
985,Fine-tuned (Qwen 2.5 1.5B Airflow),362,0,"The provided code is not a valid Airflow DAG at all. It only contains a JSON comment block with Azure volume mount configuration. There are no imports, no DAG definition, no operators (neither idiomatic Azure Container Instance operators nor PythonOperators), and no task definitions. A proper implementation should use 'airflow.providers.microsoft.azure.operators.container_instances.AzureContainerInstancesOperator' or similar Azure-specific operators.",0,"While the code doesn't import test harness modules, it fails the 'production-ready' aspect of this criterion. The submission is merely a JSON comment block and does not constitute functional Airflow code. It appears to be a configuration snippet rather than executable DAG code, making it unsuitable for production use.",0,"The instruction explicitly requests a daily automated workflow DAG that demonstrates launching different container configurations with varying registry sources and volume mounting strategies in Azure Container Instances. The provided code completely fails to meet these requirements - there is no DAG definition, no schedule configuration, no container deployment tasks, no registry source variations, and no demonstration of different volume mounting strategies. Only a single JSON comment exists, which appears to be a fragment of volume configuration but is not integrated into any workflow.",0.0
986,Fine-tuned (Qwen 2.5 1.5B Airflow),364,1,"The code correctly uses the DynamoDBValueSensor from airflow.providers.amazon.aws.sensors.dynamodb, which is the idiomatic Airflow approach for monitoring DynamoDB values. Multiple sensor configurations are demonstrated using the native operator rather than wrapping logic in PythonOperator with hooks.",1,The code uses only standard Airflow libraries and AWS provider components. There are no imports from internal testing modules like 'tests_common.test_utils.system_tests' or test harness boilerplate like 'get_test_run(dag)'. All imports are from legitimate Airflow and boto3 packages.,0,"The code fails to implement critical components requested in the instruction. It is missing: (1) Table creation logic - the 'create_table_and_start_sensor' task is referenced but never defined, (2) Actual table deletion task - no cleanup/deletion operator is implemented, (3) The code is incomplete - it cuts off mid-chain statement ('chain(create_t'), and (4) The 'table_exists' task is defined but never used in the DAG workflow. The instruction explicitly requested 'table creation, value checking with different matching strategies, and automatic table deletion' - only the value checking sensors are properly implemented.",0.6666666666666666
987,Fine-tuned (Qwen 2.5 1.5B Airflow),367,0,"The code does not use the appropriate Airflow operators for multi-DAG dependency management. For parent-child DAG relationships with external task dependencies, it should use TriggerDagRunOperator to trigger another DAG and ExternalTaskSensor to monitor task completion in another DAG. Instead, it uses XComArg and references a non-existent 'XComArgsTaskCallback' which is not a standard Airflow operator.",0,"The code contains multiple hallucinations: (1) References 'XComArgsTaskCallback' which is not a real Airflow class, (2) Imports 'from tests.system.utils.watcher import watcher' which is an internal testing module, (3) Uses 'watcher()' and 'list(dags=[child_dag])' which are test harness patterns not appropriate for production code, (4) Has malformed syntax 'list(dags=[child_dag])' which is not valid Python.",0,The instruction explicitly asks for a multi-DAG workflow demonstrating parent-child DAG relationships with external task dependencies between two separate DAGs. The provided code only contains a single DAG (example_xcom_pass) and does not create a second DAG or demonstrate triggering/monitoring between separate DAGs. It focuses on XCom passing within a single DAG rather than inter-DAG dependency management.,0.0
988,Fine-tuned (Qwen 2.5 1.5B Airflow),369,1,"The code uses native Airflow providers and operators from airflow.providers.google.cloud.operators.tasks, including GcpTasksCreateQueueOperator, GcpTasksListTasksOperator, GcpTasksGetTaskOperator, etc. It does not wrap functionality in PythonOperator with hooks, which demonstrates idiomatic Airflow usage.",0,"The code contains clear test harness boilerplate with ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID') and PROJECT_ID = os.environ.get('SYSTEM_TESTS_GCP_PROJECT'), which are typical patterns from Airflow's internal system tests. The variable naming convention with 'SYSTEM_TESTS' prefix and the overall structure suggest this is adapted from Airflow's test suite rather than being production-ready code.",0,"The instruction asks for a workflow that monitors a Google Cloud Tasks queue to ensure all tasks have been processed before proceeding with downstream operations. The provided code does not implement this monitoring logic. It creates queues, gets tasks, lists tasks, and performs various operations, but lacks: (1) a sensor or polling mechanism to monitor queue status, (2) logic to check if all tasks are processed/completed, and (3) a clear checkpoint that blocks downstream operations until the queue is empty or all tasks are complete. The code appears to be a generic example of Cloud Tasks operations rather than a solution to the specific monitoring requirement.",0.3333333333333333
989,Fine-tuned (Qwen 2.5 1.5B Airflow),370,0,"The code does not use the appropriate Airflow operators for inter-DAG dependencies. It should use ExternalTaskSensor and ExternalTaskMarker (or TriggerDagRunOperator) to demonstrate synchronization between parent and child DAGs. Instead, it only defines simple @task decorators with print statements that do not implement any actual external task sensing or signaling mechanism.",0,The code explicitly imports from 'tests.system.utils' (line: 'from tests.system.utils import get_test_run') and includes test harness boilerplate ('test_run = get_test_run(dag)'). These are internal testing modules that should not appear in production-ready DAG code.,0,"The instruction explicitly requests two separate DAGs (parent and child) that demonstrate inter-DAG dependency using external task markers and sensors, where the parent signals completion and the child waits for the parent's specific task. The provided code only contains a single DAG with no ExternalTaskSensor, ExternalTaskMarker, or any mechanism for inter-DAG synchronization. It completely fails to implement the requested workflow pattern.",0.0
990,Fine-tuned (Qwen 2.5 1.5B Airflow),371,0,"This code does not use any Airflow operators at all. It defines a DAG import but never instantiates a DAG or creates any tasks using Vertex AI operators (e.g., CreateAutoMLTrainingJobOperator, CreateCustomTrainingJobOperator, DeployModelOperator). Instead, it contains test functions (test_create_deployment, test_list_deployments) that directly call a custom SERVICE object. This is not idiomatic Airflow - it should use airflow.providers.google.cloud.operators.vertex_ai operators to define tasks within a DAG.",0,"The code contains multiple hallucinations and test harness artifacts: (1) imports from 'system.google' and 'system.test_utils.utils' which are internal testing modules, (2) uses pytest decorators and pytest.mark.skipif, (3) includes test functions like test_create_deployment() and test_list_deployments(), (4) references non-existent modules like 'system.google.cloud.vertex_ai_service', (5) contains malformed code with Chinese characters ('匿名', '定ition') suggesting corrupted generation, (6) uses environ_mode_on_off which is a test utility, and (7) references GoogleCloudSession which is not a standard Airflow class. This is clearly test boilerplate mixed with hallucinated code, not production-ready.",0,"The instruction requested an end-to-end ML workflow DAG for Vertex AI that demonstrates training, evaluation, and deployment across different data types and use cases. This code fails to deliver: (1) no actual DAG is instantiated with tasks, (2) no training pipeline tasks are defined, (3) no evaluation tasks exist, (4) no deployment tasks are created, (5) no workflow orchestration is present, and (6) the code is structured as pytest test functions rather than an Airflow DAG. It completely misses the core requirement of creating a functional Airflow pipeline for ML operations.",0.0
991,Fine-tuned (Qwen 2.5 1.5B Airflow),372,1,"The code correctly uses the native AzureSynapseRunPipelineOperator from airflow.providers.microsoft.azure.operators.synapse, which is the idiomatic way to interact with Azure Synapse in Airflow. It does not wrap functionality in PythonOperator with hooks, but instead uses the purpose-built operator for the task.",0,"The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production DAG code. This is a clear example of test infrastructure leaking into what should be clean, production-ready code.",1,"The DAG fulfills the user instruction to demonstrate Azure Synapse integration with Apache Airflow for enterprise data orchestration through automated pipeline triggering. It implements a simple DAG that connects cloud data processing services (Azure Synapse) with workflow management (Airflow), showing two examples of pipeline execution including one with branch specification. The implementation is appropriate for a demonstration/example workflow.",0.6666666666666666
992,Fine-tuned (Qwen 2.5 1.5B Airflow),375,0,"The code mixes idiomatic Airflow operators (AsanaCreateTaskOperator, AsanaDeleteTaskOperator, etc.) with a non-idiomatic pattern. Specifically, the `delete_task` function is a Python callable that directly uses `asana_client` (which is undefined and would typically be a Hook), wrapped in a @task decorator. This is the anti-pattern of using PythonOperator + Hook logic instead of the native AsanaDeleteTaskOperator that's already available and used elsewhere in the DAG. The code should consistently use the Asana operators throughout.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 1) `from tests_common.test_utils.watcher import watcher` and the line `list(dag.tasks) >> watcher()`, 2) `from tests_common.test_utils.system_tests import get_test_run` and `test_run = get_test_run(dag)`. These are testing utilities that should not appear in production DAG code. Additionally, the code references undefined variables like `project_id` and `asana_client` without importing or defining them.",1,"The DAG successfully demonstrates the full lifecycle of Asana task management as requested: creating tasks (AsanaCreateTaskOperator), finding tasks (AsanaFindTaskOperator), updating tasks (AsanaUpdateTaskOperator), and deleting tasks (AsanaDeleteTaskOperator and custom delete function). The operations are arranged in a sequential, automated workflow that showcases all four CRUD operations requested in the instruction.",0.3333333333333333
993,Fine-tuned (Qwen 2.5 1.5B Airflow),377,0,"While the code does use native Airflow DLP operators (CloudDLPCreateStoredInfoTypeOperator, CloudDLPDeleteStoredInfoTypeOperator, etc.), it misuses ExternalTaskTrigger in place of CloudDLPExecuteDLPJobOperator for executing DLP jobs. ExternalTaskTrigger is designed to wait for external DAG tasks, not to execute DLP operations. The correct idiomatic approach would be to use CloudDLPExecuteDLPJobOperator (which is imported but never used) to execute DLP jobs for de-identification.",0,"The code imports 'from system.openlineage.operator import OpenLineageTestOperator' which appears to be an internal testing module, not a standard Airflow library. Additionally, the code is incomplete (truncated at 'trig' in the last line), and uses test-specific environment variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', indicating this is test harness boilerplate rather than production-ready code.",0,"The instruction specifically requests a workflow to 'de-identify sensitive phone numbers in tabular data' with capabilities to 'mask, template, and optionally reverse sensitive information transformations.' The provided code focuses on creating/deleting stored info types and getting DLP jobs, but does not demonstrate actual de-identification of phone numbers, masking transformations, or reversible transformations. It lacks the core business logic for phone number de-identification, inspection configuration for phone numbers, and de-identification templates with masking/crypto-based transformations that would fulfill the instruction.",0.0
994,Fine-tuned (Qwen 2.5 1.5B Airflow),379,0,"The code violates idiomatic Airflow patterns in multiple ways. The 'create_content_queue' task uses S3Hook().conn.create_queue() which is incorrect (S3Hook should not be used for SQS operations). It also instantiates operators (S3KeySensor, BashOperator) inside a task function rather than at the DAG level, which is not how Airflow operators should be used. The proper approach would be to use SQSCreateQueueOperator and define sensors/operators at the DAG level with proper task dependencies.",0,"The code imports 'from system.amazon.aws.utils import SystemTestContextBuilder' which is clearly a test harness module, not a standard Airflow library. It also uses 'sys_test_context_task = SystemTestContextBuilder().build()' which is test boilerplate. The DAG is named 'sqs_with_s3_in_one_system_test' indicating it's a system test rather than production code. Additionally, there's a reference to 'SqsSensor' at the end which is not imported and the code is incomplete (truncated at '[END howto_sen').",0,"The instruction asks for a file transfer lifecycle DAG with: dynamic environment setup, S3 bucket creation, local file generation, file upload, and resource cleanup. While the code includes S3 bucket creation (S3CreateBucketOperator), it fails to demonstrate the complete lifecycle. There's no clear local file generation task, no file upload to S3 operation, and no systematic cleanup (S3DeleteBucketOperator is imported but never used). The code is heavily focused on SQS queue operations rather than file transfer, and the logic is incomplete and broken. The 'save_response_to_file' task saves locally but doesn't upload to S3.",0.0
995,Fine-tuned (Qwen 2.5 1.5B Airflow),380,0,"The code does not use standard Airflow Vertex AI operators from airflow.providers.google.cloud.operators.vertex_ai (e.g., CreateDatasetOperator, CreateTrainingPipelineOperator, DeployModelOperator). Instead, it relies on custom wrapper functions like 'third_party_config.create_dag_for_dataset_operator' and 'third_party_config.create_dag_for_pipeline_operator', which appear to be non-standard abstractions that likely wrap logic in PythonOperators or custom implementations rather than using native Vertex AI operators.",0,"The code imports from 'system.google' modules ('from system.google import third_party_config' and 'from system.google import workspace_config'), which appear to be internal testing or system test utilities rather than standard Airflow libraries. This is a clear hallucination/leakage of test infrastructure code. Additionally, the code references 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables, and uses 'airflow-system-tests-resources' as a bucket name, all indicating test harness boilerplate. The code is also incomplete (truncated at the end with 't') and contains syntax errors with unusual characters in parameter names (e.g., 'additionalＶＰＳ', 'optionalūｄ').",0,"The instruction requests a housing price prediction model pipeline, but the code implements an image classification pipeline for multi-label classification. This is a complete mismatch with the business requirement. Additionally, while the instruction asks for dataset creation, model training, versioning, and deployment, the provided code only shows dataset creation and training pipeline setup without clear model deployment, versioning logic, or housing-specific data handling. The code also lacks robust error handling mechanisms as requested.",0.0
996,Fine-tuned (Qwen 2.5 1.5B Airflow),381,0,"The code uses AzureAdvisorListPatchesOperator, which is not the appropriate operator for remote task scheduling and execution on Azure VMs. The instruction requires demonstrating cloud-based computational job execution with specific VM specifications and command line tasks. The correct approach would be to use operators like AzureBatchOperator or AzureContainerInstancesOperator that allow defining VM specifications and executing computational workloads, not an operator designed for listing Azure Advisor patches.",0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_run' and 'test_run = get_test_run(dag)'. These are internal testing utilities that should not be present in production-ready DAG code.,0,"The DAG completely fails to meet the instruction requirements. The instruction asks for: (1) remote task scheduling and execution using Azure cloud infrastructure, (2) configuration of a computational job with specific VM specifications, and (3) a basic command line task execution. Instead, the code uses AzureAdvisorListPatchesOperator which is for listing Azure Advisor recommendations/patches, not for executing computational workloads. There are no VM specifications, no command line tasks, and no demonstration of remote computational job execution.",0.0
997,Fine-tuned (Qwen 2.5 1.5B Airflow),382,1,"The DAG uses native Airflow operators from the Microsoft Azure provider package (PowerBISynchronizeDatasetsOperator, PowerBiToPowerBiTransferOperator, PowerBIDeleteDatasetOperator, PowerBIRenameDatasetOperator) rather than wrapping logic in PythonOperators with hooks. The only PythonOperator-like usage is the @task decorator for get_workspaceid(), which is appropriate for simple workspace ID retrieval and follows modern Airflow patterns.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests_common.test_utils.watcher import watcher', 'from tests_common.test_utils.system_tests import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and should not be present in a production DAG.",0,"The instruction asks for a DAG that 'manages Power BI workspace and dataset operations, focusing on retrieving workspace information and refreshing datasets automatically' with the ability to 'scan workspace modifications, track dataset refresh status, and handle administrative Power BI tasks'. While the DAG retrieves workspace information and includes administrative tasks (delete, rename, sync), it does NOT include any dataset refresh operations or refresh status tracking, which were explicitly mentioned as key requirements. The DAG focuses on synchronization and transfer operations but misses the core 'refresh datasets automatically' and 'track dataset refresh status' functionality.",0.3333333333333333
998,Fine-tuned (Qwen 2.5 1.5B Airflow),383,0,"The code does not use the idiomatic Airflow approach for Papermill integration. Airflow has a dedicated PapermillOperator in airflow.providers.papermill.operators.papermill that should be used. Instead, the code attempts to create a custom implementation using a @task decorator with non-functional IPython shell code (InteractiveShell.instance(), get_active_file(), etc.) which is not how Papermill should be invoked in Airflow. The proper approach would be: `from airflow.providers.papermill.operators.papermill import PapermillOperator`.",0,"The code contains multiple hallucinations and non-existent patterns: 1) The import statement 'from airflow.sdk import DAG, task' with a fallback to 'from airflowSDK import DAG' is fabricated - the correct import for Airflow 3.0.0 is 'from airflow import DAG' and 'from airflow.decorators import task'. 2) The run_notebook function uses completely fabricated IPython methods like 'InteractiveShell.instance().get_active_file()', 'get_logger().INFO.wait_for_output()' which don't exist. 3) The variable 'Path_TO_DATA' is referenced but undefined (should be 'PATH_TO_DATA'). 4) The docstring mentions test scenarios that suggest this is test harness code. The code is not production-ready.",0,"The instruction requires: 1) Use Papermill to execute a Jupyter notebook, 2) Pass date-based parameters dynamically, 3) Generate a new output notebook for each run, 4) Verify execution by checking a specific message. The code fails on all counts: 1) Doesn't properly use Papermill (no actual papermill.execute_notebook call), 2) No date-based parameters are passed to any notebook, 3) No output notebook path is specified or generated, 4) No verification of execution messages is implemented. The code also includes unrelated BashOperators that kill processes, which has nothing to do with the instruction. The generate_data task is disconnected from the notebook execution logic.",0.0
999,Fine-tuned (Qwen 2.5 1.5B Airflow),384,1,"The code uses native AWS SageMaker operators (SageMakerCreateNotebookInstanceOperator, SageMakerStartNoteBookOperator, SageMakerStopNoteBookOperator, SageMakerDeleteNotebookInstanceOperator) and sensors (SageMakerNotebookSensor) from airflow.providers.amazon.aws, which are the idiomatic Airflow way to interact with SageMaker. It does not wrap these in PythonOperators unnecessarily.",0,"The code contains clear hallucinations and test harness imports. Specifically: (1) It imports 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' which is an internal testing module, (2) It uses 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()' which is test boilerplate, (3) Several operators/sensors appear to be fabricated or incorrectly named (e.g., 'SageMakerNotebookStarterSensor', 'SageMakerNoteBookSensorDone', 'SageMakerNoteBookStarterSensorDone' are not standard Airflow providers), (4) The code includes irrelevant variables and logic (INSTANCES_LIST, NO_DMI, GPU_FRAMWORK, get_ami_id function with EKS AMI logic) that don't relate to SageMaker notebooks, and (5) The configure_network function is incomplete and contains nonsensical comments about ACM and S3 bucket creation.",0,"While the code attempts to demonstrate sequential management of a SageMaker notebook instance (create, start, stop, delete), it fails to provide a complete, working implementation. The DAG structure using 'chain' is not shown, the code is truncated (configure_network function is incomplete), and it includes extraneous, confusing elements (AMI lookup, network configuration, GPU framework flags) that are not relevant to the core instruction of demonstrating notebook instance lifecycle management. A proper implementation should show a clear, complete DAG with the four operational states clearly defined and chained together.",0.3333333333333333
1000,Fine-tuned (Qwen 2.5 1.5B Airflow),385,0,"The code imports many Compute Engine operators but never actually uses them to define tasks in the DAG. Instead, it defines multiple Python helper functions (_get_list_of_instances, _insert_instance, _run_shell_command_on_instance, etc.) that appear to be preparing for wrapping logic in PythonOperators rather than using the native operators directly. The DAG structure itself is incomplete - there are no actual task definitions using the imported operators. This is a clear anti-pattern as it suggests the intent to use PythonOperator wrappers instead of the idiomatic approach of directly instantiating the imported operators like ComputeEngineInsertInstanceOperator, ComputeEngineRunShellCommandOnInstanceOperator, and ComputeEngineDeleteInstanceOperator.",0,"The code contains several hallucinated/non-existent operators and patterns. Examples include: (1) ComputeEngineFromTemplateInstanceOperator, ComputeEngineInsertInstanceFromTemplateOperator, ComputeEngineInsertInstanceIntoFolderOperator, ComputeEngineInsertInstanceWithListInstancesOptionOperator, ComputeEngineInsertInstanceWithPrimaryDiskOperator, ComputeEngineInsertInstanceIntoLocationOperator, ComputeEngineInsertInstanceIntoZoneOperator - many of these are not real Airflow operators in version 3.0.6. (2) The code references undefined objects like 'compute.SurgeRebalanceRequest' and 'compute.StackBuilder()' which are not part of standard Airflow libraries. (3) Syntax errors exist such as '*.' instead of '**' in function signatures. (4) The code is incomplete and appears to be cut off mid-definition.",0,"The instruction requires a complete workflow that: (1) creates a VM instance, (2) runs parallel SSH commands with different authentication techniques, and (3) cleans up infrastructure. The provided code fails to deliver on all counts: (1) No DAG structure is defined - no 'with DAG()' block or task definitions exist, (2) No parallel task execution is demonstrated, (3) No SSH commands are actually executed using operators, (4) No cleanup/deletion tasks are defined, (5) The code is incomplete and cuts off mid-function. While relevant operators are imported, they are never instantiated or used to create the requested workflow.",0.0
1001,Fine-tuned (Qwen 2.5 1.5B Airflow),386,0,"The code fails to use idiomatic Airflow patterns. It does not use the CassandraSensor or CassandraTableSensor from airflow.providers.apache.cassandra.sensors, which are the proper operators for monitoring Cassandra data availability. Instead, it attempts to create custom sensor logic using @task decorators and non-existent constructs like 'SensorTask', 'HaveData', and 'TaskToucher'. The code wraps logic in Python callables rather than using native Cassandra operators and sensors designed for this purpose.",0,"The code contains multiple hallucinations and non-existent Airflow constructs: 1) 'SystemTestContext' is a test harness wrapper that should not appear in production code, 2) 'SensorTask', 'HaveData', and 'TaskToucher' are not real Airflow classes, 3) '@task(CassandraConnection)' is invalid syntax, 4) References to undefined functions like 'execute_command', 'cursor_to_dict', and 'executor.execute_query', 5) Bizarre shell commands referencing 'ifname' and 'OPENSEARCH_ADMIN_PASSWORD' which are unrelated to Cassandra, 6) The code structure suggests it's copied from test examples with START/END comment markers typical of documentation examples.",0,"The code does not fulfill the instruction to create a sensor-based workflow for monitoring Cassandra keyspace and table data availability. While it attempts to create some sensor-like logic, the implementation is completely broken and non-functional. It does not properly: 1) Use Cassandra sensors to check data availability, 2) Implement record-level condition validation in a working manner, 3) Create a gatekeeper workflow that blocks downstream tasks until data quality is verified, 4) Provide a production-ready pipeline. The code is a mix of pseudo-code, test harness elements, and hallucinated constructs that would not execute.",0.0
1002,Fine-tuned (Qwen 2.5 1.5B Airflow),387,0,"While the DAG uses some appropriate operators like GCSToGoogleDriveOperator, it misuses GCSToGCSOperator for operations that don't match its purpose. Specifically, 'move_file_from_google_drive_to_gcs' uses GCSToGCSOperator with drive_id and object_name parameters that don't exist in that operator - this should use a GoogleDriveToGCSOperator or similar. The 'delete_files' tasks also misuse GCSToGCSOperator when they should use GCSDeleteObjectsOperator. The code imports hooks (GCSHook, GoogleDriveHook) but never uses them, suggesting confusion about when to use operators vs hooks.",0,"The code contains clear hallucinations. The GCSToGCSOperator does not have parameters like 'drive_id', 'object_name', and 'bucket_name' as used in 'move_file_from_google_drive_to_gcs'. The code also references 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are internal testing patterns. Additionally, the DAG is incomplete (cuts off at 'gcp_c'), and the task dependencies are not defined, making this non-production-ready code.",0,"The instruction asks for bidirectional file transfer between Google Drive and GCS to demonstrate cross-platform synchronization. While the DAG attempts to show GCS-to-Drive transfers using GCSToGoogleDriveOperator, it fails to properly implement Drive-to-GCS transfers (the 'move_file_from_google_drive_to_gcs' task incorrectly uses GCSToGCSOperator). The workflow lacks clear demonstration of synchronization capabilities, has no task dependencies defined to show the flow, and the code is incomplete. It does not fulfill the requirement to 'illustrate how enterprise data can be seamlessly migrated between different cloud storage platforms.'",0.0
1003,Fine-tuned (Qwen 2.5 1.5B Airflow),388,0,"The code uses TaskFlow API (@task decorator) with QuickSightHook inside custom Python functions for create_dataset, create_visualization, delete_visualizations, and delete_datasets. While QuickSightCreateIngestionOperator is used correctly, the core dataset and visualization creation logic wraps Hooks in PythonOperators instead of using native QuickSight operators (if available). This is not idiomatic Airflow - it should leverage specific operators rather than wrapping hook logic in @task decorated functions.",0,"The code explicitly imports 'from tests.system.providers.amazon.aws.utils import SystemTestContextBuilder' and uses 'sys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()'. This is internal testing infrastructure and should not appear in production DAG code. Additionally, the code references a non-existent operator 'QuickSightCreateVisualisationOperator' at the end (incomplete/cut off), and the create_visualization and create_dataset functions reference hook methods that may not exist in the standard QuickSightHook API.",0,"The instruction requests an end-to-end ETL workflow that transforms raw project metadata from S3 into QuickSight-ready datasets with automated provisioning and cleanup. The code is incomplete (cuts off mid-operator definition), missing critical ETL steps like extracting/transforming data from S3, lacks proper data upload to S3 (S3CreateObjectOperator is imported but never used), and the cleanup tasks (delete_visualizations, delete_datasets, S3DeleteBucketOperator) are defined but not chained into the DAG workflow. The systematic ETL approach is not fully implemented.",0.0
1004,Fine-tuned (Qwen 2.5 1.5B Airflow),389,0,"The DAG violates idiomatic Airflow patterns by instantiating operators inside a @task decorated function without executing them or establishing proper task dependencies. In the 'create_queue' task, operators like ASBQueueCreateOperator, ASBQueuePopOperator, etc. are instantiated but never added to the DAG graph. These operators should be defined at the DAG level, not inside a Python function. Additionally, the code attempts to programmatically create a Connection object and call conn.create() which is not the standard way to manage connections in Airflow.",0,"The code contains multiple hallucinated/non-existent operators and methods. 'ASBQueuePublishOperator' is used but doesn't exist in the standard Airflow Azure provider (the correct operator would be ASBQueuePushOperator). 'ASBQueueMappingOperator' appears to be completely fabricated and doesn't exist in Airflow's Azure Service Bus provider. The 'ASBQueueGetMessageOperator' is also not a standard operator. The Connection.create() method is not a valid Airflow API. The code is also truncated at the end with an incomplete import statement 'from airflow.providers.mi', indicating incomplete or corrupted generation.",0,"The instruction requested a complete lifecycle demonstration of Azure Service Bus communication including queue AND topic management, message transmission strategies, and subscription handling. The code only addresses queue operations and completely omits topics and subscriptions, which are critical components of Azure Service Bus. Additionally, the DAG structure is broken with operators instantiated but not properly connected in a workflow, failing to demonstrate a coherent messaging lifecycle. The code does not illustrate message transmission strategies or subscription handling as requested.",0.0
1005,Fine-tuned (Qwen 2.5 1.5B Airflow),390,0,"The code uses a @task decorator to wrap logic that attempts to instantiate an operator (AzureTimeseriesItemsUpsertOperator) inside a Python function, rather than using the operator directly in the DAG context. This is not idiomatic Airflow - operators should be instantiated directly in the DAG, not created inside task functions. Additionally, the code references non-existent functions like 'azure_timeseries_bucket_iterable' and uses undefined modules like 'openlineage.operator.OperatorConnection', suggesting improper operator usage patterns.",0,"The code explicitly imports and uses testing modules that should not be in production code: 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run'. It also includes test harness boilerplate like 'test_run = get_test_run(dag)' and the watcher() pattern in the __main__ block. Additionally, it references non-existent functions (azure_timeseries_bucket_iterable) and modules (openlineage.operator.OperatorConnection) that are not part of standard Airflow libraries.",0,"The instruction requires building a time series forecasting system that: (1) processes retail sales data with geographic attributes (city, zip code, county), (2) transforms raw sales data, and (3) generates predictive insights with a 30-day forecast horizon. The provided code does none of this. It only attempts to upsert generic timeseries items to Azure without any data ingestion, transformation, geographic attribute handling, forecasting logic, or 30-day prediction capability. There is no retail sales data processing, no feature engineering, and no actual forecasting model or pipeline.",0.0
1006,Fine-tuned (Qwen 2.5 1.5B Airflow),391,0,"The DAG fails idiomatic Airflow practices by using @task decorated functions with boto3 clients directly (delete_eks_cluster and delete_node_group) instead of using the native EksDeleteClusterOperator and EksDeleteNodegroupOperator that are already imported but not used. The code imports these operators at the top but then wraps boto3 logic in PythonOperators (via @task decorator), which is the anti-pattern described in the scoring criteria.",0,"The code contains several hallucinated/invalid parameters that don't exist in the actual Airflow EKS operators: 1) EksCreateClusterOperator does not have 'vpc_clusters_config', 'resources_vpc_config', 'compute', 'namespace', 'security_groups', or 'resources' parameters. 2) EksCreateNodegroupOperator does not have 'nodegroup_pvc_name' parameter. 3) EksPodOperator does not have a 'startup' parameter. 4) There's a malformed string in EXECUTION_ROLE_ARN with line breaks. 5) Invalid JSON syntax in resources array (missing closing quote). These are not standard Airflow 2.9.3 parameters and represent hallucinated API surfaces.",0,"While the DAG attempts to show cluster creation, validation, and pod deployment, it has critical logical flaws: 1) The task chain is broken and illogical - 'run_example_pod' appears twice in the chain, and delete_node_group comes before the first run_example_pod but also appears in the middle of the chain. 2) There's no validation step as requested in the instruction. 3) The workflow doesn't demonstrate a proper lifecycle - the dependencies are contradictory (delete_node_group >> run_example_pod conflicts with the chain that has create_node_group >> run_example_pod >> delete_node_group). 4) Missing proper cleanup orchestration. The DAG does not fulfill the 'robust workflow' requirement with proper task chaining.",0.0
1007,Fine-tuned (Qwen 2.5 1.5B Airflow),392,1,"The code uses native Airflow providers and operators (GoogleSheetsSpreadsheetStateSensor, GoogleSheetsToGCSOperator) from airflow.providers.google packages rather than wrapping logic in PythonOperators with hooks. This is idiomatic Airflow usage.",0,The code explicitly imports testing modules and includes test harness boilerplate at the end: 'from tests_common.test_utils.system_tests import get_test_run' and 'test_run = get_test_run(dag)'. This is not production-ready code and represents test infrastructure leakage.,0,"The instruction requires a multilingual document translation pipeline that handles XLSX, DOCX, and PDF files using cloud translation services with language conversion capabilities. The provided code only transfers Google Sheets data to GCS without any translation functionality, document format handling (DOCX, PDF), or language conversion logic. It completely misses the core requirement of translation and multi-format document processing.",0.3333333333333333
1008,Fine-tuned (Qwen 2.5 1.5B Airflow),393,0,"While the code does use some native Airflow operators (RDSCreateDbInstanceOperator, S3CreateBucketOperator), it also wraps basic operations in @task decorated functions with direct boto3 calls (get_s3_bucket_location, delete_db_instance, delete_event_subscription, delete_serverless_db_instance). These should use native operators or sensors where available. For example, delete_db_instance uses RDSHook but wraps it in a PythonOperator pattern instead of using RDSDeleteDbInstanceOperator which is already imported but not used properly.",0,"The code explicitly imports and uses testing infrastructure that should not be in production code: 'from tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder' and 'sys_test_context_task = SystemTestContextBuilder().add_variable(KMS_KEY_ID_KEY).build()'. This is test harness boilerplate that indicates the code was copied from Airflow's system tests rather than being production-ready.",0,"The instruction asks for orchestrating cloud infrastructure provisioning and data migration tasks including: (1) create security groups, (2) configure database endpoints, and (3) set up a replication instance for seamless data transfer. The code focuses on RDS serverless instances and event subscriptions but does not create security groups (sg_name is defined but never used), does not set up a replication instance (which would require AWS DMS operators), and does not implement actual data migration tasks. The code is incomplete (cuts off mid-task definition) and misses key requirements.",0.0
1009,Fine-tuned (Qwen 2.5 1.5B Airflow),394,1,"The code correctly uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocUpdateClusterOperator, DataprocDeleteClusterOperator) from the Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which demonstrates proper idiomatic Airflow usage.",0,"The code imports 'from system.google import DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID' which is clearly a testing module import. This is test harness boilerplate that should not appear in production code. Additionally, the code references 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT' environment variables which are testing artifacts.",0,"The code has multiple critical issues that prevent it from fulfilling the instruction: 1) The DAG is incomplete - it ends abruptly with 'dele' suggesting truncation. 2) There are TWO DAG definitions with the same DAG_ID which would cause a conflict. 3) The second DAG definition is empty and appears to be documentation boilerplate. 4) Task dependencies are not properly defined for all tasks (create_cluster_second and delete_cluster_second have no dependencies). 5) The DAG constructor includes invalid parameters ('region' and 'create_cluster' are not valid DAG parameters). While it attempts to show cluster lifecycle management, the broken structure fails to deliver a working, complete orchestration example.",0.3333333333333333
1010,Fine-tuned (Qwen 2.5 1.5B Airflow),395,0,"The code does not use any Airflow operators or DAG structure at all. It only contains Python functions with no DAG definition, no tasks, and no use of Airflow providers. For a Kafka pipeline, it should use operators like `airflow.providers.google.cloud.operators.cloud_managed_kafka` or similar Kafka-specific operators, but instead only defines generic Python functions.",0,"The code contains multiple hallucinations and errors: (1) Imports non-existent classes like 'CryptoKeyIdeningRequest' (typo/fabricated), (2) References test context patterns like `kwargs['test_context']` which suggests test harness boilerplate, (3) The code is about KMS (Key Management Service) and cryptographic operations, not Kafka at all, (4) Contains fabricated API methods and patterns that don't exist in standard Google Cloud libraries, (5) Code is incomplete and truncated.",0,"The instruction explicitly asks for a Kafka cluster management and message processing pipeline with creating clusters, producing messages, and consuming messages with consumer groups. The provided code is completely unrelated - it deals with KMS cryptographic keys and signing requests instead of Kafka. There is no DAG structure, no Kafka cluster creation, no message production, and no message consumption. The code fails to address any aspect of the user instruction.",0.0
1011,Fine-tuned (Qwen 2.5 1.5B Airflow),396,0,"While the code attempts to use native operators like AzureContainerToGCSTransferOperator, it misuses them entirely. The operator is used for tasks that don't match its purpose (e.g., 'blob_to_local' and 'get_check_status' appear to be downloading from Azure to local, not transferring to GCS). Additionally, the create_connection task uses a @task decorator with non-idiomatic patterns like manually manipulating DAG objects and XCom in ways that are not standard. The code also uses AzureAdvisorCheckOperator which is completely unrelated to file existence checking in Azure Blob Storage.",0,"The code contains multiple hallucinations: 1) AzureAdvisorCheckOperator does not exist in the standard Airflow providers - Azure Advisor is for Azure recommendations, not file checking. 2) The import 'from airflow.providers.microsoft.azure.hooksadvisor import AdvisorHook' is malformed and non-existent. 3) The code includes test harness patterns like ENV_ID from system tests, LOCAL_PATH references to test resources, and START/END comment markers typical of documentation examples. 4) The create_connection function contains nonsensical code like 'DAG.getDAG(""test-advisor"")' which is not valid Airflow API usage.",0,"The code completely fails to implement the requested functionality. The instruction asks for: 1) Check for file existence in Azure container, 2) Transfer that file to GCS. Instead, the code: uses a non-existent AzureAdvisorCheckOperator (unrelated to file existence), creates connections in a non-standard way, and misuses the transfer operator multiple times without proper configuration for the actual Azure-to-GCS transfer. There is no proper file existence check using Azure Blob Storage sensors or operators, and the transfer logic is confused and incorrect.",0.0
1012,Fine-tuned (Qwen 2.5 1.5B Airflow),397,0,The code uses PgDBHook wrapped in a @task decorator (essentially a PythonOperator) instead of using the native PostgresOperator from airflow.providers.postgres.operators.postgres. The idiomatic approach would be to use PostgresOperator for executing SQL statements against PostgreSQL databases.,0,The code includes test harness boilerplate at the end: 'from tests.system.utils import get_test_env_id' and 'test_env_id = get_test_env_id()'. These are internal testing modules that should not appear in production DAG code.,0,"The instruction asks for a DAG that showcases task dependencies and sequential PostgreSQL operations with clear dependency management. The generated code only has a single task that executes 'select 1', with no chaining, no sequential operations, and no demonstration of dependency management between multiple database tasks. It fails to illustrate the requested concepts.",0.0
1013,Fine-tuned (Qwen 2.5 1.5B Airflow),398,1,"The code uses native Airflow operators appropriately: GCSCreateBucketOperator, GCSDeleteBucketOperator, MsSqlToGcsOperator, and GcsToMsSqlOperator from the Google Cloud provider. These are the correct, idiomatic operators for the tasks being performed rather than wrapping hooks in PythonOperators.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically: 'from tests.system.utils.watcher import watcher', 'from tests.system.utils import get_test_run', 'test_run = get_test_run(dag)', and 'list(dag.tasks) | watcher()'. These are testing utilities that should not appear in production DAG code. Additionally, comments like '# TEST SETUP', '# TEST BODY', '# TEST TEARDOWN' indicate this is test code, not production-ready code.",0,"The instruction asks for an end-to-end data governance pipeline that tracks metadata management with create, get, list, and delete operations for metadata entries. However, the provided code is a data transfer pipeline between MS SQL and GCS (Google Cloud Storage). It creates/deletes storage buckets and transfers data, but does not implement metadata management operations (create metadata, get metadata, list metadata, delete metadata). The DAG does not address the core requirement of metadata governance and tracking.",0.3333333333333333
1014,Fine-tuned (Qwen 2.5 1.5B Airflow),400,0,"The code uses non-existent operators like 'LookerProcedureRunOperator' and 'LookerQueryRunOperator' which are not real Airflow providers. Additionally, it wraps Looker logic in Python functions (looker() and looker_query()) that appear to be intended for PythonOperator usage, rather than using actual Looker provider operators. The correct approach would be to use airflow.providers.google.cloud.operators.looker (e.g., LookerStartPdtBuildOperator) for PDT builds as mentioned in the instruction.",0,"The code contains multiple hallucinations: (1) Imports 'from tests.system.utils.watcher import watcher' which is a test utility, (2) Uses non-existent operators 'MSSQLToLookerOperator', 'LocalMSSQLTransferOperator', 'LookerProcedureRunOperator', and 'LookerQueryRunOperator' that don't exist in standard Airflow libraries, (3) Includes test-related variables like 'SYSTEM_TESTS_ENV_ID' and 'SYSTEM_TESTS_GCP_PROJECT', and (4) Has incomplete test harness code at the end with comment '# wh'. The code appears to be a fabricated example rather than using real Airflow providers.",0,"The instruction specifically asks for a Looker pipeline demonstrating PDT (Persistent Derived Table) builds with both blocking and non-blocking approaches, plus sensor-based job monitoring. This code does not implement any actual Looker PDT build operations, does not demonstrate blocking vs non-blocking execution strategies, and does not include any sensors for monitoring Looker jobs. Instead, it shows fabricated MSSQL-to-Looker transfers and non-existent operators that don't fulfill the core requirement of showcasing different task execution strategies for Looker data transformations.",0.0
1015,Fine-tuned (Qwen 2.5 1.5B Airflow),401,0,"The code does not use TriggerDagRunOperator at all, which is the idiomatic Airflow way to trigger another DAG. The instruction explicitly asks for demonstrating TriggerDagRunOperator to trigger a secondary DAG from a primary DAG, but the code only contains a single DAG with a Python task and a BashOperator.",1,"The code uses standard Airflow libraries (airflow.providers.standard.operators.bash.BashOperator, airflow.sdk) and does not import any internal testing modules or include test harness boilerplate. While there are syntax errors (mismatched quotes), these are not hallucinations but rather coding mistakes.",0,"The code completely fails to meet the instruction requirements. The user asked for: (1) a primary DAG that triggers a secondary DAG, (2) use of TriggerDagRunOperator, and (3) passing a predefined configuration message between DAGs. The provided code only contains a single DAG with basic tasks and no triggering mechanism whatsoever. It does not demonstrate DAG triggering or configuration passing.",0.3333333333333333
1016,Fine-tuned (Qwen 2.5 1.5B Airflow),402,0,"While the code attempts to use specific operators (CloudWorkloadIdentityCreateJobQueueOperator, CloudWorkloadIdentityListJobQueuesOperator, CloudWorkloadIdentityDeleteJobQueueOperator), these operators do not exist in the official Airflow providers for Google Cloud as of version 2.9.3. The airflow.providers.google.cloud.operators.workload_identity module is not a real Airflow provider module. This appears to be fabricated/hallucinated operators rather than actual idiomatic Airflow code.",0,"The code contains multiple hallucinations: (1) It imports from 'tests.system.utils' and uses 'get_test_run(dag)' which is test harness boilerplate that should not be in production code. (2) The operators CloudWorkloadIdentityCreateJobQueueOperator, CloudWorkloadIdentityDeleteJobQueueOperator, and CloudWorkloadIdentityListJobQueuesOperator do not exist in the official Airflow Google Cloud provider. (3) There are syntax errors including invalid variable names with spaces ('Create WI Job Queue', 'List WI Job Queues', 'delete WI Job Queue') and incomplete assignment statements.",0,"The instruction asks for a DAG that demonstrates provisioning computational resources in Kubernetes, defining job queues, and preparing for parallel workload processing. However, this code attempts to use non-existent Google Cloud Workload Identity operators rather than Kubernetes-native operators (like KubernetesPodOperator, KubernetesJobOperator, or operators for setting up Kubernetes resources). The code does not demonstrate infrastructure-as-code for Kubernetes, does not provision computational resources, and does not set up parallel workload processing. It fundamentally misses the core requirement of working with Kubernetes.",0.0
1017,Fine-tuned (Qwen 2.5 1.5B Airflow),403,1,"The code correctly uses the InfluxDBOperator from airflow.providers.influxdb.operators.influxdb, which is the idiomatic and native way to interact with InfluxDB in Airflow. It does not wrap functionality in a PythonOperator with hooks, but instead uses the purpose-built operator for the task.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are not production-ready elements and represent test infrastructure leakage.",0,"The instruction specifically requests a dynamic time range relative to the current execution date. The provided code uses a static time range 'range(start: -10m)' which is relative to the current time, not the execution date. To properly fulfill the instruction, the query should incorporate Airflow's execution_date or logical_date using templating or dynamic query generation (e.g., using Jinja templates with {{ ds }} or {{ execution_date }}). The current implementation does not demonstrate this dynamic behavior tied to the DAG execution context.",0.3333333333333333
1018,Fine-tuned (Qwen 2.5 1.5B Airflow),404,1,"The code uses native Airflow Dataproc operators (DataprocCreateClusterOperator, DataprocUpdateClusterOperator, DataprocDeleteClusterOperator) from the official Google Cloud provider package. It does not wrap functionality in PythonOperators with hooks, which is the correct idiomatic approach for Airflow.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests_common.test_utils.watcher import watcher' and 'from tests_common.test_utils.system_tests import get_test_run', and includes 'test_run = get_test_run(dag)' and 'list(dag.tasks) >> watcher()'. These are testing artifacts that should not be present in production-ready code. The comments '# TEST SETUP', '# TEST BODY', and '# TEST TEARDOWN' also indicate this is test code rather than production code.",0,"The instruction requested a workflow demonstrating 'creating, starting, stopping, and deleting' a Dataproc cluster. The provided code only implements create, update, and delete operations. It does not include start and stop operations (DataprocStartClusterOperator and DataprocStopClusterOperator), which were explicitly requested in the instruction. While it does demonstrate lifecycle management, it misses key steps specified in the requirements.",0.3333333333333333
1019,Fine-tuned (Qwen 2.5 1.5B Airflow),405,1,"The code uses appropriate Airflow operators (BashOperator, PythonOperator) from the standard providers package and utilizes TaskGroup for organizing tasks. It uses the chain utility for task dependencies, which is idiomatic. No generic PythonOperator wrappers are used where specific operators would be more appropriate.",0,"The code contains multiple critical issues: 1) No DAG definition exists - 'dag' variable is referenced but never created with DAG() constructor, 2) Contains invalid Python syntax like 'section_{i}' as a variable name in loops, 3) Uses non-existent methods like 'section_1.end()' which is not a valid TaskGroup method, 4) Contains Chinese characters in comments (外部, 內) suggesting copy-paste from documentation without proper adaptation, 5) Invalid f-string usage in bash commands that would cause runtime errors. This code would not execute and appears to be malformed example code.",0,"The instruction specifically requested: 1) Setup and teardown operations - not present in the code, 2) Nested task groups - not demonstrated (only single-level groups shown), 3) Both decorator-based and direct task relationship methods - only direct relationships (>>) are used, no @task or @task_group decorators shown, 4) Clear initialization and finalization steps - not implemented. The code only shows basic TaskGroup usage without the requested lifecycle management, setup/teardown, or decorator patterns.",0.3333333333333333
1020,Fine-tuned (Qwen 2.5 1.5B Airflow),406,0,"The instruction explicitly asks for 'multi-stage bash script execution environment' and 'simple shell commands can be orchestrated', which should use BashOperator. Instead, the code uses a TaskFlow API @task decorator with a simple Python print statement. This is not idiomatic for the requested bash/shell command orchestration use case.",0,"The code imports 'from tests.system.utils import get_test_run' and includes 'test_run = get_test_run(dag)', which are internal testing utilities and test harness boilerplate. This is not production-ready code and represents test leakage.",0,"The instruction requires: (1) multi-stage bash script execution, (2) sequenced computational tasks, (3) complex dependency relationships, and (4) shell command orchestration. The provided code has none of these - it contains only a single Python task with a print statement, no bash commands, no multi-stage pipeline, and no dependency relationships demonstrated.",0.0
1021,Fine-tuned (Qwen 2.5 1.5B Airflow),407,0,"The code uses operators that do not exist in the Airflow Azure provider. The actual Azure Container Instances provider includes 'AzureContainerInstancesOperator' for running containers, not 'ContainerInstancesCreateClusterOperator', 'ContainerInstancesUpdateClusterOperator', etc. These operators appear to be fabricated or confused with AKS (Azure Kubernetes Service) operators. The code also creates a custom task using @task decorator to manually create ACR connections, which is not idiomatic when proper operators should be used.",0,"The code imports and uses non-existent operators: 'ContainerInstancesCreateClusterOperator', 'ContainerInstancesDeleteClusterOperator', 'ContainerInstancesGetClusterOperator', and 'ContainerInstancesUpdateClusterOperator'. These are not part of the airflow.providers.microsoft.azure.operators.container_instances module. The actual operator is 'AzureContainerInstancesOperator'. Additionally, the code references 'timedelta' without importing it, and the DAG is incomplete (cuts off mid-definition). The comment references AKS documentation, not ACI, indicating confusion between services.",0,"The instruction asks for a DAG that launches containerized workloads on Azure Container Instances with configurable resources, environment variables, and storage volume integrations. The provided code does not demonstrate any of these requirements. Instead, it attempts to create/update/delete 'clusters' (which is not an ACI concept - ACI runs individual container groups, not clusters). There is no demonstration of: (1) launching actual containerized workloads, (2) configurable resources (CPU/memory), (3) environment variables, or (4) storage volume integrations. The code fundamentally misunderstands what Azure Container Instances is and what the instruction requires.",0.0
1022,Fine-tuned (Qwen 2.5 1.5B Airflow),408,1,"The code uses native Airflow operators designed for the specific tasks: GCSCreateBucketOperator and GCSDeleteBucketOperator for bucket operations, and GoogleAdsToGCSQueryOperator and GoogleAdsToGCSUploadOperator for Google Ads data extraction. These are proper provider-specific operators rather than generic PythonOperator wrappers, which is the idiomatic Airflow approach.",0,"The code contains clear test harness boilerplate and imports from internal testing modules. Specifically, it imports 'from tests.system.utils.watcher import watcher' and 'from tests.system.utils import get_test_run', and includes 'test_run = get_test_run(dag)' at the end. It also has comments like '# TEST SETUP' and '# TEST TEARDOWN', and calls 'watcher()' which are all part of Airflow's system test infrastructure, not production-ready code.",0,"The code has several issues that prevent it from fulfilling the instruction. First, there's a typo: 'ADCLIENT_ID' is used in the operators but 'AD_CLIENT_ID' is defined. Second, the operators used (GoogleAdsToGCSQueryOperator, GoogleAdsToGCSUploadOperator) appear to be from 'airflow.providers.google.marketing_platform.operators.ads', but these don't exist in standard Airflow - the correct path would be 'airflow.providers.google.ads.operators.ads'. Third, the queries and parameters don't align with actual Google Ads API syntax (e.g., 'FROM admessages' is not valid Google Ads Query Language). While it attempts to demonstrate bucket creation, data extraction, and cleanup, the implementation has fundamental errors that would prevent it from working.",0.3333333333333333
1023,Fine-tuned (Qwen 2.5 1.5B Airflow),409,1,"The code uses the TaskFlow API with @task decorator, which is idiomatic and modern Airflow (2.0+). It doesn't wrap unnecessary logic in PythonOperator and uses native Airflow patterns appropriately for the simple task demonstrated.",1,"The code uses only standard Airflow libraries (airflow.decorators, airflow.models.dag) and includes no test harness imports, internal testing modules, or non-production boilerplate. The code is clean and production-ready from an import perspective.",0,"The instruction explicitly asks for 'dynamic parameter configuration for workflow triggering' with 'customizable input parameters with various input types and validation rules'. The provided code has NO parameters at all - no Params objects, no trigger configuration UI elements, no input types (string, int, boolean, etc.), and no validation rules. It's just a basic DAG with a simple print task. The code completely fails to demonstrate the core requirement of showcasing dynamic/interactive parameter configuration that users can customize when triggering the workflow.",0.6666666666666666
1024,Fine-tuned (Qwen 2.5 1.5B Airflow),410,0,"The code wraps SplunkHook operations in @task decorated functions (PythonOperator equivalent) instead of using native Splunk operators. For example, 'create_solr_index', 'enable_splunk_alert', etc. are all custom Python functions wrapping hook logic. Additionally, the instruction asks for a bash task, but the code defines 'BashCommand' as a string variable and never properly instantiates a BashOperator. The code also has nonsensical patterns like 'BashCommand.output()' and 'BashCommand.trigger()' which are not valid Airflow constructs.",0,"The code contains multiple hallucinations and errors: 1) 'SPLUNK_OPERTOR_TYPE_TOKEN' is referenced but never defined (typo in variable name). 2) 'BashCommand.output()' and 'BashCommand.trigger()' are not valid Airflow methods - BashCommand is just a string. 3) Methods like 'splunk_hook.create_solr_index()', 'splunk_hook.enable_app_alerting()', 'splunk_hook.upload_solr_data()', etc. do not exist in the standard SplunkHook API. 4) The bash script itself has an error (./tmp/myscript.sh should be /tmp/myscript.sh). 5) Variable 'SPLUNK_CONN_ID' is used but never defined. This code appears to be fabricated and would not run.",0,"The instruction asks for 'a bash task with integrated failure notifications to an incident management platform'. The code fails to deliver: 1) No proper BashOperator is instantiated - 'BashCommand' is just a string variable. 2) There is no error handling mechanism (no on_failure_callback, no try/except patterns). 3) While Splunk could theoretically be an incident management platform, the code doesn't demonstrate any failure notification logic - it just creates indexes and connections without any connection to bash task failures. 4) The DAG structure doesn't show how alerts would be triggered on bash task failure. The code completely misses the core requirement of demonstrating error handling and alerting.",0.0
