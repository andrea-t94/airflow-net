{"instruction": "Create a DAG that demonstrates basic Bash command execution and task dependencies using multiple BashOperators with dynamic task generation and templating.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "EmptyOperator"], "line_count": 59, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"example_bash_operator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=[\"example\", \"example2\"],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id=\"run_this_last\",\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id=\"run_after_loop\",\n        bash_command=\"echo 1\",\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id=f\"runme_{i}\",\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last\n\n# [START howto_operator_bash_skip]\nthis_will_skip = BashOperator(\n    task_id=\"this_will_skip\",\n    bash_command='echo \"hello world\"; exit 99;',\n    dag=dag,\n)\n# [END howto_operator_bash_skip]\nthis_will_skip >> run_this_last\n\nif __name__ == \"__main__\":\n    dag.test()", "metadata": {"file_name": "example_bash_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with standard BashOperator, minimal code, sequential dependencies, and basic Airflow imports", "key_concepts": ["BashOperator", "Task Dependencies", "Dynamic Task Generation", "Jinja Templating"], "learning_objectives": ["Understand task creation in Airflow", "Learn task dependency management", "Explore basic command execution", "Practice Jinja templating"]}}
{"instruction": "Design a workflow that demonstrates error handling, task skipping, and different execution strategies using Bash commands and EmptyOperator for task coordination.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "EmptyOperator"], "line_count": 59, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"example_bash_operator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=[\"example\", \"example2\"],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id=\"run_this_last\",\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id=\"run_after_loop\",\n        bash_command=\"echo 1\",\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id=f\"runme_{i}\",\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last\n\n# [START howto_operator_bash_skip]\nthis_will_skip = BashOperator(\n    task_id=\"this_will_skip\",\n    bash_command='echo \"hello world\"; exit 99;',\n    dag=dag,\n)\n# [END howto_operator_bash_skip]\nthis_will_skip >> run_this_last\n\nif __name__ == \"__main__\":\n    dag.test()", "metadata": {"file_name": "example_bash_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with error handling example, minimal operators, and basic task flow control", "key_concepts": ["Task Skipping", "Error Handling", "EmptyOperator", "Bash Command Execution"], "learning_objectives": ["Understand task failure mechanisms", "Learn how to skip tasks", "Practice task orchestration", "Explore exit code behavior"]}}
{"instruction": "Create a DAG that demonstrates conditional task execution based on minute-level time conditions, using the @task.branch decorator to dynamically select which task runs during each DAG execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of ``@task.branch`` TaskFlow API decorator with depends_on_past=True,\nwhere tasks may be run or skipped on alternating runs.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\n\n\n@task.branch()\ndef should_run(**kwargs) -> str:\n    \"\"\"\n    Determine which empty_task should be run based on if the execution date minute is even or odd.\n\n    :param dict kwargs: Context\n    :return: Id of the task to run\n    \"\"\"\n    print(\n        f\"------------- exec dttm = {kwargs['execution_date']} and minute = {kwargs['execution_date'].minute}\"\n    )\n    if kwargs[\"execution_date\"].minute % 2 == 0:\n        return \"empty_task_1\"\n    else:\n        return \"empty_task_2\"\n\n\nwith DAG(\n    dag_id=\"example_branch_dop_operator_v3\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    default_args={\"depends_on_past\": True},\n    tags=[\"example\"],\n) as dag:\n    cond = should_run()\n\n    empty_task_1 = EmptyOperator(task_id=\"empty_task_1\")\n    empty_task_2 = EmptyOperator(task_id=\"empty_task_2\")\n    cond >> [empty_task_1, empty_task_2]", "metadata": {"file_name": "example_branch_python_dop_operator_3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses TaskFlow API branching, custom decision function, and depends_on_past logic with multiple task paths", "key_concepts": ["TaskFlow API", "Branch Operator", "Conditional Execution", "Dynamic Task Selection"], "learning_objectives": ["Understand dynamic task routing", "Learn @task.branch decorator usage", "Explore time-based task selection strategies"]}}
{"instruction": "Design a workflow that demonstrates how Airflow can implement runtime task selection logic using execution context and modulo arithmetic to control task flow dynamically.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of ``@task.branch`` TaskFlow API decorator with depends_on_past=True,\nwhere tasks may be run or skipped on alternating runs.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\n\n\n@task.branch()\ndef should_run(**kwargs) -> str:\n    \"\"\"\n    Determine which empty_task should be run based on if the execution date minute is even or odd.\n\n    :param dict kwargs: Context\n    :return: Id of the task to run\n    \"\"\"\n    print(\n        f\"------------- exec dttm = {kwargs['execution_date']} and minute = {kwargs['execution_date'].minute}\"\n    )\n    if kwargs[\"execution_date\"].minute % 2 == 0:\n        return \"empty_task_1\"\n    else:\n        return \"empty_task_2\"\n\n\nwith DAG(\n    dag_id=\"example_branch_dop_operator_v3\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    default_args={\"depends_on_past\": True},\n    tags=[\"example\"],\n) as dag:\n    cond = should_run()\n\n    empty_task_1 = EmptyOperator(task_id=\"empty_task_1\")\n    empty_task_2 = EmptyOperator(task_id=\"empty_task_2\")\n    cond >> [empty_task_1, empty_task_2]", "metadata": {"file_name": "example_branch_python_dop_operator_3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves advanced task routing, uses execution date context, and implements conditional logic within DAG structure", "key_concepts": ["Execution Context", "Modulo Operator", "Task Dependency Management", "Scheduling Patterns"], "learning_objectives": ["Understand DAG scheduling nuances", "Learn context-based task selection", "Explore depends_on_past behavior"]}}
{"instruction": "Create a DAG that demonstrates dynamic task branching using random selection, where the workflow randomly chooses one of four predefined branches and executes tasks accordingly.", "input": {"airflow_version": "2.7.2", "operators": ["BranchPythonOperator", "EmptyOperator"], "line_count": 48, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the BranchPythonOperator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(\n        task_id=\"run_this_first\",\n    )\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    branching = BranchPythonOperator(\n        task_id=\"branching\",\n        python_callable=lambda: random.choice(options),\n    )\n    run_this_first >> branching\n\n    join = EmptyOperator(\n        task_id=\"join\",\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n    )\n\n    for option in options:\n        t = EmptyOperator(\n            task_id=option,\n        )\n\n        empty_follow = EmptyOperator(\n            task_id=\"follow_\" + option,\n        )\n\n        # Label is optional here, but it can help identify more complex branches\n        branching >> Label(option) >> t >> empty_follow >> join", "metadata": {"file_name": "example_branch_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses BranchPythonOperator with random selection, multiple task paths, and a join task with custom trigger rule, indicating moderate complexity", "key_concepts": ["BranchPythonOperator", "Dynamic Task Selection", "Conditional Workflow", "Task Dependencies"], "learning_objectives": ["Understand how to create conditional workflow paths", "Learn to use random selection in Airflow tasks", "Explore task dependency management"]}}
{"instruction": "Design a workflow that demonstrates flexible task routing using a branching strategy, where a single decision point determines subsequent task execution paths with minimal overhead.", "input": {"airflow_version": "2.7.2", "operators": ["BranchPythonOperator", "EmptyOperator"], "line_count": 48, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the BranchPythonOperator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(\n        task_id=\"run_this_first\",\n    )\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    branching = BranchPythonOperator(\n        task_id=\"branching\",\n        python_callable=lambda: random.choice(options),\n    )\n    run_this_first >> branching\n\n    join = EmptyOperator(\n        task_id=\"join\",\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n    )\n\n    for option in options:\n        t = EmptyOperator(\n            task_id=option,\n        )\n\n        empty_follow = EmptyOperator(\n            task_id=\"follow_\" + option,\n        )\n\n        # Label is optional here, but it can help identify more complex branches\n        branching >> Label(option) >> t >> empty_follow >> join", "metadata": {"file_name": "example_branch_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements advanced task routing with EmptyOperators, uses Label for path identification, and applies a custom trigger rule for task joining", "key_concepts": ["Task Routing", "Trigger Rules", "Edge Modification", "Workflow Flexibility"], "learning_objectives": ["Understand non-linear workflow design", "Learn to create flexible task execution paths", "Explore advanced Airflow dependency management"]}}
{"instruction": "Build a DAG demonstration of Airflow's Dataset feature, creating producer and consumer DAGs that showcase how datasets trigger downstream workflows based on data updates.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 113, "is_multifile": false}, "output": "\"\"\"\nExample DAG for demonstrating behavior of Datasets feature.\n\nNotes on usage:\n\nTurn on all the dags.\n\nDAG dataset_produces_1 should run because it's on a schedule.\n\nAfter dataset_produces_1 runs, dataset_consumes_1 should be triggered immediately\nbecause its only dataset dependency is managed by dataset_produces_1.\n\nNo other dags should be triggered.  Note that even though dataset_consumes_1_and_2 depends on\nthe dataset in dataset_produces_1, it will not be triggered until dataset_produces_2 runs\n(and dataset_produces_2 is left with no schedule so that we can trigger it manually).\n\nNext, trigger dataset_produces_2.  After dataset_produces_2 finishes,\ndataset_consumes_1_and_2 should run.\n\nDags dataset_consumes_1_never_scheduled and dataset_consumes_unknown_never_scheduled should not run because\nthey depend on datasets that never get updated.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, Dataset\nfrom airflow.operators.bash import BashOperator\n\n# [START dataset_def]\ndag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n# [END dataset_def]\ndag2_dataset = Dataset(\"s3://dag2/output_1.txt\", extra={\"hi\": \"bye\"})\n\nwith DAG(\n    dag_id=\"dataset_produces_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@daily\",\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag1:\n    # [START task_outlet]\n    BashOperator(outlets=[dag1_dataset], task_id=\"producing_task_1\", bash_command=\"sleep 5\")\n    # [END task_outlet]\n\nwith DAG(\n    dag_id=\"dataset_produces_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag2:\n    BashOperator(outlets=[dag2_dataset], task_id=\"producing_task_2\", bash_command=\"sleep 5\")\n\n# [START dag_dep]\nwith DAG(\n    dag_id=\"dataset_consumes_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag3:\n    # [END dag_dep]\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_1_task/dataset_other.txt\")],\n        task_id=\"consuming_1\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_and_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset, dag2_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag4:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_2\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        dag1_dataset,\n        Dataset(\"s3://this-dataset-doesnt-get-triggered\"),\n    ],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag5:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_3\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_unknown_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        Dataset(\"s3://unrelated/dataset3.txt\"),\n        Dataset(\"s3://unrelated/dataset_other_unknown.txt\"),\n    ],\n    tags=[\"dataset-scheduled\"],\n) as dag6:\n    BashOperator(\n        task_id=\"unrelated_task\",\n        outlets=[Dataset(\"s3://unrelated_task/dataset_other_unknown.txt\")],\n        bash_command=\"sleep 5\",\n    )", "metadata": {"file_name": "example_datasets.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple DAGs with dataset dependencies, demonstrates advanced scheduling logic, involves custom dataset definitions and inter-DAG dependencies", "key_concepts": ["Airflow Datasets", "Dependency Management", "Dynamic DAG Triggering", "Task Scheduling"], "learning_objectives": ["Understand dataset-based workflow dependencies", "Learn how datasets trigger downstream tasks", "Explore advanced Airflow scheduling mechanisms"]}}
{"instruction": "Design an event-driven data pipeline using Airflow's Dataset feature, demonstrating how different DAGs can be conditionally triggered based on data production events.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 113, "is_multifile": false}, "output": "\"\"\"\nExample DAG for demonstrating behavior of Datasets feature.\n\nNotes on usage:\n\nTurn on all the dags.\n\nDAG dataset_produces_1 should run because it's on a schedule.\n\nAfter dataset_produces_1 runs, dataset_consumes_1 should be triggered immediately\nbecause its only dataset dependency is managed by dataset_produces_1.\n\nNo other dags should be triggered.  Note that even though dataset_consumes_1_and_2 depends on\nthe dataset in dataset_produces_1, it will not be triggered until dataset_produces_2 runs\n(and dataset_produces_2 is left with no schedule so that we can trigger it manually).\n\nNext, trigger dataset_produces_2.  After dataset_produces_2 finishes,\ndataset_consumes_1_and_2 should run.\n\nDags dataset_consumes_1_never_scheduled and dataset_consumes_unknown_never_scheduled should not run because\nthey depend on datasets that never get updated.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, Dataset\nfrom airflow.operators.bash import BashOperator\n\n# [START dataset_def]\ndag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n# [END dataset_def]\ndag2_dataset = Dataset(\"s3://dag2/output_1.txt\", extra={\"hi\": \"bye\"})\n\nwith DAG(\n    dag_id=\"dataset_produces_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@daily\",\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag1:\n    # [START task_outlet]\n    BashOperator(outlets=[dag1_dataset], task_id=\"producing_task_1\", bash_command=\"sleep 5\")\n    # [END task_outlet]\n\nwith DAG(\n    dag_id=\"dataset_produces_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag2:\n    BashOperator(outlets=[dag2_dataset], task_id=\"producing_task_2\", bash_command=\"sleep 5\")\n\n# [START dag_dep]\nwith DAG(\n    dag_id=\"dataset_consumes_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag3:\n    # [END dag_dep]\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_1_task/dataset_other.txt\")],\n        task_id=\"consuming_1\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_and_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset, dag2_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag4:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_2\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        dag1_dataset,\n        Dataset(\"s3://this-dataset-doesnt-get-triggered\"),\n    ],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag5:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_3\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_unknown_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        Dataset(\"s3://unrelated/dataset3.txt\"),\n        Dataset(\"s3://unrelated/dataset_other_unknown.txt\"),\n    ],\n    tags=[\"dataset-scheduled\"],\n) as dag6:\n    BashOperator(\n        task_id=\"unrelated_task\",\n        outlets=[Dataset(\"s3://unrelated_task/dataset_other_unknown.txt\")],\n        bash_command=\"sleep 5\",\n    )", "metadata": {"file_name": "example_datasets.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements complex scheduling logic, uses multiple DAGs with conditional execution, demonstrates event-driven workflow orchestration", "key_concepts": ["Event-Driven Architecture", "Workflow Orchestration", "Conditional DAG Execution", "Data Lineage"], "learning_objectives": ["Implement event-based workflow triggers", "Understand dataset dependency management", "Learn how to create loosely coupled data pipelines"]}}
{"instruction": "Create a DAG that demonstrates dynamic task branching using Python's random module and Airflow's @task.branch decorator, where the workflow randomly selects and executes one of multiple predefined task branches.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 41, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the ``@task.branch`` TaskFlow API decorator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_python_operator_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"run_this_first\")\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    @task.branch(task_id=\"branching\")\n    def random_choice(choices: list[str]) -> str:\n        return random.choice(choices)\n\n    random_choice_instance = random_choice(choices=options)\n\n    run_this_first >> random_choice_instance\n\n    join = EmptyOperator(task_id=\"join\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n\n    for option in options:\n        t = EmptyOperator(task_id=option)\n\n        empty_follow = EmptyOperator(task_id=\"follow_\" + option)\n\n        # Label is optional here, but it can help identify more complex branches\n        random_choice_instance >> Label(option) >> t >> empty_follow >> join", "metadata": {"file_name": "example_branch_operator_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses TaskFlow API, branching logic, multiple dynamic tasks, and introduces conditional task execution with random selection", "key_concepts": ["TaskFlow API", "Branch Operator", "Dynamic Task Generation", "Random Selection", "Trigger Rules"], "learning_objectives": ["Understand dynamic task branching", "Learn how to use @task.branch decorator", "Explore conditional workflow execution", "Practice creating flexible Airflow DAGs"]}}
{"instruction": "Design a workflow that demonstrates Airflow's task dependency management and edge labeling by creating a multi-branch pipeline with explicit path visualization and flexible execution rules.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 41, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the ``@task.branch`` TaskFlow API decorator.\"\"\"\nfrom __future__ import annotations\n\nimport random\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_branch_python_operator_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\", \"example2\"],\n) as dag:\n    run_this_first = EmptyOperator(task_id=\"run_this_first\")\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n\n    @task.branch(task_id=\"branching\")\n    def random_choice(choices: list[str]) -> str:\n        return random.choice(choices)\n\n    random_choice_instance = random_choice(choices=options)\n\n    run_this_first >> random_choice_instance\n\n    join = EmptyOperator(task_id=\"join\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n\n    for option in options:\n        t = EmptyOperator(task_id=option)\n\n        empty_follow = EmptyOperator(task_id=\"follow_\" + option)\n\n        # Label is optional here, but it can help identify more complex branches\n        random_choice_instance >> Label(option) >> t >> empty_follow >> join", "metadata": {"file_name": "example_branch_operator_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple task dependencies, custom edge labeling, non-standard trigger rules, and demonstrates advanced DAG configuration", "key_concepts": ["Task Dependencies", "Edge Labeling", "TriggerRules", "Workflow Flexibility", "Operator Chaining"], "learning_objectives": ["Master task dependency management", "Understand edge modification techniques", "Learn about advanced trigger rule configurations", "Explore DAG visualization strategies"]}}
{"instruction": "Build a DAG that demonstrates conditional task execution based on the day of the week using BranchDayOfWeekOperator, showing how to create workflow logic that changes based on calendar conditions.", "input": {"airflow_version": "2.7.2", "operators": ["BranchDayOfWeekOperator", "EmptyOperator"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of BranchDayOfWeekOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.weekday import BranchDayOfWeekOperator\nfrom airflow.utils.weekday import WeekDay\n\nwith DAG(\n    dag_id=\"example_weekday_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n) as dag:\n    # [START howto_operator_day_of_week_branch]\n    empty_task_1 = EmptyOperator(task_id=\"branch_true\")\n    empty_task_2 = EmptyOperator(task_id=\"branch_false\")\n    empty_task_3 = EmptyOperator(task_id=\"branch_weekend\")\n    empty_task_4 = EmptyOperator(task_id=\"branch_mid_week\")\n\n    branch = BranchDayOfWeekOperator(\n        task_id=\"make_choice\",\n        follow_task_ids_if_true=\"branch_true\",\n        follow_task_ids_if_false=\"branch_false\",\n        week_day=\"Monday\",\n    )\n    branch_weekend = BranchDayOfWeekOperator(\n        task_id=\"make_weekend_choice\",\n        follow_task_ids_if_true=\"branch_weekend\",\n        follow_task_ids_if_false=\"branch_mid_week\",\n        week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},\n    )\n\n    # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise\n    branch >> [empty_task_1, empty_task_2]\n    # Run empty_task_3 if it's a weekend, empty_task_4 otherwise\n    empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]\n    # [END howto_operator_day_of_week_branch]", "metadata": {"file_name": "example_branch_day_of_week_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Uses simple EmptyOperators, basic branching with BranchDayOfWeekOperator, and straightforward task dependencies with minimal custom logic", "key_concepts": ["Airflow branching", "Conditional task execution", "Day of week filtering", "Task dependencies"], "learning_objectives": ["Understand how to use BranchDayOfWeekOperator", "Learn conditional workflow routing", "Explore Airflow task dependency management"]}}
{"instruction": "Design a workflow that demonstrates advanced scheduling strategies by implementing different task paths for weekdays and weekends using Airflow's branching capabilities.", "input": {"airflow_version": "2.7.2", "operators": ["BranchDayOfWeekOperator", "EmptyOperator"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of BranchDayOfWeekOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.weekday import BranchDayOfWeekOperator\nfrom airflow.utils.weekday import WeekDay\n\nwith DAG(\n    dag_id=\"example_weekday_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n) as dag:\n    # [START howto_operator_day_of_week_branch]\n    empty_task_1 = EmptyOperator(task_id=\"branch_true\")\n    empty_task_2 = EmptyOperator(task_id=\"branch_false\")\n    empty_task_3 = EmptyOperator(task_id=\"branch_weekend\")\n    empty_task_4 = EmptyOperator(task_id=\"branch_mid_week\")\n\n    branch = BranchDayOfWeekOperator(\n        task_id=\"make_choice\",\n        follow_task_ids_if_true=\"branch_true\",\n        follow_task_ids_if_false=\"branch_false\",\n        week_day=\"Monday\",\n    )\n    branch_weekend = BranchDayOfWeekOperator(\n        task_id=\"make_weekend_choice\",\n        follow_task_ids_if_true=\"branch_weekend\",\n        follow_task_ids_if_false=\"branch_mid_week\",\n        week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},\n    )\n\n    # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise\n    branch >> [empty_task_1, empty_task_2]\n    # Run empty_task_3 if it's a weekend, empty_task_4 otherwise\n    empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]\n    # [END howto_operator_day_of_week_branch]", "metadata": {"file_name": "example_branch_day_of_week_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple branching operators, set-based day selection, and more complex task routing beyond simple sequential execution", "key_concepts": ["Advanced scheduling", "Workflow branching", "Pendulum datetime handling", "Task routing logic"], "learning_objectives": ["Master conditional workflow design", "Understand complex task routing techniques", "Learn dynamic task selection strategies"]}}
{"instruction": "Build a DAG that demonstrates conditional task execution based on datetime ranges using BranchDateTimeOperator, exploring how to dynamically route workflow tasks depending on specific time conditions.", "input": {"airflow_version": "2.7.2", "operators": ["BranchDateTimeOperator", "EmptyOperator"], "line_count": 87, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of DateTimeBranchOperator with datetime as well as time objects as\ntargets.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.datetime import BranchDateTimeOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndag1 = DAG(\n    dag_id=\"example_branch_datetime_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n\n# [START howto_branch_datetime_operator]\nempty_task_11 = EmptyOperator(task_id=\"date_in_range\", dag=dag1)\nempty_task_21 = EmptyOperator(task_id=\"date_outside_range\", dag=dag1)\n\ncond1 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag1,\n)\n\n# Run empty_task_11 if cond1 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond1 >> [empty_task_11, empty_task_21]\n# [END howto_branch_datetime_operator]\n\n\ndag2 = DAG(\n    dag_id=\"example_branch_datetime_operator_2\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_next_day]\nempty_task_12 = EmptyOperator(task_id=\"date_in_range\", dag=dag2)\nempty_task_22 = EmptyOperator(task_id=\"date_outside_range\", dag=dag2)\n\ncond2 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.time(0, 0, 0),\n    target_lower=pendulum.time(15, 0, 0),\n    dag=dag2,\n)\n\n# Since target_lower happens after target_upper, target_upper will be moved to the following day\n# Run empty_task_12 if cond2 executes between 15:00:00, and 00:00:00 of the following day\ncond2 >> [empty_task_12, empty_task_22]\n# [END howto_branch_datetime_operator_next_day]\n\ndag3 = DAG(\n    dag_id=\"example_branch_datetime_operator_3\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_logical_date]\nempty_task_13 = EmptyOperator(task_id=\"date_in_range\", dag=dag3)\nempty_task_23 = EmptyOperator(task_id=\"date_outside_range\", dag=dag3)\n\ncond3 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    use_task_logical_date=True,\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag3,\n)\n\n# Run empty_task_13 if cond3 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond3 >> [empty_task_13, empty_task_23]\n# [END howto_branch_datetime_operator_logical_date]", "metadata": {"file_name": "example_branch_datetime_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses advanced branching operator, multiple DAG configurations, time-based conditional logic, and demonstrates different datetime targeting strategies", "key_concepts": ["BranchDateTimeOperator", "Conditional Task Routing", "Pendulum Datetime Manipulation", "Task Dependencies"], "learning_objectives": ["Understand dynamic task routing in Airflow", "Learn datetime-based workflow control", "Explore time range targeting techniques"]}}
{"instruction": "Create a workflow demonstrating advanced time-based scheduling strategies, focusing on how Airflow can handle complex time-range decisions and logical date processing across different DAG configurations.", "input": {"airflow_version": "2.7.2", "operators": ["BranchDateTimeOperator", "EmptyOperator"], "line_count": 87, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of DateTimeBranchOperator with datetime as well as time objects as\ntargets.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.datetime import BranchDateTimeOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndag1 = DAG(\n    dag_id=\"example_branch_datetime_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n\n# [START howto_branch_datetime_operator]\nempty_task_11 = EmptyOperator(task_id=\"date_in_range\", dag=dag1)\nempty_task_21 = EmptyOperator(task_id=\"date_outside_range\", dag=dag1)\n\ncond1 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag1,\n)\n\n# Run empty_task_11 if cond1 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond1 >> [empty_task_11, empty_task_21]\n# [END howto_branch_datetime_operator]\n\n\ndag2 = DAG(\n    dag_id=\"example_branch_datetime_operator_2\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_next_day]\nempty_task_12 = EmptyOperator(task_id=\"date_in_range\", dag=dag2)\nempty_task_22 = EmptyOperator(task_id=\"date_outside_range\", dag=dag2)\n\ncond2 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.time(0, 0, 0),\n    target_lower=pendulum.time(15, 0, 0),\n    dag=dag2,\n)\n\n# Since target_lower happens after target_upper, target_upper will be moved to the following day\n# Run empty_task_12 if cond2 executes between 15:00:00, and 00:00:00 of the following day\ncond2 >> [empty_task_12, empty_task_22]\n# [END howto_branch_datetime_operator_next_day]\n\ndag3 = DAG(\n    dag_id=\"example_branch_datetime_operator_3\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_logical_date]\nempty_task_13 = EmptyOperator(task_id=\"date_in_range\", dag=dag3)\nempty_task_23 = EmptyOperator(task_id=\"date_outside_range\", dag=dag3)\n\ncond3 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    use_task_logical_date=True,\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag3,\n)\n\n# Run empty_task_13 if cond3 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond3 >> [empty_task_13, empty_task_23]\n# [END howto_branch_datetime_operator_logical_date]", "metadata": {"file_name": "example_branch_datetime_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Multiple DAG examples showing nuanced time-based execution, use of logical dates, and cross-day time range handling", "key_concepts": ["Time-Based Scheduling", "Logical Date Processing", "Workflow Branching", "Datetime Comparisons"], "learning_objectives": ["Master time-based task execution logic", "Understand datetime range comparisons", "Learn flexible scheduling techniques"]}}
{"instruction": "Design a cross-DAG dependency workflow that demonstrates how to use ExternalTaskSensor and ExternalTaskMarker to create inter-DAG task dependencies, showing how tasks in different DAGs can synchronize and communicate their execution status.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "ExternalTaskSensor"], "line_count": 80, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating setting up inter-DAG dependencies using ExternalTaskSensor and\nExternalTaskMarker.\n\nIn this example, child_task1 in example_external_task_marker_child depends on parent_task in\nexample_external_task_marker_parent. When parent_task is cleared with 'Recursive' selected,\nthe presence of ExternalTaskMarker tells Airflow to clear child_task1 and its downstream tasks.\n\nExternalTaskSensor will keep poking for the status of remote ExternalTaskMarker task at a regular\ninterval till one of the following will happen:\n\nExternalTaskMarker reaches the states mentioned in the allowed_states list.\nIn this case, ExternalTaskSensor will exit with a success status code\n\nExternalTaskMarker reaches the states mentioned in the failed_states list\nIn this case, ExternalTaskSensor will raise an AirflowException and user need to handle this\nwith multiple downstream tasks\n\nExternalTaskSensor times out. In this case, ExternalTaskSensor will raise AirflowSkipException\nor AirflowSensorTimeout exception\n\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.external_task import ExternalTaskMarker, ExternalTaskSensor\n\nstart_date = pendulum.datetime(2021, 1, 1, tz=\"UTC\")\n\nwith DAG(\n    dag_id=\"example_external_task_marker_parent\",\n    start_date=start_date,\n    catchup=False,\n    schedule=None,\n    tags=[\"example2\"],\n) as parent_dag:\n    # [START howto_operator_external_task_marker]\n    parent_task = ExternalTaskMarker(\n        task_id=\"parent_task\",\n        external_dag_id=\"example_external_task_marker_child\",\n        external_task_id=\"child_task1\",\n    )\n    # [END howto_operator_external_task_marker]\n\nwith DAG(\n    dag_id=\"example_external_task_marker_child\",\n    start_date=start_date,\n    schedule=None,\n    catchup=False,\n    tags=[\"example2\"],\n) as child_dag:\n    # [START howto_operator_external_task_sensor]\n    child_task1 = ExternalTaskSensor(\n        task_id=\"child_task1\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_id=parent_task.task_id,\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor]\n\n    # [START howto_operator_external_task_sensor_with_task_group]\n    child_task2 = ExternalTaskSensor(\n        task_id=\"child_task2\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_group_id=\"parent_dag_task_group_id\",\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor_with_task_group]\n\n    child_task3 = EmptyOperator(task_id=\"child_task3\")\n    child_task1 >> child_task2 >> child_task3", "metadata": {"file_name": "example_external_task_marker_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses advanced Airflow sensor with multiple configuration parameters, involves inter-DAG communication, and requires understanding of task synchronization mechanisms", "key_concepts": ["ExternalTaskSensor", "ExternalTaskMarker", "Inter-DAG Dependencies", "Task Synchronization", "Airflow Sensors"], "learning_objectives": ["Understand how to create dependencies between tasks in different DAGs", "Learn to configure task sensors with allowed and failed states", "Explore Airflow's task coordination mechanisms"]}}
{"instruction": "Create a robust task dependency system that demonstrates error handling and task state management across multiple DAGs, focusing on how to use timeout configurations and state monitoring in complex workflow scenarios.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "ExternalTaskSensor"], "line_count": 80, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating setting up inter-DAG dependencies using ExternalTaskSensor and\nExternalTaskMarker.\n\nIn this example, child_task1 in example_external_task_marker_child depends on parent_task in\nexample_external_task_marker_parent. When parent_task is cleared with 'Recursive' selected,\nthe presence of ExternalTaskMarker tells Airflow to clear child_task1 and its downstream tasks.\n\nExternalTaskSensor will keep poking for the status of remote ExternalTaskMarker task at a regular\ninterval till one of the following will happen:\n\nExternalTaskMarker reaches the states mentioned in the allowed_states list.\nIn this case, ExternalTaskSensor will exit with a success status code\n\nExternalTaskMarker reaches the states mentioned in the failed_states list\nIn this case, ExternalTaskSensor will raise an AirflowException and user need to handle this\nwith multiple downstream tasks\n\nExternalTaskSensor times out. In this case, ExternalTaskSensor will raise AirflowSkipException\nor AirflowSensorTimeout exception\n\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.external_task import ExternalTaskMarker, ExternalTaskSensor\n\nstart_date = pendulum.datetime(2021, 1, 1, tz=\"UTC\")\n\nwith DAG(\n    dag_id=\"example_external_task_marker_parent\",\n    start_date=start_date,\n    catchup=False,\n    schedule=None,\n    tags=[\"example2\"],\n) as parent_dag:\n    # [START howto_operator_external_task_marker]\n    parent_task = ExternalTaskMarker(\n        task_id=\"parent_task\",\n        external_dag_id=\"example_external_task_marker_child\",\n        external_task_id=\"child_task1\",\n    )\n    # [END howto_operator_external_task_marker]\n\nwith DAG(\n    dag_id=\"example_external_task_marker_child\",\n    start_date=start_date,\n    schedule=None,\n    catchup=False,\n    tags=[\"example2\"],\n) as child_dag:\n    # [START howto_operator_external_task_sensor]\n    child_task1 = ExternalTaskSensor(\n        task_id=\"child_task1\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_id=parent_task.task_id,\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor]\n\n    # [START howto_operator_external_task_sensor_with_task_group]\n    child_task2 = ExternalTaskSensor(\n        task_id=\"child_task2\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_group_id=\"parent_dag_task_group_id\",\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor_with_task_group]\n\n    child_task3 = EmptyOperator(task_id=\"child_task3\")\n    child_task1 >> child_task2 >> child_task3", "metadata": {"file_name": "example_external_task_marker_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple configuration parameters, error state handling, timeout management, and complex task dependency strategies", "key_concepts": ["Task State Management", "Error Handling", "Sensor Timeout Configuration", "Workflow Resilience", "Execution Mode"], "learning_objectives": ["Understand how to handle different task execution states", "Learn to configure sensor timeout and retry mechanisms", "Explore strategies for managing complex workflow dependencies"]}}
{"instruction": "Design a data catalog workflow that demonstrates task dependencies and lifecycle management for creating, retrieving, and deleting catalog entries across different resources.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 203, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows the complex DAG structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\n\nwith models.DAG(\n    dag_id=\"example_complex\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"example2\", \"example3\"],\n) as dag:\n    # Create\n    create_entry_group = BashOperator(task_id=\"create_entry_group\", bash_command=\"echo create_entry_group\")\n\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\", bash_command=\"echo create_entry_group_result\"\n    )\n\n    create_entry_group_result2 = BashOperator(\n        task_id=\"create_entry_group_result2\", bash_command=\"echo create_entry_group_result2\"\n    )\n\n    create_entry_gcs = BashOperator(task_id=\"create_entry_gcs\", bash_command=\"echo create_entry_gcs\")\n\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\", bash_command=\"echo create_entry_gcs_result\"\n    )\n\n    create_entry_gcs_result2 = BashOperator(\n        task_id=\"create_entry_gcs_result2\", bash_command=\"echo create_entry_gcs_result2\"\n    )\n\n    create_tag = BashOperator(task_id=\"create_tag\", bash_command=\"echo create_tag\")\n\n    create_tag_result = BashOperator(task_id=\"create_tag_result\", bash_command=\"echo create_tag_result\")\n\n    create_tag_result2 = BashOperator(task_id=\"create_tag_result2\", bash_command=\"echo create_tag_result2\")\n\n    create_tag_template = BashOperator(task_id=\"create_tag_template\", bash_command=\"echo create_tag_template\")\n\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\", bash_command=\"echo create_tag_template_result\"\n    )\n\n    create_tag_template_result2 = BashOperator(\n        task_id=\"create_tag_template_result2\", bash_command=\"echo create_tag_template_result2\"\n    )\n\n    create_tag_template_field = BashOperator(\n        task_id=\"create_tag_template_field\", bash_command=\"echo create_tag_template_field\"\n    )\n\n    create_tag_template_field_result = BashOperator(\n        task_id=\"create_tag_template_field_result\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    create_tag_template_field_result2 = BashOperator(\n        task_id=\"create_tag_template_field_result2\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    # Delete\n    delete_entry = BashOperator(task_id=\"delete_entry\", bash_command=\"echo delete_entry\")\n    create_entry_gcs >> delete_entry\n\n    delete_entry_group = BashOperator(task_id=\"delete_entry_group\", bash_command=\"echo delete_entry_group\")\n    create_entry_group >> delete_entry_group\n\n    delete_tag = BashOperator(task_id=\"delete_tag\", bash_command=\"echo delete_tag\")\n    create_tag >> delete_tag\n\n    delete_tag_template_field = BashOperator(\n        task_id=\"delete_tag_template_field\", bash_command=\"echo delete_tag_template_field\"\n    )\n\n    delete_tag_template = BashOperator(task_id=\"delete_tag_template\", bash_command=\"echo delete_tag_template\")\n\n    # Get\n    get_entry_group = BashOperator(task_id=\"get_entry_group\", bash_command=\"echo get_entry_group\")\n\n    get_entry_group_result = BashOperator(\n        task_id=\"get_entry_group_result\", bash_command=\"echo get_entry_group_result\"\n    )\n\n    get_entry = BashOperator(task_id=\"get_entry\", bash_command=\"echo get_entry\")\n\n    get_entry_result = BashOperator(task_id=\"get_entry_result\", bash_command=\"echo get_entry_result\")\n\n    get_tag_template = BashOperator(task_id=\"get_tag_template\", bash_command=\"echo get_tag_template\")\n\n    get_tag_template_result = BashOperator(\n        task_id=\"get_tag_template_result\", bash_command=\"echo get_tag_template_result\"\n    )\n\n    # List\n    list_tags = BashOperator(task_id=\"list_tags\", bash_command=\"echo list_tags\")\n\n    list_tags_result = BashOperator(task_id=\"list_tags_result\", bash_command=\"echo list_tags_result\")\n\n    # Lookup\n    lookup_entry = BashOperator(task_id=\"lookup_entry\", bash_command=\"echo lookup_entry\")\n\n    lookup_entry_result = BashOperator(task_id=\"lookup_entry_result\", bash_command=\"echo lookup_entry_result\")\n\n    # Rename\n    rename_tag_template_field = BashOperator(\n        task_id=\"rename_tag_template_field\", bash_command=\"echo rename_tag_template_field\"\n    )\n\n    # Search\n    search_catalog = BashOperator(task_id=\"search_catalog\", bash_command=\"echo search_catalog\")\n\n    search_catalog_result = BashOperator(\n        task_id=\"search_catalog_result\", bash_command=\"echo search_catalog_result\"\n    )\n\n    # Update\n    update_entry = BashOperator(task_id=\"update_entry\", bash_command=\"echo update_entry\")\n\n    update_tag = BashOperator(task_id=\"update_tag\", bash_command=\"echo update_tag\")\n\n    update_tag_template = BashOperator(task_id=\"update_tag_template\", bash_command=\"echo update_tag_template\")\n\n    update_tag_template_field = BashOperator(\n        task_id=\"update_tag_template_field\", bash_command=\"echo update_tag_template_field\"\n    )\n\n    # Create\n    create_tasks = [\n        create_entry_group,\n        create_entry_gcs,\n        create_tag_template,\n        create_tag_template_field,\n        create_tag,\n    ]\n    chain(*create_tasks)\n\n    create_entry_group >> delete_entry_group\n    create_entry_group >> create_entry_group_result\n    create_entry_group >> create_entry_group_result2\n\n    create_entry_gcs >> delete_entry\n    create_entry_gcs >> create_entry_gcs_result\n    create_entry_gcs >> create_entry_gcs_result2\n\n    create_tag_template >> delete_tag_template_field\n    create_tag_template >> create_tag_template_result\n    create_tag_template >> create_tag_template_result2\n\n    create_tag_template_field >> delete_tag_template_field\n    create_tag_template_field >> create_tag_template_field_result\n    create_tag_template_field >> create_tag_template_field_result2\n\n    create_tag >> delete_tag\n    create_tag >> create_tag_result\n    create_tag >> create_tag_result2\n\n    # Delete\n    delete_tasks = [\n        delete_tag,\n        delete_tag_template_field,\n        delete_tag_template,\n        delete_entry_group,\n        delete_entry,\n    ]\n    chain(*delete_tasks)\n\n    # Get\n    create_tag_template >> get_tag_template >> delete_tag_template\n    get_tag_template >> get_tag_template_result\n\n    create_entry_gcs >> get_entry >> delete_entry\n    get_entry >> get_entry_result\n\n    create_entry_group >> get_entry_group >> delete_entry_group\n    get_entry_group >> get_entry_group_result\n\n    # List\n    create_tag >> list_tags >> delete_tag\n    list_tags >> list_tags_result\n\n    # Lookup\n    create_entry_gcs >> lookup_entry >> delete_entry\n    lookup_entry >> lookup_entry_result\n\n    # Rename\n    create_tag_template_field >> rename_tag_template_field >> delete_tag_template_field\n\n    # Search\n    chain(create_tasks, search_catalog, delete_tasks)\n    search_catalog >> search_catalog_result\n\n    # Update\n    create_entry_gcs >> update_entry >> delete_entry\n    create_tag >> update_tag >> delete_tag\n    create_tag_template >> update_tag_template >> delete_tag_template\n    create_tag_template_field >> update_tag_template_field >> rename_tag_template_field", "metadata": {"file_name": "example_complex.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Multiple task types with sequential and parallel dependencies, uses BashOperator for simulating complex workflow steps, demonstrates resource management lifecycle", "key_concepts": ["Task Dependencies", "Workflow Orchestration", "Resource Lifecycle Management", "DAG Configuration", "Operator Chaining"], "learning_objectives": ["Understand Airflow task dependency patterns", "Learn how to structure complex workflows", "Explore resource creation and deletion sequences"]}}
{"instruction": "Create a comprehensive data catalog management system that showcases different operational stages including creation, retrieval, listing, and deletion of catalog entries and associated metadata.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 203, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows the complex DAG structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\n\nwith models.DAG(\n    dag_id=\"example_complex\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"example2\", \"example3\"],\n) as dag:\n    # Create\n    create_entry_group = BashOperator(task_id=\"create_entry_group\", bash_command=\"echo create_entry_group\")\n\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\", bash_command=\"echo create_entry_group_result\"\n    )\n\n    create_entry_group_result2 = BashOperator(\n        task_id=\"create_entry_group_result2\", bash_command=\"echo create_entry_group_result2\"\n    )\n\n    create_entry_gcs = BashOperator(task_id=\"create_entry_gcs\", bash_command=\"echo create_entry_gcs\")\n\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\", bash_command=\"echo create_entry_gcs_result\"\n    )\n\n    create_entry_gcs_result2 = BashOperator(\n        task_id=\"create_entry_gcs_result2\", bash_command=\"echo create_entry_gcs_result2\"\n    )\n\n    create_tag = BashOperator(task_id=\"create_tag\", bash_command=\"echo create_tag\")\n\n    create_tag_result = BashOperator(task_id=\"create_tag_result\", bash_command=\"echo create_tag_result\")\n\n    create_tag_result2 = BashOperator(task_id=\"create_tag_result2\", bash_command=\"echo create_tag_result2\")\n\n    create_tag_template = BashOperator(task_id=\"create_tag_template\", bash_command=\"echo create_tag_template\")\n\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\", bash_command=\"echo create_tag_template_result\"\n    )\n\n    create_tag_template_result2 = BashOperator(\n        task_id=\"create_tag_template_result2\", bash_command=\"echo create_tag_template_result2\"\n    )\n\n    create_tag_template_field = BashOperator(\n        task_id=\"create_tag_template_field\", bash_command=\"echo create_tag_template_field\"\n    )\n\n    create_tag_template_field_result = BashOperator(\n        task_id=\"create_tag_template_field_result\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    create_tag_template_field_result2 = BashOperator(\n        task_id=\"create_tag_template_field_result2\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    # Delete\n    delete_entry = BashOperator(task_id=\"delete_entry\", bash_command=\"echo delete_entry\")\n    create_entry_gcs >> delete_entry\n\n    delete_entry_group = BashOperator(task_id=\"delete_entry_group\", bash_command=\"echo delete_entry_group\")\n    create_entry_group >> delete_entry_group\n\n    delete_tag = BashOperator(task_id=\"delete_tag\", bash_command=\"echo delete_tag\")\n    create_tag >> delete_tag\n\n    delete_tag_template_field = BashOperator(\n        task_id=\"delete_tag_template_field\", bash_command=\"echo delete_tag_template_field\"\n    )\n\n    delete_tag_template = BashOperator(task_id=\"delete_tag_template\", bash_command=\"echo delete_tag_template\")\n\n    # Get\n    get_entry_group = BashOperator(task_id=\"get_entry_group\", bash_command=\"echo get_entry_group\")\n\n    get_entry_group_result = BashOperator(\n        task_id=\"get_entry_group_result\", bash_command=\"echo get_entry_group_result\"\n    )\n\n    get_entry = BashOperator(task_id=\"get_entry\", bash_command=\"echo get_entry\")\n\n    get_entry_result = BashOperator(task_id=\"get_entry_result\", bash_command=\"echo get_entry_result\")\n\n    get_tag_template = BashOperator(task_id=\"get_tag_template\", bash_command=\"echo get_tag_template\")\n\n    get_tag_template_result = BashOperator(\n        task_id=\"get_tag_template_result\", bash_command=\"echo get_tag_template_result\"\n    )\n\n    # List\n    list_tags = BashOperator(task_id=\"list_tags\", bash_command=\"echo list_tags\")\n\n    list_tags_result = BashOperator(task_id=\"list_tags_result\", bash_command=\"echo list_tags_result\")\n\n    # Lookup\n    lookup_entry = BashOperator(task_id=\"lookup_entry\", bash_command=\"echo lookup_entry\")\n\n    lookup_entry_result = BashOperator(task_id=\"lookup_entry_result\", bash_command=\"echo lookup_entry_result\")\n\n    # Rename\n    rename_tag_template_field = BashOperator(\n        task_id=\"rename_tag_template_field\", bash_command=\"echo rename_tag_template_field\"\n    )\n\n    # Search\n    search_catalog = BashOperator(task_id=\"search_catalog\", bash_command=\"echo search_catalog\")\n\n    search_catalog_result = BashOperator(\n        task_id=\"search_catalog_result\", bash_command=\"echo search_catalog_result\"\n    )\n\n    # Update\n    update_entry = BashOperator(task_id=\"update_entry\", bash_command=\"echo update_entry\")\n\n    update_tag = BashOperator(task_id=\"update_tag\", bash_command=\"echo update_tag\")\n\n    update_tag_template = BashOperator(task_id=\"update_tag_template\", bash_command=\"echo update_tag_template\")\n\n    update_tag_template_field = BashOperator(\n        task_id=\"update_tag_template_field\", bash_command=\"echo update_tag_template_field\"\n    )\n\n    # Create\n    create_tasks = [\n        create_entry_group,\n        create_entry_gcs,\n        create_tag_template,\n        create_tag_template_field,\n        create_tag,\n    ]\n    chain(*create_tasks)\n\n    create_entry_group >> delete_entry_group\n    create_entry_group >> create_entry_group_result\n    create_entry_group >> create_entry_group_result2\n\n    create_entry_gcs >> delete_entry\n    create_entry_gcs >> create_entry_gcs_result\n    create_entry_gcs >> create_entry_gcs_result2\n\n    create_tag_template >> delete_tag_template_field\n    create_tag_template >> create_tag_template_result\n    create_tag_template >> create_tag_template_result2\n\n    create_tag_template_field >> delete_tag_template_field\n    create_tag_template_field >> create_tag_template_field_result\n    create_tag_template_field >> create_tag_template_field_result2\n\n    create_tag >> delete_tag\n    create_tag >> create_tag_result\n    create_tag >> create_tag_result2\n\n    # Delete\n    delete_tasks = [\n        delete_tag,\n        delete_tag_template_field,\n        delete_tag_template,\n        delete_entry_group,\n        delete_entry,\n    ]\n    chain(*delete_tasks)\n\n    # Get\n    create_tag_template >> get_tag_template >> delete_tag_template\n    get_tag_template >> get_tag_template_result\n\n    create_entry_gcs >> get_entry >> delete_entry\n    get_entry >> get_entry_result\n\n    create_entry_group >> get_entry_group >> delete_entry_group\n    get_entry_group >> get_entry_group_result\n\n    # List\n    create_tag >> list_tags >> delete_tag\n    list_tags >> list_tags_result\n\n    # Lookup\n    create_entry_gcs >> lookup_entry >> delete_entry\n    lookup_entry >> lookup_entry_result\n\n    # Rename\n    create_tag_template_field >> rename_tag_template_field >> delete_tag_template_field\n\n    # Search\n    chain(create_tasks, search_catalog, delete_tasks)\n    search_catalog >> search_catalog_result\n\n    # Update\n    create_entry_gcs >> update_entry >> delete_entry\n    create_tag >> update_tag >> delete_tag\n    create_tag_template >> update_tag_template >> delete_tag_template\n    create_tag_template_field >> update_tag_template_field >> rename_tag_template_field", "metadata": {"file_name": "example_complex.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates multiple operational stages with distinct task groups, uses consistent BashOperator for workflow simulation, shows complex workflow structure", "key_concepts": ["Metadata Management", "Workflow Segmentation", "Task Group Organization", "Resource Tracking", "Operational Workflow Design"], "learning_objectives": ["Understand multi-stage workflow design", "Learn task grouping and organization strategies", "Explore systematic resource management approaches"]}}
{"instruction": "Create a data processing workflow that demonstrates task branching with conditional labels, showing how different paths can be taken based on an integrity check.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator"], "line_count": 28, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of labels with different branches.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\nwith DAG(\n    \"example_branch_labels\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    ingest = EmptyOperator(task_id=\"ingest\")\n    analyse = EmptyOperator(task_id=\"analyze\")\n    check = EmptyOperator(task_id=\"check_integrity\")\n    describe = EmptyOperator(task_id=\"describe_integrity\")\n    error = EmptyOperator(task_id=\"email_error\")\n    save = EmptyOperator(task_id=\"save\")\n    report = EmptyOperator(task_id=\"report\")\n\n    ingest >> analyse >> check\n    check >> Label(\"No errors\") >> save >> report\n    check >> Label(\"Errors found\") >> describe >> error >> report", "metadata": {"file_name": "example_branch_labels.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Uses only EmptyOperators, simple sequential flow, and basic branching with labels, making it a straightforward Airflow DAG example", "key_concepts": ["DAG definition", "Task dependencies", "Conditional branching", "Labels in Airflow"], "learning_objectives": ["Understand how to create task dependencies", "Learn to use labels for different workflow paths", "Explore basic Airflow DAG structure"]}}
{"instruction": "Design an error handling workflow that illustrates how to create multiple execution paths in a data pipeline, with explicit error detection and reporting mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator"], "line_count": 28, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of labels with different branches.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\nwith DAG(\n    \"example_branch_labels\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    ingest = EmptyOperator(task_id=\"ingest\")\n    analyse = EmptyOperator(task_id=\"analyze\")\n    check = EmptyOperator(task_id=\"check_integrity\")\n    describe = EmptyOperator(task_id=\"describe_integrity\")\n    error = EmptyOperator(task_id=\"email_error\")\n    save = EmptyOperator(task_id=\"save\")\n    report = EmptyOperator(task_id=\"report\")\n\n    ingest >> analyse >> check\n    check >> Label(\"No errors\") >> save >> report\n    check >> Label(\"Errors found\") >> describe >> error >> report", "metadata": {"file_name": "example_branch_labels.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces conditional logic with branching, demonstrates error handling path, and uses multiple task dependencies beyond simple sequential flow", "key_concepts": ["Error handling in data pipelines", "Conditional task routing", "Workflow error management", "Task dependency visualization"], "learning_objectives": ["Implement branching logic in data workflows", "Create alternative execution paths", "Understand error detection and reporting strategies"]}}
{"instruction": "Create a dynamic task mapping workflow that demonstrates how to transform a list of input values using a custom AddOneOperator and then aggregate the results using a SumItOperator.", "input": {"airflow_version": "2.7.2", "operators": ["AddOneOperator", "SumItOperator"], "line_count": 45, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import BaseOperator\n\n\nclass AddOneOperator(BaseOperator):\n    \"\"\"A custom operator that adds one to the input.\"\"\"\n\n    def __init__(self, value, **kwargs):\n        super().__init__(**kwargs)\n        self.value = value\n\n    def execute(self, context):\n        return self.value + 1\n\n\nclass SumItOperator(BaseOperator):\n    \"\"\"A custom operator that sums the input.\"\"\"\n\n    template_fields = (\"values\",)\n\n    def __init__(self, values, **kwargs):\n        super().__init__(**kwargs)\n        self.values = values\n\n    def execute(self, context):\n        total = sum(self.values)\n        print(f\"Total was {total}\")\n        return total\n\n\nwith DAG(\n    dag_id=\"example_dynamic_task_mapping_with_no_taskflow_operators\",\n    start_date=datetime(2022, 3, 4),\n    catchup=False,\n):\n    # map the task to a list of values\n    add_one_task = AddOneOperator.partial(task_id=\"add_one\").expand(value=[1, 2, 3])\n\n    # aggregate (reduce) the mapped tasks results\n    sum_it_task = SumItOperator(task_id=\"sum_it\", values=add_one_task.output)", "metadata": {"file_name": "example_dynamic_task_mapping_with_no_taskflow_operators.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses custom operators, dynamic task mapping, and involves multiple task transformations with non-standard operator implementations", "key_concepts": ["Dynamic Task Mapping", "Custom Operators", "Task Expansion", "List Transformation", "Airflow 2.x Features"], "learning_objectives": ["Understand dynamic task mapping in Airflow", "Learn how to create custom operators", "Explore task expansion and output mapping", "Practice advanced Airflow task orchestration"]}}
{"instruction": "Design a modular data processing pipeline that demonstrates how to extend BaseOperator to create reusable, configurable task implementations with dynamic input handling.", "input": {"airflow_version": "2.7.2", "operators": ["AddOneOperator", "SumItOperator"], "line_count": 45, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import BaseOperator\n\n\nclass AddOneOperator(BaseOperator):\n    \"\"\"A custom operator that adds one to the input.\"\"\"\n\n    def __init__(self, value, **kwargs):\n        super().__init__(**kwargs)\n        self.value = value\n\n    def execute(self, context):\n        return self.value + 1\n\n\nclass SumItOperator(BaseOperator):\n    \"\"\"A custom operator that sums the input.\"\"\"\n\n    template_fields = (\"values\",)\n\n    def __init__(self, values, **kwargs):\n        super().__init__(**kwargs)\n        self.values = values\n\n    def execute(self, context):\n        total = sum(self.values)\n        print(f\"Total was {total}\")\n        return total\n\n\nwith DAG(\n    dag_id=\"example_dynamic_task_mapping_with_no_taskflow_operators\",\n    start_date=datetime(2022, 3, 4),\n    catchup=False,\n):\n    # map the task to a list of values\n    add_one_task = AddOneOperator.partial(task_id=\"add_one\").expand(value=[1, 2, 3])\n\n    # aggregate (reduce) the mapped tasks results\n    sum_it_task = SumItOperator(task_id=\"sum_it\", values=add_one_task.output)", "metadata": {"file_name": "example_dynamic_task_mapping_with_no_taskflow_operators.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves creating custom operators with specialized logic, implementing dynamic input processing, and demonstrating Airflow's extensibility", "key_concepts": ["BaseOperator Extension", "Custom Operator Design", "Task Flexibility", "Operator Inheritance", "Dynamic Input Processing"], "learning_objectives": ["Learn how to create custom Airflow operators", "Understand operator inheritance and customization", "Practice implementing flexible task logic", "Explore advanced operator design patterns"]}}
{"instruction": "Build an Airflow DAG that retrieves your server's external IP address and sends an email notification with the IP details using custom operators and task decorators.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task", "EmailOperator", "GetRequestOperator"], "line_count": 56, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom typing import Any\n\nimport httpx\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.email import EmailOperator\nfrom airflow.utils.context import Context\n\n\nclass GetRequestOperator(BaseOperator):\n    \"\"\"Custom operator to send GET request to provided url\"\"\"\n\n    def __init__(self, *, url: str, **kwargs):\n        super().__init__(**kwargs)\n        self.url = url\n\n    def execute(self, context: Context):\n        return httpx.get(self.url).json()\n\n\n# [START dag_decorator_usage]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_decorator(email: str = \"example@example.com\"):\n    \"\"\"\n    DAG to send server IP to email.\n\n    :param email: Email to send IP to. Defaults to example@example.com.\n    \"\"\"\n    get_ip = GetRequestOperator(task_id=\"get_ip\", url=\"http://httpbin.org/get\")\n\n    @task(multiple_outputs=True)\n    def prepare_email(raw_json: dict[str, Any]) -> dict[str, str]:\n        external_ip = raw_json[\"origin\"]\n        return {\n            \"subject\": f\"Server connected from {external_ip}\",\n            \"body\": f\"Seems like today your server executing Airflow is connected from IP {external_ip}<br>\",\n        }\n\n    email_info = prepare_email(get_ip.output)\n\n    EmailOperator(\n        task_id=\"send_email\", to=email, subject=email_info[\"subject\"], html_content=email_info[\"body\"]\n    )\n\n\nexample_dag = example_dag_decorator()\n# [END dag_decorator_usage]", "metadata": {"file_name": "example_dag_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses custom operator, external API integration, task decorator, and email notification with multiple transformation steps", "key_concepts": ["Custom Operators", "Task Decorators", "HTTP Requests", "Email Notifications", "TaskFlow API"], "learning_objectives": ["Create custom Airflow operators", "Use task decorators for data transformation", "Integrate external API calls", "Send dynamic email notifications"]}}
{"instruction": "Design an Airflow workflow that demonstrates dynamic task generation and external service interaction by creating a flexible IP tracking and notification system.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task", "EmailOperator", "GetRequestOperator"], "line_count": 56, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom typing import Any\n\nimport httpx\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.email import EmailOperator\nfrom airflow.utils.context import Context\n\n\nclass GetRequestOperator(BaseOperator):\n    \"\"\"Custom operator to send GET request to provided url\"\"\"\n\n    def __init__(self, *, url: str, **kwargs):\n        super().__init__(**kwargs)\n        self.url = url\n\n    def execute(self, context: Context):\n        return httpx.get(self.url).json()\n\n\n# [START dag_decorator_usage]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_decorator(email: str = \"example@example.com\"):\n    \"\"\"\n    DAG to send server IP to email.\n\n    :param email: Email to send IP to. Defaults to example@example.com.\n    \"\"\"\n    get_ip = GetRequestOperator(task_id=\"get_ip\", url=\"http://httpbin.org/get\")\n\n    @task(multiple_outputs=True)\n    def prepare_email(raw_json: dict[str, Any]) -> dict[str, str]:\n        external_ip = raw_json[\"origin\"]\n        return {\n            \"subject\": f\"Server connected from {external_ip}\",\n            \"body\": f\"Seems like today your server executing Airflow is connected from IP {external_ip}<br>\",\n        }\n\n    email_info = prepare_email(get_ip.output)\n\n    EmailOperator(\n        task_id=\"send_email\", to=email, subject=email_info[\"subject\"], html_content=email_info[\"body\"]\n    )\n\n\nexample_dag = example_dag_decorator()\n# [END dag_decorator_usage]", "metadata": {"file_name": "example_dag_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves custom operator design, dynamic data processing, and inter-task communication using Airflow's modern decorators", "key_concepts": ["Dynamic Task Generation", "Operator Extensibility", "Context Passing", "External Service Integration", "Parameterized DAGs"], "learning_objectives": ["Understand operator extensibility", "Learn context passing between tasks", "Implement flexible DAG design", "Handle external API interactions"]}}
{"instruction": "Create a dynamic task mapping workflow that demonstrates how to transform a list of integers by adding 1 to each value and then calculating their total sum using Airflow's TaskFlow API.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 21, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nwith DAG(dag_id=\"example_dynamic_task_mapping\", start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def add_one(x: int):\n        return x + 1\n\n    @task\n    def sum_it(values):\n        total = sum(values)\n        print(f\"Total was {total}\")\n\n    added_values = add_one.expand(x=[1, 2, 3])\n    sum_it(added_values)", "metadata": {"file_name": "example_dynamic_task_mapping.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses dynamic task mapping with expand() method, involves multiple tasks with transformation logic, demonstrates TaskFlow API capabilities", "key_concepts": ["TaskFlow API", "Dynamic Task Mapping", "@task decorator", "List transformation", "Task expansion"], "learning_objectives": ["Understand dynamic task generation", "Learn how to use expand() method", "Explore task chaining with decorators"]}}
{"instruction": "Design an Airflow pipeline that showcases how to dynamically generate parallel tasks from a single input list, demonstrating the flexibility of task mapping and parallel processing in data workflows.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 21, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of dynamic task mapping.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nwith DAG(dag_id=\"example_dynamic_task_mapping\", start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def add_one(x: int):\n        return x + 1\n\n    @task\n    def sum_it(values):\n        total = sum(values)\n        print(f\"Total was {total}\")\n\n    added_values = add_one.expand(x=[1, 2, 3])\n    sum_it(added_values)", "metadata": {"file_name": "example_dynamic_task_mapping.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Advanced task mapping technique, uses functional programming concepts, demonstrates Airflow's ability to handle dynamic task generation", "key_concepts": ["Parallel Task Execution", "List Comprehension", "Task Mapping", "Functional Programming", "Dynamic Workflow Design"], "learning_objectives": ["Understand parallel task generation", "Learn dynamic workflow construction", "Explore Airflow's flexible task mapping"]}}
{"instruction": "Create a nested branching workflow that demonstrates conditional task execution using Airflow's branch task and trigger rules, showing how tasks can dynamically select execution paths based on runtime conditions.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 39, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating a workflow with nested branching. The join tasks are created with\n``none_failed_min_one_success`` trigger rule such that they are skipped whenever their corresponding\nbranching tasks are skipped.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_nested_branch_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\"],\n) as dag:\n\n    @task.branch()\n    def branch(task_id_to_return: str) -> str:\n        return task_id_to_return\n\n    branch_1 = branch.override(task_id=\"branch_1\")(task_id_to_return=\"true_1\")\n    join_1 = EmptyOperator(task_id=\"join_1\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_1 = EmptyOperator(task_id=\"true_1\")\n    false_1 = EmptyOperator(task_id=\"false_1\")\n\n    branch_2 = branch.override(task_id=\"branch_2\")(task_id_to_return=\"true_2\")\n    join_2 = EmptyOperator(task_id=\"join_2\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_2 = EmptyOperator(task_id=\"true_2\")\n    false_2 = EmptyOperator(task_id=\"false_2\")\n    false_3 = EmptyOperator(task_id=\"false_3\")\n\n    branch_1 >> true_1 >> join_1\n    branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1", "metadata": {"file_name": "example_nested_branch_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses branch task decorator, multiple task paths, custom trigger rules, and demonstrates nested branching logic beyond simple sequential execution", "key_concepts": ["Branch Task Decorator", "Trigger Rules", "Conditional Task Execution", "Task Dependencies"], "learning_objectives": ["Understand dynamic task routing", "Learn how to create conditional workflow paths", "Explore advanced Airflow task dependency management"]}}
{"instruction": "Design a workflow that illustrates advanced task dependency management using join tasks and complex branching strategies, focusing on how Airflow handles task skipping and conditional execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmptyOperator"], "line_count": 39, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating a workflow with nested branching. The join tasks are created with\n``none_failed_min_one_success`` trigger rule such that they are skipped whenever their corresponding\nbranching tasks are skipped.\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_nested_branch_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\"],\n) as dag:\n\n    @task.branch()\n    def branch(task_id_to_return: str) -> str:\n        return task_id_to_return\n\n    branch_1 = branch.override(task_id=\"branch_1\")(task_id_to_return=\"true_1\")\n    join_1 = EmptyOperator(task_id=\"join_1\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_1 = EmptyOperator(task_id=\"true_1\")\n    false_1 = EmptyOperator(task_id=\"false_1\")\n\n    branch_2 = branch.override(task_id=\"branch_2\")(task_id_to_return=\"true_2\")\n    join_2 = EmptyOperator(task_id=\"join_2\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_2 = EmptyOperator(task_id=\"true_2\")\n    false_2 = EmptyOperator(task_id=\"false_2\")\n    false_3 = EmptyOperator(task_id=\"false_3\")\n\n    branch_1 >> true_1 >> join_1\n    branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1", "metadata": {"file_name": "example_nested_branch_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements non-standard trigger rules, demonstrates task skipping behavior, and shows complex task dependency resolution", "key_concepts": ["NONE_FAILED_MIN_ONE_SUCCESS Trigger Rule", "Task Skipping", "Nested Branching", "Task Dependency Resolution"], "learning_objectives": ["Understand advanced trigger rule behaviors", "Learn how Airflow manages complex task dependencies", "Explore task skipping and conditional execution strategies"]}}
{"instruction": "Build a DAG that demonstrates how the LatestOnlyOperator can be used to ensure that a task only runs during the most recent scheduled interval, preventing unnecessary task execution for backfilled or historical runs.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "LatestOnlyOperator"], "line_count": 20, "is_multifile": false}, "output": "\"\"\"Example of the LatestOnlyOperator\"\"\"\nfrom __future__ import annotations\n\nimport datetime as dt\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\n\nwith DAG(\n    dag_id=\"latest_only\",\n    schedule=dt.timedelta(hours=4),\n    start_date=dt.datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example2\", \"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n\n    latest_only >> task1", "metadata": {"file_name": "example_latest_only.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with only two operators, basic scheduling, and a single dependency. Uses standard Airflow imports and a straightforward task structure.", "key_concepts": ["LatestOnlyOperator", "DAG scheduling", "Task dependencies", "Airflow scheduling logic"], "learning_objectives": ["Understand LatestOnlyOperator behavior", "Learn how to prevent task execution in historical runs", "Explore basic DAG configuration"]}}
{"instruction": "Create a workflow that illustrates conditional task execution based on the most recent DAG run, demonstrating how to optimize resource usage by skipping unnecessary historical task processing.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "LatestOnlyOperator"], "line_count": 20, "is_multifile": false}, "output": "\"\"\"Example of the LatestOnlyOperator\"\"\"\nfrom __future__ import annotations\n\nimport datetime as dt\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\n\nwith DAG(\n    dag_id=\"latest_only\",\n    schedule=dt.timedelta(hours=4),\n    start_date=dt.datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example2\", \"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n\n    latest_only >> task1", "metadata": {"file_name": "example_latest_only.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Minimal complexity with only two operators, focusing on a specific Airflow scheduling feature that prevents redundant task execution.", "key_concepts": ["Conditional task execution", "Scheduling optimization", "DAG run filtering", "Task skipping mechanisms"], "learning_objectives": ["Understand task execution filtering", "Learn resource optimization techniques", "Explore Airflow's run-time decision making"]}}
{"instruction": "Create an interactive DAG that generates multilingual greetings for a list of names, allowing users to dynamically select greeting languages through a trigger UI form.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 87, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage DAG params to model a trigger UI with a user form.\n\nThis example DAG generates greetings to a list of provided names in selected languages in the logs.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        \"names\": Param(\n            [\"Linda\", \"Martha\", \"Thomas\"],\n            type=\"array\",\n            description=\"Define the list of names for which greetings should be generated in the logs.\"\n            \" Please have one name per line.\",\n            title=\"Names to greet\",\n        ),\n        \"english\": Param(True, type=\"boolean\", title=\"English\"),\n        \"german\": Param(True, type=\"boolean\", title=\"German (Formal)\"),\n        \"french\": Param(True, type=\"boolean\", title=\"French\"),\n    },\n) as dag:\n\n    @task(task_id=\"get_names\")\n    def get_names(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if \"names\" not in dag_run.conf:\n            print(\"Uuups, no names given, was no UI used to trigger?\")\n            return []\n        return dag_run.conf[\"names\"]\n\n    @task.branch(task_id=\"select_languages\")\n    def select_languages(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        selected_languages = []\n        for lang in [\"english\", \"german\", \"french\"]:\n            if lang in dag_run.conf and dag_run.conf[lang]:\n                selected_languages.append(f\"generate_{lang}_greeting\")\n        return selected_languages\n\n    @task(task_id=\"generate_english_greeting\")\n    def generate_english_greeting(name: str) -> str:\n        return f\"Hello {name}!\"\n\n    @task(task_id=\"generate_german_greeting\")\n    def generate_german_greeting(name: str) -> str:\n        return f\"Sehr geehrter Herr/Frau {name}.\"\n\n    @task(task_id=\"generate_french_greeting\")\n    def generate_french_greeting(name: str) -> str:\n        return f\"Bonjour {name}!\"\n\n    @task(task_id=\"print_greetings\", trigger_rule=TriggerRule.ALL_DONE)\n    def print_greetings(greetings1, greetings2, greetings3) -> None:\n        for g in greetings1 if greetings1 else []:\n            print(g)\n        for g in greetings2 if greetings2 else []:\n            print(g)\n        for g in greetings3 if greetings3 else []:\n            print(g)\n        if not greetings1 and not greetings2 and not greetings3:\n            print(\"sad, nobody to greet :-(\")\n\n    lang_select = select_languages()\n    names = get_names()\n    english_greetings = generate_english_greeting.expand(name=names)\n    german_greetings = generate_german_greeting.expand(name=names)\n    french_greetings = generate_french_greeting.expand(name=names)\n    lang_select >> [english_greetings, german_greetings, french_greetings]\n    results_print = print_greetings(english_greetings, german_greetings, french_greetings)", "metadata": {"file_name": "example_params_trigger_ui.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses TaskFlow API, dynamic task expansion, branching logic, and custom parameter configuration with multiple language options", "key_concepts": ["TaskFlow API", "Dynamic Task Mapping", "Branching Tasks", "DAG Parameters", "Trigger UI Configuration"], "learning_objectives": ["Understand dynamic task generation", "Learn how to create interactive DAG configurations", "Explore task branching and conditional execution", "Practice using Airflow decorators and parameters"]}}
{"instruction": "Design a flexible greeting generation workflow that demonstrates advanced Airflow task orchestration techniques, focusing on runtime configuration and dynamic task selection.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 87, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage DAG params to model a trigger UI with a user form.\n\nThis example DAG generates greetings to a list of provided names in selected languages in the logs.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        \"names\": Param(\n            [\"Linda\", \"Martha\", \"Thomas\"],\n            type=\"array\",\n            description=\"Define the list of names for which greetings should be generated in the logs.\"\n            \" Please have one name per line.\",\n            title=\"Names to greet\",\n        ),\n        \"english\": Param(True, type=\"boolean\", title=\"English\"),\n        \"german\": Param(True, type=\"boolean\", title=\"German (Formal)\"),\n        \"french\": Param(True, type=\"boolean\", title=\"French\"),\n    },\n) as dag:\n\n    @task(task_id=\"get_names\")\n    def get_names(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if \"names\" not in dag_run.conf:\n            print(\"Uuups, no names given, was no UI used to trigger?\")\n            return []\n        return dag_run.conf[\"names\"]\n\n    @task.branch(task_id=\"select_languages\")\n    def select_languages(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        selected_languages = []\n        for lang in [\"english\", \"german\", \"french\"]:\n            if lang in dag_run.conf and dag_run.conf[lang]:\n                selected_languages.append(f\"generate_{lang}_greeting\")\n        return selected_languages\n\n    @task(task_id=\"generate_english_greeting\")\n    def generate_english_greeting(name: str) -> str:\n        return f\"Hello {name}!\"\n\n    @task(task_id=\"generate_german_greeting\")\n    def generate_german_greeting(name: str) -> str:\n        return f\"Sehr geehrter Herr/Frau {name}.\"\n\n    @task(task_id=\"generate_french_greeting\")\n    def generate_french_greeting(name: str) -> str:\n        return f\"Bonjour {name}!\"\n\n    @task(task_id=\"print_greetings\", trigger_rule=TriggerRule.ALL_DONE)\n    def print_greetings(greetings1, greetings2, greetings3) -> None:\n        for g in greetings1 if greetings1 else []:\n            print(g)\n        for g in greetings2 if greetings2 else []:\n            print(g)\n        for g in greetings3 if greetings3 else []:\n            print(g)\n        if not greetings1 and not greetings2 and not greetings3:\n            print(\"sad, nobody to greet :-(\")\n\n    lang_select = select_languages()\n    names = get_names()\n    english_greetings = generate_english_greeting.expand(name=names)\n    german_greetings = generate_german_greeting.expand(name=names)\n    french_greetings = generate_french_greeting.expand(name=names)\n    lang_select >> [english_greetings, german_greetings, french_greetings]\n    results_print = print_greetings(english_greetings, german_greetings, french_greetings)", "metadata": {"file_name": "example_params_trigger_ui.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements complex task dependencies, runtime parameter parsing, and conditional task execution with multiple language options", "key_concepts": ["Runtime Configuration", "Task Dependency Management", "Conditional Task Execution", "Trigger Rule Handling", "Expandable Tasks"], "learning_objectives": ["Master dynamic task dependency management", "Learn how to create flexible workflow configurations", "Understand runtime parameter processing", "Explore advanced task selection strategies"]}}
{"instruction": "Create a DAG that demonstrates how the LatestOnlyOperator controls task execution based on the most recent scheduled run, exploring task dependencies and trigger rules in Airflow.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "LatestOnlyOperator"], "line_count": 31, "is_multifile": false}, "output": "\"\"\"\nExample LatestOnlyOperator and TriggerRule interactions\n\"\"\"\nfrom __future__ import annotations\n\n# [START example]\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=TriggerRule.ALL_DONE)\n\n    latest_only >> task1 >> [task3, task4]\n    task2 >> [task3, task4]\n# [END example]", "metadata": {"file_name": "example_latest_only_with_trigger.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with EmptyOperators, basic task dependencies, and a single LatestOnlyOperator with minimal code complexity", "key_concepts": ["LatestOnlyOperator", "Task Dependencies", "Trigger Rules", "Scheduling"], "learning_objectives": ["Understand how LatestOnlyOperator filters task execution", "Learn about different task dependency patterns", "Explore Airflow trigger rule configurations"]}}
{"instruction": "Design a workflow that illustrates how different trigger rules and task dependencies interact, focusing on controlling task execution based on upstream task statuses.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "LatestOnlyOperator"], "line_count": 31, "is_multifile": false}, "output": "\"\"\"\nExample LatestOnlyOperator and TriggerRule interactions\n\"\"\"\nfrom __future__ import annotations\n\n# [START example]\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=TriggerRule.ALL_DONE)\n\n    latest_only >> task1 >> [task3, task4]\n    task2 >> [task3, task4]\n# [END example]", "metadata": {"file_name": "example_latest_only_with_trigger.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces more advanced trigger rule concepts with multiple task interactions and conditional execution logic", "key_concepts": ["TriggerRule", "Task Branching", "Conditional Execution", "Airflow Task States"], "learning_objectives": ["Understand how TriggerRule affects task execution", "Learn to create complex task dependency graphs", "Explore different task execution scenarios"]}}
{"instruction": "Build a DAG that demonstrates multiple Python task execution strategies, including standard task decorators, SQL template rendering, and virtual environment task execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "ExternalPythonOperator", "PythonVirtualenvOperator"], "line_count": 141, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a\nvirtual environment.\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport tempfile\nimport time\nfrom pprint import pprint\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import ExternalPythonOperator, PythonVirtualenvOperator, is_venv_installed\n\nlog = logging.getLogger(__name__)\n\nPATH_TO_PYTHON_BINARY = sys.executable\n\nBASE_DIR = tempfile.gettempdir()\n\n\ndef x():\n    pass\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_python]\n    @task(task_id=\"print_the_context\")\n    def print_context(ds=None, **kwargs):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        pprint(kwargs)\n        print(ds)\n        return \"Whatever you return gets printed in the logs\"\n\n    run_this = print_context()\n    # [END howto_operator_python]\n\n    # [START howto_operator_python_render_sql]\n    @task(task_id=\"log_sql_query\", templates_dict={\"query\": \"sql/sample.sql\"}, templates_exts=[\".sql\"])\n    def log_sql(**kwargs):\n        logging.info(\"Python task decorator query: %s\", str(kwargs[\"templates_dict\"][\"query\"]))\n\n    log_the_sql = log_sql()\n    # [END howto_operator_python_render_sql]\n\n    # [START howto_operator_python_kwargs]\n    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively\n    for i in range(5):\n\n        @task(task_id=f\"sleep_for_{i}\")\n        def my_sleeping_function(random_base):\n            \"\"\"This is a function that will run within the DAG execution\"\"\"\n            time.sleep(random_base)\n\n        sleeping_task = my_sleeping_function(random_base=i / 10)\n\n        run_this >> log_the_sql >> sleeping_task\n    # [END howto_operator_python_kwargs]\n\n    if not is_venv_installed():\n        log.warning(\"The virtalenv_python example task requires virtualenv, please install it.\")\n    else:\n        # [START howto_operator_python_venv]\n        @task.virtualenv(\n            task_id=\"virtualenv_python\", requirements=[\"colorama==0.4.0\"], system_site_packages=False\n        )\n        def callable_virtualenv():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            from time import sleep\n\n            from colorama import Back, Fore, Style\n\n            print(Fore.RED + \"some red text\")\n            print(Back.GREEN + \"and with a green background\")\n            print(Style.DIM + \"and in dim text\")\n            print(Style.RESET_ALL)\n            for _ in range(4):\n                print(Style.DIM + \"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        virtualenv_task = callable_virtualenv()\n        # [END howto_operator_python_venv]\n\n        sleeping_task >> virtualenv_task\n\n        # [START howto_operator_external_python]\n        @task.external_python(task_id=\"external_python\", python=PATH_TO_PYTHON_BINARY)\n        def callable_external_python():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            import sys\n            from time import sleep\n\n            print(f\"Running task via {sys.executable}\")\n            print(\"Sleeping\")\n            for _ in range(4):\n                print(\"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        external_python_task = callable_external_python()\n        # [END howto_operator_external_python]\n\n        # [START howto_operator_external_python_classic]\n        external_classic = ExternalPythonOperator(\n            task_id=\"external_python_classic\",\n            python=PATH_TO_PYTHON_BINARY,\n            python_callable=x,\n        )\n        # [END howto_operator_external_python_classic]\n\n        # [START howto_operator_python_venv_classic]\n        virtual_classic = PythonVirtualenvOperator(\n            task_id=\"virtualenv_classic\",\n            requirements=\"colorama==0.4.0\",\n            python_callable=x,\n        )\n        # [END howto_operator_python_venv_classic]\n\n        run_this >> external_classic >> external_python_task >> virtual_classic", "metadata": {"file_name": "example_python_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple task decorators, includes virtual environment execution, demonstrates template rendering, and has dynamic task generation with loops", "key_concepts": ["TaskFlow API", "Task Decorators", "Virtual Environment Tasks", "Context Passing", "Dynamic Task Generation"], "learning_objectives": ["Understand different Python task execution methods", "Learn how to use task decorators", "Explore context passing in Airflow tasks", "Practice creating dynamic task workflows"]}}
{"instruction": "Create a DAG that showcases advanced Python operator techniques, focusing on executing tasks in isolated environments and demonstrating flexible task configuration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "ExternalPythonOperator", "PythonVirtualenvOperator"], "line_count": 141, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a\nvirtual environment.\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport tempfile\nimport time\nfrom pprint import pprint\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import ExternalPythonOperator, PythonVirtualenvOperator, is_venv_installed\n\nlog = logging.getLogger(__name__)\n\nPATH_TO_PYTHON_BINARY = sys.executable\n\nBASE_DIR = tempfile.gettempdir()\n\n\ndef x():\n    pass\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_python]\n    @task(task_id=\"print_the_context\")\n    def print_context(ds=None, **kwargs):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        pprint(kwargs)\n        print(ds)\n        return \"Whatever you return gets printed in the logs\"\n\n    run_this = print_context()\n    # [END howto_operator_python]\n\n    # [START howto_operator_python_render_sql]\n    @task(task_id=\"log_sql_query\", templates_dict={\"query\": \"sql/sample.sql\"}, templates_exts=[\".sql\"])\n    def log_sql(**kwargs):\n        logging.info(\"Python task decorator query: %s\", str(kwargs[\"templates_dict\"][\"query\"]))\n\n    log_the_sql = log_sql()\n    # [END howto_operator_python_render_sql]\n\n    # [START howto_operator_python_kwargs]\n    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively\n    for i in range(5):\n\n        @task(task_id=f\"sleep_for_{i}\")\n        def my_sleeping_function(random_base):\n            \"\"\"This is a function that will run within the DAG execution\"\"\"\n            time.sleep(random_base)\n\n        sleeping_task = my_sleeping_function(random_base=i / 10)\n\n        run_this >> log_the_sql >> sleeping_task\n    # [END howto_operator_python_kwargs]\n\n    if not is_venv_installed():\n        log.warning(\"The virtalenv_python example task requires virtualenv, please install it.\")\n    else:\n        # [START howto_operator_python_venv]\n        @task.virtualenv(\n            task_id=\"virtualenv_python\", requirements=[\"colorama==0.4.0\"], system_site_packages=False\n        )\n        def callable_virtualenv():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            from time import sleep\n\n            from colorama import Back, Fore, Style\n\n            print(Fore.RED + \"some red text\")\n            print(Back.GREEN + \"and with a green background\")\n            print(Style.DIM + \"and in dim text\")\n            print(Style.RESET_ALL)\n            for _ in range(4):\n                print(Style.DIM + \"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        virtualenv_task = callable_virtualenv()\n        # [END howto_operator_python_venv]\n\n        sleeping_task >> virtualenv_task\n\n        # [START howto_operator_external_python]\n        @task.external_python(task_id=\"external_python\", python=PATH_TO_PYTHON_BINARY)\n        def callable_external_python():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            import sys\n            from time import sleep\n\n            print(f\"Running task via {sys.executable}\")\n            print(\"Sleeping\")\n            for _ in range(4):\n                print(\"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        external_python_task = callable_external_python()\n        # [END howto_operator_external_python]\n\n        # [START howto_operator_external_python_classic]\n        external_classic = ExternalPythonOperator(\n            task_id=\"external_python_classic\",\n            python=PATH_TO_PYTHON_BINARY,\n            python_callable=x,\n        )\n        # [END howto_operator_external_python_classic]\n\n        # [START howto_operator_python_venv_classic]\n        virtual_classic = PythonVirtualenvOperator(\n            task_id=\"virtualenv_classic\",\n            requirements=\"colorama==0.4.0\",\n            python_callable=x,\n        )\n        # [END howto_operator_python_venv_classic]\n\n        run_this >> external_classic >> external_python_task >> virtual_classic", "metadata": {"file_name": "example_python_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Utilizes advanced task execution strategies like external Python and virtualenv, includes complex task dependencies and configuration", "key_concepts": ["External Python Operator", "Virtual Environment Tasks", "Task Dependencies", "Dynamic Task Configuration", "Isolated Execution Environments"], "learning_objectives": ["Master advanced task isolation techniques", "Understand external environment task execution", "Learn flexible task configuration strategies", "Explore complex task dependency management"]}}
{"instruction": "Create an interactive Airflow DAG that demonstrates dynamic parameter configuration using the Airflow UI, focusing on how to leverage the `Param` class to generate flexible trigger forms with different input types and validation.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 295, "is_multifile": false}, "output": "\"\"\"DAG demonstrating various options for a trigger form generated by DAG params.\n\nThe DAG attribute `params` is used to define a default dictionary of parameters which are usually passed\nto the DAG and which are used to render a trigger form.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport json\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        # Let's start simple: Standard dict values are detected from type and offered as entry form fields.\n        # Detected types are numbers, text, boolean, lists and dicts.\n        # Note that such auto-detected parameters are treated as optional (not required to contain a value)\n        \"x\": 3,\n        \"text\": \"Hello World!\",\n        \"flag\": False,\n        \"a_simple_list\": [\"one\", \"two\", \"three\", \"actually one value is made per line\"],\n        # But of course you might want to have it nicer! Let's add some description to parameters.\n        # Note if you can add any HTML formatting to the description, you need to use the description_html\n        # attribute.\n        \"most_loved_number\": Param(\n            42,\n            type=\"integer\",\n            title=\"Your favorite number\",\n            description_html=\"\"\"Everybody should have a favorite number. Not only math teachers.\n            If you can not think of any at the moment please think of the 42 which is very famous because\n            of the book\n            <a href='https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#\n            The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42'>\n            The Hitchhiker's Guide to the Galaxy</a>\"\"\",\n        ),\n        # If you want to have a selection list box then you can use the enum feature of JSON schema\n        \"pick_one\": Param(\n            \"value 42\",\n            type=\"string\",\n            title=\"Select one Value\",\n            description=\"You can use JSON schema enum's to generate drop down selection boxes.\",\n            enum=[f\"value {i}\" for i in range(16, 64)],\n        ),\n        # You can also label the selected values via values_display attribute\n        \"pick_with_label\": Param(\n            3,\n            type=\"number\",\n            title=\"Select one Number\",\n            description=\"With drop down selections you can also have nice display labels for the values.\",\n            enum=[*range(1, 10)],\n            values_display={\n                1: \"One\",\n                2: \"Two\",\n                3: \"Three\",\n                4: \"Four - is like you take three and get one for free!\",\n                5: \"Five\",\n                6: \"Six\",\n                7: \"Seven\",\n                8: \"Eight\",\n                9: \"Nine\",\n            },\n        ),\n        # If you want to have a list box with proposals but not enforcing a fixed list\n        # then you can use the examples feature of JSON schema\n        \"proposals\": Param(\n            \"some value\",\n            type=\"string\",\n            title=\"Field with proposals\",\n            description=\"You can use JSON schema examples's to generate drop down selection boxes \"\n            \"but allow also to enter custom values. Try typing an 'a' and see options.\",\n            examples=(\n                \"Alpha,Bravo,Charlie,Delta,Echo,Foxtrot,Golf,Hotel,India,Juliett,Kilo,Lima,Mike,November,Oscar,Papa,\"\n                \"Quebec,Romeo,Sierra,Tango,Uniform,Victor,Whiskey,X-ray,Yankee,Zulu\"\n            ).split(\",\"),\n        ),\n        # If you want to select multiple items from a fixed list JSON schema des not allow to use enum\n        # In this case the type \"array\" is being used together with \"examples\" as pick list\n        \"multi_select\": Param(\n            [\"two\", \"three\"],\n            \"Select from the list of options.\",\n            type=\"array\",\n            title=\"Multi Select\",\n            examples=[\"one\", \"two\", \"three\", \"four\", \"five\"],\n        ),\n        # A multiple options selection can also be combined with values_display\n        \"multi_select_with_label\": Param(\n            [\"2\", \"3\"],\n            \"Select from the list of options. See that options can have nicer text and still technical values\"\n            \"are propagated as values during trigger to the DAG.\",\n            type=\"array\",\n            title=\"Multi Select with Labels\",\n            examples=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n            values_display={\n                \"1\": \"One box of choccolate\",\n                \"2\": \"Two bananas\",\n                \"3\": \"Three apples\",\n                # Note: Value display mapping does not need to be complete.s\n            },\n        ),\n        # An array of numbers\n        \"array_of_numbers\": Param(\n            [1, 2, 3],\n            \"Only integers are accepted in this array\",\n            type=\"array\",\n            title=\"Array of numbers\",\n            items={\"type\": \"number\"},\n        ),\n        # Boolean as proper parameter with description\n        \"bool\": Param(\n            True,\n            type=\"boolean\",\n            title=\"Please confirm\",\n            description=\"A On/Off selection with a proper description.\",\n        ),\n        # Dates and Times are also supported\n        \"date_time\": Param(\n            f\"{datetime.date.today()}T{datetime.time(hour=12, minute=17, second=00)}+00:00\",\n            type=\"string\",\n            format=\"date-time\",\n            title=\"Date-Time Picker\",\n            description=\"Please select a date and time, use the button on the left for a pup-up calendar.\",\n        ),\n        \"date\": Param(\n            f\"{datetime.date.today()}\",\n            type=\"string\",\n            format=\"date\",\n            title=\"Date Picker\",\n            description=\"Please select a date, use the button on the left for a pup-up calendar. \"\n            \"See that here are no times!\",\n        ),\n        \"time\": Param(\n            f\"{datetime.time(hour=12, minute=13, second=14)}\",\n            type=[\"string\", \"null\"],\n            format=\"time\",\n            title=\"Time Picker\",\n            description=\"Please select a time, use the button on the left for a pup-up tool.\",\n        ),\n        # Fields can be required or not. If the defined fields are typed they are getting required by default\n        # (else they would not pass JSON schema validation) - to make typed fields optional you must\n        # permit the optional \"null\" type\n        \"required_field\": Param(\n            \"You can not trigger if no text is given here!\",\n            type=\"string\",\n            title=\"Required text field\",\n            description=\"This field is required. You can not submit without having text in here.\",\n        ),\n        \"optional_field\": Param(\n            \"optional text, you can trigger also w/o text\",\n            type=[\"null\", \"string\"],\n            title=\"Optional text field\",\n            description_html=\"This field is optional. As field content is JSON schema validated you must \"\n            \"allow the <code>null</code> type.\",\n        ),\n        # You can arrange the entry fields in sections so that you can have a better overview for the user\n        # Therefore you can add the \"section\" attribute.\n        # The benefit of the Params class definition is that the full scope of JSON schema validation\n        # can be leveraged for form fields and they will be validated before DAG submission.\n        \"checked_text\": Param(\n            \"length-checked-field\",\n            type=\"string\",\n            title=\"Text field with length check\",\n            description_html=\"\"\"This field is required. And you need to provide something between 10 and 30\n            characters. See the\n            <a href='https://json-schema.org/understanding-json-schema/reference/string.html'>\n            JSON schema description (string)</a> in for more details\"\"\",\n            minLength=10,\n            maxLength=20,\n            section=\"JSON Schema validation options\",\n        ),\n        \"checked_number\": Param(\n            100,\n            type=\"number\",\n            title=\"Number field with value check\",\n            description_html=\"\"\"This field is required. You need to provide any number between 64 and 128.\n            See the <a href='https://json-schema.org/understanding-json-schema/reference/numeric.html'>\n            JSON schema description (numbers)</a> in for more details\"\"\",\n            minimum=64,\n            maximum=128,\n            section=\"JSON Schema validation options\",\n        ),\n        # Some further cool stuff as advanced options are also possible\n        # You can have the user entering a dict object as a JSON with validation\n        \"object\": Param(\n            {\"key\": \"value\"},\n            type=[\"object\", \"null\"],\n            title=\"JSON entry field\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n        \"array_of_objects\": Param(\n            [{\"name\": \"account_name\", \"country\": \"country_name\"}],\n            \"Array with complex objects and validation rules. \"\n            \"See <a href='https://json-schema.org/understanding-json-schema\"\n            \"/reference/array.html#items'>JSON Schema validation options in specs.</a>\",\n            type=\"array\",\n            title=\"JSON array field\",\n            items={\n                \"type\": \"object\",\n                \"properties\": {\"name\": {\"type\": \"string\"}, \"country_name\": {\"type\": \"string\"}},\n                \"required\": [\"name\"],\n            },\n            section=\"Special advanced stuff with form fields\",\n        ),\n        # If you want to have static parameters which are always passed and not editable by the user\n        # then you can use the JSON schema option of passing constant values. These parameters\n        # will not be displayed but passed to the DAG\n        \"hidden_secret_field\": Param(\"constant value\", const=\"constant value\"),\n        # Finally besides the standard provided field generator you can have you own HTML form code\n        # injected - but be careful, you can also mess-up the layout!\n        \"color_picker\": Param(\n            \"#FF8800\",\n            type=\"string\",\n            title=\"Pick a color\",\n            description_html=\"\"\"This is a special HTML widget as custom implementation in the DAG code.\n            It is templated with the following parameter to render proper HTML form fields:\n            <ul>\n                <li><code>{name}</code>: Name of the HTML input field that is expected.</li>\n                <li><code>{value}</code>:\n                    (Default) value that should be displayed when showing/loading the form.</li>\n                <li>Note: If you have elements changing a value, call <code>updateJSONconf()</code> to update\n                    the form data to be posted as <code>dag_run.conf</code>.</li>\n            </ul>\n            Example: <code>&lt;input name='{name}' value='{value}' onchange='updateJSONconf()' /&gt;</code>\n            \"\"\",\n            custom_html_form=\"\"\"\n            <table width=\"100%\" cellspacing=\"5\"><tbody><tr><td>\n                <label for=\"r_{name}\">Red:</label>\n            </td><td width=\"80%\">\n                <input id=\"r_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td><td rowspan=\"3\" style=\"padding-left: 10px;\">\n                <div id=\"preview_{name}\"\n                style=\"line-height: 40px; margin-bottom: 7px; width: 100%; background-color: {value};\"\n                >&nbsp;</div>\n                <input class=\"form-control\" type=\"text\" maxlength=\"7\" id=\"{name}\" name=\"{name}\"\n                value=\"{value}\" onchange=\"v_{name}()\" />\n            </td></tr><tr><td>\n                <label for=\"g_{name}\">Green:</label>\n            </td><td>\n                <input id=\"g_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr><tr><td>\n                <label for=\"b_{name}\">Blue:</label>\n            </td><td>\n                <input id=\"b_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr></tbody></table>\n            <script lang=\"javascript\">\n                const hex_chars = \"0123456789ABCDEF\";\n                function i2hex(name) {\n                    var i = document.getElementById(name).value;\n                    return hex_chars.substr(parseInt(i / 16), 1) + hex_chars.substr(parseInt(i % 16), 1)\n                }\n                function u_{name}() {\n                    var hex_val = \"#\"+i2hex(\"r_{name}\")+i2hex(\"g_{name}\")+i2hex(\"b_{name}\");\n                    document.getElementById(\"{name}\").value = hex_val;\n                    document.getElementById(\"preview_{name}\").style.background = hex_val;\n                    updateJSONconf();\n                }\n                function hex2i(text) {\n                    return hex_chars.indexOf(text.substr(0,1)) * 16 + hex_chars.indexOf(text.substr(1,1));\n                }\n                function v_{name}() {\n                    var value = document.getElementById(\"{name}\").value.toUpperCase();\n                    document.getElementById(\"r_{name}\").value = hex2i(value.substr(1,2));\n                    document.getElementById(\"g_{name}\").value = hex2i(value.substr(3,2));\n                    document.getElementById(\"b_{name}\").value = hex2i(value.substr(5,2));\n                    document.getElementById(\"preview_{name}\").style.background = value;\n                }\n                v_{name}();\n            </script>\"\"\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n    },\n) as dag:\n\n    @task(task_id=\"show_params\")\n    def show_params(**kwargs) -> None:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if not dag_run.conf:\n            print(\"Uups, no parameters supplied as DagRun.conf, was the trigger w/o form?\")\n            raise AirflowSkipException(\"No DagRun.conf parameters supplied.\")\n        print(f\"This DAG was triggered with the following parameters:\\n{json.dumps(dag_run.conf, indent=4)}\")\n\n    show_params()", "metadata": {"file_name": "example_params_ui_tutorial.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses advanced Param configuration with multiple input types, enum selections, and custom descriptions, requiring understanding of JSON schema and Airflow parameter rendering", "key_concepts": ["Airflow Parameters", "DAG Configuration", "UI Trigger Forms", "JSON Schema", "Dynamic Input Validation"], "learning_objectives": ["Understand how to create dynamic DAG parameters", "Learn to configure input types and constraints", "Explore UI-driven DAG parameter generation"]}}
{"instruction": "Design a comprehensive parameter configuration tutorial that explores different ways of defining input parameters in Airflow, demonstrating how to create rich, user-friendly trigger interfaces with type checking, descriptions, and selection mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 295, "is_multifile": false}, "output": "\"\"\"DAG demonstrating various options for a trigger form generated by DAG params.\n\nThe DAG attribute `params` is used to define a default dictionary of parameters which are usually passed\nto the DAG and which are used to render a trigger form.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport json\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\nfrom airflow.models.taskinstance import TaskInstance\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example_ui\"],\n    params={\n        # Let's start simple: Standard dict values are detected from type and offered as entry form fields.\n        # Detected types are numbers, text, boolean, lists and dicts.\n        # Note that such auto-detected parameters are treated as optional (not required to contain a value)\n        \"x\": 3,\n        \"text\": \"Hello World!\",\n        \"flag\": False,\n        \"a_simple_list\": [\"one\", \"two\", \"three\", \"actually one value is made per line\"],\n        # But of course you might want to have it nicer! Let's add some description to parameters.\n        # Note if you can add any HTML formatting to the description, you need to use the description_html\n        # attribute.\n        \"most_loved_number\": Param(\n            42,\n            type=\"integer\",\n            title=\"Your favorite number\",\n            description_html=\"\"\"Everybody should have a favorite number. Not only math teachers.\n            If you can not think of any at the moment please think of the 42 which is very famous because\n            of the book\n            <a href='https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#\n            The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42'>\n            The Hitchhiker's Guide to the Galaxy</a>\"\"\",\n        ),\n        # If you want to have a selection list box then you can use the enum feature of JSON schema\n        \"pick_one\": Param(\n            \"value 42\",\n            type=\"string\",\n            title=\"Select one Value\",\n            description=\"You can use JSON schema enum's to generate drop down selection boxes.\",\n            enum=[f\"value {i}\" for i in range(16, 64)],\n        ),\n        # You can also label the selected values via values_display attribute\n        \"pick_with_label\": Param(\n            3,\n            type=\"number\",\n            title=\"Select one Number\",\n            description=\"With drop down selections you can also have nice display labels for the values.\",\n            enum=[*range(1, 10)],\n            values_display={\n                1: \"One\",\n                2: \"Two\",\n                3: \"Three\",\n                4: \"Four - is like you take three and get one for free!\",\n                5: \"Five\",\n                6: \"Six\",\n                7: \"Seven\",\n                8: \"Eight\",\n                9: \"Nine\",\n            },\n        ),\n        # If you want to have a list box with proposals but not enforcing a fixed list\n        # then you can use the examples feature of JSON schema\n        \"proposals\": Param(\n            \"some value\",\n            type=\"string\",\n            title=\"Field with proposals\",\n            description=\"You can use JSON schema examples's to generate drop down selection boxes \"\n            \"but allow also to enter custom values. Try typing an 'a' and see options.\",\n            examples=(\n                \"Alpha,Bravo,Charlie,Delta,Echo,Foxtrot,Golf,Hotel,India,Juliett,Kilo,Lima,Mike,November,Oscar,Papa,\"\n                \"Quebec,Romeo,Sierra,Tango,Uniform,Victor,Whiskey,X-ray,Yankee,Zulu\"\n            ).split(\",\"),\n        ),\n        # If you want to select multiple items from a fixed list JSON schema des not allow to use enum\n        # In this case the type \"array\" is being used together with \"examples\" as pick list\n        \"multi_select\": Param(\n            [\"two\", \"three\"],\n            \"Select from the list of options.\",\n            type=\"array\",\n            title=\"Multi Select\",\n            examples=[\"one\", \"two\", \"three\", \"four\", \"five\"],\n        ),\n        # A multiple options selection can also be combined with values_display\n        \"multi_select_with_label\": Param(\n            [\"2\", \"3\"],\n            \"Select from the list of options. See that options can have nicer text and still technical values\"\n            \"are propagated as values during trigger to the DAG.\",\n            type=\"array\",\n            title=\"Multi Select with Labels\",\n            examples=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n            values_display={\n                \"1\": \"One box of choccolate\",\n                \"2\": \"Two bananas\",\n                \"3\": \"Three apples\",\n                # Note: Value display mapping does not need to be complete.s\n            },\n        ),\n        # An array of numbers\n        \"array_of_numbers\": Param(\n            [1, 2, 3],\n            \"Only integers are accepted in this array\",\n            type=\"array\",\n            title=\"Array of numbers\",\n            items={\"type\": \"number\"},\n        ),\n        # Boolean as proper parameter with description\n        \"bool\": Param(\n            True,\n            type=\"boolean\",\n            title=\"Please confirm\",\n            description=\"A On/Off selection with a proper description.\",\n        ),\n        # Dates and Times are also supported\n        \"date_time\": Param(\n            f\"{datetime.date.today()}T{datetime.time(hour=12, minute=17, second=00)}+00:00\",\n            type=\"string\",\n            format=\"date-time\",\n            title=\"Date-Time Picker\",\n            description=\"Please select a date and time, use the button on the left for a pup-up calendar.\",\n        ),\n        \"date\": Param(\n            f\"{datetime.date.today()}\",\n            type=\"string\",\n            format=\"date\",\n            title=\"Date Picker\",\n            description=\"Please select a date, use the button on the left for a pup-up calendar. \"\n            \"See that here are no times!\",\n        ),\n        \"time\": Param(\n            f\"{datetime.time(hour=12, minute=13, second=14)}\",\n            type=[\"string\", \"null\"],\n            format=\"time\",\n            title=\"Time Picker\",\n            description=\"Please select a time, use the button on the left for a pup-up tool.\",\n        ),\n        # Fields can be required or not. If the defined fields are typed they are getting required by default\n        # (else they would not pass JSON schema validation) - to make typed fields optional you must\n        # permit the optional \"null\" type\n        \"required_field\": Param(\n            \"You can not trigger if no text is given here!\",\n            type=\"string\",\n            title=\"Required text field\",\n            description=\"This field is required. You can not submit without having text in here.\",\n        ),\n        \"optional_field\": Param(\n            \"optional text, you can trigger also w/o text\",\n            type=[\"null\", \"string\"],\n            title=\"Optional text field\",\n            description_html=\"This field is optional. As field content is JSON schema validated you must \"\n            \"allow the <code>null</code> type.\",\n        ),\n        # You can arrange the entry fields in sections so that you can have a better overview for the user\n        # Therefore you can add the \"section\" attribute.\n        # The benefit of the Params class definition is that the full scope of JSON schema validation\n        # can be leveraged for form fields and they will be validated before DAG submission.\n        \"checked_text\": Param(\n            \"length-checked-field\",\n            type=\"string\",\n            title=\"Text field with length check\",\n            description_html=\"\"\"This field is required. And you need to provide something between 10 and 30\n            characters. See the\n            <a href='https://json-schema.org/understanding-json-schema/reference/string.html'>\n            JSON schema description (string)</a> in for more details\"\"\",\n            minLength=10,\n            maxLength=20,\n            section=\"JSON Schema validation options\",\n        ),\n        \"checked_number\": Param(\n            100,\n            type=\"number\",\n            title=\"Number field with value check\",\n            description_html=\"\"\"This field is required. You need to provide any number between 64 and 128.\n            See the <a href='https://json-schema.org/understanding-json-schema/reference/numeric.html'>\n            JSON schema description (numbers)</a> in for more details\"\"\",\n            minimum=64,\n            maximum=128,\n            section=\"JSON Schema validation options\",\n        ),\n        # Some further cool stuff as advanced options are also possible\n        # You can have the user entering a dict object as a JSON with validation\n        \"object\": Param(\n            {\"key\": \"value\"},\n            type=[\"object\", \"null\"],\n            title=\"JSON entry field\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n        \"array_of_objects\": Param(\n            [{\"name\": \"account_name\", \"country\": \"country_name\"}],\n            \"Array with complex objects and validation rules. \"\n            \"See <a href='https://json-schema.org/understanding-json-schema\"\n            \"/reference/array.html#items'>JSON Schema validation options in specs.</a>\",\n            type=\"array\",\n            title=\"JSON array field\",\n            items={\n                \"type\": \"object\",\n                \"properties\": {\"name\": {\"type\": \"string\"}, \"country_name\": {\"type\": \"string\"}},\n                \"required\": [\"name\"],\n            },\n            section=\"Special advanced stuff with form fields\",\n        ),\n        # If you want to have static parameters which are always passed and not editable by the user\n        # then you can use the JSON schema option of passing constant values. These parameters\n        # will not be displayed but passed to the DAG\n        \"hidden_secret_field\": Param(\"constant value\", const=\"constant value\"),\n        # Finally besides the standard provided field generator you can have you own HTML form code\n        # injected - but be careful, you can also mess-up the layout!\n        \"color_picker\": Param(\n            \"#FF8800\",\n            type=\"string\",\n            title=\"Pick a color\",\n            description_html=\"\"\"This is a special HTML widget as custom implementation in the DAG code.\n            It is templated with the following parameter to render proper HTML form fields:\n            <ul>\n                <li><code>{name}</code>: Name of the HTML input field that is expected.</li>\n                <li><code>{value}</code>:\n                    (Default) value that should be displayed when showing/loading the form.</li>\n                <li>Note: If you have elements changing a value, call <code>updateJSONconf()</code> to update\n                    the form data to be posted as <code>dag_run.conf</code>.</li>\n            </ul>\n            Example: <code>&lt;input name='{name}' value='{value}' onchange='updateJSONconf()' /&gt;</code>\n            \"\"\",\n            custom_html_form=\"\"\"\n            <table width=\"100%\" cellspacing=\"5\"><tbody><tr><td>\n                <label for=\"r_{name}\">Red:</label>\n            </td><td width=\"80%\">\n                <input id=\"r_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td><td rowspan=\"3\" style=\"padding-left: 10px;\">\n                <div id=\"preview_{name}\"\n                style=\"line-height: 40px; margin-bottom: 7px; width: 100%; background-color: {value};\"\n                >&nbsp;</div>\n                <input class=\"form-control\" type=\"text\" maxlength=\"7\" id=\"{name}\" name=\"{name}\"\n                value=\"{value}\" onchange=\"v_{name}()\" />\n            </td></tr><tr><td>\n                <label for=\"g_{name}\">Green:</label>\n            </td><td>\n                <input id=\"g_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr><tr><td>\n                <label for=\"b_{name}\">Blue:</label>\n            </td><td>\n                <input id=\"b_{name}\" type=\"range\" min=\"0\" max=\"255\" value=\"0\" onchange=\"u_{name}()\"/>\n            </td></tr></tbody></table>\n            <script lang=\"javascript\">\n                const hex_chars = \"0123456789ABCDEF\";\n                function i2hex(name) {\n                    var i = document.getElementById(name).value;\n                    return hex_chars.substr(parseInt(i / 16), 1) + hex_chars.substr(parseInt(i % 16), 1)\n                }\n                function u_{name}() {\n                    var hex_val = \"#\"+i2hex(\"r_{name}\")+i2hex(\"g_{name}\")+i2hex(\"b_{name}\");\n                    document.getElementById(\"{name}\").value = hex_val;\n                    document.getElementById(\"preview_{name}\").style.background = hex_val;\n                    updateJSONconf();\n                }\n                function hex2i(text) {\n                    return hex_chars.indexOf(text.substr(0,1)) * 16 + hex_chars.indexOf(text.substr(1,1));\n                }\n                function v_{name}() {\n                    var value = document.getElementById(\"{name}\").value.toUpperCase();\n                    document.getElementById(\"r_{name}\").value = hex2i(value.substr(1,2));\n                    document.getElementById(\"g_{name}\").value = hex2i(value.substr(3,2));\n                    document.getElementById(\"b_{name}\").value = hex2i(value.substr(5,2));\n                    document.getElementById(\"preview_{name}\").style.background = value;\n                }\n                v_{name}();\n            </script>\"\"\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n    },\n) as dag:\n\n    @task(task_id=\"show_params\")\n    def show_params(**kwargs) -> None:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if not dag_run.conf:\n            print(\"Uups, no parameters supplied as DagRun.conf, was the trigger w/o form?\")\n            raise AirflowSkipException(\"No DagRun.conf parameters supplied.\")\n        print(f\"This DAG was triggered with the following parameters:\\n{json.dumps(dag_run.conf, indent=4)}\")\n\n    show_params()", "metadata": {"file_name": "example_params_ui_tutorial.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple parameter configuration techniques including enums, examples, custom descriptions, and type-specific validations across different input scenarios", "key_concepts": ["Parameter Validation", "User Interface Design", "Input Type Handling", "DAG Metadata", "Interactive Configuration"], "learning_objectives": ["Master Airflow parameter configuration techniques", "Create flexible and user-friendly DAG interfaces", "Implement advanced input validation strategies"]}}
{"instruction": "Build an Airflow DAG that demonstrates parameter passing techniques using both Python tasks and Bash operators, focusing on how to inject and access parameters during task execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 69, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the params arguments in templated arguments.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom textwrap import dedent\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef my_py_command(params, test_mode=None, task=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command run_this <date>\n    -t '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(\n            f\" 'foo' was passed in via test={test_mode} command : kwargs[params][foo] = {task.params['foo']}\"\n        )\n    # Print out the value of \"miff\", passed in below via the Python Operator\n    print(f\" 'miff' was passed in via task params = {params['miff']}\")\n    return 1\n\n\n@task(task_id=\"env_var_test_task\")\ndef print_env_vars(test_mode=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command env_var_test_task <date>\n    --env-vars '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(f\"foo={os.environ.get('foo')}\")\n        print(f\"AIRFLOW_TEST_MODE={os.environ.get('AIRFLOW_TEST_MODE')}\")\n\n\nwith DAG(\n    \"example_passing_params_via_test_command\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=4),\n    tags=[\"example\"],\n) as dag:\n    run_this = my_py_command(params={\"miff\": \"agg\"})\n\n    my_command = dedent(\n        \"\"\"\n        echo \"'foo' was passed in via Airflow CLI Test command with value '$FOO'\"\n        echo \"'miff' was passed in via BashOperator with value '$MIFF'\"\n        \"\"\"\n    )\n\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command=my_command,\n        params={\"miff\": \"agg\"},\n        env={\"FOO\": \"{{ params.foo }}\", \"MIFF\": \"{{ params.miff }}\"},\n    )\n\n    env_var_test_task = print_env_vars()\n\n    run_this >> also_run_this", "metadata": {"file_name": "example_passing_params_via_test_command.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple task types, decorators, custom functions, and demonstrates parameter injection via different methods (CLI, task params, environment variables)", "key_concepts": ["Task Decorators", "Parameter Passing", "Environment Variables", "Task Parameters", "Airflow CLI Testing"], "learning_objectives": ["Understand parameter injection in Airflow tasks", "Learn how to pass parameters between tasks", "Explore different parameter access methods"]}}
{"instruction": "Create an Airflow workflow that showcases advanced testing and debugging techniques by implementing a DAG with multiple parameter injection strategies and environment variable manipulation.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 69, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the params arguments in templated arguments.\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom textwrap import dedent\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef my_py_command(params, test_mode=None, task=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command run_this <date>\n    -t '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(\n            f\" 'foo' was passed in via test={test_mode} command : kwargs[params][foo] = {task.params['foo']}\"\n        )\n    # Print out the value of \"miff\", passed in below via the Python Operator\n    print(f\" 'miff' was passed in via task params = {params['miff']}\")\n    return 1\n\n\n@task(task_id=\"env_var_test_task\")\ndef print_env_vars(test_mode=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command env_var_test_task <date>\n    --env-vars '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(f\"foo={os.environ.get('foo')}\")\n        print(f\"AIRFLOW_TEST_MODE={os.environ.get('AIRFLOW_TEST_MODE')}\")\n\n\nwith DAG(\n    \"example_passing_params_via_test_command\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=4),\n    tags=[\"example\"],\n) as dag:\n    run_this = my_py_command(params={\"miff\": \"agg\"})\n\n    my_command = dedent(\n        \"\"\"\n        echo \"'foo' was passed in via Airflow CLI Test command with value '$FOO'\"\n        echo \"'miff' was passed in via BashOperator with value '$MIFF'\"\n        \"\"\"\n    )\n\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command=my_command,\n        params={\"miff\": \"agg\"},\n        env={\"FOO\": \"{{ params.foo }}\", \"MIFF\": \"{{ params.miff }}\"},\n    )\n\n    env_var_test_task = print_env_vars()\n\n    run_this >> also_run_this", "metadata": {"file_name": "example_passing_params_via_test_command.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves conditional logic, environment variable handling, and demonstrates multiple ways of passing runtime parameters to tasks", "key_concepts": ["Task Testing", "Conditional Execution", "Runtime Parameter Injection", "Debug Techniques", "Task Instrumentation"], "learning_objectives": ["Master Airflow task testing approaches", "Understand runtime parameter configuration", "Learn debugging strategies for Airflow tasks"]}}
{"instruction": "Create an Airflow DAG that demonstrates setup and teardown tasks using BashOperator, showcasing how to define preparatory and cleanup tasks within both root-level and task group contexts.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 31, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"example_setup_teardown\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    root_setup = BashOperator(task_id=\"root_setup\", bash_command=\"echo 'Hello from root_setup'\").as_setup()\n    root_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n    root_teardown = BashOperator(\n        task_id=\"root_teardown\", bash_command=\"echo 'Goodbye from root_teardown'\"\n    ).as_teardown(setups=root_setup)\n    root_setup >> root_normal >> root_teardown\n    with TaskGroup(\"section_1\") as section_1:\n        inner_setup = BashOperator(\n            task_id=\"taskgroup_setup\", bash_command=\"echo 'Hello from taskgroup_setup'\"\n        ).as_setup()\n        inner_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n        inner_teardown = BashOperator(\n            task_id=\"taskgroup_teardown\", bash_command=\"echo 'Hello from taskgroup_teardown'\"\n        ).as_teardown(setups=inner_setup)\n        inner_setup >> inner_normal >> inner_teardown\n    root_normal >> section_1", "metadata": {"file_name": "example_setup_teardown.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Uses single operator type, simple sequential dependencies, minimal code, and basic Airflow setup/teardown pattern", "key_concepts": ["Setup and Teardown Tasks", "BashOperator", "TaskGroup", "Task Dependencies"], "learning_objectives": ["Understand setup and teardown task mechanics", "Learn task dependency management", "Explore task group organization"]}}
{"instruction": "Design an Airflow workflow that illustrates task lifecycle management by implementing setup and teardown operations at both root and task group levels, emphasizing clean task orchestration patterns.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator"], "line_count": 31, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"example_setup_teardown\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    root_setup = BashOperator(task_id=\"root_setup\", bash_command=\"echo 'Hello from root_setup'\").as_setup()\n    root_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n    root_teardown = BashOperator(\n        task_id=\"root_teardown\", bash_command=\"echo 'Goodbye from root_teardown'\"\n    ).as_teardown(setups=root_setup)\n    root_setup >> root_normal >> root_teardown\n    with TaskGroup(\"section_1\") as section_1:\n        inner_setup = BashOperator(\n            task_id=\"taskgroup_setup\", bash_command=\"echo 'Hello from taskgroup_setup'\"\n        ).as_setup()\n        inner_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n        inner_teardown = BashOperator(\n            task_id=\"taskgroup_teardown\", bash_command=\"echo 'Hello from taskgroup_teardown'\"\n        ).as_teardown(setups=inner_setup)\n        inner_setup >> inner_normal >> inner_teardown\n    root_normal >> section_1", "metadata": {"file_name": "example_setup_teardown.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces task group complexity, multiple dependency levels, and demonstrates advanced task relationship management", "key_concepts": ["Task Lifecycle Management", "Hierarchical Task Organization", "Dependency Chaining", "Modular DAG Design"], "learning_objectives": ["Master setup and teardown task relationships", "Understand nested task group structures", "Learn clean workflow design principles"]}}
{"instruction": "Build a DAG that demonstrates the @task.sensor decorator for waiting on upstream conditions, showing how to create a simple sensor task that checks readiness before executing downstream tasks.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the sensor decorator.\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.sensors.base import PokeReturnValue\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_sensor_decorator():\n    # [END instantiate_dag]\n\n    # [START wait_function]\n    # Using a sensor operator to wait for the upstream data to be ready.\n    @task.sensor(poke_interval=60, timeout=3600, mode=\"reschedule\")\n    def wait_for_upstream() -> PokeReturnValue:\n        return PokeReturnValue(is_done=True, xcom_value=\"xcom_value\")\n\n    # [END wait_function]\n\n    # [START dummy_function]\n    @task\n    def dummy_operator() -> None:\n        pass\n\n    # [END dummy_function]\n\n    # [START main_flow]\n    wait_for_upstream() >> dummy_operator()\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_etl_dag = example_sensor_decorator()\n# [END dag_invocation]\n\n# [END tutorial]", "metadata": {"file_name": "example_sensor_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with minimal tasks, using basic decorator syntax, no complex logic, and straightforward task dependency", "key_concepts": ["Sensor Decorators", "Task Dependencies", "TaskFlow API", "Pendulum Datetime"], "learning_objectives": ["Understand sensor decorator usage", "Learn task chaining with >> operator", "Explore basic DAG configuration"]}}
{"instruction": "Create an event-driven workflow that uses a sensor to simulate waiting for an external condition, demonstrating how to implement non-blocking wait mechanisms in Airflow using the reschedule mode.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the sensor decorator.\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.sensors.base import PokeReturnValue\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_sensor_decorator():\n    # [END instantiate_dag]\n\n    # [START wait_function]\n    # Using a sensor operator to wait for the upstream data to be ready.\n    @task.sensor(poke_interval=60, timeout=3600, mode=\"reschedule\")\n    def wait_for_upstream() -> PokeReturnValue:\n        return PokeReturnValue(is_done=True, xcom_value=\"xcom_value\")\n\n    # [END wait_function]\n\n    # [START dummy_function]\n    @task\n    def dummy_operator() -> None:\n        pass\n\n    # [END dummy_function]\n\n    # [START main_flow]\n    wait_for_upstream() >> dummy_operator()\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_etl_dag = example_sensor_decorator()\n# [END dag_invocation]\n\n# [END tutorial]", "metadata": {"file_name": "example_sensor_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses advanced sensor configuration with poke_interval and timeout, introduces reschedule mode for more complex waiting scenarios", "key_concepts": ["Sensor Modes", "PokeReturnValue", "Non-blocking Sensors", "Task Scheduling"], "learning_objectives": ["Understand sensor configuration options", "Learn about reschedule mode", "Explore conditional task execution"]}}
{"instruction": "Create a DAG that demonstrates SubDagOperator usage by building a workflow with nested sub-workflows that execute sequential tasks across multiple sections.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "SubDagOperator"], "line_count": 45, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the SubDagOperator.\"\"\"\nfrom __future__ import annotations\n\n# [START example_subdag_operator]\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.example_dags.subdags.subdag import subdag\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.subdag import SubDagOperator\n\nDAG_NAME = \"example_subdag_operator\"\n\nwith DAG(\n    dag_id=DAG_NAME,\n    default_args={\"retries\": 2},\n    start_date=datetime.datetime(2022, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n\n    start = EmptyOperator(\n        task_id=\"start\",\n    )\n\n    section_1 = SubDagOperator(\n        task_id=\"section-1\",\n        subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),\n    )\n\n    some_other_task = EmptyOperator(\n        task_id=\"some-other-task\",\n    )\n\n    section_2 = SubDagOperator(\n        task_id=\"section-2\",\n        subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),\n    )\n\n    end = EmptyOperator(\n        task_id=\"end\",\n    )\n\n    start >> section_1 >> some_other_task >> section_2 >> end\n# [END example_subdag_operator]", "metadata": {"file_name": "example_subdag_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses SubDagOperator with nested workflow logic, requires understanding of Airflow task dependencies and sub-dag creation, involves multiple task types", "key_concepts": ["SubDagOperator", "Task Dependencies", "Workflow Composition", "Airflow DAG Structure"], "learning_objectives": ["Understand SubDagOperator mechanics", "Learn nested workflow design patterns", "Practice task sequencing in Airflow"]}}
{"instruction": "Design a modular data processing pipeline demonstrating how SubDagOperators can be used to break complex workflows into manageable, reusable sections with consistent error handling and retry mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "SubDagOperator"], "line_count": 45, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the SubDagOperator.\"\"\"\nfrom __future__ import annotations\n\n# [START example_subdag_operator]\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.example_dags.subdags.subdag import subdag\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.subdag import SubDagOperator\n\nDAG_NAME = \"example_subdag_operator\"\n\nwith DAG(\n    dag_id=DAG_NAME,\n    default_args={\"retries\": 2},\n    start_date=datetime.datetime(2022, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n\n    start = EmptyOperator(\n        task_id=\"start\",\n    )\n\n    section_1 = SubDagOperator(\n        task_id=\"section-1\",\n        subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),\n    )\n\n    some_other_task = EmptyOperator(\n        task_id=\"some-other-task\",\n    )\n\n    section_2 = SubDagOperator(\n        task_id=\"section-2\",\n        subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),\n    )\n\n    end = EmptyOperator(\n        task_id=\"end\",\n    )\n\n    start >> section_1 >> some_other_task >> section_2 >> end\n# [END example_subdag_operator]", "metadata": {"file_name": "example_subdag_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves advanced workflow decomposition, uses default arguments for consistent configuration, demonstrates modularity and error management strategies", "key_concepts": ["Workflow Modularity", "Error Handling", "Default Arguments", "Task Retry Configurations"], "learning_objectives": ["Learn workflow decomposition techniques", "Understand retry and error management", "Practice creating modular Airflow pipelines"]}}
{"instruction": "Build an Airflow DAG that demonstrates various sensor types and their usage, showcasing how different sensors can wait for specific conditions before allowing downstream tasks to execute.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "BashSensor", "DayOfWeekSensor", "FileSensor", "PythonSensor", "TimeDeltaSensor", "TimeSensor"], "line_count": 109, "is_multifile": false}, "output": "from __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.python import PythonSensor\nfrom airflow.sensors.time_delta import TimeDeltaSensor, TimeDeltaSensorAsync\nfrom airflow.sensors.time_sensor import TimeSensor, TimeSensorAsync\nfrom airflow.sensors.weekday import DayOfWeekSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n\n# [START example_callables]\ndef success_callable():\n    return True\n\n\ndef failure_callable():\n    return False\n\n\n# [END example_callables]\n\n\nwith DAG(\n    dag_id=\"example_sensors\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START example_time_delta_sensor]\n    t0 = TimeDeltaSensor(task_id=\"wait_some_seconds\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor]\n\n    # [START example_time_delta_sensor_async]\n    t0a = TimeDeltaSensorAsync(task_id=\"wait_some_seconds_async\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor_async]\n\n    # [START example_time_sensors]\n    t1 = TimeSensor(\n        task_id=\"fire_immediately\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2 = TimeSensor(\n        task_id=\"timeout_after_second_date_in_the_future\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors]\n\n    # [START example_time_sensors_async]\n    t1a = TimeSensorAsync(\n        task_id=\"fire_immediately_async\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2a = TimeSensorAsync(\n        task_id=\"timeout_after_second_date_in_the_future_async\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors_async]\n\n    # [START example_bash_sensors]\n    t3 = BashSensor(task_id=\"Sensor_succeeds\", bash_command=\"exit 0\")\n\n    t4 = BashSensor(task_id=\"Sensor_fails_after_3_seconds\", timeout=3, soft_fail=True, bash_command=\"exit 1\")\n    # [END example_bash_sensors]\n\n    t5 = BashOperator(task_id=\"remove_file\", bash_command=\"rm -rf /tmp/temporary_file_for_testing\")\n\n    # [START example_file_sensor]\n    t6 = FileSensor(task_id=\"wait_for_file\", filepath=\"/tmp/temporary_file_for_testing\")\n    # [END example_file_sensor]\n\n    t7 = BashOperator(\n        task_id=\"create_file_after_3_seconds\", bash_command=\"sleep 3; touch /tmp/temporary_file_for_testing\"\n    )\n\n    # [START example_python_sensors]\n    t8 = PythonSensor(task_id=\"success_sensor_python\", python_callable=success_callable)\n\n    t9 = PythonSensor(\n        task_id=\"failure_timeout_sensor_python\", timeout=3, soft_fail=True, python_callable=failure_callable\n    )\n    # [END example_python_sensors]\n\n    # [START example_day_of_week_sensor]\n    t10 = DayOfWeekSensor(\n        task_id=\"week_day_sensor_failing_on_timeout\", timeout=3, soft_fail=True, week_day=WeekDay.MONDAY\n    )\n    # [END example_day_of_week_sensor]\n\n    tx = BashOperator(task_id=\"print_date_in_bash\", bash_command=\"date\")\n\n    tx.trigger_rule = TriggerRule.NONE_FAILED\n    [t0, t0a, t1, t1a, t2, t2a, t3, t4] >> tx\n    t5 >> t6 >> tx\n    t7 >> tx\n    [t8, t9] >> tx\n    t10 >> tx", "metadata": {"file_name": "example_sensors.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Multiple sensor types (time, file, bash, python), async sensors, different trigger conditions, and complex task dependencies increase complexity beyond basic DAGs", "key_concepts": ["Airflow Sensors", "Task Dependencies", "Conditional Execution", "Trigger Rules", "Async Sensors"], "learning_objectives": ["Understand different sensor types in Airflow", "Learn how to create conditional task execution", "Explore timeout and soft fail mechanisms", "Practice task dependency management"]}}
{"instruction": "Design a workflow that demonstrates error handling and resilient task execution using various sensor types, focusing on how Airflow can manage different failure scenarios and continue processing.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "BashSensor", "DayOfWeekSensor", "FileSensor", "PythonSensor", "TimeDeltaSensor", "TimeSensor"], "line_count": 109, "is_multifile": false}, "output": "from __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.python import PythonSensor\nfrom airflow.sensors.time_delta import TimeDeltaSensor, TimeDeltaSensorAsync\nfrom airflow.sensors.time_sensor import TimeSensor, TimeSensorAsync\nfrom airflow.sensors.weekday import DayOfWeekSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n\n# [START example_callables]\ndef success_callable():\n    return True\n\n\ndef failure_callable():\n    return False\n\n\n# [END example_callables]\n\n\nwith DAG(\n    dag_id=\"example_sensors\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START example_time_delta_sensor]\n    t0 = TimeDeltaSensor(task_id=\"wait_some_seconds\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor]\n\n    # [START example_time_delta_sensor_async]\n    t0a = TimeDeltaSensorAsync(task_id=\"wait_some_seconds_async\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor_async]\n\n    # [START example_time_sensors]\n    t1 = TimeSensor(\n        task_id=\"fire_immediately\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2 = TimeSensor(\n        task_id=\"timeout_after_second_date_in_the_future\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors]\n\n    # [START example_time_sensors_async]\n    t1a = TimeSensorAsync(\n        task_id=\"fire_immediately_async\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2a = TimeSensorAsync(\n        task_id=\"timeout_after_second_date_in_the_future_async\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors_async]\n\n    # [START example_bash_sensors]\n    t3 = BashSensor(task_id=\"Sensor_succeeds\", bash_command=\"exit 0\")\n\n    t4 = BashSensor(task_id=\"Sensor_fails_after_3_seconds\", timeout=3, soft_fail=True, bash_command=\"exit 1\")\n    # [END example_bash_sensors]\n\n    t5 = BashOperator(task_id=\"remove_file\", bash_command=\"rm -rf /tmp/temporary_file_for_testing\")\n\n    # [START example_file_sensor]\n    t6 = FileSensor(task_id=\"wait_for_file\", filepath=\"/tmp/temporary_file_for_testing\")\n    # [END example_file_sensor]\n\n    t7 = BashOperator(\n        task_id=\"create_file_after_3_seconds\", bash_command=\"sleep 3; touch /tmp/temporary_file_for_testing\"\n    )\n\n    # [START example_python_sensors]\n    t8 = PythonSensor(task_id=\"success_sensor_python\", python_callable=success_callable)\n\n    t9 = PythonSensor(\n        task_id=\"failure_timeout_sensor_python\", timeout=3, soft_fail=True, python_callable=failure_callable\n    )\n    # [END example_python_sensors]\n\n    # [START example_day_of_week_sensor]\n    t10 = DayOfWeekSensor(\n        task_id=\"week_day_sensor_failing_on_timeout\", timeout=3, soft_fail=True, week_day=WeekDay.MONDAY\n    )\n    # [END example_day_of_week_sensor]\n\n    tx = BashOperator(task_id=\"print_date_in_bash\", bash_command=\"date\")\n\n    tx.trigger_rule = TriggerRule.NONE_FAILED\n    [t0, t0a, t1, t1a, t2, t2a, t3, t4] >> tx\n    t5 >> t6 >> tx\n    t7 >> tx\n    [t8, t9] >> tx\n    t10 >> tx", "metadata": {"file_name": "example_sensors.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses soft fail options, multiple sensor types with timeouts, and demonstrates different task execution strategies under potential failure conditions", "key_concepts": ["Error Handling", "Soft Fail Mechanisms", "Task Resilience", "Sensor Timeout", "Trigger Rules"], "learning_objectives": ["Implement fault-tolerant workflow designs", "Understand sensor timeout behaviors", "Learn how to configure task execution under failure conditions", "Explore different trigger rule strategies"]}}
{"instruction": "Create a DAG that demonstrates different trigger rules and task skipping behaviors by implementing two parallel pipelines with EmptyOperator and a custom EmptySkipOperator.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "EmptySkipOperator"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the EmptyOperator and a custom EmptySkipOperator which skips by default.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# Create some placeholder operators\nclass EmptySkipOperator(BaseOperator):\n    \"\"\"Empty operator which always skips the task.\"\"\"\n\n    ui_color = \"#e8b7e4\"\n\n    def execute(self, context: Context):\n        raise AirflowSkipException\n\n\ndef create_test_pipeline(suffix, trigger_rule):\n    \"\"\"\n    Instantiate a number of operators for the given DAG.\n\n    :param str suffix: Suffix to append to the operator task_ids\n    :param str trigger_rule: TriggerRule for the join task\n    :param DAG dag_: The DAG to run the operators on\n    \"\"\"\n    skip_operator = EmptySkipOperator(task_id=f\"skip_operator_{suffix}\")\n    always_true = EmptyOperator(task_id=f\"always_true_{suffix}\")\n    join = EmptyOperator(task_id=trigger_rule, trigger_rule=trigger_rule)\n    final = EmptyOperator(task_id=f\"final_{suffix}\")\n\n    skip_operator >> join\n    always_true >> join\n    join >> final\n\n\nwith DAG(\n    dag_id=\"example_skip_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_test_pipeline(\"1\", TriggerRule.ALL_SUCCESS)\n    create_test_pipeline(\"2\", TriggerRule.ONE_SUCCESS)", "metadata": {"file_name": "example_skip_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with EmptyOperators, uses basic Airflow concepts, minimal code, sequential dependencies, and standard imports", "key_concepts": ["Trigger Rules", "Task Skipping", "Custom Operators", "DAG Dependencies"], "learning_objectives": ["Understand how trigger rules affect task execution", "Learn to create custom operators", "Explore task dependency management"]}}
{"instruction": "Design an experimental workflow that showcases advanced task execution strategies by implementing conditional task skipping and different pipeline completion scenarios.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "EmptySkipOperator"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the EmptyOperator and a custom EmptySkipOperator which skips by default.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n# Create some placeholder operators\nclass EmptySkipOperator(BaseOperator):\n    \"\"\"Empty operator which always skips the task.\"\"\"\n\n    ui_color = \"#e8b7e4\"\n\n    def execute(self, context: Context):\n        raise AirflowSkipException\n\n\ndef create_test_pipeline(suffix, trigger_rule):\n    \"\"\"\n    Instantiate a number of operators for the given DAG.\n\n    :param str suffix: Suffix to append to the operator task_ids\n    :param str trigger_rule: TriggerRule for the join task\n    :param DAG dag_: The DAG to run the operators on\n    \"\"\"\n    skip_operator = EmptySkipOperator(task_id=f\"skip_operator_{suffix}\")\n    always_true = EmptyOperator(task_id=f\"always_true_{suffix}\")\n    join = EmptyOperator(task_id=trigger_rule, trigger_rule=trigger_rule)\n    final = EmptyOperator(task_id=f\"final_{suffix}\")\n\n    skip_operator >> join\n    always_true >> join\n    join >> final\n\n\nwith DAG(\n    dag_id=\"example_skip_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_test_pipeline(\"1\", TriggerRule.ALL_SUCCESS)\n    create_test_pipeline(\"2\", TriggerRule.ONE_SUCCESS)", "metadata": {"file_name": "example_skip_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces custom operator logic, multiple trigger rules, and demonstrates nuanced task execution patterns beyond basic sequential workflows", "key_concepts": ["AirflowSkipException", "Conditional Task Execution", "Trigger Rule Variations", "Task State Management"], "learning_objectives": ["Understand task state transitions", "Learn how to implement conditional task execution", "Explore advanced DAG design patterns"]}}
{"instruction": "Build an Airflow DAG that demonstrates Service Level Agreement (SLA) monitoring by creating two tasks with different sleep durations and an SLA callback function to handle missed SLAs.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating SLA use in Tasks\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport time\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n\n# [START howto_task_sla]\ndef sla_callback(dag, task_list, blocking_task_list, slas, blocking_tis):\n    print(\n        \"The callback arguments are: \",\n        {\n            \"dag\": dag,\n            \"task_list\": task_list,\n            \"blocking_task_list\": blocking_task_list,\n            \"slas\": slas,\n            \"blocking_tis\": blocking_tis,\n        },\n    )\n\n\n@dag(\n    schedule=\"*/2 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    sla_miss_callback=sla_callback,\n    default_args={\"email\": \"email@example.com\"},\n)\ndef example_sla_dag():\n    @task(sla=datetime.timedelta(seconds=10))\n    def sleep_20():\n        \"\"\"Sleep for 20 seconds\"\"\"\n        time.sleep(20)\n\n    @task\n    def sleep_30():\n        \"\"\"Sleep for 30 seconds\"\"\"\n        time.sleep(30)\n\n    sleep_20() >> sleep_30()\n\n\nexample_dag = example_sla_dag()\n\n# [END howto_task_sla]", "metadata": {"file_name": "example_sla_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with two sequential tasks, using decorators, minimal code, and basic time-based SLA monitoring", "key_concepts": ["Airflow SLA", "Task Decorators", "Callback Functions", "Scheduling", "Task Dependencies"], "learning_objectives": ["Understand SLA configuration in Airflow", "Learn how to define task-level time constraints", "Implement a callback for SLA miss events"]}}
{"instruction": "Design an operational monitoring workflow that tracks task execution times and demonstrates how to handle and log Service Level Agreement (SLA) violations using Airflow's built-in SLA mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task"], "line_count": 49, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating SLA use in Tasks\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nimport time\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n\n# [START howto_task_sla]\ndef sla_callback(dag, task_list, blocking_task_list, slas, blocking_tis):\n    print(\n        \"The callback arguments are: \",\n        {\n            \"dag\": dag,\n            \"task_list\": task_list,\n            \"blocking_task_list\": blocking_task_list,\n            \"slas\": slas,\n            \"blocking_tis\": blocking_tis,\n        },\n    )\n\n\n@dag(\n    schedule=\"*/2 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    sla_miss_callback=sla_callback,\n    default_args={\"email\": \"email@example.com\"},\n)\ndef example_sla_dag():\n    @task(sla=datetime.timedelta(seconds=10))\n    def sleep_20():\n        \"\"\"Sleep for 20 seconds\"\"\"\n        time.sleep(20)\n\n    @task\n    def sleep_30():\n        \"\"\"Sleep for 30 seconds\"\"\"\n        time.sleep(30)\n\n    sleep_20() >> sleep_30()\n\n\nexample_dag = example_sla_dag()\n\n# [END howto_task_sla]", "metadata": {"file_name": "example_sla_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces more advanced SLA monitoring concepts, custom callback function, and task timing considerations", "key_concepts": ["Operational Monitoring", "Task Performance Tracking", "Error Handling", "Logging", "Task Execution Timing"], "learning_objectives": ["Implement custom SLA miss handling", "Understand task execution time constraints", "Learn to log and track task performance metrics"]}}
{"instruction": "Build a DAG that demonstrates Airflow's setup and teardown task workflow using decorators, showcasing how to create tasks with initialization and cleanup phases across different scopes.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 88, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import setup, task, task_group, teardown\nfrom airflow.models.dag import DAG\n\nwith DAG(\n    dag_id=\"example_setup_teardown_taskflow\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def my_first_task():\n        print(\"Hello 1\")\n\n    @task\n    def my_second_task():\n        print(\"Hello 2\")\n\n    @task\n    def my_third_task():\n        print(\"Hello 3\")\n\n    # you can set setup / teardown relationships with the `as_teardown` method.\n    task_1 = my_first_task()\n    task_2 = my_second_task()\n    task_3 = my_third_task()\n    task_1 >> task_2 >> task_3.as_teardown(setups=task_1)\n\n    # The method `as_teardown` will mark task_3 as teardown, task_1 as setup, and\n    # arrow task_1 >> task_3.\n    # Now if you clear task_2, then its setup task, task_1, will be cleared in\n    # addition to its teardown task, task_3\n\n    # it's also possible to use a decorator to mark a task as setup or\n    # teardown when you define it. see below.\n\n    @setup\n    def outer_setup():\n        print(\"I am outer_setup\")\n        return \"some cluster id\"\n\n    @teardown\n    def outer_teardown(cluster_id):\n        print(\"I am outer_teardown\")\n        print(f\"Tearing down cluster: {cluster_id}\")\n\n    @task\n    def outer_work():\n        print(\"I am just a normal task\")\n\n    @task_group\n    def section_1():\n        @setup\n        def inner_setup():\n            print(\"I set up\")\n            return \"some_cluster_id\"\n\n        @task\n        def inner_work(cluster_id):\n            print(f\"doing some work with {cluster_id=}\")\n\n        @teardown\n        def inner_teardown(cluster_id):\n            print(f\"tearing down {cluster_id=}\")\n\n        # this passes the return value of `inner_setup` to both `inner_work` and `inner_teardown`\n        inner_setup_task = inner_setup()\n        inner_work(inner_setup_task) >> inner_teardown(inner_setup_task)\n\n    # by using the decorators, outer_setup and outer_teardown are already marked as setup / teardown\n    # now we just need to make sure they are linked directly.  At a low level, what we need\n    # to do so is the following::\n    #     s = outer_setup()\n    #     t = outer_teardown()\n    #     s >> t\n    #     s >> outer_work() >> t\n    # Thus, s and t are linked directly, and outer_work runs in between.  We can take advantage of\n    # the fact that we are in taskflow, along with the context manager on teardowns, as follows:\n    with outer_teardown(outer_setup()):\n        outer_work()\n\n        # and let's put section 1 inside the outer setup and teardown tasks\n        section_1()", "metadata": {"file_name": "example_setup_teardown_taskflow.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses advanced TaskFlow decorators, multiple task types (@setup, @teardown, @task), nested task groups, and demonstrates context management of tasks", "key_concepts": ["TaskFlow API", "Setup and Teardown Tasks", "Task Dependencies", "Task Decorators", "Context Management"], "learning_objectives": ["Understand setup and teardown task patterns", "Learn how to use task decorators", "Explore task dependency management", "Practice nested task group creation"]}}
{"instruction": "Design a workflow that illustrates how Airflow manages task lifecycle and resource management using setup and teardown tasks, focusing on creating modular and self-contained task groups.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 88, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import setup, task, task_group, teardown\nfrom airflow.models.dag import DAG\n\nwith DAG(\n    dag_id=\"example_setup_teardown_taskflow\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def my_first_task():\n        print(\"Hello 1\")\n\n    @task\n    def my_second_task():\n        print(\"Hello 2\")\n\n    @task\n    def my_third_task():\n        print(\"Hello 3\")\n\n    # you can set setup / teardown relationships with the `as_teardown` method.\n    task_1 = my_first_task()\n    task_2 = my_second_task()\n    task_3 = my_third_task()\n    task_1 >> task_2 >> task_3.as_teardown(setups=task_1)\n\n    # The method `as_teardown` will mark task_3 as teardown, task_1 as setup, and\n    # arrow task_1 >> task_3.\n    # Now if you clear task_2, then its setup task, task_1, will be cleared in\n    # addition to its teardown task, task_3\n\n    # it's also possible to use a decorator to mark a task as setup or\n    # teardown when you define it. see below.\n\n    @setup\n    def outer_setup():\n        print(\"I am outer_setup\")\n        return \"some cluster id\"\n\n    @teardown\n    def outer_teardown(cluster_id):\n        print(\"I am outer_teardown\")\n        print(f\"Tearing down cluster: {cluster_id}\")\n\n    @task\n    def outer_work():\n        print(\"I am just a normal task\")\n\n    @task_group\n    def section_1():\n        @setup\n        def inner_setup():\n            print(\"I set up\")\n            return \"some_cluster_id\"\n\n        @task\n        def inner_work(cluster_id):\n            print(f\"doing some work with {cluster_id=}\")\n\n        @teardown\n        def inner_teardown(cluster_id):\n            print(f\"tearing down {cluster_id=}\")\n\n        # this passes the return value of `inner_setup` to both `inner_work` and `inner_teardown`\n        inner_setup_task = inner_setup()\n        inner_work(inner_setup_task) >> inner_teardown(inner_setup_task)\n\n    # by using the decorators, outer_setup and outer_teardown are already marked as setup / teardown\n    # now we just need to make sure they are linked directly.  At a low level, what we need\n    # to do so is the following::\n    #     s = outer_setup()\n    #     t = outer_teardown()\n    #     s >> t\n    #     s >> outer_work() >> t\n    # Thus, s and t are linked directly, and outer_work runs in between.  We can take advantage of\n    # the fact that we are in taskflow, along with the context manager on teardowns, as follows:\n    with outer_teardown(outer_setup()):\n        outer_work()\n\n        # and let's put section 1 inside the outer setup and teardown tasks\n        section_1()", "metadata": {"file_name": "example_setup_teardown_taskflow.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements complex task relationships, uses nested task groups, demonstrates resource initialization and cleanup across different scopes", "key_concepts": ["Resource Management", "Task Lifecycle", "Dependency Injection", "Task Group Modularity", "Context Passing"], "learning_objectives": ["Learn resource initialization strategies", "Understand task group encapsulation", "Practice passing context between tasks", "Explore advanced task dependency techniques"]}}
{"instruction": "Build a DAG that demonstrates short-circuit task flow behavior using conditional task execution, showing how tasks can be dynamically skipped based on a boolean condition.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task", "EmptyOperator"], "line_count": 44, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the `@task.short_circuit()` TaskFlow decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@dag(start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"), catchup=False, tags=[\"example\"])\ndef example_short_circuit_decorator():\n    # [START howto_operator_short_circuit]\n    @task.short_circuit()\n    def check_condition(condition):\n        return condition\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    condition_is_true = check_condition.override(task_id=\"condition_is_true\")(condition=True)\n    condition_is_false = check_condition.override(task_id=\"condition_is_false\")(condition=False)\n\n    chain(condition_is_true, *ds_true)\n    chain(condition_is_false, *ds_false)\n    # [END howto_operator_short_circuit]\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = check_condition.override(task_id=\"short_circuit\", ignore_downstream_trigger_rules=False)(\n        condition=False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)\n    # [END howto_operator_short_circuit_trigger_rules]\n\n\nexample_dag = example_short_circuit_decorator()", "metadata": {"file_name": "example_short_circuit_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses TaskFlow decorators, multiple task branches, custom condition checking, and demonstrates advanced task routing with short-circuit logic", "key_concepts": ["TaskFlow API", "Short-circuit tasks", "Conditional task execution", "Task chaining", "Trigger rules"], "learning_objectives": ["Understand how to create conditional task execution", "Learn @task.short_circuit() decorator usage", "Explore dynamic task skipping strategies"]}}
{"instruction": "Design a workflow that explores different trigger rules and task dependency management, focusing on how tasks can be conditionally executed and managed using Airflow's advanced task routing mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["@dag", "@task", "EmptyOperator"], "line_count": 44, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the `@task.short_circuit()` TaskFlow decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@dag(start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"), catchup=False, tags=[\"example\"])\ndef example_short_circuit_decorator():\n    # [START howto_operator_short_circuit]\n    @task.short_circuit()\n    def check_condition(condition):\n        return condition\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    condition_is_true = check_condition.override(task_id=\"condition_is_true\")(condition=True)\n    condition_is_false = check_condition.override(task_id=\"condition_is_false\")(condition=False)\n\n    chain(condition_is_true, *ds_true)\n    chain(condition_is_false, *ds_false)\n    # [END howto_operator_short_circuit]\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = check_condition.override(task_id=\"short_circuit\", ignore_downstream_trigger_rules=False)(\n        condition=False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)\n    # [END howto_operator_short_circuit_trigger_rules]\n\n\nexample_dag = example_short_circuit_decorator()", "metadata": {"file_name": "example_short_circuit_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates complex task dependencies, multiple task groups, and advanced trigger rule configurations with ignore_downstream_trigger_rules", "key_concepts": ["Trigger rules", "Task dependencies", "Downstream task management", "Conditional workflow design", "Task chaining strategies"], "learning_objectives": ["Understand complex task dependency management", "Learn how to control task execution flow", "Explore Airflow's flexible task routing capabilities"]}}
{"instruction": "Design a DAG that demonstrates conditional task execution using ShortCircuitOperator, showing how to control workflow progression based on boolean conditions.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "ShortCircuitOperator"], "line_count": 44, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the ShortCircuitOperator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import ShortCircuitOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_short_circuit_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    cond_true = ShortCircuitOperator(\n        task_id=\"condition_is_True\",\n        python_callable=lambda: True,\n    )\n\n    cond_false = ShortCircuitOperator(\n        task_id=\"condition_is_False\",\n        python_callable=lambda: False,\n    )\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    chain(cond_true, *ds_true)\n    chain(cond_false, *ds_false)\n\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = ShortCircuitOperator(\n        task_id=\"short_circuit\", ignore_downstream_trigger_rules=False, python_callable=lambda: False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)", "metadata": {"file_name": "example_short_circuit_operator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple operators, demonstrates advanced flow control with ShortCircuitOperator, includes lambda functions and task chaining", "key_concepts": ["ShortCircuitOperator", "Task Chaining", "Conditional Execution", "Lambda Functions", "Trigger Rules"], "learning_objectives": ["Understand how to conditionally skip downstream tasks", "Learn task dependency management", "Explore dynamic workflow control"]}}
{"instruction": "Create a workflow that illustrates complex task dependencies and trigger rules, focusing on how different operators can be connected and controlled in an Airflow pipeline.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator", "ShortCircuitOperator"], "line_count": 44, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the ShortCircuitOperator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import ShortCircuitOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_short_circuit_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    cond_true = ShortCircuitOperator(\n        task_id=\"condition_is_True\",\n        python_callable=lambda: True,\n    )\n\n    cond_false = ShortCircuitOperator(\n        task_id=\"condition_is_False\",\n        python_callable=lambda: False,\n    )\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    chain(cond_true, *ds_true)\n    chain(cond_false, *ds_false)\n\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = ShortCircuitOperator(\n        task_id=\"short_circuit\", ignore_downstream_trigger_rules=False, python_callable=lambda: False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)", "metadata": {"file_name": "example_short_circuit_operator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates advanced task dependencies, uses multiple task lists, includes custom trigger rules and short-circuiting logic", "key_concepts": ["Task Dependencies", "Trigger Rules", "Parallel Task Execution", "EmptyOperator", "Task Chaining"], "learning_objectives": ["Understand complex task dependency patterns", "Learn how to manage task execution flow", "Explore different task connection strategies"]}}
{"instruction": "Build a DAG that demonstrates task grouping and nested task groups, showing how to organize and structure complex workflows using Airflow's TaskGroup functionality.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "EmptyOperator"], "line_count": 47, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\nwith DAG(\n    dag_id=\"example_task_group\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start = EmptyOperator(task_id=\"start\")\n\n    # [START howto_task_group_section_1]\n    with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n        task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n        task_3 = EmptyOperator(task_id=\"task_3\")\n\n        task_1 >> [task_2, task_3]\n    # [END howto_task_group_section_1]\n\n    # [START howto_task_group_section_2]\n    with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n\n        # [START howto_task_group_inner_section_2]\n        with TaskGroup(\"inner_section_2\", tooltip=\"Tasks for inner_section2\") as inner_section_2:\n            task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            [task_2, task_3] >> task_4\n        # [END howto_task_group_inner_section_2]\n\n    # [END howto_task_group_section_2]\n\n    end = EmptyOperator(task_id=\"end\")\n\n    start >> section_1 >> section_2 >> end\n# [END howto_task_group]", "metadata": {"file_name": "example_task_group.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses nested TaskGroups, multiple task dependencies, and demonstrates intermediate Airflow workflow organization techniques beyond simple linear execution", "key_concepts": ["TaskGroup", "Task Dependencies", "Workflow Structuring", "Nested Groups", "Operator Composition"], "learning_objectives": ["Understand how to create task groups", "Learn nested task group organization", "Practice defining task dependencies within groups"]}}
{"instruction": "Design a modular workflow that demonstrates how to logically segment different stages of data processing using Airflow's TaskGroup, showing separation of concerns and hierarchical task management.", "input": {"airflow_version": "2.7.2", "operators": ["BashOperator", "EmptyOperator"], "line_count": 47, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\nwith DAG(\n    dag_id=\"example_task_group\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start = EmptyOperator(task_id=\"start\")\n\n    # [START howto_task_group_section_1]\n    with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n        task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n        task_3 = EmptyOperator(task_id=\"task_3\")\n\n        task_1 >> [task_2, task_3]\n    # [END howto_task_group_section_1]\n\n    # [START howto_task_group_section_2]\n    with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n\n        # [START howto_task_group_inner_section_2]\n        with TaskGroup(\"inner_section_2\", tooltip=\"Tasks for inner_section2\") as inner_section_2:\n            task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            [task_2, task_3] >> task_4\n        # [END howto_task_group_inner_section_2]\n\n    # [END howto_task_group_section_2]\n\n    end = EmptyOperator(task_id=\"end\")\n\n    start >> section_1 >> section_2 >> end\n# [END howto_task_group]", "metadata": {"file_name": "example_task_group.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple task groups with inner sections, demonstrates advanced workflow decomposition and parallel task execution strategies", "key_concepts": ["Workflow Modularity", "Task Isolation", "Hierarchical Workflow Design", "Parallel Task Execution", "Logical Segmentation"], "learning_objectives": ["Create logically separated workflow sections", "Understand task group hierarchy", "Practice modular workflow design"]}}
{"instruction": "Create a DAG that demonstrates XCom argument passing between tasks using the TaskFlow API, showing how tasks can dynamically exchange data and communicate in an Airflow pipeline.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 48, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the XComArgs.\"\"\"\nfrom __future__ import annotations\n\nimport logging\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef generate_value():\n    \"\"\"Empty function\"\"\"\n    return \"Bring me a shrubbery!\"\n\n\n@task\ndef print_value(value, ts=None):\n    \"\"\"Empty function\"\"\"\n    log.info(\"The knights of Ni say: %s (at %s)\", value, ts)\n\n\nwith DAG(\n    dag_id=\"example_xcom_args\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    print_value(generate_value())\n\nwith DAG(\n    \"example_xcom_args_with_operators\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag2:\n    bash_op1 = BashOperator(task_id=\"c\", bash_command=\"echo c\")\n    bash_op2 = BashOperator(task_id=\"d\", bash_command=\"echo c\")\n    xcom_args_a = print_value(\"first!\")\n    xcom_args_b = print_value(\"second!\")\n\n    bash_op1 >> xcom_args_a >> xcom_args_b >> bash_op2", "metadata": {"file_name": "example_xcomargs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with two tasks using @task decorators, minimal code, straightforward XCom argument passing, no complex dependencies", "key_concepts": ["TaskFlow API", "XCom arguments", "Task decorators", "Logging", "DAG configuration"], "learning_objectives": ["Understand XCom argument passing", "Learn how to use @task decorators", "Explore basic task communication in Airflow"]}}
{"instruction": "Design a workflow that demonstrates task dependencies and logging using different Airflow operators, showcasing how BashOperators and custom tasks can be sequenced and share context.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 48, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the XComArgs.\"\"\"\nfrom __future__ import annotations\n\nimport logging\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef generate_value():\n    \"\"\"Empty function\"\"\"\n    return \"Bring me a shrubbery!\"\n\n\n@task\ndef print_value(value, ts=None):\n    \"\"\"Empty function\"\"\"\n    log.info(\"The knights of Ni say: %s (at %s)\", value, ts)\n\n\nwith DAG(\n    dag_id=\"example_xcom_args\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    print_value(generate_value())\n\nwith DAG(\n    \"example_xcom_args_with_operators\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag2:\n    bash_op1 = BashOperator(task_id=\"c\", bash_command=\"echo c\")\n    bash_op2 = BashOperator(task_id=\"d\", bash_command=\"echo c\")\n    xcom_args_a = print_value(\"first!\")\n    xcom_args_b = print_value(\"second!\")\n\n    bash_op1 >> xcom_args_a >> xcom_args_b >> bash_op2", "metadata": {"file_name": "example_xcomargs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Uses multiple operators (BashOperator and custom tasks), simple sequential dependencies, minimal code complexity", "key_concepts": ["Task dependencies", "BashOperator", "Operator chaining", "Logging in Airflow", "DAG configuration"], "learning_objectives": ["Understand task dependency management", "Learn how to chain different operator types", "Explore logging and context in Airflow tasks"]}}
{"instruction": "Build an Airflow DAG that demonstrates an asynchronous time-based sensor which waits for a specific time delta before completing the workflow.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator"], "line_count": 24, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating ``TimeDeltaSensorAsync``, a drop in replacement for ``TimeDeltaSensor`` that\ndefers and doesn't occupy a worker slot while it waits\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.time_delta import TimeDeltaSensorAsync\n\nwith DAG(\n    dag_id=\"example_time_delta_sensor_async\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    wait = TimeDeltaSensorAsync(task_id=\"wait\", delta=datetime.timedelta(seconds=30))\n    finish = EmptyOperator(task_id=\"finish\")\n    wait >> finish", "metadata": {"file_name": "example_time_delta_sensor_async.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with only two operators, no complex logic, using a basic async sensor with sequential task dependency", "key_concepts": ["Airflow DAG configuration", "Async sensors", "Task dependencies", "TimeDeltaSensorAsync"], "learning_objectives": ["Understand async sensor usage", "Learn task dependency syntax", "Explore non-blocking waiting mechanisms"]}}
{"instruction": "Create a workflow that illustrates how asynchronous sensors can optimize worker resource utilization by deferring task execution without blocking worker slots.", "input": {"airflow_version": "2.7.2", "operators": ["EmptyOperator"], "line_count": 24, "is_multifile": false}, "output": "\"\"\"\nExample DAG demonstrating ``TimeDeltaSensorAsync``, a drop in replacement for ``TimeDeltaSensor`` that\ndefers and doesn't occupy a worker slot while it waits\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.time_delta import TimeDeltaSensorAsync\n\nwith DAG(\n    dag_id=\"example_time_delta_sensor_async\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    wait = TimeDeltaSensorAsync(task_id=\"wait\", delta=datetime.timedelta(seconds=30))\n    finish = EmptyOperator(task_id=\"finish\")\n    wait >> finish", "metadata": {"file_name": "example_time_delta_sensor_async.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces advanced sensor concept with performance optimization perspective, requires understanding of Airflow's async execution model", "key_concepts": ["Resource management", "Async task execution", "Sensor optimization", "Worker slot efficiency"], "learning_objectives": ["Recognize performance implications of async sensors", "Understand non-blocking task waiting strategies", "Learn about Airflow's deferred execution model"]}}
{"instruction": "Create a DAG that demonstrates task grouping using the @task_group decorator, showing how to organize related tasks into a logical workflow with dynamic task generation.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 62, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the @taskgroup decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task, task_group\nfrom airflow.models.dag import DAG\n\n\n# [START howto_task_group_decorator]\n# Creating Tasks\n@task\ndef task_start():\n    \"\"\"Empty Task which is First Task of Dag\"\"\"\n    return \"[Task_start]\"\n\n\n@task\ndef task_1(value: int) -> str:\n    \"\"\"Empty Task1\"\"\"\n    return f\"[ Task1 {value} ]\"\n\n\n@task\ndef task_2(value: str) -> str:\n    \"\"\"Empty Task2\"\"\"\n    return f\"[ Task2 {value} ]\"\n\n\n@task\ndef task_3(value: str) -> None:\n    \"\"\"Empty Task3\"\"\"\n    print(f\"[ Task3 {value} ]\")\n\n\n@task\ndef task_end() -> None:\n    \"\"\"Empty Task which is Last Task of Dag\"\"\"\n    print(\"[ Task_End  ]\")\n\n\n# Creating TaskGroups\n@task_group\ndef task_group_function(value: int) -> None:\n    \"\"\"TaskGroup for grouping related Tasks\"\"\"\n    task_3(task_2(task_1(value)))\n\n\n# Executing Tasks and TaskGroups\nwith DAG(\n    dag_id=\"example_task_group_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start_task = task_start()\n    end_task = task_end()\n    for i in range(5):\n        current_task_group = task_group_function(i)\n        start_task >> current_task_group >> end_task\n\n# [END howto_task_group_decorator]", "metadata": {"file_name": "example_task_group_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses TaskFlow API, dynamic task generation with loop, nested task composition, and task group decorator - more advanced than basic sequential tasks", "key_concepts": ["TaskFlow API", "@task decorator", "@task_group decorator", "Dynamic task generation", "Task dependencies"], "learning_objectives": ["Understand task grouping in Airflow", "Learn how to create dynamic workflows", "Practice using decorators for task definition"]}}
{"instruction": "Design a modular data processing workflow that demonstrates how to create reusable task groups with parameterized inputs and chained task execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task"], "line_count": 62, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the @taskgroup decorator.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task, task_group\nfrom airflow.models.dag import DAG\n\n\n# [START howto_task_group_decorator]\n# Creating Tasks\n@task\ndef task_start():\n    \"\"\"Empty Task which is First Task of Dag\"\"\"\n    return \"[Task_start]\"\n\n\n@task\ndef task_1(value: int) -> str:\n    \"\"\"Empty Task1\"\"\"\n    return f\"[ Task1 {value} ]\"\n\n\n@task\ndef task_2(value: str) -> str:\n    \"\"\"Empty Task2\"\"\"\n    return f\"[ Task2 {value} ]\"\n\n\n@task\ndef task_3(value: str) -> None:\n    \"\"\"Empty Task3\"\"\"\n    print(f\"[ Task3 {value} ]\")\n\n\n@task\ndef task_end() -> None:\n    \"\"\"Empty Task which is Last Task of Dag\"\"\"\n    print(\"[ Task_End  ]\")\n\n\n# Creating TaskGroups\n@task_group\ndef task_group_function(value: int) -> None:\n    \"\"\"TaskGroup for grouping related Tasks\"\"\"\n    task_3(task_2(task_1(value)))\n\n\n# Executing Tasks and TaskGroups\nwith DAG(\n    dag_id=\"example_task_group_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start_task = task_start()\n    end_task = task_end()\n    for i in range(5):\n        current_task_group = task_group_function(i)\n        start_task >> current_task_group >> end_task\n\n# [END howto_task_group_decorator]", "metadata": {"file_name": "example_task_group_decorator.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves function composition, task chaining, parameterized tasks, and demonstrating how tasks can be dynamically generated and connected", "key_concepts": ["Task composition", "Parameterized tasks", "Workflow modularity", "Task chaining", "Functional programming in Airflow"], "learning_objectives": ["Create modular and reusable task groups", "Understand task input/output relationships", "Learn how to create flexible workflows"]}}
{"instruction": "Create a DAG that demonstrates how to pass configuration data between DAG runs using the @task decorator and BashOperator, focusing on dynamic message passing and task configuration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 38, "is_multifile": false}, "output": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef run_this_func(dag_run=None):\n    \"\"\"\n    Print the payload \"message\" passed to the DagRun conf attribute.\n\n    :param dag_run: The DagRun object\n    \"\"\"\n    print(f\"Remotely received value of {dag_run.conf.get('message')} for key=message\")\n\n\nwith DAG(\n    dag_id=\"example_trigger_target_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_this = run_this_func()\n\n    bash_task = BashOperator(\n        task_id=\"bash_task\",\n        bash_command='echo \"Here is the message: $message\"',\n        env={\"message\": '{{ dag_run.conf.get(\"message\") }}'},\n    )", "metadata": {"file_name": "example_trigger_target_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with two tasks, uses basic @task decorator and BashOperator, no complex dependencies or external integrations", "key_concepts": ["DAG configuration", "Task decorators", "Dynamic task parameters", "Conf attribute usage"], "learning_objectives": ["Understand how to pass runtime configuration to DAG tasks", "Learn to use dag_run.conf for dynamic task parameterization", "Explore @task decorator and BashOperator interaction"]}}
{"instruction": "Design a workflow that demonstrates inter-DAG communication by creating a target DAG that can receive and process dynamic configuration parameters from a triggering DAG.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 38, "is_multifile": false}, "output": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef run_this_func(dag_run=None):\n    \"\"\"\n    Print the payload \"message\" passed to the DagRun conf attribute.\n\n    :param dag_run: The DagRun object\n    \"\"\"\n    print(f\"Remotely received value of {dag_run.conf.get('message')} for key=message\")\n\n\nwith DAG(\n    dag_id=\"example_trigger_target_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_this = run_this_func()\n\n    bash_task = BashOperator(\n        task_id=\"bash_task\",\n        bash_command='echo \"Here is the message: $message\"',\n        env={\"message\": '{{ dag_run.conf.get(\"message\") }}'},\n    )", "metadata": {"file_name": "example_trigger_target_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves inter-DAG communication, uses more advanced configuration passing techniques, requires understanding of trigger mechanisms", "key_concepts": ["Inter-DAG communication", "Dynamic configuration", "Task runtime parameters", "Trigger mechanisms"], "learning_objectives": ["Understand how DAGs can communicate through configuration", "Learn to create flexible, parameterized workflows", "Explore runtime configuration injection techniques"]}}
{"instruction": "Build an Airflow DAG that demonstrates XCom data sharing techniques between Python tasks and Bash operators, focusing on how to push and pull values across different task types.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 76, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of XComs.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, XComArg\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nvalue_1 = [1, 2, 3]\nvalue_2 = {\"a\": \"b\"}\n\n\n@task\ndef push(ti=None):\n    \"\"\"Pushes an XCom without a specific target\"\"\"\n    ti.xcom_push(key=\"value from pusher 1\", value=value_1)\n\n\n@task\ndef push_by_returning():\n    \"\"\"Pushes an XCom without a specific target, just by returning it\"\"\"\n    return value_2\n\n\ndef _compare_values(pulled_value, check_value):\n    if pulled_value != check_value:\n        raise ValueError(f\"The two values differ {pulled_value} and {check_value}\")\n\n\n@task\ndef puller(pulled_value_2, ti=None):\n    \"\"\"Pull all previously pushed XComs and check if the pushed values match the pulled values.\"\"\"\n    pulled_value_1 = ti.xcom_pull(task_ids=\"push\", key=\"value from pusher 1\")\n\n    _compare_values(pulled_value_1, value_1)\n    _compare_values(pulled_value_2, value_2)\n\n\n@task\ndef pull_value_from_bash_push(ti=None):\n    bash_pushed_via_return_value = ti.xcom_pull(key=\"return_value\", task_ids=\"bash_push\")\n    bash_manually_pushed_value = ti.xcom_pull(key=\"manually_pushed_value\", task_ids=\"bash_push\")\n    print(f\"The xcom value pushed by task push via return value is {bash_pushed_via_return_value}\")\n    print(f\"The xcom value pushed by task push manually is {bash_manually_pushed_value}\")\n\n\nwith DAG(\n    \"example_xcom\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    bash_push = BashOperator(\n        task_id=\"bash_push\",\n        bash_command='echo \"bash_push demo\"  && '\n        'echo \"Manually set xcom value '\n        '{{ ti.xcom_push(key=\"manually_pushed_value\", value=\"manually_pushed_value\") }}\" && '\n        'echo \"value_by_return\"',\n    )\n\n    bash_pull = BashOperator(\n        task_id=\"bash_pull\",\n        bash_command='echo \"bash pull demo\" && '\n        f'echo \"The xcom pushed manually is {XComArg(bash_push, key=\"manually_pushed_value\")}\" && '\n        f'echo \"The returned_value xcom is {XComArg(bash_push)}\" && '\n        'echo \"finished\"',\n        do_xcom_push=False,\n    )\n\n    python_pull_from_bash = pull_value_from_bash_push()\n\n    [bash_pull, python_pull_from_bash] << bash_push\n\n    puller(push_by_returning()) << push()", "metadata": {"file_name": "example_xcom.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple task types, demonstrates XCom push/pull mechanisms, includes custom Python functions and Bash operators with advanced XCom interactions", "key_concepts": ["XCom data sharing", "Task decorators", "BashOperator", "Python task functions", "Task dependencies"], "learning_objectives": ["Understand XCom value pushing techniques", "Learn how to share data between tasks", "Explore different XCom push methods", "Practice task dependency management"]}}
{"instruction": "Create an Airflow workflow that explores XCom value validation and cross-task data verification, demonstrating how to implement custom comparison logic between task outputs.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BashOperator"], "line_count": 76, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of XComs.\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG, XComArg\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nvalue_1 = [1, 2, 3]\nvalue_2 = {\"a\": \"b\"}\n\n\n@task\ndef push(ti=None):\n    \"\"\"Pushes an XCom without a specific target\"\"\"\n    ti.xcom_push(key=\"value from pusher 1\", value=value_1)\n\n\n@task\ndef push_by_returning():\n    \"\"\"Pushes an XCom without a specific target, just by returning it\"\"\"\n    return value_2\n\n\ndef _compare_values(pulled_value, check_value):\n    if pulled_value != check_value:\n        raise ValueError(f\"The two values differ {pulled_value} and {check_value}\")\n\n\n@task\ndef puller(pulled_value_2, ti=None):\n    \"\"\"Pull all previously pushed XComs and check if the pushed values match the pulled values.\"\"\"\n    pulled_value_1 = ti.xcom_pull(task_ids=\"push\", key=\"value from pusher 1\")\n\n    _compare_values(pulled_value_1, value_1)\n    _compare_values(pulled_value_2, value_2)\n\n\n@task\ndef pull_value_from_bash_push(ti=None):\n    bash_pushed_via_return_value = ti.xcom_pull(key=\"return_value\", task_ids=\"bash_push\")\n    bash_manually_pushed_value = ti.xcom_pull(key=\"manually_pushed_value\", task_ids=\"bash_push\")\n    print(f\"The xcom value pushed by task push via return value is {bash_pushed_via_return_value}\")\n    print(f\"The xcom value pushed by task push manually is {bash_manually_pushed_value}\")\n\n\nwith DAG(\n    \"example_xcom\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    bash_push = BashOperator(\n        task_id=\"bash_push\",\n        bash_command='echo \"bash_push demo\"  && '\n        'echo \"Manually set xcom value '\n        '{{ ti.xcom_push(key=\"manually_pushed_value\", value=\"manually_pushed_value\") }}\" && '\n        'echo \"value_by_return\"',\n    )\n\n    bash_pull = BashOperator(\n        task_id=\"bash_pull\",\n        bash_command='echo \"bash pull demo\" && '\n        f'echo \"The xcom pushed manually is {XComArg(bash_push, key=\"manually_pushed_value\")}\" && '\n        f'echo \"The returned_value xcom is {XComArg(bash_push)}\" && '\n        'echo \"finished\"',\n        do_xcom_push=False,\n    )\n\n    python_pull_from_bash = pull_value_from_bash_push()\n\n    [bash_pull, python_pull_from_bash] << bash_push\n\n    puller(push_by_returning()) << push()", "metadata": {"file_name": "example_xcom.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Includes custom validation function, multiple XCom retrieval methods, and complex task interaction patterns with error checking", "key_concepts": ["XCom value validation", "Custom error handling", "Task return values", "Programmatic task interaction", "Data consistency checking"], "learning_objectives": ["Implement custom XCom validation logic", "Understand task-to-task data verification", "Learn advanced XCom retrieval techniques", "Practice defensive programming in Airflow"]}}
{"instruction": "Create a DAG that demonstrates inter-DAG triggering by building a controller DAG that triggers another target DAG with a configuration payload, showcasing how Airflow can orchestrate complex workflow dependencies.", "input": {"airflow_version": "2.7.2", "operators": ["TriggerDagRunOperator"], "line_count": 24, "is_multifile": false}, "output": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(\n    dag_id=\"example_trigger_controller_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n    trigger = TriggerDagRunOperator(\n        task_id=\"test_trigger_dagrun\",\n        trigger_dag_id=\"example_trigger_target_dag\",  # Ensure this equals the dag_id of the DAG to trigger\n        conf={\"message\": \"Hello World\"},\n    )", "metadata": {"file_name": "example_trigger_controller_dag.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with single TriggerDagRunOperator, minimal code, no complex dependencies, straightforward scheduling", "key_concepts": ["TriggerDagRunOperator", "DAG Triggering", "Configuration Passing", "Workflow Orchestration"], "learning_objectives": ["Understand inter-DAG communication", "Learn how to trigger DAGs dynamically", "Explore configuration passing between DAGs"]}}
{"instruction": "Design a modular workflow system where a controller DAG manages the execution sequence of multiple target DAGs, demonstrating Airflow's capability to create complex, interconnected data pipeline architectures.", "input": {"airflow_version": "2.7.2", "operators": ["TriggerDagRunOperator"], "line_count": 24, "is_multifile": false}, "output": "\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow import DAG\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(\n    dag_id=\"example_trigger_controller_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n    trigger = TriggerDagRunOperator(\n        task_id=\"test_trigger_dagrun\",\n        trigger_dag_id=\"example_trigger_target_dag\",  # Ensure this equals the dag_id of the DAG to trigger\n        conf={\"message\": \"Hello World\"},\n    )", "metadata": {"file_name": "example_trigger_controller_dag.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Introduces more advanced DAG interaction concept, requires understanding of operator configuration and inter-DAG communication patterns", "key_concepts": ["Workflow Modularity", "Dynamic DAG Execution", "Airflow Orchestration Patterns", "Configuration Management"], "learning_objectives": ["Explore advanced DAG interaction techniques", "Understand dynamic workflow orchestration", "Learn configuration-driven pipeline design"]}}
{"instruction": "Build an Airflow DAG that demonstrates Cloud SQL query execution across multiple database types (PostgreSQL and MySQL) using environment variables and secure connection configurations.", "input": {"airflow_version": "2.7.2", "operators": ["CloudSQLExecuteQueryOperator"], "line_count": 271, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that performs query in a Cloud SQL instance.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project for the Cloud SQL instance\n* GCP_REGION - Google Cloud region where the database is created\n*\n* GCSQL_POSTGRES_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_POSTGRES_USER - Name of the postgres database user\n* GCSQL_POSTGRES_PASSWORD - Password of the postgres database user\n* GCSQL_POSTGRES_PUBLIC_IP - Public IP of the Postgres database\n* GCSQL_POSTGRES_PUBLIC_PORT - Port of the postgres database\n*\n* GCSQL_MYSQL_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_MYSQL_USER - Name of the mysql database user\n* GCSQL_MYSQL_PASSWORD - Password of the mysql database user\n* GCSQL_MYSQL_PUBLIC_IP - Public IP of the mysql database\n* GCSQL_MYSQL_PUBLIC_PORT - Port of the mysql database\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom urllib.parse import quote_plus\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.cloud_sql import CloudSQLExecuteQueryOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_REGION = os.environ.get(\"GCP_REGION\", \"europe-west1\")\n\nGCSQL_POSTGRES_INSTANCE_NAME_QUERY = os.environ.get(\n    \"GCSQL_POSTGRES_INSTANCE_NAME_QUERY\", \"test-postgres-query\"\n)\nGCSQL_POSTGRES_DATABASE_NAME = os.environ.get(\"GCSQL_POSTGRES_DATABASE_NAME\", \"postgresdb\")\nGCSQL_POSTGRES_USER = os.environ.get(\"GCSQL_POSTGRES_USER\", \"postgres_user\")\nGCSQL_POSTGRES_PASSWORD = os.environ.get(\"GCSQL_POSTGRES_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_POSTGRES_PUBLIC_IP = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_POSTGRES_PUBLIC_PORT = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_PORT\", 5432)\nGCSQL_POSTGRES_CLIENT_CERT_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_CERT_FILE\", \".key/postgres-client-cert.pem\"\n)\nGCSQL_POSTGRES_CLIENT_KEY_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_KEY_FILE\", \".key/postgres-client-key.pem\"\n)\nGCSQL_POSTGRES_SERVER_CA_FILE = os.environ.get(\"GCSQL_POSTGRES_SERVER_CA_FILE\", \".key/postgres-server-ca.pem\")\n\nGCSQL_MYSQL_INSTANCE_NAME_QUERY = os.environ.get(\"GCSQL_MYSQL_INSTANCE_NAME_QUERY\", \"test-mysql-query\")\nGCSQL_MYSQL_DATABASE_NAME = os.environ.get(\"GCSQL_MYSQL_DATABASE_NAME\", \"mysqldb\")\nGCSQL_MYSQL_USER = os.environ.get(\"GCSQL_MYSQL_USER\", \"mysql_user\")\nGCSQL_MYSQL_PASSWORD = os.environ.get(\"GCSQL_MYSQL_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_MYSQL_PUBLIC_IP = os.environ.get(\"GCSQL_MYSQL_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_MYSQL_PUBLIC_PORT = os.environ.get(\"GCSQL_MYSQL_PUBLIC_PORT\", 3306)\nGCSQL_MYSQL_CLIENT_CERT_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_CERT_FILE\", \".key/mysql-client-cert.pem\")\nGCSQL_MYSQL_CLIENT_KEY_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_KEY_FILE\", \".key/mysql-client-key.pem\")\nGCSQL_MYSQL_SERVER_CA_FILE = os.environ.get(\"GCSQL_MYSQL_SERVER_CA_FILE\", \".key/mysql-server-ca.pem\")\n\nSQL = [\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",  # shows warnings logged\n    \"INSERT INTO TABLE_TEST VALUES (0)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST2 (I INTEGER)\",\n    \"DROP TABLE TABLE_TEST\",\n    \"DROP TABLE TABLE_TEST2\",\n]\n\n\n# [START howto_operator_cloudsql_query_connections]\n\nHOME_DIR = Path.home()\n\n\ndef get_absolute_path(path):\n    \"\"\"\n    Returns absolute path.\n    \"\"\"\n    return os.fspath(HOME_DIR / path)\n\n\npostgres_kwargs = dict(\n    user=quote_plus(GCSQL_POSTGRES_USER),\n    password=quote_plus(GCSQL_POSTGRES_PASSWORD),\n    public_port=GCSQL_POSTGRES_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_POSTGRES_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_POSTGRES_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_POSTGRES_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_SERVER_CA_FILE)),\n)\n\n# The connections below are created using one of the standard approaches - via environment\n# variables named AIRFLOW_CONN_* . The connections can also be created in the database\n# of AIRFLOW (using command line or UI).\n\n# Postgres: connect via proxy over TCP\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=True\".format(**postgres_kwargs)\n)\n\n# Postgres: connect via proxy over UNIX socket (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**postgres_kwargs)\n)\n\nmysql_kwargs = dict(\n    user=quote_plus(GCSQL_MYSQL_USER),\n    password=quote_plus(GCSQL_MYSQL_PASSWORD),\n    public_port=GCSQL_MYSQL_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_MYSQL_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_MYSQL_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_MYSQL_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_MYSQL_SERVER_CA_FILE)),\n)\n\n# MySQL: connect via proxy over TCP (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=True\".format(**mysql_kwargs)\n)\n\n# MySQL: connect via proxy over UNIX socket using pre-downloaded Cloud Sql Proxy binary\ntry:\n    sql_proxy_binary_path = subprocess.check_output([\"which\", \"cloud_sql_proxy\"]).decode(\"utf-8\").rstrip()\nexcept subprocess.CalledProcessError:\n    sql_proxy_binary_path = \"/tmp/anyhow_download_cloud_sql_proxy\"\n\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql Proxy binary path\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n# Special case: MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql\n# Proxy binary path AND with missing project_id\n\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL_NO_PROJECT_ID\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n\n# [END howto_operator_cloudsql_query_connections]\n\n# [START howto_operator_cloudsql_query_operators]\n\nconnection_names = [\n    \"proxy_postgres_tcp\",\n    \"proxy_postgres_socket\",\n    \"public_postgres_tcp\",\n    \"public_postgres_tcp_ssl\",\n    \"proxy_mysql_tcp\",\n    \"proxy_mysql_socket\",\n    \"public_mysql_tcp\",\n    \"public_mysql_tcp_ssl\",\n    \"public_mysql_tcp_ssl_no_project_id\",\n]\n\ntasks = []\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_sql_query\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    prev_task = None\n\n    for connection_name in connection_names:\n        task = CloudSQLExecuteQueryOperator(\n            gcp_cloudsql_conn_id=connection_name,\n            task_id=\"example_gcp_sql_task_\" + connection_name,\n            sql=SQL,\n            sql_proxy_binary_path=sql_proxy_binary_path,\n        )\n        tasks.append(task)\n        if prev_task:\n            prev_task >> task\n        prev_task = task\n\n# [END howto_operator_cloudsql_query_operators]", "metadata": {"file_name": "example_cloud_sql_query.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operator, multiple database connections, environment variable configurations, and involves external service integration with security certificates", "key_concepts": ["CloudSQLExecuteQueryOperator", "Environment Variable Configuration", "Database Connection Management", "Cloud SQL Interactions"], "learning_objectives": ["Understand secure database connection strategies", "Learn dynamic configuration using environment variables", "Practice Cloud SQL query execution in Airflow"]}}
{"instruction": "Create an Airflow DAG that demonstrates robust database connection management by implementing secure, parameterized database interactions with comprehensive error handling and logging.", "input": {"airflow_version": "2.7.2", "operators": ["CloudSQLExecuteQueryOperator"], "line_count": 271, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that performs query in a Cloud SQL instance.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project for the Cloud SQL instance\n* GCP_REGION - Google Cloud region where the database is created\n*\n* GCSQL_POSTGRES_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_POSTGRES_USER - Name of the postgres database user\n* GCSQL_POSTGRES_PASSWORD - Password of the postgres database user\n* GCSQL_POSTGRES_PUBLIC_IP - Public IP of the Postgres database\n* GCSQL_POSTGRES_PUBLIC_PORT - Port of the postgres database\n*\n* GCSQL_MYSQL_INSTANCE_NAME - Name of the postgres Cloud SQL instance\n* GCSQL_MYSQL_USER - Name of the mysql database user\n* GCSQL_MYSQL_PASSWORD - Password of the mysql database user\n* GCSQL_MYSQL_PUBLIC_IP - Public IP of the mysql database\n* GCSQL_MYSQL_PUBLIC_PORT - Port of the mysql database\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom urllib.parse import quote_plus\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.cloud_sql import CloudSQLExecuteQueryOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_REGION = os.environ.get(\"GCP_REGION\", \"europe-west1\")\n\nGCSQL_POSTGRES_INSTANCE_NAME_QUERY = os.environ.get(\n    \"GCSQL_POSTGRES_INSTANCE_NAME_QUERY\", \"test-postgres-query\"\n)\nGCSQL_POSTGRES_DATABASE_NAME = os.environ.get(\"GCSQL_POSTGRES_DATABASE_NAME\", \"postgresdb\")\nGCSQL_POSTGRES_USER = os.environ.get(\"GCSQL_POSTGRES_USER\", \"postgres_user\")\nGCSQL_POSTGRES_PASSWORD = os.environ.get(\"GCSQL_POSTGRES_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_POSTGRES_PUBLIC_IP = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_POSTGRES_PUBLIC_PORT = os.environ.get(\"GCSQL_POSTGRES_PUBLIC_PORT\", 5432)\nGCSQL_POSTGRES_CLIENT_CERT_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_CERT_FILE\", \".key/postgres-client-cert.pem\"\n)\nGCSQL_POSTGRES_CLIENT_KEY_FILE = os.environ.get(\n    \"GCSQL_POSTGRES_CLIENT_KEY_FILE\", \".key/postgres-client-key.pem\"\n)\nGCSQL_POSTGRES_SERVER_CA_FILE = os.environ.get(\"GCSQL_POSTGRES_SERVER_CA_FILE\", \".key/postgres-server-ca.pem\")\n\nGCSQL_MYSQL_INSTANCE_NAME_QUERY = os.environ.get(\"GCSQL_MYSQL_INSTANCE_NAME_QUERY\", \"test-mysql-query\")\nGCSQL_MYSQL_DATABASE_NAME = os.environ.get(\"GCSQL_MYSQL_DATABASE_NAME\", \"mysqldb\")\nGCSQL_MYSQL_USER = os.environ.get(\"GCSQL_MYSQL_USER\", \"mysql_user\")\nGCSQL_MYSQL_PASSWORD = os.environ.get(\"GCSQL_MYSQL_PASSWORD\", \"JoxHlwrPzwch0gz9\")\nGCSQL_MYSQL_PUBLIC_IP = os.environ.get(\"GCSQL_MYSQL_PUBLIC_IP\", \"0.0.0.0\")\nGCSQL_MYSQL_PUBLIC_PORT = os.environ.get(\"GCSQL_MYSQL_PUBLIC_PORT\", 3306)\nGCSQL_MYSQL_CLIENT_CERT_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_CERT_FILE\", \".key/mysql-client-cert.pem\")\nGCSQL_MYSQL_CLIENT_KEY_FILE = os.environ.get(\"GCSQL_MYSQL_CLIENT_KEY_FILE\", \".key/mysql-client-key.pem\")\nGCSQL_MYSQL_SERVER_CA_FILE = os.environ.get(\"GCSQL_MYSQL_SERVER_CA_FILE\", \".key/mysql-server-ca.pem\")\n\nSQL = [\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST (I INTEGER)\",  # shows warnings logged\n    \"INSERT INTO TABLE_TEST VALUES (0)\",\n    \"CREATE TABLE IF NOT EXISTS TABLE_TEST2 (I INTEGER)\",\n    \"DROP TABLE TABLE_TEST\",\n    \"DROP TABLE TABLE_TEST2\",\n]\n\n\n# [START howto_operator_cloudsql_query_connections]\n\nHOME_DIR = Path.home()\n\n\ndef get_absolute_path(path):\n    \"\"\"\n    Returns absolute path.\n    \"\"\"\n    return os.fspath(HOME_DIR / path)\n\n\npostgres_kwargs = dict(\n    user=quote_plus(GCSQL_POSTGRES_USER),\n    password=quote_plus(GCSQL_POSTGRES_PASSWORD),\n    public_port=GCSQL_POSTGRES_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_POSTGRES_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_POSTGRES_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_POSTGRES_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_POSTGRES_SERVER_CA_FILE)),\n)\n\n# The connections below are created using one of the standard approaches - via environment\n# variables named AIRFLOW_CONN_* . The connections can also be created in the database\n# of AIRFLOW (using command line or UI).\n\n# Postgres: connect via proxy over TCP\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=True\".format(**postgres_kwargs)\n)\n\n# Postgres: connect via proxy over UNIX socket (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_POSTGRES_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**postgres_kwargs)\n)\n\n# Postgres: connect directly via TCP (SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_POSTGRES_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=postgres&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**postgres_kwargs)\n)\n\nmysql_kwargs = dict(\n    user=quote_plus(GCSQL_MYSQL_USER),\n    password=quote_plus(GCSQL_MYSQL_PASSWORD),\n    public_port=GCSQL_MYSQL_PUBLIC_PORT,\n    public_ip=quote_plus(GCSQL_MYSQL_PUBLIC_IP),\n    project_id=quote_plus(GCP_PROJECT_ID),\n    location=quote_plus(GCP_REGION),\n    instance=quote_plus(GCSQL_MYSQL_INSTANCE_NAME_QUERY),\n    database=quote_plus(GCSQL_MYSQL_DATABASE_NAME),\n    client_cert_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_CERT_FILE)),\n    client_key_file=quote_plus(get_absolute_path(GCSQL_MYSQL_CLIENT_KEY_FILE)),\n    server_ca_file=quote_plus(get_absolute_path(GCSQL_MYSQL_SERVER_CA_FILE)),\n)\n\n# MySQL: connect via proxy over TCP (specific proxy version)\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_version=v1.13&\"\n    \"sql_proxy_use_tcp=True\".format(**mysql_kwargs)\n)\n\n# MySQL: connect via proxy over UNIX socket using pre-downloaded Cloud Sql Proxy binary\ntry:\n    sql_proxy_binary_path = subprocess.check_output([\"which\", \"cloud_sql_proxy\"]).decode(\"utf-8\").rstrip()\nexcept subprocess.CalledProcessError:\n    sql_proxy_binary_path = \"/tmp/anyhow_download_cloud_sql_proxy\"\n\nos.environ[\"AIRFLOW_CONN_PROXY_MYSQL_SOCKET\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=True&\"\n    \"sql_proxy_use_tcp=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (non-SSL)\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=False\".format(**mysql_kwargs)\n)\n\n# MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql Proxy binary path\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"project_id={project_id}&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n# Special case: MySQL: connect directly via TCP (SSL) and with fixed Cloud Sql\n# Proxy binary path AND with missing project_id\n\nos.environ[\"AIRFLOW_CONN_PUBLIC_MYSQL_TCP_SSL_NO_PROJECT_ID\"] = (\n    \"gcpcloudsql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n    \"database_type=mysql&\"\n    \"location={location}&\"\n    \"instance={instance}&\"\n    \"use_proxy=False&\"\n    \"use_ssl=True&\"\n    \"sslcert={client_cert_file}&\"\n    \"sslkey={client_key_file}&\"\n    \"sslrootcert={server_ca_file}\".format(**mysql_kwargs)\n)\n\n\n# [END howto_operator_cloudsql_query_connections]\n\n# [START howto_operator_cloudsql_query_operators]\n\nconnection_names = [\n    \"proxy_postgres_tcp\",\n    \"proxy_postgres_socket\",\n    \"public_postgres_tcp\",\n    \"public_postgres_tcp_ssl\",\n    \"proxy_mysql_tcp\",\n    \"proxy_mysql_socket\",\n    \"public_mysql_tcp\",\n    \"public_mysql_tcp_ssl\",\n    \"public_mysql_tcp_ssl_no_project_id\",\n]\n\ntasks = []\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_sql_query\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    prev_task = None\n\n    for connection_name in connection_names:\n        task = CloudSQLExecuteQueryOperator(\n            gcp_cloudsql_conn_id=connection_name,\n            task_id=\"example_gcp_sql_task_\" + connection_name,\n            sql=SQL,\n            sql_proxy_binary_path=sql_proxy_binary_path,\n        )\n        tasks.append(task)\n        if prev_task:\n            prev_task >> task\n        prev_task = task\n\n# [END howto_operator_cloudsql_query_operators]", "metadata": {"file_name": "example_cloud_sql_query.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex connection setup, multiple security layers, quote_plus encoding, and multiple database interaction scenarios", "key_concepts": ["Connection Security", "Parameterized Database Queries", "Error Handling", "Multi-Database Support"], "learning_objectives": ["Master secure database connection techniques", "Understand dynamic connection parameter management", "Learn cross-database query execution strategies"]}}
{"instruction": "Build an Airflow DAG that demonstrates ArangoDB data querying using AQLSensor and AQLOperator, focusing on retrieving student records through different query methods.", "input": {"airflow_version": "2.7.2", "operators": ["AQLOperator", "AQLSensor"], "line_count": 61, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.arangodb.operators.arangodb import AQLOperator\nfrom airflow.providers.arangodb.sensors.arangodb import AQLSensor\n\ndag = DAG(\n    \"example_arangodb_operator\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n)\n\n# [START howto_aql_sensor_arangodb]\n\nsensor = AQLSensor(\n    task_id=\"aql_sensor\",\n    query=\"FOR doc IN students FILTER doc.name == 'judy' RETURN doc\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_arangodb]\n\n# [START howto_aql_sensor_template_file_arangodb]\n\nsensor2 = AQLSensor(\n    task_id=\"aql_sensor_template_file\",\n    query=\"search_judy.sql\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_template_file_arangodb]\n\n\n# [START howto_aql_operator_arangodb]\n\noperator = AQLOperator(\n    task_id=\"aql_operator\",\n    query=\"FOR doc IN students RETURN doc\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n)\n\n# [END howto_aql_operator_arangodb]\n\n# [START howto_aql_operator_template_file_arangodb]\n\noperator2 = AQLOperator(\n    task_id=\"aql_operator_template_file\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n    query=\"search_all.sql\",\n)\n\n# [END howto_aql_operator_template_file_arangodb]", "metadata": {"file_name": "example_arangodb.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses external database integration, multiple operators with sensors and custom result processing, template file queries, and lambda function processing", "key_concepts": ["ArangoDB Integration", "AQL Querying", "Sensors", "Operators", "Result Processing"], "learning_objectives": ["Understand ArangoDB query patterns", "Learn Airflow sensor and operator usage", "Implement data retrieval workflows"]}}
{"instruction": "Design a workflow demonstrating flexible data retrieval techniques in Airflow, highlighting template-based queries and dynamic result processing for database interactions.", "input": {"airflow_version": "2.7.2", "operators": ["AQLOperator", "AQLSensor"], "line_count": 61, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.arangodb.operators.arangodb import AQLOperator\nfrom airflow.providers.arangodb.sensors.arangodb import AQLSensor\n\ndag = DAG(\n    \"example_arangodb_operator\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n)\n\n# [START howto_aql_sensor_arangodb]\n\nsensor = AQLSensor(\n    task_id=\"aql_sensor\",\n    query=\"FOR doc IN students FILTER doc.name == 'judy' RETURN doc\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_arangodb]\n\n# [START howto_aql_sensor_template_file_arangodb]\n\nsensor2 = AQLSensor(\n    task_id=\"aql_sensor_template_file\",\n    query=\"search_judy.sql\",\n    timeout=60,\n    poke_interval=10,\n    dag=dag,\n)\n\n# [END howto_aql_sensor_template_file_arangodb]\n\n\n# [START howto_aql_operator_arangodb]\n\noperator = AQLOperator(\n    task_id=\"aql_operator\",\n    query=\"FOR doc IN students RETURN doc\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n)\n\n# [END howto_aql_operator_arangodb]\n\n# [START howto_aql_operator_template_file_arangodb]\n\noperator2 = AQLOperator(\n    task_id=\"aql_operator_template_file\",\n    dag=dag,\n    result_processor=lambda cursor: print([document[\"name\"] for document in cursor]),\n    query=\"search_all.sql\",\n)\n\n# [END howto_aql_operator_template_file_arangodb]", "metadata": {"file_name": "example_arangodb.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves template file queries, lambda result processing, sensor polling, and custom query execution strategies with timeout configurations", "key_concepts": ["Query Templating", "Dynamic Result Processing", "Sensor Polling", "Database Interaction", "Task Configuration"], "learning_objectives": ["Master flexible query strategies", "Implement conditional data retrieval", "Understand Airflow task configuration"]}}
{"instruction": "Build an Airflow DAG that demonstrates asynchronous PDT (Persistent Derived Table) materialization in Looker, showing how to start a PDT build and monitor its completion using specialized Looker operators.", "input": {"airflow_version": "2.7.2", "operators": ["LookerCheckPdtBuildSensor", "LookerStartPdtBuildOperator"], "line_count": 46, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that show how to use various Looker\noperators to submit PDT materialization job and manage it.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.looker import LookerStartPdtBuildOperator\nfrom airflow.providers.google.cloud.sensors.looker import LookerCheckPdtBuildSensor\n\nwith models.DAG(\n    dag_id=\"example_gcp_looker\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START cloud_looker_async_start_pdt_sensor]\n    start_pdt_task_async = LookerStartPdtBuildOperator(\n        task_id=\"start_pdt_task_async\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n        asynchronous=True,\n    )\n\n    check_pdt_task_async_sensor = LookerCheckPdtBuildSensor(\n        task_id=\"check_pdt_task_async_sensor\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        materialization_id=start_pdt_task_async.output,\n        poke_interval=10,\n    )\n    # [END cloud_looker_async_start_pdt_sensor]\n\n    # [START how_to_cloud_looker_start_pdt_build_operator]\n    build_pdt_task = LookerStartPdtBuildOperator(\n        task_id=\"build_pdt_task\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n    )\n    # [END how_to_cloud_looker_start_pdt_build_operator]\n\n    start_pdt_task_async >> check_pdt_task_async_sensor\n\n    build_pdt_task", "metadata": {"file_name": "example_looker.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operators, includes a sensor for async task monitoring, involves external service integration with Looker, and demonstrates advanced task dependency", "key_concepts": ["Looker PDT Materialization", "Asynchronous Task Execution", "Cloud Service Operators", "Task Dependency", "Sensor Operators"], "learning_objectives": ["Understand Looker integration in Airflow", "Learn async task monitoring techniques", "Implement cloud service orchestration"]}}
{"instruction": "Create a workflow that explores different execution modes for Looker PDT builds, comparing synchronous and asynchronous task execution strategies while demonstrating proper connection management and task chaining.", "input": {"airflow_version": "2.7.2", "operators": ["LookerCheckPdtBuildSensor", "LookerStartPdtBuildOperator"], "line_count": 46, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that show how to use various Looker\noperators to submit PDT materialization job and manage it.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.looker import LookerStartPdtBuildOperator\nfrom airflow.providers.google.cloud.sensors.looker import LookerCheckPdtBuildSensor\n\nwith models.DAG(\n    dag_id=\"example_gcp_looker\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START cloud_looker_async_start_pdt_sensor]\n    start_pdt_task_async = LookerStartPdtBuildOperator(\n        task_id=\"start_pdt_task_async\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n        asynchronous=True,\n    )\n\n    check_pdt_task_async_sensor = LookerCheckPdtBuildSensor(\n        task_id=\"check_pdt_task_async_sensor\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        materialization_id=start_pdt_task_async.output,\n        poke_interval=10,\n    )\n    # [END cloud_looker_async_start_pdt_sensor]\n\n    # [START how_to_cloud_looker_start_pdt_build_operator]\n    build_pdt_task = LookerStartPdtBuildOperator(\n        task_id=\"build_pdt_task\",\n        looker_conn_id=\"your_airflow_connection_for_looker\",\n        model=\"your_lookml_model\",\n        view=\"your_lookml_view\",\n    )\n    # [END how_to_cloud_looker_start_pdt_build_operator]\n\n    start_pdt_task_async >> check_pdt_task_async_sensor\n\n    build_pdt_task", "metadata": {"file_name": "example_looker.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Compares multiple execution modes, uses specialized Looker operators, involves task dependency management, and requires understanding of async vs sync processing", "key_concepts": ["Execution Mode Comparison", "Task Chaining", "Connection Management", "Operator Configuration", "Service Integration"], "learning_objectives": ["Understand different task execution strategies", "Learn Looker operator configuration", "Master task dependency management"]}}
{"instruction": "Build an Airflow DAG that demonstrates Google Cloud Dataflow SQL job execution, focusing on extracting and aggregating sales data from BigQuery using a parameterized SQL query with dynamic filtering.", "input": {"airflow_version": "2.7.2", "operators": ["DataflowStartSqlJobOperator"], "line_count": 50, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartSqlJobOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nBQ_SQL_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_DATASET\", \"airflow_dataflow_samples\")\nBQ_SQL_TABLE_INPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_INPUT\", \"beam_input\")\nBQ_SQL_TABLE_OUTPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_OUTPUT\", \"beam_output\")\nDATAFLOW_SQL_JOB_NAME = os.environ.get(\"GCP_DATAFLOW_SQL_JOB_NAME\", \"dataflow-sql\")\nDATAFLOW_SQL_LOCATION = os.environ.get(\"GCP_DATAFLOW_SQL_LOCATION\", \"us-west1\")\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_sql\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag_sql:\n    # [START howto_operator_start_sql_job]\n    start_sql = DataflowStartSqlJobOperator(\n        task_id=\"start_sql_query\",\n        job_name=DATAFLOW_SQL_JOB_NAME,\n        query=f\"\"\"\n            SELECT\n                sales_region as sales_region,\n                count(state_id) as count_state\n            FROM\n                bigquery.table.`{GCP_PROJECT_ID}`.`{BQ_SQL_DATASET}`.`{BQ_SQL_TABLE_INPUT}`\n            WHERE state_id >= @state_id_min\n            GROUP BY sales_region;\n        \"\"\",\n        options={\n            \"bigquery-project\": GCP_PROJECT_ID,\n            \"bigquery-dataset\": BQ_SQL_DATASET,\n            \"bigquery-table\": BQ_SQL_TABLE_OUTPUT,\n            \"bigquery-write-disposition\": \"write-truncate\",\n            \"parameter\": \"state_id_min:INT64:2\",\n        },\n        location=DATAFLOW_SQL_LOCATION,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_start_sql_job]", "metadata": {"file_name": "example_dataflow_sql.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operator, external integration with BigQuery, parameterized query, and configuration via environment variables", "key_concepts": ["Cloud Dataflow", "BigQuery Integration", "SQL Job Execution", "Parameterized Queries", "Environment Configuration"], "learning_objectives": ["Understand Dataflow SQL job configuration", "Learn how to parameterize BigQuery queries", "Practice cloud service operator usage"]}}
{"instruction": "Design an Airflow workflow that demonstrates secure, configurable data processing by leveraging environment variables for project, dataset, and job location settings in a Google Cloud Dataflow SQL job.", "input": {"airflow_version": "2.7.2", "operators": ["DataflowStartSqlJobOperator"], "line_count": 50, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartSqlJobOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nBQ_SQL_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_DATASET\", \"airflow_dataflow_samples\")\nBQ_SQL_TABLE_INPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_INPUT\", \"beam_input\")\nBQ_SQL_TABLE_OUTPUT = os.environ.get(\"GCP_DATAFLOW_BQ_SQL_TABLE_OUTPUT\", \"beam_output\")\nDATAFLOW_SQL_JOB_NAME = os.environ.get(\"GCP_DATAFLOW_SQL_JOB_NAME\", \"dataflow-sql\")\nDATAFLOW_SQL_LOCATION = os.environ.get(\"GCP_DATAFLOW_SQL_LOCATION\", \"us-west1\")\n\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_sql\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag_sql:\n    # [START howto_operator_start_sql_job]\n    start_sql = DataflowStartSqlJobOperator(\n        task_id=\"start_sql_query\",\n        job_name=DATAFLOW_SQL_JOB_NAME,\n        query=f\"\"\"\n            SELECT\n                sales_region as sales_region,\n                count(state_id) as count_state\n            FROM\n                bigquery.table.`{GCP_PROJECT_ID}`.`{BQ_SQL_DATASET}`.`{BQ_SQL_TABLE_INPUT}`\n            WHERE state_id >= @state_id_min\n            GROUP BY sales_region;\n        \"\"\",\n        options={\n            \"bigquery-project\": GCP_PROJECT_ID,\n            \"bigquery-dataset\": BQ_SQL_DATASET,\n            \"bigquery-table\": BQ_SQL_TABLE_OUTPUT,\n            \"bigquery-write-disposition\": \"write-truncate\",\n            \"parameter\": \"state_id_min:INT64:2\",\n        },\n        location=DATAFLOW_SQL_LOCATION,\n        do_xcom_push=True,\n    )\n    # [END howto_operator_start_sql_job]", "metadata": {"file_name": "example_dataflow_sql.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves environment-based configuration, cloud service operator, and demonstrates best practices for parameterization and job management", "key_concepts": ["Environment Variable Configuration", "Secure Job Parameterization", "Cloud Service Orchestration", "Data Aggregation Patterns", "Modular DAG Design"], "learning_objectives": ["Implement secure configuration management", "Understand dynamic job parameter injection", "Learn cloud-native workflow design"]}}
{"instruction": "Build an Airflow DAG that demonstrates Google Cloud Storage Transfer Service for migrating data from AWS S3 to Google Cloud Storage, focusing on configuring a scheduled, automated data transfer job.", "input": {"airflow_version": "2.7.2", "operators": ["CloudDataTransferServiceCancelOperationOperator", "CloudDataTransferServiceCreateJobOperator", "CloudDataTransferServiceDeleteJobOperator", "CloudDataTransferServiceGetOperationOperator", "CloudDataTransferServiceJobStatusSensor", "CloudDataTransferServiceListOperationsOperator", "CloudDataTransferServicePauseOperationOperator", "CloudDataTransferServiceResumeOperationOperator"], "line_count": 177, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Cloud Transfer. This DAG relies on\nthe following OS environment variables\n\nNote that you need to provide a large enough set of data so that operations do not execute too quickly.\nOtherwise, DAG will fail.\n\n* GCP_PROJECT_ID - Google Cloud Project to use for the Google Cloud Transfer Service.\n* GCP_DESCRIPTION - Description of transfer job\n* GCP_TRANSFER_SOURCE_AWS_BUCKET - Amazon Web Services Storage bucket from which files are copied.\n* GCP_TRANSFER_SECOND_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied\n* WAIT_FOR_OPERATION_POKE_INTERVAL - interval of what to check the status of the operation\n  A smaller value than the default value accelerates the system test and ensures its correct execution with\n  smaller quantities of files in the source bucket\n  Look at documentation of :class:`~airflow.operators.sensors.BaseSensorOperator` for more information\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    ALREADY_EXISTING_IN_SINK,\n    AWS_S3_DATA_SOURCE,\n    BUCKET_NAME,\n    DESCRIPTION,\n    FILTER_JOB_NAMES,\n    FILTER_PROJECT_ID,\n    GCS_DATA_SINK,\n    JOB_NAME,\n    PROJECT_ID,\n    SCHEDULE,\n    SCHEDULE_END_DATE,\n    SCHEDULE_START_DATE,\n    START_TIME_OF_DAY,\n    STATUS,\n    TRANSFER_OPTIONS,\n    TRANSFER_SPEC,\n    GcpTransferJobsStatus,\n    GcpTransferOperationStatus,\n)\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceCancelOperationOperator,\n    CloudDataTransferServiceCreateJobOperator,\n    CloudDataTransferServiceDeleteJobOperator,\n    CloudDataTransferServiceGetOperationOperator,\n    CloudDataTransferServiceListOperationsOperator,\n    CloudDataTransferServicePauseOperationOperator,\n    CloudDataTransferServiceResumeOperationOperator,\n)\nfrom airflow.providers.google.cloud.sensors.cloud_storage_transfer_service import (\n    CloudDataTransferServiceJobStatusSensor,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_DESCRIPTION = os.environ.get(\"GCP_DESCRIPTION\", \"description\")\nGCP_TRANSFER_TARGET_BUCKET = os.environ.get(\"GCP_TRANSFER_TARGET_BUCKET\")\nWAIT_FOR_OPERATION_POKE_INTERVAL = int(os.environ.get(\"WAIT_FOR_OPERATION_POKE_INTERVAL\", 5))\n\nGCP_TRANSFER_SOURCE_AWS_BUCKET = os.environ.get(\"GCP_TRANSFER_SOURCE_AWS_BUCKET\")\nGCP_TRANSFER_FIRST_TARGET_BUCKET = os.environ.get(\n    \"GCP_TRANSFER_FIRST_TARGET_BUCKET\", \"gcp-transfer-first-target\"\n)\n\nGCP_TRANSFER_JOB_NAME = os.environ.get(\"GCP_TRANSFER_JOB_NAME\", \"transferJobs/sampleJob\")\n\n# [START howto_operator_gcp_transfer_create_job_body_aws]\naws_to_gcs_transfer_body = {\n    DESCRIPTION: GCP_DESCRIPTION,\n    STATUS: GcpTransferJobsStatus.ENABLED,\n    PROJECT_ID: GCP_PROJECT_ID,\n    JOB_NAME: GCP_TRANSFER_JOB_NAME,\n    SCHEDULE: {\n        SCHEDULE_START_DATE: datetime(2015, 1, 1).date(),\n        SCHEDULE_END_DATE: datetime(2030, 1, 1).date(),\n        START_TIME_OF_DAY: (datetime.utcnow() + timedelta(minutes=2)).time(),\n    },\n    TRANSFER_SPEC: {\n        AWS_S3_DATA_SOURCE: {BUCKET_NAME: GCP_TRANSFER_SOURCE_AWS_BUCKET},\n        GCS_DATA_SINK: {BUCKET_NAME: GCP_TRANSFER_FIRST_TARGET_BUCKET},\n        TRANSFER_OPTIONS: {ALREADY_EXISTING_IN_SINK: True},\n    },\n}\n# [END howto_operator_gcp_transfer_create_job_body_aws]\n\n\nwith models.DAG(\n    \"example_gcp_transfer_aws\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_gcp_transfer_create_job]\n    create_transfer_job_from_aws = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_transfer_job_from_aws\", body=aws_to_gcs_transfer_body\n    )\n    # [END howto_operator_gcp_transfer_create_job]\n\n    wait_for_operation_to_start = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_start\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.IN_PROGRESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n\n    # [START howto_operator_gcp_transfer_pause_operation]\n    pause_operation = CloudDataTransferServicePauseOperationOperator(\n        task_id=\"pause_operation\",\n        operation_name=\"{{task_instance.xcom_pull('wait_for_operation_to_start', \"\n        \"key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_pause_operation]\n\n    # [START howto_operator_gcp_transfer_list_operations]\n    list_operations = CloudDataTransferServiceListOperationsOperator(\n        task_id=\"list_operations\",\n        request_filter={\n            FILTER_PROJECT_ID: GCP_PROJECT_ID,\n            FILTER_JOB_NAMES: [\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\"],\n        },\n    )\n    # [END howto_operator_gcp_transfer_list_operations]\n\n    # [START howto_operator_gcp_transfer_get_operation]\n    get_operation = CloudDataTransferServiceGetOperationOperator(\n        task_id=\"get_operation\", operation_name=\"{{task_instance.xcom_pull('list_operations')[0]['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_get_operation]\n\n    # [START howto_operator_gcp_transfer_resume_operation]\n    resume_operation = CloudDataTransferServiceResumeOperationOperator(\n        task_id=\"resume_operation\", operation_name=\"{{task_instance.xcom_pull('get_operation')['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_resume_operation]\n\n    # [START howto_operator_gcp_transfer_wait_operation]\n    wait_for_operation_to_end = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_end\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.SUCCESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n    # [END howto_operator_gcp_transfer_wait_operation]\n\n    # [START howto_operator_gcp_transfer_cancel_operation]\n    cancel_operation = CloudDataTransferServiceCancelOperationOperator(\n        task_id=\"cancel_operation\",\n        operation_name=\"{{task_instance.xcom_pull(\"\n        \"'wait_for_second_operation_to_start', key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_cancel_operation]\n\n    # [START howto_operator_gcp_transfer_delete_job]\n    delete_transfer_from_aws_job = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_transfer_from_aws_job\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_gcp_transfer_delete_job]\n\n    chain(\n        create_transfer_job_from_aws,\n        wait_for_operation_to_start,\n        pause_operation,\n        list_operations,\n        get_operation,\n        resume_operation,\n        wait_for_operation_to_end,\n        cancel_operation,\n        delete_transfer_from_aws_job,\n    )", "metadata": {"file_name": "example_cloud_storage_transfer_service_aws.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operators, involves external service integration, has configuration complexity with transfer specifications and scheduling", "key_concepts": ["Cloud Storage Transfer Service", "GCP Operators", "Data Migration", "Scheduled Jobs", "Transfer Configuration"], "learning_objectives": ["Configure cross-cloud data transfer jobs", "Understand transfer specification parameters", "Learn cloud service operator usage", "Implement scheduled data migration workflows"]}}
{"instruction": "Design an Airflow DAG that explores error handling and operational monitoring for cloud data transfer processes, emphasizing job status tracking and transfer operation management.", "input": {"airflow_version": "2.7.2", "operators": ["CloudDataTransferServiceCancelOperationOperator", "CloudDataTransferServiceCreateJobOperator", "CloudDataTransferServiceDeleteJobOperator", "CloudDataTransferServiceGetOperationOperator", "CloudDataTransferServiceJobStatusSensor", "CloudDataTransferServiceListOperationsOperator", "CloudDataTransferServicePauseOperationOperator", "CloudDataTransferServiceResumeOperationOperator"], "line_count": 177, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that demonstrates interactions with Google Cloud Transfer. This DAG relies on\nthe following OS environment variables\n\nNote that you need to provide a large enough set of data so that operations do not execute too quickly.\nOtherwise, DAG will fail.\n\n* GCP_PROJECT_ID - Google Cloud Project to use for the Google Cloud Transfer Service.\n* GCP_DESCRIPTION - Description of transfer job\n* GCP_TRANSFER_SOURCE_AWS_BUCKET - Amazon Web Services Storage bucket from which files are copied.\n* GCP_TRANSFER_SECOND_TARGET_BUCKET - Google Cloud Storage bucket to which files are copied\n* WAIT_FOR_OPERATION_POKE_INTERVAL - interval of what to check the status of the operation\n  A smaller value than the default value accelerates the system test and ensures its correct execution with\n  smaller quantities of files in the source bucket\n  Look at documentation of :class:`~airflow.operators.sensors.BaseSensorOperator` for more information\n\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (\n    ALREADY_EXISTING_IN_SINK,\n    AWS_S3_DATA_SOURCE,\n    BUCKET_NAME,\n    DESCRIPTION,\n    FILTER_JOB_NAMES,\n    FILTER_PROJECT_ID,\n    GCS_DATA_SINK,\n    JOB_NAME,\n    PROJECT_ID,\n    SCHEDULE,\n    SCHEDULE_END_DATE,\n    SCHEDULE_START_DATE,\n    START_TIME_OF_DAY,\n    STATUS,\n    TRANSFER_OPTIONS,\n    TRANSFER_SPEC,\n    GcpTransferJobsStatus,\n    GcpTransferOperationStatus,\n)\nfrom airflow.providers.google.cloud.operators.cloud_storage_transfer_service import (\n    CloudDataTransferServiceCancelOperationOperator,\n    CloudDataTransferServiceCreateJobOperator,\n    CloudDataTransferServiceDeleteJobOperator,\n    CloudDataTransferServiceGetOperationOperator,\n    CloudDataTransferServiceListOperationsOperator,\n    CloudDataTransferServicePauseOperationOperator,\n    CloudDataTransferServiceResumeOperationOperator,\n)\nfrom airflow.providers.google.cloud.sensors.cloud_storage_transfer_service import (\n    CloudDataTransferServiceJobStatusSensor,\n)\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_DESCRIPTION = os.environ.get(\"GCP_DESCRIPTION\", \"description\")\nGCP_TRANSFER_TARGET_BUCKET = os.environ.get(\"GCP_TRANSFER_TARGET_BUCKET\")\nWAIT_FOR_OPERATION_POKE_INTERVAL = int(os.environ.get(\"WAIT_FOR_OPERATION_POKE_INTERVAL\", 5))\n\nGCP_TRANSFER_SOURCE_AWS_BUCKET = os.environ.get(\"GCP_TRANSFER_SOURCE_AWS_BUCKET\")\nGCP_TRANSFER_FIRST_TARGET_BUCKET = os.environ.get(\n    \"GCP_TRANSFER_FIRST_TARGET_BUCKET\", \"gcp-transfer-first-target\"\n)\n\nGCP_TRANSFER_JOB_NAME = os.environ.get(\"GCP_TRANSFER_JOB_NAME\", \"transferJobs/sampleJob\")\n\n# [START howto_operator_gcp_transfer_create_job_body_aws]\naws_to_gcs_transfer_body = {\n    DESCRIPTION: GCP_DESCRIPTION,\n    STATUS: GcpTransferJobsStatus.ENABLED,\n    PROJECT_ID: GCP_PROJECT_ID,\n    JOB_NAME: GCP_TRANSFER_JOB_NAME,\n    SCHEDULE: {\n        SCHEDULE_START_DATE: datetime(2015, 1, 1).date(),\n        SCHEDULE_END_DATE: datetime(2030, 1, 1).date(),\n        START_TIME_OF_DAY: (datetime.utcnow() + timedelta(minutes=2)).time(),\n    },\n    TRANSFER_SPEC: {\n        AWS_S3_DATA_SOURCE: {BUCKET_NAME: GCP_TRANSFER_SOURCE_AWS_BUCKET},\n        GCS_DATA_SINK: {BUCKET_NAME: GCP_TRANSFER_FIRST_TARGET_BUCKET},\n        TRANSFER_OPTIONS: {ALREADY_EXISTING_IN_SINK: True},\n    },\n}\n# [END howto_operator_gcp_transfer_create_job_body_aws]\n\n\nwith models.DAG(\n    \"example_gcp_transfer_aws\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # [START howto_operator_gcp_transfer_create_job]\n    create_transfer_job_from_aws = CloudDataTransferServiceCreateJobOperator(\n        task_id=\"create_transfer_job_from_aws\", body=aws_to_gcs_transfer_body\n    )\n    # [END howto_operator_gcp_transfer_create_job]\n\n    wait_for_operation_to_start = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_start\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.IN_PROGRESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n\n    # [START howto_operator_gcp_transfer_pause_operation]\n    pause_operation = CloudDataTransferServicePauseOperationOperator(\n        task_id=\"pause_operation\",\n        operation_name=\"{{task_instance.xcom_pull('wait_for_operation_to_start', \"\n        \"key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_pause_operation]\n\n    # [START howto_operator_gcp_transfer_list_operations]\n    list_operations = CloudDataTransferServiceListOperationsOperator(\n        task_id=\"list_operations\",\n        request_filter={\n            FILTER_PROJECT_ID: GCP_PROJECT_ID,\n            FILTER_JOB_NAMES: [\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\"],\n        },\n    )\n    # [END howto_operator_gcp_transfer_list_operations]\n\n    # [START howto_operator_gcp_transfer_get_operation]\n    get_operation = CloudDataTransferServiceGetOperationOperator(\n        task_id=\"get_operation\", operation_name=\"{{task_instance.xcom_pull('list_operations')[0]['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_get_operation]\n\n    # [START howto_operator_gcp_transfer_resume_operation]\n    resume_operation = CloudDataTransferServiceResumeOperationOperator(\n        task_id=\"resume_operation\", operation_name=\"{{task_instance.xcom_pull('get_operation')['name']}}\"\n    )\n    # [END howto_operator_gcp_transfer_resume_operation]\n\n    # [START howto_operator_gcp_transfer_wait_operation]\n    wait_for_operation_to_end = CloudDataTransferServiceJobStatusSensor(\n        task_id=\"wait_for_operation_to_end\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n        expected_statuses={GcpTransferOperationStatus.SUCCESS},\n        poke_interval=WAIT_FOR_OPERATION_POKE_INTERVAL,\n    )\n    # [END howto_operator_gcp_transfer_wait_operation]\n\n    # [START howto_operator_gcp_transfer_cancel_operation]\n    cancel_operation = CloudDataTransferServiceCancelOperationOperator(\n        task_id=\"cancel_operation\",\n        operation_name=\"{{task_instance.xcom_pull(\"\n        \"'wait_for_second_operation_to_start', key='sensed_operations')[0]['name']}}\",\n    )\n    # [END howto_operator_gcp_transfer_cancel_operation]\n\n    # [START howto_operator_gcp_transfer_delete_job]\n    delete_transfer_from_aws_job = CloudDataTransferServiceDeleteJobOperator(\n        task_id=\"delete_transfer_from_aws_job\",\n        job_name=\"{{task_instance.xcom_pull('create_transfer_job_from_aws')['name']}}\",\n        project_id=GCP_PROJECT_ID,\n    )\n    # [END howto_operator_gcp_transfer_delete_job]\n\n    chain(\n        create_transfer_job_from_aws,\n        wait_for_operation_to_start,\n        pause_operation,\n        list_operations,\n        get_operation,\n        resume_operation,\n        wait_for_operation_to_end,\n        cancel_operation,\n        delete_transfer_from_aws_job,\n    )", "metadata": {"file_name": "example_cloud_storage_transfer_service_aws.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operators for job management, uses sensors for status tracking, demonstrates advanced workflow control mechanisms", "key_concepts": ["Job Status Monitoring", "Transfer Operation Sensors", "Cloud Service Error Handling", "Workflow Resilience", "Operational Observability"], "learning_objectives": ["Implement job status tracking", "Understand transfer operation lifecycle", "Learn sensor-based workflow control", "Practice cloud service error management"]}}
{"instruction": "Build an Airflow DAG that extracts Facebook Ads performance data, transfers it to Google Cloud Storage, and then loads the data into BigQuery for analysis, demonstrating end-to-end data pipeline orchestration.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateEmptyTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "FacebookAdsReportToGcsOperator", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToBigQueryOperator"], "line_count": 124, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use FacebookAdsReportToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom facebook_business.adobjects.adsinsights import AdsInsights\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.facebook_ads_to_gcs import FacebookAdsReportToGcsOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\n# [START howto_GCS_env_variables]\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"free-tier-1997\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow_bucket_fb\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"Temp/this_is_my_report_csv.csv\")\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nDATASET_NAME = os.environ.get(\"DATASET_NAME\", \"airflow_test_dataset\")\nTABLE_NAME = os.environ.get(\"FB_TABLE_NAME\", \"airflow_test_datatable\")\n# [END howto_GCS_env_variables]\n\n# [START howto_FB_ADS_variables]\nFIELDS = [\n    AdsInsights.Field.campaign_name,\n    AdsInsights.Field.campaign_id,\n    AdsInsights.Field.ad_id,\n    AdsInsights.Field.clicks,\n    AdsInsights.Field.impressions,\n]\nPARAMETERS = {\"level\": \"ad\", \"date_preset\": \"yesterday\"}\n# [END howto_FB_ADS_variables]\n\nwith models.DAG(\n    \"example_facebook_ads_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"campaign_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"campaign_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"ad_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"clicks\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"impressions\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_operator_facebook_ads_to_gcs]\n    run_operator = FacebookAdsReportToGcsOperator(\n        task_id=\"run_fetch_data\",\n        owner=\"airflow\",\n        bucket_name=GCS_BUCKET,\n        parameters=PARAMETERS,\n        fields=FIELDS,\n        gcp_conn_id=GCS_CONN_ID,\n        object_name=GCS_OBJ_PATH,\n    )\n    # [END howto_operator_facebook_ads_to_gcs]\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq_example\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    chain(\n        create_bucket,\n        create_dataset,\n        create_table,\n        run_operator,\n        load_csv,\n        read_data_from_gcs_many_chunks,\n        delete_bucket,\n        delete_dataset,\n    )", "metadata": {"file_name": "example_facebook_ads_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Multiple cloud service operators, external API integration (Facebook Ads), involves data transfer between services, and uses 5+ different operators with cloud-specific configurations", "key_concepts": ["Cloud Data Pipeline", "Data Extraction", "Google Cloud Integration", "Facebook Ads Reporting", "Operator Chaining"], "learning_objectives": ["Understand multi-step data transfer workflows", "Learn cloud service operator usage", "Practice Airflow task dependency management"]}}
{"instruction": "Design an automated data reporting system that demonstrates resource management in Airflow by creating, populating, and then cleaning up cloud resources after data processing is complete.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateEmptyTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "FacebookAdsReportToGcsOperator", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToBigQueryOperator"], "line_count": 124, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use FacebookAdsReportToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom facebook_business.adobjects.adsinsights import AdsInsights\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.facebook_ads_to_gcs import FacebookAdsReportToGcsOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n\n# [START howto_GCS_env_variables]\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"free-tier-1997\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow_bucket_fb\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"Temp/this_is_my_report_csv.csv\")\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nDATASET_NAME = os.environ.get(\"DATASET_NAME\", \"airflow_test_dataset\")\nTABLE_NAME = os.environ.get(\"FB_TABLE_NAME\", \"airflow_test_datatable\")\n# [END howto_GCS_env_variables]\n\n# [START howto_FB_ADS_variables]\nFIELDS = [\n    AdsInsights.Field.campaign_name,\n    AdsInsights.Field.campaign_id,\n    AdsInsights.Field.ad_id,\n    AdsInsights.Field.clicks,\n    AdsInsights.Field.impressions,\n]\nPARAMETERS = {\"level\": \"ad\", \"date_preset\": \"yesterday\"}\n# [END howto_FB_ADS_variables]\n\nwith models.DAG(\n    \"example_facebook_ads_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n    )\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\",\n        dataset_id=DATASET_NAME,\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"campaign_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"campaign_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"ad_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"clicks\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"impressions\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    # [START howto_operator_facebook_ads_to_gcs]\n    run_operator = FacebookAdsReportToGcsOperator(\n        task_id=\"run_fetch_data\",\n        owner=\"airflow\",\n        bucket_name=GCS_BUCKET,\n        parameters=PARAMETERS,\n        fields=FIELDS,\n        gcp_conn_id=GCS_CONN_ID,\n        object_name=GCS_OBJ_PATH,\n    )\n    # [END howto_operator_facebook_ads_to_gcs]\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq_example\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    chain(\n        create_bucket,\n        create_dataset,\n        create_table,\n        run_operator,\n        load_csv,\n        read_data_from_gcs_many_chunks,\n        delete_bucket,\n        delete_dataset,\n    )", "metadata": {"file_name": "example_facebook_ads_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves dynamic resource creation and deletion, uses multiple Google Cloud operators, demonstrates infrastructure-as-code principles in data workflows", "key_concepts": ["Resource Lifecycle Management", "Cloud Resource Operators", "Idempotent Workflow Design", "Automated Cleanup", "Transient Infrastructure"], "learning_objectives": ["Understand cloud resource management in Airflow", "Learn safe resource creation and deletion patterns", "Practice infrastructure teardown techniques"]}}
{"instruction": "Build an Airflow DAG that demonstrates Airbyte data synchronization using both synchronous and asynchronous job triggering methods, showcasing how to integrate external data pipeline tools with Airflow workflows.", "input": {"airflow_version": "2.7.2", "operators": ["AirbyteJobSensor", "AirbyteTriggerSyncOperator"], "line_count": 50, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the AirbyteTriggerSyncOperator.\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_airbyte_operator\"\nCONN_ID = \"15bc3800-82e4-48c3-a32d-620661273f28\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_airbyte_synchronous]\n    sync_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_sync_source_dest_example\",\n        connection_id=CONN_ID,\n    )\n    # [END howto_operator_airbyte_synchronous]\n\n    # [START howto_operator_airbyte_asynchronous]\n    async_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_async_source_dest_example\",\n        connection_id=CONN_ID,\n        asynchronous=True,\n    )\n\n    airbyte_sensor = AirbyteJobSensor(\n        task_id=\"airbyte_sensor_source_dest_example\",\n        airbyte_job_id=async_source_destination.output,\n    )\n    # [END howto_operator_airbyte_asynchronous]\n\n    # Task dependency created via `XComArgs`:\n    #   async_source_destination >> airbyte_sensor\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_airbyte_trigger_job.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses external service integration (Airbyte), multiple operators, async/sync job handling, and demonstrates sensor usage for job monitoring", "key_concepts": ["Airbyte integration", "Synchronous vs Asynchronous job execution", "Job sensors", "External data pipeline orchestration"], "learning_objectives": ["Understand Airbyte operator usage", "Learn job triggering strategies", "Implement job monitoring with sensors"]}}
{"instruction": "Design an Airflow DAG that explores different job execution patterns by comparing synchronous and asynchronous data synchronization techniques, focusing on how to handle long-running external data transfer jobs.", "input": {"airflow_version": "2.7.2", "operators": ["AirbyteJobSensor", "AirbyteTriggerSyncOperator"], "line_count": 50, "is_multifile": false}, "output": "\"\"\"Example DAG demonstrating the usage of the AirbyteTriggerSyncOperator.\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"example_airbyte_operator\"\nCONN_ID = \"15bc3800-82e4-48c3-a32d-620661273f28\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    # [START howto_operator_airbyte_synchronous]\n    sync_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_sync_source_dest_example\",\n        connection_id=CONN_ID,\n    )\n    # [END howto_operator_airbyte_synchronous]\n\n    # [START howto_operator_airbyte_asynchronous]\n    async_source_destination = AirbyteTriggerSyncOperator(\n        task_id=\"airbyte_async_source_dest_example\",\n        connection_id=CONN_ID,\n        asynchronous=True,\n    )\n\n    airbyte_sensor = AirbyteJobSensor(\n        task_id=\"airbyte_sensor_source_dest_example\",\n        airbyte_job_id=async_source_destination.output,\n    )\n    # [END howto_operator_airbyte_asynchronous]\n\n    # Task dependency created via `XComArgs`:\n    #   async_source_destination >> airbyte_sensor\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_airbyte_trigger_job.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates advanced operator configuration, job monitoring techniques, and introduces non-blocking workflow design patterns", "key_concepts": ["Asynchronous task execution", "Job monitoring", "External service integration", "Task dependency management"], "learning_objectives": ["Compare sync vs async job execution", "Implement job status tracking", "Understand non-blocking workflow design"]}}
{"instruction": "Build an Airflow DAG that demonstrates Google Display Video 360 data pipeline integration, focusing on downloading line items, uploading to Google Cloud Storage, and transferring data to BigQuery.", "input": {"airflow_version": "2.7.2", "operators": ["GCSToBigQueryOperator", "GoogleDisplayVideo360CreateQueryOperator", "GoogleDisplayVideo360CreateSDFDownloadTaskOperator", "GoogleDisplayVideo360DeleteReportOperator", "GoogleDisplayVideo360DownloadLineItemsOperator", "GoogleDisplayVideo360DownloadReportV2Operator", "GoogleDisplayVideo360GetSDFDownloadOperationSensor", "GoogleDisplayVideo360RunQueryOperator", "GoogleDisplayVideo360RunQuerySensor", "GoogleDisplayVideo360SDFtoGCSOperator", "GoogleDisplayVideo360UploadLineItemsOperator"], "line_count": 195, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use DisplayVideo.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook\nfrom airflow.providers.google.marketing_platform.operators.display_video import (\n    GoogleDisplayVideo360CreateQueryOperator,\n    GoogleDisplayVideo360CreateSDFDownloadTaskOperator,\n    GoogleDisplayVideo360DeleteReportOperator,\n    GoogleDisplayVideo360DownloadLineItemsOperator,\n    GoogleDisplayVideo360DownloadReportV2Operator,\n    GoogleDisplayVideo360RunQueryOperator,\n    GoogleDisplayVideo360SDFtoGCSOperator,\n    GoogleDisplayVideo360UploadLineItemsOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.display_video import (\n    GoogleDisplayVideo360GetSDFDownloadOperationSensor,\n    GoogleDisplayVideo360RunQuerySensor,\n)\n\n# [START howto_display_video_env_variables]\nBUCKET = os.environ.get(\"GMP_DISPLAY_VIDEO_BUCKET\", \"gs://INVALID BUCKET NAME\")\nADVERTISER_ID = os.environ.get(\"GMP_ADVERTISER_ID\", 1234567)\nOBJECT_NAME = os.environ.get(\"GMP_OBJECT_NAME\", \"files/report.csv\")\nPATH_TO_UPLOAD_FILE = os.environ.get(\"GCP_GCS_PATH_TO_UPLOAD_FILE\", \"test-gcs-example.txt\")\nPATH_TO_SAVED_FILE = os.environ.get(\"GCP_GCS_PATH_TO_SAVED_FILE\", \"test-gcs-example-download.txt\")\nBUCKET_FILE_LOCATION = PATH_TO_UPLOAD_FILE.rpartition(\"/\")[-1]\nSDF_VERSION = os.environ.get(\"GMP_SDF_VERSION\", \"SDF_VERSION_5_5\")\nBQ_DATA_SET = os.environ.get(\"GMP_BQ_DATA_SET\", \"airflow_test\")\nGMP_PARTNER_ID = os.environ.get(\"GMP_PARTNER_ID\", 123)\nENTITY_TYPE = os.environ.get(\"GMP_ENTITY_TYPE\", \"LineItem\")\nERF_SOURCE_OBJECT = GoogleDisplayVideo360Hook.erf_uri(GMP_PARTNER_ID, ENTITY_TYPE)\n\nREPORT_V2 = {\n    \"metadata\": {\n        \"title\": \"Airflow Test Report\",\n        \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n        \"format\": \"CSV\",\n        \"sendNotification\": False,\n    },\n    \"params\": {\n        \"type\": \"STANDARD\",\n        \"groupBys\": [\"FILTER_DATE\", \"FILTER_PARTNER\"],\n        \"filters\": [{\"type\": \"FILTER_PARTNER\", \"value\": ADVERTISER_ID}],\n        \"metrics\": [\"METRIC_IMPRESSIONS\", \"METRIC_CLICKS\"],\n    },\n    \"schedule\": {\"frequency\": \"ONE_TIME\"},\n}\n\nPARAMETERS = {\n    \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n}\n\nCREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST: dict = {\n    \"version\": SDF_VERSION,\n    \"advertiserId\": ADVERTISER_ID,\n    \"inventorySourceFilter\": {\"inventorySourceIds\": []},\n}\n\nDOWNLOAD_LINE_ITEMS_REQUEST: dict = {\"filterType\": ADVERTISER_ID, \"format\": \"CSV\", \"fileSpec\": \"EWF\"}\n# [END howto_display_video_env_variables]\n\nSTART_DATE = datetime(2021, 1, 1)\n\nwith models.DAG(\n    \"example_display_video_misc\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag2:\n    # [START howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n    upload_erf_to_bq = GCSToBigQueryOperator(\n        task_id=\"upload_erf_to_bq\",\n        bucket=BUCKET,\n        source_objects=ERF_SOURCE_OBJECT,\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n\n    # [START howto_google_display_video_download_line_items_operator]\n    download_line_items = GoogleDisplayVideo360DownloadLineItemsOperator(\n        task_id=\"download_line_items\",\n        request_body=DOWNLOAD_LINE_ITEMS_REQUEST,\n        bucket_name=BUCKET,\n        object_name=OBJECT_NAME,\n        gzip=False,\n    )\n    # [END howto_google_display_video_download_line_items_operator]\n\n    # [START howto_google_display_video_upload_line_items_operator]\n    upload_line_items = GoogleDisplayVideo360UploadLineItemsOperator(\n        task_id=\"upload_line_items\",\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n    )\n    # [END howto_google_display_video_upload_line_items_operator]\n\nwith models.DAG(\n    \"example_display_video_sdf\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag3:\n    # [START howto_google_display_video_create_sdf_download_task_operator]\n    create_sdf_download_task = GoogleDisplayVideo360CreateSDFDownloadTaskOperator(\n        task_id=\"create_sdf_download_task\", body_request=CREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST\n    )\n    operation_name = '{{ task_instance.xcom_pull(\"create_sdf_download_task\")[\"name\"] }}'\n    # [END howto_google_display_video_create_sdf_download_task_operator]\n\n    # [START howto_google_display_video_wait_for_operation_sensor]\n    wait_for_operation = GoogleDisplayVideo360GetSDFDownloadOperationSensor(\n        task_id=\"wait_for_operation\",\n        operation_name=operation_name,\n    )\n    # [END howto_google_display_video_wait_for_operation_sensor]\n\n    # [START howto_google_display_video_save_sdf_in_gcs_operator]\n    save_sdf_in_gcs = GoogleDisplayVideo360SDFtoGCSOperator(\n        task_id=\"save_sdf_in_gcs\",\n        operation_name=operation_name,\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n        gzip=False,\n    )\n    # [END howto_google_display_video_save_sdf_in_gcs_operator]\n\n    # [START howto_google_display_video_gcs_to_big_query_operator]\n    upload_sdf_to_big_query = GCSToBigQueryOperator(\n        task_id=\"upload_sdf_to_big_query\",\n        bucket=BUCKET,\n        source_objects=[save_sdf_in_gcs.output],\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_gcs_to_big_query_operator]\n\n    create_sdf_download_task >> wait_for_operation >> save_sdf_in_gcs\n\n    # Task dependency created via `XComArgs`:\n    #   save_sdf_in_gcs >> upload_sdf_to_big_query\n\nwith models.DAG(\n    \"example_display_video_v2\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag:\n    # [START howto_google_display_video_create_query_operator]\n    create_query_v2 = GoogleDisplayVideo360CreateQueryOperator(body=REPORT_V2, task_id=\"create_query\")\n\n    query_id = cast(str, XComArg(create_query_v2, key=\"query_id\"))\n    # [END howto_google_display_video_create_query_operator]\n\n    # [START howto_google_display_video_run_query_report_operator]\n    run_query_v2 = GoogleDisplayVideo360RunQueryOperator(\n        query_id=query_id, parameters=PARAMETERS, task_id=\"run_report\"\n    )\n\n    query_id = cast(str, XComArg(run_query_v2, key=\"query_id\"))\n    report_id = cast(str, XComArg(run_query_v2, key=\"report_id\"))\n    # [END howto_google_display_video_run_query_report_operator]\n\n    # [START howto_google_display_video_wait_run_query_sensor]\n    wait_for_query = GoogleDisplayVideo360RunQuerySensor(\n        task_id=\"wait_for_query\",\n        query_id=query_id,\n        report_id=report_id,\n    )\n    # [END howto_google_display_video_wait_run_query_sensor]\n\n    # [START howto_google_display_video_get_report_operator]\n    get_report_v2 = GoogleDisplayVideo360DownloadReportV2Operator(\n        query_id=query_id,\n        report_id=report_id,\n        task_id=\"get_report\",\n        bucket_name=BUCKET,\n        report_name=\"test1.csv\",\n    )\n    # # [END howto_google_display_video_get_report_operator]\n    # # [START howto_google_display_video_delete_query_report_operator]\n    delete_report_v2 = GoogleDisplayVideo360DeleteReportOperator(report_id=report_id, task_id=\"delete_report\")\n    # # [END howto_google_display_video_delete_query_report_operator]\n\n    create_query_v2 >> run_query_v2 >> wait_for_query >> get_report_v2 >> delete_report_v2", "metadata": {"file_name": "example_display_video.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple Google Cloud operators, involves external service integration, has environment variable configuration, and demonstrates data transfer between platforms", "key_concepts": ["Cloud Service Integration", "Data Pipeline Orchestration", "Google Marketing Platform", "Environment Configuration", "Operator Chaining"], "learning_objectives": ["Understand Google Display Video 360 data workflows", "Learn cloud service operator usage", "Practice configuring complex Airflow DAGs"]}}
{"instruction": "Create an Airflow DAG that showcases advanced configuration techniques for marketing data reporting, emphasizing dynamic parameter management and multi-step reporting processes.", "input": {"airflow_version": "2.7.2", "operators": ["GCSToBigQueryOperator", "GoogleDisplayVideo360CreateQueryOperator", "GoogleDisplayVideo360CreateSDFDownloadTaskOperator", "GoogleDisplayVideo360DeleteReportOperator", "GoogleDisplayVideo360DownloadLineItemsOperator", "GoogleDisplayVideo360DownloadReportV2Operator", "GoogleDisplayVideo360GetSDFDownloadOperationSensor", "GoogleDisplayVideo360RunQueryOperator", "GoogleDisplayVideo360RunQuerySensor", "GoogleDisplayVideo360SDFtoGCSOperator", "GoogleDisplayVideo360UploadLineItemsOperator"], "line_count": 195, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use DisplayVideo.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom typing import cast\n\nfrom airflow import models\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook\nfrom airflow.providers.google.marketing_platform.operators.display_video import (\n    GoogleDisplayVideo360CreateQueryOperator,\n    GoogleDisplayVideo360CreateSDFDownloadTaskOperator,\n    GoogleDisplayVideo360DeleteReportOperator,\n    GoogleDisplayVideo360DownloadLineItemsOperator,\n    GoogleDisplayVideo360DownloadReportV2Operator,\n    GoogleDisplayVideo360RunQueryOperator,\n    GoogleDisplayVideo360SDFtoGCSOperator,\n    GoogleDisplayVideo360UploadLineItemsOperator,\n)\nfrom airflow.providers.google.marketing_platform.sensors.display_video import (\n    GoogleDisplayVideo360GetSDFDownloadOperationSensor,\n    GoogleDisplayVideo360RunQuerySensor,\n)\n\n# [START howto_display_video_env_variables]\nBUCKET = os.environ.get(\"GMP_DISPLAY_VIDEO_BUCKET\", \"gs://INVALID BUCKET NAME\")\nADVERTISER_ID = os.environ.get(\"GMP_ADVERTISER_ID\", 1234567)\nOBJECT_NAME = os.environ.get(\"GMP_OBJECT_NAME\", \"files/report.csv\")\nPATH_TO_UPLOAD_FILE = os.environ.get(\"GCP_GCS_PATH_TO_UPLOAD_FILE\", \"test-gcs-example.txt\")\nPATH_TO_SAVED_FILE = os.environ.get(\"GCP_GCS_PATH_TO_SAVED_FILE\", \"test-gcs-example-download.txt\")\nBUCKET_FILE_LOCATION = PATH_TO_UPLOAD_FILE.rpartition(\"/\")[-1]\nSDF_VERSION = os.environ.get(\"GMP_SDF_VERSION\", \"SDF_VERSION_5_5\")\nBQ_DATA_SET = os.environ.get(\"GMP_BQ_DATA_SET\", \"airflow_test\")\nGMP_PARTNER_ID = os.environ.get(\"GMP_PARTNER_ID\", 123)\nENTITY_TYPE = os.environ.get(\"GMP_ENTITY_TYPE\", \"LineItem\")\nERF_SOURCE_OBJECT = GoogleDisplayVideo360Hook.erf_uri(GMP_PARTNER_ID, ENTITY_TYPE)\n\nREPORT_V2 = {\n    \"metadata\": {\n        \"title\": \"Airflow Test Report\",\n        \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n        \"format\": \"CSV\",\n        \"sendNotification\": False,\n    },\n    \"params\": {\n        \"type\": \"STANDARD\",\n        \"groupBys\": [\"FILTER_DATE\", \"FILTER_PARTNER\"],\n        \"filters\": [{\"type\": \"FILTER_PARTNER\", \"value\": ADVERTISER_ID}],\n        \"metrics\": [\"METRIC_IMPRESSIONS\", \"METRIC_CLICKS\"],\n    },\n    \"schedule\": {\"frequency\": \"ONE_TIME\"},\n}\n\nPARAMETERS = {\n    \"dataRange\": {\"range\": \"LAST_7_DAYS\"},\n}\n\nCREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST: dict = {\n    \"version\": SDF_VERSION,\n    \"advertiserId\": ADVERTISER_ID,\n    \"inventorySourceFilter\": {\"inventorySourceIds\": []},\n}\n\nDOWNLOAD_LINE_ITEMS_REQUEST: dict = {\"filterType\": ADVERTISER_ID, \"format\": \"CSV\", \"fileSpec\": \"EWF\"}\n# [END howto_display_video_env_variables]\n\nSTART_DATE = datetime(2021, 1, 1)\n\nwith models.DAG(\n    \"example_display_video_misc\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag2:\n    # [START howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n    upload_erf_to_bq = GCSToBigQueryOperator(\n        task_id=\"upload_erf_to_bq\",\n        bucket=BUCKET,\n        source_objects=ERF_SOURCE_OBJECT,\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_upload_multiple_entity_read_files_to_big_query]\n\n    # [START howto_google_display_video_download_line_items_operator]\n    download_line_items = GoogleDisplayVideo360DownloadLineItemsOperator(\n        task_id=\"download_line_items\",\n        request_body=DOWNLOAD_LINE_ITEMS_REQUEST,\n        bucket_name=BUCKET,\n        object_name=OBJECT_NAME,\n        gzip=False,\n    )\n    # [END howto_google_display_video_download_line_items_operator]\n\n    # [START howto_google_display_video_upload_line_items_operator]\n    upload_line_items = GoogleDisplayVideo360UploadLineItemsOperator(\n        task_id=\"upload_line_items\",\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n    )\n    # [END howto_google_display_video_upload_line_items_operator]\n\nwith models.DAG(\n    \"example_display_video_sdf\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag3:\n    # [START howto_google_display_video_create_sdf_download_task_operator]\n    create_sdf_download_task = GoogleDisplayVideo360CreateSDFDownloadTaskOperator(\n        task_id=\"create_sdf_download_task\", body_request=CREATE_SDF_DOWNLOAD_TASK_BODY_REQUEST\n    )\n    operation_name = '{{ task_instance.xcom_pull(\"create_sdf_download_task\")[\"name\"] }}'\n    # [END howto_google_display_video_create_sdf_download_task_operator]\n\n    # [START howto_google_display_video_wait_for_operation_sensor]\n    wait_for_operation = GoogleDisplayVideo360GetSDFDownloadOperationSensor(\n        task_id=\"wait_for_operation\",\n        operation_name=operation_name,\n    )\n    # [END howto_google_display_video_wait_for_operation_sensor]\n\n    # [START howto_google_display_video_save_sdf_in_gcs_operator]\n    save_sdf_in_gcs = GoogleDisplayVideo360SDFtoGCSOperator(\n        task_id=\"save_sdf_in_gcs\",\n        operation_name=operation_name,\n        bucket_name=BUCKET,\n        object_name=BUCKET_FILE_LOCATION,\n        gzip=False,\n    )\n    # [END howto_google_display_video_save_sdf_in_gcs_operator]\n\n    # [START howto_google_display_video_gcs_to_big_query_operator]\n    upload_sdf_to_big_query = GCSToBigQueryOperator(\n        task_id=\"upload_sdf_to_big_query\",\n        bucket=BUCKET,\n        source_objects=[save_sdf_in_gcs.output],\n        destination_project_dataset_table=f\"{BQ_DATA_SET}.gcs_to_bq_table\",\n        schema_fields=[\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n        ],\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n    # [END howto_google_display_video_gcs_to_big_query_operator]\n\n    create_sdf_download_task >> wait_for_operation >> save_sdf_in_gcs\n\n    # Task dependency created via `XComArgs`:\n    #   save_sdf_in_gcs >> upload_sdf_to_big_query\n\nwith models.DAG(\n    \"example_display_video_v2\",\n    start_date=START_DATE,\n    catchup=False,\n) as dag:\n    # [START howto_google_display_video_create_query_operator]\n    create_query_v2 = GoogleDisplayVideo360CreateQueryOperator(body=REPORT_V2, task_id=\"create_query\")\n\n    query_id = cast(str, XComArg(create_query_v2, key=\"query_id\"))\n    # [END howto_google_display_video_create_query_operator]\n\n    # [START howto_google_display_video_run_query_report_operator]\n    run_query_v2 = GoogleDisplayVideo360RunQueryOperator(\n        query_id=query_id, parameters=PARAMETERS, task_id=\"run_report\"\n    )\n\n    query_id = cast(str, XComArg(run_query_v2, key=\"query_id\"))\n    report_id = cast(str, XComArg(run_query_v2, key=\"report_id\"))\n    # [END howto_google_display_video_run_query_report_operator]\n\n    # [START howto_google_display_video_wait_run_query_sensor]\n    wait_for_query = GoogleDisplayVideo360RunQuerySensor(\n        task_id=\"wait_for_query\",\n        query_id=query_id,\n        report_id=report_id,\n    )\n    # [END howto_google_display_video_wait_run_query_sensor]\n\n    # [START howto_google_display_video_get_report_operator]\n    get_report_v2 = GoogleDisplayVideo360DownloadReportV2Operator(\n        query_id=query_id,\n        report_id=report_id,\n        task_id=\"get_report\",\n        bucket_name=BUCKET,\n        report_name=\"test1.csv\",\n    )\n    # # [END howto_google_display_video_get_report_operator]\n    # # [START howto_google_display_video_delete_query_report_operator]\n    delete_report_v2 = GoogleDisplayVideo360DeleteReportOperator(report_id=report_id, task_id=\"delete_report\")\n    # # [END howto_google_display_video_delete_query_report_operator]\n\n    create_query_v2 >> run_query_v2 >> wait_for_query >> get_report_v2 >> delete_report_v2", "metadata": {"file_name": "example_display_video.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex configuration dictionaries, multiple operator types, sensor usage, and sophisticated reporting parameters", "key_concepts": ["Dynamic Configuration", "Marketing Data Reporting", "Sensor Operators", "Environment Variable Management", "Data Transformation"], "learning_objectives": ["Master configuration management in Airflow", "Understand marketing data pipeline design", "Learn parameter-driven workflow construction"]}}
{"instruction": "Build an Azure Blob Storage monitoring DAG that demonstrates how to use sensors to detect the presence of specific blobs or blob prefixes in a container, using environment variables for flexible configuration.", "input": {"airflow_version": "2.7.2", "operators": ["WasbBlobSensor", "WasbPrefixSensor"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that senses blob(s) in Azure Blob Storage.\n\nThis DAG relies on the following OS environment variables\n\n* CONTAINER_NAME - The container under which to look for the blob.\n* BLOB_NAME - The name of the blob to match.\n* PREFIX - The blob with the specified prefix to match.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor, WasbPrefixSensor\n\nCONTAINER_NAME = os.environ.get(\"CONTAINER_NAME\", \"example-container-name\")\nBLOB_NAME = os.environ.get(\"BLOB_NAME\", \"example-blob-name\")\nPREFIX = os.environ.get(\"PREFIX\", \"example-prefix\")\n\n\nwith DAG(\n    \"example_wasb_sensors\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START wasb_blob_sensor]\n    azure_wasb_sensor = WasbBlobSensor(\n        container_name=CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        task_id=\"wasb_sense_blob\",\n    )\n    # [END wasb_blob_sensor]\n\n    # [START wasb_prefix_sensor]\n    azure_wasb_prefix_sensor = WasbPrefixSensor(\n        container_name=CONTAINER_NAME,\n        prefix=PREFIX,\n        task_id=\"wasb_sense_prefix\",\n    )\n    # [END wasb_prefix_sensor]", "metadata": {"file_name": "example_wasb_sensors.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service sensor operators, external Azure integration, environment variable configuration, and demonstrates two different sensing approaches", "key_concepts": ["Azure Blob Storage", "Airflow Sensors", "Environment Variable Configuration", "Cloud Service Integration"], "learning_objectives": ["Understand how to create Azure Blob Storage sensors", "Learn to use environment variables in Airflow DAGs", "Explore different blob detection strategies"]}}
{"instruction": "Design a flexible file detection workflow that demonstrates how to conditionally trigger downstream tasks based on the existence of specific files in a cloud storage container using parameterized sensor configurations.", "input": {"airflow_version": "2.7.2", "operators": ["WasbBlobSensor", "WasbPrefixSensor"], "line_count": 43, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that senses blob(s) in Azure Blob Storage.\n\nThis DAG relies on the following OS environment variables\n\n* CONTAINER_NAME - The container under which to look for the blob.\n* BLOB_NAME - The name of the blob to match.\n* PREFIX - The blob with the specified prefix to match.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor, WasbPrefixSensor\n\nCONTAINER_NAME = os.environ.get(\"CONTAINER_NAME\", \"example-container-name\")\nBLOB_NAME = os.environ.get(\"BLOB_NAME\", \"example-blob-name\")\nPREFIX = os.environ.get(\"PREFIX\", \"example-prefix\")\n\n\nwith DAG(\n    \"example_wasb_sensors\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START wasb_blob_sensor]\n    azure_wasb_sensor = WasbBlobSensor(\n        container_name=CONTAINER_NAME,\n        blob_name=BLOB_NAME,\n        task_id=\"wasb_sense_blob\",\n    )\n    # [END wasb_blob_sensor]\n\n    # [START wasb_prefix_sensor]\n    azure_wasb_prefix_sensor = WasbPrefixSensor(\n        container_name=CONTAINER_NAME,\n        prefix=PREFIX,\n        task_id=\"wasb_sense_prefix\",\n    )\n    # [END wasb_prefix_sensor]", "metadata": {"file_name": "example_wasb_sensors.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves cloud service operators, sensor logic, dynamic configuration through environment variables, and showcases modular DAG design", "key_concepts": ["Conditional Task Triggering", "Sensor Operators", "Parameterized DAG Configuration", "Cloud Storage Monitoring"], "learning_objectives": ["Implement file existence checking in Airflow", "Understand sensor-based workflow design", "Learn dynamic DAG configuration techniques"]}}
{"instruction": "Build a Google Cloud Dataflow streaming job orchestration DAG that demonstrates how to launch a Flex Template for processing real-time data from Pub/Sub to BigQuery using Airflow's DataflowStartFlexTemplateOperator.", "input": {"airflow_version": "2.7.2", "operators": ["DataflowStartFlexTemplateOperator"], "line_count": 52, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nDATAFLOW_FLEX_TEMPLATE_JOB_NAME = os.environ.get(\n    \"GCP_DATAFLOW_FLEX_TEMPLATE_JOB_NAME\", \"dataflow-flex-template\"\n)\n\n# For simplicity we use the same topic name as the subscription name.\nPUBSUB_FLEX_TEMPLATE_TOPIC = os.environ.get(\n    \"GCP_DATAFLOW_PUBSUB_FLEX_TEMPLATE_TOPIC\", \"dataflow-flex-template\"\n)\nPUBSUB_FLEX_TEMPLATE_SUBSCRIPTION = PUBSUB_FLEX_TEMPLATE_TOPIC\nGCS_FLEX_TEMPLATE_TEMPLATE_PATH = os.environ.get(\n    \"GCP_DATAFLOW_GCS_FLEX_TEMPLATE_TEMPLATE_PATH\",\n    \"gs://INVALID BUCKET NAME/samples/dataflow/templates/streaming-beam-sql.json\",\n)\nBQ_FLEX_TEMPLATE_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_DATASET\", \"airflow_dataflow_samples\")\nBQ_FLEX_TEMPLATE_LOCATION = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_LOCATION>\", \"us-west1\")\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_flex_template_java\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag_flex_template:\n    # [START howto_operator_start_template_job]\n    start_flex_template = DataflowStartFlexTemplateOperator(\n        task_id=\"start_flex_template_streaming_beam_sql\",\n        project_id=GCP_PROJECT_ID,\n        body={\n            \"launchParameter\": {\n                \"containerSpecGcsPath\": GCS_FLEX_TEMPLATE_TEMPLATE_PATH,\n                \"jobName\": DATAFLOW_FLEX_TEMPLATE_JOB_NAME,\n                \"parameters\": {\n                    \"inputSubscription\": PUBSUB_FLEX_TEMPLATE_SUBSCRIPTION,\n                    \"outputTable\": f\"{GCP_PROJECT_ID}:{BQ_FLEX_TEMPLATE_DATASET}.streaming_beam_sql\",\n                },\n            }\n        },\n        do_xcom_push=True,\n        location=BQ_FLEX_TEMPLATE_LOCATION,\n    )\n    # [END howto_operator_start_template_job]", "metadata": {"file_name": "example_dataflow_flex_template.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operator, external integration with GCP services, environment variable configuration, and requires understanding of Dataflow Flex Template parameters", "key_concepts": ["Dataflow Flex Templates", "Cloud Service Operators", "Streaming Data Processing", "Environment Configuration", "GCP Integration"], "learning_objectives": ["Configure Dataflow Flex Template job parameters", "Use DataflowStartFlexTemplateOperator", "Set up cloud service integration in Airflow", "Understand streaming data pipeline configuration"]}}
{"instruction": "Design an Airflow DAG that demonstrates secure and flexible cloud job launching using environment variables, parameterized configuration, and dynamic job naming for a streaming data processing workflow.", "input": {"airflow_version": "2.7.2", "operators": ["DataflowStartFlexTemplateOperator"], "line_count": 52, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG for Google Cloud Dataflow service\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\n\nDATAFLOW_FLEX_TEMPLATE_JOB_NAME = os.environ.get(\n    \"GCP_DATAFLOW_FLEX_TEMPLATE_JOB_NAME\", \"dataflow-flex-template\"\n)\n\n# For simplicity we use the same topic name as the subscription name.\nPUBSUB_FLEX_TEMPLATE_TOPIC = os.environ.get(\n    \"GCP_DATAFLOW_PUBSUB_FLEX_TEMPLATE_TOPIC\", \"dataflow-flex-template\"\n)\nPUBSUB_FLEX_TEMPLATE_SUBSCRIPTION = PUBSUB_FLEX_TEMPLATE_TOPIC\nGCS_FLEX_TEMPLATE_TEMPLATE_PATH = os.environ.get(\n    \"GCP_DATAFLOW_GCS_FLEX_TEMPLATE_TEMPLATE_PATH\",\n    \"gs://INVALID BUCKET NAME/samples/dataflow/templates/streaming-beam-sql.json\",\n)\nBQ_FLEX_TEMPLATE_DATASET = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_DATASET\", \"airflow_dataflow_samples\")\nBQ_FLEX_TEMPLATE_LOCATION = os.environ.get(\"GCP_DATAFLOW_BQ_FLEX_TEMPLATE_LOCATION>\", \"us-west1\")\n\nwith models.DAG(\n    dag_id=\"example_gcp_dataflow_flex_template_java\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag_flex_template:\n    # [START howto_operator_start_template_job]\n    start_flex_template = DataflowStartFlexTemplateOperator(\n        task_id=\"start_flex_template_streaming_beam_sql\",\n        project_id=GCP_PROJECT_ID,\n        body={\n            \"launchParameter\": {\n                \"containerSpecGcsPath\": GCS_FLEX_TEMPLATE_TEMPLATE_PATH,\n                \"jobName\": DATAFLOW_FLEX_TEMPLATE_JOB_NAME,\n                \"parameters\": {\n                    \"inputSubscription\": PUBSUB_FLEX_TEMPLATE_SUBSCRIPTION,\n                    \"outputTable\": f\"{GCP_PROJECT_ID}:{BQ_FLEX_TEMPLATE_DATASET}.streaming_beam_sql\",\n                },\n            }\n        },\n        do_xcom_push=True,\n        location=BQ_FLEX_TEMPLATE_LOCATION,\n    )\n    # [END howto_operator_start_template_job]", "metadata": {"file_name": "example_dataflow_flex_template.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple configuration strategies, environment-based parameterization, and demonstrates advanced configuration management techniques", "key_concepts": ["Environment Variable Management", "Parameterized Configuration", "Job Naming Strategies", "Secure Configuration", "Dynamic Job Parameters"], "learning_objectives": ["Use environment variables for configuration", "Create flexible job configuration patterns", "Implement dynamic job naming", "Understand parameter injection in cloud workflows"]}}
{"instruction": "Build an end-to-end machine learning workflow using Google Vertex AI operators that demonstrates automated model training, deployment, and prediction pipeline stages across different ML domains.", "input": {"airflow_version": "2.7.2", "operators": ["CreateAutoMLForecastingTrainingJobOperator", "CreateAutoMLImageTrainingJobOperator", "CreateAutoMLTabularTrainingJobOperator", "CreateAutoMLTextTrainingJobOperator", "CreateAutoMLVideoTrainingJobOperator", "CreateBatchPredictionJobOperator", "CreateCustomContainerTrainingJobOperator", "CreateCustomPythonPackageTrainingJobOperator", "CreateCustomTrainingJobOperator", "CreateDatasetOperator", "CreateEndpointOperator", "CreateHyperparameterTuningJobOperator", "DeleteAutoMLTrainingJobOperator", "DeleteBatchPredictionJobOperator", "DeleteCustomTrainingJobOperator", "DeleteDatasetOperator", "DeleteEndpointOperator", "DeleteHyperparameterTuningJobOperator", "DeleteModelOperator", "DeployModelOperator", "ExportDataOperator", "ExportModelOperator", "GetDatasetOperator", "GetHyperparameterTuningJobOperator", "ImportDataOperator", "ListAutoMLTrainingJobOperator", "ListBatchPredictionJobsOperator", "ListCustomTrainingJobOperator", "ListDatasetsOperator", "ListEndpointsOperator", "ListHyperparameterTuningJobOperator", "ListModelsOperator", "UndeployModelOperator", "UpdateDatasetOperator", "UploadModelOperator"], "line_count": 737, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that demonstrates operators for the Google Vertex AI service in the Google\nCloud Platform.\n\nThis DAG relies on the following OS environment variables:\n\n* GCP_VERTEX_AI_BUCKET - Google Cloud Storage bucket where the model will be saved\n  after training process was finished.\n* CUSTOM_CONTAINER_URI - path to container with model.\n* PYTHON_PACKAGE_GSC_URI - path to test model in archive.\n* LOCAL_TRAINING_SCRIPT_PATH - path to local training script.\n* DATASET_ID - ID of dataset which will be used in training process.\n* MODEL_ID - ID of model which will be used in predict process.\n* MODEL_ARTIFACT_URI - The artifact_uri should be the path to a GCS directory containing saved model\n  artifacts.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom uuid import uuid4\n\nfrom google.cloud import aiplatform\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLForecastingTrainingJobOperator,\n    CreateAutoMLImageTrainingJobOperator,\n    CreateAutoMLTabularTrainingJobOperator,\n    CreateAutoMLTextTrainingJobOperator,\n    CreateAutoMLVideoTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n    ListAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.batch_prediction_job import (\n    CreateBatchPredictionJobOperator,\n    DeleteBatchPredictionJobOperator,\n    ListBatchPredictionJobsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomContainerTrainingJobOperator,\n    CreateCustomPythonPackageTrainingJobOperator,\n    CreateCustomTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n    ListCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ExportDataOperator,\n    GetDatasetOperator,\n    ImportDataOperator,\n    ListDatasetsOperator,\n    UpdateDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.endpoint_service import (\n    CreateEndpointOperator,\n    DeleteEndpointOperator,\n    DeployModelOperator,\n    ListEndpointsOperator,\n    UndeployModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.hyperparameter_tuning_job import (\n    CreateHyperparameterTuningJobOperator,\n    DeleteHyperparameterTuningJobOperator,\n    GetHyperparameterTuningJobOperator,\n    ListHyperparameterTuningJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.model_service import (\n    DeleteModelOperator,\n    ExportModelOperator,\n    ListModelsOperator,\n    UploadModelOperator,\n)\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"an-id\")\nREGION = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\nBUCKET = os.environ.get(\"GCP_VERTEX_AI_BUCKET\", \"vertex-ai-system-tests\")\n\nSTAGING_BUCKET = f\"gs://{BUCKET}\"\nDISPLAY_NAME = str(uuid4())  # Create random display name\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nCUSTOM_CONTAINER_URI = os.environ.get(\"CUSTOM_CONTAINER_URI\", \"path_to_container_with_model\")\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nTRAINING_FRACTION_SPLIT = 0.7\nTEST_FRACTION_SPLIT = 0.15\nVALIDATION_FRACTION_SPLIT = 0.15\n\nPYTHON_PACKAGE_GCS_URI = os.environ.get(\"PYTHON_PACKAGE_GSC_URI\", \"path_to_test_model_in_arch\")\nPYTHON_MODULE_NAME = \"aiplatform_custom_trainer_script.task\"\n\nLOCAL_TRAINING_SCRIPT_PATH = os.environ.get(\"LOCAL_TRAINING_SCRIPT_PATH\", \"path_to_training_script\")\n\nTRAINING_PIPELINE_ID = \"test-training-pipeline-id\"\nCUSTOM_JOB_ID = \"test-custom-job-id\"\n\nIMAGE_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-image-dataset\"),\n}\nTABULAR_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-tabular-dataset\"),\n}\nTEXT_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-text-dataset\"),\n}\nVIDEO_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/video_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nTIME_SERIES_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/time_series_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nDATASET_ID = os.environ.get(\"DATASET_ID\", \"test-dataset-id\")\nTEST_EXPORT_CONFIG = {\"gcs_destination\": {\"output_uri_prefix\": \"gs://test-vertex-ai-bucket/exports\"}}\nTEST_IMPORT_CONFIG = [\n    {\n        \"data_item_labels\": {\n            \"test-labels-name\": \"test-labels-value\",\n        },\n        \"import_schema_uri\": (\n            \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_bounding_box_io_format_1.0.0.yaml\"\n        ),\n        \"gcs_source\": {\n            \"uris\": [\"gs://ucaip-test-us-central1/dataset/salads_oid_ml_use_public_unassigned.jsonl\"]\n        },\n    },\n]\nDATASET_TO_UPDATE = {\"display_name\": \"test-name\"}\nTEST_UPDATE_MASK = {\"paths\": [\"displayName\"]}\n\nTEST_TIME_COLUMN = \"date\"\nTEST_TIME_SERIES_IDENTIFIER_COLUMN = \"store_name\"\nTEST_TARGET_COLUMN = \"sale_dollars\"\n\nCOLUMN_SPECS = {\n    TEST_TIME_COLUMN: \"timestamp\",\n    TEST_TARGET_COLUMN: \"numeric\",\n    \"city\": \"categorical\",\n    \"zip_code\": \"categorical\",\n    \"county\": \"categorical\",\n}\n\nCOLUMN_TRANSFORMATIONS = [\n    {\"categorical\": {\"column_name\": \"Type\"}},\n    {\"numeric\": {\"column_name\": \"Age\"}},\n    {\"categorical\": {\"column_name\": \"Breed1\"}},\n    {\"categorical\": {\"column_name\": \"Color1\"}},\n    {\"categorical\": {\"column_name\": \"Color2\"}},\n    {\"categorical\": {\"column_name\": \"MaturitySize\"}},\n    {\"categorical\": {\"column_name\": \"FurLength\"}},\n    {\"categorical\": {\"column_name\": \"Vaccinated\"}},\n    {\"categorical\": {\"column_name\": \"Sterilized\"}},\n    {\"categorical\": {\"column_name\": \"Health\"}},\n    {\"numeric\": {\"column_name\": \"Fee\"}},\n    {\"numeric\": {\"column_name\": \"PhotoAmt\"}},\n]\n\nMODEL_ID = os.environ.get(\"MODEL_ID\", \"test-model-id\")\nMODEL_ARTIFACT_URI = os.environ.get(\"MODEL_ARTIFACT_URI\", \"path_to_folder_with_model_artifacts\")\nMODEL_NAME = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\"\nJOB_DISPLAY_NAME = f\"temp_create_batch_prediction_job_test_{uuid4()}\"\nBIGQUERY_SOURCE = f\"bq://{PROJECT_ID}.test_iowa_liquor_sales_forecasting_us.2021_sales_predict\"\nGCS_DESTINATION_PREFIX = \"gs://test-vertex-ai-bucket-us/output\"\nMODEL_PARAMETERS: dict | None = {}\n\nENDPOINT_CONF = {\n    \"display_name\": f\"endpoint_test_{uuid4()}\",\n}\nDEPLOYED_MODEL = {\n    # format: 'projects/{project}/locations/{location}/models/{model}'\n    \"model\": f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\",\n    \"display_name\": f\"temp_endpoint_test_{uuid4()}\",\n    \"dedicated_resources\": {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-2\",\n            \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n            \"accelerator_count\": 1,\n        },\n        \"min_replica_count\": 1,\n        \"max_replica_count\": 1,\n    },\n}\n\nMODEL_OUTPUT_CONFIG = {\n    \"artifact_destination\": {\n        \"output_uri_prefix\": STAGING_BUCKET,\n    },\n    \"export_format_id\": \"custom-trained\",\n}\nMODEL_OBJ = {\n    \"display_name\": f\"model-{str(uuid4())}\",\n    \"artifact_uri\": MODEL_ARTIFACT_URI,\n    \"container_spec\": {\n        \"image_uri\": MODEL_SERVING_CONTAINER_URI,\n        \"command\": [],\n        \"args\": [],\n        \"env\": [],\n        \"ports\": [],\n        \"predict_route\": \"\",\n        \"health_route\": \"\",\n    },\n}\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_custom_jobs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as custom_jobs_dag:\n    # [START how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n    create_custom_container_training_job = CreateCustomContainerTrainingJobOperator(\n        task_id=\"custom_container_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-container-{DISPLAY_NAME}\",\n        container_uri=CUSTOM_CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        command=[\"python3\", \"task.py\"],\n        model_display_name=f\"container-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n    create_custom_python_package_training_job = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"python_package_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-py-package-{DISPLAY_NAME}\",\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        container_uri=CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        model_display_name=f\"py-package-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_training_job_operator]\n    create_custom_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"custom_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-custom-{DISPLAY_NAME}\",\n        script_path=LOCAL_TRAINING_SCRIPT_PATH,\n        container_uri=CONTAINER_URI,\n        requirements=[\"gcsfs==0.7.1\"],\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        replica_count=1,\n        model_display_name=f\"custom-housing-model-{DISPLAY_NAME}\",\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        custom_job_id=CUSTOM_JOB_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_custom_training_job_operator]\n    list_custom_training_job = ListCustomTrainingJobOperator(\n        task_id=\"list_custom_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_custom_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_dataset\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dataset_dag:\n    # [START how_to_cloud_vertex_ai_create_dataset_operator]\n    create_image_dataset_job = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_tabular_dataset_job = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_text_dataset_job = CreateDatasetOperator(\n        task_id=\"text_dataset\",\n        dataset=TEXT_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_video_dataset_job = CreateDatasetOperator(\n        task_id=\"video_dataset\",\n        dataset=VIDEO_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_time_series_dataset_job = CreateDatasetOperator(\n        task_id=\"time_series_dataset\",\n        dataset=TIME_SERIES_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_dataset_operator]\n    delete_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=create_text_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_get_dataset_operator]\n    get_dataset = GetDatasetOperator(\n        task_id=\"get_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_tabular_dataset_job.output[\"dataset_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_export_data_operator]\n    export_data_job = ExportDataOperator(\n        task_id=\"export_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        export_config=TEST_EXPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_data_operator]\n\n    # [START how_to_cloud_vertex_ai_import_data_operator]\n    import_data_job = ImportDataOperator(\n        task_id=\"import_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=TEST_IMPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_import_data_operator]\n\n    # [START how_to_cloud_vertex_ai_list_dataset_operator]\n    list_dataset_job = ListDatasetsOperator(\n        task_id=\"list_dataset\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_update_dataset_operator]\n    update_dataset_job = UpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_video_dataset_job.output[\"dataset_id\"],\n        dataset=DATASET_TO_UPDATE,\n        update_mask=TEST_UPDATE_MASK,\n    )\n    # [END how_to_cloud_vertex_ai_update_dataset_operator]\n\n    create_time_series_dataset_job\n    create_text_dataset_job >> delete_dataset_job\n    create_tabular_dataset_job >> get_dataset\n    create_image_dataset_job >> import_data_job >> export_data_job\n    create_video_dataset_job >> update_dataset_job\n    list_dataset_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_auto_ml\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as auto_ml_dag:\n    # [START how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n    create_auto_ml_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(\n        task_id=\"auto_ml_forecasting_task\",\n        display_name=f\"auto-ml-forecasting-{DISPLAY_NAME}\",\n        optimization_objective=\"minimize-rmse\",\n        column_specs=COLUMN_SPECS,\n        # run params\n        dataset_id=DATASET_ID,\n        target_column=TEST_TARGET_COLUMN,\n        time_column=TEST_TIME_COLUMN,\n        time_series_identifier_column=TEST_TIME_SERIES_IDENTIFIER_COLUMN,\n        available_at_forecast_columns=[TEST_TIME_COLUMN],\n        unavailable_at_forecast_columns=[TEST_TARGET_COLUMN],\n        time_series_attribute_columns=[\"city\", \"zip_code\", \"county\"],\n        forecast_horizon=30,\n        context_window=30,\n        data_granularity_unit=\"day\",\n        data_granularity_count=1,\n        weight_column=None,\n        budget_milli_node_hours=1000,\n        model_display_name=f\"auto-ml-forecasting-model-{DISPLAY_NAME}\",\n        predefined_split_column_name=None,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n    create_auto_ml_image_training_job = CreateAutoMLImageTrainingJobOperator(\n        task_id=\"auto_ml_image_task\",\n        display_name=f\"auto-ml-image-{DISPLAY_NAME}\",\n        dataset_id=DATASET_ID,\n        prediction_type=\"classification\",\n        multi_label=False,\n        model_type=\"CLOUD\",\n        training_fraction_split=0.6,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.2,\n        budget_milli_node_hours=8000,\n        model_display_name=f\"auto-ml-image-model-{DISPLAY_NAME}\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n    create_auto_ml_tabular_training_job = CreateAutoMLTabularTrainingJobOperator(\n        task_id=\"auto_ml_tabular_task\",\n        display_name=f\"auto-ml-tabular-{DISPLAY_NAME}\",\n        optimization_prediction_type=\"classification\",\n        column_transformations=COLUMN_TRANSFORMATIONS,\n        dataset_id=DATASET_ID,\n        target_column=\"Adopted\",\n        training_fraction_split=0.8,\n        validation_fraction_split=0.1,\n        test_fraction_split=0.1,\n        model_display_name=\"adopted-prediction-model\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n    create_auto_ml_text_training_job = CreateAutoMLTextTrainingJobOperator(\n        task_id=\"auto_ml_text_task\",\n        display_name=f\"auto-ml-text-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        multi_label=False,\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-text-model-{DISPLAY_NAME}\",\n        training_fraction_split=0.7,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.1,\n        sync=True,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n    create_auto_ml_video_training_job = CreateAutoMLVideoTrainingJobOperator(\n        task_id=\"auto_ml_video_task\",\n        display_name=f\"auto-ml-video-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        model_type=\"CLOUD\",\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-video-model-{DISPLAY_NAME}\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n    delete_auto_ml_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n    list_auto_ml_training_job = ListAutoMLTrainingJobOperator(\n        task_id=\"list_auto_ml_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_batch_prediction_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as batch_prediction_job_dag:\n    # [START how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n    create_batch_prediction_job = CreateBatchPredictionJobOperator(\n        task_id=\"create_batch_prediction_job\",\n        job_display_name=JOB_DISPLAY_NAME,\n        model_name=MODEL_NAME,\n        predictions_format=\"csv\",\n        bigquery_source=BIGQUERY_SOURCE,\n        gcs_destination_prefix=GCS_DESTINATION_PREFIX,\n        model_parameters=MODEL_PARAMETERS,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n    list_batch_prediction_job = ListBatchPredictionJobsOperator(\n        task_id=\"list_batch_prediction_jobs\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n    delete_batch_prediction_job = DeleteBatchPredictionJobOperator(\n        task_id=\"delete_batch_prediction_job\",\n        batch_prediction_job_id=create_batch_prediction_job.output[\"batch_prediction_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n\n    create_batch_prediction_job >> delete_batch_prediction_job\n    list_batch_prediction_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_endpoint\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as endpoint_dag:\n    # [START how_to_cloud_vertex_ai_create_endpoint_operator]\n    create_endpoint = CreateEndpointOperator(\n        task_id=\"create_endpoint\",\n        endpoint=ENDPOINT_CONF,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_endpoint_operator]\n    delete_endpoint = DeleteEndpointOperator(\n        task_id=\"delete_endpoint\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_list_endpoints_operator]\n    list_endpoints = ListEndpointsOperator(\n        task_id=\"list_endpoints\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_endpoints_operator]\n\n    # [START how_to_cloud_vertex_ai_deploy_model_operator]\n    deploy_model = DeployModelOperator(\n        task_id=\"deploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model=DEPLOYED_MODEL,\n        traffic_split={\"0\": 100},\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_deploy_model_operator]\n\n    # [START how_to_cloud_vertex_ai_undeploy_model_operator]\n    undeploy_model = UndeployModelOperator(\n        task_id=\"undeploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model_id=deploy_model.output[\"deployed_model_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_undeploy_model_operator]\n\n    create_endpoint >> deploy_model >> undeploy_model >> delete_endpoint\n    list_endpoints\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_hyperparameter_tuning_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as hyperparameter_tuning_job_dag:\n    # [START how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n    create_hyperparameter_tuning_job = CreateHyperparameterTuningJobOperator(\n        task_id=\"create_hyperparameter_tuning_job\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"horses-humans-hyptertune-{DISPLAY_NAME}\",\n        worker_pool_specs=[\n            {\n                \"machine_spec\": {\n                    \"machine_type\": MACHINE_TYPE,\n                    \"accelerator_type\": ACCELERATOR_TYPE,\n                    \"accelerator_count\": ACCELERATOR_COUNT,\n                },\n                \"replica_count\": REPLICA_COUNT,\n                \"container_spec\": {\n                    \"image_uri\": f\"gcr.io/{PROJECT_ID}/horse-human:hypertune\",\n                },\n            }\n        ],\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n        parameter_spec={\n            \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n                min=0.01, max=1, scale=\"log\"\n            ),\n            \"momentum\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale=\"linear\"),\n            \"num_neurons\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n                values=[64, 128, 512], scale=\"linear\"\n            ),\n        },\n        metric_spec={\n            \"accuracy\": \"maximize\",\n        },\n        max_trial_count=15,\n        parallel_trial_count=3,\n    )\n    # [END how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n    get_hyperparameter_tuning_job = GetHyperparameterTuningJobOperator(\n        task_id=\"get_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n    delete_hyperparameter_tuning_job = DeleteHyperparameterTuningJobOperator(\n        task_id=\"delete_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n    list_hyperparameter_tuning_job = ListHyperparameterTuningJobOperator(\n        task_id=\"list_hyperparameter_tuning_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n\n    create_hyperparameter_tuning_job >> get_hyperparameter_tuning_job >> delete_hyperparameter_tuning_job\n    list_hyperparameter_tuning_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_model_service\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as model_service_dag:\n    # [START how_to_cloud_vertex_ai_upload_model_operator]\n    upload_model = UploadModelOperator(\n        task_id=\"upload_model\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        model=MODEL_OBJ,\n    )\n    # [END how_to_cloud_vertex_ai_upload_model_operator]\n\n    # [START how_to_cloud_vertex_ai_export_model_operator]\n    export_model = ExportModelOperator(\n        task_id=\"export_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n        output_config=MODEL_OUTPUT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_model_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_model_operator]\n    delete_model = DeleteModelOperator(\n        task_id=\"delete_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_model_operator]\n\n    # [START how_to_cloud_vertex_ai_list_models_operator]\n    list_models = ListModelsOperator(\n        task_id=\"list_models\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_models_operator]\n\n    upload_model >> export_model >> delete_model\n    list_models", "metadata": {"file_name": "example_vertex_ai.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Multiple complex Vertex AI operators, extensive cloud service integration, advanced ML workflow with training, deployment, and prediction components across different model types", "key_concepts": ["Vertex AI Operators", "Cloud ML Orchestration", "Multi-Model Training", "Automated ML Workflows", "Google Cloud Platform Integration"], "learning_objectives": ["Understand Vertex AI operator interactions", "Learn cloud-based ML pipeline construction", "Master dynamic ML workflow design", "Explore automated model management"]}}
{"instruction": "Design a comprehensive machine learning infrastructure automation script that demonstrates flexible configuration management, environment variable handling, and scalable ML job orchestration using Airflow and Vertex AI.", "input": {"airflow_version": "2.7.2", "operators": ["CreateAutoMLForecastingTrainingJobOperator", "CreateAutoMLImageTrainingJobOperator", "CreateAutoMLTabularTrainingJobOperator", "CreateAutoMLTextTrainingJobOperator", "CreateAutoMLVideoTrainingJobOperator", "CreateBatchPredictionJobOperator", "CreateCustomContainerTrainingJobOperator", "CreateCustomPythonPackageTrainingJobOperator", "CreateCustomTrainingJobOperator", "CreateDatasetOperator", "CreateEndpointOperator", "CreateHyperparameterTuningJobOperator", "DeleteAutoMLTrainingJobOperator", "DeleteBatchPredictionJobOperator", "DeleteCustomTrainingJobOperator", "DeleteDatasetOperator", "DeleteEndpointOperator", "DeleteHyperparameterTuningJobOperator", "DeleteModelOperator", "DeployModelOperator", "ExportDataOperator", "ExportModelOperator", "GetDatasetOperator", "GetHyperparameterTuningJobOperator", "ImportDataOperator", "ListAutoMLTrainingJobOperator", "ListBatchPredictionJobsOperator", "ListCustomTrainingJobOperator", "ListDatasetsOperator", "ListEndpointsOperator", "ListHyperparameterTuningJobOperator", "ListModelsOperator", "UndeployModelOperator", "UpdateDatasetOperator", "UploadModelOperator"], "line_count": 737, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that demonstrates operators for the Google Vertex AI service in the Google\nCloud Platform.\n\nThis DAG relies on the following OS environment variables:\n\n* GCP_VERTEX_AI_BUCKET - Google Cloud Storage bucket where the model will be saved\n  after training process was finished.\n* CUSTOM_CONTAINER_URI - path to container with model.\n* PYTHON_PACKAGE_GSC_URI - path to test model in archive.\n* LOCAL_TRAINING_SCRIPT_PATH - path to local training script.\n* DATASET_ID - ID of dataset which will be used in training process.\n* MODEL_ID - ID of model which will be used in predict process.\n* MODEL_ARTIFACT_URI - The artifact_uri should be the path to a GCS directory containing saved model\n  artifacts.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom uuid import uuid4\n\nfrom google.cloud import aiplatform\nfrom google.protobuf.struct_pb2 import Value\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.vertex_ai.auto_ml import (\n    CreateAutoMLForecastingTrainingJobOperator,\n    CreateAutoMLImageTrainingJobOperator,\n    CreateAutoMLTabularTrainingJobOperator,\n    CreateAutoMLTextTrainingJobOperator,\n    CreateAutoMLVideoTrainingJobOperator,\n    DeleteAutoMLTrainingJobOperator,\n    ListAutoMLTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.batch_prediction_job import (\n    CreateBatchPredictionJobOperator,\n    DeleteBatchPredictionJobOperator,\n    ListBatchPredictionJobsOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.custom_job import (\n    CreateCustomContainerTrainingJobOperator,\n    CreateCustomPythonPackageTrainingJobOperator,\n    CreateCustomTrainingJobOperator,\n    DeleteCustomTrainingJobOperator,\n    ListCustomTrainingJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.dataset import (\n    CreateDatasetOperator,\n    DeleteDatasetOperator,\n    ExportDataOperator,\n    GetDatasetOperator,\n    ImportDataOperator,\n    ListDatasetsOperator,\n    UpdateDatasetOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.endpoint_service import (\n    CreateEndpointOperator,\n    DeleteEndpointOperator,\n    DeployModelOperator,\n    ListEndpointsOperator,\n    UndeployModelOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.hyperparameter_tuning_job import (\n    CreateHyperparameterTuningJobOperator,\n    DeleteHyperparameterTuningJobOperator,\n    GetHyperparameterTuningJobOperator,\n    ListHyperparameterTuningJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.vertex_ai.model_service import (\n    DeleteModelOperator,\n    ExportModelOperator,\n    ListModelsOperator,\n    UploadModelOperator,\n)\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\n\n\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"an-id\")\nREGION = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\nBUCKET = os.environ.get(\"GCP_VERTEX_AI_BUCKET\", \"vertex-ai-system-tests\")\n\nSTAGING_BUCKET = f\"gs://{BUCKET}\"\nDISPLAY_NAME = str(uuid4())  # Create random display name\nCONTAINER_URI = \"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\"\nCUSTOM_CONTAINER_URI = os.environ.get(\"CUSTOM_CONTAINER_URI\", \"path_to_container_with_model\")\nMODEL_SERVING_CONTAINER_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\"\nREPLICA_COUNT = 1\nMACHINE_TYPE = \"n1-standard-4\"\nACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\nACCELERATOR_COUNT = 0\nTRAINING_FRACTION_SPLIT = 0.7\nTEST_FRACTION_SPLIT = 0.15\nVALIDATION_FRACTION_SPLIT = 0.15\n\nPYTHON_PACKAGE_GCS_URI = os.environ.get(\"PYTHON_PACKAGE_GSC_URI\", \"path_to_test_model_in_arch\")\nPYTHON_MODULE_NAME = \"aiplatform_custom_trainer_script.task\"\n\nLOCAL_TRAINING_SCRIPT_PATH = os.environ.get(\"LOCAL_TRAINING_SCRIPT_PATH\", \"path_to_training_script\")\n\nTRAINING_PIPELINE_ID = \"test-training-pipeline-id\"\nCUSTOM_JOB_ID = \"test-custom-job-id\"\n\nIMAGE_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-image-dataset\"),\n}\nTABULAR_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-tabular-dataset\"),\n}\nTEXT_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-text-dataset\"),\n}\nVIDEO_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/video_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nTIME_SERIES_DATASET = {\n    \"display_name\": str(uuid4()),\n    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/time_series_1.0.0.yaml\",\n    \"metadata\": Value(string_value=\"test-video-dataset\"),\n}\nDATASET_ID = os.environ.get(\"DATASET_ID\", \"test-dataset-id\")\nTEST_EXPORT_CONFIG = {\"gcs_destination\": {\"output_uri_prefix\": \"gs://test-vertex-ai-bucket/exports\"}}\nTEST_IMPORT_CONFIG = [\n    {\n        \"data_item_labels\": {\n            \"test-labels-name\": \"test-labels-value\",\n        },\n        \"import_schema_uri\": (\n            \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_bounding_box_io_format_1.0.0.yaml\"\n        ),\n        \"gcs_source\": {\n            \"uris\": [\"gs://ucaip-test-us-central1/dataset/salads_oid_ml_use_public_unassigned.jsonl\"]\n        },\n    },\n]\nDATASET_TO_UPDATE = {\"display_name\": \"test-name\"}\nTEST_UPDATE_MASK = {\"paths\": [\"displayName\"]}\n\nTEST_TIME_COLUMN = \"date\"\nTEST_TIME_SERIES_IDENTIFIER_COLUMN = \"store_name\"\nTEST_TARGET_COLUMN = \"sale_dollars\"\n\nCOLUMN_SPECS = {\n    TEST_TIME_COLUMN: \"timestamp\",\n    TEST_TARGET_COLUMN: \"numeric\",\n    \"city\": \"categorical\",\n    \"zip_code\": \"categorical\",\n    \"county\": \"categorical\",\n}\n\nCOLUMN_TRANSFORMATIONS = [\n    {\"categorical\": {\"column_name\": \"Type\"}},\n    {\"numeric\": {\"column_name\": \"Age\"}},\n    {\"categorical\": {\"column_name\": \"Breed1\"}},\n    {\"categorical\": {\"column_name\": \"Color1\"}},\n    {\"categorical\": {\"column_name\": \"Color2\"}},\n    {\"categorical\": {\"column_name\": \"MaturitySize\"}},\n    {\"categorical\": {\"column_name\": \"FurLength\"}},\n    {\"categorical\": {\"column_name\": \"Vaccinated\"}},\n    {\"categorical\": {\"column_name\": \"Sterilized\"}},\n    {\"categorical\": {\"column_name\": \"Health\"}},\n    {\"numeric\": {\"column_name\": \"Fee\"}},\n    {\"numeric\": {\"column_name\": \"PhotoAmt\"}},\n]\n\nMODEL_ID = os.environ.get(\"MODEL_ID\", \"test-model-id\")\nMODEL_ARTIFACT_URI = os.environ.get(\"MODEL_ARTIFACT_URI\", \"path_to_folder_with_model_artifacts\")\nMODEL_NAME = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\"\nJOB_DISPLAY_NAME = f\"temp_create_batch_prediction_job_test_{uuid4()}\"\nBIGQUERY_SOURCE = f\"bq://{PROJECT_ID}.test_iowa_liquor_sales_forecasting_us.2021_sales_predict\"\nGCS_DESTINATION_PREFIX = \"gs://test-vertex-ai-bucket-us/output\"\nMODEL_PARAMETERS: dict | None = {}\n\nENDPOINT_CONF = {\n    \"display_name\": f\"endpoint_test_{uuid4()}\",\n}\nDEPLOYED_MODEL = {\n    # format: 'projects/{project}/locations/{location}/models/{model}'\n    \"model\": f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\",\n    \"display_name\": f\"temp_endpoint_test_{uuid4()}\",\n    \"dedicated_resources\": {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-2\",\n            \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n            \"accelerator_count\": 1,\n        },\n        \"min_replica_count\": 1,\n        \"max_replica_count\": 1,\n    },\n}\n\nMODEL_OUTPUT_CONFIG = {\n    \"artifact_destination\": {\n        \"output_uri_prefix\": STAGING_BUCKET,\n    },\n    \"export_format_id\": \"custom-trained\",\n}\nMODEL_OBJ = {\n    \"display_name\": f\"model-{str(uuid4())}\",\n    \"artifact_uri\": MODEL_ARTIFACT_URI,\n    \"container_spec\": {\n        \"image_uri\": MODEL_SERVING_CONTAINER_URI,\n        \"command\": [],\n        \"args\": [],\n        \"env\": [],\n        \"ports\": [],\n        \"predict_route\": \"\",\n        \"health_route\": \"\",\n    },\n}\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_custom_jobs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as custom_jobs_dag:\n    # [START how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n    create_custom_container_training_job = CreateCustomContainerTrainingJobOperator(\n        task_id=\"custom_container_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-container-{DISPLAY_NAME}\",\n        container_uri=CUSTOM_CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        command=[\"python3\", \"task.py\"],\n        model_display_name=f\"container-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_container_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n    create_custom_python_package_training_job = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"python_package_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-py-package-{DISPLAY_NAME}\",\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        container_uri=CONTAINER_URI,\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        model_display_name=f\"py-package-housing-model-{DISPLAY_NAME}\",\n        replica_count=REPLICA_COUNT,\n        machine_type=MACHINE_TYPE,\n        accelerator_type=ACCELERATOR_TYPE,\n        accelerator_count=ACCELERATOR_COUNT,\n        training_fraction_split=TRAINING_FRACTION_SPLIT,\n        validation_fraction_split=VALIDATION_FRACTION_SPLIT,\n        test_fraction_split=TEST_FRACTION_SPLIT,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_python_package_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_custom_training_job_operator]\n    create_custom_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"custom_task\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"train-housing-custom-{DISPLAY_NAME}\",\n        script_path=LOCAL_TRAINING_SCRIPT_PATH,\n        container_uri=CONTAINER_URI,\n        requirements=[\"gcsfs==0.7.1\"],\n        model_serving_container_image_uri=MODEL_SERVING_CONTAINER_URI,\n        # run params\n        dataset_id=DATASET_ID,\n        replica_count=1,\n        model_display_name=f\"custom-housing-model-{DISPLAY_NAME}\",\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n    delete_custom_training_job = DeleteCustomTrainingJobOperator(\n        task_id=\"delete_custom_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        custom_job_id=CUSTOM_JOB_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_custom_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_custom_training_job_operator]\n    list_custom_training_job = ListCustomTrainingJobOperator(\n        task_id=\"list_custom_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_custom_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_dataset\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dataset_dag:\n    # [START how_to_cloud_vertex_ai_create_dataset_operator]\n    create_image_dataset_job = CreateDatasetOperator(\n        task_id=\"image_dataset\",\n        dataset=IMAGE_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_tabular_dataset_job = CreateDatasetOperator(\n        task_id=\"tabular_dataset\",\n        dataset=TABULAR_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_text_dataset_job = CreateDatasetOperator(\n        task_id=\"text_dataset\",\n        dataset=TEXT_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_video_dataset_job = CreateDatasetOperator(\n        task_id=\"video_dataset\",\n        dataset=VIDEO_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    create_time_series_dataset_job = CreateDatasetOperator(\n        task_id=\"time_series_dataset\",\n        dataset=TIME_SERIES_DATASET,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_dataset_operator]\n    delete_dataset_job = DeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        dataset_id=create_text_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_get_dataset_operator]\n    get_dataset = GetDatasetOperator(\n        task_id=\"get_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_tabular_dataset_job.output[\"dataset_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_export_data_operator]\n    export_data_job = ExportDataOperator(\n        task_id=\"export_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        export_config=TEST_EXPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_data_operator]\n\n    # [START how_to_cloud_vertex_ai_import_data_operator]\n    import_data_job = ImportDataOperator(\n        task_id=\"import_data\",\n        dataset_id=create_image_dataset_job.output[\"dataset_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n        import_configs=TEST_IMPORT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_import_data_operator]\n\n    # [START how_to_cloud_vertex_ai_list_dataset_operator]\n    list_dataset_job = ListDatasetsOperator(\n        task_id=\"list_dataset\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_dataset_operator]\n\n    # [START how_to_cloud_vertex_ai_update_dataset_operator]\n    update_dataset_job = UpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        dataset_id=create_video_dataset_job.output[\"dataset_id\"],\n        dataset=DATASET_TO_UPDATE,\n        update_mask=TEST_UPDATE_MASK,\n    )\n    # [END how_to_cloud_vertex_ai_update_dataset_operator]\n\n    create_time_series_dataset_job\n    create_text_dataset_job >> delete_dataset_job\n    create_tabular_dataset_job >> get_dataset\n    create_image_dataset_job >> import_data_job >> export_data_job\n    create_video_dataset_job >> update_dataset_job\n    list_dataset_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_auto_ml\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as auto_ml_dag:\n    # [START how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n    create_auto_ml_forecasting_training_job = CreateAutoMLForecastingTrainingJobOperator(\n        task_id=\"auto_ml_forecasting_task\",\n        display_name=f\"auto-ml-forecasting-{DISPLAY_NAME}\",\n        optimization_objective=\"minimize-rmse\",\n        column_specs=COLUMN_SPECS,\n        # run params\n        dataset_id=DATASET_ID,\n        target_column=TEST_TARGET_COLUMN,\n        time_column=TEST_TIME_COLUMN,\n        time_series_identifier_column=TEST_TIME_SERIES_IDENTIFIER_COLUMN,\n        available_at_forecast_columns=[TEST_TIME_COLUMN],\n        unavailable_at_forecast_columns=[TEST_TARGET_COLUMN],\n        time_series_attribute_columns=[\"city\", \"zip_code\", \"county\"],\n        forecast_horizon=30,\n        context_window=30,\n        data_granularity_unit=\"day\",\n        data_granularity_count=1,\n        weight_column=None,\n        budget_milli_node_hours=1000,\n        model_display_name=f\"auto-ml-forecasting-model-{DISPLAY_NAME}\",\n        predefined_split_column_name=None,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_forecasting_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n    create_auto_ml_image_training_job = CreateAutoMLImageTrainingJobOperator(\n        task_id=\"auto_ml_image_task\",\n        display_name=f\"auto-ml-image-{DISPLAY_NAME}\",\n        dataset_id=DATASET_ID,\n        prediction_type=\"classification\",\n        multi_label=False,\n        model_type=\"CLOUD\",\n        training_fraction_split=0.6,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.2,\n        budget_milli_node_hours=8000,\n        model_display_name=f\"auto-ml-image-model-{DISPLAY_NAME}\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_image_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n    create_auto_ml_tabular_training_job = CreateAutoMLTabularTrainingJobOperator(\n        task_id=\"auto_ml_tabular_task\",\n        display_name=f\"auto-ml-tabular-{DISPLAY_NAME}\",\n        optimization_prediction_type=\"classification\",\n        column_transformations=COLUMN_TRANSFORMATIONS,\n        dataset_id=DATASET_ID,\n        target_column=\"Adopted\",\n        training_fraction_split=0.8,\n        validation_fraction_split=0.1,\n        test_fraction_split=0.1,\n        model_display_name=\"adopted-prediction-model\",\n        disable_early_stopping=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_tabular_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n    create_auto_ml_text_training_job = CreateAutoMLTextTrainingJobOperator(\n        task_id=\"auto_ml_text_task\",\n        display_name=f\"auto-ml-text-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        multi_label=False,\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-text-model-{DISPLAY_NAME}\",\n        training_fraction_split=0.7,\n        validation_fraction_split=0.2,\n        test_fraction_split=0.1,\n        sync=True,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_text_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n    create_auto_ml_video_training_job = CreateAutoMLVideoTrainingJobOperator(\n        task_id=\"auto_ml_video_task\",\n        display_name=f\"auto-ml-video-{DISPLAY_NAME}\",\n        prediction_type=\"classification\",\n        model_type=\"CLOUD\",\n        dataset_id=DATASET_ID,\n        model_display_name=f\"auto-ml-video-model-{DISPLAY_NAME}\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_auto_ml_video_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n    delete_auto_ml_training_job = DeleteAutoMLTrainingJobOperator(\n        task_id=\"delete_auto_ml_training_job\",\n        training_pipeline_id=TRAINING_PIPELINE_ID,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_auto_ml_training_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n    list_auto_ml_training_job = ListAutoMLTrainingJobOperator(\n        task_id=\"list_auto_ml_training_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_auto_ml_training_job_operator]\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_batch_prediction_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as batch_prediction_job_dag:\n    # [START how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n    create_batch_prediction_job = CreateBatchPredictionJobOperator(\n        task_id=\"create_batch_prediction_job\",\n        job_display_name=JOB_DISPLAY_NAME,\n        model_name=MODEL_NAME,\n        predictions_format=\"csv\",\n        bigquery_source=BIGQUERY_SOURCE,\n        gcs_destination_prefix=GCS_DESTINATION_PREFIX,\n        model_parameters=MODEL_PARAMETERS,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n    list_batch_prediction_job = ListBatchPredictionJobsOperator(\n        task_id=\"list_batch_prediction_jobs\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_batch_prediction_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n    delete_batch_prediction_job = DeleteBatchPredictionJobOperator(\n        task_id=\"delete_batch_prediction_job\",\n        batch_prediction_job_id=create_batch_prediction_job.output[\"batch_prediction_job_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_batch_prediction_job_operator]\n\n    create_batch_prediction_job >> delete_batch_prediction_job\n    list_batch_prediction_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_endpoint\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as endpoint_dag:\n    # [START how_to_cloud_vertex_ai_create_endpoint_operator]\n    create_endpoint = CreateEndpointOperator(\n        task_id=\"create_endpoint\",\n        endpoint=ENDPOINT_CONF,\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_create_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_endpoint_operator]\n    delete_endpoint = DeleteEndpointOperator(\n        task_id=\"delete_endpoint\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_delete_endpoint_operator]\n\n    # [START how_to_cloud_vertex_ai_list_endpoints_operator]\n    list_endpoints = ListEndpointsOperator(\n        task_id=\"list_endpoints\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_endpoints_operator]\n\n    # [START how_to_cloud_vertex_ai_deploy_model_operator]\n    deploy_model = DeployModelOperator(\n        task_id=\"deploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model=DEPLOYED_MODEL,\n        traffic_split={\"0\": 100},\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_deploy_model_operator]\n\n    # [START how_to_cloud_vertex_ai_undeploy_model_operator]\n    undeploy_model = UndeployModelOperator(\n        task_id=\"undeploy_model\",\n        endpoint_id=create_endpoint.output[\"endpoint_id\"],\n        deployed_model_id=deploy_model.output[\"deployed_model_id\"],\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_undeploy_model_operator]\n\n    create_endpoint >> deploy_model >> undeploy_model >> delete_endpoint\n    list_endpoints\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_hyperparameter_tuning_job\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as hyperparameter_tuning_job_dag:\n    # [START how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n    create_hyperparameter_tuning_job = CreateHyperparameterTuningJobOperator(\n        task_id=\"create_hyperparameter_tuning_job\",\n        staging_bucket=STAGING_BUCKET,\n        display_name=f\"horses-humans-hyptertune-{DISPLAY_NAME}\",\n        worker_pool_specs=[\n            {\n                \"machine_spec\": {\n                    \"machine_type\": MACHINE_TYPE,\n                    \"accelerator_type\": ACCELERATOR_TYPE,\n                    \"accelerator_count\": ACCELERATOR_COUNT,\n                },\n                \"replica_count\": REPLICA_COUNT,\n                \"container_spec\": {\n                    \"image_uri\": f\"gcr.io/{PROJECT_ID}/horse-human:hypertune\",\n                },\n            }\n        ],\n        sync=False,\n        region=REGION,\n        project_id=PROJECT_ID,\n        parameter_spec={\n            \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n                min=0.01, max=1, scale=\"log\"\n            ),\n            \"momentum\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale=\"linear\"),\n            \"num_neurons\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n                values=[64, 128, 512], scale=\"linear\"\n            ),\n        },\n        metric_spec={\n            \"accuracy\": \"maximize\",\n        },\n        max_trial_count=15,\n        parallel_trial_count=3,\n    )\n    # [END how_to_cloud_vertex_ai_create_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n    get_hyperparameter_tuning_job = GetHyperparameterTuningJobOperator(\n        task_id=\"get_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_get_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n    delete_hyperparameter_tuning_job = DeleteHyperparameterTuningJobOperator(\n        task_id=\"delete_hyperparameter_tuning_job\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        hyperparameter_tuning_job_id=create_hyperparameter_tuning_job.output[\"hyperparameter_tuning_job_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_hyperparameter_tuning_job_operator]\n\n    # [START how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n    list_hyperparameter_tuning_job = ListHyperparameterTuningJobOperator(\n        task_id=\"list_hyperparameter_tuning_job\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_hyperparameter_tuning_job_operator]\n\n    create_hyperparameter_tuning_job >> get_hyperparameter_tuning_job >> delete_hyperparameter_tuning_job\n    list_hyperparameter_tuning_job\n\nwith models.DAG(\n    \"example_gcp_vertex_ai_model_service\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as model_service_dag:\n    # [START how_to_cloud_vertex_ai_upload_model_operator]\n    upload_model = UploadModelOperator(\n        task_id=\"upload_model\",\n        region=REGION,\n        project_id=PROJECT_ID,\n        model=MODEL_OBJ,\n    )\n    # [END how_to_cloud_vertex_ai_upload_model_operator]\n\n    # [START how_to_cloud_vertex_ai_export_model_operator]\n    export_model = ExportModelOperator(\n        task_id=\"export_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n        output_config=MODEL_OUTPUT_CONFIG,\n    )\n    # [END how_to_cloud_vertex_ai_export_model_operator]\n\n    # [START how_to_cloud_vertex_ai_delete_model_operator]\n    delete_model = DeleteModelOperator(\n        task_id=\"delete_model\",\n        project_id=PROJECT_ID,\n        region=REGION,\n        model_id=upload_model.output[\"model_id\"],\n    )\n    # [END how_to_cloud_vertex_ai_delete_model_operator]\n\n    # [START how_to_cloud_vertex_ai_list_models_operator]\n    list_models = ListModelsOperator(\n        task_id=\"list_models\",\n        region=REGION,\n        project_id=PROJECT_ID,\n    )\n    # [END how_to_cloud_vertex_ai_list_models_operator]\n\n    upload_model >> export_model >> delete_model\n    list_models", "metadata": {"file_name": "example_vertex_ai.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operator types, cloud service integration, environment configuration management, and demonstrates intermediate Airflow workflow design patterns", "key_concepts": ["Environment Configuration", "Dynamic Job Creation", "ML Infrastructure Automation", "Operator Flexibility", "Cloud Service Parameterization"], "learning_objectives": ["Learn dynamic configuration strategies", "Understand ML job parameterization", "Master cloud service operator usage", "Explore infrastructure-as-code principles"]}}
{"instruction": "Build an Oracle database interaction DAG that demonstrates executing a simple SQL query and calling stored procedures using different parameter passing techniques (list and dictionary).", "input": {"airflow_version": "2.7.2", "operators": ["OracleStoredProcedureOperator", "SQLExecuteQueryOperator"], "line_count": 45, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nwith DAG(\n    max_active_runs=1,\n    max_active_tasks=3,\n    catchup=False,\n    start_date=datetime(2023, 1, 1),\n    dag_id=\"example_oracle\",\n) as dag:\n\n    # [START howto_oracle_operator]\n\n    opr_sql = SQLExecuteQueryOperator(\n        task_id=\"task_sql\", conn_id=\"oracle\", sql=\"SELECT 1 FROM DUAL\", autocommit=True\n    )\n\n    # [END howto_oracle_operator]\n\n    # [START howto_oracle_stored_procedure_operator_with_list_inout]\n\n    opr_stored_procedure_with_list_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_list_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters=[3, int],\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_list_inout]\n\n    # [START howto_oracle_stored_procedure_operator_with_dict_inout]\n\n    opr_stored_procedure_with_dict_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_dict_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters={\"val_in\": 3, \"val_out\": int},\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_dict_inout]", "metadata": {"file_name": "example_oracle.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple Oracle-specific operators, involves database connection, demonstrates two different parameter passing methods for stored procedures", "key_concepts": ["Oracle database integration", "SQLExecuteQueryOperator", "OracleStoredProcedureOperator", "parameter passing techniques", "database connection management"], "learning_objectives": ["Understand Oracle database operators in Airflow", "Learn different methods of passing parameters to stored procedures", "Practice configuring database connection parameters"]}}
{"instruction": "Create a DAG that showcases flexible database interaction patterns, focusing on how to dynamically pass input parameters to database procedures using different data structures.", "input": {"airflow_version": "2.7.2", "operators": ["OracleStoredProcedureOperator", "SQLExecuteQueryOperator"], "line_count": 45, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.providers.oracle.operators.oracle import OracleStoredProcedureOperator\n\nwith DAG(\n    max_active_runs=1,\n    max_active_tasks=3,\n    catchup=False,\n    start_date=datetime(2023, 1, 1),\n    dag_id=\"example_oracle\",\n) as dag:\n\n    # [START howto_oracle_operator]\n\n    opr_sql = SQLExecuteQueryOperator(\n        task_id=\"task_sql\", conn_id=\"oracle\", sql=\"SELECT 1 FROM DUAL\", autocommit=True\n    )\n\n    # [END howto_oracle_operator]\n\n    # [START howto_oracle_stored_procedure_operator_with_list_inout]\n\n    opr_stored_procedure_with_list_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_list_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters=[3, int],\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_list_inout]\n\n    # [START howto_oracle_stored_procedure_operator_with_dict_inout]\n\n    opr_stored_procedure_with_dict_input_output = OracleStoredProcedureOperator(\n        task_id=\"opr_stored_procedure_with_dict_input_output\",\n        oracle_conn_id=\"oracle\",\n        procedure=\"TEST_PROCEDURE\",\n        parameters={\"val_in\": 3, \"val_out\": int},\n    )\n\n    # [END howto_oracle_stored_procedure_operator_with_dict_inout]", "metadata": {"file_name": "example_oracle.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates advanced parameter handling, uses multiple input methods (list and dictionary), requires understanding of type conversion", "key_concepts": ["Dynamic parameter passing", "Stored procedure invocation", "Type handling in database operators", "Connection configuration", "Flexible data interaction"], "learning_objectives": ["Explore flexible input methods for database operations", "Understand type conversion in database interactions", "Learn how to configure database connections dynamically"]}}
{"instruction": "Build an Airflow DAG that monitors a specific document in Azure Cosmos DB using a document sensor, demonstrating how to create a simple monitoring workflow for cloud-based document tracking.", "input": {"airflow_version": "2.7.2", "operators": ["AzureCosmosDocumentSensor"], "line_count": 36, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that senses document in Azure Cosmos DB.\n\nThis DAG relies on the following OS environment variables\n\n* DATABASE_NAME - Target CosmosDB database_name.\n* COLLECTION_NAME - Target CosmosDB collection_name.\n* DOCUMENT_ID - The ID of the target document.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.cosmos import AzureCosmosDocumentSensor\n\nDATABASE_NAME = os.environ.get(\"DATABASE_NAME\", \"example-database-name\")\nCOLLECTION_NAME = os.environ.get(\"COLLECTION_NAME\", \"example-collection-name\")\nDOCUMENT_ID = os.environ.get(\"DOCUMENT_ID\", \"example-document-id\")\n\n\nwith DAG(\n    \"example_cosmos_document_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cosmos_document_sensor]\n    azure_wasb_sensor = AzureCosmosDocumentSensor(\n        database_name=DATABASE_NAME,\n        collection_name=COLLECTION_NAME,\n        document_id=DOCUMENT_ID,\n        task_id=\"cosmos_document_sensor\",\n    )\n    # [END cosmos_document_sensor]", "metadata": {"file_name": "example_cosmos_document_sensor.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses Azure-specific sensor, involves cloud service integration, requires environment variable configuration, and demonstrates external service monitoring", "key_concepts": ["Airflow Sensors", "Cloud Service Integration", "Environment Variable Configuration", "Azure Cosmos DB Monitoring"], "learning_objectives": ["Understand how to create a document monitoring DAG", "Learn to use cloud service sensors in Airflow", "Practice configuring environment-based parameters"]}}
{"instruction": "Design a robust document tracking system using Airflow's Azure Cosmos DB sensor, focusing on parameterizing your workflow through flexible environment variable configurations.", "input": {"airflow_version": "2.7.2", "operators": ["AzureCosmosDocumentSensor"], "line_count": 36, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that senses document in Azure Cosmos DB.\n\nThis DAG relies on the following OS environment variables\n\n* DATABASE_NAME - Target CosmosDB database_name.\n* COLLECTION_NAME - Target CosmosDB collection_name.\n* DOCUMENT_ID - The ID of the target document.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow.models import DAG\nfrom airflow.providers.microsoft.azure.sensors.cosmos import AzureCosmosDocumentSensor\n\nDATABASE_NAME = os.environ.get(\"DATABASE_NAME\", \"example-database-name\")\nCOLLECTION_NAME = os.environ.get(\"COLLECTION_NAME\", \"example-collection-name\")\nDOCUMENT_ID = os.environ.get(\"DOCUMENT_ID\", \"example-document-id\")\n\n\nwith DAG(\n    \"example_cosmos_document_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cosmos_document_sensor]\n    azure_wasb_sensor = AzureCosmosDocumentSensor(\n        database_name=DATABASE_NAME,\n        collection_name=COLLECTION_NAME,\n        document_id=DOCUMENT_ID,\n        task_id=\"cosmos_document_sensor\",\n    )\n    # [END cosmos_document_sensor]", "metadata": {"file_name": "example_cosmos_document_sensor.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves dynamic configuration through environment variables, uses specialized cloud sensor, requires understanding of external service integration", "key_concepts": ["Parameterized DAGs", "Environment Variable Management", "Cloud Service Sensors", "Flexible Workflow Design"], "learning_objectives": ["Master dynamic DAG configuration techniques", "Understand sensor-based workflow monitoring", "Learn best practices for cloud service integration"]}}
{"instruction": "Create a Google Cloud Tasks monitoring DAG that demonstrates how to sense and validate the emptiness of a specific task queue using Airflow's TaskQueueEmptySensor, focusing on cloud task queue management and sensor configuration.", "input": {"airflow_version": "2.7.2", "operators": ["TaskQueueEmptySensor"], "line_count": 36, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that sense a cloud task queue being empty.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project where the Compute Engine instance exists.\n* GCP_ZONE - Google Cloud zone where the cloud task queue exists.\n* QUEUE_NAME - Name of the cloud task queue.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.sensors.tasks import TaskQueueEmptySensor\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_ZONE = os.environ.get(\"GCE_ZONE\", \"europe-west1-b\")\nQUEUE_NAME = os.environ.get(\"GCP_QUEUE_NAME\", \"testqueue\")\n\n\nwith models.DAG(\n    \"example_gcp_cloud_tasks_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cloud_tasks_empty_sensor]\n    gcp_cloud_tasks_sensor = TaskQueueEmptySensor(\n        project_id=GCP_PROJECT_ID,\n        location=GCP_ZONE,\n        task_id=\"gcp_sense_cloud_tasks_empty\",\n        queue_name=QUEUE_NAME,\n    )\n    # [END cloud_tasks_empty_sensor]", "metadata": {"file_name": "example_cloud_task.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud-specific sensor, requires environment variable configuration, involves external GCP service integration with moderate configuration complexity", "key_concepts": ["Cloud Tasks", "Sensors", "Environment Variables", "GCP Integration", "Task Queue Monitoring"], "learning_objectives": ["Understand TaskQueueEmptySensor configuration", "Learn cloud service integration in Airflow", "Practice environment-based configuration"]}}
{"instruction": "Design an operational workflow that demonstrates dynamic configuration of cloud task queue monitoring using environment variables, emphasizing flexible and configurable Airflow DAG development across different cloud environments.", "input": {"airflow_version": "2.7.2", "operators": ["TaskQueueEmptySensor"], "line_count": 36, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that sense a cloud task queue being empty.\n\nThis DAG relies on the following OS environment variables\n\n* GCP_PROJECT_ID - Google Cloud project where the Compute Engine instance exists.\n* GCP_ZONE - Google Cloud zone where the cloud task queue exists.\n* QUEUE_NAME - Name of the cloud task queue.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.sensors.tasks import TaskQueueEmptySensor\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCP_ZONE = os.environ.get(\"GCE_ZONE\", \"europe-west1-b\")\nQUEUE_NAME = os.environ.get(\"GCP_QUEUE_NAME\", \"testqueue\")\n\n\nwith models.DAG(\n    \"example_gcp_cloud_tasks_sensor\",\n    start_date=datetime(2022, 8, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START cloud_tasks_empty_sensor]\n    gcp_cloud_tasks_sensor = TaskQueueEmptySensor(\n        project_id=GCP_PROJECT_ID,\n        location=GCP_ZONE,\n        task_id=\"gcp_sense_cloud_tasks_empty\",\n        queue_name=QUEUE_NAME,\n    )\n    # [END cloud_tasks_empty_sensor]", "metadata": {"file_name": "example_cloud_task.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves parameterized configuration, external service interaction, and demonstrates flexible DAG design with environment-driven parameters", "key_concepts": ["Parameterization", "Environment Configuration", "Cloud Service Abstraction", "Sensor Patterns", "Dynamic DAG Design"], "learning_objectives": ["Master environment-based DAG configuration", "Understand sensor abstraction techniques", "Learn cloud-agnostic workflow design"]}}
{"instruction": "Build a data pipeline that transfers data from Presto to Google Cloud Storage (GCS) and creates an external BigQuery table, demonstrating cross-platform data movement and transformation.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateExternalTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "PrestoToGCSOperator"], "line_count": 177, "is_multifile": false}, "output": "\"\"\"\nExample DAG using PrestoToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.presto_to_gcs import PrestoToGCSOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCP_PRESTO_TO_GCS_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nDATASET_NAME = os.environ.get(\"GCP_PRESTO_TO_GCS_DATASET_NAME\", \"test_presto_to_gcs_dataset\")\n\nSOURCE_MULTIPLE_TYPES = \"memory.default.test_multiple_types\"\nSOURCE_CUSTOMER_TABLE = \"tpch.sf1.customer\"\n\n\ndef safe_name(s: str) -> str:\n    \"\"\"\n    Remove invalid characters for filename\n    \"\"\"\n    return re.sub(\"[^0-9a-zA-Z_]+\", \"_\", s)\n\n\nwith models.DAG(\n    dag_id=\"example_presto_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create-dataset\", dataset_id=DATASET_NAME)\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    # [START howto_operator_presto_to_gcs_basic]\n    presto_to_gcs_basic = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_basic\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n    )\n    # [END howto_operator_presto_to_gcs_basic]\n\n    # [START howto_operator_presto_to_gcs_multiple_types]\n    presto_to_gcs_multiple_types = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_multiple_types\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_multiple_types]\n\n    # [START howto_operator_create_external_table_multiple_types]\n    create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_multiple_types\",\n        bucket=GCS_BUCKET,\n        source_objects=[f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.*.json\"],\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_MULTIPLE_TYPES)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        schema_object=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n    )\n    # [END howto_operator_create_external_table_multiple_types]\n\n    read_data_from_gcs_multiple_types = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_multiple_types\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_MULTIPLE_TYPES)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_operator_presto_to_gcs_many_chunks]\n    presto_to_gcs_many_chunks = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_many_chunks\",\n        sql=f\"select * from {SOURCE_CUSTOMER_TABLE}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n        approx_max_file_size_bytes=10_000_000,\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_many_chunks]\n\n    create_external_table_many_chunks = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_many_chunks\",\n        bucket=GCS_BUCKET,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_CUSTOMER_TABLE)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        source_objects=[f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.*.json\"],\n        schema_object=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n    )\n\n    # [START howto_operator_read_data_from_gcs_many_chunks]\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_CUSTOMER_TABLE)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_read_data_from_gcs_many_chunks]\n\n    # [START howto_operator_presto_to_gcs_csv]\n    presto_to_gcs_csv = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_csv\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.csv\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        export_format=\"csv\",\n    )\n    # [END howto_operator_presto_to_gcs_csv]\n\n    create_dataset >> presto_to_gcs_basic\n    create_dataset >> presto_to_gcs_multiple_types\n    create_dataset >> presto_to_gcs_many_chunks\n    create_dataset >> presto_to_gcs_csv\n\n    presto_to_gcs_multiple_types >> create_external_table_multiple_types >> read_data_from_gcs_multiple_types\n    presto_to_gcs_many_chunks >> create_external_table_many_chunks >> read_data_from_gcs_many_chunks\n\n    presto_to_gcs_basic >> delete_dataset\n    presto_to_gcs_csv >> delete_dataset\n    read_data_from_gcs_multiple_types >> delete_dataset\n    read_data_from_gcs_many_chunks >> delete_dataset", "metadata": {"file_name": "example_presto_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple cloud operators, involves data transfer between Presto and GCS, creates external BigQuery table with schema configuration, requires environment variables", "key_concepts": ["Cloud Data Transfer", "PrestoToGCSOperator", "BigQuery External Tables", "Data Orchestration", "Cloud Service Integration"], "learning_objectives": ["Understand cross-platform data movement", "Learn cloud service operator configurations", "Practice dynamic file naming and schema handling", "Explore external table creation strategies"]}}
{"instruction": "Design a robust data extraction and validation workflow that demonstrates safe data transfer, schema management, and basic data quality checks using cloud-native Airflow operators.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateExternalTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "PrestoToGCSOperator"], "line_count": 177, "is_multifile": false}, "output": "\"\"\"\nExample DAG using PrestoToGCSOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateExternalTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.transfers.presto_to_gcs import PrestoToGCSOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCP_PRESTO_TO_GCS_BUCKET_NAME\", \"INVALID BUCKET NAME\")\nDATASET_NAME = os.environ.get(\"GCP_PRESTO_TO_GCS_DATASET_NAME\", \"test_presto_to_gcs_dataset\")\n\nSOURCE_MULTIPLE_TYPES = \"memory.default.test_multiple_types\"\nSOURCE_CUSTOMER_TABLE = \"tpch.sf1.customer\"\n\n\ndef safe_name(s: str) -> str:\n    \"\"\"\n    Remove invalid characters for filename\n    \"\"\"\n    return re.sub(\"[^0-9a-zA-Z_]+\", \"_\", s)\n\n\nwith models.DAG(\n    dag_id=\"example_presto_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(task_id=\"create-dataset\", dataset_id=DATASET_NAME)\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\", dataset_id=DATASET_NAME, delete_contents=True\n    )\n\n    # [START howto_operator_presto_to_gcs_basic]\n    presto_to_gcs_basic = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_basic\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n    )\n    # [END howto_operator_presto_to_gcs_basic]\n\n    # [START howto_operator_presto_to_gcs_multiple_types]\n    presto_to_gcs_multiple_types = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_multiple_types\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_multiple_types]\n\n    # [START howto_operator_create_external_table_multiple_types]\n    create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_multiple_types\",\n        bucket=GCS_BUCKET,\n        source_objects=[f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.*.json\"],\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_MULTIPLE_TYPES)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        schema_object=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n    )\n    # [END howto_operator_create_external_table_multiple_types]\n\n    read_data_from_gcs_multiple_types = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_multiple_types\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_MULTIPLE_TYPES)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    # [START howto_operator_presto_to_gcs_many_chunks]\n    presto_to_gcs_many_chunks = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_many_chunks\",\n        sql=f\"select * from {SOURCE_CUSTOMER_TABLE}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.{{}}.json\",\n        schema_filename=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n        approx_max_file_size_bytes=10_000_000,\n        gzip=False,\n    )\n    # [END howto_operator_presto_to_gcs_many_chunks]\n\n    create_external_table_many_chunks = BigQueryCreateExternalTableOperator(\n        task_id=\"create_external_table_many_chunks\",\n        bucket=GCS_BUCKET,\n        table_resource={\n            \"tableReference\": {\n                \"projectId\": GCP_PROJECT_ID,\n                \"datasetId\": DATASET_NAME,\n                \"tableId\": f\"{safe_name(SOURCE_CUSTOMER_TABLE)}\",\n            },\n            \"schema\": {\n                \"fields\": [\n                    {\"name\": \"name\", \"type\": \"STRING\"},\n                    {\"name\": \"post_abbr\", \"type\": \"STRING\"},\n                ]\n            },\n            \"externalDataConfiguration\": {\n                \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n                \"compression\": \"NONE\",\n                \"csvOptions\": {\"skipLeadingRows\": 1},\n            },\n        },\n        source_objects=[f\"{safe_name(SOURCE_CUSTOMER_TABLE)}.*.json\"],\n        schema_object=f\"{safe_name(SOURCE_CUSTOMER_TABLE)}-schema.json\",\n    )\n\n    # [START howto_operator_read_data_from_gcs_many_chunks]\n    read_data_from_gcs_many_chunks = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs_many_chunks\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.\"\n                f\"{safe_name(SOURCE_CUSTOMER_TABLE)}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    # [END howto_operator_read_data_from_gcs_many_chunks]\n\n    # [START howto_operator_presto_to_gcs_csv]\n    presto_to_gcs_csv = PrestoToGCSOperator(\n        task_id=\"presto_to_gcs_csv\",\n        sql=f\"select * from {SOURCE_MULTIPLE_TYPES}\",\n        bucket=GCS_BUCKET,\n        filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}.{{}}.csv\",\n        schema_filename=f\"{safe_name(SOURCE_MULTIPLE_TYPES)}-schema.json\",\n        export_format=\"csv\",\n    )\n    # [END howto_operator_presto_to_gcs_csv]\n\n    create_dataset >> presto_to_gcs_basic\n    create_dataset >> presto_to_gcs_multiple_types\n    create_dataset >> presto_to_gcs_many_chunks\n    create_dataset >> presto_to_gcs_csv\n\n    presto_to_gcs_multiple_types >> create_external_table_multiple_types >> read_data_from_gcs_multiple_types\n    presto_to_gcs_many_chunks >> create_external_table_many_chunks >> read_data_from_gcs_many_chunks\n\n    presto_to_gcs_basic >> delete_dataset\n    presto_to_gcs_csv >> delete_dataset\n    read_data_from_gcs_multiple_types >> delete_dataset\n    read_data_from_gcs_many_chunks >> delete_dataset", "metadata": {"file_name": "example_presto_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements multiple data validation steps, uses safe naming functions, handles different data types, includes error handling and dataset management operators", "key_concepts": ["Data Validation", "Safe Naming Conventions", "Schema Management", "Error Handling", "Cloud Data Pipelines"], "learning_objectives": ["Learn safe data transfer techniques", "Understand schema preservation", "Practice defensive programming in data pipelines", "Explore cloud operator error mitigation"]}}
{"instruction": "Build an Airflow DAG that demonstrates a complete data pipeline for extracting Salesforce lead data, transferring it to Google Cloud Storage, and then loading it into BigQuery for analysis.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateEmptyTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToBigQueryOperator", "SalesforceToGcsOperator"], "line_count": 112, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use SalesforceToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.salesforce_to_gcs import SalesforceToGcsOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow-salesforce-bucket\")\nDATASET_NAME = os.environ.get(\"SALESFORCE_DATASET_NAME\", \"salesforce_test_dataset\")\nTABLE_NAME = os.environ.get(\"SALESFORCE_TABLE_NAME\", \"salesforce_test_datatable\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"results.csv\")\nQUERY = \"SELECT Id, Name, Company, Phone, Email, CreatedDate, LastModifiedDate, IsDeleted FROM Lead\"\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nSALESFORCE_CONN_ID = os.environ.get(\"SALESFORCE_CONN_ID\", \"salesforce_default\")\n\n\nwith models.DAG(\n    \"example_salesforce_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n        gcp_conn_id=GCS_CONN_ID,\n    )\n\n    # [START howto_operator_salesforce_to_gcs]\n    gcs_upload_task = SalesforceToGcsOperator(\n        query=QUERY,\n        include_deleted=True,\n        bucket_name=GCS_BUCKET,\n        object_name=GCS_OBJ_PATH,\n        salesforce_conn_id=SALESFORCE_CONN_ID,\n        export_format=\"csv\",\n        coerce_to_timestamp=False,\n        record_time_added=False,\n        gcp_conn_id=GCS_CONN_ID,\n        task_id=\"upload_to_gcs\",\n        dag=dag,\n    )\n    # [END howto_operator_salesforce_to_gcs]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=DATASET_NAME, project_id=GCP_PROJECT_ID, gcp_conn_id=GCS_CONN_ID\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"company\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"phone\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"email\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"createddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"lastmodifieddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"isdeleted\", \"type\": \"BOOL\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    create_bucket >> gcs_upload_task >> load_csv\n    create_dataset >> create_table >> load_csv\n    load_csv >> read_data_from_gcs\n    read_data_from_gcs >> delete_bucket\n    read_data_from_gcs >> delete_dataset", "metadata": {"file_name": "example_salesforce_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Multiple cloud operators, external service integration (Salesforce), data transformation between systems, and sequential task dependencies", "key_concepts": ["Cloud Data Transfer", "SalesforceToGcsOperator", "GCSToBigQueryOperator", "Task Dependencies", "Data Pipeline Orchestration"], "learning_objectives": ["Understand inter-service data migration", "Learn Airflow cloud operator usage", "Practice task sequencing", "Explore data extraction and loading patterns"]}}
{"instruction": "Design an Airflow workflow that demonstrates robust cloud resource management, including automated creation, data transfer, and cleanup of Google Cloud Storage and BigQuery resources.", "input": {"airflow_version": "2.7.2", "operators": ["BigQueryCreateEmptyDatasetOperator", "BigQueryCreateEmptyTableOperator", "BigQueryDeleteDatasetOperator", "BigQueryInsertJobOperator", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToBigQueryOperator", "SalesforceToGcsOperator"], "line_count": 112, "is_multifile": false}, "output": "\"\"\"\nExample Airflow DAG that shows how to use SalesforceToGcsOperator.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n    BigQueryCreateEmptyTableOperator,\n    BigQueryDeleteDatasetOperator,\n    BigQueryInsertJobOperator,\n)\nfrom airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator, GCSDeleteBucketOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\nfrom airflow.providers.google.cloud.transfers.salesforce_to_gcs import SalesforceToGcsOperator\n\nGCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"example-project\")\nGCS_BUCKET = os.environ.get(\"GCS_BUCKET\", \"airflow-salesforce-bucket\")\nDATASET_NAME = os.environ.get(\"SALESFORCE_DATASET_NAME\", \"salesforce_test_dataset\")\nTABLE_NAME = os.environ.get(\"SALESFORCE_TABLE_NAME\", \"salesforce_test_datatable\")\nGCS_OBJ_PATH = os.environ.get(\"GCS_OBJ_PATH\", \"results.csv\")\nQUERY = \"SELECT Id, Name, Company, Phone, Email, CreatedDate, LastModifiedDate, IsDeleted FROM Lead\"\nGCS_CONN_ID = os.environ.get(\"GCS_CONN_ID\", \"google_cloud_default\")\nSALESFORCE_CONN_ID = os.environ.get(\"SALESFORCE_CONN_ID\", \"salesforce_default\")\n\n\nwith models.DAG(\n    \"example_salesforce_to_gcs\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    create_bucket = GCSCreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=GCS_BUCKET,\n        project_id=GCP_PROJECT_ID,\n        gcp_conn_id=GCS_CONN_ID,\n    )\n\n    # [START howto_operator_salesforce_to_gcs]\n    gcs_upload_task = SalesforceToGcsOperator(\n        query=QUERY,\n        include_deleted=True,\n        bucket_name=GCS_BUCKET,\n        object_name=GCS_OBJ_PATH,\n        salesforce_conn_id=SALESFORCE_CONN_ID,\n        export_format=\"csv\",\n        coerce_to_timestamp=False,\n        record_time_added=False,\n        gcp_conn_id=GCS_CONN_ID,\n        task_id=\"upload_to_gcs\",\n        dag=dag,\n    )\n    # [END howto_operator_salesforce_to_gcs]\n\n    create_dataset = BigQueryCreateEmptyDatasetOperator(\n        task_id=\"create_dataset\", dataset_id=DATASET_NAME, project_id=GCP_PROJECT_ID, gcp_conn_id=GCS_CONN_ID\n    )\n\n    create_table = BigQueryCreateEmptyTableOperator(\n        task_id=\"create_table\",\n        dataset_id=DATASET_NAME,\n        table_id=TABLE_NAME,\n        schema_fields=[\n            {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"company\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"phone\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"email\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"createddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"lastmodifieddate\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n            {\"name\": \"isdeleted\", \"type\": \"BOOL\", \"mode\": \"NULLABLE\"},\n        ],\n    )\n\n    load_csv = GCSToBigQueryOperator(\n        task_id=\"gcs_to_bq\",\n        bucket=GCS_BUCKET,\n        source_objects=[GCS_OBJ_PATH],\n        destination_project_dataset_table=f\"{DATASET_NAME}.{TABLE_NAME}\",\n        write_disposition=\"WRITE_TRUNCATE\",\n    )\n\n    read_data_from_gcs = BigQueryInsertJobOperator(\n        task_id=\"read_data_from_gcs\",\n        configuration={\n            \"query\": {\n                \"query\": f\"SELECT COUNT(*) FROM `{GCP_PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n    delete_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=GCS_BUCKET,\n    )\n\n    delete_dataset = BigQueryDeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=GCP_PROJECT_ID,\n        dataset_id=DATASET_NAME,\n        delete_contents=True,\n    )\n\n    create_bucket >> gcs_upload_task >> load_csv\n    create_dataset >> create_table >> load_csv\n    load_csv >> read_data_from_gcs\n    read_data_from_gcs >> delete_bucket\n    read_data_from_gcs >> delete_dataset", "metadata": {"file_name": "example_salesforce_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple cloud management operators, resource lifecycle management, and demonstrates infrastructure-as-code principles", "key_concepts": ["Cloud Resource Management", "Idempotent Workflows", "Operator Chaining", "Cleanup Mechanisms", "Environment Provisioning"], "learning_objectives": ["Understand cloud resource lifecycle", "Learn safe resource creation/deletion", "Practice defensive workflow design", "Explore infrastructure automation"]}}
{"instruction": "Create an Alibaba Cloud OSS bucket management workflow that demonstrates creating and then deleting a cloud storage bucket using Airflow operators, focusing on cloud storage orchestration.", "input": {"airflow_version": "2.7.2", "operators": ["OSSCreateBucketOperator", "OSSDeleteBucketOperator"], "line_count": 39, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import OSSCreateBucketOperator, OSSDeleteBucketOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_bucket_dag\"\n# [START howto_operator_oss_bucket]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_bucket = OSSCreateBucketOperator(task_id=\"task1\")\n\n    delete_bucket = OSSDeleteBucketOperator(task_id=\"task2\")\n\n    create_bucket >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_oss_bucket]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_oss_bucket.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with only two sequential operators, minimal code, no complex logic, and straightforward task dependency", "key_concepts": ["DAG configuration", "Cloud storage operators", "Task dependencies", "Alibaba Cloud integration"], "learning_objectives": ["Understand basic Airflow DAG structure", "Learn cloud storage management with operators", "Practice task sequencing"]}}
{"instruction": "Design a cloud infrastructure management DAG that showcases safe bucket lifecycle management, implementing a controlled create-then-delete workflow with proper error handling and logging considerations.", "input": {"airflow_version": "2.7.2", "operators": ["OSSCreateBucketOperator", "OSSDeleteBucketOperator"], "line_count": 39, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import OSSCreateBucketOperator, OSSDeleteBucketOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_bucket_dag\"\n# [START howto_operator_oss_bucket]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_bucket = OSSCreateBucketOperator(task_id=\"task1\")\n\n    delete_bucket = OSSDeleteBucketOperator(task_id=\"task2\")\n\n    create_bucket >> delete_bucket\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_oss_bucket]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_oss_bucket.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves cloud service operator, requires understanding of infrastructure as code principles, and demonstrates basic cloud resource management", "key_concepts": ["Cloud resource lifecycle", "Operator chaining", "Infrastructure automation", "Error handling strategies"], "learning_objectives": ["Understand cloud resource provisioning", "Learn safe infrastructure management patterns", "Practice sequential task execution"]}}
{"instruction": "Build an Alibaba Cloud Object Storage (OSS) workflow that demonstrates file management operations including uploading, downloading, and deleting objects in a sequential pipeline.", "input": {"airflow_version": "2.7.2", "operators": ["OSSDeleteBatchObjectOperator", "OSSDeleteObjectOperator", "OSSDownloadObjectOperator", "OSSUploadObjectOperator"], "line_count": 61, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import (\n    OSSDeleteBatchObjectOperator,\n    OSSDeleteObjectOperator,\n    OSSDownloadObjectOperator,\n    OSSUploadObjectOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_object_dag\"\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_object = OSSUploadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task1\",\n    )\n\n    download_object = OSSDownloadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task2\",\n    )\n\n    delete_object = OSSDeleteObjectOperator(\n        key=\"your oss key\",\n        task_id=\"task3\",\n    )\n\n    delete_batch_object = OSSDeleteBatchObjectOperator(\n        keys=[\"obj1\", \"obj2\", \"obj3\"],\n        task_id=\"task4\",\n    )\n\n    create_object >> download_object >> delete_object >> delete_batch_object\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_oss_object.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple cloud service operators, involves external service integration, and demonstrates sequential task dependencies with OSS interactions", "key_concepts": ["Cloud Storage Operators", "Task Dependencies", "Object Storage Management", "Airflow Provider Integration"], "learning_objectives": ["Understand cloud storage workflow orchestration", "Learn task chaining in Airflow", "Practice using provider-specific operators"]}}
{"instruction": "Design a data migration and cleanup DAG that showcases object lifecycle management using Alibaba Cloud OSS, focusing on safe file transfer and systematic object deletion strategies.", "input": {"airflow_version": "2.7.2", "operators": ["OSSDeleteBatchObjectOperator", "OSSDeleteObjectOperator", "OSSDownloadObjectOperator", "OSSUploadObjectOperator"], "line_count": 61, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.oss import (\n    OSSDeleteBatchObjectOperator,\n    OSSDeleteObjectOperator,\n    OSSDownloadObjectOperator,\n    OSSUploadObjectOperator,\n)\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"oss_object_dag\"\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"bucket_name\": \"your bucket\", \"region\": \"your region\"},\n    max_active_runs=1,\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n\n    create_object = OSSUploadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task1\",\n    )\n\n    download_object = OSSDownloadObjectOperator(\n        file=\"your local file\",\n        key=\"your oss key\",\n        task_id=\"task2\",\n    )\n\n    delete_object = OSSDeleteObjectOperator(\n        key=\"your oss key\",\n        task_id=\"task3\",\n    )\n\n    delete_batch_object = OSSDeleteBatchObjectOperator(\n        keys=[\"obj1\", \"obj2\", \"obj3\"],\n        task_id=\"task4\",\n    )\n\n    create_object >> download_object >> delete_object >> delete_batch_object\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_oss_object.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operators with different storage actions, requires understanding of cloud storage interactions and task sequencing", "key_concepts": ["Data Migration", "Object Lifecycle Management", "Cloud Storage Cleanup", "Operator Chaining"], "learning_objectives": ["Implement safe file transfer workflows", "Practice batch and individual object deletion", "Understand cloud storage orchestration patterns"]}}
{"instruction": "Build an Airflow DAG that demonstrates how to use the AnalyticDBSparkSQLOperator to execute SQL queries against an Alibaba Cloud Analytic Database Spark cluster, showing basic database and table discovery operations.", "input": {"airflow_version": "2.7.2", "operators": ["AnalyticDBSparkSQLOperator"], "line_count": 38, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.analyticdb_spark import AnalyticDBSparkSQLOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"adb_spark_sql_dag\"\n# [START howto_operator_adb_spark_sql]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"cluster_id\": \"your cluster\", \"rg_name\": \"your resource group\", \"region\": \"your region\"},\n    max_active_runs=1,\n    catchup=False,\n) as dag:\n\n    show_databases = AnalyticDBSparkSQLOperator(task_id=\"task1\", sql=\"SHOE DATABASES;\")\n\n    show_tables = AnalyticDBSparkSQLOperator(task_id=\"task2\", sql=\"SHOW TABLES;\")\n\n    show_databases >> show_tables\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_adb_spark_sql]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_adb_spark_sql.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 1, "claude_complexity_level": "easy", "claude_complexity_reasoning": "Simple DAG with two sequential tasks using a single operator type, minimal configuration, and straightforward task dependencies", "key_concepts": ["DAG configuration", "Airflow operators", "SQL query execution", "Cloud service integration"], "learning_objectives": ["Understand Airflow DAG structure", "Learn how to use cloud-specific SQL operators", "Practice task sequencing in Airflow"]}}
{"instruction": "Create an Alibaba Cloud data exploration workflow that demonstrates configuring and chaining AnalyticDBSparkSQLOperators with proper cluster and resource group settings, focusing on cloud service operator configuration.", "input": {"airflow_version": "2.7.2", "operators": ["AnalyticDBSparkSQLOperator"], "line_count": 38, "is_multifile": false}, "output": "from __future__ import annotations\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nimport os\nfrom datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.alibaba.cloud.operators.analyticdb_spark import AnalyticDBSparkSQLOperator\n\nENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\nDAG_ID = \"adb_spark_sql_dag\"\n# [START howto_operator_adb_spark_sql]\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    default_args={\"cluster_id\": \"your cluster\", \"rg_name\": \"your resource group\", \"region\": \"your region\"},\n    max_active_runs=1,\n    catchup=False,\n) as dag:\n\n    show_databases = AnalyticDBSparkSQLOperator(task_id=\"task1\", sql=\"SHOE DATABASES;\")\n\n    show_tables = AnalyticDBSparkSQLOperator(task_id=\"task2\", sql=\"SHOW TABLES;\")\n\n    show_databases >> show_tables\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n# [END howto_operator_adb_spark_sql]\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_adb_spark_sql.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud-specific operator with multiple configuration parameters, requires understanding of external service integration and default arguments", "key_concepts": ["Cloud service configuration", "Operator default arguments", "Task dependencies", "SQL query orchestration"], "learning_objectives": ["Master cloud operator configuration", "Understand task chaining techniques", "Learn parameter management in Airflow"]}}
{"instruction": "Design an AWS Batch job orchestration workflow that demonstrates creating, running, and cleaning up batch compute resources using Airflow's task dependencies and AWS integration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BatchComputeEnvironmentSensor", "BatchCreateComputeEnvironmentOperator", "BatchJobQueueSensor", "BatchOperator", "BatchSensor"], "line_count": 270, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.batch import BatchCreateComputeEnvironmentOperator, BatchOperator\nfrom airflow.providers.amazon.aws.sensors.batch import (\n    BatchComputeEnvironmentSensor,\n    BatchJobQueueSensor,\n    BatchSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import (\n    ENV_ID_KEY,\n    SystemTestContextBuilder,\n    prune_logs,\n    split_string,\n)\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_batch\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY)\n    .add_variable(SECURITY_GROUPS_KEY)\n    .build()\n)\n\nJOB_OVERRIDES: dict = {}\n\n\n@task\ndef create_job_definition(role_arn, job_definition_name):\n    boto3.client(\"batch\").register_job_definition(\n        type=\"container\",\n        containerProperties={\n            \"command\": [\n                \"sleep\",\n                \"2\",\n            ],\n            \"executionRoleArn\": role_arn,\n            \"image\": \"busybox\",\n            \"resourceRequirements\": [\n                {\"value\": \"1\", \"type\": \"VCPU\"},\n                {\"value\": \"2048\", \"type\": \"MEMORY\"},\n            ],\n            \"networkConfiguration\": {\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        jobDefinitionName=job_definition_name,\n        platformCapabilities=[\"FARGATE\"],\n    )\n\n\n@task\ndef create_job_queue(job_compute_environment_name, job_queue_name):\n    boto3.client(\"batch\").create_job_queue(\n        computeEnvironmentOrder=[\n            {\n                \"computeEnvironment\": job_compute_environment_name,\n                \"order\": 1,\n            },\n        ],\n        jobQueueName=job_queue_name,\n        priority=1,\n        state=\"ENABLED\",\n    )\n\n\n# Only describe the job if a previous task failed, to help diagnose\n@task(trigger_rule=TriggerRule.ONE_FAILED)\ndef describe_job(job_id):\n    client = boto3.client(\"batch\")\n    response = client.describe_jobs(jobs=[job_id])\n    log.info(\"Describing the job %s for debugging purposes\", job_id)\n    log.info(response[\"jobs\"])\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_definition(job_definition_name):\n    client = boto3.client(\"batch\")\n\n    response = client.describe_job_definitions(\n        jobDefinitionName=job_definition_name,\n        status=\"ACTIVE\",\n    )\n\n    for job_definition in response[\"jobDefinitions\"]:\n        client.deregister_job_definition(\n            jobDefinition=job_definition[\"jobDefinitionArn\"],\n        )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").update_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").delete_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_job_queue(job_queue_name):\n    boto3.client(\"batch\").update_job_queue(\n        jobQueue=job_queue_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_queue(job_queue_name):\n    boto3.client(\"batch\").delete_job_queue(\n        jobQueue=job_queue_name,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    batch_job_name: str = f\"{env_id}-test-job\"\n    batch_job_definition_name: str = f\"{env_id}-test-job-definition\"\n    batch_job_compute_environment_name: str = f\"{env_id}-test-job-compute-environment\"\n    batch_job_queue_name: str = f\"{env_id}-test-job-queue\"\n\n    security_groups = split_string(test_context[SECURITY_GROUPS_KEY])\n    subnets = split_string(test_context[SUBNETS_KEY])\n\n    # [START howto_operator_batch_create_compute_environment]\n    create_compute_environment = BatchCreateComputeEnvironmentOperator(\n        task_id=\"create_compute_environment\",\n        compute_environment_name=batch_job_compute_environment_name,\n        environment_type=\"MANAGED\",\n        state=\"ENABLED\",\n        compute_resources={\n            \"type\": \"FARGATE\",\n            \"maxvCpus\": 10,\n            \"securityGroupIds\": security_groups,\n            \"subnets\": subnets,\n        },\n    )\n    # [END howto_operator_batch_create_compute_environment]\n\n    # [START howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_valid\",\n        compute_environment=batch_job_compute_environment_name,\n    )\n    # [END howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid.poke_interval = 1\n\n    # [START howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_valid\",\n        job_queue=batch_job_queue_name,\n    )\n    # [END howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid.poke_interval = 1\n\n    # [START howto_operator_batch]\n    submit_batch_job = BatchOperator(\n        task_id=\"submit_batch_job\",\n        job_name=batch_job_name,\n        job_queue=batch_job_queue_name,\n        job_definition=batch_job_definition_name,\n        overrides=JOB_OVERRIDES,\n    )\n    # [END howto_operator_batch]\n\n    # BatchOperator waits by default, setting as False to test the Sensor below.\n    submit_batch_job.wait_for_completion = False\n\n    # [START howto_sensor_batch]\n    wait_for_batch_job = BatchSensor(\n        task_id=\"wait_for_batch_job\",\n        job_id=submit_batch_job.output,\n    )\n    # [END howto_sensor_batch]\n    wait_for_batch_job.poke_interval = 10\n\n    wait_for_compute_environment_disabled = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_disabled\",\n        compute_environment=batch_job_compute_environment_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_modified = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_modified\",\n        job_queue=batch_job_queue_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_deleted = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_deleted\",\n        job_queue=batch_job_queue_name,\n        treat_non_existing_as_deleted=True,\n        poke_interval=10,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/batch/job\", env_id)\n        ],\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        security_groups,\n        subnets,\n        create_job_definition(test_context[ROLE_ARN_KEY], batch_job_definition_name),\n        # TEST BODY\n        create_compute_environment,\n        wait_for_compute_environment_valid,\n        # ``create_job_queue`` is part of test setup but need the compute-environment to be created before\n        create_job_queue(batch_job_compute_environment_name, batch_job_queue_name),\n        wait_for_job_queue_valid,\n        submit_batch_job,\n        wait_for_batch_job,\n        # TEST TEARDOWN\n        describe_job(submit_batch_job.output),\n        disable_job_queue(batch_job_queue_name),\n        wait_for_job_queue_modified,\n        delete_job_queue(batch_job_queue_name),\n        wait_for_job_queue_deleted,\n        disable_compute_environment(batch_job_compute_environment_name),\n        wait_for_compute_environment_disabled,\n        delete_compute_environment(batch_job_compute_environment_name),\n        delete_job_definition(batch_job_definition_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_batch.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS Batch operators, involves cloud service integration, includes task chaining with custom functions, and demonstrates resource lifecycle management", "key_concepts": ["AWS Batch integration", "Task dependencies", "Cloud service orchestration", "Resource lifecycle management", "Boto3 interactions"], "learning_objectives": ["Understand AWS Batch resource creation", "Learn task dependency management", "Practice cloud service orchestration in Airflow", "Implement cleanup and error handling strategies"]}}
{"instruction": "Create an advanced Airflow DAG that demonstrates dynamic AWS Batch job provisioning with comprehensive error tracking and automatic resource teardown using trigger rules.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "BatchComputeEnvironmentSensor", "BatchCreateComputeEnvironmentOperator", "BatchJobQueueSensor", "BatchOperator", "BatchSensor"], "line_count": 270, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.batch import BatchCreateComputeEnvironmentOperator, BatchOperator\nfrom airflow.providers.amazon.aws.sensors.batch import (\n    BatchComputeEnvironmentSensor,\n    BatchJobQueueSensor,\n    BatchSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import (\n    ENV_ID_KEY,\n    SystemTestContextBuilder,\n    prune_logs,\n    split_string,\n)\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_batch\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY)\n    .add_variable(SECURITY_GROUPS_KEY)\n    .build()\n)\n\nJOB_OVERRIDES: dict = {}\n\n\n@task\ndef create_job_definition(role_arn, job_definition_name):\n    boto3.client(\"batch\").register_job_definition(\n        type=\"container\",\n        containerProperties={\n            \"command\": [\n                \"sleep\",\n                \"2\",\n            ],\n            \"executionRoleArn\": role_arn,\n            \"image\": \"busybox\",\n            \"resourceRequirements\": [\n                {\"value\": \"1\", \"type\": \"VCPU\"},\n                {\"value\": \"2048\", \"type\": \"MEMORY\"},\n            ],\n            \"networkConfiguration\": {\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        jobDefinitionName=job_definition_name,\n        platformCapabilities=[\"FARGATE\"],\n    )\n\n\n@task\ndef create_job_queue(job_compute_environment_name, job_queue_name):\n    boto3.client(\"batch\").create_job_queue(\n        computeEnvironmentOrder=[\n            {\n                \"computeEnvironment\": job_compute_environment_name,\n                \"order\": 1,\n            },\n        ],\n        jobQueueName=job_queue_name,\n        priority=1,\n        state=\"ENABLED\",\n    )\n\n\n# Only describe the job if a previous task failed, to help diagnose\n@task(trigger_rule=TriggerRule.ONE_FAILED)\ndef describe_job(job_id):\n    client = boto3.client(\"batch\")\n    response = client.describe_jobs(jobs=[job_id])\n    log.info(\"Describing the job %s for debugging purposes\", job_id)\n    log.info(response[\"jobs\"])\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_definition(job_definition_name):\n    client = boto3.client(\"batch\")\n\n    response = client.describe_job_definitions(\n        jobDefinitionName=job_definition_name,\n        status=\"ACTIVE\",\n    )\n\n    for job_definition in response[\"jobDefinitions\"]:\n        client.deregister_job_definition(\n            jobDefinition=job_definition[\"jobDefinitionArn\"],\n        )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").update_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_compute_environment(job_compute_environment_name):\n    boto3.client(\"batch\").delete_compute_environment(\n        computeEnvironment=job_compute_environment_name,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef disable_job_queue(job_queue_name):\n    boto3.client(\"batch\").update_job_queue(\n        jobQueue=job_queue_name,\n        state=\"DISABLED\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_job_queue(job_queue_name):\n    boto3.client(\"batch\").delete_job_queue(\n        jobQueue=job_queue_name,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    batch_job_name: str = f\"{env_id}-test-job\"\n    batch_job_definition_name: str = f\"{env_id}-test-job-definition\"\n    batch_job_compute_environment_name: str = f\"{env_id}-test-job-compute-environment\"\n    batch_job_queue_name: str = f\"{env_id}-test-job-queue\"\n\n    security_groups = split_string(test_context[SECURITY_GROUPS_KEY])\n    subnets = split_string(test_context[SUBNETS_KEY])\n\n    # [START howto_operator_batch_create_compute_environment]\n    create_compute_environment = BatchCreateComputeEnvironmentOperator(\n        task_id=\"create_compute_environment\",\n        compute_environment_name=batch_job_compute_environment_name,\n        environment_type=\"MANAGED\",\n        state=\"ENABLED\",\n        compute_resources={\n            \"type\": \"FARGATE\",\n            \"maxvCpus\": 10,\n            \"securityGroupIds\": security_groups,\n            \"subnets\": subnets,\n        },\n    )\n    # [END howto_operator_batch_create_compute_environment]\n\n    # [START howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_valid\",\n        compute_environment=batch_job_compute_environment_name,\n    )\n    # [END howto_sensor_batch_compute_environment]\n    wait_for_compute_environment_valid.poke_interval = 1\n\n    # [START howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_valid\",\n        job_queue=batch_job_queue_name,\n    )\n    # [END howto_sensor_batch_job_queue]\n    wait_for_job_queue_valid.poke_interval = 1\n\n    # [START howto_operator_batch]\n    submit_batch_job = BatchOperator(\n        task_id=\"submit_batch_job\",\n        job_name=batch_job_name,\n        job_queue=batch_job_queue_name,\n        job_definition=batch_job_definition_name,\n        overrides=JOB_OVERRIDES,\n    )\n    # [END howto_operator_batch]\n\n    # BatchOperator waits by default, setting as False to test the Sensor below.\n    submit_batch_job.wait_for_completion = False\n\n    # [START howto_sensor_batch]\n    wait_for_batch_job = BatchSensor(\n        task_id=\"wait_for_batch_job\",\n        job_id=submit_batch_job.output,\n    )\n    # [END howto_sensor_batch]\n    wait_for_batch_job.poke_interval = 10\n\n    wait_for_compute_environment_disabled = BatchComputeEnvironmentSensor(\n        task_id=\"wait_for_compute_environment_disabled\",\n        compute_environment=batch_job_compute_environment_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_modified = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_modified\",\n        job_queue=batch_job_queue_name,\n        poke_interval=1,\n    )\n\n    wait_for_job_queue_deleted = BatchJobQueueSensor(\n        task_id=\"wait_for_job_queue_deleted\",\n        job_queue=batch_job_queue_name,\n        treat_non_existing_as_deleted=True,\n        poke_interval=10,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/batch/job\", env_id)\n        ],\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        security_groups,\n        subnets,\n        create_job_definition(test_context[ROLE_ARN_KEY], batch_job_definition_name),\n        # TEST BODY\n        create_compute_environment,\n        wait_for_compute_environment_valid,\n        # ``create_job_queue`` is part of test setup but need the compute-environment to be created before\n        create_job_queue(batch_job_compute_environment_name, batch_job_queue_name),\n        wait_for_job_queue_valid,\n        submit_batch_job,\n        wait_for_batch_job,\n        # TEST TEARDOWN\n        describe_job(submit_batch_job.output),\n        disable_job_queue(batch_job_queue_name),\n        wait_for_job_queue_modified,\n        delete_job_queue(batch_job_queue_name),\n        wait_for_job_queue_deleted,\n        disable_compute_environment(batch_job_compute_environment_name),\n        wait_for_compute_environment_disabled,\n        delete_compute_environment(batch_job_compute_environment_name),\n        delete_job_definition(batch_job_definition_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_batch.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Advanced task decorators, complex trigger rules, multiple custom tasks, intricate error handling, and sophisticated resource management across AWS services", "key_concepts": ["Trigger rules", "Error handling", "Dynamic task generation", "AWS service interactions", "Resource cleanup patterns"], "learning_objectives": ["Master advanced Airflow task configuration", "Implement robust error tracking strategies", "Understand dynamic resource provisioning", "Learn comprehensive cloud workflow design"]}}
{"instruction": "Build an AWS Database Migration Service (DMS) orchestration pipeline that demonstrates automated RDS database migration to S3 using Airflow tasks, focusing on creating security groups, database endpoints, and managing migration assets.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DmsCreateTaskOperator", "DmsDeleteTaskOperator", "DmsDescribeTasksOperator", "DmsStartTaskOperator", "DmsStopTaskOperator", "DmsTaskBaseSensor", "DmsTaskCompletedSensor", "RdsCreateDbInstanceOperator", "RdsDeleteDbInstanceOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 376, "is_multifile": false}, "output": "SAMPLE_DATA = [\n    (\"Airflow\", \"2015\"),\n    (\"OpenOffice\", \"2012\"),\n    (\"Subversion\", \"2000\"),\n    (\"NiFi\", \"2006\"),\n]\nSG_IP_PERMISSION = {\n    \"FromPort\": 5432,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}],\n}\n\n\ndef _get_rds_instance_endpoint(instance_name: str):\n    print(\"Retrieving RDS instance endpoint.\")\n    rds_client = boto3.client(\"rds\")\n\n    response = rds_client.describe_db_instances(DBInstanceIdentifier=instance_name)\n    rds_instance_endpoint = response[\"DBInstances\"][0][\"Endpoint\"]\n    return rds_instance_endpoint\n\n\n@task\ndef create_security_group(security_group_name: str, vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        GroupName=security_group_name,\n        Description=\"Created for DMS system test\",\n        VpcId=vpc_id,\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"],\n        IpPermissions=[SG_IP_PERMISSION],\n    )\n\n    return security_group[\"GroupId\"]\n\n\n@task\ndef create_sample_table(instance_name: str, db_name: str, table_name: str):\n    print(\"Creating sample table.\")\n\n    rds_endpoint = _get_rds_instance_endpoint(instance_name)\n    hostname = rds_endpoint[\"Address\"]\n    port = rds_endpoint[\"Port\"]\n    rds_url = f\"{RDS_PROTOCOL}://{RDS_USERNAME}:{RDS_PASSWORD}@{hostname}:{port}/{db_name}\"\n    engine = create_engine(rds_url)\n\n    table = Table(\n        table_name,\n        MetaData(engine),\n        Column(TABLE_HEADERS[0], String, primary_key=True),\n        Column(TABLE_HEADERS[1], String),\n    )\n\n    with engine.connect() as connection:\n        # Create the Table.\n        table.create()\n        load_data = table.insert().values(SAMPLE_DATA)\n        connection.execute(load_data)\n\n        # Read the data back to verify everything is working.\n        connection.execute(table.select())\n\n\n@task(multiple_outputs=True)\ndef create_dms_assets(\n    db_name: str,\n    instance_name: str,\n    replication_instance_name: str,\n    bucket_name: str,\n    role_arn,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    table_definition: dict,\n):\n    print(\"Creating DMS assets.\")\n    dms_client = boto3.client(\"dms\")\n    rds_instance_endpoint = _get_rds_instance_endpoint(instance_name)\n\n    print(\"Creating replication instance.\")\n    instance_arn = dms_client.create_replication_instance(\n        ReplicationInstanceIdentifier=replication_instance_name,\n        ReplicationInstanceClass=\"dms.t3.micro\",\n    )[\"ReplicationInstance\"][\"ReplicationInstanceArn\"]\n\n    print(\"Creating DMS source endpoint.\")\n    source_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=source_endpoint_identifier,\n        EndpointType=\"source\",\n        EngineName=RDS_ENGINE,\n        Username=RDS_USERNAME,\n        Password=RDS_PASSWORD,\n        ServerName=rds_instance_endpoint[\"Address\"],\n        Port=rds_instance_endpoint[\"Port\"],\n        DatabaseName=db_name,\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Creating DMS target endpoint.\")\n    target_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=target_endpoint_identifier,\n        EndpointType=\"target\",\n        EngineName=\"s3\",\n        S3Settings={\n            \"BucketName\": bucket_name,\n            \"BucketFolder\": \"folder\",\n            \"ServiceAccessRoleArn\": role_arn,\n            \"ExternalTableDefinition\": json.dumps(table_definition),\n        },\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Awaiting replication instance provisioning.\")\n    dms_client.get_waiter(\"replication_instance_available\").wait(\n        Filters=[{\"Name\": \"replication-instance-arn\", \"Values\": [instance_arn]}]\n    )\n\n    return {\n        \"replication_instance_arn\": instance_arn,\n        \"source_endpoint_arn\": source_endpoint_arn,\n        \"target_endpoint_arn\": target_endpoint_arn,\n    }\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dms_assets(\n    replication_instance_arn: str,\n    source_endpoint_arn: str,\n    target_endpoint_arn: str,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    replication_instance_name: str,\n):\n    dms_client = boto3.client(\"dms\")\n\n    print(\"Deleting DMS assets.\")\n    dms_client.delete_replication_instance(ReplicationInstanceArn=replication_instance_arn)\n    dms_client.delete_endpoint(EndpointArn=source_endpoint_arn)\n    dms_client.delete_endpoint(EndpointArn=target_endpoint_arn)\n\n    print(\"Awaiting DMS assets tear-down.\")\n    dms_client.get_waiter(\"replication_instance_deleted\").wait(\n        Filters=[{\"Name\": \"replication-instance-id\", \"Values\": [replication_instance_name]}]\n    )\n    dms_client.get_waiter(\"endpoint_deleted\").wait(\n        Filters=[\n            {\n                \"Name\": \"endpoint-id\",\n                \"Values\": [source_endpoint_identifier, target_endpoint_identifier],\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(security_group_id: str, security_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=security_group_id, GroupName=security_group_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    bucket_name = f\"{env_id}-dms-bucket\"\n    rds_instance_name = f\"{env_id}-instance\"\n    rds_db_name = f\"{env_id}_source_database\"  # dashes are not allowed in db name\n    rds_table_name = f\"{env_id}-table\"\n    dms_replication_instance_name = f\"{env_id}-replication-instance\"\n    dms_replication_task_id = f\"{env_id}-replication-task\"\n    source_endpoint_identifier = f\"{env_id}-source-endpoint\"\n    target_endpoint_identifier = f\"{env_id}-target-endpoint\"\n    security_group_name = f\"{env_id}-dms-security-group\"\n\n    # Sample data.\n    table_definition = {\n        \"TableCount\": \"1\",\n        \"Tables\": [\n            {\n                \"TableName\": rds_table_name,\n                \"TableColumns\": [\n                    {\n                        \"ColumnName\": TABLE_HEADERS[0],\n                        \"ColumnType\": \"STRING\",\n                        \"ColumnNullable\": \"false\",\n                        \"ColumnIsPk\": \"true\",\n                    },\n                    {\"ColumnName\": TABLE_HEADERS[1], \"ColumnType\": \"STRING\", \"ColumnLength\": \"4\"},\n                ],\n                \"TableColumnsTotal\": \"2\",\n            }\n        ],\n    }\n    table_mappings = {\n        \"rules\": [\n            {\n                \"rule-type\": \"selection\",\n                \"rule-id\": \"1\",\n                \"rule-name\": \"1\",\n                \"object-locator\": {\n                    \"schema-name\": \"public\",\n                    \"table-name\": rds_table_name,\n                },\n                \"rule-action\": \"include\",\n            }\n        ]\n    }\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    get_vpc_id = get_default_vpc_id()\n\n    create_sg = create_security_group(security_group_name, get_vpc_id)\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t3.micro\",\n        engine=RDS_ENGINE,\n        rds_kwargs={\n            \"DBName\": rds_db_name,\n            \"AllocatedStorage\": 20,\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"PubliclyAccessible\": True,\n            \"VpcSecurityGroupIds\": [\n                create_sg,\n            ],\n        },\n    )\n\n    create_assets = create_dms_assets(\n        db_name=rds_db_name,\n        instance_name=rds_instance_name,\n        replication_instance_name=dms_replication_instance_name,\n        bucket_name=bucket_name,\n        role_arn=role_arn,\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        table_definition=table_definition,\n    )\n\n    # [START howto_operator_dms_create_task]\n    create_task = DmsCreateTaskOperator(\n        task_id=\"create_task\",\n        replication_task_id=dms_replication_task_id,\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        table_mappings=table_mappings,\n    )\n    # [END howto_operator_dms_create_task]\n\n    task_arn = cast(str, create_task.output)\n\n    # [START howto_operator_dms_start_task]\n    start_task = DmsStartTaskOperator(\n        task_id=\"start_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_start_task]\n\n    # [START howto_operator_dms_describe_tasks]\n    describe_tasks = DmsDescribeTasksOperator(\n        task_id=\"describe_tasks\",\n        describe_tasks_kwargs={\n            \"Filters\": [\n                {\n                    \"Name\": \"replication-instance-arn\",\n                    \"Values\": [create_assets[\"replication_instance_arn\"]],\n                }\n            ]\n        },\n        do_xcom_push=False,\n    )\n    # [END howto_operator_dms_describe_tasks]\n\n    await_task_start = DmsTaskBaseSensor(\n        task_id=\"await_task_start\",\n        replication_task_arn=task_arn,\n        target_statuses=[\"running\"],\n        termination_statuses=[\"stopped\", \"deleting\", \"failed\"],\n        poke_interval=10,\n    )\n\n    # [START howto_operator_dms_stop_task]\n    stop_task = DmsStopTaskOperator(\n        task_id=\"stop_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_stop_task]\n\n    # TaskCompletedSensor actually waits until task reaches the \"Stopped\" state, so it will work here.\n    # [START howto_sensor_dms_task_completed]\n    await_task_stop = DmsTaskCompletedSensor(\n        task_id=\"await_task_stop\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_sensor_dms_task_completed]\n    await_task_stop.poke_interval = 10\n\n    # [START howto_operator_dms_delete_task]\n    delete_task = DmsDeleteTaskOperator(\n        task_id=\"delete_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_delete_task]\n    delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_assets = delete_dms_assets(\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        replication_instance_name=dms_replication_instance_name,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        get_vpc_id,\n        create_sg,\n        create_db_instance,\n        create_sample_table(rds_instance_name, rds_db_name, rds_table_name),\n        create_assets,\n        # TEST BODY\n        create_task,\n        start_task,\n        describe_tasks,\n        await_task_start,\n        stop_task,\n        await_task_stop,\n        # TEST TEARDOWN\n        delete_task,\n        delete_assets,\n        delete_db_instance,\n        delete_security_group(create_sg, security_group_name),\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dms.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Multiple AWS service integrations, complex custom tasks, advanced boto3 interactions, dynamic endpoint creation, and intricate cloud resource management", "key_concepts": ["AWS DMS", "Boto3 Interactions", "TaskFlow API", "Cloud Resource Management", "Database Migration"], "learning_objectives": ["Understand AWS service orchestration in Airflow", "Learn dynamic task creation with @task decorator", "Master cloud resource provisioning programmatically", "Implement complex multi-step migration workflows"]}}
{"instruction": "Design an Airflow DAG that demonstrates secure, programmatic cloud infrastructure provisioning by creating RDS security groups, generating database endpoints, and managing migration tasks with robust error handling.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DmsCreateTaskOperator", "DmsDeleteTaskOperator", "DmsDescribeTasksOperator", "DmsStartTaskOperator", "DmsStopTaskOperator", "DmsTaskBaseSensor", "DmsTaskCompletedSensor", "RdsCreateDbInstanceOperator", "RdsDeleteDbInstanceOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 376, "is_multifile": false}, "output": "SAMPLE_DATA = [\n    (\"Airflow\", \"2015\"),\n    (\"OpenOffice\", \"2012\"),\n    (\"Subversion\", \"2000\"),\n    (\"NiFi\", \"2006\"),\n]\nSG_IP_PERMISSION = {\n    \"FromPort\": 5432,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}],\n}\n\n\ndef _get_rds_instance_endpoint(instance_name: str):\n    print(\"Retrieving RDS instance endpoint.\")\n    rds_client = boto3.client(\"rds\")\n\n    response = rds_client.describe_db_instances(DBInstanceIdentifier=instance_name)\n    rds_instance_endpoint = response[\"DBInstances\"][0][\"Endpoint\"]\n    return rds_instance_endpoint\n\n\n@task\ndef create_security_group(security_group_name: str, vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        GroupName=security_group_name,\n        Description=\"Created for DMS system test\",\n        VpcId=vpc_id,\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"],\n        IpPermissions=[SG_IP_PERMISSION],\n    )\n\n    return security_group[\"GroupId\"]\n\n\n@task\ndef create_sample_table(instance_name: str, db_name: str, table_name: str):\n    print(\"Creating sample table.\")\n\n    rds_endpoint = _get_rds_instance_endpoint(instance_name)\n    hostname = rds_endpoint[\"Address\"]\n    port = rds_endpoint[\"Port\"]\n    rds_url = f\"{RDS_PROTOCOL}://{RDS_USERNAME}:{RDS_PASSWORD}@{hostname}:{port}/{db_name}\"\n    engine = create_engine(rds_url)\n\n    table = Table(\n        table_name,\n        MetaData(engine),\n        Column(TABLE_HEADERS[0], String, primary_key=True),\n        Column(TABLE_HEADERS[1], String),\n    )\n\n    with engine.connect() as connection:\n        # Create the Table.\n        table.create()\n        load_data = table.insert().values(SAMPLE_DATA)\n        connection.execute(load_data)\n\n        # Read the data back to verify everything is working.\n        connection.execute(table.select())\n\n\n@task(multiple_outputs=True)\ndef create_dms_assets(\n    db_name: str,\n    instance_name: str,\n    replication_instance_name: str,\n    bucket_name: str,\n    role_arn,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    table_definition: dict,\n):\n    print(\"Creating DMS assets.\")\n    dms_client = boto3.client(\"dms\")\n    rds_instance_endpoint = _get_rds_instance_endpoint(instance_name)\n\n    print(\"Creating replication instance.\")\n    instance_arn = dms_client.create_replication_instance(\n        ReplicationInstanceIdentifier=replication_instance_name,\n        ReplicationInstanceClass=\"dms.t3.micro\",\n    )[\"ReplicationInstance\"][\"ReplicationInstanceArn\"]\n\n    print(\"Creating DMS source endpoint.\")\n    source_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=source_endpoint_identifier,\n        EndpointType=\"source\",\n        EngineName=RDS_ENGINE,\n        Username=RDS_USERNAME,\n        Password=RDS_PASSWORD,\n        ServerName=rds_instance_endpoint[\"Address\"],\n        Port=rds_instance_endpoint[\"Port\"],\n        DatabaseName=db_name,\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Creating DMS target endpoint.\")\n    target_endpoint_arn = dms_client.create_endpoint(\n        EndpointIdentifier=target_endpoint_identifier,\n        EndpointType=\"target\",\n        EngineName=\"s3\",\n        S3Settings={\n            \"BucketName\": bucket_name,\n            \"BucketFolder\": \"folder\",\n            \"ServiceAccessRoleArn\": role_arn,\n            \"ExternalTableDefinition\": json.dumps(table_definition),\n        },\n    )[\"Endpoint\"][\"EndpointArn\"]\n\n    print(\"Awaiting replication instance provisioning.\")\n    dms_client.get_waiter(\"replication_instance_available\").wait(\n        Filters=[{\"Name\": \"replication-instance-arn\", \"Values\": [instance_arn]}]\n    )\n\n    return {\n        \"replication_instance_arn\": instance_arn,\n        \"source_endpoint_arn\": source_endpoint_arn,\n        \"target_endpoint_arn\": target_endpoint_arn,\n    }\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dms_assets(\n    replication_instance_arn: str,\n    source_endpoint_arn: str,\n    target_endpoint_arn: str,\n    source_endpoint_identifier: str,\n    target_endpoint_identifier: str,\n    replication_instance_name: str,\n):\n    dms_client = boto3.client(\"dms\")\n\n    print(\"Deleting DMS assets.\")\n    dms_client.delete_replication_instance(ReplicationInstanceArn=replication_instance_arn)\n    dms_client.delete_endpoint(EndpointArn=source_endpoint_arn)\n    dms_client.delete_endpoint(EndpointArn=target_endpoint_arn)\n\n    print(\"Awaiting DMS assets tear-down.\")\n    dms_client.get_waiter(\"replication_instance_deleted\").wait(\n        Filters=[{\"Name\": \"replication-instance-id\", \"Values\": [replication_instance_name]}]\n    )\n    dms_client.get_waiter(\"endpoint_deleted\").wait(\n        Filters=[\n            {\n                \"Name\": \"endpoint-id\",\n                \"Values\": [source_endpoint_identifier, target_endpoint_identifier],\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(security_group_id: str, security_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=security_group_id, GroupName=security_group_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    bucket_name = f\"{env_id}-dms-bucket\"\n    rds_instance_name = f\"{env_id}-instance\"\n    rds_db_name = f\"{env_id}_source_database\"  # dashes are not allowed in db name\n    rds_table_name = f\"{env_id}-table\"\n    dms_replication_instance_name = f\"{env_id}-replication-instance\"\n    dms_replication_task_id = f\"{env_id}-replication-task\"\n    source_endpoint_identifier = f\"{env_id}-source-endpoint\"\n    target_endpoint_identifier = f\"{env_id}-target-endpoint\"\n    security_group_name = f\"{env_id}-dms-security-group\"\n\n    # Sample data.\n    table_definition = {\n        \"TableCount\": \"1\",\n        \"Tables\": [\n            {\n                \"TableName\": rds_table_name,\n                \"TableColumns\": [\n                    {\n                        \"ColumnName\": TABLE_HEADERS[0],\n                        \"ColumnType\": \"STRING\",\n                        \"ColumnNullable\": \"false\",\n                        \"ColumnIsPk\": \"true\",\n                    },\n                    {\"ColumnName\": TABLE_HEADERS[1], \"ColumnType\": \"STRING\", \"ColumnLength\": \"4\"},\n                ],\n                \"TableColumnsTotal\": \"2\",\n            }\n        ],\n    }\n    table_mappings = {\n        \"rules\": [\n            {\n                \"rule-type\": \"selection\",\n                \"rule-id\": \"1\",\n                \"rule-name\": \"1\",\n                \"object-locator\": {\n                    \"schema-name\": \"public\",\n                    \"table-name\": rds_table_name,\n                },\n                \"rule-action\": \"include\",\n            }\n        ]\n    }\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    get_vpc_id = get_default_vpc_id()\n\n    create_sg = create_security_group(security_group_name, get_vpc_id)\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t3.micro\",\n        engine=RDS_ENGINE,\n        rds_kwargs={\n            \"DBName\": rds_db_name,\n            \"AllocatedStorage\": 20,\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"PubliclyAccessible\": True,\n            \"VpcSecurityGroupIds\": [\n                create_sg,\n            ],\n        },\n    )\n\n    create_assets = create_dms_assets(\n        db_name=rds_db_name,\n        instance_name=rds_instance_name,\n        replication_instance_name=dms_replication_instance_name,\n        bucket_name=bucket_name,\n        role_arn=role_arn,\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        table_definition=table_definition,\n    )\n\n    # [START howto_operator_dms_create_task]\n    create_task = DmsCreateTaskOperator(\n        task_id=\"create_task\",\n        replication_task_id=dms_replication_task_id,\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        table_mappings=table_mappings,\n    )\n    # [END howto_operator_dms_create_task]\n\n    task_arn = cast(str, create_task.output)\n\n    # [START howto_operator_dms_start_task]\n    start_task = DmsStartTaskOperator(\n        task_id=\"start_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_start_task]\n\n    # [START howto_operator_dms_describe_tasks]\n    describe_tasks = DmsDescribeTasksOperator(\n        task_id=\"describe_tasks\",\n        describe_tasks_kwargs={\n            \"Filters\": [\n                {\n                    \"Name\": \"replication-instance-arn\",\n                    \"Values\": [create_assets[\"replication_instance_arn\"]],\n                }\n            ]\n        },\n        do_xcom_push=False,\n    )\n    # [END howto_operator_dms_describe_tasks]\n\n    await_task_start = DmsTaskBaseSensor(\n        task_id=\"await_task_start\",\n        replication_task_arn=task_arn,\n        target_statuses=[\"running\"],\n        termination_statuses=[\"stopped\", \"deleting\", \"failed\"],\n        poke_interval=10,\n    )\n\n    # [START howto_operator_dms_stop_task]\n    stop_task = DmsStopTaskOperator(\n        task_id=\"stop_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_stop_task]\n\n    # TaskCompletedSensor actually waits until task reaches the \"Stopped\" state, so it will work here.\n    # [START howto_sensor_dms_task_completed]\n    await_task_stop = DmsTaskCompletedSensor(\n        task_id=\"await_task_stop\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_sensor_dms_task_completed]\n    await_task_stop.poke_interval = 10\n\n    # [START howto_operator_dms_delete_task]\n    delete_task = DmsDeleteTaskOperator(\n        task_id=\"delete_task\",\n        replication_task_arn=task_arn,\n    )\n    # [END howto_operator_dms_delete_task]\n    delete_task.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_assets = delete_dms_assets(\n        replication_instance_arn=create_assets[\"replication_instance_arn\"],\n        source_endpoint_arn=create_assets[\"source_endpoint_arn\"],\n        target_endpoint_arn=create_assets[\"target_endpoint_arn\"],\n        source_endpoint_identifier=source_endpoint_identifier,\n        target_endpoint_identifier=target_endpoint_identifier,\n        replication_instance_name=dms_replication_instance_name,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        get_vpc_id,\n        create_sg,\n        create_db_instance,\n        create_sample_table(rds_instance_name, rds_db_name, rds_table_name),\n        create_assets,\n        # TEST BODY\n        create_task,\n        start_task,\n        describe_tasks,\n        await_task_start,\n        stop_task,\n        await_task_stop,\n        # TEST TEARDOWN\n        delete_task,\n        delete_assets,\n        delete_db_instance,\n        delete_security_group(create_sg, security_group_name),\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dms.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS service interactions, custom functions, security group management, and endpoint creation with moderate complexity", "key_concepts": ["Cloud Security", "Infrastructure as Code", "Endpoint Management", "Error Handling", "AWS Service Integration"], "learning_objectives": ["Learn secure cloud resource provisioning", "Understand dynamic task configuration", "Practice error-resilient cloud orchestration", "Implement modular infrastructure tasks"]}}
{"instruction": "Build an AWS ECS Fargate task orchestration workflow that demonstrates creating an ECS cluster, registering a task definition, running a containerized task, and cleaning up resources using Airflow's AWS providers and task decorators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EcsRunTaskOperator", "EcsTaskStateSensor"], "line_count": 153, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsTaskStates\nfrom airflow.providers.amazon.aws.operators.ecs import EcsRunTaskOperator\nfrom airflow.providers.amazon.aws.sensors.ecs import EcsTaskStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs_fargate\"\n\n# Externally fetched variables:\nSUBNETS_KEY = \"SUBNETS\"  # At least one public subnet is required.\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .add_variable(SECURITY_GROUPS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef create_cluster(cluster_name: str) -> None:\n    \"\"\"Creates an ECS cluster.\"\"\"\n    boto3.client(\"ecs\").create_cluster(clusterName=cluster_name)\n\n\n@task\ndef register_task_definition(task_name: str, container_name: str) -> str:\n    \"\"\"Creates a Task Definition.\"\"\"\n    response = boto3.client(\"ecs\").register_task_definition(\n        family=task_name,\n        # CPU and Memory are required for Fargate and are set to the lowest currently allowed values.\n        cpu=\"256\",\n        memory=\"512\",\n        containerDefinitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n            }\n        ],\n        requiresCompatibilities=[\"FARGATE\"],\n        networkMode=\"awsvpc\",\n    )\n\n    return response[\"taskDefinition\"][\"taskDefinitionArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_definition(task_definition_arn: str) -> None:\n    \"\"\"Deletes the Task Definition.\"\"\"\n    boto3.client(\"ecs\").deregister_task_definition(taskDefinition=task_definition_arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster(cluster_name: str) -> None:\n    \"\"\"Deletes the ECS cluster.\"\"\"\n    boto3.client(\"ecs\").delete_cluster(cluster=cluster_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-test-cluster\"\n    container_name = f\"{env_id}-test-container\"\n    task_definition_name = f\"{env_id}-test-definition\"\n\n    create_task_definition = register_task_definition(task_definition_name, container_name)\n\n    # [START howto_operator_ecs]\n    hello_world = EcsRunTaskOperator(\n        task_id=\"hello_world\",\n        cluster=cluster_name,\n        task_definition=task_definition_name,\n        launch_type=\"FARGATE\",\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo\", \"hello\", \"world\"],\n                },\n            ],\n        },\n        network_configuration={\n            \"awsvpcConfiguration\": {\n                \"subnets\": test_context[SUBNETS_KEY],\n                \"securityGroups\": test_context[SECURITY_GROUPS_KEY],\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        # You must set `reattach=True` in order to get ecs_task_arn if you plan to use a Sensor.\n        reattach=True,\n    )\n    # [END howto_operator_ecs]\n\n    # EcsRunTaskOperator waits by default, setting as False to test the Sensor below.\n    hello_world.wait_for_completion = False\n\n    # [START howto_sensor_ecs_task_state]\n    # By default, EcsTaskStateSensor waits until the task has started, but the\n    # demo task runs so fast that the sensor misses it.  This sensor instead\n    # demonstrates how to wait until the ECS Task has completed by providing\n    # the target_state and failure_states parameters.\n    await_task_finish = EcsTaskStateSensor(\n        task_id=\"await_task_finish\",\n        cluster=cluster_name,\n        task=hello_world.output[\"ecs_task_arn\"],\n        target_state=EcsTaskStates.STOPPED,\n        failure_states={EcsTaskStates.NONE},\n    )\n    # [END howto_sensor_ecs_task_state]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_cluster(cluster_name),\n        create_task_definition,\n        # TEST BODY\n        hello_world,\n        await_task_finish,\n        # TEST TEARDOWN\n        delete_task_definition(create_task_definition),\n        delete_cluster(cluster_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ecs_fargate.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS service interactions, custom task functions, ECS operators, and involves cloud service integration with dynamic configuration management", "key_concepts": ["AWS ECS Fargate", "Task Decorators", "Cloud Service Orchestration", "Boto3 Interactions", "Dynamic Configuration"], "learning_objectives": ["Understand ECS task lifecycle management", "Learn AWS provider integration in Airflow", "Practice task dependency management", "Explore dynamic resource configuration"]}}
{"instruction": "Design a cloud-native task execution pipeline demonstrating infrastructure-as-code principles by programmatically managing container tasks, network configurations, and resource cleanup using Airflow's declarative workflow capabilities.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EcsRunTaskOperator", "EcsTaskStateSensor"], "line_count": 153, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsTaskStates\nfrom airflow.providers.amazon.aws.operators.ecs import EcsRunTaskOperator\nfrom airflow.providers.amazon.aws.sensors.ecs import EcsTaskStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs_fargate\"\n\n# Externally fetched variables:\nSUBNETS_KEY = \"SUBNETS\"  # At least one public subnet is required.\nSECURITY_GROUPS_KEY = \"SECURITY_GROUPS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .add_variable(SECURITY_GROUPS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef create_cluster(cluster_name: str) -> None:\n    \"\"\"Creates an ECS cluster.\"\"\"\n    boto3.client(\"ecs\").create_cluster(clusterName=cluster_name)\n\n\n@task\ndef register_task_definition(task_name: str, container_name: str) -> str:\n    \"\"\"Creates a Task Definition.\"\"\"\n    response = boto3.client(\"ecs\").register_task_definition(\n        family=task_name,\n        # CPU and Memory are required for Fargate and are set to the lowest currently allowed values.\n        cpu=\"256\",\n        memory=\"512\",\n        containerDefinitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n            }\n        ],\n        requiresCompatibilities=[\"FARGATE\"],\n        networkMode=\"awsvpc\",\n    )\n\n    return response[\"taskDefinition\"][\"taskDefinitionArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_definition(task_definition_arn: str) -> None:\n    \"\"\"Deletes the Task Definition.\"\"\"\n    boto3.client(\"ecs\").deregister_task_definition(taskDefinition=task_definition_arn)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_cluster(cluster_name: str) -> None:\n    \"\"\"Deletes the ECS cluster.\"\"\"\n    boto3.client(\"ecs\").delete_cluster(cluster=cluster_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-test-cluster\"\n    container_name = f\"{env_id}-test-container\"\n    task_definition_name = f\"{env_id}-test-definition\"\n\n    create_task_definition = register_task_definition(task_definition_name, container_name)\n\n    # [START howto_operator_ecs]\n    hello_world = EcsRunTaskOperator(\n        task_id=\"hello_world\",\n        cluster=cluster_name,\n        task_definition=task_definition_name,\n        launch_type=\"FARGATE\",\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo\", \"hello\", \"world\"],\n                },\n            ],\n        },\n        network_configuration={\n            \"awsvpcConfiguration\": {\n                \"subnets\": test_context[SUBNETS_KEY],\n                \"securityGroups\": test_context[SECURITY_GROUPS_KEY],\n                \"assignPublicIp\": \"ENABLED\",\n            },\n        },\n        # You must set `reattach=True` in order to get ecs_task_arn if you plan to use a Sensor.\n        reattach=True,\n    )\n    # [END howto_operator_ecs]\n\n    # EcsRunTaskOperator waits by default, setting as False to test the Sensor below.\n    hello_world.wait_for_completion = False\n\n    # [START howto_sensor_ecs_task_state]\n    # By default, EcsTaskStateSensor waits until the task has started, but the\n    # demo task runs so fast that the sensor misses it.  This sensor instead\n    # demonstrates how to wait until the ECS Task has completed by providing\n    # the target_state and failure_states parameters.\n    await_task_finish = EcsTaskStateSensor(\n        task_id=\"await_task_finish\",\n        cluster=cluster_name,\n        task=hello_world.output[\"ecs_task_arn\"],\n        target_state=EcsTaskStates.STOPPED,\n        failure_states={EcsTaskStates.NONE},\n    )\n    # [END howto_sensor_ecs_task_state]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_cluster(cluster_name),\n        create_task_definition,\n        # TEST BODY\n        hello_world,\n        await_task_finish,\n        # TEST TEARDOWN\n        delete_task_definition(create_task_definition),\n        delete_cluster(cluster_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ecs_fargate.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Advanced cloud orchestration pattern with multiple task functions, network configuration management, and explicit resource lifecycle handling", "key_concepts": ["Infrastructure as Code", "Container Orchestration", "Cloud Resource Management", "Declarative Workflows", "Task State Management"], "learning_objectives": ["Implement cloud resource lifecycle management", "Understand declarative task configuration", "Learn network configuration in containerized environments", "Practice clean resource provisioning and teardown"]}}
{"instruction": "Build an Airflow DAG that demonstrates cloud data transfer between Azure Blob Storage and Amazon S3, focusing on understanding cross-cloud storage migration and operator configuration.", "input": {"airflow_version": "2.7.2", "operators": ["AzureBlobStorageToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 67, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.azure_blob_to_s3 import AzureBlobStorageToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_azure_blob_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-azure_blob-to-s3-bucket\"\n    s3_key = f\"{env_id}-azure_blob-to-s3-key\"\n    s3_key_url = f\"s3://{s3_bucket}/{s3_key}\"\n    azure_container_name = f\"{env_id}-azure_blob-to-s3-container\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_azure_blob_to_s3]\n    azure_blob_to_s3 = AzureBlobStorageToS3Operator(\n        task_id=\"azure_blob_to_s3\",\n        container_name=azure_container_name,\n        dest_s3_key=s3_key_url,\n    )\n    # [END howto_transfer_azure_blob_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        azure_blob_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_azure_blob_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operators, involves multiple external services, requires configuration of Azure and S3 connections, demonstrates inter-cloud data movement", "key_concepts": ["Cloud Storage Transfer", "AWS S3 Operators", "Azure Blob Storage", "Operator Configuration", "DAG Chaining"], "learning_objectives": ["Configure cloud storage transfer operators", "Understand cross-cloud data migration patterns", "Learn Airflow task chaining techniques"]}}
{"instruction": "Design an Airflow workflow that demonstrates cloud resource lifecycle management, including bucket creation, data transfer, and automatic cleanup using trigger rules and system test context.", "input": {"airflow_version": "2.7.2", "operators": ["AzureBlobStorageToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 67, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.azure_blob_to_s3 import AzureBlobStorageToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_azure_blob_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-azure_blob-to-s3-bucket\"\n    s3_key = f\"{env_id}-azure_blob-to-s3-key\"\n    s3_key_url = f\"s3://{s3_bucket}/{s3_key}\"\n    azure_container_name = f\"{env_id}-azure_blob-to-s3-container\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_azure_blob_to_s3]\n    azure_blob_to_s3 = AzureBlobStorageToS3Operator(\n        task_id=\"azure_blob_to_s3\",\n        container_name=azure_container_name,\n        dest_s3_key=s3_key_url,\n    )\n    # [END howto_transfer_azure_blob_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        azure_blob_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_azure_blob_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operators with different lifecycle stages, uses advanced trigger rules, incorporates system testing context and cleanup mechanisms", "key_concepts": ["Resource Lifecycle Management", "Trigger Rules", "System Test Context", "Cloud Resource Cleanup", "Idempotent DAG Design"], "learning_objectives": ["Implement cloud resource management workflows", "Understand Airflow trigger rule configurations", "Learn systematic approach to test environment setup and teardown"]}}
{"instruction": "Build an Airflow DAG that demonstrates DynamoDB interaction using a sensor to wait for a specific item value, showcasing how to create, monitor, and delete a DynamoDB table dynamically within a workflow.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DynamoDBValueSensor"], "line_count": 96, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.sensors.dynamodb import DynamoDBValueSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_dynamodbvaluesensor\"\nsys_test_context_task = SystemTestContextBuilder().build()\n\nPK_ATTRIBUTE_NAME = \"PK\"\nSK_ATTRIBUTE_NAME = \"SK\"\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": PK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n    {\"AttributeName\": SK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"PK\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"SK\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10}\n\n\n@task\ndef create_table(table_name: str):\n    ddb = boto3.resource(\"dynamodb\")\n    table = ddb.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(TableName=table_name)\n    table.put_item(Item={\"PK\": \"Test\", \"SK\": \"2022-07-12T11:11:25-0400\", \"Value\": \"Testing\"})\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_table(table_name: str):\n    client = boto3.client(\"dynamodb\")\n    client.delete_table(TableName=table_name)\n    client.get_waiter(\"table_not_exists\").wait(TableName=table_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    create_table = create_table(table_name=table_name)\n    delete_table = delete_table(table_name)\n\n    # [START howto_sensor_dynamodb_value]\n    dynamodb_sensor = DynamoDBValueSensor(\n        task_id=\"waiting_for_dynamodb_item_value\",\n        table_name=table_name,\n        partition_key_name=PK_ATTRIBUTE_NAME,\n        partition_key_value=\"Test\",\n        sort_key_name=SK_ATTRIBUTE_NAME,\n        sort_key_value=\"2022-07-12T11:11:25-0400\",\n        attribute_name=\"Value\",\n        attribute_value=\"Testing\",\n    )\n    # [END howto_sensor_dynamodb_value]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        # TEST BODY\n        dynamodb_sensor,\n        # TEST TEARDOWN\n        delete_table,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dynamodb.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS integration, custom tasks with @task decorator, DynamoDB sensor, and involves multiple AWS service interactions with boto3", "key_concepts": ["AWS Integration", "DynamoDB Sensor", "Task Decorators", "Boto3 Interactions", "Dynamic Table Management"], "learning_objectives": ["Understand AWS service integration in Airflow", "Learn how to use DynamoDB sensors", "Practice creating dynamic task workflows", "Explore conditional task execution"]}}
{"instruction": "Design an Airflow workflow that demonstrates advanced sensor capabilities by creating a DynamoDB table, inserting test data, and using a value sensor to validate specific item conditions before proceeding.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DynamoDBValueSensor"], "line_count": 96, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.sensors.dynamodb import DynamoDBValueSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_dynamodbvaluesensor\"\nsys_test_context_task = SystemTestContextBuilder().build()\n\nPK_ATTRIBUTE_NAME = \"PK\"\nSK_ATTRIBUTE_NAME = \"SK\"\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": PK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n    {\"AttributeName\": SK_ATTRIBUTE_NAME, \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"PK\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"SK\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10}\n\n\n@task\ndef create_table(table_name: str):\n    ddb = boto3.resource(\"dynamodb\")\n    table = ddb.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(TableName=table_name)\n    table.put_item(Item={\"PK\": \"Test\", \"SK\": \"2022-07-12T11:11:25-0400\", \"Value\": \"Testing\"})\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_table(table_name: str):\n    client = boto3.client(\"dynamodb\")\n    client.delete_table(TableName=table_name)\n    client.get_waiter(\"table_not_exists\").wait(TableName=table_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    create_table = create_table(table_name=table_name)\n    delete_table = delete_table(table_name)\n\n    # [START howto_sensor_dynamodb_value]\n    dynamodb_sensor = DynamoDBValueSensor(\n        task_id=\"waiting_for_dynamodb_item_value\",\n        table_name=table_name,\n        partition_key_name=PK_ATTRIBUTE_NAME,\n        partition_key_value=\"Test\",\n        sort_key_name=SK_ATTRIBUTE_NAME,\n        sort_key_value=\"2022-07-12T11:11:25-0400\",\n        attribute_name=\"Value\",\n        attribute_value=\"Testing\",\n    )\n    # [END howto_sensor_dynamodb_value]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        # TEST BODY\n        dynamodb_sensor,\n        # TEST TEARDOWN\n        delete_table,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dynamodb.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex sensor logic, multiple task interactions, AWS resource management, and demonstrates conditional workflow progression", "key_concepts": ["Sensor Mechanisms", "Conditional Workflow", "Resource Lifecycle Management", "AWS Service Interaction", "Task Chaining"], "learning_objectives": ["Understand sensor-driven workflow control", "Learn resource creation and cleanup in Airflow", "Practice task dependency management", "Explore dynamic task execution strategies"]}}
{"instruction": "Build an Airflow DAG that demonstrates a complete AWS DynamoDB to S3 data transfer workflow, including table creation, backup, and cleanup using AWS service operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DynamoDBToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 194, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom tenacity import before_log, before_sleep_log\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_dynamodb_to_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": \"ID\", \"AttributeType\": \"S\"},\n    {\"AttributeName\": \"Value\", \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"ID\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"Value\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 1, \"WriteCapacityUnits\": 1}\nS3_KEY_PREFIX = \"dynamodb-segmented-file\"\n\n\n# UpdateContinuousBackups API might need multiple attempts to succeed\n# Sometimes the API returns the error \"Backups are being enabled for the table: <...>. Please retry later\"\n# Using a retry strategy with exponential backoff to remediate that\n@tenacity.retry(\n    stop=tenacity.stop_after_attempt(20),\n    wait=tenacity.wait_exponential(min=5),\n    before=before_log(log, logging.INFO),\n    before_sleep=before_sleep_log(log, logging.WARNING),\n)\ndef enable_point_in_time_recovery(table_name: str):\n    boto3.client(\"dynamodb\").update_continuous_backups(\n        TableName=table_name,\n        PointInTimeRecoverySpecification={\n            \"PointInTimeRecoveryEnabled\": True,\n        },\n    )\n\n\n@task\ndef set_up_table(table_name: str):\n    dynamo_resource = boto3.resource(\"dynamodb\")\n    table = dynamo_resource.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n    enable_point_in_time_recovery(table_name)\n    table.put_item(Item={\"ID\": \"123\", \"Value\": \"Testing\"})\n\n\n@task\ndef get_export_time(table_name: str):\n    r = boto3.client(\"dynamodb\").describe_continuous_backups(\n        TableName=table_name,\n    )\n\n    return r[\"ContinuousBackupsDescription\"][\"PointInTimeRecoveryDescription\"][\"EarliestRestorableDateTime\"]\n\n\n@task\ndef wait_for_bucket(s3_bucket_name):\n    waiter = boto3.client(\"s3\").get_waiter(\"bucket_exists\")\n    waiter.wait(Bucket=s3_bucket_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dynamodb_table(table_name: str):\n    boto3.resource(\"dynamodb\").Table(table_name).delete()\n    boto3.client(\"dynamodb\").get_waiter(\"table_not_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    bucket_name = f\"{env_id}-dynamodb-bucket\"\n\n    create_table = set_up_table(table_name=table_name)\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    # [START howto_transfer_dynamodb_to_s3]\n    backup_db = DynamoDBToS3Operator(\n        task_id=\"backup_db\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=20,\n    )\n    # [END howto_transfer_dynamodb_to_s3]\n\n    # [START howto_transfer_dynamodb_to_s3_segmented]\n    # Segmenting allows the transfer to be parallelized into {segment} number of parallel tasks.\n    backup_db_segment_1 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_1\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-1-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 0,\n        },\n    )\n\n    backup_db_segment_2 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_2\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-2-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 1,\n        },\n    )\n    # [END howto_transfer_dynamodb_to_s3_segmented]\n\n    export_time = get_export_time(table_name)\n    # [START howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n    backup_db_to_point_in_time = DynamoDBToS3Operator(\n        task_id=\"backup_db_to_point_in_time\",\n        dynamodb_table_name=table_name,\n        file_size=1000,\n        s3_bucket_name=bucket_name,\n        export_time=export_time,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-3-\",\n    )\n    # [END howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n\n    delete_table = delete_dynamodb_table(table_name=table_name)\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        create_bucket,\n        wait_for_bucket(s3_bucket_name=bucket_name),\n        # TEST BODY\n        backup_db,\n        backup_db_segment_1,\n        backup_db_segment_2,\n        export_time,\n        backup_db_to_point_in_time,\n        # TEST TEARDOWN\n        delete_table,\n        delete_bucket,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dynamodb_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, includes custom tasks with boto3, demonstrates service integration, and involves task chaining with retry mechanisms", "key_concepts": ["AWS Service Integration", "Task Decorators", "Boto3 Interactions", "Error Handling", "Cloud Data Transfer"], "learning_objectives": ["Understand AWS service orchestration in Airflow", "Learn dynamic task creation with @task decorator", "Practice error handling and retry strategies", "Implement cloud data migration workflows"]}}
{"instruction": "Create an Airflow DAG that showcases advanced error resilience and retry mechanisms when performing cloud infrastructure operations across DynamoDB and S3.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DynamoDBToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 194, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nimport boto3\nimport tenacity\nfrom tenacity import before_log, before_sleep_log\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nlog = logging.getLogger(__name__)\n\nDAG_ID = \"example_dynamodb_to_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nTABLE_ATTRIBUTES = [\n    {\"AttributeName\": \"ID\", \"AttributeType\": \"S\"},\n    {\"AttributeName\": \"Value\", \"AttributeType\": \"S\"},\n]\nTABLE_KEY_SCHEMA = [\n    {\"AttributeName\": \"ID\", \"KeyType\": \"HASH\"},\n    {\"AttributeName\": \"Value\", \"KeyType\": \"RANGE\"},\n]\nTABLE_THROUGHPUT = {\"ReadCapacityUnits\": 1, \"WriteCapacityUnits\": 1}\nS3_KEY_PREFIX = \"dynamodb-segmented-file\"\n\n\n# UpdateContinuousBackups API might need multiple attempts to succeed\n# Sometimes the API returns the error \"Backups are being enabled for the table: <...>. Please retry later\"\n# Using a retry strategy with exponential backoff to remediate that\n@tenacity.retry(\n    stop=tenacity.stop_after_attempt(20),\n    wait=tenacity.wait_exponential(min=5),\n    before=before_log(log, logging.INFO),\n    before_sleep=before_sleep_log(log, logging.WARNING),\n)\ndef enable_point_in_time_recovery(table_name: str):\n    boto3.client(\"dynamodb\").update_continuous_backups(\n        TableName=table_name,\n        PointInTimeRecoverySpecification={\n            \"PointInTimeRecoveryEnabled\": True,\n        },\n    )\n\n\n@task\ndef set_up_table(table_name: str):\n    dynamo_resource = boto3.resource(\"dynamodb\")\n    table = dynamo_resource.create_table(\n        AttributeDefinitions=TABLE_ATTRIBUTES,\n        TableName=table_name,\n        KeySchema=TABLE_KEY_SCHEMA,\n        ProvisionedThroughput=TABLE_THROUGHPUT,\n    )\n    boto3.client(\"dynamodb\").get_waiter(\"table_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n    enable_point_in_time_recovery(table_name)\n    table.put_item(Item={\"ID\": \"123\", \"Value\": \"Testing\"})\n\n\n@task\ndef get_export_time(table_name: str):\n    r = boto3.client(\"dynamodb\").describe_continuous_backups(\n        TableName=table_name,\n    )\n\n    return r[\"ContinuousBackupsDescription\"][\"PointInTimeRecoveryDescription\"][\"EarliestRestorableDateTime\"]\n\n\n@task\ndef wait_for_bucket(s3_bucket_name):\n    waiter = boto3.client(\"s3\").get_waiter(\"bucket_exists\")\n    waiter.wait(Bucket=s3_bucket_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dynamodb_table(table_name: str):\n    boto3.resource(\"dynamodb\").Table(table_name).delete()\n    boto3.client(\"dynamodb\").get_waiter(\"table_not_exists\").wait(\n        TableName=table_name, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 10}\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    table_name = f\"{env_id}-dynamodb-table\"\n    bucket_name = f\"{env_id}-dynamodb-bucket\"\n\n    create_table = set_up_table(table_name=table_name)\n\n    create_bucket = S3CreateBucketOperator(task_id=\"create_bucket\", bucket_name=bucket_name)\n\n    # [START howto_transfer_dynamodb_to_s3]\n    backup_db = DynamoDBToS3Operator(\n        task_id=\"backup_db\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=20,\n    )\n    # [END howto_transfer_dynamodb_to_s3]\n\n    # [START howto_transfer_dynamodb_to_s3_segmented]\n    # Segmenting allows the transfer to be parallelized into {segment} number of parallel tasks.\n    backup_db_segment_1 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_1\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-1-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 0,\n        },\n    )\n\n    backup_db_segment_2 = DynamoDBToS3Operator(\n        task_id=\"backup_db_segment_2\",\n        dynamodb_table_name=table_name,\n        s3_bucket_name=bucket_name,\n        # Max output file size in bytes.  If the Table is too large, multiple files will be created.\n        file_size=1000,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-2-\",\n        dynamodb_scan_kwargs={\n            \"TotalSegments\": 2,\n            \"Segment\": 1,\n        },\n    )\n    # [END howto_transfer_dynamodb_to_s3_segmented]\n\n    export_time = get_export_time(table_name)\n    # [START howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n    backup_db_to_point_in_time = DynamoDBToS3Operator(\n        task_id=\"backup_db_to_point_in_time\",\n        dynamodb_table_name=table_name,\n        file_size=1000,\n        s3_bucket_name=bucket_name,\n        export_time=export_time,\n        s3_key_prefix=f\"{S3_KEY_PREFIX}-3-\",\n    )\n    # [END howto_transfer_dynamodb_to_s3_in_some_point_in_time]\n\n    delete_table = delete_dynamodb_table(table_name=table_name)\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_table,\n        create_bucket,\n        wait_for_bucket(s3_bucket_name=bucket_name),\n        # TEST BODY\n        backup_db,\n        backup_db_segment_1,\n        backup_db_segment_2,\n        export_time,\n        backup_db_to_point_in_time,\n        # TEST TEARDOWN\n        delete_table,\n        delete_bucket,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_dynamodb_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements sophisticated retry decorators, uses waiter mechanisms, handles potential AWS API inconsistencies, and manages stateful cloud resources", "key_concepts": ["Tenacity Retry Decorators", "AWS Waiters", "Idempotent Task Design", "Cloud Resource Management", "Fault Tolerance"], "learning_objectives": ["Implement robust error handling in cloud workflows", "Understand exponential backoff strategies", "Learn safe resource creation and deletion patterns", "Master resilient cloud service interactions"]}}
{"instruction": "Build an AWS DataSync workflow that demonstrates creating S3 buckets, generating DataSync locations, creating a data transfer task, and executing the task using Airflow's AWS providers and TaskFlow API.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DataSyncOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 230, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_datasync\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n\ndef get_s3_bucket_arn(bucket_name):\n    return f\"arn:aws:s3:::{bucket_name}\"\n\n\ndef create_location(bucket_name, role_arn):\n    client = boto3.client(\"datasync\")\n    response = client.create_location_s3(\n        Subdirectory=\"test\",\n        S3BucketArn=get_s3_bucket_arn(bucket_name),\n        S3Config={\n            \"BucketAccessRoleArn\": role_arn,\n        },\n    )\n    return response[\"LocationArn\"]\n\n\n@task\ndef create_source_location(bucket_source, role_arn):\n    return create_location(bucket_source, role_arn)\n\n\n@task\ndef create_destination_location(bucket_destination, role_arn):\n    return create_location(bucket_destination, role_arn)\n\n\n@task\ndef create_task(**kwargs):\n    client = boto3.client(\"datasync\")\n    response = client.create_task(\n        SourceLocationArn=kwargs[\"ti\"].xcom_pull(\"create_source_location\"),\n        DestinationLocationArn=kwargs[\"ti\"].xcom_pull(\"create_destination_location\"),\n    )\n    return response[\"TaskArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task(task_arn):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=task_arn,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_created_by_operator(**kwargs):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=kwargs[\"ti\"].xcom_pull(\"create_and_execute_task\")[\"TaskArn\"],\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef list_locations(bucket_source, bucket_destination):\n    client = boto3.client(\"datasync\")\n    return client.list_locations(\n        Filters=[\n            {\n                \"Name\": \"LocationUri\",\n                \"Values\": [\n                    f\"s3://{bucket_source}/test/\",\n                    f\"s3://{bucket_destination}/test/\",\n                    f\"s3://{bucket_source}/test_create/\",\n                    f\"s3://{bucket_destination}/test_create/\",\n                ],\n                \"Operator\": \"In\",\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_locations(locations):\n    client = boto3.client(\"datasync\")\n    for location in locations[\"Locations\"]:\n        client.delete_location(\n            LocationArn=location[\"LocationArn\"],\n        )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    s3_bucket_source: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-source\"\n    s3_bucket_destination: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-destination\"\n\n    create_s3_bucket_source = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_source\", bucket_name=s3_bucket_source\n    )\n\n    create_s3_bucket_destination = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_destination\", bucket_name=s3_bucket_destination\n    )\n\n    source_location = create_source_location(s3_bucket_source, test_context[ROLE_ARN_KEY])\n    destination_location = create_destination_location(s3_bucket_destination, test_context[ROLE_ARN_KEY])\n\n    created_task_arn = create_task()\n\n    # [START howto_operator_datasync_specific_task]\n    # Execute a specific task\n    execute_task_by_arn = DataSyncOperator(\n        task_id=\"execute_task_by_arn\",\n        task_arn=created_task_arn,\n    )\n    # [END howto_operator_datasync_specific_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_arn.wait_for_completion = False\n\n    # [START howto_operator_datasync_search_task]\n    # Search and execute a task\n    execute_task_by_locations = DataSyncOperator(\n        task_id=\"execute_task_by_locations\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test\",\n        # Only transfer files from /test/subdir folder\n        task_execution_kwargs={\n            \"Includes\": [{\"FilterType\": \"SIMPLE_PATTERN\", \"Value\": \"/test/subdir\"}],\n        },\n    )\n    # [END howto_operator_datasync_search_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_locations.wait_for_completion = False\n\n    # [START howto_operator_datasync_create_task]\n    # Create a task (the task does not exist)\n    create_and_execute_task = DataSyncOperator(\n        task_id=\"create_and_execute_task\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test_create\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test_create\",\n        create_task_kwargs={\"Name\": \"Created by Airflow\"},\n        create_source_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_source),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        create_destination_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_destination),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        delete_task_after_execution=False,\n    )\n    # [END howto_operator_datasync_create_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    create_and_execute_task.wait_for_completion = False\n\n    locations_task = list_locations(s3_bucket_source, s3_bucket_destination)\n    delete_locations_task = delete_locations(locations_task)\n\n    delete_s3_bucket_source = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_source\",\n        bucket_name=s3_bucket_source,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket_destination = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_destination\",\n        bucket_name=s3_bucket_destination,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket_source,\n        create_s3_bucket_destination,\n        source_location,\n        destination_location,\n        created_task_arn,\n        # TEST BODY\n        execute_task_by_arn,\n        execute_task_by_locations,\n        create_and_execute_task,\n        # TEST TEARDOWN\n        delete_task(created_task_arn),\n        delete_task_created_by_operator(),\n        locations_task,\n        delete_locations_task,\n        delete_s3_bucket_source,\n        delete_s3_bucket_destination,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_datasync.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, custom task functions, and requires understanding of boto3 and DataSync interactions", "key_concepts": ["TaskFlow API", "AWS Operators", "boto3 integration", "XCom communication", "Cloud service orchestration"], "learning_objectives": ["Create AWS S3 buckets programmatically", "Generate DataSync locations and tasks", "Use task decorators for complex workflows", "Manage cloud resource lifecycle in Airflow"]}}
{"instruction": "Design an Airflow DAG that demonstrates advanced resource management techniques, including dynamic location creation, task execution, and comprehensive cleanup using trigger rules and AWS SDK interactions.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "DataSyncOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 230, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_datasync\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n\ndef get_s3_bucket_arn(bucket_name):\n    return f\"arn:aws:s3:::{bucket_name}\"\n\n\ndef create_location(bucket_name, role_arn):\n    client = boto3.client(\"datasync\")\n    response = client.create_location_s3(\n        Subdirectory=\"test\",\n        S3BucketArn=get_s3_bucket_arn(bucket_name),\n        S3Config={\n            \"BucketAccessRoleArn\": role_arn,\n        },\n    )\n    return response[\"LocationArn\"]\n\n\n@task\ndef create_source_location(bucket_source, role_arn):\n    return create_location(bucket_source, role_arn)\n\n\n@task\ndef create_destination_location(bucket_destination, role_arn):\n    return create_location(bucket_destination, role_arn)\n\n\n@task\ndef create_task(**kwargs):\n    client = boto3.client(\"datasync\")\n    response = client.create_task(\n        SourceLocationArn=kwargs[\"ti\"].xcom_pull(\"create_source_location\"),\n        DestinationLocationArn=kwargs[\"ti\"].xcom_pull(\"create_destination_location\"),\n    )\n    return response[\"TaskArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task(task_arn):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=task_arn,\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_task_created_by_operator(**kwargs):\n    client = boto3.client(\"datasync\")\n    client.delete_task(\n        TaskArn=kwargs[\"ti\"].xcom_pull(\"create_and_execute_task\")[\"TaskArn\"],\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef list_locations(bucket_source, bucket_destination):\n    client = boto3.client(\"datasync\")\n    return client.list_locations(\n        Filters=[\n            {\n                \"Name\": \"LocationUri\",\n                \"Values\": [\n                    f\"s3://{bucket_source}/test/\",\n                    f\"s3://{bucket_destination}/test/\",\n                    f\"s3://{bucket_source}/test_create/\",\n                    f\"s3://{bucket_destination}/test_create/\",\n                ],\n                \"Operator\": \"In\",\n            }\n        ]\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_locations(locations):\n    client = boto3.client(\"datasync\")\n    for location in locations[\"Locations\"]:\n        client.delete_location(\n            LocationArn=location[\"LocationArn\"],\n        )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    s3_bucket_source: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-source\"\n    s3_bucket_destination: str = f\"{test_context[ENV_ID_KEY]}-datasync-bucket-destination\"\n\n    create_s3_bucket_source = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_source\", bucket_name=s3_bucket_source\n    )\n\n    create_s3_bucket_destination = S3CreateBucketOperator(\n        task_id=\"create_s3_bucket_destination\", bucket_name=s3_bucket_destination\n    )\n\n    source_location = create_source_location(s3_bucket_source, test_context[ROLE_ARN_KEY])\n    destination_location = create_destination_location(s3_bucket_destination, test_context[ROLE_ARN_KEY])\n\n    created_task_arn = create_task()\n\n    # [START howto_operator_datasync_specific_task]\n    # Execute a specific task\n    execute_task_by_arn = DataSyncOperator(\n        task_id=\"execute_task_by_arn\",\n        task_arn=created_task_arn,\n    )\n    # [END howto_operator_datasync_specific_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_arn.wait_for_completion = False\n\n    # [START howto_operator_datasync_search_task]\n    # Search and execute a task\n    execute_task_by_locations = DataSyncOperator(\n        task_id=\"execute_task_by_locations\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test\",\n        # Only transfer files from /test/subdir folder\n        task_execution_kwargs={\n            \"Includes\": [{\"FilterType\": \"SIMPLE_PATTERN\", \"Value\": \"/test/subdir\"}],\n        },\n    )\n    # [END howto_operator_datasync_search_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    execute_task_by_locations.wait_for_completion = False\n\n    # [START howto_operator_datasync_create_task]\n    # Create a task (the task does not exist)\n    create_and_execute_task = DataSyncOperator(\n        task_id=\"create_and_execute_task\",\n        source_location_uri=f\"s3://{s3_bucket_source}/test_create\",\n        destination_location_uri=f\"s3://{s3_bucket_destination}/test_create\",\n        create_task_kwargs={\"Name\": \"Created by Airflow\"},\n        create_source_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_source),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        create_destination_location_kwargs={\n            \"Subdirectory\": \"test_create\",\n            \"S3BucketArn\": get_s3_bucket_arn(s3_bucket_destination),\n            \"S3Config\": {\n                \"BucketAccessRoleArn\": test_context[ROLE_ARN_KEY],\n            },\n        },\n        delete_task_after_execution=False,\n    )\n    # [END howto_operator_datasync_create_task]\n\n    # DataSyncOperator waits by default, setting as False to test the Sensor below.\n    create_and_execute_task.wait_for_completion = False\n\n    locations_task = list_locations(s3_bucket_source, s3_bucket_destination)\n    delete_locations_task = delete_locations(locations_task)\n\n    delete_s3_bucket_source = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_source\",\n        bucket_name=s3_bucket_source,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_s3_bucket_destination = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket_destination\",\n        bucket_name=s3_bucket_destination,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket_source,\n        create_s3_bucket_destination,\n        source_location,\n        destination_location,\n        created_task_arn,\n        # TEST BODY\n        execute_task_by_arn,\n        execute_task_by_locations,\n        create_and_execute_task,\n        # TEST TEARDOWN\n        delete_task(created_task_arn),\n        delete_task_created_by_operator(),\n        locations_task,\n        delete_locations_task,\n        delete_s3_bucket_source,\n        delete_s3_bucket_destination,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_datasync.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Advanced workflow with multiple custom tasks, complex trigger rules, dynamic resource management, and sophisticated error handling across cloud services", "key_concepts": ["Dynamic task generation", "Trigger rules", "Resource lifecycle management", "Error handling", "AWS SDK interactions"], "learning_objectives": ["Implement robust cloud resource management", "Use advanced Airflow trigger configurations", "Handle complex task dependencies", "Create self-cleaning cloud workflows"]}}
{"instruction": "Build an Airflow DAG that demonstrates AWS CloudFormation stack lifecycle management, creating and deleting an SQS queue using CloudFormation operators and sensors.", "input": {"airflow_version": "2.7.2", "operators": ["CloudFormationCreateStackOperator", "CloudFormationCreateStackSensor", "CloudFormationDeleteStackOperator", "CloudFormationDeleteStackSensor"], "line_count": 104, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_cloudformation\"\n\n# The CloudFormation template must have at least one resource to\n# be usable, this example uses SQS as a free and serverless option.\nCLOUDFORMATION_TEMPLATE = {\n    \"Description\": \"Stack from Airflow CloudFormation example DAG\",\n    \"Resources\": {\n        \"ExampleQueue\": {\n            \"Type\": \"AWS::SQS::Queue\",\n        }\n    },\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    cloudformation_stack_name = f\"{env_id}-stack\"\n    cloudformation_create_parameters = {\n        \"StackName\": cloudformation_stack_name,\n        \"TemplateBody\": json.dumps(CLOUDFORMATION_TEMPLATE),\n        \"TimeoutInMinutes\": 2,\n        \"OnFailure\": \"DELETE\",  # Don't leave stacks behind if creation fails.\n    }\n\n    # [START howto_operator_cloudformation_create_stack]\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=cloudformation_stack_name,\n        cloudformation_parameters=cloudformation_create_parameters,\n    )\n    # [END howto_operator_cloudformation_create_stack]\n\n    # [START howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create = CloudFormationCreateStackSensor(\n        task_id=\"wait_for_stack_create\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create.poke_interval = 10\n\n    # [START howto_operator_cloudformation_delete_stack]\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_operator_cloudformation_delete_stack]\n    delete_stack.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete = CloudFormationDeleteStackSensor(\n        task_id=\"wait_for_stack_delete\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete.poke_interval = 10\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_stack,\n        wait_for_stack_create,\n        delete_stack,\n        wait_for_stack_delete,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_cloudformation.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS-specific operators, includes sensors for async operations, demonstrates cloud service integration with multiple task dependencies", "key_concepts": ["AWS CloudFormation", "Operators", "Sensors", "Task Chaining", "Cloud Service Orchestration"], "learning_objectives": ["Understand CloudFormation stack management in Airflow", "Learn how to use AWS-specific operators", "Practice task dependency and chaining", "Explore asynchronous task monitoring with sensors"]}}
{"instruction": "Design an Airflow workflow that showcases infrastructure-as-code principles by dynamically creating and removing cloud resources using CloudFormation with robust error handling and cleanup mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["CloudFormationCreateStackOperator", "CloudFormationCreateStackSensor", "CloudFormationDeleteStackOperator", "CloudFormationDeleteStackSensor"], "line_count": 104, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.cloud_formation import (\n    CloudFormationCreateStackOperator,\n    CloudFormationDeleteStackOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.cloud_formation import (\n    CloudFormationCreateStackSensor,\n    CloudFormationDeleteStackSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_cloudformation\"\n\n# The CloudFormation template must have at least one resource to\n# be usable, this example uses SQS as a free and serverless option.\nCLOUDFORMATION_TEMPLATE = {\n    \"Description\": \"Stack from Airflow CloudFormation example DAG\",\n    \"Resources\": {\n        \"ExampleQueue\": {\n            \"Type\": \"AWS::SQS::Queue\",\n        }\n    },\n}\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    cloudformation_stack_name = f\"{env_id}-stack\"\n    cloudformation_create_parameters = {\n        \"StackName\": cloudformation_stack_name,\n        \"TemplateBody\": json.dumps(CLOUDFORMATION_TEMPLATE),\n        \"TimeoutInMinutes\": 2,\n        \"OnFailure\": \"DELETE\",  # Don't leave stacks behind if creation fails.\n    }\n\n    # [START howto_operator_cloudformation_create_stack]\n    create_stack = CloudFormationCreateStackOperator(\n        task_id=\"create_stack\",\n        stack_name=cloudformation_stack_name,\n        cloudformation_parameters=cloudformation_create_parameters,\n    )\n    # [END howto_operator_cloudformation_create_stack]\n\n    # [START howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create = CloudFormationCreateStackSensor(\n        task_id=\"wait_for_stack_create\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_create_stack]\n    wait_for_stack_create.poke_interval = 10\n\n    # [START howto_operator_cloudformation_delete_stack]\n    delete_stack = CloudFormationDeleteStackOperator(\n        task_id=\"delete_stack\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_operator_cloudformation_delete_stack]\n    delete_stack.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete = CloudFormationDeleteStackSensor(\n        task_id=\"wait_for_stack_delete\",\n        stack_name=cloudformation_stack_name,\n    )\n    # [END howto_sensor_cloudformation_delete_stack]\n    wait_for_stack_delete.poke_interval = 10\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_stack,\n        wait_for_stack_create,\n        delete_stack,\n        wait_for_stack_delete,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_cloudformation.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves cloud resource provisioning, uses trigger rules, demonstrates infrastructure management patterns with multiple task states", "key_concepts": ["Infrastructure as Code", "Resource Provisioning", "Error Handling", "Trigger Rules", "Cloud Resource Management"], "learning_objectives": ["Implement infrastructure deployment workflows", "Understand resource lifecycle management", "Learn about Airflow's error and state management", "Practice clean resource provisioning and teardown"]}}
{"instruction": "Build an AWS AppFlow data synchronization DAG that demonstrates multiple data extraction strategies from Salesforce, including full dump, daily incremental, and filtered data runs using different AppFlow operators.", "input": {"airflow_version": "2.7.2", "operators": ["AppflowRecordsShortCircuitOperator", "AppflowRunAfterOperator", "AppflowRunBeforeOperator", "AppflowRunDailyOperator", "AppflowRunFullOperator", "AppflowRunOperator", "BashOperator"], "line_count": 116, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.operators.appflow import (\n    AppflowRecordsShortCircuitOperator,\n    AppflowRunAfterOperator,\n    AppflowRunBeforeOperator,\n    AppflowRunDailyOperator,\n    AppflowRunFullOperator,\n    AppflowRunOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_appflow\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    source_name = \"salesforce\"\n    flow_name = f\"{env_id}-salesforce-campaign\"\n\n    # [START howto_operator_appflow_run]\n    campaign_dump = AppflowRunOperator(\n        task_id=\"campaign_dump\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run]\n\n    # [START howto_operator_appflow_run_full]\n    campaign_dump_full = AppflowRunFullOperator(\n        task_id=\"campaign_dump_full\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run_full]\n\n    # [START howto_operator_appflow_run_daily]\n    campaign_dump_daily = AppflowRunDailyOperator(\n        task_id=\"campaign_dump_daily\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_daily]\n\n    # [START howto_operator_appflow_run_before]\n    campaign_dump_before = AppflowRunBeforeOperator(\n        task_id=\"campaign_dump_before\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_before]\n\n    # [START howto_operator_appflow_run_after]\n    campaign_dump_after = AppflowRunAfterOperator(\n        task_id=\"campaign_dump_after\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"3000-01-01\",  # Future date, so no records to dump\n    )\n    # [END howto_operator_appflow_run_after]\n\n    # [START howto_operator_appflow_shortcircuit]\n    campaign_dump_short_circuit = AppflowRecordsShortCircuitOperator(\n        task_id=\"campaign_dump_short_circuit\",\n        flow_name=flow_name,\n        appflow_run_task_id=\"campaign_dump_after\",  # Should shortcircuit, no records expected\n    )\n    # [END howto_operator_appflow_shortcircuit]\n\n    should_be_skipped = BashOperator(\n        task_id=\"should_be_skipped\",\n        bash_command=\"echo 1\",\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        campaign_dump,\n        campaign_dump_full,\n        campaign_dump_daily,\n        campaign_dump_before,\n        campaign_dump_after,\n        campaign_dump_short_circuit,\n        should_be_skipped,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_appflow.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS-specific AppFlow operators, involves external service integration, and demonstrates varied data extraction techniques beyond simple sequential tasks", "key_concepts": ["AWS AppFlow", "Data Extraction Strategies", "Incremental Data Loading", "Conditional Task Execution"], "learning_objectives": ["Understand AppFlow operator variations", "Learn incremental data synchronization techniques", "Explore conditional task execution in Airflow"]}}
{"instruction": "Design a data pipeline that demonstrates advanced Airflow task chaining and short-circuit logic, focusing on how different AppFlow operators can be sequenced and conditionally executed based on data availability.", "input": {"airflow_version": "2.7.2", "operators": ["AppflowRecordsShortCircuitOperator", "AppflowRunAfterOperator", "AppflowRunBeforeOperator", "AppflowRunDailyOperator", "AppflowRunFullOperator", "AppflowRunOperator", "BashOperator"], "line_count": 116, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.operators.appflow import (\n    AppflowRecordsShortCircuitOperator,\n    AppflowRunAfterOperator,\n    AppflowRunBeforeOperator,\n    AppflowRunDailyOperator,\n    AppflowRunFullOperator,\n    AppflowRunOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_appflow\"\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    source_name = \"salesforce\"\n    flow_name = f\"{env_id}-salesforce-campaign\"\n\n    # [START howto_operator_appflow_run]\n    campaign_dump = AppflowRunOperator(\n        task_id=\"campaign_dump\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run]\n\n    # [START howto_operator_appflow_run_full]\n    campaign_dump_full = AppflowRunFullOperator(\n        task_id=\"campaign_dump_full\",\n        source=source_name,\n        flow_name=flow_name,\n    )\n    # [END howto_operator_appflow_run_full]\n\n    # [START howto_operator_appflow_run_daily]\n    campaign_dump_daily = AppflowRunDailyOperator(\n        task_id=\"campaign_dump_daily\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_daily]\n\n    # [START howto_operator_appflow_run_before]\n    campaign_dump_before = AppflowRunBeforeOperator(\n        task_id=\"campaign_dump_before\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"{{ ds }}\",\n    )\n    # [END howto_operator_appflow_run_before]\n\n    # [START howto_operator_appflow_run_after]\n    campaign_dump_after = AppflowRunAfterOperator(\n        task_id=\"campaign_dump_after\",\n        source=source_name,\n        flow_name=flow_name,\n        source_field=\"LastModifiedDate\",\n        filter_date=\"3000-01-01\",  # Future date, so no records to dump\n    )\n    # [END howto_operator_appflow_run_after]\n\n    # [START howto_operator_appflow_shortcircuit]\n    campaign_dump_short_circuit = AppflowRecordsShortCircuitOperator(\n        task_id=\"campaign_dump_short_circuit\",\n        flow_name=flow_name,\n        appflow_run_task_id=\"campaign_dump_after\",  # Should shortcircuit, no records expected\n    )\n    # [END howto_operator_appflow_shortcircuit]\n\n    should_be_skipped = BashOperator(\n        task_id=\"should_be_skipped\",\n        bash_command=\"echo 1\",\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        campaign_dump,\n        campaign_dump_full,\n        campaign_dump_daily,\n        campaign_dump_before,\n        campaign_dump_after,\n        campaign_dump_short_circuit,\n        should_be_skipped,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_appflow.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex task dependencies, uses short-circuit operator, demonstrates advanced workflow control beyond basic sequential execution", "key_concepts": ["Task Chaining", "Short-Circuit Operators", "Conditional Workflow Execution", "Data Filtering"], "learning_objectives": ["Master task dependency management", "Understand conditional task execution", "Learn how to implement workflow logic control"]}}
{"instruction": "Build an AWS ECS Task Orchestration DAG that demonstrates creating an ECS cluster, registering a task definition, and running a container with logging configuration using Airflow's AWS providers.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EcsClusterStateSensor", "EcsCreateClusterOperator", "EcsDeleteClusterOperator", "EcsDeregisterTaskDefinitionOperator", "EcsRegisterTaskDefinitionOperator", "EcsRunTaskOperator", "EcsTaskDefinitionStateSensor"], "line_count": 208, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates\nfrom airflow.providers.amazon.aws.operators.ecs import (\n    EcsCreateClusterOperator,\n    EcsDeleteClusterOperator,\n    EcsDeregisterTaskDefinitionOperator,\n    EcsRegisterTaskDefinitionOperator,\n    EcsRunTaskOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ecs import (\n    EcsClusterStateSensor,\n    EcsTaskDefinitionStateSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs\"\n\n# Externally fetched variables:\nEXISTING_CLUSTER_NAME_KEY = \"CLUSTER_NAME\"\nEXISTING_CLUSTER_SUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    # NOTE:  Creating a functional ECS Cluster which uses EC2 requires manually creating\n    # and configuring a number of resources such as autoscaling groups, networking\n    # etc. which is out of scope for this demo and time-consuming for a system test\n    # To simplify this demo and make it run in a reasonable length of time as a\n    # system test, follow the steps below to create a new cluster on the AWS Console\n    # which handles all asset creation and configuration using default values:\n    # 1. https://us-east-1.console.aws.amazon.com/ecs/home?region=us-east-1#/clusters\n    # 2. Select \"EC2 Linux + Networking\" and hit \"Next\"\n    # 3. Name your cluster in the first field and click Create\n    .add_variable(EXISTING_CLUSTER_NAME_KEY)\n    .add_variable(EXISTING_CLUSTER_SUBNETS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef get_region():\n    return boto3.session.Session().region_name\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_logs(group_name: str):\n    client = boto3.client(\"logs\")\n    client.delete_log_group(logGroupName=group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    existing_cluster_name = test_context[EXISTING_CLUSTER_NAME_KEY]\n    existing_cluster_subnets = test_context[EXISTING_CLUSTER_SUBNETS_KEY]\n\n    new_cluster_name = f\"{env_id}-cluster\"\n    container_name = f\"{env_id}-container\"\n    family_name = f\"{env_id}-task-definition\"\n    asg_name = f\"{env_id}-asg\"\n\n    aws_region = get_region()\n    log_group_name = f\"/ecs_test/{env_id}\"\n\n    # [START howto_operator_ecs_create_cluster]\n    create_cluster = EcsCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_create_cluster]\n\n    # EcsCreateClusterOperator waits by default, setting as False to test the Sensor below.\n    create_cluster.wait_for_completion = False\n\n    # [START howto_sensor_ecs_cluster_state]\n    await_cluster = EcsClusterStateSensor(\n        task_id=\"await_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_sensor_ecs_cluster_state]\n\n    # [START howto_operator_ecs_register_task_definition]\n    register_task = EcsRegisterTaskDefinitionOperator(\n        task_id=\"register_task\",\n        family=family_name,\n        container_definitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": log_group_name,\n                        \"awslogs-region\": aws_region,\n                        \"awslogs-create-group\": \"true\",\n                        \"awslogs-stream-prefix\": \"ecs\",\n                    },\n                },\n            },\n        ],\n        register_task_kwargs={\n            \"cpu\": \"256\",\n            \"memory\": \"512\",\n            \"networkMode\": \"awsvpc\",\n        },\n    )\n    # [END howto_operator_ecs_register_task_definition]\n\n    # [START howto_sensor_ecs_task_definition_state]\n    await_task_definition = EcsTaskDefinitionStateSensor(\n        task_id=\"await_task_definition\",\n        task_definition=register_task.output,\n    )\n    # [END howto_sensor_ecs_task_definition_state]\n\n    # [START howto_operator_ecs_run_task]\n    run_task = EcsRunTaskOperator(\n        task_id=\"run_task\",\n        cluster=existing_cluster_name,\n        task_definition=register_task.output,\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo hello world\"],\n                },\n            ],\n        },\n        network_configuration={\"awsvpcConfiguration\": {\"subnets\": existing_cluster_subnets}},\n        # [START howto_awslogs_ecs]\n        awslogs_group=log_group_name,\n        awslogs_region=aws_region,\n        awslogs_stream_prefix=f\"ecs/{container_name}\",\n        # [END howto_awslogs_ecs]\n    )\n    # [END howto_operator_ecs_run_task]\n\n    # [START howto_operator_ecs_deregister_task_definition]\n    deregister_task = EcsDeregisterTaskDefinitionOperator(\n        task_id=\"deregister_task\",\n        task_definition=register_task.output,\n    )\n    # [END howto_operator_ecs_deregister_task_definition]\n    deregister_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ecs_delete_cluster]\n    delete_cluster = EcsDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # EcsDeleteClusterOperator waits by default, setting as False to test the Sensor below.\n    delete_cluster.wait_for_completion = False\n\n    # [START howto_operator_ecs_delete_cluster]\n    await_delete_cluster = EcsClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        cluster_name=new_cluster_name,\n        target_state=EcsClusterStates.INACTIVE,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        aws_region,\n        # TEST BODY\n        create_cluster,\n        await_cluster,\n        register_task,\n        await_task_definition,\n        run_task,\n        deregister_task,\n        delete_cluster,\n        await_delete_cluster,\n        clean_logs(log_group_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ecs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS ECS operators, involves cloud service integration, includes sensors and task registration with custom logging configuration", "key_concepts": ["AWS ECS Integration", "Task Definitions", "Cloud Service Orchestration", "Airflow Sensors", "Logging Configuration"], "learning_objectives": ["Understand ECS cluster management with Airflow", "Learn AWS provider operator usage", "Implement cloud service task orchestration", "Configure container logging strategies"]}}
{"instruction": "Design an Airflow DAG that demonstrates dynamic cloud resource management using AWS ECS, focusing on cluster lifecycle management, task registration, and clean-up procedures.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EcsClusterStateSensor", "EcsCreateClusterOperator", "EcsDeleteClusterOperator", "EcsDeregisterTaskDefinitionOperator", "EcsRegisterTaskDefinitionOperator", "EcsRunTaskOperator", "EcsTaskDefinitionStateSensor"], "line_count": 208, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates\nfrom airflow.providers.amazon.aws.operators.ecs import (\n    EcsCreateClusterOperator,\n    EcsDeleteClusterOperator,\n    EcsDeregisterTaskDefinitionOperator,\n    EcsRegisterTaskDefinitionOperator,\n    EcsRunTaskOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ecs import (\n    EcsClusterStateSensor,\n    EcsTaskDefinitionStateSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ecs\"\n\n# Externally fetched variables:\nEXISTING_CLUSTER_NAME_KEY = \"CLUSTER_NAME\"\nEXISTING_CLUSTER_SUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    # NOTE:  Creating a functional ECS Cluster which uses EC2 requires manually creating\n    # and configuring a number of resources such as autoscaling groups, networking\n    # etc. which is out of scope for this demo and time-consuming for a system test\n    # To simplify this demo and make it run in a reasonable length of time as a\n    # system test, follow the steps below to create a new cluster on the AWS Console\n    # which handles all asset creation and configuration using default values:\n    # 1. https://us-east-1.console.aws.amazon.com/ecs/home?region=us-east-1#/clusters\n    # 2. Select \"EC2 Linux + Networking\" and hit \"Next\"\n    # 3. Name your cluster in the first field and click Create\n    .add_variable(EXISTING_CLUSTER_NAME_KEY)\n    .add_variable(EXISTING_CLUSTER_SUBNETS_KEY, split_string=True)\n    .build()\n)\n\n\n@task\ndef get_region():\n    return boto3.session.Session().region_name\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef clean_logs(group_name: str):\n    client = boto3.client(\"logs\")\n    client.delete_log_group(logGroupName=group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    existing_cluster_name = test_context[EXISTING_CLUSTER_NAME_KEY]\n    existing_cluster_subnets = test_context[EXISTING_CLUSTER_SUBNETS_KEY]\n\n    new_cluster_name = f\"{env_id}-cluster\"\n    container_name = f\"{env_id}-container\"\n    family_name = f\"{env_id}-task-definition\"\n    asg_name = f\"{env_id}-asg\"\n\n    aws_region = get_region()\n    log_group_name = f\"/ecs_test/{env_id}\"\n\n    # [START howto_operator_ecs_create_cluster]\n    create_cluster = EcsCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_create_cluster]\n\n    # EcsCreateClusterOperator waits by default, setting as False to test the Sensor below.\n    create_cluster.wait_for_completion = False\n\n    # [START howto_sensor_ecs_cluster_state]\n    await_cluster = EcsClusterStateSensor(\n        task_id=\"await_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_sensor_ecs_cluster_state]\n\n    # [START howto_operator_ecs_register_task_definition]\n    register_task = EcsRegisterTaskDefinitionOperator(\n        task_id=\"register_task\",\n        family=family_name,\n        container_definitions=[\n            {\n                \"name\": container_name,\n                \"image\": \"ubuntu\",\n                \"workingDirectory\": \"/usr/bin\",\n                \"entryPoint\": [\"sh\", \"-c\"],\n                \"command\": [\"ls\"],\n                \"logConfiguration\": {\n                    \"logDriver\": \"awslogs\",\n                    \"options\": {\n                        \"awslogs-group\": log_group_name,\n                        \"awslogs-region\": aws_region,\n                        \"awslogs-create-group\": \"true\",\n                        \"awslogs-stream-prefix\": \"ecs\",\n                    },\n                },\n            },\n        ],\n        register_task_kwargs={\n            \"cpu\": \"256\",\n            \"memory\": \"512\",\n            \"networkMode\": \"awsvpc\",\n        },\n    )\n    # [END howto_operator_ecs_register_task_definition]\n\n    # [START howto_sensor_ecs_task_definition_state]\n    await_task_definition = EcsTaskDefinitionStateSensor(\n        task_id=\"await_task_definition\",\n        task_definition=register_task.output,\n    )\n    # [END howto_sensor_ecs_task_definition_state]\n\n    # [START howto_operator_ecs_run_task]\n    run_task = EcsRunTaskOperator(\n        task_id=\"run_task\",\n        cluster=existing_cluster_name,\n        task_definition=register_task.output,\n        overrides={\n            \"containerOverrides\": [\n                {\n                    \"name\": container_name,\n                    \"command\": [\"echo hello world\"],\n                },\n            ],\n        },\n        network_configuration={\"awsvpcConfiguration\": {\"subnets\": existing_cluster_subnets}},\n        # [START howto_awslogs_ecs]\n        awslogs_group=log_group_name,\n        awslogs_region=aws_region,\n        awslogs_stream_prefix=f\"ecs/{container_name}\",\n        # [END howto_awslogs_ecs]\n    )\n    # [END howto_operator_ecs_run_task]\n\n    # [START howto_operator_ecs_deregister_task_definition]\n    deregister_task = EcsDeregisterTaskDefinitionOperator(\n        task_id=\"deregister_task\",\n        task_definition=register_task.output,\n    )\n    # [END howto_operator_ecs_deregister_task_definition]\n    deregister_task.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ecs_delete_cluster]\n    delete_cluster = EcsDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=new_cluster_name,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # EcsDeleteClusterOperator waits by default, setting as False to test the Sensor below.\n    delete_cluster.wait_for_completion = False\n\n    # [START howto_operator_ecs_delete_cluster]\n    await_delete_cluster = EcsClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        cluster_name=new_cluster_name,\n        target_state=EcsClusterStates.INACTIVE,\n    )\n    # [END howto_operator_ecs_delete_cluster]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        aws_region,\n        # TEST BODY\n        create_cluster,\n        await_cluster,\n        register_task,\n        await_task_definition,\n        run_task,\n        deregister_task,\n        delete_cluster,\n        await_delete_cluster,\n        clean_logs(log_group_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ecs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS operators, uses dynamic resource naming, includes cleanup tasks, demonstrates infrastructure lifecycle management", "key_concepts": ["Infrastructure as Code", "Resource Lifecycle Management", "AWS Cloud Automation", "Dynamic Task Configuration", "Cleanup Procedures"], "learning_objectives": ["Implement dynamic cloud resource naming", "Understand infrastructure lifecycle in Airflow", "Learn resource provisioning and deprovisioning", "Practice cloud service task orchestration"]}}
{"instruction": "Build an AWS EKS Cluster Management Workflow that demonstrates creating, running a pod, and then deleting an EKS cluster using templated configuration passed dynamically at runtime.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateNodegroupOperator", "EksDeleteClusterOperator", "EksDeleteNodegroupOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 146, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_eks_templated\"\n\n# Example Jinja Template format, substitute your values:\n# {\n#     \"cluster_name\": \"templated-cluster\",\n#     \"cluster_role_arn\": \"arn:aws:iam::123456789012:role/role_name\",\n#     \"resources_vpc_config\": {\n#         \"subnetIds\": [\"subnet-12345ab\", \"subnet-67890cd\"],\n#         \"endpointPublicAccess\": true,\n#         \"endpointPrivateAccess\": false\n#     },\n#     \"nodegroup_name\": \"templated-nodegroup\",\n#     \"nodegroup_subnets\": \"['subnet-12345ab', 'subnet-67890cd']\",\n#     \"nodegroup_role_arn\": \"arn:aws:iam::123456789012:role/role_name\"\n# }\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"templated\"],\n    catchup=False,\n    # render_template_as_native_obj=True is what converts the Jinja to Python objects, instead of a string.\n    render_template_as_native_obj=True,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    CLUSTER_NAME = \"{{ dag_run.conf['cluster_name'] }}\"\n    NODEGROUP_NAME = \"{{ dag_run.conf['nodegroup_name'] }}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        compute=None,\n        cluster_role_arn=\"{{ dag_run.conf['cluster_role_arn'] }}\",\n        # This only works with render_template_as_native_obj flag (this dag has it set)\n        resources_vpc_config=\"{{ dag_run.conf['resources_vpc_config'] }}\",  # type: ignore[arg-type]\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        nodegroup_subnets=\"{{ dag_run.conf['nodegroup_subnets'] }}\",\n        nodegroup_role_arn=\"{{ dag_run.conf['nodegroup_role_arn'] }}\",\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_create_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=CLUSTER_NAME,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"ls\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Delete the pod when it reaches its final state, or the execution is interrupted.\n        on_finish_action=\"delete_pod\",\n    )\n\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n    )\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_delete_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.NONEXISTENT,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        delete_nodegroup,\n        await_delete_nodegroup,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_templated.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EKS operators, involves cloud service integration, includes sensors for state tracking, and leverages Jinja templating for dynamic configuration", "key_concepts": ["AWS EKS Operators", "Dynamic Templating", "Cloud Infrastructure Orchestration", "Airflow Sensors", "Runtime Configuration"], "learning_objectives": ["Understand EKS cluster lifecycle management", "Learn dynamic task configuration with Jinja templates", "Practice cloud service orchestration in Airflow"]}}
{"instruction": "Design an Infrastructure-as-Code Workflow that demonstrates secure, templated Kubernetes cluster provisioning with dynamic runtime parameters and controlled pod execution.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateNodegroupOperator", "EksDeleteClusterOperator", "EksDeleteNodegroupOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 146, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\n\n# mypy ignore arg types (for templated fields)\n# type: ignore[arg-type]\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_eks_templated\"\n\n# Example Jinja Template format, substitute your values:\n# {\n#     \"cluster_name\": \"templated-cluster\",\n#     \"cluster_role_arn\": \"arn:aws:iam::123456789012:role/role_name\",\n#     \"resources_vpc_config\": {\n#         \"subnetIds\": [\"subnet-12345ab\", \"subnet-67890cd\"],\n#         \"endpointPublicAccess\": true,\n#         \"endpointPrivateAccess\": false\n#     },\n#     \"nodegroup_name\": \"templated-nodegroup\",\n#     \"nodegroup_subnets\": \"['subnet-12345ab', 'subnet-67890cd']\",\n#     \"nodegroup_role_arn\": \"arn:aws:iam::123456789012:role/role_name\"\n# }\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\", \"templated\"],\n    catchup=False,\n    # render_template_as_native_obj=True is what converts the Jinja to Python objects, instead of a string.\n    render_template_as_native_obj=True,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    CLUSTER_NAME = \"{{ dag_run.conf['cluster_name'] }}\"\n    NODEGROUP_NAME = \"{{ dag_run.conf['nodegroup_name'] }}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n        compute=None,\n        cluster_role_arn=\"{{ dag_run.conf['cluster_role_arn'] }}\",\n        # This only works with render_template_as_native_obj flag (this dag has it set)\n        resources_vpc_config=\"{{ dag_run.conf['resources_vpc_config'] }}\",  # type: ignore[arg-type]\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        nodegroup_subnets=\"{{ dag_run.conf['nodegroup_subnets'] }}\",\n        nodegroup_role_arn=\"{{ dag_run.conf['nodegroup_role_arn'] }}\",\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_create_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=CLUSTER_NAME,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"ls\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Delete the pod when it reaches its final state, or the execution is interrupted.\n        on_finish_action=\"delete_pod\",\n    )\n\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_eks_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n    )\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"wait_for_delete_nodegroup\",\n        cluster_name=CLUSTER_NAME,\n        nodegroup_name=NODEGROUP_NAME,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=CLUSTER_NAME,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=CLUSTER_NAME,\n        target_state=ClusterStates.NONEXISTENT,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        delete_nodegroup,\n        await_delete_nodegroup,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_templated.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Advanced cloud infrastructure management with multiple stateful operators, complex dependency chain, and runtime configuration injection", "key_concepts": ["Infrastructure Automation", "Kubernetes Cluster Management", "Secure Deployment Patterns", "Dynamic Configuration", "Cloud Service Integration"], "learning_objectives": ["Implement infrastructure provisioning workflows", "Understand dynamic configuration strategies", "Learn safe cluster and pod lifecycle management"]}}
{"instruction": "Design an automated EC2 instance lifecycle management workflow that demonstrates provisioning, stopping, and starting an AWS EC2 instance using Airflow operators, focusing on cloud infrastructure orchestration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EC2CreateInstanceOperator", "EC2InstanceStateSensor", "EC2StartInstanceOperator", "EC2StopInstanceOperator", "EC2TerminateInstanceOperator"], "line_count": 162, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom operator import itemgetter\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    EC2CreateInstanceOperator,\n    EC2StartInstanceOperator,\n    EC2StopInstanceOperator,\n    EC2TerminateInstanceOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ec2 import EC2InstanceStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ec2\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef get_latest_ami_id():\n    \"\"\"Returns the AMI ID of the most recently-created Amazon Linux image\"\"\"\n\n    # Amazon is retiring AL2 in 2023 and replacing it with Amazon Linux 2022.\n    # This image prefix should be futureproof, but may need adjusting depending\n    # on how they name the new images.  This page should have AL2022 info when\n    # it comes available: https://aws.amazon.com/linux/amazon-linux-2022/faqs/\n    image_prefix = \"Amazon Linux*\"\n\n    images = boto3.client(\"ec2\").describe_images(\n        Filters=[\n            {\"Name\": \"description\", \"Values\": [image_prefix]},\n            {\"Name\": \"architecture\", \"Values\": [\"arm64\"]},\n        ],\n        Owners=[\"amazon\"],\n    )\n    # Sort on CreationDate\n    return max(images[\"Images\"], key=itemgetter(\"CreationDate\"))[\"ImageId\"]\n\n\n@task\ndef create_key_pair(key_name: str):\n    client = boto3.client(\"ec2\")\n\n    key_pair_id = client.create_key_pair(KeyName=key_name)[\"KeyName\"]\n    # Creating the key takes a very short but measurable time, preventing race condition:\n    client.get_waiter(\"key_pair_exists\").wait(KeyNames=[key_pair_id])\n\n    return key_pair_id\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_key_pair(key_pair_id: str):\n    boto3.client(\"ec2\").delete_key_pair(KeyName=key_pair_id)\n\n\n@task\ndef parse_response(instance_ids: list):\n    return instance_ids[0]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    instance_name = f\"{env_id}-instance\"\n    key_name = create_key_pair(key_name=f\"{env_id}_key_pair\")\n    image_id = get_latest_ami_id()\n\n    config = {\n        \"InstanceType\": \"t4g.micro\",\n        \"KeyName\": key_name,\n        \"TagSpecifications\": [\n            {\"ResourceType\": \"instance\", \"Tags\": [{\"Key\": \"Name\", \"Value\": instance_name}]}\n        ],\n        # Use IMDSv2 for greater security, see the following doc for more details:\n        # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html\n        \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n    }\n\n    # EC2CreateInstanceOperator creates and starts the EC2 instances. To test the EC2StartInstanceOperator,\n    # we will stop the instance, then start them again before terminating them.\n\n    # [START howto_operator_ec2_create_instance]\n    create_instance = EC2CreateInstanceOperator(\n        task_id=\"create_instance\",\n        image_id=image_id,\n        max_count=1,\n        min_count=1,\n        config=config,\n    )\n    # [END howto_operator_ec2_create_instance]\n    create_instance.wait_for_completion = True\n    instance_id = parse_response(create_instance.output)\n    # [START howto_operator_ec2_stop_instance]\n    stop_instance = EC2StopInstanceOperator(\n        task_id=\"stop_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_stop_instance]\n    stop_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ec2_start_instance]\n    start_instance = EC2StartInstanceOperator(\n        task_id=\"start_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_start_instance]\n\n    # [START howto_sensor_ec2_instance_state]\n    await_instance = EC2InstanceStateSensor(\n        task_id=\"await_instance\",\n        instance_id=instance_id,\n        target_state=\"running\",\n    )\n    # [END howto_sensor_ec2_instance_state]\n\n    # [START howto_operator_ec2_terminate_instance]\n    terminate_instance = EC2TerminateInstanceOperator(\n        task_id=\"terminate_instance\",\n        instance_ids=instance_id,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_ec2_terminate_instance]\n    terminate_instance.trigger_rule = TriggerRule.ALL_DONE\n    chain(\n        # TEST SETUP\n        test_context,\n        key_name,\n        image_id,\n        # TEST BODY\n        create_instance,\n        instance_id,\n        stop_instance,\n        start_instance,\n        await_instance,\n        terminate_instance,\n        # TEST TEARDOWN\n        delete_key_pair(key_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ec2.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EC2 operators, involves cloud service integration, includes custom tasks for AMI selection and key pair management, demonstrates intermediate Airflow workflow complexity", "key_concepts": ["AWS Integration", "Cloud Infrastructure Automation", "Airflow Operators", "Task Dependencies", "Dynamic Resource Provisioning"], "learning_objectives": ["Understand EC2 instance lifecycle management", "Learn AWS service integration in Airflow", "Practice dynamic task creation", "Implement cloud infrastructure workflows"]}}
{"instruction": "Create a secure EC2 instance deployment workflow that demonstrates best practices in cloud resource management, including dynamic AMI selection, secure metadata configuration, and automated key pair management.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EC2CreateInstanceOperator", "EC2InstanceStateSensor", "EC2StartInstanceOperator", "EC2StopInstanceOperator", "EC2TerminateInstanceOperator"], "line_count": 162, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom operator import itemgetter\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.ec2 import (\n    EC2CreateInstanceOperator,\n    EC2StartInstanceOperator,\n    EC2StopInstanceOperator,\n    EC2TerminateInstanceOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.ec2 import EC2InstanceStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_ec2\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef get_latest_ami_id():\n    \"\"\"Returns the AMI ID of the most recently-created Amazon Linux image\"\"\"\n\n    # Amazon is retiring AL2 in 2023 and replacing it with Amazon Linux 2022.\n    # This image prefix should be futureproof, but may need adjusting depending\n    # on how they name the new images.  This page should have AL2022 info when\n    # it comes available: https://aws.amazon.com/linux/amazon-linux-2022/faqs/\n    image_prefix = \"Amazon Linux*\"\n\n    images = boto3.client(\"ec2\").describe_images(\n        Filters=[\n            {\"Name\": \"description\", \"Values\": [image_prefix]},\n            {\"Name\": \"architecture\", \"Values\": [\"arm64\"]},\n        ],\n        Owners=[\"amazon\"],\n    )\n    # Sort on CreationDate\n    return max(images[\"Images\"], key=itemgetter(\"CreationDate\"))[\"ImageId\"]\n\n\n@task\ndef create_key_pair(key_name: str):\n    client = boto3.client(\"ec2\")\n\n    key_pair_id = client.create_key_pair(KeyName=key_name)[\"KeyName\"]\n    # Creating the key takes a very short but measurable time, preventing race condition:\n    client.get_waiter(\"key_pair_exists\").wait(KeyNames=[key_pair_id])\n\n    return key_pair_id\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_key_pair(key_pair_id: str):\n    boto3.client(\"ec2\").delete_key_pair(KeyName=key_pair_id)\n\n\n@task\ndef parse_response(instance_ids: list):\n    return instance_ids[0]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    instance_name = f\"{env_id}-instance\"\n    key_name = create_key_pair(key_name=f\"{env_id}_key_pair\")\n    image_id = get_latest_ami_id()\n\n    config = {\n        \"InstanceType\": \"t4g.micro\",\n        \"KeyName\": key_name,\n        \"TagSpecifications\": [\n            {\"ResourceType\": \"instance\", \"Tags\": [{\"Key\": \"Name\", \"Value\": instance_name}]}\n        ],\n        # Use IMDSv2 for greater security, see the following doc for more details:\n        # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html\n        \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n    }\n\n    # EC2CreateInstanceOperator creates and starts the EC2 instances. To test the EC2StartInstanceOperator,\n    # we will stop the instance, then start them again before terminating them.\n\n    # [START howto_operator_ec2_create_instance]\n    create_instance = EC2CreateInstanceOperator(\n        task_id=\"create_instance\",\n        image_id=image_id,\n        max_count=1,\n        min_count=1,\n        config=config,\n    )\n    # [END howto_operator_ec2_create_instance]\n    create_instance.wait_for_completion = True\n    instance_id = parse_response(create_instance.output)\n    # [START howto_operator_ec2_stop_instance]\n    stop_instance = EC2StopInstanceOperator(\n        task_id=\"stop_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_stop_instance]\n    stop_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_ec2_start_instance]\n    start_instance = EC2StartInstanceOperator(\n        task_id=\"start_instance\",\n        instance_id=instance_id,\n    )\n    # [END howto_operator_ec2_start_instance]\n\n    # [START howto_sensor_ec2_instance_state]\n    await_instance = EC2InstanceStateSensor(\n        task_id=\"await_instance\",\n        instance_id=instance_id,\n        target_state=\"running\",\n    )\n    # [END howto_sensor_ec2_instance_state]\n\n    # [START howto_operator_ec2_terminate_instance]\n    terminate_instance = EC2TerminateInstanceOperator(\n        task_id=\"terminate_instance\",\n        instance_ids=instance_id,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_ec2_terminate_instance]\n    terminate_instance.trigger_rule = TriggerRule.ALL_DONE\n    chain(\n        # TEST SETUP\n        test_context,\n        key_name,\n        image_id,\n        # TEST BODY\n        create_instance,\n        instance_id,\n        stop_instance,\n        start_instance,\n        await_instance,\n        terminate_instance,\n        # TEST TEARDOWN\n        delete_key_pair(key_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ec2.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves security-focused configuration, custom task functions, multiple AWS service interactions, and demonstrates intermediate Airflow workflow patterns", "key_concepts": ["Cloud Security", "Dynamic Resource Configuration", "AWS Security Best Practices", "Airflow Task Decorators", "Metadata Management"], "learning_objectives": ["Implement secure cloud resource provisioning", "Understand dynamic AMI selection strategies", "Learn IMDSv2 security configuration", "Practice key pair management in cloud workflows"]}}
{"instruction": "Build an Airflow DAG that demonstrates creating an Amazon EKS cluster with Fargate profile, running a test pod, and then cleaning up the infrastructure using AWS EKS operators.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksFargateProfileStateSensor", "EksPodOperator"], "line_count": 137, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_in_one_step\"\n\n# Externally fetched variables\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # [START howto_operator_eks_create_cluster_with_fargate_profile]\n    # Create an Amazon EKS cluster control plane and an AWS Fargate compute platform in one step.\n    create_cluster_and_fargate_profile = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster_and_fargate_profile\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=\"fargate\",\n        fargate_profile_name=fargate_profile_name,\n        # Opting to use the same ARN for the cluster and the pod here,\n        # but a different ARN could be configured and passed if desired.\n        fargate_pod_execution_role_arn=fargate_pod_role_arn,\n    )\n    # [END howto_operator_eks_create_cluster_with_fargate_profile]\n\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"await_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        pod_name=\"run_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        startup_timeout_seconds=600,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_cluster_and_fargate_profile = EksDeleteClusterOperator(\n        task_id=\"delete_fargate_profile_and_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster_and_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_cluster_and_fargate_profile,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_fargate_in_one_step.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EKS operators, involves cloud service integration, includes sensors and conditional task execution, requires external configuration", "key_concepts": ["AWS EKS Operators", "Cloud Infrastructure Orchestration", "Kubernetes Pod Management", "Sensor Operators", "Infrastructure as Code"], "learning_objectives": ["Understand EKS cluster creation workflow", "Learn how to manage Kubernetes infrastructure with Airflow", "Practice using AWS-specific Airflow operators", "Implement infrastructure lifecycle management"]}}
{"instruction": "Design an Airflow DAG that demonstrates secure, automated deployment of a containerized workload on AWS Fargate using dynamic configuration and cleanup mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksFargateProfileStateSensor", "EksPodOperator"], "line_count": 137, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_in_one_step\"\n\n# Externally fetched variables\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # [START howto_operator_eks_create_cluster_with_fargate_profile]\n    # Create an Amazon EKS cluster control plane and an AWS Fargate compute platform in one step.\n    create_cluster_and_fargate_profile = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster_and_fargate_profile\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=\"fargate\",\n        fargate_profile_name=fargate_profile_name,\n        # Opting to use the same ARN for the cluster and the pod here,\n        # but a different ARN could be configured and passed if desired.\n        fargate_pod_execution_role_arn=fargate_pod_role_arn,\n    )\n    # [END howto_operator_eks_create_cluster_with_fargate_profile]\n\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"await_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        pod_name=\"run_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        startup_timeout_seconds=600,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_cluster_and_fargate_profile = EksDeleteClusterOperator(\n        task_id=\"delete_fargate_profile_and_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster_and_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_cluster_and_fargate_profile,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_fargate_in_one_step.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex AWS configuration, uses multiple operators with conditional execution, requires external role and network configurations", "key_concepts": ["Container Deployment", "AWS Fargate", "Dynamic Configuration", "Infrastructure Teardown", "Secure Workload Execution"], "learning_objectives": ["Understand container deployment strategies", "Learn dynamic infrastructure provisioning", "Practice secure cloud resource management", "Implement automated infrastructure lifecycle"]}}
{"instruction": "Build an AWS EKS Cluster Deployment DAG that demonstrates creating an EKS cluster, configuring a Fargate profile, and running a test pod using AWS Airflow providers.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateFargateProfileOperator", "EksDeleteClusterOperator", "EksDeleteFargateProfileOperator", "EksFargateProfileStateSensor", "EksPodOperator"], "line_count": 173, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateFargateProfileOperator,\n    EksDeleteClusterOperator,\n    EksDeleteFargateProfileOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_profile\"\n\n# Externally fetched variables:\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nSELECTORS = [{\"namespace\": \"default\"}]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=None,\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    # [START howto_operator_eks_create_fargate_profile]\n    create_fargate_profile = EksCreateFargateProfileOperator(\n        task_id=\"create_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        pod_execution_role_arn=fargate_pod_role_arn,\n        fargate_profile_name=fargate_profile_name,\n        selectors=SELECTORS,\n    )\n    # [END howto_operator_eks_create_fargate_profile]\n\n    # [START howto_sensor_eks_fargate]\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_fargate]\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=cluster_name,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n        startup_timeout_seconds=200,\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile = EksDeleteFargateProfileOperator(\n        task_id=\"delete_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n    )\n    # [END howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_delete_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=cluster_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEARDOWN\n        describe_pod,\n        delete_fargate_profile,  # part of the test AND teardown\n        await_delete_fargate_profile,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_fargate_profile.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EKS operators, involves cloud service integration, requires external role and subnet configurations, and includes sensor-based state tracking", "key_concepts": ["AWS EKS Operators", "Fargate Profiles", "Cloud Service Orchestration", "Kubernetes Pod Deployment", "State Sensors"], "learning_objectives": ["Understand EKS cluster creation workflow", "Learn AWS provider operator usage", "Implement cloud infrastructure as code", "Explore Airflow's cloud service integration"]}}
{"instruction": "Design an Airflow DAG that demonstrates advanced cloud infrastructure provisioning using AWS EKS, focusing on dynamic configuration management and infrastructure state monitoring.", "input": {"airflow_version": "2.7.2", "operators": ["EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateFargateProfileOperator", "EksDeleteClusterOperator", "EksDeleteFargateProfileOperator", "EksFargateProfileStateSensor", "EksPodOperator"], "line_count": 173, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, FargateProfileStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateFargateProfileOperator,\n    EksDeleteClusterOperator,\n    EksDeleteFargateProfileOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksFargateProfileStateSensor\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_fargate_profile\"\n\n# Externally fetched variables:\n# See https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\nCLUSTER_ROLE_ARN_KEY = \"CLUSTER_ROLE_ARN\"\n# See https://docs.aws.amazon.com/eks/latest/userguide/pod-execution-role.html\nFARGATE_POD_ROLE_ARN_KEY = \"FARGATE_POD_ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(CLUSTER_ROLE_ARN_KEY)\n    .add_variable(FARGATE_POD_ROLE_ARN_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nSELECTORS = [{\"namespace\": \"default\"}]\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    cluster_role_arn = test_context[CLUSTER_ROLE_ARN_KEY]\n    fargate_pod_role_arn = test_context[FARGATE_POD_ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    fargate_profile_name = f\"{env_id}-profile\"\n    test_name = f\"{env_id}_{DAG_ID}\"\n\n    # Create an Amazon EKS Cluster control plane without attaching a compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_eks_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=cluster_role_arn,\n        resources_vpc_config={\n            \"subnetIds\": subnets,\n            \"endpointPublicAccess\": True,\n            \"endpointPrivateAccess\": False,\n        },\n        compute=None,\n    )\n\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n\n    # [START howto_operator_eks_create_fargate_profile]\n    create_fargate_profile = EksCreateFargateProfileOperator(\n        task_id=\"create_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        pod_execution_role_arn=fargate_pod_role_arn,\n        fargate_profile_name=fargate_profile_name,\n        selectors=SELECTORS,\n    )\n    # [END howto_operator_eks_create_fargate_profile]\n\n    # [START howto_sensor_eks_fargate]\n    await_create_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_create_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_fargate]\n\n    start_pod = EksPodOperator(\n        task_id=\"run_pod\",\n        cluster_name=cluster_name,\n        pod_name=\"run_pod\",\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n        startup_timeout_seconds=200,\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile = EksDeleteFargateProfileOperator(\n        task_id=\"delete_eks_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n    )\n    # [END howto_operator_eks_delete_fargate_profile]\n    delete_fargate_profile.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_fargate_profile = EksFargateProfileStateSensor(\n        task_id=\"wait_for_delete_fargate_profile\",\n        cluster_name=cluster_name,\n        fargate_profile_name=fargate_profile_name,\n        target_state=FargateProfileStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=cluster_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"wait_for_delete_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_fargate_profile,\n        await_create_fargate_profile,\n        start_pod,\n        # TEARDOWN\n        describe_pod,\n        delete_fargate_profile,  # part of the test AND teardown\n        await_delete_fargate_profile,\n        delete_cluster,\n        await_delete_cluster,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_fargate_profile.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex cloud resource management, uses multiple stateful operators, requires external context building, and demonstrates infrastructure-as-code principles", "key_concepts": ["Dynamic Configuration", "Infrastructure Provisioning", "Cloud Resource State Management", "AWS Service Integration", "Kubernetes Deployment"], "learning_objectives": ["Understand cloud resource lifecycle management", "Learn dynamic configuration techniques", "Explore state tracking in cloud deployments", "Master AWS provider operator interactions"]}}
{"instruction": "Build an EMR Serverless workflow that demonstrates creating an AWS Spark application, running a word count job, and managing the application lifecycle using Airflow operators and sensors.", "input": {"airflow_version": "2.7.2", "operators": ["EmrServerlessApplicationSensor", "EmrServerlessCreateApplicationOperator", "EmrServerlessDeleteApplicationOperator", "EmrServerlessJobSensor", "EmrServerlessStartJobOperator", "EmrServerlessStopApplicationOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 149, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrServerlessCreateApplicationOperator,\n    EmrServerlessDeleteApplicationOperator,\n    EmrServerlessStartJobOperator,\n    EmrServerlessStopApplicationOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrServerlessApplicationSensor, EmrServerlessJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_serverless\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"{env_id}-emr-serverless-bucket\"\n    region = boto3.session.Session().region_name\n    entryPoint = f\"s3://{region}.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py\"\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    SPARK_JOB_DRIVER = {\n        \"sparkSubmit\": {\n            \"entryPoint\": entryPoint,\n            \"entryPointArguments\": [f\"s3://{bucket_name}/output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g\\\n                --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\",\n        }\n    }\n\n    SPARK_CONFIGURATION_OVERRIDES = {\n        \"monitoringConfiguration\": {\"s3MonitoringConfiguration\": {\"logUri\": f\"s3://{bucket_name}/logs\"}}\n    }\n\n    # [START howto_operator_emr_serverless_create_application]\n    emr_serverless_app = EmrServerlessCreateApplicationOperator(\n        task_id=\"create_emr_serverless_task\",\n        release_label=\"emr-6.6.0\",\n        job_type=\"SPARK\",\n        config={\"name\": \"new_application\"},\n    )\n    # [END howto_operator_emr_serverless_create_application]\n\n    # EmrServerlessCreateApplicationOperator waits by default, setting as False to test the Sensor below.\n    emr_serverless_app.wait_for_completion = False\n\n    emr_serverless_app_id = emr_serverless_app.output\n\n    # [START howto_sensor_emr_serverless_application]\n    wait_for_app_creation = EmrServerlessApplicationSensor(\n        task_id=\"wait_for_app_creation\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_sensor_emr_serverless_application]\n    wait_for_app_creation.poke_interval = 1\n\n    # [START howto_operator_emr_serverless_start_job]\n    start_job = EmrServerlessStartJobOperator(\n        task_id=\"start_emr_serverless_job\",\n        application_id=emr_serverless_app_id,\n        execution_role_arn=role_arn,\n        job_driver=SPARK_JOB_DRIVER,\n        configuration_overrides=SPARK_CONFIGURATION_OVERRIDES,\n    )\n    # [END howto_operator_emr_serverless_start_job]\n    start_job.wait_for_completion = False\n\n    # [START howto_sensor_emr_serverless_job]\n    wait_for_job = EmrServerlessJobSensor(\n        task_id=\"wait_for_job\",\n        application_id=emr_serverless_app_id,\n        job_run_id=start_job.output,\n        # the default is to wait for job completion, here we just wait for the job to be running.\n        target_states={\"RUNNING\"},\n    )\n    # [END howto_sensor_emr_serverless_job]\n    wait_for_job.poke_interval = 10\n\n    # [START howto_operator_emr_serverless_stop_application]\n    stop_app = EmrServerlessStopApplicationOperator(\n        task_id=\"stop_application\",\n        application_id=emr_serverless_app_id,\n        force_stop=True,\n    )\n    # [END howto_operator_emr_serverless_stop_application]\n    stop_app.waiter_check_interval_seconds = 1\n\n    # [START howto_operator_emr_serverless_delete_application]\n    delete_app = EmrServerlessDeleteApplicationOperator(\n        task_id=\"delete_application\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_operator_emr_serverless_delete_application]\n    delete_app.waiter_check_interval_seconds = 1\n    delete_app.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        emr_serverless_app,\n        wait_for_app_creation,\n        start_job,\n        wait_for_job,\n        stop_app,\n        # TEST TEARDOWN\n        delete_app,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_serverless.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EMR Serverless operators, involves cloud service integration, includes sensors for job and application tracking, and requires configuration of Spark job parameters", "key_concepts": ["EMR Serverless", "AWS Integration", "Spark Job Orchestration", "Airflow Sensors", "Cloud Workflow Management"], "learning_objectives": ["Understand EMR Serverless application creation", "Learn how to configure and submit Spark jobs via Airflow", "Master using sensors for asynchronous job tracking"]}}
{"instruction": "Design a cloud-native data processing pipeline that demonstrates advanced Airflow task coordination, focusing on dynamic application management and monitoring of distributed computing jobs.", "input": {"airflow_version": "2.7.2", "operators": ["EmrServerlessApplicationSensor", "EmrServerlessCreateApplicationOperator", "EmrServerlessDeleteApplicationOperator", "EmrServerlessJobSensor", "EmrServerlessStartJobOperator", "EmrServerlessStopApplicationOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 149, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrServerlessCreateApplicationOperator,\n    EmrServerlessDeleteApplicationOperator,\n    EmrServerlessStartJobOperator,\n    EmrServerlessStopApplicationOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrServerlessApplicationSensor, EmrServerlessJobSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_serverless\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    bucket_name = f\"{env_id}-emr-serverless-bucket\"\n    region = boto3.session.Session().region_name\n    entryPoint = f\"s3://{region}.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py\"\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    SPARK_JOB_DRIVER = {\n        \"sparkSubmit\": {\n            \"entryPoint\": entryPoint,\n            \"entryPointArguments\": [f\"s3://{bucket_name}/output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g\\\n                --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\",\n        }\n    }\n\n    SPARK_CONFIGURATION_OVERRIDES = {\n        \"monitoringConfiguration\": {\"s3MonitoringConfiguration\": {\"logUri\": f\"s3://{bucket_name}/logs\"}}\n    }\n\n    # [START howto_operator_emr_serverless_create_application]\n    emr_serverless_app = EmrServerlessCreateApplicationOperator(\n        task_id=\"create_emr_serverless_task\",\n        release_label=\"emr-6.6.0\",\n        job_type=\"SPARK\",\n        config={\"name\": \"new_application\"},\n    )\n    # [END howto_operator_emr_serverless_create_application]\n\n    # EmrServerlessCreateApplicationOperator waits by default, setting as False to test the Sensor below.\n    emr_serverless_app.wait_for_completion = False\n\n    emr_serverless_app_id = emr_serverless_app.output\n\n    # [START howto_sensor_emr_serverless_application]\n    wait_for_app_creation = EmrServerlessApplicationSensor(\n        task_id=\"wait_for_app_creation\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_sensor_emr_serverless_application]\n    wait_for_app_creation.poke_interval = 1\n\n    # [START howto_operator_emr_serverless_start_job]\n    start_job = EmrServerlessStartJobOperator(\n        task_id=\"start_emr_serverless_job\",\n        application_id=emr_serverless_app_id,\n        execution_role_arn=role_arn,\n        job_driver=SPARK_JOB_DRIVER,\n        configuration_overrides=SPARK_CONFIGURATION_OVERRIDES,\n    )\n    # [END howto_operator_emr_serverless_start_job]\n    start_job.wait_for_completion = False\n\n    # [START howto_sensor_emr_serverless_job]\n    wait_for_job = EmrServerlessJobSensor(\n        task_id=\"wait_for_job\",\n        application_id=emr_serverless_app_id,\n        job_run_id=start_job.output,\n        # the default is to wait for job completion, here we just wait for the job to be running.\n        target_states={\"RUNNING\"},\n    )\n    # [END howto_sensor_emr_serverless_job]\n    wait_for_job.poke_interval = 10\n\n    # [START howto_operator_emr_serverless_stop_application]\n    stop_app = EmrServerlessStopApplicationOperator(\n        task_id=\"stop_application\",\n        application_id=emr_serverless_app_id,\n        force_stop=True,\n    )\n    # [END howto_operator_emr_serverless_stop_application]\n    stop_app.waiter_check_interval_seconds = 1\n\n    # [START howto_operator_emr_serverless_delete_application]\n    delete_app = EmrServerlessDeleteApplicationOperator(\n        task_id=\"delete_application\",\n        application_id=emr_serverless_app_id,\n    )\n    # [END howto_operator_emr_serverless_delete_application]\n    delete_app.waiter_check_interval_seconds = 1\n    delete_app.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        emr_serverless_app,\n        wait_for_app_creation,\n        start_job,\n        wait_for_job,\n        stop_app,\n        # TEST TEARDOWN\n        delete_app,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_serverless.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex task chaining, uses multiple AWS operators, implements application lifecycle management with sensors and conditional execution", "key_concepts": ["Cloud Workflow Orchestration", "Dynamic Job Management", "Asynchronous Task Execution", "AWS Service Integration", "Distributed Computing"], "learning_objectives": ["Explore advanced Airflow task dependencies", "Understand application state management", "Learn real-world cloud job orchestration patterns"]}}
{"instruction": "Build an AWS EKS Cluster Deployment DAG that demonstrates creating a Kubernetes cluster, adding a nodegroup, and launching a pod using Airflow's AWS EKS operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateNodegroupOperator", "EksDeleteClusterOperator", "EksDeleteNodegroupOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 196, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\nDAG_ID = \"example_eks_with_nodegroups\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster]\n    # Create an Amazon EKS Cluster control plane without attaching compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        compute=None,\n    )\n    # [END howto_operator_eks_create_cluster]\n\n    # [START howto_sensor_eks_cluster]\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"await_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_cluster]\n\n    # [START howto_operator_eks_create_nodegroup]\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        nodegroup_subnets=test_context[SUBNETS_KEY],\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n    )\n    # [END howto_operator_eks_create_nodegroup]\n    # The launch template enforces IMDSv2 and is required for internal compliance\n    # when running these system tests on AWS infrastructure.  It is not required\n    # for the operator to work, so I'm placing it outside the demo snippet.\n    create_nodegroup.create_nodegroup_kwargs = {\"launchTemplate\": {\"name\": launch_template_name}}\n\n    # [START howto_sensor_eks_nodegroup]\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_nodegroup]\n    await_create_nodegroup.poke_interval = 10\n\n    # [START howto_operator_eks_pod_operator]\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n    )\n    # [END howto_operator_eks_pod_operator]\n    # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n    start_pod.is_delete_operator_pod = False\n\n    # In this specific situation we want to keep the pod to be able to describe it,\n    # it is cleaned anyway with the cluster later on.\n    start_pod.is_delete_operator_pod = False\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_nodegroup]\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n    )\n    # [END howto_operator_eks_delete_nodegroup]\n    delete_nodegroup.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_delete_nodegroup\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    # [START howto_operator_eks_delete_cluster]\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=cluster_name,\n    )\n    # [END howto_operator_eks_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup,  # part of the test AND teardown\n        await_delete_nodegroup,\n        delete_cluster,  # part of the test AND teardown\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_nodegroups.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EKS operators, involves cloud service integration, includes sensors for state tracking, and demonstrates complex infrastructure provisioning workflow", "key_concepts": ["AWS EKS Operators", "Cloud Infrastructure Provisioning", "Kubernetes Cluster Management", "State Sensors", "Task Chaining"], "learning_objectives": ["Understand EKS cluster creation workflow", "Learn how to use Airflow AWS providers", "Implement infrastructure-as-code with Airflow", "Manage Kubernetes cluster lifecycle"]}}
{"instruction": "Design an Airflow DAG that demonstrates secure, compliance-driven Kubernetes infrastructure deployment using launch templates and IMDSv2 configuration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksCreateNodegroupOperator", "EksDeleteClusterOperator", "EksDeleteNodegroupOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 196, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksCreateNodegroupOperator,\n    EksDeleteClusterOperator,\n    EksDeleteNodegroupOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\n# Ignore missing args provided by default_args\n# type: ignore[call-arg]\n\nDAG_ID = \"example_eks_with_nodegroups\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster]\n    # Create an Amazon EKS Cluster control plane without attaching compute service.\n    create_cluster = EksCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_name=cluster_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        compute=None,\n    )\n    # [END howto_operator_eks_create_cluster]\n\n    # [START howto_sensor_eks_cluster]\n    await_create_cluster = EksClusterStateSensor(\n        task_id=\"await_create_cluster\",\n        cluster_name=cluster_name,\n        target_state=ClusterStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_cluster]\n\n    # [START howto_operator_eks_create_nodegroup]\n    create_nodegroup = EksCreateNodegroupOperator(\n        task_id=\"create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        nodegroup_subnets=test_context[SUBNETS_KEY],\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n    )\n    # [END howto_operator_eks_create_nodegroup]\n    # The launch template enforces IMDSv2 and is required for internal compliance\n    # when running these system tests on AWS infrastructure.  It is not required\n    # for the operator to work, so I'm placing it outside the demo snippet.\n    create_nodegroup.create_nodegroup_kwargs = {\"launchTemplate\": {\"name\": launch_template_name}}\n\n    # [START howto_sensor_eks_nodegroup]\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n    )\n    # [END howto_sensor_eks_nodegroup]\n    await_create_nodegroup.poke_interval = 10\n\n    # [START howto_operator_eks_pod_operator]\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n    )\n    # [END howto_operator_eks_pod_operator]\n    # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n    start_pod.is_delete_operator_pod = False\n\n    # In this specific situation we want to keep the pod to be able to describe it,\n    # it is cleaned anyway with the cluster later on.\n    start_pod.is_delete_operator_pod = False\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_delete_nodegroup]\n    delete_nodegroup = EksDeleteNodegroupOperator(\n        task_id=\"delete_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n    )\n    # [END howto_operator_eks_delete_nodegroup]\n    delete_nodegroup.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_delete_nodegroup\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.NONEXISTENT,\n    )\n\n    # [START howto_operator_eks_delete_cluster]\n    delete_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_name=cluster_name,\n    )\n    # [END howto_operator_eks_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster,\n        await_create_cluster,\n        create_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup,  # part of the test AND teardown\n        await_delete_nodegroup,\n        delete_cluster,  # part of the test AND teardown\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_nodegroups.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves advanced AWS configuration, uses custom launch template creation, demonstrates security best practices with metadata token requirements", "key_concepts": ["Cloud Security", "Metadata Service Configuration", "Infrastructure Compliance", "AWS Launch Templates", "Secure Kubernetes Deployment"], "learning_objectives": ["Implement secure infrastructure deployment", "Understand IMDSv2 configuration", "Learn dynamic resource management", "Practice infrastructure security patterns"]}}
{"instruction": "Build an AWS EMR (Elastic MapReduce) workflow that demonstrates creating a Spark job cluster, configuring security settings, and executing a Pi calculation step using Airflow's AWS providers.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmrAddStepsOperator", "EmrCreateJobFlowOperator", "EmrJobFlowSensor", "EmrModifyClusterOperator", "EmrStepSensor", "EmrTerminateJobFlowOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 214, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyClusterOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrJobFlowSensor, EmrStepSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr\"\nCONFIG_NAME = \"EMR Runtime Role Security Configuration\"\nEXECUTION_ROLE_ARN_KEY = \"EXECUTION_ROLE_ARN\"\n\nSECURITY_CONFIGURATION = {\n    \"AuthorizationConfiguration\": {\n        \"IAMConfiguration\": {\n            \"EnableApplicationScopedIAMRole\": True,\n        },\n    },\n    # Use IMDSv2 for greater security, see the following doc for more details:\n    # https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html\n    \"InstanceMetadataServiceConfiguration\": {\n        \"MinimumInstanceMetadataServiceVersion\": 2,\n        \"HttpPutResponseHopLimit\": 2,\n    },\n}\n\n# [START howto_operator_emr_steps_config]\nSPARK_STEPS = [\n    {\n        \"Name\": \"calculate_pi\",\n        \"ActionOnFailure\": \"CONTINUE\",\n        \"HadoopJarStep\": {\n            \"Jar\": \"command-runner.jar\",\n            \"Args\": [\"/usr/lib/spark/bin/run-example\", \"SparkPi\", \"10\"],\n        },\n    }\n]\n\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"PiCalc\",\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"InstanceCount\": 1,\n            },\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": SPARK_STEPS,\n    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n    \"ServiceRole\": \"EMR_DefaultRole\",\n}\n# [END howto_operator_emr_steps_config]\n\n\n@task\ndef get_ami_id():\n    \"\"\"\n    Returns an AL2 AMI compatible with EMR\n    \"\"\"\n    return SsmHook(aws_conn_id=None).get_parameter_value(\n        \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-ebs\"\n    )\n\n\n@task\ndef configure_security_config(config_name: str):\n    boto3.client(\"emr\").create_security_configuration(\n        Name=config_name,\n        SecurityConfiguration=json.dumps(SECURITY_CONFIGURATION),\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_config(config_name: str):\n    boto3.client(\"emr\").delete_security_configuration(\n        Name=config_name,\n    )\n\n\n@task\ndef get_step_id(step_ids: list):\n    return step_ids[0]\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    config_name = f\"{CONFIG_NAME}-{env_id}\"\n    execution_role_arn = test_context[EXECUTION_ROLE_ARN_KEY]\n    s3_bucket = f\"{env_id}-emr-bucket\"\n\n    JOB_FLOW_OVERRIDES[\"LogUri\"] = f\"s3://{s3_bucket}/\"\n    JOB_FLOW_OVERRIDES[\"SecurityConfiguration\"] = config_name\n    JOB_FLOW_OVERRIDES[\"Instances\"][\"InstanceGroups\"][0][\"CustomAmiId\"] = get_ami_id()\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    create_security_configuration = configure_security_config(config_name)\n\n    # [START howto_operator_emr_create_job_flow]\n    create_job_flow = EmrCreateJobFlowOperator(\n        task_id=\"create_job_flow\",\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n    )\n    # [END howto_operator_emr_create_job_flow]\n\n    # [START howto_operator_emr_modify_cluster]\n    modify_cluster = EmrModifyClusterOperator(\n        task_id=\"modify_cluster\", cluster_id=create_job_flow.output, step_concurrency_level=1\n    )\n    # [END howto_operator_emr_modify_cluster]\n\n    # [START howto_operator_emr_add_steps]\n    add_steps = EmrAddStepsOperator(\n        task_id=\"add_steps\",\n        job_flow_id=create_job_flow.output,\n        steps=SPARK_STEPS,\n        execution_role_arn=execution_role_arn,\n    )\n    # [END howto_operator_emr_add_steps]\n    add_steps.wait_for_completion = True\n    # On rare occasion (1 in 50ish?) this system test times out.  Extending the\n    # max_attempts from the default 60 to attempt to mitigate the flaky test.\n    add_steps.waiter_max_attempts = 90\n\n    # [START howto_sensor_emr_step]\n    wait_for_step = EmrStepSensor(\n        task_id=\"wait_for_step\",\n        job_flow_id=create_job_flow.output,\n        step_id=get_step_id(add_steps.output),\n    )\n    # [END howto_sensor_emr_step]\n\n    # [START howto_operator_emr_terminate_job_flow]\n    remove_cluster = EmrTerminateJobFlowOperator(\n        task_id=\"remove_cluster\",\n        job_flow_id=create_job_flow.output,\n    )\n    # [END howto_operator_emr_terminate_job_flow]\n    remove_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_emr_job_flow]\n    check_job_flow = EmrJobFlowSensor(task_id=\"check_job_flow\", job_flow_id=create_job_flow.output)\n    # [END howto_sensor_emr_job_flow]\n    check_job_flow.poke_interval = 10\n\n    delete_security_configuration = delete_security_config(config_name)\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        create_security_configuration,\n        # TEST BODY\n        create_job_flow,\n        modify_cluster,\n        add_steps,\n        wait_for_step,\n        # TEST TEARDOWN\n        remove_cluster,\n        check_job_flow,\n        delete_security_configuration,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, includes custom tasks, security configuration, and EMR cluster management with moderate code complexity", "key_concepts": ["AWS EMR Integration", "Airflow Providers", "Task Decorators", "Cloud Orchestration", "Security Configuration"], "learning_objectives": ["Understand EMR cluster creation", "Learn AWS service integration in Airflow", "Implement dynamic task configuration", "Explore security configuration in cloud workflows"]}}
{"instruction": "Design an advanced Airflow DAG that demonstrates dynamic AWS resource management, including programmatic security configuration, AMI selection, and EMR job execution with error handling.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EmrAddStepsOperator", "EmrCreateJobFlowOperator", "EmrJobFlowSensor", "EmrModifyClusterOperator", "EmrStepSensor", "EmrTerminateJobFlowOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 214, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrAddStepsOperator,\n    EmrCreateJobFlowOperator,\n    EmrModifyClusterOperator,\n    EmrTerminateJobFlowOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.emr import EmrJobFlowSensor, EmrStepSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr\"\nCONFIG_NAME = \"EMR Runtime Role Security Configuration\"\nEXECUTION_ROLE_ARN_KEY = \"EXECUTION_ROLE_ARN\"\n\nSECURITY_CONFIGURATION = {\n    \"AuthorizationConfiguration\": {\n        \"IAMConfiguration\": {\n            \"EnableApplicationScopedIAMRole\": True,\n        },\n    },\n    # Use IMDSv2 for greater security, see the following doc for more details:\n    # https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html\n    \"InstanceMetadataServiceConfiguration\": {\n        \"MinimumInstanceMetadataServiceVersion\": 2,\n        \"HttpPutResponseHopLimit\": 2,\n    },\n}\n\n# [START howto_operator_emr_steps_config]\nSPARK_STEPS = [\n    {\n        \"Name\": \"calculate_pi\",\n        \"ActionOnFailure\": \"CONTINUE\",\n        \"HadoopJarStep\": {\n            \"Jar\": \"command-runner.jar\",\n            \"Args\": [\"/usr/lib/spark/bin/run-example\", \"SparkPi\", \"10\"],\n        },\n    }\n]\n\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"PiCalc\",\n    \"ReleaseLabel\": \"emr-6.7.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Primary node\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"InstanceCount\": 1,\n            },\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": SPARK_STEPS,\n    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n    \"ServiceRole\": \"EMR_DefaultRole\",\n}\n# [END howto_operator_emr_steps_config]\n\n\n@task\ndef get_ami_id():\n    \"\"\"\n    Returns an AL2 AMI compatible with EMR\n    \"\"\"\n    return SsmHook(aws_conn_id=None).get_parameter_value(\n        \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-ebs\"\n    )\n\n\n@task\ndef configure_security_config(config_name: str):\n    boto3.client(\"emr\").create_security_configuration(\n        Name=config_name,\n        SecurityConfiguration=json.dumps(SECURITY_CONFIGURATION),\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_config(config_name: str):\n    boto3.client(\"emr\").delete_security_configuration(\n        Name=config_name,\n    )\n\n\n@task\ndef get_step_id(step_ids: list):\n    return step_ids[0]\n\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(EXECUTION_ROLE_ARN_KEY).build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    config_name = f\"{CONFIG_NAME}-{env_id}\"\n    execution_role_arn = test_context[EXECUTION_ROLE_ARN_KEY]\n    s3_bucket = f\"{env_id}-emr-bucket\"\n\n    JOB_FLOW_OVERRIDES[\"LogUri\"] = f\"s3://{s3_bucket}/\"\n    JOB_FLOW_OVERRIDES[\"SecurityConfiguration\"] = config_name\n    JOB_FLOW_OVERRIDES[\"Instances\"][\"InstanceGroups\"][0][\"CustomAmiId\"] = get_ami_id()\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    create_security_configuration = configure_security_config(config_name)\n\n    # [START howto_operator_emr_create_job_flow]\n    create_job_flow = EmrCreateJobFlowOperator(\n        task_id=\"create_job_flow\",\n        job_flow_overrides=JOB_FLOW_OVERRIDES,\n    )\n    # [END howto_operator_emr_create_job_flow]\n\n    # [START howto_operator_emr_modify_cluster]\n    modify_cluster = EmrModifyClusterOperator(\n        task_id=\"modify_cluster\", cluster_id=create_job_flow.output, step_concurrency_level=1\n    )\n    # [END howto_operator_emr_modify_cluster]\n\n    # [START howto_operator_emr_add_steps]\n    add_steps = EmrAddStepsOperator(\n        task_id=\"add_steps\",\n        job_flow_id=create_job_flow.output,\n        steps=SPARK_STEPS,\n        execution_role_arn=execution_role_arn,\n    )\n    # [END howto_operator_emr_add_steps]\n    add_steps.wait_for_completion = True\n    # On rare occasion (1 in 50ish?) this system test times out.  Extending the\n    # max_attempts from the default 60 to attempt to mitigate the flaky test.\n    add_steps.waiter_max_attempts = 90\n\n    # [START howto_sensor_emr_step]\n    wait_for_step = EmrStepSensor(\n        task_id=\"wait_for_step\",\n        job_flow_id=create_job_flow.output,\n        step_id=get_step_id(add_steps.output),\n    )\n    # [END howto_sensor_emr_step]\n\n    # [START howto_operator_emr_terminate_job_flow]\n    remove_cluster = EmrTerminateJobFlowOperator(\n        task_id=\"remove_cluster\",\n        job_flow_id=create_job_flow.output,\n    )\n    # [END howto_operator_emr_terminate_job_flow]\n    remove_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_sensor_emr_job_flow]\n    check_job_flow = EmrJobFlowSensor(task_id=\"check_job_flow\", job_flow_id=create_job_flow.output)\n    # [END howto_sensor_emr_job_flow]\n    check_job_flow.poke_interval = 10\n\n    delete_security_configuration = delete_security_config(config_name)\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        create_security_configuration,\n        # TEST BODY\n        create_job_flow,\n        modify_cluster,\n        add_steps,\n        wait_for_step,\n        # TEST TEARDOWN\n        remove_cluster,\n        check_job_flow,\n        delete_security_configuration,\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Advanced cloud orchestration with multiple custom tasks, dynamic configuration generation, complex AWS service interactions, and sophisticated workflow design", "key_concepts": ["Dynamic Task Generation", "AWS Resource Management", "Security Configuration", "Error Handling", "Cloud Service Automation"], "learning_objectives": ["Master dynamic DAG configuration", "Implement advanced cloud resource management", "Understand programmatic security settings", "Learn complex workflow design patterns"]}}
{"instruction": "Build an EMR on EKS workflow that demonstrates AWS cloud infrastructure provisioning and job execution using Airflow's AWS providers, focusing on creating an EKS cluster, configuring IAM roles, and running a distributed Python job.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksNodegroupStateSensor", "EmrContainerOperator", "EmrContainerSensor", "EmrEksCreateClusterOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 334, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import EksCreateClusterOperator, EksDeleteClusterOperator\nfrom airflow.providers.amazon.aws.operators.emr import EmrContainerOperator, EmrEksCreateClusterOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.providers.amazon.aws.sensors.emr import EmrContainerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_eks\"\n\n# Externally fetched variables\nROLE_ARN_KEY = \"ROLE_ARN\"\nJOB_ROLE_ARN_KEY = \"JOB_ROLE_ARN\"\nJOB_ROLE_NAME_KEY = \"JOB_ROLE_NAME\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_NAME_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nS3_FILE_NAME = \"pi.py\"\nS3_FILE_CONTENT = \"\"\"\nk = 1\ns = 0\n\nfor i in range(1000000):\n    if i % 2 == 0:\n        s += 4/k\n    else:\n        s -= 4/k\n\n    k += 2\n\nprint(s)\n\"\"\"\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\n@task\ndef enable_access_emr_on_eks(cluster, ns):\n    # Install eksctl and enable access for EMR on EKS\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-cluster-access.html\n    file = \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\"\n    commands = f\"\"\"\n        curl --silent --location \"{file}\" | tar xz -C /tmp &&\n        sudo mv /tmp/eksctl /usr/local/bin &&\n        eksctl create iamidentitymapping --cluster {cluster} --namespace {ns} --service-name \"emr-containers\"\n    \"\"\"\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef create_iam_oidc_identity_provider(cluster_name):\n    # Create an IAM OIDC identity provider\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html\n    command = f\"eksctl utils associate-iam-oidc-provider --cluster {cluster_name} --approve\"\n\n    build = subprocess.Popen(\n        command,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef delete_iam_oidc_identity_provider(cluster_name):\n    oidc_provider_issuer_url = boto3.client(\"eks\").describe_cluster(name=cluster_name,)[\"cluster\"][\n        \"identity\"\n    ][\"oidc\"][\"issuer\"]\n    oidc_provider_issuer_endpoint = oidc_provider_issuer_url.replace(\"https://\", \"\")\n\n    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n    boto3.client(\"iam\").delete_open_id_connect_provider(\n        OpenIDConnectProviderArn=f\"arn:aws:iam::{account_id}:oidc-provider/{oidc_provider_issuer_endpoint}\"\n    )\n\n\n@task\ndef update_trust_policy_execution_role(cluster_name, cluster_namespace, role_name):\n    # Remove any already existing trusted entities added with \"update-role-trust-policy\"\n    # Prevent getting an error \"Cannot exceed quota for ACLSizePerRole\"\n    client = boto3.client(\"iam\")\n    role_trust_policy = client.get_role(RoleName=role_name)[\"Role\"][\"AssumeRolePolicyDocument\"]\n    # We assume if the action is sts:AssumeRoleWithWebIdentity, the statement had been added with\n    # \"update-role-trust-policy\". Removing it to not exceed the quota\n    role_trust_policy[\"Statement\"] = list(\n        filter(\n            lambda statement: statement[\"Action\"] != \"sts:AssumeRoleWithWebIdentity\",\n            role_trust_policy[\"Statement\"],\n        )\n    )\n\n    client.update_assume_role_policy(\n        RoleName=role_name,\n        PolicyDocument=json.dumps(role_trust_policy),\n    )\n\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-trust-policy.html\n    # The action \"update-role-trust-policy\" is not available in boto3, thus we need to do it using AWS CLI\n    commands = (\n        f\"aws emr-containers update-role-trust-policy --cluster-name {cluster_name} \"\n        f\"--namespace {cluster_namespace} --role-name {role_name}\"\n    )\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_virtual_cluster(virtual_cluster_id):\n    boto3.client(\"emr-containers\").delete_virtual_cluster(\n        id=virtual_cluster_id,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n    job_role_arn = test_context[JOB_ROLE_ARN_KEY]\n    job_role_name = test_context[JOB_ROLE_NAME_KEY]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    eks_cluster_name = f\"{env_id}-cluster\"\n    virtual_cluster_name = f\"{env_id}-virtual-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    eks_namespace = \"default\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_emr_eks_config]\n    job_driver_arg = {\n        \"sparkSubmitJobDriver\": {\n            \"entryPoint\": f\"s3://{s3_bucket_name}/{S3_FILE_NAME}\",\n            \"sparkSubmitParameters\": \"--conf spark.executors.instances=2 --conf spark.executors.memory=2G \"\n            \"--conf spark.executor.cores=2 --conf spark.driver.cores=1\",\n        }\n    }\n\n    configuration_overrides_arg = {\n        \"monitoringConfiguration\": {\n            \"cloudWatchMonitoringConfiguration\": {\n                \"logGroupName\": \"/emr-eks-jobs\",\n                \"logStreamNamePrefix\": \"airflow\",\n            }\n        },\n    }\n    # [END howto_operator_emr_eks_config]\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    upload_s3_file = S3CreateObjectOperator(\n        task_id=\"upload_s3_file\",\n        s3_bucket=s3_bucket_name,\n        s3_key=S3_FILE_NAME,\n        data=S3_FILE_CONTENT,\n    )\n\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=role_arn,\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=role_arn,\n        resources_vpc_config={\"subnetIds\": subnets},\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    # [START howto_operator_emr_eks_create_cluster]\n    create_emr_eks_cluster = EmrEksCreateClusterOperator(\n        task_id=\"create_emr_eks_cluster\",\n        virtual_cluster_name=virtual_cluster_name,\n        eks_cluster_name=eks_cluster_name,\n        eks_namespace=eks_namespace,\n    )\n    # [END howto_operator_emr_eks_create_cluster]\n\n    # [START howto_operator_emr_container]\n    job_starter = EmrContainerOperator(\n        task_id=\"start_job\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        execution_role_arn=job_role_arn,\n        release_label=\"emr-6.3.0-latest\",\n        job_driver=job_driver_arg,\n        configuration_overrides=configuration_overrides_arg,\n        name=\"pi.py\",\n    )\n    # [END howto_operator_emr_container]\n    job_starter.wait_for_completion = False\n\n    # [START howto_sensor_emr_container]\n    job_waiter = EmrContainerSensor(\n        task_id=\"job_waiter\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        job_id=str(job_starter.output),\n    )\n    # [END howto_sensor_emr_container]\n\n    delete_eks_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        force_delete_compute=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_eks_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_s3_file,\n        create_launch_template(launch_template_name),\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        enable_access_emr_on_eks(eks_cluster_name, eks_namespace),\n        create_iam_oidc_identity_provider(eks_cluster_name),\n        update_trust_policy_execution_role(eks_cluster_name, eks_namespace, job_role_name),\n        # TEST BODY\n        create_emr_eks_cluster,\n        job_starter,\n        job_waiter,\n        # TEST TEARDOWN\n        delete_iam_oidc_identity_provider(eks_cluster_name),\n        delete_virtual_cluster(str(create_emr_eks_cluster.output)),\n        delete_eks_cluster,\n        await_delete_eks_cluster,\n        delete_launch_template(launch_template_name),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_eks.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Multiple AWS services, advanced task functions, subprocess interactions, IAM configuration, and complex cloud infrastructure setup with custom Python logic", "key_concepts": ["AWS Integration", "EKS Cluster Management", "EMR Containerized Jobs", "IAM Role Configuration", "Task Decorators", "Cloud Infrastructure as Code"], "learning_objectives": ["Understand Airflow's AWS provider operators", "Learn cloud infrastructure orchestration", "Master dynamic task creation", "Implement secure AWS service interactions"]}}
{"instruction": "Design a secure, scalable data processing pipeline using EMR on EKS, demonstrating best practices for cloud-native workflow management with dynamic IAM and network configuration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksNodegroupStateSensor", "EmrContainerOperator", "EmrContainerSensor", "EmrEksCreateClusterOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 334, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nimport subprocess\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import EksCreateClusterOperator, EksDeleteClusterOperator\nfrom airflow.providers.amazon.aws.operators.emr import EmrContainerOperator, EmrEksCreateClusterOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.providers.amazon.aws.sensors.emr import EmrContainerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_eks\"\n\n# Externally fetched variables\nROLE_ARN_KEY = \"ROLE_ARN\"\nJOB_ROLE_ARN_KEY = \"JOB_ROLE_ARN\"\nJOB_ROLE_NAME_KEY = \"JOB_ROLE_NAME\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_ARN_KEY)\n    .add_variable(JOB_ROLE_NAME_KEY)\n    .add_variable(SUBNETS_KEY, split_string=True)\n    .build()\n)\n\nS3_FILE_NAME = \"pi.py\"\nS3_FILE_CONTENT = \"\"\"\nk = 1\ns = 0\n\nfor i in range(1000000):\n    if i % 2 == 0:\n        s += 4/k\n    else:\n        s -= 4/k\n\n    k += 2\n\nprint(s)\n\"\"\"\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\n@task\ndef enable_access_emr_on_eks(cluster, ns):\n    # Install eksctl and enable access for EMR on EKS\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-cluster-access.html\n    file = \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\"\n    commands = f\"\"\"\n        curl --silent --location \"{file}\" | tar xz -C /tmp &&\n        sudo mv /tmp/eksctl /usr/local/bin &&\n        eksctl create iamidentitymapping --cluster {cluster} --namespace {ns} --service-name \"emr-containers\"\n    \"\"\"\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef create_iam_oidc_identity_provider(cluster_name):\n    # Create an IAM OIDC identity provider\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html\n    command = f\"eksctl utils associate-iam-oidc-provider --cluster {cluster_name} --approve\"\n\n    build = subprocess.Popen(\n        command,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task\ndef delete_iam_oidc_identity_provider(cluster_name):\n    oidc_provider_issuer_url = boto3.client(\"eks\").describe_cluster(name=cluster_name,)[\"cluster\"][\n        \"identity\"\n    ][\"oidc\"][\"issuer\"]\n    oidc_provider_issuer_endpoint = oidc_provider_issuer_url.replace(\"https://\", \"\")\n\n    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n    boto3.client(\"iam\").delete_open_id_connect_provider(\n        OpenIDConnectProviderArn=f\"arn:aws:iam::{account_id}:oidc-provider/{oidc_provider_issuer_endpoint}\"\n    )\n\n\n@task\ndef update_trust_policy_execution_role(cluster_name, cluster_namespace, role_name):\n    # Remove any already existing trusted entities added with \"update-role-trust-policy\"\n    # Prevent getting an error \"Cannot exceed quota for ACLSizePerRole\"\n    client = boto3.client(\"iam\")\n    role_trust_policy = client.get_role(RoleName=role_name)[\"Role\"][\"AssumeRolePolicyDocument\"]\n    # We assume if the action is sts:AssumeRoleWithWebIdentity, the statement had been added with\n    # \"update-role-trust-policy\". Removing it to not exceed the quota\n    role_trust_policy[\"Statement\"] = list(\n        filter(\n            lambda statement: statement[\"Action\"] != \"sts:AssumeRoleWithWebIdentity\",\n            role_trust_policy[\"Statement\"],\n        )\n    )\n\n    client.update_assume_role_policy(\n        RoleName=role_name,\n        PolicyDocument=json.dumps(role_trust_policy),\n    )\n\n    # See https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-trust-policy.html\n    # The action \"update-role-trust-policy\" is not available in boto3, thus we need to do it using AWS CLI\n    commands = (\n        f\"aws emr-containers update-role-trust-policy --cluster-name {cluster_name} \"\n        f\"--namespace {cluster_namespace} --role-name {role_name}\"\n    )\n\n    build = subprocess.Popen(\n        commands,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, err = build.communicate()\n\n    if build.returncode != 0:\n        raise RuntimeError(err)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_virtual_cluster(virtual_cluster_id):\n    boto3.client(\"emr-containers\").delete_virtual_cluster(\n        id=virtual_cluster_id,\n    )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    subnets = test_context[SUBNETS_KEY]\n    job_role_arn = test_context[JOB_ROLE_ARN_KEY]\n    job_role_name = test_context[JOB_ROLE_NAME_KEY]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    eks_cluster_name = f\"{env_id}-cluster\"\n    virtual_cluster_name = f\"{env_id}-virtual-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    eks_namespace = \"default\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_emr_eks_config]\n    job_driver_arg = {\n        \"sparkSubmitJobDriver\": {\n            \"entryPoint\": f\"s3://{s3_bucket_name}/{S3_FILE_NAME}\",\n            \"sparkSubmitParameters\": \"--conf spark.executors.instances=2 --conf spark.executors.memory=2G \"\n            \"--conf spark.executor.cores=2 --conf spark.driver.cores=1\",\n        }\n    }\n\n    configuration_overrides_arg = {\n        \"monitoringConfiguration\": {\n            \"cloudWatchMonitoringConfiguration\": {\n                \"logGroupName\": \"/emr-eks-jobs\",\n                \"logStreamNamePrefix\": \"airflow\",\n            }\n        },\n    }\n    # [END howto_operator_emr_eks_config]\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    upload_s3_file = S3CreateObjectOperator(\n        task_id=\"upload_s3_file\",\n        s3_bucket=s3_bucket_name,\n        s3_key=S3_FILE_NAME,\n        data=S3_FILE_CONTENT,\n    )\n\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=role_arn,\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=role_arn,\n        resources_vpc_config={\"subnetIds\": subnets},\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=eks_cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    # [START howto_operator_emr_eks_create_cluster]\n    create_emr_eks_cluster = EmrEksCreateClusterOperator(\n        task_id=\"create_emr_eks_cluster\",\n        virtual_cluster_name=virtual_cluster_name,\n        eks_cluster_name=eks_cluster_name,\n        eks_namespace=eks_namespace,\n    )\n    # [END howto_operator_emr_eks_create_cluster]\n\n    # [START howto_operator_emr_container]\n    job_starter = EmrContainerOperator(\n        task_id=\"start_job\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        execution_role_arn=job_role_arn,\n        release_label=\"emr-6.3.0-latest\",\n        job_driver=job_driver_arg,\n        configuration_overrides=configuration_overrides_arg,\n        name=\"pi.py\",\n    )\n    # [END howto_operator_emr_container]\n    job_starter.wait_for_completion = False\n\n    # [START howto_sensor_emr_container]\n    job_waiter = EmrContainerSensor(\n        task_id=\"job_waiter\",\n        virtual_cluster_id=str(create_emr_eks_cluster.output),\n        job_id=str(job_starter.output),\n    )\n    # [END howto_sensor_emr_container]\n\n    delete_eks_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        force_delete_compute=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    await_delete_eks_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_eks_cluster\",\n        cluster_name=eks_cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        trigger_rule=TriggerRule.ALL_DONE,\n        poke_interval=10,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_s3_file,\n        create_launch_template(launch_template_name),\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        enable_access_emr_on_eks(eks_cluster_name, eks_namespace),\n        create_iam_oidc_identity_provider(eks_cluster_name),\n        update_trust_policy_execution_role(eks_cluster_name, eks_namespace, job_role_name),\n        # TEST BODY\n        create_emr_eks_cluster,\n        job_starter,\n        job_waiter,\n        # TEST TEARDOWN\n        delete_iam_oidc_identity_provider(eks_cluster_name),\n        delete_virtual_cluster(str(create_emr_eks_cluster.output)),\n        delete_eks_cluster,\n        await_delete_eks_cluster,\n        delete_launch_template(launch_template_name),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_eks.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS operators, custom task functions, security configurations, and external service interactions requiring moderate complexity", "key_concepts": ["Cloud Security", "Containerized Data Processing", "Dynamic Infrastructure Provisioning", "AWS Service Integration", "Workflow Automation"], "learning_objectives": ["Implement secure cloud workflows", "Understand container-based job execution", "Learn dynamic infrastructure management", "Practice AWS service orchestration"]}}
{"instruction": "Build an Airflow DAG that demonstrates the complete lifecycle of an Amazon EMR Notebook execution, including starting, monitoring, stopping, and re-executing a notebook on an EMR cluster.", "input": {"airflow_version": "2.7.2", "operators": ["EmrNotebookExecutionSensor", "EmrStartNotebookExecutionOperator", "EmrStopNotebookExecutionOperator"], "line_count": 105, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrStartNotebookExecutionOperator,\n    EmrStopNotebookExecutionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.emr import EmrNotebookExecutionSensor\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_notebook\"\n# Externally fetched variables:\nEDITOR_ID_KEY = \"EDITOR_ID\"\nCLUSTER_ID_KEY = \"CLUSTER_ID\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(EDITOR_ID_KEY).add_variable(CLUSTER_ID_KEY).build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    editor_id = test_context[EDITOR_ID_KEY]\n    cluster_id = test_context[CLUSTER_ID_KEY]\n\n    # [START howto_operator_emr_start_notebook_execution]\n    start_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"start_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    # [END howto_operator_emr_start_notebook_execution]\n\n    notebook_execution_id_1 = start_execution.output\n\n    # [START howto_sensor_emr_notebook_execution]\n    wait_for_execution_start = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_start\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"RUNNING\"},\n        poke_interval=5,\n    )\n    # [END howto_sensor_emr_notebook_execution]\n\n    # [START howto_operator_emr_stop_notebook_execution]\n    stop_execution = EmrStopNotebookExecutionOperator(\n        task_id=\"stop_execution\",\n        notebook_execution_id=notebook_execution_id_1,\n    )\n    # [END howto_operator_emr_stop_notebook_execution]\n\n    wait_for_execution_stop = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_stop\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"STOPPED\"},\n        poke_interval=5,\n    )\n    finish_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"finish_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    notebook_execution_id_2 = finish_execution.output\n    wait_for_execution_finish = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_finish\",\n        notebook_execution_id=notebook_execution_id_2,\n        poke_interval=5,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        start_execution,\n        wait_for_execution_start,\n        stop_execution,\n        wait_for_execution_stop,\n        finish_execution,\n        # TEST TEARDOWN\n        wait_for_execution_finish,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_notebook_execution.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS EMR operators, includes sensors for monitoring execution states, and demonstrates complex task chaining with external service integration", "key_concepts": ["EMR Notebook Execution", "Task Chaining", "AWS Service Integration", "Execution Monitoring", "Sensors"], "learning_objectives": ["Understand EMR Notebook workflow in Airflow", "Learn how to use AWS EMR operators", "Practice task dependency management", "Implement execution state monitoring"]}}
{"instruction": "Create an Airflow workflow that showcases advanced error handling and state management for cloud-based notebook executions, focusing on the interaction between start, stop, and monitoring operators.", "input": {"airflow_version": "2.7.2", "operators": ["EmrNotebookExecutionSensor", "EmrStartNotebookExecutionOperator", "EmrStopNotebookExecutionOperator"], "line_count": 105, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.emr import (\n    EmrStartNotebookExecutionOperator,\n    EmrStopNotebookExecutionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.emr import EmrNotebookExecutionSensor\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_emr_notebook\"\n# Externally fetched variables:\nEDITOR_ID_KEY = \"EDITOR_ID\"\nCLUSTER_ID_KEY = \"CLUSTER_ID\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(EDITOR_ID_KEY).add_variable(CLUSTER_ID_KEY).build()\n)\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    editor_id = test_context[EDITOR_ID_KEY]\n    cluster_id = test_context[CLUSTER_ID_KEY]\n\n    # [START howto_operator_emr_start_notebook_execution]\n    start_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"start_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    # [END howto_operator_emr_start_notebook_execution]\n\n    notebook_execution_id_1 = start_execution.output\n\n    # [START howto_sensor_emr_notebook_execution]\n    wait_for_execution_start = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_start\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"RUNNING\"},\n        poke_interval=5,\n    )\n    # [END howto_sensor_emr_notebook_execution]\n\n    # [START howto_operator_emr_stop_notebook_execution]\n    stop_execution = EmrStopNotebookExecutionOperator(\n        task_id=\"stop_execution\",\n        notebook_execution_id=notebook_execution_id_1,\n    )\n    # [END howto_operator_emr_stop_notebook_execution]\n\n    wait_for_execution_stop = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_stop\",\n        notebook_execution_id=notebook_execution_id_1,\n        target_states={\"STOPPED\"},\n        poke_interval=5,\n    )\n    finish_execution = EmrStartNotebookExecutionOperator(\n        task_id=\"finish_execution\",\n        editor_id=editor_id,\n        cluster_id=cluster_id,\n        relative_path=\"EMR-System-Test.ipynb\",\n        service_role=\"EMR_Notebooks_DefaultRole\",\n    )\n    notebook_execution_id_2 = finish_execution.output\n    wait_for_execution_finish = EmrNotebookExecutionSensor(\n        task_id=\"wait_for_execution_finish\",\n        notebook_execution_id=notebook_execution_id_2,\n        poke_interval=5,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        start_execution,\n        wait_for_execution_start,\n        stop_execution,\n        wait_for_execution_stop,\n        finish_execution,\n        # TEST TEARDOWN\n        wait_for_execution_finish,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_emr_notebook_execution.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple stateful operators, uses sensors for precise execution tracking, and demonstrates complex workflow control in cloud environments", "key_concepts": ["Stateful Operators", "Cloud Workflow Management", "Execution Monitoring", "Task Dependency", "Service Integration"], "learning_objectives": ["Understand cloud notebook execution workflows", "Learn state tracking techniques", "Implement robust task dependencies", "Master AWS EMR operator interactions"]}}
{"instruction": "Build an AWS EKS Cluster Deployment DAG that demonstrates creating an EKS cluster with a nodegroup, launching a test pod, and implementing AWS infrastructure as code using Airflow's EKS operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 151, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_nodegroup_in_one_step\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster_with_nodegroup]\n    # Create an Amazon EKS cluster control plane and an EKS nodegroup compute platform in one step.\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        # ``compute='nodegroup'`` is the default, explicitly set here for demo purposes.\n        compute=\"nodegroup\",\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n    # [END howto_operator_eks_create_cluster_with_nodegroup]\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_force_delete_cluster]\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_nodegroup_and_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_nodegroup_and_cluster\",\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n    # [END howto_operator_eks_force_delete_cluster]\n    delete_nodegroup_and_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup_and_cluster,\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_nodegroup_in_one_step.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS-specific operators, involves cloud infrastructure creation, includes sensors and custom tasks, requires AWS service integration", "key_concepts": ["AWS EKS Cluster Management", "Cloud Infrastructure Deployment", "Airflow Operators for Cloud Services", "Task Dependencies", "Sensor Operators"], "learning_objectives": ["Understand EKS cluster creation workflow", "Learn AWS service integration in Airflow", "Implement infrastructure deployment as code", "Use task chaining and conditional execution"]}}
{"instruction": "Design an Airflow DAG that demonstrates secure, automated Kubernetes infrastructure provisioning using launch templates, IMDSv2 configuration, and pod execution with comprehensive error handling and logging.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "EksClusterStateSensor", "EksCreateClusterOperator", "EksDeleteClusterOperator", "EksNodegroupStateSensor", "EksPodOperator"], "line_count": 151, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.providers.amazon.aws.hooks.eks import ClusterStates, NodegroupStates\nfrom airflow.providers.amazon.aws.operators.eks import (\n    EksCreateClusterOperator,\n    EksDeleteClusterOperator,\n    EksPodOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.eks import EksClusterStateSensor, EksNodegroupStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.k8s import get_describe_pod_operator\n\nDAG_ID = \"example_eks_with_nodegroup_in_one_step\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\nSUBNETS_KEY = \"SUBNETS\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).add_variable(SUBNETS_KEY, split_string=True).build()\n)\n\n\n@task\ndef create_launch_template(template_name: str):\n    # This launch template enables IMDSv2.\n    boto3.client(\"ec2\").create_launch_template(\n        LaunchTemplateName=template_name,\n        LaunchTemplateData={\n            \"MetadataOptions\": {\"HttpEndpoint\": \"enabled\", \"HttpTokens\": \"required\"},\n        },\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_launch_template(template_name: str):\n    boto3.client(\"ec2\").delete_launch_template(LaunchTemplateName=template_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    cluster_name = f\"{env_id}-cluster\"\n    nodegroup_name = f\"{env_id}-nodegroup\"\n    launch_template_name = f\"{env_id}-launch-template\"\n\n    # [START howto_operator_eks_create_cluster_with_nodegroup]\n    # Create an Amazon EKS cluster control plane and an EKS nodegroup compute platform in one step.\n    create_cluster_and_nodegroup = EksCreateClusterOperator(\n        task_id=\"create_cluster_and_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        cluster_role_arn=test_context[ROLE_ARN_KEY],\n        # Opting to use the same ARN for the cluster and the nodegroup here,\n        # but a different ARN could be configured and passed if desired.\n        nodegroup_role_arn=test_context[ROLE_ARN_KEY],\n        resources_vpc_config={\"subnetIds\": test_context[SUBNETS_KEY]},\n        # ``compute='nodegroup'`` is the default, explicitly set here for demo purposes.\n        compute=\"nodegroup\",\n        # The launch template enforces IMDSv2 and is required for internal\n        # compliance when running these system tests on AWS infrastructure.\n        create_nodegroup_kwargs={\"launchTemplate\": {\"name\": launch_template_name}},\n    )\n    # [END howto_operator_eks_create_cluster_with_nodegroup]\n\n    await_create_nodegroup = EksNodegroupStateSensor(\n        task_id=\"await_create_nodegroup\",\n        cluster_name=cluster_name,\n        nodegroup_name=nodegroup_name,\n        target_state=NodegroupStates.ACTIVE,\n        poke_interval=10,\n    )\n\n    start_pod = EksPodOperator(\n        task_id=\"start_pod\",\n        pod_name=\"test_pod\",\n        cluster_name=cluster_name,\n        image=\"amazon/aws-cli:latest\",\n        cmds=[\"sh\", \"-c\", \"echo Test Airflow; date\"],\n        labels={\"demo\": \"hello_world\"},\n        get_logs=True,\n        # Keep the pod alive, so we can describe it in case of trouble. It's deleted with the cluster anyway.\n        on_finish_action=\"keep_pod\",\n    )\n\n    describe_pod = get_describe_pod_operator(\n        cluster_name, pod_name=\"{{ ti.xcom_pull(key='pod_name', task_ids='run_pod') }}\"\n    )\n    # only describe the pod if the task above failed, to help diagnose\n    describe_pod.trigger_rule = TriggerRule.ONE_FAILED\n\n    # [START howto_operator_eks_force_delete_cluster]\n    # An Amazon EKS cluster can not be deleted with attached resources such as nodegroups or Fargate profiles.\n    # Setting the `force` to `True` will delete any attached resources before deleting the cluster.\n    delete_nodegroup_and_cluster = EksDeleteClusterOperator(\n        task_id=\"delete_nodegroup_and_cluster\",\n        cluster_name=cluster_name,\n        force_delete_compute=True,\n    )\n    # [END howto_operator_eks_force_delete_cluster]\n    delete_nodegroup_and_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    await_delete_cluster = EksClusterStateSensor(\n        task_id=\"await_delete_cluster\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        cluster_name=cluster_name,\n        target_state=ClusterStates.NONEXISTENT,\n        poke_interval=10,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_launch_template(launch_template_name),\n        # TEST BODY\n        create_cluster_and_nodegroup,\n        await_create_nodegroup,\n        start_pod,\n        # TEST TEARDOWN\n        describe_pod,\n        delete_nodegroup_and_cluster,\n        await_delete_cluster,\n        delete_launch_template(launch_template_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eks_with_nodegroup_in_one_step.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves security configuration, custom launch template creation, pod execution, and advanced error handling techniques", "key_concepts": ["Kubernetes Infrastructure Security", "Launch Template Configuration", "Pod Execution Strategies", "Error Handling in Cloud Deployments", "Metadata Service Configuration"], "learning_objectives": ["Implement secure Kubernetes infrastructure", "Configure IMDSv2 metadata protection", "Understand pod deployment strategies", "Learn conditional task execution techniques"]}}
{"instruction": "Build an AWS EventBridge event workflow that demonstrates how to create and publish custom events using Airflow operators, focusing on understanding event-driven architecture and AWS cloud integration.", "input": {"airflow_version": "2.7.2", "operators": ["EventBridgePutEventsOperator", "EventBridgePutRuleOperator"], "line_count": 55, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.eventbridge import (\n    EventBridgePutEventsOperator,\n    EventBridgePutRuleOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eventbridge\"\nENTRIES = [\n    {\n        \"Detail\": '{\"event-name\": \"custom-event\"}',\n        \"EventBusName\": \"custom-bus\",\n        \"Source\": \"example.myapp\",\n        \"DetailType\": \"Sample Custom Event\",\n    }\n]\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n\n    # [START howto_operator_eventbridge_put_events]\n    put_events = EventBridgePutEventsOperator(task_id=\"put_events_task\", entries=ENTRIES)\n    # [END howto_operator_eventbridge_put_events]\n\n    # [START howto_operator_eventbridge_put_rule]\n    put_rule = EventBridgePutRuleOperator(\n        task_id=\"put_rule_task\",\n        name=\"example_rule\",\n        event_pattern='{\"source\": [\"example.myapp\"]}',\n        description=\"This rule matches events from example.myapp.\",\n    )\n    # [END howto_operator_eventbridge_put_rule]\n\n    chain(test_context, put_events, put_rule)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eventbridge.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS-specific operators, involves cloud service integration, requires understanding of event patterns and event bus configuration", "key_concepts": ["AWS EventBridge", "Event-driven architecture", "Cloud service operators", "Event publishing", "DAG configuration"], "learning_objectives": ["Configure EventBridge events in Airflow", "Understand event publishing mechanisms", "Learn AWS cloud integration patterns"]}}
{"instruction": "Design an event routing system that demonstrates how to create event rules and publish custom events, exploring the interaction between event sources, rules, and potential downstream processing.", "input": {"airflow_version": "2.7.2", "operators": ["EventBridgePutEventsOperator", "EventBridgePutRuleOperator"], "line_count": 55, "is_multifile": false}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.eventbridge import (\n    EventBridgePutEventsOperator,\n    EventBridgePutRuleOperator,\n)\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_eventbridge\"\nENTRIES = [\n    {\n        \"Detail\": '{\"event-name\": \"custom-event\"}',\n        \"EventBusName\": \"custom-bus\",\n        \"Source\": \"example.myapp\",\n        \"DetailType\": \"Sample Custom Event\",\n    }\n]\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n\n    # [START howto_operator_eventbridge_put_events]\n    put_events = EventBridgePutEventsOperator(task_id=\"put_events_task\", entries=ENTRIES)\n    # [END howto_operator_eventbridge_put_events]\n\n    # [START howto_operator_eventbridge_put_rule]\n    put_rule = EventBridgePutRuleOperator(\n        task_id=\"put_rule_task\",\n        name=\"example_rule\",\n        event_pattern='{\"source\": [\"example.myapp\"]}',\n        description=\"This rule matches events from example.myapp.\",\n    )\n    # [END howto_operator_eventbridge_put_rule]\n\n    chain(test_context, put_events, put_rule)\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_eventbridge.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple EventBridge operators, requires understanding of event routing logic and custom event configuration", "key_concepts": ["Event routing", "Custom event design", "Operator chaining", "Event pattern matching", "Cloud event management"], "learning_objectives": ["Create complex event routing workflows", "Understand event pattern configuration", "Explore event-driven system design"]}}
{"instruction": "Build an Airflow DAG that demonstrates transferring files from an FTP server to an Amazon S3 bucket, showcasing cloud storage integration and file transfer workflows.", "input": {"airflow_version": "2.7.2", "operators": ["FTPToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 67, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.ftp_to_s3 import FTPToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_ftp_to_s3\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-ftp-to-s3-bucket\"\n    s3_key = f\"{env_id}-ftp-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_ftp_to_s3]\n    ftp_to_s3_task = FTPToS3Operator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        replace=True,\n    )\n    # [END howto_transfer_ftp_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        ftp_to_s3_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ftp_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses cloud service operators, involves S3 bucket creation/deletion, and integrates multiple AWS-specific operators with external service transfer", "key_concepts": ["Cloud Storage Integration", "File Transfer Workflows", "AWS S3 Operators", "Operator Chaining", "Trigger Rules"], "learning_objectives": ["Understand FTP to S3 data transfer process", "Learn AWS operator configurations", "Practice DAG task dependency management"]}}
{"instruction": "Design an Airflow DAG that demonstrates robust cloud resource management, including dynamic bucket creation, file transfer, and automatic cleanup using AWS operators and trigger rules.", "input": {"airflow_version": "2.7.2", "operators": ["FTPToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 67, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.ftp_to_s3 import FTPToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_ftp_to_s3\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-ftp-to-s3-bucket\"\n    s3_key = f\"{env_id}-ftp-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_ftp_to_s3]\n    ftp_to_s3_task = FTPToS3Operator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        replace=True,\n    )\n    # [END howto_transfer_ftp_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        ftp_to_s3_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_ftp_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements infrastructure management with S3 bucket lifecycle, uses conditional trigger rules, and shows advanced operator interactions", "key_concepts": ["Cloud Resource Management", "Operator Lifecycle", "Trigger Rule Configuration", "AWS Service Orchestration", "Idempotent DAG Design"], "learning_objectives": ["Master cloud resource provisioning", "Understand trigger rule applications", "Learn safe infrastructure teardown techniques"]}}
{"instruction": "Build a data transfer workflow that demonstrates cross-cloud storage migration from Google Cloud Storage (GCS) to Amazon S3, showcasing inter-cloud data movement and bucket management.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 110, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\n# Externally fetched variables:\nGCP_PROJECT_ID = \"GCP_PROJECT_ID\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(GCP_PROJECT_ID).build()\n\nDAG_ID = \"example_gcs_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    gcp_user_project = test_context[GCP_PROJECT_ID]\n\n    s3_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    s3_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    gcs_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    gcs_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_gcs_bucket = GCSCreateBucketOperator(\n        task_id=\"create_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        resource={\"billing\": {\"requesterPays\": True}},\n        project_id=gcp_user_project,\n    )\n\n    @task\n    def upload_gcs_file(bucket_name: str, object_name: str, user_project: str):\n        hook = GCSHook()\n        with hook.provide_file_and_upload(\n            bucket_name=bucket_name,\n            object_name=object_name,\n            user_project=user_project,\n        ) as temp_file:\n            temp_file.write(b\"test\")\n\n    # [START howto_transfer_gcs_to_s3]\n    gcs_to_s3 = GCSToS3Operator(\n        task_id=\"gcs_to_s3\",\n        bucket=gcs_bucket,\n        dest_s3_key=f\"s3://{s3_bucket}/{s3_key}\",\n        replace=True,\n        gcp_user_project=gcp_user_project,\n    )\n    # [END howto_transfer_gcs_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gcs_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        trigger_rule=TriggerRule.ALL_DONE,\n        user_project=gcp_user_project,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_gcs_bucket,\n        upload_gcs_file(gcs_bucket, gcs_key, gcp_user_project),\n        create_s3_bucket,\n        # TEST BODY\n        gcs_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_gcs_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_gcs_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple cloud operators, involves cross-cloud transfer, includes bucket creation/deletion, and leverages custom task with GCS hook", "key_concepts": ["Cloud Storage Transfer", "Multi-Cloud Orchestration", "Operator Chaining", "GCS Interactions", "S3 Management"], "learning_objectives": ["Understand inter-cloud data transfer", "Learn cloud storage operator usage", "Practice Airflow task dependencies", "Explore dynamic task generation"]}}
{"instruction": "Design a cloud infrastructure provisioning and cleanup DAG that demonstrates automated resource management across Google Cloud and Amazon Web Services using Airflow's provider-specific operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GCSCreateBucketOperator", "GCSDeleteBucketOperator", "GCSToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 110, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.operators.gcs import (\n    GCSCreateBucketOperator,\n    GCSDeleteBucketOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\n# Externally fetched variables:\nGCP_PROJECT_ID = \"GCP_PROJECT_ID\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(GCP_PROJECT_ID).build()\n\nDAG_ID = \"example_gcs_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    gcp_user_project = test_context[GCP_PROJECT_ID]\n\n    s3_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    s3_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    gcs_bucket = f\"{env_id}-gcs-to-s3-bucket\"\n    gcs_key = f\"{env_id}-gcs-to-s3-key\"\n\n    create_gcs_bucket = GCSCreateBucketOperator(\n        task_id=\"create_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        resource={\"billing\": {\"requesterPays\": True}},\n        project_id=gcp_user_project,\n    )\n\n    @task\n    def upload_gcs_file(bucket_name: str, object_name: str, user_project: str):\n        hook = GCSHook()\n        with hook.provide_file_and_upload(\n            bucket_name=bucket_name,\n            object_name=object_name,\n            user_project=user_project,\n        ) as temp_file:\n            temp_file.write(b\"test\")\n\n    # [START howto_transfer_gcs_to_s3]\n    gcs_to_s3 = GCSToS3Operator(\n        task_id=\"gcs_to_s3\",\n        bucket=gcs_bucket,\n        dest_s3_key=f\"s3://{s3_bucket}/{s3_key}\",\n        replace=True,\n        gcp_user_project=gcp_user_project,\n    )\n    # [END howto_transfer_gcs_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_gcs_bucket = GCSDeleteBucketOperator(\n        task_id=\"delete_gcs_bucket\",\n        bucket_name=gcs_bucket,\n        trigger_rule=TriggerRule.ALL_DONE,\n        user_project=gcp_user_project,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_gcs_bucket,\n        upload_gcs_file(gcs_bucket, gcs_key, gcp_user_project),\n        create_s3_bucket,\n        # TEST BODY\n        gcs_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_gcs_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_gcs_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple cloud providers, uses create/delete operators, implements trigger rules, and demonstrates infrastructure-as-code principles", "key_concepts": ["Cloud Resource Management", "Infrastructure Automation", "Trigger Rules", "Provider Operators", "Cleanup Workflows"], "learning_objectives": ["Understand cloud resource lifecycle management", "Learn about trigger rule configurations", "Practice multi-cloud orchestration", "Explore safe resource provisioning and teardown"]}}
{"instruction": "Build an AWS Glue ETL workflow that demonstrates end-to-end data processing using Airflow, including S3 data upload, Glue crawler configuration, and Spark job execution.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GlueCrawlerOperator", "GlueCrawlerSensor", "GlueJobOperator", "GlueJobSensor", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 198, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.client import BaseClient\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.glue import GlueJobOperator\nfrom airflow.providers.amazon.aws.operators.glue_crawler import GlueCrawlerOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glue import GlueJobSensor\nfrom airflow.providers.amazon.aws.sensors.glue_crawler import GlueCrawlerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_glue\"\n\n# Externally fetched variables:\n# Role needs S3 putobject/getobject access as well as the glue service role,\n# see docs here: https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# Example csv data used as input to the example AWS Glue Job.\nEXAMPLE_CSV = \"\"\"\napple,0.5\nmilk,2.5\nbread,4.0\n\"\"\"\n\n# Example Spark script to operate on the above sample csv data.\nEXAMPLE_SCRIPT = \"\"\"\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nglueContext = GlueContext(SparkContext.getOrCreate())\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n             database='{db_name}', table_name='input')\nprint('There are %s items in the table' % datasource.count())\n\ndatasource.toDF().write.format('csv').mode(\"append\").save('s3://{bucket_name}/output')\n\"\"\"\n\n\n@task\ndef get_role_name(arn: str) -> str:\n    return arn.split(\"/\")[-1]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef glue_cleanup(crawler_name: str, job_name: str, db_name: str) -> None:\n    client: BaseClient = boto3.client(\"glue\")\n\n    client.delete_crawler(Name=crawler_name)\n    client.delete_job(JobName=job_name)\n    client.delete_database(Name=db_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    glue_crawler_name = f\"{env_id}_crawler\"\n    glue_db_name = f\"{env_id}_glue_db\"\n    glue_job_name = f\"{env_id}_glue_job\"\n    bucket_name = f\"{env_id}-bucket\"\n    role_name = get_role_name(role_arn)\n\n    glue_crawler_config = {\n        \"Name\": glue_crawler_name,\n        \"Role\": role_arn,\n        \"DatabaseName\": glue_db_name,\n        \"Targets\": {\"S3Targets\": [{\"Path\": f\"{bucket_name}/input\"}]},\n    }\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    upload_csv = S3CreateObjectOperator(\n        task_id=\"upload_csv\",\n        s3_bucket=bucket_name,\n        s3_key=\"input/input.csv\",\n        data=EXAMPLE_CSV,\n        replace=True,\n    )\n\n    upload_script = S3CreateObjectOperator(\n        task_id=\"upload_script\",\n        s3_bucket=bucket_name,\n        s3_key=\"etl_script.py\",\n        data=EXAMPLE_SCRIPT.format(db_name=glue_db_name, bucket_name=bucket_name),\n        replace=True,\n    )\n\n    # [START howto_operator_glue_crawler]\n    crawl_s3 = GlueCrawlerOperator(\n        task_id=\"crawl_s3\",\n        config=glue_crawler_config,\n    )\n    # [END howto_operator_glue_crawler]\n\n    # GlueCrawlerOperator waits by default, setting as False to test the Sensor below.\n    crawl_s3.wait_for_completion = False\n\n    # [START howto_sensor_glue_crawler]\n    wait_for_crawl = GlueCrawlerSensor(\n        task_id=\"wait_for_crawl\",\n        crawler_name=glue_crawler_name,\n    )\n    # [END howto_sensor_glue_crawler]\n\n    # [START howto_operator_glue]\n    submit_glue_job = GlueJobOperator(\n        task_id=\"submit_glue_job\",\n        job_name=glue_job_name,\n        script_location=f\"s3://{bucket_name}/etl_script.py\",\n        s3_bucket=bucket_name,\n        iam_role_name=role_name,\n        create_job_kwargs={\"GlueVersion\": \"3.0\", \"NumberOfWorkers\": 2, \"WorkerType\": \"G.1X\"},\n    )\n    # [END howto_operator_glue]\n\n    # GlueJobOperator waits by default, setting as False to test the Sensor below.\n    submit_glue_job.wait_for_completion = False\n\n    # [START howto_sensor_glue]\n    wait_for_job = GlueJobSensor(\n        task_id=\"wait_for_job\",\n        job_name=glue_job_name,\n        # Job ID extracted from previous Glue Job Operator task\n        run_id=submit_glue_job.output,\n        verbose=True,  # prints glue job logs in airflow logs\n    )\n    # [END howto_sensor_glue]\n    wait_for_job.poke_interval = 5\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws-glue/crawlers\", glue_crawler_name),\n            (\"/aws-glue/jobs/logs-v2\", submit_glue_job.output),\n            (\"/aws-glue/jobs/error\", submit_glue_job.output),\n            (\"/aws-glue/jobs/output\", submit_glue_job.output),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_csv,\n        upload_script,\n        # TEST BODY\n        crawl_s3,\n        wait_for_crawl,\n        submit_glue_job,\n        wait_for_job,\n        # TEST TEARDOWN\n        glue_cleanup(glue_crawler_name, glue_job_name, glue_db_name),\n        delete_bucket,\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_glue.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, includes sensors and custom tasks, requires understanding of Glue and S3 interactions", "key_concepts": ["AWS Glue Integration", "S3 Object Management", "Airflow Sensors", "Cloud ETL Orchestration", "Task Chaining"], "learning_objectives": ["Configure AWS Glue crawlers", "Submit Spark jobs via Airflow", "Manage cloud storage workflows", "Use Airflow sensors for job tracking"]}}
{"instruction": "Design a modular data pipeline demonstrating AWS service orchestration, focusing on dynamic task generation, error handling, and clean-up mechanisms in Airflow.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GlueCrawlerOperator", "GlueCrawlerSensor", "GlueJobOperator", "GlueJobSensor", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 198, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\nfrom botocore.client import BaseClient\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.glue import GlueJobOperator\nfrom airflow.providers.amazon.aws.operators.glue_crawler import GlueCrawlerOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glue import GlueJobSensor\nfrom airflow.providers.amazon.aws.sensors.glue_crawler import GlueCrawlerSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_glue\"\n\n# Externally fetched variables:\n# Role needs S3 putobject/getobject access as well as the glue service role,\n# see docs here: https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# Example csv data used as input to the example AWS Glue Job.\nEXAMPLE_CSV = \"\"\"\napple,0.5\nmilk,2.5\nbread,4.0\n\"\"\"\n\n# Example Spark script to operate on the above sample csv data.\nEXAMPLE_SCRIPT = \"\"\"\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nglueContext = GlueContext(SparkContext.getOrCreate())\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n             database='{db_name}', table_name='input')\nprint('There are %s items in the table' % datasource.count())\n\ndatasource.toDF().write.format('csv').mode(\"append\").save('s3://{bucket_name}/output')\n\"\"\"\n\n\n@task\ndef get_role_name(arn: str) -> str:\n    return arn.split(\"/\")[-1]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef glue_cleanup(crawler_name: str, job_name: str, db_name: str) -> None:\n    client: BaseClient = boto3.client(\"glue\")\n\n    client.delete_crawler(Name=crawler_name)\n    client.delete_job(JobName=job_name)\n    client.delete_database(Name=db_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    env_id = test_context[ENV_ID_KEY]\n    role_arn = test_context[ROLE_ARN_KEY]\n    glue_crawler_name = f\"{env_id}_crawler\"\n    glue_db_name = f\"{env_id}_glue_db\"\n    glue_job_name = f\"{env_id}_glue_job\"\n    bucket_name = f\"{env_id}-bucket\"\n    role_name = get_role_name(role_arn)\n\n    glue_crawler_config = {\n        \"Name\": glue_crawler_name,\n        \"Role\": role_arn,\n        \"DatabaseName\": glue_db_name,\n        \"Targets\": {\"S3Targets\": [{\"Path\": f\"{bucket_name}/input\"}]},\n    }\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    upload_csv = S3CreateObjectOperator(\n        task_id=\"upload_csv\",\n        s3_bucket=bucket_name,\n        s3_key=\"input/input.csv\",\n        data=EXAMPLE_CSV,\n        replace=True,\n    )\n\n    upload_script = S3CreateObjectOperator(\n        task_id=\"upload_script\",\n        s3_bucket=bucket_name,\n        s3_key=\"etl_script.py\",\n        data=EXAMPLE_SCRIPT.format(db_name=glue_db_name, bucket_name=bucket_name),\n        replace=True,\n    )\n\n    # [START howto_operator_glue_crawler]\n    crawl_s3 = GlueCrawlerOperator(\n        task_id=\"crawl_s3\",\n        config=glue_crawler_config,\n    )\n    # [END howto_operator_glue_crawler]\n\n    # GlueCrawlerOperator waits by default, setting as False to test the Sensor below.\n    crawl_s3.wait_for_completion = False\n\n    # [START howto_sensor_glue_crawler]\n    wait_for_crawl = GlueCrawlerSensor(\n        task_id=\"wait_for_crawl\",\n        crawler_name=glue_crawler_name,\n    )\n    # [END howto_sensor_glue_crawler]\n\n    # [START howto_operator_glue]\n    submit_glue_job = GlueJobOperator(\n        task_id=\"submit_glue_job\",\n        job_name=glue_job_name,\n        script_location=f\"s3://{bucket_name}/etl_script.py\",\n        s3_bucket=bucket_name,\n        iam_role_name=role_name,\n        create_job_kwargs={\"GlueVersion\": \"3.0\", \"NumberOfWorkers\": 2, \"WorkerType\": \"G.1X\"},\n    )\n    # [END howto_operator_glue]\n\n    # GlueJobOperator waits by default, setting as False to test the Sensor below.\n    submit_glue_job.wait_for_completion = False\n\n    # [START howto_sensor_glue]\n    wait_for_job = GlueJobSensor(\n        task_id=\"wait_for_job\",\n        job_name=glue_job_name,\n        # Job ID extracted from previous Glue Job Operator task\n        run_id=submit_glue_job.output,\n        verbose=True,  # prints glue job logs in airflow logs\n    )\n    # [END howto_sensor_glue]\n    wait_for_job.poke_interval = 5\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws-glue/crawlers\", glue_crawler_name),\n            (\"/aws-glue/jobs/logs-v2\", submit_glue_job.output),\n            (\"/aws-glue/jobs/error\", submit_glue_job.output),\n            (\"/aws-glue/jobs/output\", submit_glue_job.output),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        upload_csv,\n        upload_script,\n        # TEST BODY\n        crawl_s3,\n        wait_for_crawl,\n        submit_glue_job,\n        wait_for_job,\n        # TEST TEARDOWN\n        glue_cleanup(glue_crawler_name, glue_job_name, glue_db_name),\n        delete_bucket,\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_glue.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Advanced task interactions, multiple external service integrations, custom cleanup function, dynamic role and resource naming", "key_concepts": ["Dynamic Task Generation", "Resource Cleanup", "IAM Role Management", "Cloud Service Orchestration", "Error Handling Strategies"], "learning_objectives": ["Generate dynamic task configurations", "Implement robust cleanup procedures", "Handle IAM role interactions", "Create fault-tolerant cloud workflows"]}}
{"instruction": "Build an AWS Glacier data archiving and transfer workflow that demonstrates cloud storage migration using Airflow operators, focusing on creating a vault, uploading an archive, and transferring data to Google Cloud Storage.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GlacierCreateJobOperator", "GlacierJobOperationSensor", "GlacierToGCSOperator", "GlacierUploadArchiveOperator"], "line_count": 103, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import task\nfrom airflow.providers.amazon.aws.operators.glacier import (\n    GlacierCreateJobOperator,\n    GlacierUploadArchiveOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glacier import GlacierJobOperationSensor\nfrom airflow.providers.amazon.aws.transfers.glacier_to_gcs import GlacierToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_glacier_to_gcs\"\n\n\n@task\ndef create_vault(vault_name):\n    boto3.client(\"glacier\").create_vault(vaultName=vault_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_vault(vault_name):\n    boto3.client(\"glacier\").delete_vault(vaultName=vault_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    vault_name = f\"{env_id}-vault\"\n    gcs_bucket_name = f\"{env_id}-bucket\"\n    gcs_object_name = f\"{env_id}-object\"\n\n    # [START howto_operator_glacier_create_job]\n    create_glacier_job = GlacierCreateJobOperator(task_id=\"create_glacier_job\", vault_name=vault_name)\n    JOB_ID = '{{ task_instance.xcom_pull(\"create_glacier_job\")[\"jobId\"] }}'\n    # [END howto_operator_glacier_create_job]\n\n    # [START howto_sensor_glacier_job_operation]\n    wait_for_operation_complete = GlacierJobOperationSensor(\n        vault_name=vault_name,\n        job_id=JOB_ID,\n        task_id=\"wait_for_operation_complete\",\n    )\n    # [END howto_sensor_glacier_job_operation]\n\n    # [START howto_operator_glacier_upload_archive]\n    upload_archive_to_glacier = GlacierUploadArchiveOperator(\n        task_id=\"upload_data_to_glacier\", vault_name=vault_name, body=b\"Test Data\"\n    )\n    # [END howto_operator_glacier_upload_archive]\n\n    # [START howto_transfer_glacier_to_gcs]\n    transfer_archive_to_gcs = GlacierToGCSOperator(\n        task_id=\"transfer_archive_to_gcs\",\n        vault_name=vault_name,\n        bucket_name=gcs_bucket_name,\n        object_name=gcs_object_name,\n        gzip=False,\n        # Override to match your needs\n        # If chunk size is bigger than actual file size\n        # then whole file will be downloaded\n        chunk_size=1024,\n    )\n    # [END howto_transfer_glacier_to_gcs]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_vault(vault_name),\n        # TEST BODY\n        create_glacier_job,\n        wait_for_operation_complete,\n        upload_archive_to_glacier,\n        transfer_archive_to_gcs,\n        # TEST TEARDOWN\n        delete_vault(vault_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_glacier_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple cloud service operators, involves AWS and GCS integration, includes sensors and custom tasks with external service interactions", "key_concepts": ["Cloud Storage Migration", "AWS Glacier", "GCS Transfer", "Airflow Operators", "Task Chaining"], "learning_objectives": ["Understand cloud data archiving workflows", "Learn inter-cloud data transfer techniques", "Master Airflow operator composition"]}}
{"instruction": "Design an Airflow DAG that demonstrates advanced cloud service orchestration by creating, managing, and deleting an AWS Glacier vault while implementing job tracking and data transfer mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "GlacierCreateJobOperator", "GlacierJobOperationSensor", "GlacierToGCSOperator", "GlacierUploadArchiveOperator"], "line_count": 103, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import task\nfrom airflow.providers.amazon.aws.operators.glacier import (\n    GlacierCreateJobOperator,\n    GlacierUploadArchiveOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.glacier import GlacierJobOperationSensor\nfrom airflow.providers.amazon.aws.transfers.glacier_to_gcs import GlacierToGCSOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_glacier_to_gcs\"\n\n\n@task\ndef create_vault(vault_name):\n    boto3.client(\"glacier\").create_vault(vaultName=vault_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_vault(vault_name):\n    boto3.client(\"glacier\").delete_vault(vaultName=vault_name)\n\n\nwith DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    vault_name = f\"{env_id}-vault\"\n    gcs_bucket_name = f\"{env_id}-bucket\"\n    gcs_object_name = f\"{env_id}-object\"\n\n    # [START howto_operator_glacier_create_job]\n    create_glacier_job = GlacierCreateJobOperator(task_id=\"create_glacier_job\", vault_name=vault_name)\n    JOB_ID = '{{ task_instance.xcom_pull(\"create_glacier_job\")[\"jobId\"] }}'\n    # [END howto_operator_glacier_create_job]\n\n    # [START howto_sensor_glacier_job_operation]\n    wait_for_operation_complete = GlacierJobOperationSensor(\n        vault_name=vault_name,\n        job_id=JOB_ID,\n        task_id=\"wait_for_operation_complete\",\n    )\n    # [END howto_sensor_glacier_job_operation]\n\n    # [START howto_operator_glacier_upload_archive]\n    upload_archive_to_glacier = GlacierUploadArchiveOperator(\n        task_id=\"upload_data_to_glacier\", vault_name=vault_name, body=b\"Test Data\"\n    )\n    # [END howto_operator_glacier_upload_archive]\n\n    # [START howto_transfer_glacier_to_gcs]\n    transfer_archive_to_gcs = GlacierToGCSOperator(\n        task_id=\"transfer_archive_to_gcs\",\n        vault_name=vault_name,\n        bucket_name=gcs_bucket_name,\n        object_name=gcs_object_name,\n        gzip=False,\n        # Override to match your needs\n        # If chunk size is bigger than actual file size\n        # then whole file will be downloaded\n        chunk_size=1024,\n    )\n    # [END howto_transfer_glacier_to_gcs]\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_vault(vault_name),\n        # TEST BODY\n        create_glacier_job,\n        wait_for_operation_complete,\n        upload_archive_to_glacier,\n        transfer_archive_to_gcs,\n        # TEST TEARDOWN\n        delete_vault(vault_name),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_glacier_to_gcs.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS service interactions, uses sensors for job completion, implements task chaining with custom Python tasks", "key_concepts": ["Cloud Service Orchestration", "Job Tracking", "Sensor Operators", "Task Lifecycle Management", "Error Handling"], "learning_objectives": ["Understand cloud service workflow management", "Learn job tracking and monitoring techniques", "Implement complex task dependencies"]}}
{"instruction": "Build an Airflow DAG that demonstrates transferring data from a Google Sheet to an S3 bucket using cloud service operators, focusing on understanding API-to-storage data transfer workflows.", "input": {"airflow_version": "2.7.2", "operators": ["GoogleApiToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 77, "is_multifile": true}, "output": "\"\"\"\nThis is a basic example dag for using `GoogleApiToS3Operator` to retrieve Google Sheets data\nYou need to set all env variables to request the data.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_google_api_sheets_to_s3\"\n\nGOOGLE_SHEET_ID = os.getenv(\"GOOGLE_SHEET_ID\", \"test-google-sheet-id\")\nGOOGLE_SHEET_RANGE = os.getenv(\"GOOGLE_SHEET_RANGE\", \"test-google-sheet-range\")\nS3_DESTINATION_KEY = os.getenv(\"S3_DESTINATION_KEY\", \"s3://test-bucket/key.json\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-google-api-sheets\"\n    s3_key = f\"{env_id}-google-api-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_google_api_sheets_to_s3]\n    task_google_sheets_values_to_s3 = GoogleApiToS3Operator(\n        task_id=\"google_sheet_data_to_s3\",\n        google_api_service_name=\"sheets\",\n        google_api_service_version=\"v4\",\n        google_api_endpoint_path=\"sheets.spreadsheets.values.get\",\n        google_api_endpoint_params={\"spreadsheetId\": GOOGLE_SHEET_ID, \"range\": GOOGLE_SHEET_RANGE},\n        s3_destination_key=f\"s3://{s3_bucket}/{s3_key}\",\n    )\n    # [END howto_transfer_google_api_sheets_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_google_sheets_values_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_google_api_sheets_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple cloud service operators (Google API, S3), involves external service integration, and demonstrates data transfer between platforms", "key_concepts": ["Cloud Service Integration", "Data Transfer Operators", "Google Sheets API", "S3 Bucket Management", "Environment Configuration"], "learning_objectives": ["Configure cloud service operators", "Transfer data between Google Sheets and S3", "Manage dynamic environment variables", "Implement task chaining in Airflow"]}}
{"instruction": "Design an Airflow DAG that showcases infrastructure management and cleanup techniques using S3 bucket creation and deletion with conditional task execution.", "input": {"airflow_version": "2.7.2", "operators": ["GoogleApiToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 77, "is_multifile": true}, "output": "\"\"\"\nThis is a basic example dag for using `GoogleApiToS3Operator` to retrieve Google Sheets data\nYou need to set all env variables to request the data.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_google_api_sheets_to_s3\"\n\nGOOGLE_SHEET_ID = os.getenv(\"GOOGLE_SHEET_ID\", \"test-google-sheet-id\")\nGOOGLE_SHEET_RANGE = os.getenv(\"GOOGLE_SHEET_RANGE\", \"test-google-sheet-range\")\nS3_DESTINATION_KEY = os.getenv(\"S3_DESTINATION_KEY\", \"s3://test-bucket/key.json\")\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-google-api-sheets\"\n    s3_key = f\"{env_id}-google-api-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_google_api_sheets_to_s3]\n    task_google_sheets_values_to_s3 = GoogleApiToS3Operator(\n        task_id=\"google_sheet_data_to_s3\",\n        google_api_service_name=\"sheets\",\n        google_api_service_version=\"v4\",\n        google_api_endpoint_path=\"sheets.spreadsheets.values.get\",\n        google_api_endpoint_params={\"spreadsheetId\": GOOGLE_SHEET_ID, \"range\": GOOGLE_SHEET_RANGE},\n        s3_destination_key=f\"s3://{s3_bucket}/{s3_key}\",\n    )\n    # [END howto_transfer_google_api_sheets_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_google_sheets_values_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_google_api_sheets_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates advanced task management with trigger rules, bucket lifecycle operations, and systematic test environment setup", "key_concepts": ["Infrastructure as Code", "Trigger Rules", "Cloud Resource Management", "Test Environment Setup", "Task Dependency Chaining"], "learning_objectives": ["Implement conditional task execution", "Manage cloud infrastructure programmatically", "Use trigger rules for task dependencies", "Create reproducible test environments"]}}
{"instruction": "Build an Airflow DAG that demonstrates transferring an email attachment from an IMAP server to an Amazon S3 bucket using the ImapAttachmentToS3Operator, showcasing cloud service integration and file transfer workflows.", "input": {"airflow_version": "2.7.2", "operators": ["ImapAttachmentToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 84, "is_multifile": true}, "output": "\"\"\"\nThis is an example dag for using `ImapAttachmentToS3Operator` to transfer an email attachment via IMAP\nprotocol from a mail server to S3 Bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.imap_attachment_to_s3 import ImapAttachmentToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_imap_attachment_to_s3\"\n\n# Externally fetched variables:\nIMAP_ATTACHMENT_NAME_KEY = \"IMAP_ATTACHMENT_NAME\"\nIMAP_MAIL_FOLDER_KEY = \"IMAP_MAIL_FOLDER\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(IMAP_ATTACHMENT_NAME_KEY)\n    .add_variable(IMAP_MAIL_FOLDER_KEY)\n    .build()\n)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    imap_attachment_name = test_context[IMAP_ATTACHMENT_NAME_KEY]\n    imap_mail_folder = test_context[IMAP_MAIL_FOLDER_KEY]\n\n    s3_bucket = f\"{env_id}-imap-attachment-to-s3-bucket\"\n    s3_key = f\"{env_id}-imap-attachment-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_imap_attachment_to_s3]\n    task_transfer_imap_attachment_to_s3 = ImapAttachmentToS3Operator(\n        task_id=\"transfer_imap_attachment_to_s3\",\n        imap_attachment_name=imap_attachment_name,\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        imap_mail_folder=imap_mail_folder,\n        imap_mail_filter=\"All\",\n    )\n    # [END howto_transfer_imap_attachment_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_transfer_imap_attachment_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_imap_attachment_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, requires external configuration variables, and demonstrates a multi-step transfer process", "key_concepts": ["Cloud Service Integration", "Email Attachment Transfer", "S3 Bucket Operations", "IMAP Protocol Handling", "Operator Chaining"], "learning_objectives": ["Understand cloud service operator usage", "Learn file transfer workflows", "Practice configuring multi-step DAGs", "Explore AWS service interactions"]}}
{"instruction": "Design an Airflow DAG that demonstrates robust cloud infrastructure management by creating, transferring data to, and then deleting an S3 bucket using AWS operators with proper trigger rules and error handling.", "input": {"airflow_version": "2.7.2", "operators": ["ImapAttachmentToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 84, "is_multifile": true}, "output": "\"\"\"\nThis is an example dag for using `ImapAttachmentToS3Operator` to transfer an email attachment via IMAP\nprotocol from a mail server to S3 Bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.imap_attachment_to_s3 import ImapAttachmentToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_imap_attachment_to_s3\"\n\n# Externally fetched variables:\nIMAP_ATTACHMENT_NAME_KEY = \"IMAP_ATTACHMENT_NAME\"\nIMAP_MAIL_FOLDER_KEY = \"IMAP_MAIL_FOLDER\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder()\n    .add_variable(IMAP_ATTACHMENT_NAME_KEY)\n    .add_variable(IMAP_MAIL_FOLDER_KEY)\n    .build()\n)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    imap_attachment_name = test_context[IMAP_ATTACHMENT_NAME_KEY]\n    imap_mail_folder = test_context[IMAP_MAIL_FOLDER_KEY]\n\n    s3_bucket = f\"{env_id}-imap-attachment-to-s3-bucket\"\n    s3_key = f\"{env_id}-imap-attachment-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_imap_attachment_to_s3]\n    task_transfer_imap_attachment_to_s3 = ImapAttachmentToS3Operator(\n        task_id=\"transfer_imap_attachment_to_s3\",\n        imap_attachment_name=imap_attachment_name,\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        imap_mail_folder=imap_mail_folder,\n        imap_mail_filter=\"All\",\n    )\n    # [END howto_transfer_imap_attachment_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        task_transfer_imap_attachment_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_imap_attachment_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements infrastructure lifecycle management, uses multiple AWS operators with advanced configuration like trigger rules, involves system test context building", "key_concepts": ["Infrastructure as Code", "AWS Operator Lifecycle", "Trigger Rule Configuration", "System Test Context Management", "Error Handling Strategies"], "learning_objectives": ["Understand infrastructure management in Airflow", "Learn AWS operator configuration", "Practice implementing robust DAG workflows", "Explore trigger rule applications"]}}
{"instruction": "Build an AWS QuickSight data pipeline that demonstrates end-to-end data ingestion from S3, creating a data source, dataset, and performing an ingestion using Airflow's AWS providers.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "QuickSightCreateIngestionOperator", "QuickSightSensor", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 214, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\n\"\"\"\nPrerequisites:\n1. The account which runs this test must manually be activated in Quicksight here:\nhttps://quicksight.aws.amazon.com/sn/console/signup?#\n2. The activation process creates an IAM Role called `aws-quicksight-service-role-v0`.\n You have to add a policy named 'AWSQuickSightS3Policy' with the S3 access permissions.\n The policy name is enforced, and the permissions json can be copied from `AmazonS3FullAccess`.\n\nNOTES:  If Create Ingestion fails for any reason, that ingestion name will remain in use and\n future runs will stall with the sensor returning a status of QUEUED \"forever\".  If you run\n into this behavior, changing the template for the ingestion name or the ENV_ID and re-running\n the test should resolve the issue.\n\"\"\"\n\nDAG_ID = \"example_quicksight\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_DATA_COLUMNS = [\"Project\", \"Year\"]\nSAMPLE_DATA = \"\"\"'Airflow','2015'\n    'OpenOffice','2012'\n    'Subversion','2000'\n    'NiFi','2006'\n\"\"\"\n\n\n@task\ndef get_aws_account_id() -> int:\n    return boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n\n\n@task\ndef create_quicksight_data_source(\n    aws_account_id: str, datasource_name: str, bucket: str, manifest_key: str\n) -> str:\n    response = boto3.client(\"quicksight\").create_data_source(\n        AwsAccountId=aws_account_id,\n        DataSourceId=datasource_name,\n        Name=datasource_name,\n        Type=\"S3\",\n        DataSourceParameters={\n            \"S3Parameters\": {\"ManifestFileLocation\": {\"Bucket\": bucket, \"Key\": manifest_key}}\n        },\n    )\n    return response[\"Arn\"]\n\n\n@task\ndef create_quicksight_dataset(aws_account_id: int, dataset_name: str, data_source_arn: str) -> None:\n    table_map = {\n        \"default\": {\n            \"S3Source\": {\n                \"DataSourceArn\": data_source_arn,\n                \"InputColumns\": [{\"Name\": name, \"Type\": \"STRING\"} for name in SAMPLE_DATA_COLUMNS],\n            }\n        }\n    }\n\n    boto3.client(\"quicksight\").create_data_set(\n        AwsAccountId=aws_account_id,\n        DataSetId=dataset_name,\n        Name=dataset_name,\n        PhysicalTableMap=table_map,\n        ImportMode=\"SPICE\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_quicksight_data_source(aws_account_id: str, datasource_name: str):\n    boto3.client(\"quicksight\").delete_data_source(AwsAccountId=aws_account_id, DataSourceId=datasource_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dataset(aws_account_id: str, dataset_name: str):\n    boto3.client(\"quicksight\").delete_data_set(AwsAccountId=aws_account_id, DataSetId=dataset_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(aws_account_id: str, dataset_name: str, ingestion_name: str) -> None:\n    client = boto3.client(\"quicksight\")\n    try:\n        client.cancel_ingestion(\n            AwsAccountId=aws_account_id,\n            DataSetId=dataset_name,\n            IngestionId=ingestion_name,\n        )\n    except client.exceptions.ResourceNotFoundException:\n        # Ingestion has already terminated on its own.\n        pass\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    account_id = get_aws_account_id()\n\n    env_id = test_context[ENV_ID_KEY]\n    bucket_name = f\"{env_id}-quicksight-bucket\"\n    data_filename = \"sample_data.csv\"\n    dataset_id = f\"{env_id}-data-set\"\n    datasource_id = f\"{env_id}-data-source\"\n    ingestion_id = f\"{env_id}-ingestion\"\n    manifest_filename = f\"{env_id}-manifest.json\"\n    manifest_contents = {\"fileLocations\": [{\"URIs\": [f\"s3://{bucket_name}/{data_filename}\"]}]}\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    upload_manifest_file = S3CreateObjectOperator(\n        task_id=\"upload_manifest_file\",\n        s3_bucket=bucket_name,\n        s3_key=manifest_filename,\n        data=json.dumps(manifest_contents),\n        replace=True,\n    )\n\n    upload_sample_data = S3CreateObjectOperator(\n        task_id=\"upload_sample_data\",\n        s3_bucket=bucket_name,\n        s3_key=data_filename,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    data_source = create_quicksight_data_source(\n        aws_account_id=account_id,\n        datasource_name=datasource_id,\n        bucket=bucket_name,\n        manifest_key=manifest_filename,\n    )\n\n    create_dataset = create_quicksight_dataset(account_id, dataset_id, data_source)\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightCreateIngestionOperator(\n        task_id=\"create_ingestion\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_operator_quicksight_create_ingestion]\n\n    # QuickSightCreateIngestionOperator waits by default, setting as False to test the Sensor below.\n    create_ingestion.wait_for_completion = False\n\n    # If this sensor appears to freeze with a \"QUEUED\" status, see note above.\n    # [START howto_sensor_quicksight]\n    await_job = QuickSightSensor(\n        task_id=\"await_job\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_sensor_quicksight]\n    await_job.poke_interval = 10\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        account_id,\n        create_s3_bucket,\n        upload_manifest_file,\n        upload_sample_data,\n        data_source,\n        create_dataset,\n        # TEST BODY\n        create_ingestion,\n        await_job,\n        # TEST TEARDOWN\n        delete_dataset(account_id, dataset_id),\n        delete_quicksight_data_source(account_id, datasource_id),\n        delete_ingestion(account_id, dataset_id, ingestion_id),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_quicksight.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS service operators, involves custom tasks with boto3, integrates S3 and QuickSight services, and requires complex task dependencies", "key_concepts": ["AWS Service Integration", "Task Dependencies", "Boto3 Interactions", "QuickSight Data Workflow", "Dynamic Task Creation"], "learning_objectives": ["Understand AWS service orchestration in Airflow", "Learn dynamic task generation with @task decorator", "Practice cloud service data pipeline construction"]}}
{"instruction": "Create a robust AWS data ingestion workflow demonstrating error handling, cleanup tasks, and systematic resource management using Airflow's trigger rules and task chaining.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "QuickSightCreateIngestionOperator", "QuickSightSensor", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator"], "line_count": 214, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.quicksight import QuickSightCreateIngestionOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.quicksight import QuickSightSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\n\"\"\"\nPrerequisites:\n1. The account which runs this test must manually be activated in Quicksight here:\nhttps://quicksight.aws.amazon.com/sn/console/signup?#\n2. The activation process creates an IAM Role called `aws-quicksight-service-role-v0`.\n You have to add a policy named 'AWSQuickSightS3Policy' with the S3 access permissions.\n The policy name is enforced, and the permissions json can be copied from `AmazonS3FullAccess`.\n\nNOTES:  If Create Ingestion fails for any reason, that ingestion name will remain in use and\n future runs will stall with the sensor returning a status of QUEUED \"forever\".  If you run\n into this behavior, changing the template for the ingestion name or the ENV_ID and re-running\n the test should resolve the issue.\n\"\"\"\n\nDAG_ID = \"example_quicksight\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nSAMPLE_DATA_COLUMNS = [\"Project\", \"Year\"]\nSAMPLE_DATA = \"\"\"'Airflow','2015'\n    'OpenOffice','2012'\n    'Subversion','2000'\n    'NiFi','2006'\n\"\"\"\n\n\n@task\ndef get_aws_account_id() -> int:\n    return boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n\n\n@task\ndef create_quicksight_data_source(\n    aws_account_id: str, datasource_name: str, bucket: str, manifest_key: str\n) -> str:\n    response = boto3.client(\"quicksight\").create_data_source(\n        AwsAccountId=aws_account_id,\n        DataSourceId=datasource_name,\n        Name=datasource_name,\n        Type=\"S3\",\n        DataSourceParameters={\n            \"S3Parameters\": {\"ManifestFileLocation\": {\"Bucket\": bucket, \"Key\": manifest_key}}\n        },\n    )\n    return response[\"Arn\"]\n\n\n@task\ndef create_quicksight_dataset(aws_account_id: int, dataset_name: str, data_source_arn: str) -> None:\n    table_map = {\n        \"default\": {\n            \"S3Source\": {\n                \"DataSourceArn\": data_source_arn,\n                \"InputColumns\": [{\"Name\": name, \"Type\": \"STRING\"} for name in SAMPLE_DATA_COLUMNS],\n            }\n        }\n    }\n\n    boto3.client(\"quicksight\").create_data_set(\n        AwsAccountId=aws_account_id,\n        DataSetId=dataset_name,\n        Name=dataset_name,\n        PhysicalTableMap=table_map,\n        ImportMode=\"SPICE\",\n    )\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_quicksight_data_source(aws_account_id: str, datasource_name: str):\n    boto3.client(\"quicksight\").delete_data_source(AwsAccountId=aws_account_id, DataSourceId=datasource_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_dataset(aws_account_id: str, dataset_name: str):\n    boto3.client(\"quicksight\").delete_data_set(AwsAccountId=aws_account_id, DataSetId=dataset_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ingestion(aws_account_id: str, dataset_name: str, ingestion_name: str) -> None:\n    client = boto3.client(\"quicksight\")\n    try:\n        client.cancel_ingestion(\n            AwsAccountId=aws_account_id,\n            DataSetId=dataset_name,\n            IngestionId=ingestion_name,\n        )\n    except client.exceptions.ResourceNotFoundException:\n        # Ingestion has already terminated on its own.\n        pass\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    account_id = get_aws_account_id()\n\n    env_id = test_context[ENV_ID_KEY]\n    bucket_name = f\"{env_id}-quicksight-bucket\"\n    data_filename = \"sample_data.csv\"\n    dataset_id = f\"{env_id}-data-set\"\n    datasource_id = f\"{env_id}-data-source\"\n    ingestion_id = f\"{env_id}-ingestion\"\n    manifest_filename = f\"{env_id}-manifest.json\"\n    manifest_contents = {\"fileLocations\": [{\"URIs\": [f\"s3://{bucket_name}/{data_filename}\"]}]}\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=bucket_name)\n\n    upload_manifest_file = S3CreateObjectOperator(\n        task_id=\"upload_manifest_file\",\n        s3_bucket=bucket_name,\n        s3_key=manifest_filename,\n        data=json.dumps(manifest_contents),\n        replace=True,\n    )\n\n    upload_sample_data = S3CreateObjectOperator(\n        task_id=\"upload_sample_data\",\n        s3_bucket=bucket_name,\n        s3_key=data_filename,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    data_source = create_quicksight_data_source(\n        aws_account_id=account_id,\n        datasource_name=datasource_id,\n        bucket=bucket_name,\n        manifest_key=manifest_filename,\n    )\n\n    create_dataset = create_quicksight_dataset(account_id, dataset_id, data_source)\n\n    # [START howto_operator_quicksight_create_ingestion]\n    create_ingestion = QuickSightCreateIngestionOperator(\n        task_id=\"create_ingestion\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_operator_quicksight_create_ingestion]\n\n    # QuickSightCreateIngestionOperator waits by default, setting as False to test the Sensor below.\n    create_ingestion.wait_for_completion = False\n\n    # If this sensor appears to freeze with a \"QUEUED\" status, see note above.\n    # [START howto_sensor_quicksight]\n    await_job = QuickSightSensor(\n        task_id=\"await_job\",\n        data_set_id=dataset_id,\n        ingestion_id=ingestion_id,\n    )\n    # [END howto_sensor_quicksight]\n    await_job.poke_interval = 10\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        account_id,\n        create_s3_bucket,\n        upload_manifest_file,\n        upload_sample_data,\n        data_source,\n        create_dataset,\n        # TEST BODY\n        create_ingestion,\n        await_job,\n        # TEST TEARDOWN\n        delete_dataset(account_id, dataset_id),\n        delete_quicksight_data_source(account_id, datasource_id),\n        delete_ingestion(account_id, dataset_id, ingestion_id),\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_quicksight.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements advanced task management with TriggerRule, includes cleanup and error handling tasks, uses complex task dependencies and AWS interactions", "key_concepts": ["Trigger Rules", "Resource Cleanup", "Error Handling", "Task Chaining", "AWS Resource Management"], "learning_objectives": ["Master Airflow task dependency management", "Learn resource cleanup strategies", "Understand graceful error handling in data pipelines"]}}
{"instruction": "Build an Airflow DAG that demonstrates local file creation and transfer to an S3 bucket using AWS operators, showcasing file management and cloud storage integration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "LocalFilesystemToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 87, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nDAG_ID = \"example_local_to_s3\"\nTEMP_FILE_PATH = \"/tmp/sample-txt.txt\"\nSAMPLE_TEXT = \"This is some sample text.\"\n\n\n@task\ndef create_temp_file():\n    file = open(TEMP_FILE_PATH, \"w\")\n    file.write(SAMPLE_TEXT)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_temp_file():\n    if os.path.exists(TEMP_FILE_PATH):\n        os.remove(TEMP_FILE_PATH)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/my-temp-file.txt\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create-s3-bucket\", bucket_name=s3_bucket_name)\n    # [START howto_transfer_local_to_s3]\n    create_local_to_s3_job = LocalFilesystemToS3Operator(\n        task_id=\"create_local_to_s3_job\",\n        filename=TEMP_FILE_PATH,\n        dest_key=s3_key,\n        dest_bucket=s3_bucket_name,\n        replace=True,\n    )\n    # [END howto_transfer_local_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_temp_file(),\n        create_s3_bucket,\n        # TEST BODY\n        create_local_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_temp_file(),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_local_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS S3 operators, involves file system interactions, includes task chaining with multiple steps, and demonstrates cloud service integration", "key_concepts": ["LocalFilesystemToS3Operator", "Task Chaining", "AWS S3 Interactions", "Decorator Tasks", "Trigger Rules"], "learning_objectives": ["Create and manipulate local files programmatically", "Transfer files to cloud storage using Airflow", "Understand task dependencies and chaining", "Implement cleanup and setup tasks in workflows"]}}
{"instruction": "Design an Airflow workflow that demonstrates robust file management practices, including temporary file creation, cloud storage upload, and automatic cleanup using trigger rules.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "LocalFilesystemToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 87, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport os\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nDAG_ID = \"example_local_to_s3\"\nTEMP_FILE_PATH = \"/tmp/sample-txt.txt\"\nSAMPLE_TEXT = \"This is some sample text.\"\n\n\n@task\ndef create_temp_file():\n    file = open(TEMP_FILE_PATH, \"w\")\n    file.write(SAMPLE_TEXT)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_temp_file():\n    if os.path.exists(TEMP_FILE_PATH):\n        os.remove(TEMP_FILE_PATH)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),  # Override to match your needs\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/my-temp-file.txt\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create-s3-bucket\", bucket_name=s3_bucket_name)\n    # [START howto_transfer_local_to_s3]\n    create_local_to_s3_job = LocalFilesystemToS3Operator(\n        task_id=\"create_local_to_s3_job\",\n        filename=TEMP_FILE_PATH,\n        dest_key=s3_key,\n        dest_bucket=s3_bucket_name,\n        replace=True,\n    )\n    # [END howto_transfer_local_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_temp_file(),\n        create_s3_bucket,\n        # TEST BODY\n        create_local_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n        delete_temp_file(),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_local_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Implements advanced task management with TriggerRule, involves file system operations, uses multiple task types, and demonstrates error-resilient workflow design", "key_concepts": ["TriggerRule", "File Management", "Error Handling", "Task Decorators", "Conditional Execution"], "learning_objectives": ["Implement safe file handling in data pipelines", "Use trigger rules for flexible task execution", "Create self-cleaning workflows", "Understand task dependency management"]}}
{"instruction": "Build an AWS RDS Event Monitoring DAG that demonstrates creating a PostgreSQL database instance, setting up an event subscription to an SNS topic, and then cleaning up resources in a controlled sequence.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RdsCreateDbInstanceOperator", "RdsCreateEventSubscriptionOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteEventSubscriptionOperator"], "line_count": 114, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateEventSubscriptionOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteEventSubscriptionOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_event\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_sns_topic(env_id) -> str:\n    return boto3.client(\"sns\").create_topic(Name=f\"{env_id}-topic\")[\"TopicArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_sns_topic(topic_arn) -> None:\n    boto3.client(\"sns\").delete_topic(TopicArn=topic_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_subscription_name = f\"{test_context[ENV_ID_KEY]}-subscription\"\n\n    sns_topic = create_sns_topic(test_context[ENV_ID_KEY])\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_event_subscription]\n    create_subscription = RdsCreateEventSubscriptionOperator(\n        task_id=\"create_subscription\",\n        subscription_name=rds_subscription_name,\n        sns_topic_arn=sns_topic,\n        source_type=\"db-instance\",\n        source_ids=[rds_instance_name],\n        event_categories=[\"availability\"],\n    )\n    # [END howto_operator_rds_create_event_subscription]\n\n    # [START howto_operator_rds_delete_event_subscription]\n    delete_subscription = RdsDeleteEventSubscriptionOperator(\n        task_id=\"delete_subscription\",\n        subscription_name=rds_subscription_name,\n    )\n    # [END howto_operator_rds_delete_event_subscription]\n    delete_subscription.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        sns_topic,\n        create_db_instance,\n        # TEST BODY\n        create_subscription,\n        delete_subscription,\n        # TEST TEARDOWN\n        delete_db_instance,\n        delete_sns_topic(sns_topic),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_event.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS RDS operators, involves cloud service integration, includes task chaining with custom tasks, and demonstrates resource lifecycle management", "key_concepts": ["AWS RDS Operators", "Event Subscriptions", "Resource Lifecycle Management", "Task Chaining", "SNS Integration"], "learning_objectives": ["Understand AWS RDS operator interactions", "Learn task dependency management", "Practice cloud resource orchestration", "Implement cleanup and teardown strategies"]}}
{"instruction": "Design an Airflow DAG that showcases safe database provisioning practices, including secure credential handling, event monitoring, and automated resource cleanup using AWS services.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RdsCreateDbInstanceOperator", "RdsCreateEventSubscriptionOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteEventSubscriptionOperator"], "line_count": 114, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsCreateEventSubscriptionOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteEventSubscriptionOperator,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_event\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_sns_topic(env_id) -> str:\n    return boto3.client(\"sns\").create_topic(Name=f\"{env_id}-topic\")[\"TopicArn\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_sns_topic(topic_arn) -> None:\n    boto3.client(\"sns\").delete_topic(TopicArn=topic_arn)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_subscription_name = f\"{test_context[ENV_ID_KEY]}-subscription\"\n\n    sns_topic = create_sns_topic(test_context[ENV_ID_KEY])\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_event_subscription]\n    create_subscription = RdsCreateEventSubscriptionOperator(\n        task_id=\"create_subscription\",\n        subscription_name=rds_subscription_name,\n        sns_topic_arn=sns_topic,\n        source_type=\"db-instance\",\n        source_ids=[rds_instance_name],\n        event_categories=[\"availability\"],\n    )\n    # [END howto_operator_rds_create_event_subscription]\n\n    # [START howto_operator_rds_delete_event_subscription]\n    delete_subscription = RdsDeleteEventSubscriptionOperator(\n        task_id=\"delete_subscription\",\n        subscription_name=rds_subscription_name,\n    )\n    # [END howto_operator_rds_delete_event_subscription]\n    delete_subscription.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        sns_topic,\n        create_db_instance,\n        # TEST BODY\n        create_subscription,\n        delete_subscription,\n        # TEST TEARDOWN\n        delete_db_instance,\n        delete_sns_topic(sns_topic),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_event.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS operators, demonstrates security considerations, uses trigger rules, and implements complex task dependencies", "key_concepts": ["Cloud Security", "Trigger Rules", "Resource Provisioning", "Event Monitoring", "Automated Cleanup"], "learning_objectives": ["Understand secure cloud resource management", "Learn about event subscription mechanisms", "Practice safe credential handling", "Implement robust task dependency strategies"]}}
{"instruction": "Build an Airflow DAG that demonstrates transferring filtered MongoDB documents to an S3 bucket, showcasing data migration and cloud storage integration.", "input": {"airflow_version": "2.7.2", "operators": ["MongoToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 78, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.mongo_to_s3 import MongoToS3Operator\nfrom airflow.utils.dates import datetime\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_mongo_to_s3\"\n\n# Externally fetched variables:\nMONGO_DATABASE_KEY = \"MONGO_DATABASE\"\nMONGO_COLLECTION_KEY = \"MONGO_COLLECTION\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(MONGO_DATABASE_KEY).add_variable(MONGO_COLLECTION_KEY).build()\n)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    mongo_database = test_context[MONGO_DATABASE_KEY]\n    mongo_collection = test_context[MONGO_COLLECTION_KEY]\n\n    s3_bucket = f\"{env_id}-mongo-to-s3-bucket\"\n    s3_key = f\"{env_id}-mongo-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_mongo_to_s3]\n    mongo_to_s3_job = MongoToS3Operator(\n        task_id=\"mongo_to_s3_job\",\n        mongo_collection=mongo_collection,\n        # Mongo query by matching values\n        # Here returns all documents which have \"OK\" as value for the key \"status\"\n        mongo_query={\"status\": \"OK\"},\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        mongo_db=mongo_database,\n        replace=True,\n    )\n    # [END howto_transfer_mongo_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        mongo_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_mongo_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, external service integration (MongoDB to S3), includes query filtering, and demonstrates cloud data transfer workflow", "key_concepts": ["Data Transfer", "Cloud Storage", "Operator Chaining", "MongoDB Integration", "S3 Bucket Management"], "learning_objectives": ["Understand inter-service data migration", "Learn AWS operator usage", "Practice document filtering techniques", "Implement DAG task sequencing"]}}
{"instruction": "Design an Airflow workflow that demonstrates robust cloud resource management, including dynamic bucket creation, conditional data transfer, and automatic cleanup.", "input": {"airflow_version": "2.7.2", "operators": ["MongoToS3Operator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 78, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.mongo_to_s3 import MongoToS3Operator\nfrom airflow.utils.dates import datetime\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nDAG_ID = \"example_mongo_to_s3\"\n\n# Externally fetched variables:\nMONGO_DATABASE_KEY = \"MONGO_DATABASE\"\nMONGO_COLLECTION_KEY = \"MONGO_COLLECTION\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(MONGO_DATABASE_KEY).add_variable(MONGO_COLLECTION_KEY).build()\n)\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n    mongo_database = test_context[MONGO_DATABASE_KEY]\n    mongo_collection = test_context[MONGO_COLLECTION_KEY]\n\n    s3_bucket = f\"{env_id}-mongo-to-s3-bucket\"\n    s3_key = f\"{env_id}-mongo-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_mongo_to_s3]\n    mongo_to_s3_job = MongoToS3Operator(\n        task_id=\"mongo_to_s3_job\",\n        mongo_collection=mongo_collection,\n        # Mongo query by matching values\n        # Here returns all documents which have \"OK\" as value for the key \"status\"\n        mongo_query={\"status\": \"OK\"},\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n        mongo_db=mongo_database,\n        replace=True,\n    )\n    # [END howto_transfer_mongo_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        mongo_to_s3_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_mongo_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple AWS operators, uses dynamic environment variables, implements trigger rules, and shows resource lifecycle management", "key_concepts": ["Cloud Resource Management", "Dynamic Configuration", "Trigger Rules", "Environment Isolation", "Automated Cleanup"], "learning_objectives": ["Understand cloud resource provisioning", "Learn dynamic task configuration", "Practice safe resource deletion", "Implement conditional workflow execution"]}}
{"instruction": "Build an AWS Lambda function deployment and invocation workflow using Airflow, demonstrating how to create, wait for, and invoke a Lambda function with a simple Python runtime.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "LambdaCreateFunctionOperator", "LambdaFunctionStateSensor", "LambdaInvokeFunctionOperator"], "line_count": 124, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport io\nimport json\nimport zipfile\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.lambda_function import (\n    LambdaCreateFunctionOperator,\n    LambdaInvokeFunctionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.lambda_function import LambdaFunctionStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_lambda\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nCODE_CONTENT = \"\"\"\ndef test(*args):\n    print('Hello')\n\"\"\"\n\n\n# Create a zip file containing one file \"lambda_function.py\" to deploy to the lambda function\ndef create_zip(content: str):\n    zip_output = io.BytesIO()\n    with zipfile.ZipFile(zip_output, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        info = zipfile.ZipInfo(\"lambda_function.py\")\n        info.external_attr = 0o777 << 16\n        zip_file.writestr(info, content)\n    zip_output.seek(0)\n    return zip_output.read()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_lambda(function_name: str):\n    client = boto3.client(\"lambda\")\n    client.delete_function(\n        FunctionName=function_name,\n    )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    lambda_function_name: str = f\"{test_context[ENV_ID_KEY]}-function\"\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    # [START howto_operator_create_lambda_function]\n    create_lambda_function = LambdaCreateFunctionOperator(\n        task_id=\"create_lambda_function\",\n        function_name=lambda_function_name,\n        runtime=\"python3.9\",\n        role=role_arn,\n        handler=\"lambda_function.test\",\n        code={\n            \"ZipFile\": create_zip(CODE_CONTENT),\n        },\n    )\n    # [END howto_operator_create_lambda_function]\n\n    # [START howto_sensor_lambda_function_state]\n    wait_lambda_function_state = LambdaFunctionStateSensor(\n        task_id=\"wait_lambda_function_state\",\n        function_name=lambda_function_name,\n    )\n    # [END howto_sensor_lambda_function_state]\n    wait_lambda_function_state.poke_interval = 1\n\n    # [START howto_operator_invoke_lambda_function]\n    invoke_lambda_function = LambdaInvokeFunctionOperator(\n        task_id=\"invoke_lambda_function\",\n        function_name=lambda_function_name,\n        payload=json.dumps({\"SampleEvent\": {\"SampleData\": {\"Name\": \"XYZ\", \"DoB\": \"1993-01-01\"}}}),\n    )\n    # [END howto_operator_invoke_lambda_function]\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (f\"/aws/lambda/{lambda_function_name}\", None),\n        ],\n        force_delete=True,\n        retry=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_lambda_function,\n        wait_lambda_function_state,\n        invoke_lambda_function,\n        # TEST TEARDOWN\n        delete_lambda(lambda_function_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_lambda.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS Lambda operators, involves external service integration, includes custom function creation, and demonstrates sensor and invocation logic", "key_concepts": ["AWS Lambda integration", "Airflow operators", "Function deployment", "Task chaining", "Cloud service orchestration"], "learning_objectives": ["Create Lambda functions programmatically", "Use Airflow sensors to check function state", "Invoke Lambda functions with custom payloads", "Manage cloud function lifecycle"]}}
{"instruction": "Design an Airflow DAG that demonstrates serverless function management with comprehensive error handling, including function creation, state validation, invocation, and automatic cleanup.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "LambdaCreateFunctionOperator", "LambdaFunctionStateSensor", "LambdaInvokeFunctionOperator"], "line_count": 124, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport io\nimport json\nimport zipfile\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import models\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.lambda_function import (\n    LambdaCreateFunctionOperator,\n    LambdaInvokeFunctionOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.lambda_function import LambdaFunctionStateSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_lambda\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\nCODE_CONTENT = \"\"\"\ndef test(*args):\n    print('Hello')\n\"\"\"\n\n\n# Create a zip file containing one file \"lambda_function.py\" to deploy to the lambda function\ndef create_zip(content: str):\n    zip_output = io.BytesIO()\n    with zipfile.ZipFile(zip_output, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        info = zipfile.ZipInfo(\"lambda_function.py\")\n        info.external_attr = 0o777 << 16\n        zip_file.writestr(info, content)\n    zip_output.seek(0)\n    return zip_output.read()\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_lambda(function_name: str):\n    client = boto3.client(\"lambda\")\n    client.delete_function(\n        FunctionName=function_name,\n    )\n\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    lambda_function_name: str = f\"{test_context[ENV_ID_KEY]}-function\"\n    role_arn = test_context[ROLE_ARN_KEY]\n\n    # [START howto_operator_create_lambda_function]\n    create_lambda_function = LambdaCreateFunctionOperator(\n        task_id=\"create_lambda_function\",\n        function_name=lambda_function_name,\n        runtime=\"python3.9\",\n        role=role_arn,\n        handler=\"lambda_function.test\",\n        code={\n            \"ZipFile\": create_zip(CODE_CONTENT),\n        },\n    )\n    # [END howto_operator_create_lambda_function]\n\n    # [START howto_sensor_lambda_function_state]\n    wait_lambda_function_state = LambdaFunctionStateSensor(\n        task_id=\"wait_lambda_function_state\",\n        function_name=lambda_function_name,\n    )\n    # [END howto_sensor_lambda_function_state]\n    wait_lambda_function_state.poke_interval = 1\n\n    # [START howto_operator_invoke_lambda_function]\n    invoke_lambda_function = LambdaInvokeFunctionOperator(\n        task_id=\"invoke_lambda_function\",\n        function_name=lambda_function_name,\n        payload=json.dumps({\"SampleEvent\": {\"SampleData\": {\"Name\": \"XYZ\", \"DoB\": \"1993-01-01\"}}}),\n    )\n    # [END howto_operator_invoke_lambda_function]\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (f\"/aws/lambda/{lambda_function_name}\", None),\n        ],\n        force_delete=True,\n        retry=True,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_lambda_function,\n        wait_lambda_function_state,\n        invoke_lambda_function,\n        # TEST TEARDOWN\n        delete_lambda(lambda_function_name),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_lambda.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Incorporates advanced workflow patterns, uses multiple AWS operators, includes custom task for function deletion, and implements log pruning", "key_concepts": ["Serverless orchestration", "Error handling", "Resource lifecycle management", "Cloud service cleanup", "Dynamic function deployment"], "learning_objectives": ["Implement complete serverless function workflow", "Handle function state and monitoring", "Perform automated resource cleanup", "Use trigger rules for task dependencies"]}}
{"instruction": "Build an AWS RDS database lifecycle management workflow that demonstrates creating, monitoring, stopping, starting, and deleting a PostgreSQL database instance using Airflow's RDS operators.", "input": {"airflow_version": "2.7.2", "operators": ["RdsCreateDbInstanceOperator", "RdsDbSensor", "RdsDeleteDbInstanceOperator", "RdsStartDbOperator", "RdsStopDbOperator"], "line_count": 107, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsStartDbOperator,\n    RdsStopDbOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsDbSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_rds_instance\"\n\nRDS_USERNAME = \"database_username\"\n# NEVER store your production password in plaintext in a DAG like this.\n# Use Airflow Secrets or a secret manager for this in production.\nRDS_PASSWORD = \"database_password\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    rds_db_identifier = f\"{test_context[ENV_ID_KEY]}-database\"\n\n    # [START howto_operator_rds_create_db_instance]\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"AllocatedStorage\": 20,\n            \"PubliclyAccessible\": False,\n        },\n    )\n    # [END howto_operator_rds_create_db_instance]\n\n    # RdsCreateDbInstanceOperator waits by default, setting as False to test the Sensor below.\n    create_db_instance.wait_for_completion = False\n\n    # [START howto_sensor_rds_instance]\n    await_db_instance = RdsDbSensor(\n        task_id=\"await_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_sensor_rds_instance]\n\n    # [START howto_operator_rds_stop_db]\n    stop_db_instance = RdsStopDbOperator(\n        task_id=\"stop_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_stop_db]\n\n    # [START howto_operator_rds_start_db]\n    start_db_instance = RdsStartDbOperator(\n        task_id=\"start_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_start_db]\n\n    # [START howto_operator_rds_delete_db_instance]\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n    )\n    # [END howto_operator_rds_delete_db_instance]\n    delete_db_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_db_instance,\n        await_db_instance,\n        stop_db_instance,\n        start_db_instance,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_instance.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS RDS operators, includes a sensor for monitoring, involves cloud service integration, and demonstrates sequential task chaining", "key_concepts": ["AWS RDS Integration", "Airflow Operators", "Cloud Service Orchestration", "Task Chaining", "Database Lifecycle Management"], "learning_objectives": ["Understand RDS operator usage", "Learn task dependency management", "Practice cloud service automation", "Explore Airflow sensor functionality"]}}
{"instruction": "Design a robust database provisioning and teardown automation workflow that demonstrates safe, controlled database instance management using trigger rules and sensors in Airflow.", "input": {"airflow_version": "2.7.2", "operators": ["RdsCreateDbInstanceOperator", "RdsDbSensor", "RdsDeleteDbInstanceOperator", "RdsStartDbOperator", "RdsStopDbOperator"], "line_count": 107, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCreateDbInstanceOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsStartDbOperator,\n    RdsStopDbOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsDbSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_rds_instance\"\n\nRDS_USERNAME = \"database_username\"\n# NEVER store your production password in plaintext in a DAG like this.\n# Use Airflow Secrets or a secret manager for this in production.\nRDS_PASSWORD = \"database_password\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    rds_db_identifier = f\"{test_context[ENV_ID_KEY]}-database\"\n\n    # [START howto_operator_rds_create_db_instance]\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": RDS_USERNAME,\n            \"MasterUserPassword\": RDS_PASSWORD,\n            \"AllocatedStorage\": 20,\n            \"PubliclyAccessible\": False,\n        },\n    )\n    # [END howto_operator_rds_create_db_instance]\n\n    # RdsCreateDbInstanceOperator waits by default, setting as False to test the Sensor below.\n    create_db_instance.wait_for_completion = False\n\n    # [START howto_sensor_rds_instance]\n    await_db_instance = RdsDbSensor(\n        task_id=\"await_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_sensor_rds_instance]\n\n    # [START howto_operator_rds_stop_db]\n    stop_db_instance = RdsStopDbOperator(\n        task_id=\"stop_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_stop_db]\n\n    # [START howto_operator_rds_start_db]\n    start_db_instance = RdsStartDbOperator(\n        task_id=\"start_db_instance\",\n        db_identifier=rds_db_identifier,\n    )\n    # [END howto_operator_rds_start_db]\n\n    # [START howto_operator_rds_delete_db_instance]\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_db_identifier,\n        rds_kwargs={\n            \"SkipFinalSnapshot\": True,\n        },\n    )\n    # [END howto_operator_rds_delete_db_instance]\n    delete_db_instance.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_db_instance,\n        await_db_instance,\n        stop_db_instance,\n        start_db_instance,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_instance.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves advanced task configuration, uses trigger rules, implements a sensor for state monitoring, and handles complex task dependencies", "key_concepts": ["Trigger Rules", "Database Provisioning", "Airflow Sensors", "Task Dependency Management", "Operational Automation"], "learning_objectives": ["Understand conditional task execution", "Learn safe resource management", "Practice state monitoring techniques", "Explore advanced Airflow configuration"]}}
{"instruction": "Build an AWS RDS database export workflow that demonstrates creating a PostgreSQL database instance, generating a snapshot, and exporting that snapshot to an S3 bucket using AWS RDS and S3 operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RdsCancelExportTaskOperator", "RdsCreateDbInstanceOperator", "RdsCreateDbSnapshotOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteDbSnapshotOperator", "RdsExportTaskExistenceSensor", "RdsSnapshotExistenceSensor", "RdsStartExportTaskOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 173, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCancelExportTaskOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n    RdsStartExportTaskOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RdsExportTaskExistenceSensor, RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_export\"\n\n# Externally fetched variables:\nKMS_KEY_ID_KEY = \"KMS_KEY_ID\"\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(KMS_KEY_ID_KEY).add_variable(ROLE_ARN_KEY).build()\n)\n\n\n@task\ndef get_snapshot_arn(snapshot_name: str) -> str:\n    result = RdsHook().conn.describe_db_snapshots(DBSnapshotIdentifier=snapshot_name)\n    return result[\"DBSnapshots\"][0][\"DBSnapshotArn\"]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name: str = f\"{env_id}-bucket\"\n\n    rds_db_name: str = f\"{env_id}_db\"\n    rds_instance_name: str = f\"{env_id}-instance\"\n    rds_snapshot_name: str = f\"{env_id}-snapshot\"\n    rds_export_task_id: str = f\"{env_id}-export-task\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n\n    await_snapshot = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n\n    snapshot_arn = get_snapshot_arn(rds_snapshot_name)\n\n    # [START howto_operator_rds_start_export_task]\n    start_export = RdsStartExportTaskOperator(\n        task_id=\"start_export\",\n        export_task_identifier=rds_export_task_id,\n        source_arn=snapshot_arn,\n        s3_bucket_name=bucket_name,\n        s3_prefix=\"rds-test\",\n        iam_role_arn=test_context[ROLE_ARN_KEY],\n        kms_key_id=test_context[KMS_KEY_ID_KEY],\n    )\n    # [END howto_operator_rds_start_export_task]\n\n    # RdsStartExportTaskOperator waits by default, setting as False to test the Sensor below.\n    start_export.wait_for_completion = False\n\n    # [START howto_operator_rds_cancel_export]\n    cancel_export = RdsCancelExportTaskOperator(\n        task_id=\"cancel_export\",\n        export_task_identifier=rds_export_task_id,\n    )\n    # [END howto_operator_rds_cancel_export]\n    cancel_export.check_interval = 10\n    cancel_export.max_attempts = 120\n\n    # [START howto_sensor_rds_export_task_existence]\n    export_sensor = RdsExportTaskExistenceSensor(\n        task_id=\"export_sensor\",\n        export_task_identifier=rds_export_task_id,\n        target_statuses=[\"canceled\"],\n    )\n    # [END howto_sensor_rds_export_task_existence]\n\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        create_db_instance,\n        create_snapshot,\n        await_snapshot,\n        snapshot_arn,\n        # TEST BODY\n        start_export,\n        cancel_export,\n        export_sensor,\n        # TEST TEARDOWN\n        delete_snapshot,\n        delete_bucket,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_export.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS service operators, includes sensors, custom task function, and involves cloud service interactions with conditional export task management", "key_concepts": ["AWS RDS Operators", "S3 Integration", "Database Snapshot Export", "Task Chaining", "Cloud Service Orchestration"], "learning_objectives": ["Understand RDS database lifecycle management", "Learn AWS service integration in Airflow", "Practice task dependency configuration", "Explore export task monitoring techniques"]}}
{"instruction": "Design an automated database backup and export system that demonstrates secure, configurable snapshot management using AWS RDS operators, with emphasis on task cancellation and export task monitoring.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RdsCancelExportTaskOperator", "RdsCreateDbInstanceOperator", "RdsCreateDbSnapshotOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteDbSnapshotOperator", "RdsExportTaskExistenceSensor", "RdsSnapshotExistenceSensor", "RdsStartExportTaskOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator"], "line_count": 173, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCancelExportTaskOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n    RdsStartExportTaskOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.rds import RdsExportTaskExistenceSensor, RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_export\"\n\n# Externally fetched variables:\nKMS_KEY_ID_KEY = \"KMS_KEY_ID\"\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = (\n    SystemTestContextBuilder().add_variable(KMS_KEY_ID_KEY).add_variable(ROLE_ARN_KEY).build()\n)\n\n\n@task\ndef get_snapshot_arn(snapshot_name: str) -> str:\n    result = RdsHook().conn.describe_db_snapshots(DBSnapshotIdentifier=snapshot_name)\n    return result[\"DBSnapshots\"][0][\"DBSnapshotArn\"]\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name: str = f\"{env_id}-bucket\"\n\n    rds_db_name: str = f\"{env_id}_db\"\n    rds_instance_name: str = f\"{env_id}-instance\"\n    rds_snapshot_name: str = f\"{env_id}-snapshot\"\n    rds_export_task_id: str = f\"{env_id}-export-task\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n\n    await_snapshot = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n\n    snapshot_arn = get_snapshot_arn(rds_snapshot_name)\n\n    # [START howto_operator_rds_start_export_task]\n    start_export = RdsStartExportTaskOperator(\n        task_id=\"start_export\",\n        export_task_identifier=rds_export_task_id,\n        source_arn=snapshot_arn,\n        s3_bucket_name=bucket_name,\n        s3_prefix=\"rds-test\",\n        iam_role_arn=test_context[ROLE_ARN_KEY],\n        kms_key_id=test_context[KMS_KEY_ID_KEY],\n    )\n    # [END howto_operator_rds_start_export_task]\n\n    # RdsStartExportTaskOperator waits by default, setting as False to test the Sensor below.\n    start_export.wait_for_completion = False\n\n    # [START howto_operator_rds_cancel_export]\n    cancel_export = RdsCancelExportTaskOperator(\n        task_id=\"cancel_export\",\n        export_task_identifier=rds_export_task_id,\n    )\n    # [END howto_operator_rds_cancel_export]\n    cancel_export.check_interval = 10\n    cancel_export.max_attempts = 120\n\n    # [START howto_sensor_rds_export_task_existence]\n    export_sensor = RdsExportTaskExistenceSensor(\n        task_id=\"export_sensor\",\n        export_task_identifier=rds_export_task_id,\n        target_statuses=[\"canceled\"],\n    )\n    # [END howto_sensor_rds_export_task_existence]\n\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        create_db_instance,\n        create_snapshot,\n        await_snapshot,\n        snapshot_arn,\n        # TEST BODY\n        start_export,\n        cancel_export,\n        export_sensor,\n        # TEST TEARDOWN\n        delete_snapshot,\n        delete_bucket,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_export.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves advanced operator interactions, custom task logic, sensor usage for task state tracking, and demonstrates error handling patterns", "key_concepts": ["Export Task Management", "Database Snapshot Sensors", "AWS IAM Role Usage", "Conditional Task Execution", "Secure Credential Handling"], "learning_objectives": ["Implement secure database export workflows", "Understand task cancellation mechanisms", "Learn sensor-based task monitoring", "Practice dynamic task configuration"]}}
{"instruction": "Build an Airflow DAG that demonstrates AWS Redshift cluster management, including creating a cluster, setting up security groups, and establishing database connections using Airflow's AWS providers.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftCreateClusterSnapshotOperator", "RedshiftDataOperator", "RedshiftDeleteClusterOperator", "RedshiftDeleteClusterSnapshotOperator", "RedshiftPauseClusterOperator", "RedshiftResumeClusterOperator", "SQLExecuteQueryOperator"], "line_count": 270, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftCreateClusterSnapshotOperator,\n    RedshiftDeleteClusterOperator,\n    RedshiftDeleteClusterSnapshotOperator,\n    RedshiftPauseClusterOperator,\n    RedshiftResumeClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.redshift_data import RedshiftDataOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\nPOLL_INTERVAL = 10\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    redshift_cluster_snapshot_identifier = f\"{env_id}-snapshot\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    # [START howto_operator_redshift_cluster]\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n    # [END howto_operator_redshift_cluster]\n\n    # [START howto_sensor_redshift_cluster]\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n    # [END howto_sensor_redshift_cluster]\n\n    # [START howto_operator_redshift_create_cluster_snapshot]\n    create_cluster_snapshot = RedshiftCreateClusterSnapshotOperator(\n        task_id=\"create_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n        poll_interval=30,\n        max_attempt=100,\n        retention_period=1,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_create_cluster_snapshot]\n\n    wait_cluster_available_before_pause = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_before_pause\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_pause_cluster]\n    pause_cluster = RedshiftPauseClusterOperator(\n        task_id=\"pause_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_pause_cluster]\n\n    wait_cluster_paused = RedshiftClusterSensor(\n        task_id=\"wait_cluster_paused\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"paused\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_resume_cluster]\n    resume_cluster = RedshiftResumeClusterOperator(\n        task_id=\"resume_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_resume_cluster]\n\n    wait_cluster_available_after_resume = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_after_resume\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    # [START howto_operator_redshift_data]\n    create_table_redshift_data = RedshiftDataOperator(\n        task_id=\"create_table_redshift_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS fruit (\n            fruit_id INTEGER,\n            name VARCHAR NOT NULL,\n            color VARCHAR NOT NULL\n            );\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_data]\n\n    insert_data = RedshiftDataOperator(\n        task_id=\"insert_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            INSERT INTO fruit VALUES ( 1, 'Banana', 'Yellow');\n            INSERT INTO fruit VALUES ( 2, 'Apple', 'Red');\n            INSERT INTO fruit VALUES ( 3, 'Lemon', 'Yellow');\n            INSERT INTO fruit VALUES ( 4, 'Grape', 'Purple');\n            INSERT INTO fruit VALUES ( 5, 'Pear', 'Green');\n            INSERT INTO fruit VALUES ( 6, 'Strawberry', 'Red');\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=\"DROP TABLE IF EXISTS fruit\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_redshift_delete_cluster]\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_redshift_delete_cluster_snapshot]\n    delete_cluster_snapshot = RedshiftDeleteClusterSnapshotOperator(\n        task_id=\"delete_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster_snapshot]\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        # TEST BODY\n        create_cluster,\n        wait_cluster_available,\n        create_cluster_snapshot,\n        wait_cluster_available_before_pause,\n        pause_cluster,\n        wait_cluster_paused,\n        resume_cluster,\n        wait_cluster_available_after_resume,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        drop_table,\n        delete_cluster_snapshot,\n        delete_cluster,\n        # TEST TEARDOWN\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_redshift.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, includes custom tasks for security group and connection management, requires understanding of AWS Redshift infrastructure", "key_concepts": ["AWS Redshift Integration", "Cloud Service Operators", "Security Group Configuration", "Dynamic Connection Creation", "Task Dependencies"], "learning_objectives": ["Understand AWS Redshift cluster lifecycle management", "Learn how to configure cloud service connections in Airflow", "Practice creating dynamic tasks with AWS providers"]}}
{"instruction": "Design an Airflow workflow that demonstrates infrastructure-as-code principles by programmatically managing cloud resources, focusing on secure, reproducible cloud infrastructure deployment.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftCreateClusterSnapshotOperator", "RedshiftDataOperator", "RedshiftDeleteClusterOperator", "RedshiftDeleteClusterSnapshotOperator", "RedshiftPauseClusterOperator", "RedshiftResumeClusterOperator", "SQLExecuteQueryOperator"], "line_count": 270, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftCreateClusterSnapshotOperator,\n    RedshiftDeleteClusterOperator,\n    RedshiftDeleteClusterSnapshotOperator,\n    RedshiftPauseClusterOperator,\n    RedshiftResumeClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.redshift_data import RedshiftDataOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\nPOLL_INTERVAL = 10\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    redshift_cluster_snapshot_identifier = f\"{env_id}-snapshot\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    # [START howto_operator_redshift_cluster]\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n    # [END howto_operator_redshift_cluster]\n\n    # [START howto_sensor_redshift_cluster]\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n    # [END howto_sensor_redshift_cluster]\n\n    # [START howto_operator_redshift_create_cluster_snapshot]\n    create_cluster_snapshot = RedshiftCreateClusterSnapshotOperator(\n        task_id=\"create_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n        poll_interval=30,\n        max_attempt=100,\n        retention_period=1,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_create_cluster_snapshot]\n\n    wait_cluster_available_before_pause = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_before_pause\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_pause_cluster]\n    pause_cluster = RedshiftPauseClusterOperator(\n        task_id=\"pause_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_pause_cluster]\n\n    wait_cluster_paused = RedshiftClusterSensor(\n        task_id=\"wait_cluster_paused\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"paused\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    # [START howto_operator_redshift_resume_cluster]\n    resume_cluster = RedshiftResumeClusterOperator(\n        task_id=\"resume_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_resume_cluster]\n\n    wait_cluster_available_after_resume = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available_after_resume\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    # [START howto_operator_redshift_data]\n    create_table_redshift_data = RedshiftDataOperator(\n        task_id=\"create_table_redshift_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS fruit (\n            fruit_id INTEGER,\n            name VARCHAR NOT NULL,\n            color VARCHAR NOT NULL\n            );\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n    # [END howto_operator_redshift_data]\n\n    insert_data = RedshiftDataOperator(\n        task_id=\"insert_data\",\n        cluster_identifier=redshift_cluster_identifier,\n        database=DB_NAME,\n        db_user=DB_LOGIN,\n        sql=\"\"\"\n            INSERT INTO fruit VALUES ( 1, 'Banana', 'Yellow');\n            INSERT INTO fruit VALUES ( 2, 'Apple', 'Red');\n            INSERT INTO fruit VALUES ( 3, 'Lemon', 'Yellow');\n            INSERT INTO fruit VALUES ( 4, 'Grape', 'Purple');\n            INSERT INTO fruit VALUES ( 5, 'Pear', 'Green');\n            INSERT INTO fruit VALUES ( 6, 'Strawberry', 'Red');\n        \"\"\",\n        poll_interval=POLL_INTERVAL,\n        wait_for_completion=True,\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=\"DROP TABLE IF EXISTS fruit\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    # [START howto_operator_redshift_delete_cluster]\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster]\n    delete_cluster.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_redshift_delete_cluster_snapshot]\n    delete_cluster_snapshot = RedshiftDeleteClusterSnapshotOperator(\n        task_id=\"delete_cluster_snapshot\",\n        cluster_identifier=redshift_cluster_identifier,\n        snapshot_identifier=redshift_cluster_snapshot_identifier,\n    )\n    # [END howto_operator_redshift_delete_cluster_snapshot]\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        # TEST BODY\n        create_cluster,\n        wait_cluster_available,\n        create_cluster_snapshot,\n        wait_cluster_available_before_pause,\n        pause_cluster,\n        wait_cluster_paused,\n        resume_cluster,\n        wait_cluster_available_after_resume,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        drop_table,\n        delete_cluster_snapshot,\n        delete_cluster,\n        # TEST TEARDOWN\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_redshift.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple infrastructure tasks, uses boto3 and AWS operators, requires understanding of security group creation, cluster configuration, and task orchestration", "key_concepts": ["Infrastructure as Code", "Cloud Resource Management", "Security Configuration", "Dynamic Task Generation", "Idempotent Workflow Design"], "learning_objectives": ["Learn infrastructure provisioning through code", "Understand secure cloud resource management", "Practice creating reproducible cloud deployment workflows"]}}
{"instruction": "Build an Airflow DAG that demonstrates end-to-end data transfer between Redshift and S3, showcasing cloud service integration and data pipeline orchestration.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "RedshiftToS3Operator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3KeySensor", "S3ToRedshiftOperator", "SQLExecuteQueryOperator"], "line_count": 266, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.amazon.aws.transfers.redshift_to_s3 import RedshiftToS3Operator\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift_to_s3\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nS3_KEY = \"s3_output_\"\nS3_KEY_2 = \"s3_key_2\"\nS3_KEY_PREFIX = \"s3_k\"\nREDSHIFT_TABLE = \"test_table\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\nSQL_DROP_TABLE = f\"DROP TABLE IF EXISTS {REDSHIFT_TABLE};\"\n\nDATA = \"0, 'Airflow', 'testing'\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-bucket\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"s3_create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        data=DATA,\n        replace=True,\n    )\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_redshift_to_s3]\n    transfer_redshift_to_s3 = RedshiftToS3Operator(\n        task_id=\"transfer_redshift_to_s3\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n    )\n    # [END howto_transfer_redshift_to_s3]\n\n    check_if_key_exists = S3KeySensor(\n        task_id=\"check_if_key_exists\",\n        bucket_name=bucket_name,\n        bucket_key=f\"{S3_KEY}/{REDSHIFT_TABLE}_0000_part_00\",\n    )\n\n    # [START howto_transfer_s3_to_redshift]\n    transfer_s3_to_redshift = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift]\n\n    # [START howto_transfer_s3_to_redshift_multiple_keys]\n    transfer_s3_to_redshift_multiple = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift_multiple\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_PREFIX,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift_multiple_keys]\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=SQL_DROP_TABLE,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_bucket,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_object,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        transfer_redshift_to_s3,\n        check_if_key_exists,\n        transfer_s3_to_redshift,\n        transfer_s3_to_redshift_multiple,\n        # TEST TEARDOWN\n        drop_table,\n        delete_cluster,\n        delete_sg,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_redshift_s3_transfers.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service interactions, includes custom tasks for security group and connection management, requires external service coordination", "key_concepts": ["AWS Service Integration", "Redshift Data Transfer", "S3 Bucket Management", "Dynamic Connection Creation", "Security Group Configuration"], "learning_objectives": ["Understand cloud service orchestration in Airflow", "Learn dynamic task creation with @task decorator", "Practice creating database and storage pipeline workflows"]}}
{"instruction": "Design an Airflow workflow that demonstrates secure cloud infrastructure provisioning and data migration, focusing on infrastructure-as-code principles and AWS resource management.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "RedshiftToS3Operator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3KeySensor", "S3ToRedshiftOperator", "SQLExecuteQueryOperator"], "line_count": 266, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.amazon.aws.transfers.redshift_to_s3 import RedshiftToS3Operator\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_redshift_to_s3\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nS3_KEY = \"s3_output_\"\nS3_KEY_2 = \"s3_key_2\"\nS3_KEY_PREFIX = \"s3_k\"\nREDSHIFT_TABLE = \"test_table\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\nSQL_DROP_TABLE = f\"DROP TABLE IF EXISTS {REDSHIFT_TABLE};\"\n\nDATA = \"0, 'Airflow', 'testing'\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-bucket\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"s3_create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        data=DATA,\n        replace=True,\n    )\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_redshift_to_s3]\n    transfer_redshift_to_s3 = RedshiftToS3Operator(\n        task_id=\"transfer_redshift_to_s3\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n    )\n    # [END howto_transfer_redshift_to_s3]\n\n    check_if_key_exists = S3KeySensor(\n        task_id=\"check_if_key_exists\",\n        bucket_name=bucket_name,\n        bucket_key=f\"{S3_KEY}/{REDSHIFT_TABLE}_0000_part_00\",\n    )\n\n    # [START howto_transfer_s3_to_redshift]\n    transfer_s3_to_redshift = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_2,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift]\n\n    # [START howto_transfer_s3_to_redshift_multiple_keys]\n    transfer_s3_to_redshift_multiple = S3ToRedshiftOperator(\n        task_id=\"transfer_s3_to_redshift_multiple\",\n        redshift_conn_id=conn_id_name,\n        s3_bucket=bucket_name,\n        s3_key=S3_KEY_PREFIX,\n        schema=\"PUBLIC\",\n        table=REDSHIFT_TABLE,\n        copy_options=[\"csv\"],\n    )\n    # [END howto_transfer_s3_to_redshift_multiple_keys]\n\n    drop_table = SQLExecuteQueryOperator(\n        task_id=\"drop_table\",\n        conn_id=conn_id_name,\n        sql=SQL_DROP_TABLE,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_bucket,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_object,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        transfer_redshift_to_s3,\n        check_if_key_exists,\n        transfer_s3_to_redshift,\n        transfer_s3_to_redshift_multiple,\n        # TEST TEARDOWN\n        drop_table,\n        delete_cluster,\n        delete_sg,\n        delete_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_redshift_s3_transfers.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves complex AWS resource management, security group creation, cluster provisioning, requires understanding of infrastructure deployment tasks", "key_concepts": ["Infrastructure as Code", "AWS Resource Lifecycle Management", "Security Group Configuration", "Dynamic Environment Provisioning", "Idempotent Task Design"], "learning_objectives": ["Learn dynamic infrastructure provisioning", "Understand secure cloud resource management", "Practice creating self-cleaning cloud workflows"]}}
{"instruction": "Build an Airflow DAG that demonstrates transferring files from an S3 bucket to an SFTP server using AWS cloud operators, focusing on understanding cloud data transfer workflows.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "S3ToSFTPOperator"], "line_count": 66, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sftp import S3ToSFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-sftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-sftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_sftp]\n    create_s3_to_sftp_job = S3ToSFTPOperator(\n        task_id=\"create_s3_to_sftp_job\",\n        sftp_path=\"sftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_sftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        create_s3_to_sftp_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_sftp.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS cloud operators, involves multiple service interactions, demonstrates cloud data transfer with setup and teardown tasks", "key_concepts": ["Cloud Service Integration", "S3 Bucket Management", "SFTP File Transfer", "Operator Chaining", "AWS Provider Operators"], "learning_objectives": ["Understand AWS cloud operator usage", "Learn file transfer between cloud services", "Practice DAG configuration with multiple tasks"]}}
{"instruction": "Create an Airflow workflow that showcases infrastructure management and data movement, emphasizing cloud resource lifecycle and transfer mechanisms.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "S3ToSFTPOperator"], "line_count": 66, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_sftp import S3ToSFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-sftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-sftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_sftp]\n    create_s3_to_sftp_job = S3ToSFTPOperator(\n        task_id=\"create_s3_to_sftp_job\",\n        sftp_path=\"sftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_sftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        create_s3_to_sftp_job,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_sftp.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves bucket creation, file transfer, and bucket deletion, demonstrating complete resource management workflow", "key_concepts": ["Infrastructure as Code", "Cloud Resource Lifecycle", "Trigger Rules", "Task Dependency Management", "Automated Cleanup"], "learning_objectives": ["Understand cloud resource provisioning", "Learn task chaining and dependency management", "Practice implementing cleanup mechanisms"]}}
{"instruction": "Build an Airflow DAG that demonstrates S3 bucket management and file transfer to an FTP server, showcasing cloud service integration and task chaining.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "S3ToFTPOperator"], "line_count": 66, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_ftp import S3ToFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_ftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-ftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-ftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_ftp]\n    s3_to_ftp_task = S3ToFTPOperator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_ftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        s3_to_ftp_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_ftp.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses AWS S3 operators, involves multiple cloud service interactions, demonstrates task chaining with multiple steps, and includes system test context", "key_concepts": ["AWS S3 Operators", "Task Chaining", "Cloud Service Integration", "Trigger Rules", "System Test Context"], "learning_objectives": ["Understand S3 bucket creation and deletion", "Learn file transfer between cloud storage and FTP", "Practice task dependency management", "Explore AWS provider operators"]}}
{"instruction": "Design an Airflow workflow that demonstrates robust cloud data migration with explicit error handling, focusing on safe and controlled file transfer processes.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "S3ToFTPOperator"], "line_count": 66, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import models\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.s3_to_ftp import S3ToFTPOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_ftp\"\n\nwith models.DAG(\n    DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-s3-to-ftp-bucket\"\n    s3_key = f\"{env_id}-s3-to-ftp-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_s3_to_ftp]\n    s3_to_ftp_task = S3ToFTPOperator(\n        task_id=\"ftp_to_s3_task\",\n        ftp_path=\"/tmp/ftp_path\",\n        s3_bucket=s3_bucket,\n        s3_key=s3_key,\n    )\n    # [END howto_transfer_s3_to_ftp]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        s3_to_ftp_task,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_ftp.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operators with different trigger rules, external service interactions, and system test integration showing advanced workflow design", "key_concepts": ["Error Handling", "Trigger Rules", "Cloud Migration", "Operator Chaining", "System Test Practices"], "learning_objectives": ["Implement safe cloud data transfer workflows", "Understand trigger rule configurations", "Learn defensive programming in Airflow", "Practice multi-step cloud service orchestration"]}}
{"instruction": "Build an AWS RDS snapshot management workflow that demonstrates creating a database instance, taking a snapshot, copying the snapshot, and then cleaning up resources using Airflow RDS operators.", "input": {"airflow_version": "2.7.2", "operators": ["RdsCopyDbSnapshotOperator", "RdsCreateDbInstanceOperator", "RdsCreateDbSnapshotOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteDbSnapshotOperator", "RdsSnapshotExistenceSensor"], "line_count": 134, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCopyDbSnapshotOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_snapshot\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_snapshot_name = f\"{test_context[ENV_ID_KEY]}-snapshot\"\n    rds_snapshot_copy_name = f\"{rds_snapshot_name}-copy\"\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_db_snapshot]\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_create_db_snapshot]\n\n    # [START howto_sensor_rds_snapshot_existence]\n    snapshot_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n    # [END howto_sensor_rds_snapshot_existence]\n\n    # [START howto_operator_rds_copy_snapshot]\n    copy_snapshot = RdsCopyDbSnapshotOperator(\n        task_id=\"copy_snapshot\",\n        db_type=\"instance\",\n        source_db_snapshot_identifier=rds_snapshot_name,\n        target_db_snapshot_identifier=rds_snapshot_copy_name,\n    )\n    # [END howto_operator_rds_copy_snapshot]\n\n    # [START howto_operator_rds_delete_snapshot]\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_delete_snapshot]\n\n    snapshot_copy_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_copy_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        target_statuses=[\"available\"],\n    )\n\n    delete_snapshot_copy = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot_copy\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_db_instance,\n        # TEST BODY\n        create_snapshot,\n        snapshot_sensor,\n        copy_snapshot,\n        delete_snapshot,\n        # TEST TEARDOWN\n        snapshot_copy_sensor,\n        delete_snapshot_copy,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_snapshot.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS RDS operators, involves cloud service integration, includes sensors for tracking snapshot status, and demonstrates sequential task chaining", "key_concepts": ["AWS RDS Integration", "Airflow Operators", "Cloud Service Orchestration", "Task Chaining", "RDS Snapshot Management"], "learning_objectives": ["Understand RDS snapshot lifecycle management", "Learn how to use Airflow cloud service operators", "Practice task dependency and chaining in Airflow"]}}
{"instruction": "Design an infrastructure provisioning and cleanup DAG that showcases safe database instance management using Airflow sensors and trigger rules to ensure reliable resource handling.", "input": {"airflow_version": "2.7.2", "operators": ["RdsCopyDbSnapshotOperator", "RdsCreateDbInstanceOperator", "RdsCreateDbSnapshotOperator", "RdsDeleteDbInstanceOperator", "RdsDeleteDbSnapshotOperator", "RdsSnapshotExistenceSensor"], "line_count": 134, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.rds import (\n    RdsCopyDbSnapshotOperator,\n    RdsCreateDbInstanceOperator,\n    RdsCreateDbSnapshotOperator,\n    RdsDeleteDbInstanceOperator,\n    RdsDeleteDbSnapshotOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.rds import RdsSnapshotExistenceSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_rds_snapshot\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    rds_db_name = f\"{test_context[ENV_ID_KEY]}_db\"\n    rds_instance_name = f\"{test_context[ENV_ID_KEY]}-instance\"\n    rds_snapshot_name = f\"{test_context[ENV_ID_KEY]}-snapshot\"\n    rds_snapshot_copy_name = f\"{rds_snapshot_name}-copy\"\n\n    create_db_instance = RdsCreateDbInstanceOperator(\n        task_id=\"create_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        db_instance_class=\"db.t4g.micro\",\n        engine=\"postgres\",\n        rds_kwargs={\n            \"MasterUsername\": \"rds_username\",\n            # NEVER store your production password in plaintext in a DAG like this.\n            # Use Airflow Secrets or a secret manager for this in production.\n            \"MasterUserPassword\": \"rds_password\",\n            \"AllocatedStorage\": 20,\n            \"DBName\": rds_db_name,\n            \"PubliclyAccessible\": False,\n        },\n    )\n\n    # [START howto_operator_rds_create_db_snapshot]\n    create_snapshot = RdsCreateDbSnapshotOperator(\n        task_id=\"create_snapshot\",\n        db_type=\"instance\",\n        db_identifier=rds_instance_name,\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_create_db_snapshot]\n\n    # [START howto_sensor_rds_snapshot_existence]\n    snapshot_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n        target_statuses=[\"available\"],\n    )\n    # [END howto_sensor_rds_snapshot_existence]\n\n    # [START howto_operator_rds_copy_snapshot]\n    copy_snapshot = RdsCopyDbSnapshotOperator(\n        task_id=\"copy_snapshot\",\n        db_type=\"instance\",\n        source_db_snapshot_identifier=rds_snapshot_name,\n        target_db_snapshot_identifier=rds_snapshot_copy_name,\n    )\n    # [END howto_operator_rds_copy_snapshot]\n\n    # [START howto_operator_rds_delete_snapshot]\n    delete_snapshot = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_name,\n    )\n    # [END howto_operator_rds_delete_snapshot]\n\n    snapshot_copy_sensor = RdsSnapshotExistenceSensor(\n        task_id=\"snapshot_copy_sensor\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        target_statuses=[\"available\"],\n    )\n\n    delete_snapshot_copy = RdsDeleteDbSnapshotOperator(\n        task_id=\"delete_snapshot_copy\",\n        db_type=\"instance\",\n        db_snapshot_identifier=rds_snapshot_copy_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_db_instance = RdsDeleteDbInstanceOperator(\n        task_id=\"delete_db_instance\",\n        db_instance_identifier=rds_instance_name,\n        rds_kwargs={\"SkipFinalSnapshot\": True},\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_db_instance,\n        # TEST BODY\n        create_snapshot,\n        snapshot_sensor,\n        copy_snapshot,\n        delete_snapshot,\n        # TEST TEARDOWN\n        snapshot_copy_sensor,\n        delete_snapshot_copy,\n        delete_db_instance,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_rds_snapshot.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Demonstrates advanced task dependency management, uses sensors for status checking, employs trigger rules for conditional execution", "key_concepts": ["Infrastructure as Code", "Resource Lifecycle Management", "Airflow Sensors", "Trigger Rules", "Error-Resilient Workflows"], "learning_objectives": ["Implement robust cloud resource management", "Understand Airflow sensor and trigger rule patterns", "Learn safe infrastructure provisioning techniques"]}}
{"instruction": "Build a data transfer DAG that extracts Salesforce account data and uploads it to an S3 bucket, demonstrating cloud service integration and data migration techniques.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "SalesforceToS3Operator"], "line_count": 72, "is_multifile": true}, "output": "\"\"\"\nThis is a basic example DAG for using `SalesforceToS3Operator` to retrieve Salesforce account\ndata and upload it to an Amazon S3 bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.salesforce_to_s3 import SalesforceToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_salesforce_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 7, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-salesforce-to-s3-bucket\"\n    s3_key = f\"{env_id}-salesforce-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_salesforce_to_s3]\n    upload_salesforce_data_to_s3 = SalesforceToS3Operator(\n        task_id=\"upload_salesforce_to_s3\",\n        salesforce_query=\"SELECT AccountNumber, Name FROM Account\",\n        s3_bucket_name=s3_bucket,\n        s3_key=s3_key,\n        salesforce_conn_id=\"salesforce\",\n        replace=True,\n    )\n    # [END howto_transfer_salesforce_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        upload_salesforce_data_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_salesforce_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS and Salesforce operators, involves cloud service integration, and requires connection configuration with external services", "key_concepts": ["Cloud Data Transfer", "Salesforce Integration", "S3 Bucket Management", "Operator Chaining", "Connection Configuration"], "learning_objectives": ["Configure Salesforce data extraction", "Understand S3 bucket creation and deletion", "Learn inter-service data migration techniques", "Practice Airflow operator chaining"]}}
{"instruction": "Design an automated data pipeline that demonstrates safe cloud resource management, including bucket creation, data transfer, and cleanup using trigger rules and systematic task sequencing.", "input": {"airflow_version": "2.7.2", "operators": ["S3CreateBucketOperator", "S3DeleteBucketOperator", "SalesforceToS3Operator"], "line_count": 72, "is_multifile": true}, "output": "\"\"\"\nThis is a basic example DAG for using `SalesforceToS3Operator` to retrieve Salesforce account\ndata and upload it to an Amazon S3 bucket.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.transfers.salesforce_to_s3 import SalesforceToS3Operator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import SystemTestContextBuilder\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_salesforce_to_s3\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 7, 8),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[\"ENV_ID\"]\n\n    s3_bucket = f\"{env_id}-salesforce-to-s3-bucket\"\n    s3_key = f\"{env_id}-salesforce-to-s3-key\"\n\n    create_s3_bucket = S3CreateBucketOperator(task_id=\"create_s3_bucket\", bucket_name=s3_bucket)\n\n    # [START howto_transfer_salesforce_to_s3]\n    upload_salesforce_data_to_s3 = SalesforceToS3Operator(\n        task_id=\"upload_salesforce_to_s3\",\n        salesforce_query=\"SELECT AccountNumber, Name FROM Account\",\n        s3_bucket_name=s3_bucket,\n        s3_key=s3_key,\n        salesforce_conn_id=\"salesforce\",\n        replace=True,\n    )\n    # [END howto_transfer_salesforce_to_s3]\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_s3_bucket\",\n        bucket_name=s3_bucket,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        create_s3_bucket,\n        # TEST BODY\n        upload_salesforce_data_to_s3,\n        # TEST TEARDOWN\n        delete_s3_bucket,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_salesforce_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves multiple operators with specific configuration, uses advanced trigger rules, and demonstrates systematic cloud resource management", "key_concepts": ["Cloud Resource Lifecycle", "Trigger Rules", "Task Dependency Management", "Error Handling", "Automated Cleanup"], "learning_objectives": ["Implement safe cloud resource provisioning", "Understand task dependency and trigger rules", "Practice systematic task sequencing", "Learn cloud service interaction patterns"]}}
{"instruction": "Build an Airflow DAG that demonstrates end-to-end data transfer from a Redshift database to an S3 bucket, showcasing cloud service integration and dynamic connection management.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator", "SQLExecuteQueryOperator", "SqlToS3Operator"], "line_count": 218, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.sql_to_s3 import SqlToS3Operator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_sql_to_s3\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nREDSHIFT_TABLE = \"test_table\"\nSQL_QUERY = f\"SELECT * FROM {REDSHIFT_TABLE}\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-s3-bucket\"\n    key = f\"{env_id}-key\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_sql_to_s3]\n    sql_to_s3_task = SqlToS3Operator(\n        task_id=\"sql_to_s3_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n    )\n    # [END howto_transfer_sql_to_s3]\n\n    # [START howto_transfer_sql_to_s3_with_groupby_param]\n    sql_to_s3_task_with_groupby = SqlToS3Operator(\n        task_id=\"sql_to_s3_with_groupby_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n        groupby_kwargs={\"by\": \"color\"},\n    )\n    # [END howto_transfer_sql_to_s3_with_groupby_param]\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        sql_to_s3_task,\n        sql_to_s3_task_with_groupby,\n        # TEST TEARDOWN\n        delete_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sql_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves dynamic security group creation, Redshift cluster management, and requires custom connection setup tasks", "key_concepts": ["AWS Service Integration", "Dynamic Connection Management", "Cloud Infrastructure Orchestration", "SQL to S3 Data Transfer"], "learning_objectives": ["Create Redshift cluster programmatically", "Establish secure network configurations", "Transfer data between database and cloud storage", "Manage Airflow connections dynamically"]}}
{"instruction": "Design an Airflow workflow that demonstrates secure cloud resource provisioning and teardown, focusing on infrastructure-as-code principles using AWS services and Airflow operators.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "S3CreateBucketOperator", "S3DeleteBucketOperator", "SQLExecuteQueryOperator", "SqlToS3Operator"], "line_count": 218, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3DeleteBucketOperator\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.sql_to_s3 import SqlToS3Operator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\n\nDAG_ID = \"example_sql_to_s3\"\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nREDSHIFT_TABLE = \"test_table\"\nSQL_QUERY = f\"SELECT * FROM {REDSHIFT_TABLE}\"\n\nSQL_CREATE_TABLE = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {REDSHIFT_TABLE} (\n    fruit_id INTEGER,\n    name VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n    );\n\"\"\"\n\nSQL_INSERT_DATA = f\"INSERT INTO {REDSHIFT_TABLE} VALUES ( 1, 'Banana', 'Yellow');\"\n\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    redshift_hook = RedshiftHook()\n    cluster_endpoint = redshift_hook.get_conn().describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]],\n        GroupNames=[sec_group_name],\n        WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 4},\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2021, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    conn_id_name = f\"{env_id}-conn-id\"\n    sg_name = f\"{env_id}-sg\"\n    bucket_name = f\"{env_id}-s3-bucket\"\n    key = f\"{env_id}-key\"\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=15,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_table_redshift_data = SQLExecuteQueryOperator(\n        task_id=\"create_table_redshift_data\",\n        conn_id=conn_id_name,\n        sql=SQL_CREATE_TABLE,\n    )\n\n    insert_data = SQLExecuteQueryOperator(\n        task_id=\"insert_data\",\n        conn_id=conn_id_name,\n        sql=SQL_INSERT_DATA,\n    )\n\n    # [START howto_transfer_sql_to_s3]\n    sql_to_s3_task = SqlToS3Operator(\n        task_id=\"sql_to_s3_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n    )\n    # [END howto_transfer_sql_to_s3]\n\n    # [START howto_transfer_sql_to_s3_with_groupby_param]\n    sql_to_s3_task_with_groupby = SqlToS3Operator(\n        task_id=\"sql_to_s3_with_groupby_task\",\n        sql_conn_id=conn_id_name,\n        query=SQL_QUERY,\n        s3_bucket=bucket_name,\n        s3_key=key,\n        replace=True,\n        groupby_kwargs={\"by\": \"color\"},\n    )\n    # [END howto_transfer_sql_to_s3_with_groupby_param]\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n    chain(\n        # TEST SETUP\n        test_context,\n        create_bucket,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_table_redshift_data,\n        insert_data,\n        # TEST BODY\n        sql_to_s3_task,\n        sql_to_s3_task_with_groupby,\n        # TEST TEARDOWN\n        delete_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sql_to_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Involves creating and deleting cloud resources, managing security groups, and implementing infrastructure lifecycle management with custom tasks", "key_concepts": ["Infrastructure as Code", "Cloud Resource Management", "Security Group Configuration", "Idempotent Task Design"], "learning_objectives": ["Programmatically create cloud resources", "Implement secure network configurations", "Design self-cleaning infrastructure workflows", "Use Airflow for cloud resource orchestration"]}}
{"instruction": "Design an AWS Redshift data pipeline that transfers cocktail data from S3 to a Redshift database, demonstrating cloud service orchestration and data migration techniques.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3DeleteObjectsOperator", "S3ToSqlOperator", "SQLExecuteQueryOperator", "SQLTableCheckOperator"], "line_count": 269, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteObjectsOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator, SQLTableCheckOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\nfrom tests.system.utils.watcher import watcher\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sql\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nSQL_TABLE_NAME = \"cocktails\"\nSQL_COLUMN_LIST = [\"cocktail_id\", \"cocktail_name\", \"base_spirit\"]\nSAMPLE_DATA = r\"\"\"1,Caipirinha,Cachaca\n2,Bramble,Gin\n3,Daiquiri,Rum\n\"\"\"\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    cluster_endpoint = RedshiftHook().conn.describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]], GroupNames=[sec_group_name]\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2023, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    conn_id_name = f\"{env_id}-conn-id\"\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    sg_name = f\"{env_id}-sg\"\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/cocktail_list.csv\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    create_table = SQLExecuteQueryOperator(\n        task_id=\"create_sample_table\",\n        conn_id=conn_id_name,\n        sql=f\"\"\"\n            CREATE TABLE IF NOT EXISTS {SQL_TABLE_NAME} (\n            cocktail_id INT NOT NULL,\n            cocktail_name VARCHAR NOT NULL,\n            base_spirit VARCHAR NOT NULL);\n          \"\"\",\n    )\n\n    # [START howto_transfer_s3_to_sql]\n    #\n    # This operator requires a parser method. The Parser should take a filename as input\n    # and return an iterable of rows.\n    # This example parser uses the builtin csv library and returns a list of rows\n    #\n    def parse_csv_to_list(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            return list(csv.reader(file))\n\n    transfer_s3_to_sql = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_list,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql]\n\n    # [START howto_transfer_s3_to_sql_generator]\n    #\n    # As the parser can return any kind of iterator, a generator is also allowed.\n    # This example parser returns a generator which prevents python from loading\n    # the whole file into memory.\n    #\n\n    def parse_csv_to_generator(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            yield from csv.reader(file)\n\n    transfer_s3_to_sql_generator = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql_paser_to_generator\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_generator,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql_generator]\n\n    check_table = SQLTableCheckOperator(\n        task_id=\"check_table\",\n        conn_id=conn_id_name,\n        table=SQL_TABLE_NAME,\n        checks={\n            \"row_count_check\": {\"check_statement\": \"COUNT(*) = 6\"},\n        },\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        conn_id=conn_id_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"drop_table\",\n        sql=f\"DROP TABLE {SQL_TABLE_NAME}\",\n    )\n\n    delete_s3_objects = S3DeleteObjectsOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_objects\",\n        bucket=s3_bucket_name,\n        keys=s3_key,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_bucket,\n        create_object,\n        create_table,\n        # TEST BODY\n        transfer_s3_to_sql,\n        transfer_s3_to_sql_generator,\n        check_table,\n        # TEST TEARDOWN\n        drop_table,\n        delete_s3_objects,\n        delete_s3_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_sql.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves cloud service integration, includes security group setup, and requires dynamic connection management", "key_concepts": ["AWS Integration", "S3 Data Transfer", "Redshift Cluster Management", "Dynamic Connection Creation", "Security Group Configuration"], "learning_objectives": ["Understand cloud service orchestration", "Learn dynamic task creation with @task decorator", "Practice AWS service interactions in Airflow", "Implement secure data migration workflows"]}}
{"instruction": "Create a comprehensive cloud infrastructure provisioning and data loading workflow that demonstrates secure, automated resource management using Airflow's AWS providers.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "RedshiftClusterSensor", "RedshiftCreateClusterOperator", "RedshiftDeleteClusterOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3DeleteObjectsOperator", "S3ToSqlOperator", "SQLExecuteQueryOperator", "SQLTableCheckOperator"], "line_count": 269, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nimport boto3\n\nfrom airflow import DAG, settings\nfrom airflow.decorators import task\nfrom airflow.models import Connection\nfrom airflow.models.baseoperator import chain\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.redshift_cluster import (\n    RedshiftCreateClusterOperator,\n    RedshiftDeleteClusterOperator,\n)\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteObjectsOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor\nfrom airflow.providers.amazon.aws.transfers.s3_to_sql import S3ToSqlOperator\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator, SQLTableCheckOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\nfrom tests.system.providers.amazon.aws.utils.ec2 import get_default_vpc_id\nfrom tests.system.utils.watcher import watcher\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDAG_ID = \"example_s3_to_sql\"\n\nDB_LOGIN = \"adminuser\"\nDB_PASS = \"MyAmazonPassword1\"\nDB_NAME = \"dev\"\n\nIP_PERMISSION = {\n    \"FromPort\": -1,\n    \"IpProtocol\": \"All\",\n    \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\", \"Description\": \"Test description\"}],\n}\n\nSQL_TABLE_NAME = \"cocktails\"\nSQL_COLUMN_LIST = [\"cocktail_id\", \"cocktail_name\", \"base_spirit\"]\nSAMPLE_DATA = r\"\"\"1,Caipirinha,Cachaca\n2,Bramble,Gin\n3,Daiquiri,Rum\n\"\"\"\n\n\n@task\ndef create_connection(conn_id_name: str, cluster_id: str):\n    cluster_endpoint = RedshiftHook().conn.describe_clusters(ClusterIdentifier=cluster_id)[\"Clusters\"][0]\n    conn = Connection(\n        conn_id=conn_id_name,\n        conn_type=\"redshift\",\n        host=cluster_endpoint[\"Endpoint\"][\"Address\"],\n        login=DB_LOGIN,\n        password=DB_PASS,\n        port=cluster_endpoint[\"Endpoint\"][\"Port\"],\n        schema=cluster_endpoint[\"DBName\"],\n    )\n    session = settings.Session()\n    session.add(conn)\n    session.commit()\n\n\n@task\ndef setup_security_group(sec_group_name: str, ip_permissions: list[dict], vpc_id: str):\n    client = boto3.client(\"ec2\")\n    security_group = client.create_security_group(\n        Description=\"Redshift-system-test\", GroupName=sec_group_name, VpcId=vpc_id\n    )\n    client.get_waiter(\"security_group_exists\").wait(\n        GroupIds=[security_group[\"GroupId\"]], GroupNames=[sec_group_name]\n    )\n    client.authorize_security_group_ingress(\n        GroupId=security_group[\"GroupId\"], GroupName=sec_group_name, IpPermissions=ip_permissions\n    )\n    return security_group[\"GroupId\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_security_group(sec_group_id: str, sec_group_name: str):\n    boto3.client(\"ec2\").delete_security_group(GroupId=sec_group_id, GroupName=sec_group_name)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    start_date=datetime(2023, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n    conn_id_name = f\"{env_id}-conn-id\"\n    redshift_cluster_identifier = f\"{env_id}-redshift-cluster\"\n    sg_name = f\"{env_id}-sg\"\n    s3_bucket_name = f\"{env_id}-bucket\"\n    s3_key = f\"{env_id}/files/cocktail_list.csv\"\n\n    get_vpc_id = get_default_vpc_id()\n\n    set_up_sg = setup_security_group(sg_name, [IP_PERMISSION], get_vpc_id)\n\n    create_cluster = RedshiftCreateClusterOperator(\n        task_id=\"create_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        vpc_security_group_ids=[set_up_sg],\n        publicly_accessible=True,\n        cluster_type=\"single-node\",\n        node_type=\"dc2.large\",\n        master_username=DB_LOGIN,\n        master_user_password=DB_PASS,\n    )\n\n    wait_cluster_available = RedshiftClusterSensor(\n        task_id=\"wait_cluster_available\",\n        cluster_identifier=redshift_cluster_identifier,\n        target_status=\"available\",\n        poke_interval=5,\n        timeout=60 * 30,\n    )\n\n    set_up_connection = create_connection(conn_id_name, cluster_id=redshift_cluster_identifier)\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=s3_bucket_name,\n    )\n\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        data=SAMPLE_DATA,\n        replace=True,\n    )\n\n    create_table = SQLExecuteQueryOperator(\n        task_id=\"create_sample_table\",\n        conn_id=conn_id_name,\n        sql=f\"\"\"\n            CREATE TABLE IF NOT EXISTS {SQL_TABLE_NAME} (\n            cocktail_id INT NOT NULL,\n            cocktail_name VARCHAR NOT NULL,\n            base_spirit VARCHAR NOT NULL);\n          \"\"\",\n    )\n\n    # [START howto_transfer_s3_to_sql]\n    #\n    # This operator requires a parser method. The Parser should take a filename as input\n    # and return an iterable of rows.\n    # This example parser uses the builtin csv library and returns a list of rows\n    #\n    def parse_csv_to_list(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            return list(csv.reader(file))\n\n    transfer_s3_to_sql = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_list,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql]\n\n    # [START howto_transfer_s3_to_sql_generator]\n    #\n    # As the parser can return any kind of iterator, a generator is also allowed.\n    # This example parser returns a generator which prevents python from loading\n    # the whole file into memory.\n    #\n\n    def parse_csv_to_generator(filepath):\n        import csv\n\n        with open(filepath, newline=\"\") as file:\n            yield from csv.reader(file)\n\n    transfer_s3_to_sql_generator = S3ToSqlOperator(\n        task_id=\"transfer_s3_to_sql_paser_to_generator\",\n        s3_bucket=s3_bucket_name,\n        s3_key=s3_key,\n        table=SQL_TABLE_NAME,\n        column_list=SQL_COLUMN_LIST,\n        parser=parse_csv_to_generator,\n        sql_conn_id=conn_id_name,\n    )\n    # [END howto_transfer_s3_to_sql_generator]\n\n    check_table = SQLTableCheckOperator(\n        task_id=\"check_table\",\n        conn_id=conn_id_name,\n        table=SQL_TABLE_NAME,\n        checks={\n            \"row_count_check\": {\"check_statement\": \"COUNT(*) = 6\"},\n        },\n    )\n\n    drop_table = SQLExecuteQueryOperator(\n        conn_id=conn_id_name,\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"drop_table\",\n        sql=f\"DROP TABLE {SQL_TABLE_NAME}\",\n    )\n\n    delete_s3_objects = S3DeleteObjectsOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_objects\",\n        bucket=s3_bucket_name,\n        keys=s3_key,\n    )\n\n    delete_s3_bucket = S3DeleteBucketOperator(\n        trigger_rule=TriggerRule.ALL_DONE,\n        task_id=\"delete_bucket\",\n        bucket_name=s3_bucket_name,\n        force_delete=True,\n    )\n\n    delete_cluster = RedshiftDeleteClusterOperator(\n        task_id=\"delete_cluster\",\n        cluster_identifier=redshift_cluster_identifier,\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    delete_sg = delete_security_group(\n        sec_group_id=set_up_sg,\n        sec_group_name=sg_name,\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        set_up_sg,\n        create_cluster,\n        wait_cluster_available,\n        set_up_connection,\n        create_bucket,\n        create_object,\n        create_table,\n        # TEST BODY\n        transfer_s3_to_sql,\n        transfer_s3_to_sql_generator,\n        check_table,\n        # TEST TEARDOWN\n        drop_table,\n        delete_s3_objects,\n        delete_s3_bucket,\n        delete_cluster,\n        delete_sg,\n    )\n\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3_to_sql.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Advanced multi-step workflow involving security group creation, cluster management, dynamic connections, and data transfer with complex task dependencies", "key_concepts": ["Infrastructure as Code", "Dynamic Resource Provisioning", "Cloud Security Configuration", "Automated Database Setup", "Task Dependency Management"], "learning_objectives": ["Master complex Airflow workflow design", "Understand cloud resource lifecycle management", "Learn secure infrastructure deployment patterns", "Implement dynamic task generation strategies"]}}
{"instruction": "Build an end-to-end machine learning model deployment workflow using AWS SageMaker, demonstrating how to train a K-Nearest Neighbors model, create an endpoint, and manage the entire lifecycle of model deployment through Airflow tasks.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "SageMakerDeleteModelOperator", "SageMakerEndpointConfigOperator", "SageMakerEndpointOperator", "SageMakerEndpointSensor", "SageMakerModelOperator", "SageMakerTrainingOperator"], "line_count": 241, "is_multifile": false}, "output": "TRAIN_DATA = \"0,4.9,2.5,4.5,1.7\\n1,7.0,3.2,4.7,1.4\\n0,7.3,2.9,6.3,1.8\\n2,5.1,3.5,1.4,0.2\\n\"\nSAMPLE_TEST_DATA = \"6.4,3.2,4.5,1.5\"\n\n\n@task\ndef call_endpoint(endpoint_name):\n    response = (\n        boto3.Session()\n        .client(\"sagemaker-runtime\")\n        .invoke_endpoint(\n            EndpointName=endpoint_name,\n            ContentType=\"text/csv\",\n            Body=SAMPLE_TEST_DATA,\n        )\n    )\n\n    return json.loads(response[\"Body\"].read().decode())[\"predictions\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint_config(endpoint_config_job_name):\n    boto3.client(\"sagemaker\").delete_endpoint_config(EndpointConfigName=endpoint_config_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint(endpoint_name):\n    boto3.client(\"sagemaker\").delete_endpoint(EndpointName=endpoint_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef archive_logs(log_group_name):\n    boto3.client(\"logs\").put_retention_policy(logGroupName=log_group_name, retentionInDays=1)\n\n\n@task\ndef set_up(env_id, role_arn, ti=None):\n    bucket_name = f\"{env_id}-sagemaker\"\n    input_data_s3_key = f\"{env_id}/input-data\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    endpoint_config_job_name = f\"{env_id}-endpoint-config\"\n    endpoint_name = f\"{env_id}-endpoint\"\n    model_name = f\"{env_id}-KNN-model\"\n    training_job_name = f\"{env_id}-train\"\n\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    training_config = {\n        \"TrainingJobName\": training_job_name,\n        \"RoleArn\": role_arn,\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"4\",\n            \"k\": \"3\",\n            \"sample_size\": str(TRAIN_DATA.count(\"\\n\") - 1),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                \"CompressionType\": \"None\",\n                \"ContentType\": \"text/csv\",\n                \"DataSource\": {\n                    \"S3DataSource\": {\n                        \"S3DataDistributionType\": \"FullyReplicated\",\n                        \"S3DataType\": \"S3Prefix\",\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_s3_key}/train.csv\",\n                    }\n                },\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ResourceConfig\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n            \"VolumeSizeInGB\": 1,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 6 * 60},\n    }\n\n    model_config = {\n        \"ModelName\": model_name,\n        \"ExecutionRoleArn\": role_arn,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\",  # noqa: E501\n        },\n    }\n\n    endpoint_config_config = {\n        \"EndpointConfigName\": endpoint_config_job_name,\n        \"ProductionVariants\": [\n            {\n                \"VariantName\": f\"{env_id}-demo\",\n                \"ModelName\": model_name,\n                \"InstanceType\": \"ml.t2.medium\",\n                \"InitialInstanceCount\": 1,\n            },\n        ],\n    }\n\n    deploy_endpoint_config = {\n        \"EndpointName\": endpoint_name,\n        \"EndpointConfigName\": endpoint_config_job_name,\n    }\n\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"input_data_s3_key\", value=input_data_s3_key)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"endpoint_name\", value=endpoint_name)\n    ti.xcom_push(key=\"endpoint_config_job_name\", value=endpoint_config_job_name)\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"endpoint_config_config\", value=endpoint_config_config)\n    ti.xcom_push(key=\"deploy_endpoint_config\", value=deploy_endpoint_config)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    test_setup = set_up(\n        env_id=test_context[ENV_ID_KEY],\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_data = S3CreateObjectOperator(\n        task_id=\"upload_data\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=f'{test_setup[\"input_data_s3_key\"]}/train.csv',\n        data=TRAIN_DATA,\n    )\n\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n\n    # [START howto_operator_sagemaker_endpoint_config]\n    configure_endpoint = SageMakerEndpointConfigOperator(\n        task_id=\"configure_endpoint\",\n        config=test_setup[\"endpoint_config_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint_config]\n\n    # [START howto_operator_sagemaker_endpoint]\n    deploy_endpoint = SageMakerEndpointOperator(\n        task_id=\"deploy_endpoint\",\n        config=test_setup[\"deploy_endpoint_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint]\n\n    # SageMakerEndpointOperator waits by default, setting as False to test the Sensor below.\n    deploy_endpoint.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_endpoint]\n    await_endpoint = SageMakerEndpointSensor(\n        task_id=\"await_endpoint\",\n        endpoint_name=test_setup[\"endpoint_name\"],\n    )\n    # [END howto_sensor_sagemaker_endpoint]\n\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/TrainingJobs\", test_context[ENV_ID_KEY]),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_data,\n        # TEST BODY\n        train_model,\n        create_model,\n        configure_endpoint,\n        deploy_endpoint,\n        await_endpoint,\n        call_endpoint(test_setup[\"endpoint_name\"]),\n        # TEST TEARDOWN\n        delete_endpoint_config(test_setup[\"endpoint_config_job_name\"]),\n        delete_endpoint(test_setup[\"endpoint_name\"]),\n        delete_model,\n        delete_bucket,\n        log_cleanup,\n        archive_logs(f\"/aws/sagemaker/Endpoints/{test_setup['endpoint_name']}\"),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sagemaker_endpoint.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS services, involves custom task functions, integrates SageMaker and S3, requires complex configuration and endpoint management", "key_concepts": ["TaskFlow API", "AWS SageMaker Integration", "Machine Learning Deployment", "XCom Communication", "Boto3 AWS SDK"], "learning_objectives": ["Understand ML model deployment workflow", "Learn Airflow task configuration", "Practice cloud service orchestration", "Implement dynamic task dependencies"]}}
{"instruction": "Design a robust machine learning infrastructure automation script that demonstrates error handling, resource cleanup, and systematic endpoint management using Airflow's task decorators and AWS service interactions.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "SageMakerDeleteModelOperator", "SageMakerEndpointConfigOperator", "SageMakerEndpointOperator", "SageMakerEndpointSensor", "SageMakerModelOperator", "SageMakerTrainingOperator"], "line_count": 241, "is_multifile": false}, "output": "TRAIN_DATA = \"0,4.9,2.5,4.5,1.7\\n1,7.0,3.2,4.7,1.4\\n0,7.3,2.9,6.3,1.8\\n2,5.1,3.5,1.4,0.2\\n\"\nSAMPLE_TEST_DATA = \"6.4,3.2,4.5,1.5\"\n\n\n@task\ndef call_endpoint(endpoint_name):\n    response = (\n        boto3.Session()\n        .client(\"sagemaker-runtime\")\n        .invoke_endpoint(\n            EndpointName=endpoint_name,\n            ContentType=\"text/csv\",\n            Body=SAMPLE_TEST_DATA,\n        )\n    )\n\n    return json.loads(response[\"Body\"].read().decode())[\"predictions\"]\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint_config(endpoint_config_job_name):\n    boto3.client(\"sagemaker\").delete_endpoint_config(EndpointConfigName=endpoint_config_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_endpoint(endpoint_name):\n    boto3.client(\"sagemaker\").delete_endpoint(EndpointName=endpoint_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef archive_logs(log_group_name):\n    boto3.client(\"logs\").put_retention_policy(logGroupName=log_group_name, retentionInDays=1)\n\n\n@task\ndef set_up(env_id, role_arn, ti=None):\n    bucket_name = f\"{env_id}-sagemaker\"\n    input_data_s3_key = f\"{env_id}/input-data\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    endpoint_config_job_name = f\"{env_id}-endpoint-config\"\n    endpoint_name = f\"{env_id}-endpoint\"\n    model_name = f\"{env_id}-KNN-model\"\n    training_job_name = f\"{env_id}-train\"\n\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    training_config = {\n        \"TrainingJobName\": training_job_name,\n        \"RoleArn\": role_arn,\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"4\",\n            \"k\": \"3\",\n            \"sample_size\": str(TRAIN_DATA.count(\"\\n\") - 1),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                \"CompressionType\": \"None\",\n                \"ContentType\": \"text/csv\",\n                \"DataSource\": {\n                    \"S3DataSource\": {\n                        \"S3DataDistributionType\": \"FullyReplicated\",\n                        \"S3DataType\": \"S3Prefix\",\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_s3_key}/train.csv\",\n                    }\n                },\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ResourceConfig\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n            \"VolumeSizeInGB\": 1,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 6 * 60},\n    }\n\n    model_config = {\n        \"ModelName\": model_name,\n        \"ExecutionRoleArn\": role_arn,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\",  # noqa: E501\n        },\n    }\n\n    endpoint_config_config = {\n        \"EndpointConfigName\": endpoint_config_job_name,\n        \"ProductionVariants\": [\n            {\n                \"VariantName\": f\"{env_id}-demo\",\n                \"ModelName\": model_name,\n                \"InstanceType\": \"ml.t2.medium\",\n                \"InitialInstanceCount\": 1,\n            },\n        ],\n    }\n\n    deploy_endpoint_config = {\n        \"EndpointName\": endpoint_name,\n        \"EndpointConfigName\": endpoint_config_job_name,\n    }\n\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"input_data_s3_key\", value=input_data_s3_key)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"endpoint_name\", value=endpoint_name)\n    ti.xcom_push(key=\"endpoint_config_job_name\", value=endpoint_config_job_name)\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"endpoint_config_config\", value=endpoint_config_config)\n    ti.xcom_push(key=\"deploy_endpoint_config\", value=deploy_endpoint_config)\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n\n    test_setup = set_up(\n        env_id=test_context[ENV_ID_KEY],\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_data = S3CreateObjectOperator(\n        task_id=\"upload_data\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=f'{test_setup[\"input_data_s3_key\"]}/train.csv',\n        data=TRAIN_DATA,\n    )\n\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n\n    # [START howto_operator_sagemaker_endpoint_config]\n    configure_endpoint = SageMakerEndpointConfigOperator(\n        task_id=\"configure_endpoint\",\n        config=test_setup[\"endpoint_config_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint_config]\n\n    # [START howto_operator_sagemaker_endpoint]\n    deploy_endpoint = SageMakerEndpointOperator(\n        task_id=\"deploy_endpoint\",\n        config=test_setup[\"deploy_endpoint_config\"],\n    )\n    # [END howto_operator_sagemaker_endpoint]\n\n    # SageMakerEndpointOperator waits by default, setting as False to test the Sensor below.\n    deploy_endpoint.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_endpoint]\n    await_endpoint = SageMakerEndpointSensor(\n        task_id=\"await_endpoint\",\n        endpoint_name=test_setup[\"endpoint_name\"],\n    )\n    # [END howto_sensor_sagemaker_endpoint]\n\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/TrainingJobs\", test_context[ENV_ID_KEY]),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_data,\n        # TEST BODY\n        train_model,\n        create_model,\n        configure_endpoint,\n        deploy_endpoint,\n        await_endpoint,\n        call_endpoint(test_setup[\"endpoint_name\"]),\n        # TEST TEARDOWN\n        delete_endpoint_config(test_setup[\"endpoint_config_job_name\"]),\n        delete_endpoint(test_setup[\"endpoint_name\"]),\n        delete_model,\n        delete_bucket,\n        log_cleanup,\n        archive_logs(f\"/aws/sagemaker/Endpoints/{test_setup['endpoint_name']}\"),\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sagemaker_endpoint.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Advanced task configuration with trigger rules, multiple service interactions, complex resource management across different AWS components", "key_concepts": ["Trigger Rules", "Resource Cleanup", "Error Handling", "Cloud Service Orchestration", "Dynamic Configuration"], "learning_objectives": ["Implement graceful resource management", "Understand task dependency strategies", "Learn cloud infrastructure automation", "Practice error-resilient workflow design"]}}
{"instruction": "Build an AWS S3 data management workflow that demonstrates bucket creation, object manipulation, and metadata tagging using Airflow's S3 operators.", "input": {"airflow_version": "2.7.2", "operators": ["BranchPythonOperator", "S3CopyObjectOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3DeleteBucketTaggingOperator", "S3DeleteObjectsOperator", "S3FileTransformOperator", "S3GetBucketTaggingOperator", "S3KeySensor", "S3KeysUnchangedSensor", "S3ListOperator", "S3ListPrefixesOperator", "S3PutBucketTaggingOperator"], "line_count": 293, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CopyObjectOperator,\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteBucketTaggingOperator,\n    S3DeleteObjectsOperator,\n    S3FileTransformOperator,\n    S3GetBucketTaggingOperator,\n    S3ListOperator,\n    S3ListPrefixesOperator,\n    S3PutBucketTaggingOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor, S3KeysUnchangedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDATA = \"\"\"\n    apple,0.5\n    milk,2.5\n    bread,4.0\n\"\"\"\n\n# Empty string prefix refers to the bucket root\n# See what prefix is here https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html\nPREFIX = \"\"\nDELIMITER = \"/\"\nTAG_KEY = \"test-s3-bucket-tagging-key\"\nTAG_VALUE = \"test-s3-bucket-tagging-value\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name = f\"{env_id}-s3-bucket\"\n    bucket_name_2 = f\"{env_id}-s3-bucket-2\"\n\n    key = f\"{env_id}-key\"\n    key_2 = f\"{env_id}-key2\"\n\n    # [START howto_sensor_s3_key_function_definition]\n    def check_fn(files: list) -> bool:\n        \"\"\"\n        Example of custom check: check if all files are bigger than ``20 bytes``\n\n        :param files: List of S3 object attributes.\n        :return: true if the criteria is met\n        \"\"\"\n        return all(f.get(\"Size\", 0) > 20 for f in files)\n\n    # [END howto_sensor_s3_key_function_definition]\n\n    # [START howto_operator_s3_create_bucket]\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_create_bucket]\n\n    create_bucket_2 = S3CreateBucketOperator(\n        task_id=\"create_bucket_2\",\n        bucket_name=bucket_name_2,\n    )\n\n    # [START howto_operator_s3_put_bucket_tagging]\n    put_tagging = S3PutBucketTaggingOperator(\n        task_id=\"put_tagging\",\n        bucket_name=bucket_name,\n        key=TAG_KEY,\n        value=TAG_VALUE,\n    )\n    # [END howto_operator_s3_put_bucket_tagging]\n\n    # [START howto_operator_s3_get_bucket_tagging]\n    get_tagging = S3GetBucketTaggingOperator(\n        task_id=\"get_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_get_bucket_tagging]\n\n    # [START howto_operator_s3_delete_bucket_tagging]\n    delete_tagging = S3DeleteBucketTaggingOperator(\n        task_id=\"delete_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_delete_bucket_tagging]\n\n    # [START howto_operator_s3_create_object]\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=key,\n        data=DATA,\n        replace=True,\n    )\n    # [END howto_operator_s3_create_object]\n\n    create_object_2 = S3CreateObjectOperator(\n        task_id=\"create_object_2\",\n        s3_bucket=bucket_name,\n        s3_key=key_2,\n        data=DATA,\n        replace=True,\n    )\n\n    # [START howto_operator_s3_list_prefixes]\n    list_prefixes = S3ListPrefixesOperator(\n        task_id=\"list_prefixes\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n        delimiter=DELIMITER,\n    )\n    # [END howto_operator_s3_list_prefixes]\n\n    # [START howto_operator_s3_list]\n    list_keys = S3ListOperator(\n        task_id=\"list_keys\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n    )\n    # [END howto_operator_s3_list]\n\n    # [START howto_sensor_s3_key_single_key]\n    # Check if a file exists\n    sensor_one_key = S3KeySensor(\n        task_id=\"sensor_one_key\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n    )\n    # [END howto_sensor_s3_key_single_key]\n\n    # [START howto_sensor_s3_key_multiple_keys]\n    # Check if both files exist\n    sensor_two_keys = S3KeySensor(\n        task_id=\"sensor_two_keys\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n    )\n    # [END howto_sensor_s3_key_multiple_keys]\n\n    # [START howto_sensor_s3_key_single_key_deferrable]\n    # Check if a file exists\n    sensor_one_key_deferrable = S3KeySensor(\n        task_id=\"sensor_one_key_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_single_key_deferrable]\n\n    # [START howto_sensor_s3_key_multiple_keys_deferrable]\n    # Check if both files exist\n    sensor_two_keys_deferrable = S3KeySensor(\n        task_id=\"sensor_two_keys_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_multiple_keys_deferrable]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function_deferrable = S3KeySensor(\n        task_id=\"sensor_key_with_function_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function = S3KeySensor(\n        task_id=\"sensor_key_with_function\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_operator_s3_copy_object]\n    copy_object = S3CopyObjectOperator(\n        task_id=\"copy_object\",\n        source_bucket_name=bucket_name,\n        dest_bucket_name=bucket_name_2,\n        source_bucket_key=key,\n        dest_bucket_key=key_2,\n    )\n    # [END howto_operator_s3_copy_object]\n\n    # [START howto_operator_s3_file_transform]\n    file_transform = S3FileTransformOperator(\n        task_id=\"file_transform\",\n        source_s3_key=f\"s3://{bucket_name}/{key}\",\n        dest_s3_key=f\"s3://{bucket_name_2}/{key_2}\",\n        # Use `cp` command as transform script as an example\n        transform_script=\"cp\",\n        replace=True,\n    )\n    # [END howto_operator_s3_file_transform]\n\n    # This task skips the `sensor_keys_unchanged` task because the S3KeysUnchangedSensor\n    # runs in poke mode only, which is not supported by the DebugExecutor, causing system tests to fail.\n    branching = BranchPythonOperator(\n        task_id=\"branch_to_delete_objects\", python_callable=lambda: \"delete_objects\"\n    )\n\n    # [START howto_sensor_s3_keys_unchanged]\n    sensor_keys_unchanged = S3KeysUnchangedSensor(\n        task_id=\"sensor_keys_unchanged\",\n        bucket_name=bucket_name_2,\n        prefix=PREFIX,\n        inactivity_period=10,  # inactivity_period in seconds\n    )\n    # [END howto_sensor_s3_keys_unchanged]\n\n    # [START howto_operator_s3_delete_objects]\n    delete_objects = S3DeleteObjectsOperator(\n        task_id=\"delete_objects\",\n        bucket=bucket_name_2,\n        keys=key_2,\n    )\n    # [END howto_operator_s3_delete_objects]\n    delete_objects.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_s3_delete_bucket]\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n    # [END howto_operator_s3_delete_bucket]\n    delete_bucket.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket_2 = S3DeleteBucketOperator(\n        task_id=\"delete_bucket_2\",\n        bucket_name=bucket_name_2,\n        force_delete=True,\n    )\n    delete_bucket_2.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_bucket,\n        create_bucket_2,\n        put_tagging,\n        get_tagging,\n        delete_tagging,\n        create_object,\n        create_object_2,\n        list_prefixes,\n        list_keys,\n        [sensor_one_key, sensor_two_keys, sensor_key_with_function],\n        [sensor_one_key_deferrable, sensor_two_keys_deferrable, sensor_key_with_function_deferrable],\n        copy_object,\n        file_transform,\n        branching,\n        sensor_keys_unchanged,\n        # TEST TEARDOWN\n        delete_objects,\n        delete_bucket,\n        delete_bucket_2,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS S3 operators, includes bucket and object operations, involves custom logic with sensors and tagging, requires cloud service integration", "key_concepts": ["S3 Bucket Management", "Object Creation", "Bucket Tagging", "AWS Provider Operators", "Cloud Service Orchestration"], "learning_objectives": ["Understand S3 operator interactions", "Learn cloud service workflow design", "Practice Airflow AWS provider configuration"]}}
{"instruction": "Design an automated data staging and validation pipeline using S3 sensors and custom file checking functions to ensure data quality before downstream processing.", "input": {"airflow_version": "2.7.2", "operators": ["BranchPythonOperator", "S3CopyObjectOperator", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "S3DeleteBucketTaggingOperator", "S3DeleteObjectsOperator", "S3FileTransformOperator", "S3GetBucketTaggingOperator", "S3KeySensor", "S3KeysUnchangedSensor", "S3ListOperator", "S3ListPrefixesOperator", "S3PutBucketTaggingOperator"], "line_count": 293, "is_multifile": true}, "output": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CopyObjectOperator,\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n    S3DeleteBucketTaggingOperator,\n    S3DeleteObjectsOperator,\n    S3FileTransformOperator,\n    S3GetBucketTaggingOperator,\n    S3ListOperator,\n    S3ListPrefixesOperator,\n    S3PutBucketTaggingOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor, S3KeysUnchangedSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder\n\nDAG_ID = \"example_s3\"\n\nsys_test_context_task = SystemTestContextBuilder().build()\n\nDATA = \"\"\"\n    apple,0.5\n    milk,2.5\n    bread,4.0\n\"\"\"\n\n# Empty string prefix refers to the bucket root\n# See what prefix is here https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html\nPREFIX = \"\"\nDELIMITER = \"/\"\nTAG_KEY = \"test-s3-bucket-tagging-key\"\nTAG_VALUE = \"test-s3-bucket-tagging-value\"\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    bucket_name = f\"{env_id}-s3-bucket\"\n    bucket_name_2 = f\"{env_id}-s3-bucket-2\"\n\n    key = f\"{env_id}-key\"\n    key_2 = f\"{env_id}-key2\"\n\n    # [START howto_sensor_s3_key_function_definition]\n    def check_fn(files: list) -> bool:\n        \"\"\"\n        Example of custom check: check if all files are bigger than ``20 bytes``\n\n        :param files: List of S3 object attributes.\n        :return: true if the criteria is met\n        \"\"\"\n        return all(f.get(\"Size\", 0) > 20 for f in files)\n\n    # [END howto_sensor_s3_key_function_definition]\n\n    # [START howto_operator_s3_create_bucket]\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_create_bucket]\n\n    create_bucket_2 = S3CreateBucketOperator(\n        task_id=\"create_bucket_2\",\n        bucket_name=bucket_name_2,\n    )\n\n    # [START howto_operator_s3_put_bucket_tagging]\n    put_tagging = S3PutBucketTaggingOperator(\n        task_id=\"put_tagging\",\n        bucket_name=bucket_name,\n        key=TAG_KEY,\n        value=TAG_VALUE,\n    )\n    # [END howto_operator_s3_put_bucket_tagging]\n\n    # [START howto_operator_s3_get_bucket_tagging]\n    get_tagging = S3GetBucketTaggingOperator(\n        task_id=\"get_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_get_bucket_tagging]\n\n    # [START howto_operator_s3_delete_bucket_tagging]\n    delete_tagging = S3DeleteBucketTaggingOperator(\n        task_id=\"delete_tagging\",\n        bucket_name=bucket_name,\n    )\n    # [END howto_operator_s3_delete_bucket_tagging]\n\n    # [START howto_operator_s3_create_object]\n    create_object = S3CreateObjectOperator(\n        task_id=\"create_object\",\n        s3_bucket=bucket_name,\n        s3_key=key,\n        data=DATA,\n        replace=True,\n    )\n    # [END howto_operator_s3_create_object]\n\n    create_object_2 = S3CreateObjectOperator(\n        task_id=\"create_object_2\",\n        s3_bucket=bucket_name,\n        s3_key=key_2,\n        data=DATA,\n        replace=True,\n    )\n\n    # [START howto_operator_s3_list_prefixes]\n    list_prefixes = S3ListPrefixesOperator(\n        task_id=\"list_prefixes\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n        delimiter=DELIMITER,\n    )\n    # [END howto_operator_s3_list_prefixes]\n\n    # [START howto_operator_s3_list]\n    list_keys = S3ListOperator(\n        task_id=\"list_keys\",\n        bucket=bucket_name,\n        prefix=PREFIX,\n    )\n    # [END howto_operator_s3_list]\n\n    # [START howto_sensor_s3_key_single_key]\n    # Check if a file exists\n    sensor_one_key = S3KeySensor(\n        task_id=\"sensor_one_key\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n    )\n    # [END howto_sensor_s3_key_single_key]\n\n    # [START howto_sensor_s3_key_multiple_keys]\n    # Check if both files exist\n    sensor_two_keys = S3KeySensor(\n        task_id=\"sensor_two_keys\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n    )\n    # [END howto_sensor_s3_key_multiple_keys]\n\n    # [START howto_sensor_s3_key_single_key_deferrable]\n    # Check if a file exists\n    sensor_one_key_deferrable = S3KeySensor(\n        task_id=\"sensor_one_key_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_single_key_deferrable]\n\n    # [START howto_sensor_s3_key_multiple_keys_deferrable]\n    # Check if both files exist\n    sensor_two_keys_deferrable = S3KeySensor(\n        task_id=\"sensor_two_keys_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=[key, key_2],\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_multiple_keys_deferrable]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function_deferrable = S3KeySensor(\n        task_id=\"sensor_key_with_function_deferrable\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n        deferrable=True,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_sensor_s3_key_function]\n    # Check if a file exists and match a certain pattern defined in check_fn\n    sensor_key_with_function = S3KeySensor(\n        task_id=\"sensor_key_with_function\",\n        bucket_name=bucket_name,\n        bucket_key=key,\n        check_fn=check_fn,\n    )\n    # [END howto_sensor_s3_key_function]\n\n    # [START howto_operator_s3_copy_object]\n    copy_object = S3CopyObjectOperator(\n        task_id=\"copy_object\",\n        source_bucket_name=bucket_name,\n        dest_bucket_name=bucket_name_2,\n        source_bucket_key=key,\n        dest_bucket_key=key_2,\n    )\n    # [END howto_operator_s3_copy_object]\n\n    # [START howto_operator_s3_file_transform]\n    file_transform = S3FileTransformOperator(\n        task_id=\"file_transform\",\n        source_s3_key=f\"s3://{bucket_name}/{key}\",\n        dest_s3_key=f\"s3://{bucket_name_2}/{key_2}\",\n        # Use `cp` command as transform script as an example\n        transform_script=\"cp\",\n        replace=True,\n    )\n    # [END howto_operator_s3_file_transform]\n\n    # This task skips the `sensor_keys_unchanged` task because the S3KeysUnchangedSensor\n    # runs in poke mode only, which is not supported by the DebugExecutor, causing system tests to fail.\n    branching = BranchPythonOperator(\n        task_id=\"branch_to_delete_objects\", python_callable=lambda: \"delete_objects\"\n    )\n\n    # [START howto_sensor_s3_keys_unchanged]\n    sensor_keys_unchanged = S3KeysUnchangedSensor(\n        task_id=\"sensor_keys_unchanged\",\n        bucket_name=bucket_name_2,\n        prefix=PREFIX,\n        inactivity_period=10,  # inactivity_period in seconds\n    )\n    # [END howto_sensor_s3_keys_unchanged]\n\n    # [START howto_operator_s3_delete_objects]\n    delete_objects = S3DeleteObjectsOperator(\n        task_id=\"delete_objects\",\n        bucket=bucket_name_2,\n        keys=key_2,\n    )\n    # [END howto_operator_s3_delete_objects]\n    delete_objects.trigger_rule = TriggerRule.ALL_DONE\n\n    # [START howto_operator_s3_delete_bucket]\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        bucket_name=bucket_name,\n        force_delete=True,\n    )\n    # [END howto_operator_s3_delete_bucket]\n    delete_bucket.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket_2 = S3DeleteBucketOperator(\n        task_id=\"delete_bucket_2\",\n        bucket_name=bucket_name_2,\n        force_delete=True,\n    )\n    delete_bucket_2.trigger_rule = TriggerRule.ALL_DONE\n\n    chain(\n        # TEST SETUP\n        test_context,\n        # TEST BODY\n        create_bucket,\n        create_bucket_2,\n        put_tagging,\n        get_tagging,\n        delete_tagging,\n        create_object,\n        create_object_2,\n        list_prefixes,\n        list_keys,\n        [sensor_one_key, sensor_two_keys, sensor_key_with_function],\n        [sensor_one_key_deferrable, sensor_two_keys_deferrable, sensor_key_with_function_deferrable],\n        copy_object,\n        file_transform,\n        branching,\n        sensor_keys_unchanged,\n        # TEST TEARDOWN\n        delete_objects,\n        delete_bucket,\n        delete_bucket_2,\n    )\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_s3.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Incorporates custom sensor logic, file validation function, demonstrates advanced workflow control and data validation techniques", "key_concepts": ["S3 Sensors", "Custom Validation Functions", "Data Quality Checks", "File Existence Verification", "Conditional Task Execution"], "learning_objectives": ["Implement file-based data validation", "Use S3 sensors for workflow control", "Create dynamic task execution strategies"]}}
{"instruction": "Build an end-to-end machine learning pipeline using Amazon SageMaker and Airflow, demonstrating how to orchestrate data preprocessing, model training, and deployment using AWS cloud services.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "SageMakerAutoMLOperator", "SageMakerAutoMLSensor", "SageMakerCreateExperimentOperator", "SageMakerDeleteModelOperator", "SageMakerModelOperator", "SageMakerPipelineSensor", "SageMakerProcessingOperator", "SageMakerRegisterModelVersionOperator", "SageMakerStartPipelineOperator", "SageMakerStopPipelineOperator", "SageMakerTrainingOperator", "SageMakerTrainingSensor", "SageMakerTransformOperator", "SageMakerTransformSensor", "SageMakerTuningOperator", "SageMakerTuningSensor"], "line_count": 689, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import get_current_context\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateExperimentOperator,\n    SageMakerDeleteModelOperator,\n    SageMakerModelOperator,\n    SageMakerProcessingOperator,\n    SageMakerRegisterModelVersionOperator,\n    SageMakerStartPipelineOperator,\n    SageMakerStopPipelineOperator,\n    SageMakerTrainingOperator,\n    SageMakerTransformOperator,\n    SageMakerTuningOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerAutoMLSensor,\n    SageMakerPipelineSensor,\n    SageMakerTrainingSensor,\n    SageMakerTransformSensor,\n    SageMakerTuningSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_sagemaker\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# The URI of a Docker image for handling KNN model training.\n# To find the URI of a free Amazon-provided image that can be used, substitute your\n# desired region in the following link and find the URI under \"Registry Path\".\n# https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-east-1.html#knn-us-east-1.title\n# This URI should be in the format of {12-digits}.dkr.ecr.{region}.amazonaws.com/knn\nKNN_IMAGES_BY_REGION = {\n    \"us-east-1\": \"382416733822.dkr.ecr.us-east-1.amazonaws.com/knn:1\",\n    \"us-west-2\": \"174872318107.dkr.ecr.us-west-2.amazonaws.com/knn:1\",\n}\n\nSAMPLE_SIZE = 600\n\n# This script will be the entrypoint for the docker image which will handle preprocessing the raw data\n# NOTE:  The following string must remain dedented as it is being written to a file.\nPREPROCESS_SCRIPT_TEMPLATE = \"\"\"\nimport boto3\nimport numpy as np\nimport pandas as pd\n\ndef main():\n    # Load the dataset from {input_path}/input.csv, split it into train/test\n    # subsets, and write them to {output_path}/ for the Processing Operator.\n\n    data = pd.read_csv('{input_path}/input.csv')\n\n    # Split into test and train data\n    data_train, data_test = np.split(\n        data.sample(frac=1, random_state=np.random.RandomState()), [int(0.7 * len(data))]\n    )\n\n    # Remove the \"answers\" from the test set\n    data_test.drop(['class'], axis=1, inplace=True)\n\n    # Write the splits to disk\n    data_train.to_csv('{output_path}/train.csv', index=False, header=False)\n    data_test.to_csv('{output_path}/test.csv', index=False, header=False)\n\n    print('Preprocessing Done.')\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n\ndef _create_ecr_repository(repo_name):\n    execution_role_arn = boto3.client(\"sts\").get_caller_identity()[\"Arn\"]\n    access_policy = {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"Allow access to the system test execution role\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"AWS\": execution_role_arn},\n                \"Action\": \"ecr:*\",\n            }\n        ],\n    }\n\n    client = boto3.client(\"ecr\")\n    repo = client.create_repository(repositoryName=repo_name)[\"repository\"]\n    client.set_repository_policy(repositoryName=repo[\"repositoryName\"], policyText=json.dumps(access_policy))\n\n    return repo[\"repositoryUri\"]\n\n\ndef _build_and_upload_docker_image(preprocess_script, repository_uri):\n    \"\"\"\n    We need a Docker image with the following requirements:\n      - Has numpy, pandas, requests, and boto3 installed\n      - Has our data preprocessing script mounted and set as the entry point\n    \"\"\"\n    with NamedTemporaryFile(mode=\"w+t\") as preprocessing_script, NamedTemporaryFile(mode=\"w+t\") as dockerfile:\n        preprocessing_script.write(preprocess_script)\n        preprocessing_script.flush()\n\n        dockerfile.write(\n            f\"\"\"\n            FROM public.ecr.aws/amazonlinux/amazonlinux\n            COPY {preprocessing_script.name.split('/')[2]} /preprocessing.py\n            ADD credentials /credentials\n            ENV AWS_SHARED_CREDENTIALS_FILE=/credentials\n            RUN yum install python3 pip -y\n            RUN pip3 install boto3 pandas requests\n            CMD [ \"python3\", \"/preprocessing.py\"]\n            \"\"\"\n        )\n        dockerfile.flush()\n\n        ecr_region = repository_uri.split(\".\")[3]\n        docker_build_and_push_commands = f\"\"\"\n            cp /root/.aws/credentials /tmp/credentials &&\n            # login to public ecr repo containing amazonlinux image (public login is always on us east 1)\n            aws ecr-public get-login-password --region us-east-1 |\n            docker login --username AWS --password-stdin public.ecr.aws &&\n            docker build --platform=linux/amd64 -f {dockerfile.name} -t {repository_uri} /tmp &&\n            rm /tmp/credentials &&\n\n            # login again, this time to the private repo we created to hold that specific image\n            aws ecr get-login-password --region {ecr_region} |\n            docker login --username AWS --password-stdin {repository_uri} &&\n            docker push {repository_uri}\n            \"\"\"\n        logging.info(\"building and uploading docker image for preprocessing...\")\n        docker_build = subprocess.Popen(\n            docker_build_and_push_commands,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _, stderr = docker_build.communicate()\n        if docker_build.returncode != 0:\n            raise RuntimeError(\n                \"Failed to prepare docker image for the preprocessing job.\\n\"\n                \"The following error happened while executing the sequence of bash commands:\\n\"\n                f\"{stderr.decode()}\"\n            )\n\n\ndef generate_data() -> str:\n    \"\"\"generates a very simple csv dataset with headers\"\"\"\n    content = \"class,x,y\\n\"  # headers\n    for i in range(SAMPLE_SIZE):\n        content += f\"{i%100},{i},{SAMPLE_SIZE-i}\\n\"\n    return content\n\n\n@task\ndef set_up(env_id, role_arn):\n    bucket_name = f\"{env_id}-sagemaker-example\"\n    ecr_repository_name = f\"{env_id}-repo\"\n    model_name = f\"{env_id}-KNN-model\"\n    processing_job_name = f\"{env_id}-processing\"\n    training_job_name = f\"{env_id}-train\"\n    transform_job_name = f\"{env_id}-transform\"\n    tuning_job_name = f\"{env_id}-tune\"\n    model_package_group_name = f\"{env_id}-group\"\n    pipeline_name = f\"{env_id}-pipe\"\n    auto_ml_job_name = f\"{env_id}-automl\"\n    experiment_name = f\"{env_id}-experiment\"\n\n    input_data_S3_key = f\"{env_id}/processed-input-data\"\n    prediction_output_s3_key = f\"{env_id}/transform\"\n    processing_local_input_path = \"/opt/ml/processing/input\"\n    processing_local_output_path = \"/opt/ml/processing/output\"\n    raw_data_s3_key = f\"{env_id}/preprocessing/input.csv\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    ecr_repository_uri = _create_ecr_repository(ecr_repository_name)\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    # Json definition for a dummy pipeline of 30 chained \"conditional step\" checking that 3 < 6\n    # Each step takes roughly 1 second to execute, so the pipeline runtimes is ~30 seconds, which should be\n    # enough to test stopping and awaiting without race conditions.\n    # Built using sagemaker sdk, and using json.loads(pipeline.definition())\n    pipeline_json_definition = \"\"\"{\"Version\": \"2020-12-01\", \"Metadata\": {}, \"Parameters\": [], \"PipelineExperimentConfig\": {\"ExperimentName\": {\"Get\": \"Execution.PipelineName\"}, \"TrialName\": {\"Get\": \"Execution.PipelineExecutionId\"}}, \"Steps\": [{\"Name\": \"DummyCond29\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond28\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond27\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond26\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond25\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond24\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond23\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond22\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond21\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond20\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond19\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond18\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond17\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond16\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond15\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond14\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond13\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond12\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond11\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond10\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond9\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond8\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond7\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond6\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond5\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond4\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond3\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond2\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond1\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond0\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}]}\"\"\"  # noqa: E501\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.create_pipeline(\n        PipelineName=pipeline_name, PipelineDefinition=pipeline_json_definition, RoleArn=role_arn\n    )\n\n    resource_config = {\n        \"InstanceCount\": 1,\n        \"InstanceType\": \"ml.m5.large\",\n        \"VolumeSizeInGB\": 1,\n    }\n    input_data_uri = f\"s3://{bucket_name}/{raw_data_s3_key}\"\n    processing_config = {\n        \"ProcessingJobName\": processing_job_name,\n        \"ProcessingInputs\": [\n            {\n                \"InputName\": \"input\",\n                \"AppManaged\": False,\n                \"S3Input\": {\n                    \"S3Uri\": input_data_uri,\n                    \"LocalPath\": processing_local_input_path,\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3InputMode\": \"File\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"S3CompressionType\": \"None\",\n                },\n            },\n        ],\n        \"ProcessingOutputConfig\": {\n            \"Outputs\": [\n                {\n                    \"OutputName\": \"output\",\n                    \"S3Output\": {\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}\",\n                        \"LocalPath\": processing_local_output_path,\n                        \"S3UploadMode\": \"EndOfJob\",\n                    },\n                    \"AppManaged\": False,\n                }\n            ]\n        },\n        \"ProcessingResources\": {\n            \"ClusterConfig\": resource_config,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"AppSpecification\": {\n            \"ImageUri\": ecr_repository_uri,\n        },\n        \"RoleArn\": role_arn,\n    }\n\n    training_data_source = {\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"text/csv\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/train.csv\",\n            }\n        },\n    }\n    training_config = {\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"2\",\n            \"k\": \"3\",\n            \"sample_size\": str(SAMPLE_SIZE),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                **training_data_source,\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ExperimentConfig\": {\"ExperimentName\": experiment_name},\n        \"ResourceConfig\": resource_config,\n        \"RoleArn\": role_arn,\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"TrainingJobName\": training_job_name,\n    }\n    model_trained_weights = (\n        f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\"\n    )\n    model_config = {\n        \"ExecutionRoleArn\": role_arn,\n        \"ModelName\": model_name,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": model_trained_weights,\n        },\n    }\n    tuning_config = {\n        \"HyperParameterTuningJobName\": tuning_job_name,\n        \"HyperParameterTuningJobConfig\": {\n            \"Strategy\": \"Bayesian\",\n            \"HyperParameterTuningJobObjective\": {\n                \"MetricName\": \"test:accuracy\",\n                \"Type\": \"Maximize\",\n            },\n            \"ResourceLimits\": {\n                \"MaxNumberOfTrainingJobs\": 10,\n                \"MaxParallelTrainingJobs\": 10,\n            },\n            \"ParameterRanges\": {\n                \"CategoricalParameterRanges\": [],\n                \"IntegerParameterRanges\": [\n                    # Set the min and max values of the hyperparameters you want to tune.\n                    {\n                        \"Name\": \"k\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                    {\n                        \"Name\": \"sample_size\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                ],\n            },\n        },\n        \"TrainingJobDefinition\": {\n            \"StaticHyperParameters\": {\n                \"predictor_type\": \"classifier\",\n                \"feature_dim\": \"2\",\n            },\n            \"AlgorithmSpecification\": {\"TrainingImage\": knn_image_uri, \"TrainingInputMode\": \"File\"},\n            \"InputDataConfig\": [\n                {\n                    \"ChannelName\": \"train\",\n                    **training_data_source,\n                },\n                {\n                    \"ChannelName\": \"test\",\n                    **training_data_source,\n                },\n            ],\n            \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}\"},\n            \"ResourceConfig\": resource_config,\n            \"RoleArn\": role_arn,\n            \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        },\n    }\n    transform_config = {\n        \"TransformJobName\": transform_job_name,\n        \"TransformInput\": {\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/test.csv\",\n                }\n            },\n            \"SplitType\": \"Line\",\n            \"ContentType\": \"text/csv\",\n        },\n        \"TransformOutput\": {\"S3OutputPath\": f\"s3://{bucket_name}/{prediction_output_s3_key}\"},\n        \"TransformResources\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n        },\n        \"ModelName\": model_name,\n    }\n\n    preprocess_script = PREPROCESS_SCRIPT_TEMPLATE.format(\n        input_path=processing_local_input_path, output_path=processing_local_output_path\n    )\n    _build_and_upload_docker_image(preprocess_script, ecr_repository_uri)\n\n    ti = get_current_context()[\"ti\"]\n    ti.xcom_push(key=\"docker_image\", value=ecr_repository_uri)\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"raw_data_s3_key\", value=raw_data_s3_key)\n    ti.xcom_push(key=\"ecr_repository_name\", value=ecr_repository_name)\n    ti.xcom_push(key=\"processing_config\", value=processing_config)\n    ti.xcom_push(key=\"input_data_uri\", value=input_data_uri)\n    ti.xcom_push(key=\"output_data_uri\", value=f\"s3://{bucket_name}/{training_output_s3_key}\")\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"training_job_name\", value=training_job_name)\n    ti.xcom_push(key=\"model_package_group_name\", value=model_package_group_name)\n    ti.xcom_push(key=\"pipeline_name\", value=pipeline_name)\n    ti.xcom_push(key=\"auto_ml_job_name\", value=auto_ml_job_name)\n    ti.xcom_push(key=\"experiment_name\", value=experiment_name)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"inference_code_image\", value=knn_image_uri)\n    ti.xcom_push(key=\"model_trained_weights\", value=model_trained_weights)\n    ti.xcom_push(key=\"tuning_config\", value=tuning_config)\n    ti.xcom_push(key=\"tuning_job_name\", value=tuning_job_name)\n    ti.xcom_push(key=\"transform_config\", value=transform_config)\n    ti.xcom_push(key=\"transform_job_name\", value=transform_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ecr_repository(repository_name):\n    client = boto3.client(\"ecr\")\n\n    # All images must be removed from the repo before it can be deleted.\n    image_ids = client.list_images(repositoryName=repository_name)[\"imageIds\"]\n    client.batch_delete_image(\n        repositoryName=repository_name,\n        imageIds=[{\"imageDigest\": image[\"imageDigest\"] for image in image_ids}],\n    )\n    client.delete_repository(repositoryName=repository_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_model_group(group_name, model_version_arn):\n    sgmk_client = boto3.client(\"sagemaker\")\n    # need to destroy model registered in group first\n    sgmk_client.delete_model_package(ModelPackageName=model_version_arn)\n    sgmk_client.delete_model_package_group(ModelPackageGroupName=group_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_pipeline(pipeline_name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.delete_pipeline(PipelineName=pipeline_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_experiment(name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    trials = sgmk_client.list_trials(ExperimentName=name)\n    trials_names = [s[\"TrialName\"] for s in trials[\"TrialSummaries\"]]\n    for trial in trials_names:\n        components = sgmk_client.list_trial_components(TrialName=trial)\n        components_names = [s[\"TrialComponentName\"] for s in components[\"TrialComponentSummaries\"]]\n        for component in components_names:\n            sgmk_client.disassociate_trial_component(TrialComponentName=component, TrialName=trial)\n            sgmk_client.delete_trial_component(TrialComponentName=component)\n        sgmk_client.delete_trial(TrialName=trial)\n    sgmk_client.delete_experiment(ExperimentName=name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_docker_image(image_name):\n    docker_build = subprocess.Popen(\n        f\"docker rmi {image_name}\",\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, stderr = docker_build.communicate()\n    if docker_build.returncode != 0:\n        logging.error(\n            \"Failed to delete local docker image. \"\n            \"Run 'docker images' to see if you need to clean it yourself.\\n\"\n            f\"error message: {stderr}\"\n        )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    test_setup = set_up(\n        env_id=env_id,\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_dataset = S3CreateObjectOperator(\n        task_id=\"upload_dataset\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=test_setup[\"raw_data_s3_key\"],\n        data=generate_data(),\n        replace=True,\n    )\n\n    # [START howto_operator_sagemaker_auto_ml]\n    automl = SageMakerAutoMLOperator(\n        task_id=\"auto_ML\",\n        job_name=test_setup[\"auto_ml_job_name\"],\n        s3_input=test_setup[\"input_data_uri\"],\n        target_attribute=\"class\",\n        s3_output=test_setup[\"output_data_uri\"],\n        role_arn=test_context[ROLE_ARN_KEY],\n        time_limit=30,  # will stop the job before it can do anything, but it's not the point here\n    )\n    # [END howto_operator_sagemaker_auto_ml]\n    automl.wait_for_completion = False  # just to be able to test the sensor next\n\n    # [START howto_sensor_sagemaker_auto_ml]\n    await_automl = SageMakerAutoMLSensor(job_name=test_setup[\"auto_ml_job_name\"], task_id=\"await_auto_ML\")\n    # [END howto_sensor_sagemaker_auto_ml]\n    await_automl.poke_interval = 10\n\n    # [START howto_operator_sagemaker_start_pipeline]\n    start_pipeline1 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline1\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n    # [END howto_operator_sagemaker_start_pipeline]\n\n    # [START howto_operator_sagemaker_stop_pipeline]\n    stop_pipeline1 = SageMakerStopPipelineOperator(\n        task_id=\"stop_pipeline1\",\n        pipeline_exec_arn=start_pipeline1.output,\n    )\n    # [END howto_operator_sagemaker_stop_pipeline]\n\n    start_pipeline2 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline2\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n\n    # [START howto_sensor_sagemaker_pipeline]\n    await_pipeline2 = SageMakerPipelineSensor(\n        task_id=\"await_pipeline2\",\n        pipeline_exec_arn=start_pipeline2.output,\n    )\n    # [END howto_sensor_sagemaker_pipeline]\n    await_pipeline2.poke_interval = 10\n\n    # [START howto_operator_sagemaker_experiment]\n    create_experiment = SageMakerCreateExperimentOperator(\n        task_id=\"create_experiment\", name=test_setup[\"experiment_name\"]\n    )\n    # [END howto_operator_sagemaker_experiment]\n\n    # [START howto_operator_sagemaker_processing]\n    preprocess_raw_data = SageMakerProcessingOperator(\n        task_id=\"preprocess_raw_data\",\n        config=test_setup[\"processing_config\"],\n    )\n    # [END howto_operator_sagemaker_processing]\n\n    # [START howto_operator_sagemaker_training]\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n    # [END howto_operator_sagemaker_training]\n\n    # SageMakerTrainingOperator waits by default, setting as False to test the Sensor below.\n    train_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_training]\n    await_training = SageMakerTrainingSensor(\n        task_id=\"await_training\",\n        job_name=test_setup[\"training_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_training]\n\n    # [START howto_operator_sagemaker_model]\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n    # [END howto_operator_sagemaker_model]\n\n    # [START howto_operator_sagemaker_register]\n    register_model = SageMakerRegisterModelVersionOperator(\n        task_id=\"register_model\",\n        image_uri=test_setup[\"inference_code_image\"],\n        model_url=test_setup[\"model_trained_weights\"],\n        package_group_name=test_setup[\"model_package_group_name\"],\n    )\n    # [END howto_operator_sagemaker_register]\n\n    # [START howto_operator_sagemaker_tuning]\n    tune_model = SageMakerTuningOperator(\n        task_id=\"tune_model\",\n        config=test_setup[\"tuning_config\"],\n    )\n    # [END howto_operator_sagemaker_tuning]\n\n    # SageMakerTuningOperator waits by default, setting as False to test the Sensor below.\n    tune_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_tuning]\n    await_tuning = SageMakerTuningSensor(\n        task_id=\"await_tuning\",\n        job_name=test_setup[\"tuning_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_tuning]\n\n    # [START howto_operator_sagemaker_transform]\n    test_model = SageMakerTransformOperator(\n        task_id=\"test_model\",\n        config=test_setup[\"transform_config\"],\n    )\n    # [END howto_operator_sagemaker_transform]\n\n    # SageMakerTransformOperator waits by default, setting as False to test the Sensor below.\n    test_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_transform]\n    await_transform = SageMakerTransformSensor(\n        task_id=\"await_transform\",\n        job_name=test_setup[\"transform_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_transform]\n\n    # [START howto_operator_sagemaker_delete_model]\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n    # [END howto_operator_sagemaker_delete_model]\n    delete_model.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/ProcessingJobs\", env_id),\n            (\"/aws/sagemaker/TrainingJobs\", env_id),\n            (\"/aws/sagemaker/TransformJobs\", env_id),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_dataset,\n        # TEST BODY\n        automl,\n        await_automl,\n        start_pipeline1,\n        start_pipeline2,\n        stop_pipeline1,\n        await_pipeline2,\n        create_experiment,\n        preprocess_raw_data,\n        train_model,\n        await_training,\n        create_model,\n        register_model,\n        tune_model,\n        await_tuning,\n        test_model,\n        await_transform,\n        # TEST TEARDOWN\n        delete_ecr_repository(test_setup[\"ecr_repository_name\"]),\n        delete_model_group(test_setup[\"model_package_group_name\"], register_model.output),\n        delete_model,\n        delete_bucket,\n        delete_experiment(test_setup[\"experiment_name\"]),\n        delete_pipeline(test_setup[\"pipeline_name\"]),\n        delete_docker_image(test_setup[\"docker_image\"]),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sagemaker.py", "instruction_source": "claude-3.5", "instruction_type": "primary", "claude_complexity_score": 3, "claude_complexity_level": "hard", "claude_complexity_reasoning": "Multiple advanced SageMaker operators, complex AWS integration, custom preprocessing script, multiple task dependencies, and cloud service interactions", "key_concepts": ["AWS SageMaker Integration", "Machine Learning Orchestration", "Cloud Service DAG Design", "Task Chaining", "Operator Composition"], "learning_objectives": ["Understand cloud-based ML pipeline construction", "Learn Airflow's AWS provider integration", "Master complex task dependency management", "Implement end-to-end ML workflow automation"]}}
{"instruction": "Create a modular data science workflow demonstrating how to use Airflow decorators and AWS operators to build a reproducible machine learning experiment tracking and deployment process.", "input": {"airflow_version": "2.7.2", "operators": ["@task", "S3CreateBucketOperator", "S3CreateObjectOperator", "S3DeleteBucketOperator", "SageMakerAutoMLOperator", "SageMakerAutoMLSensor", "SageMakerCreateExperimentOperator", "SageMakerDeleteModelOperator", "SageMakerModelOperator", "SageMakerPipelineSensor", "SageMakerProcessingOperator", "SageMakerRegisterModelVersionOperator", "SageMakerStartPipelineOperator", "SageMakerStopPipelineOperator", "SageMakerTrainingOperator", "SageMakerTrainingSensor", "SageMakerTransformOperator", "SageMakerTransformSensor", "SageMakerTuningOperator", "SageMakerTuningSensor"], "line_count": 689, "is_multifile": true}, "output": "from __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom datetime import datetime\nfrom tempfile import NamedTemporaryFile\n\nimport boto3\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.python import get_current_context\nfrom airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3CreateObjectOperator,\n    S3DeleteBucketOperator,\n)\nfrom airflow.providers.amazon.aws.operators.sagemaker import (\n    SageMakerAutoMLOperator,\n    SageMakerCreateExperimentOperator,\n    SageMakerDeleteModelOperator,\n    SageMakerModelOperator,\n    SageMakerProcessingOperator,\n    SageMakerRegisterModelVersionOperator,\n    SageMakerStartPipelineOperator,\n    SageMakerStopPipelineOperator,\n    SageMakerTrainingOperator,\n    SageMakerTransformOperator,\n    SageMakerTuningOperator,\n)\nfrom airflow.providers.amazon.aws.sensors.sagemaker import (\n    SageMakerAutoMLSensor,\n    SageMakerPipelineSensor,\n    SageMakerTrainingSensor,\n    SageMakerTransformSensor,\n    SageMakerTuningSensor,\n)\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom tests.system.providers.amazon.aws.utils import ENV_ID_KEY, SystemTestContextBuilder, prune_logs\n\nDAG_ID = \"example_sagemaker\"\n\n# Externally fetched variables:\nROLE_ARN_KEY = \"ROLE_ARN\"\n\nsys_test_context_task = SystemTestContextBuilder().add_variable(ROLE_ARN_KEY).build()\n\n# The URI of a Docker image for handling KNN model training.\n# To find the URI of a free Amazon-provided image that can be used, substitute your\n# desired region in the following link and find the URI under \"Registry Path\".\n# https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-east-1.html#knn-us-east-1.title\n# This URI should be in the format of {12-digits}.dkr.ecr.{region}.amazonaws.com/knn\nKNN_IMAGES_BY_REGION = {\n    \"us-east-1\": \"382416733822.dkr.ecr.us-east-1.amazonaws.com/knn:1\",\n    \"us-west-2\": \"174872318107.dkr.ecr.us-west-2.amazonaws.com/knn:1\",\n}\n\nSAMPLE_SIZE = 600\n\n# This script will be the entrypoint for the docker image which will handle preprocessing the raw data\n# NOTE:  The following string must remain dedented as it is being written to a file.\nPREPROCESS_SCRIPT_TEMPLATE = \"\"\"\nimport boto3\nimport numpy as np\nimport pandas as pd\n\ndef main():\n    # Load the dataset from {input_path}/input.csv, split it into train/test\n    # subsets, and write them to {output_path}/ for the Processing Operator.\n\n    data = pd.read_csv('{input_path}/input.csv')\n\n    # Split into test and train data\n    data_train, data_test = np.split(\n        data.sample(frac=1, random_state=np.random.RandomState()), [int(0.7 * len(data))]\n    )\n\n    # Remove the \"answers\" from the test set\n    data_test.drop(['class'], axis=1, inplace=True)\n\n    # Write the splits to disk\n    data_train.to_csv('{output_path}/train.csv', index=False, header=False)\n    data_test.to_csv('{output_path}/test.csv', index=False, header=False)\n\n    print('Preprocessing Done.')\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n\ndef _create_ecr_repository(repo_name):\n    execution_role_arn = boto3.client(\"sts\").get_caller_identity()[\"Arn\"]\n    access_policy = {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"Allow access to the system test execution role\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"AWS\": execution_role_arn},\n                \"Action\": \"ecr:*\",\n            }\n        ],\n    }\n\n    client = boto3.client(\"ecr\")\n    repo = client.create_repository(repositoryName=repo_name)[\"repository\"]\n    client.set_repository_policy(repositoryName=repo[\"repositoryName\"], policyText=json.dumps(access_policy))\n\n    return repo[\"repositoryUri\"]\n\n\ndef _build_and_upload_docker_image(preprocess_script, repository_uri):\n    \"\"\"\n    We need a Docker image with the following requirements:\n      - Has numpy, pandas, requests, and boto3 installed\n      - Has our data preprocessing script mounted and set as the entry point\n    \"\"\"\n    with NamedTemporaryFile(mode=\"w+t\") as preprocessing_script, NamedTemporaryFile(mode=\"w+t\") as dockerfile:\n        preprocessing_script.write(preprocess_script)\n        preprocessing_script.flush()\n\n        dockerfile.write(\n            f\"\"\"\n            FROM public.ecr.aws/amazonlinux/amazonlinux\n            COPY {preprocessing_script.name.split('/')[2]} /preprocessing.py\n            ADD credentials /credentials\n            ENV AWS_SHARED_CREDENTIALS_FILE=/credentials\n            RUN yum install python3 pip -y\n            RUN pip3 install boto3 pandas requests\n            CMD [ \"python3\", \"/preprocessing.py\"]\n            \"\"\"\n        )\n        dockerfile.flush()\n\n        ecr_region = repository_uri.split(\".\")[3]\n        docker_build_and_push_commands = f\"\"\"\n            cp /root/.aws/credentials /tmp/credentials &&\n            # login to public ecr repo containing amazonlinux image (public login is always on us east 1)\n            aws ecr-public get-login-password --region us-east-1 |\n            docker login --username AWS --password-stdin public.ecr.aws &&\n            docker build --platform=linux/amd64 -f {dockerfile.name} -t {repository_uri} /tmp &&\n            rm /tmp/credentials &&\n\n            # login again, this time to the private repo we created to hold that specific image\n            aws ecr get-login-password --region {ecr_region} |\n            docker login --username AWS --password-stdin {repository_uri} &&\n            docker push {repository_uri}\n            \"\"\"\n        logging.info(\"building and uploading docker image for preprocessing...\")\n        docker_build = subprocess.Popen(\n            docker_build_and_push_commands,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _, stderr = docker_build.communicate()\n        if docker_build.returncode != 0:\n            raise RuntimeError(\n                \"Failed to prepare docker image for the preprocessing job.\\n\"\n                \"The following error happened while executing the sequence of bash commands:\\n\"\n                f\"{stderr.decode()}\"\n            )\n\n\ndef generate_data() -> str:\n    \"\"\"generates a very simple csv dataset with headers\"\"\"\n    content = \"class,x,y\\n\"  # headers\n    for i in range(SAMPLE_SIZE):\n        content += f\"{i%100},{i},{SAMPLE_SIZE-i}\\n\"\n    return content\n\n\n@task\ndef set_up(env_id, role_arn):\n    bucket_name = f\"{env_id}-sagemaker-example\"\n    ecr_repository_name = f\"{env_id}-repo\"\n    model_name = f\"{env_id}-KNN-model\"\n    processing_job_name = f\"{env_id}-processing\"\n    training_job_name = f\"{env_id}-train\"\n    transform_job_name = f\"{env_id}-transform\"\n    tuning_job_name = f\"{env_id}-tune\"\n    model_package_group_name = f\"{env_id}-group\"\n    pipeline_name = f\"{env_id}-pipe\"\n    auto_ml_job_name = f\"{env_id}-automl\"\n    experiment_name = f\"{env_id}-experiment\"\n\n    input_data_S3_key = f\"{env_id}/processed-input-data\"\n    prediction_output_s3_key = f\"{env_id}/transform\"\n    processing_local_input_path = \"/opt/ml/processing/input\"\n    processing_local_output_path = \"/opt/ml/processing/output\"\n    raw_data_s3_key = f\"{env_id}/preprocessing/input.csv\"\n    training_output_s3_key = f\"{env_id}/results\"\n\n    ecr_repository_uri = _create_ecr_repository(ecr_repository_name)\n    region = boto3.session.Session().region_name\n    try:\n        knn_image_uri = KNN_IMAGES_BY_REGION[region]\n    except KeyError:\n        raise KeyError(\n            f\"Region name {region} does not have a known KNN \"\n            f\"Image URI.  Please add the region and URI following \"\n            f\"the directions at the top of the system testfile \"\n        )\n\n    # Json definition for a dummy pipeline of 30 chained \"conditional step\" checking that 3 < 6\n    # Each step takes roughly 1 second to execute, so the pipeline runtimes is ~30 seconds, which should be\n    # enough to test stopping and awaiting without race conditions.\n    # Built using sagemaker sdk, and using json.loads(pipeline.definition())\n    pipeline_json_definition = \"\"\"{\"Version\": \"2020-12-01\", \"Metadata\": {}, \"Parameters\": [], \"PipelineExperimentConfig\": {\"ExperimentName\": {\"Get\": \"Execution.PipelineName\"}, \"TrialName\": {\"Get\": \"Execution.PipelineExecutionId\"}}, \"Steps\": [{\"Name\": \"DummyCond29\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond28\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond27\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond26\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond25\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond24\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond23\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond22\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond21\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond20\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond19\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond18\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond17\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond16\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond15\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond14\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond13\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond12\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond11\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond10\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond9\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond8\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond7\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond6\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond5\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond4\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond3\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond2\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond1\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond0\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [{\"Name\": \"DummyCond\", \"Type\": \"Condition\", \"Arguments\": {\"Conditions\": [{\"Type\": \"LessThanOrEqualTo\", \"LeftValue\": 3.0, \"RightValue\": 6.0}], \"IfSteps\": [], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}], \"ElseSteps\": []}}]}\"\"\"  # noqa: E501\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.create_pipeline(\n        PipelineName=pipeline_name, PipelineDefinition=pipeline_json_definition, RoleArn=role_arn\n    )\n\n    resource_config = {\n        \"InstanceCount\": 1,\n        \"InstanceType\": \"ml.m5.large\",\n        \"VolumeSizeInGB\": 1,\n    }\n    input_data_uri = f\"s3://{bucket_name}/{raw_data_s3_key}\"\n    processing_config = {\n        \"ProcessingJobName\": processing_job_name,\n        \"ProcessingInputs\": [\n            {\n                \"InputName\": \"input\",\n                \"AppManaged\": False,\n                \"S3Input\": {\n                    \"S3Uri\": input_data_uri,\n                    \"LocalPath\": processing_local_input_path,\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3InputMode\": \"File\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"S3CompressionType\": \"None\",\n                },\n            },\n        ],\n        \"ProcessingOutputConfig\": {\n            \"Outputs\": [\n                {\n                    \"OutputName\": \"output\",\n                    \"S3Output\": {\n                        \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}\",\n                        \"LocalPath\": processing_local_output_path,\n                        \"S3UploadMode\": \"EndOfJob\",\n                    },\n                    \"AppManaged\": False,\n                }\n            ]\n        },\n        \"ProcessingResources\": {\n            \"ClusterConfig\": resource_config,\n        },\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"AppSpecification\": {\n            \"ImageUri\": ecr_repository_uri,\n        },\n        \"RoleArn\": role_arn,\n    }\n\n    training_data_source = {\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"text/csv\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/train.csv\",\n            }\n        },\n    }\n    training_config = {\n        \"AlgorithmSpecification\": {\n            \"TrainingImage\": knn_image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        \"HyperParameters\": {\n            \"predictor_type\": \"classifier\",\n            \"feature_dim\": \"2\",\n            \"k\": \"3\",\n            \"sample_size\": str(SAMPLE_SIZE),\n        },\n        \"InputDataConfig\": [\n            {\n                \"ChannelName\": \"train\",\n                **training_data_source,\n            }\n        ],\n        \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}/\"},\n        \"ExperimentConfig\": {\"ExperimentName\": experiment_name},\n        \"ResourceConfig\": resource_config,\n        \"RoleArn\": role_arn,\n        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        \"TrainingJobName\": training_job_name,\n    }\n    model_trained_weights = (\n        f\"s3://{bucket_name}/{training_output_s3_key}/{training_job_name}/output/model.tar.gz\"\n    )\n    model_config = {\n        \"ExecutionRoleArn\": role_arn,\n        \"ModelName\": model_name,\n        \"PrimaryContainer\": {\n            \"Mode\": \"SingleModel\",\n            \"Image\": knn_image_uri,\n            \"ModelDataUrl\": model_trained_weights,\n        },\n    }\n    tuning_config = {\n        \"HyperParameterTuningJobName\": tuning_job_name,\n        \"HyperParameterTuningJobConfig\": {\n            \"Strategy\": \"Bayesian\",\n            \"HyperParameterTuningJobObjective\": {\n                \"MetricName\": \"test:accuracy\",\n                \"Type\": \"Maximize\",\n            },\n            \"ResourceLimits\": {\n                \"MaxNumberOfTrainingJobs\": 10,\n                \"MaxParallelTrainingJobs\": 10,\n            },\n            \"ParameterRanges\": {\n                \"CategoricalParameterRanges\": [],\n                \"IntegerParameterRanges\": [\n                    # Set the min and max values of the hyperparameters you want to tune.\n                    {\n                        \"Name\": \"k\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                    {\n                        \"Name\": \"sample_size\",\n                        \"MinValue\": \"1\",\n                        \"MaxValue\": str(SAMPLE_SIZE),\n                    },\n                ],\n            },\n        },\n        \"TrainingJobDefinition\": {\n            \"StaticHyperParameters\": {\n                \"predictor_type\": \"classifier\",\n                \"feature_dim\": \"2\",\n            },\n            \"AlgorithmSpecification\": {\"TrainingImage\": knn_image_uri, \"TrainingInputMode\": \"File\"},\n            \"InputDataConfig\": [\n                {\n                    \"ChannelName\": \"train\",\n                    **training_data_source,\n                },\n                {\n                    \"ChannelName\": \"test\",\n                    **training_data_source,\n                },\n            ],\n            \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket_name}/{training_output_s3_key}\"},\n            \"ResourceConfig\": resource_config,\n            \"RoleArn\": role_arn,\n            \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60},\n        },\n    }\n    transform_config = {\n        \"TransformJobName\": transform_job_name,\n        \"TransformInput\": {\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": f\"s3://{bucket_name}/{input_data_S3_key}/test.csv\",\n                }\n            },\n            \"SplitType\": \"Line\",\n            \"ContentType\": \"text/csv\",\n        },\n        \"TransformOutput\": {\"S3OutputPath\": f\"s3://{bucket_name}/{prediction_output_s3_key}\"},\n        \"TransformResources\": {\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n        },\n        \"ModelName\": model_name,\n    }\n\n    preprocess_script = PREPROCESS_SCRIPT_TEMPLATE.format(\n        input_path=processing_local_input_path, output_path=processing_local_output_path\n    )\n    _build_and_upload_docker_image(preprocess_script, ecr_repository_uri)\n\n    ti = get_current_context()[\"ti\"]\n    ti.xcom_push(key=\"docker_image\", value=ecr_repository_uri)\n    ti.xcom_push(key=\"bucket_name\", value=bucket_name)\n    ti.xcom_push(key=\"raw_data_s3_key\", value=raw_data_s3_key)\n    ti.xcom_push(key=\"ecr_repository_name\", value=ecr_repository_name)\n    ti.xcom_push(key=\"processing_config\", value=processing_config)\n    ti.xcom_push(key=\"input_data_uri\", value=input_data_uri)\n    ti.xcom_push(key=\"output_data_uri\", value=f\"s3://{bucket_name}/{training_output_s3_key}\")\n    ti.xcom_push(key=\"training_config\", value=training_config)\n    ti.xcom_push(key=\"training_job_name\", value=training_job_name)\n    ti.xcom_push(key=\"model_package_group_name\", value=model_package_group_name)\n    ti.xcom_push(key=\"pipeline_name\", value=pipeline_name)\n    ti.xcom_push(key=\"auto_ml_job_name\", value=auto_ml_job_name)\n    ti.xcom_push(key=\"experiment_name\", value=experiment_name)\n    ti.xcom_push(key=\"model_config\", value=model_config)\n    ti.xcom_push(key=\"model_name\", value=model_name)\n    ti.xcom_push(key=\"inference_code_image\", value=knn_image_uri)\n    ti.xcom_push(key=\"model_trained_weights\", value=model_trained_weights)\n    ti.xcom_push(key=\"tuning_config\", value=tuning_config)\n    ti.xcom_push(key=\"tuning_job_name\", value=tuning_job_name)\n    ti.xcom_push(key=\"transform_config\", value=transform_config)\n    ti.xcom_push(key=\"transform_job_name\", value=transform_job_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_ecr_repository(repository_name):\n    client = boto3.client(\"ecr\")\n\n    # All images must be removed from the repo before it can be deleted.\n    image_ids = client.list_images(repositoryName=repository_name)[\"imageIds\"]\n    client.batch_delete_image(\n        repositoryName=repository_name,\n        imageIds=[{\"imageDigest\": image[\"imageDigest\"] for image in image_ids}],\n    )\n    client.delete_repository(repositoryName=repository_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_model_group(group_name, model_version_arn):\n    sgmk_client = boto3.client(\"sagemaker\")\n    # need to destroy model registered in group first\n    sgmk_client.delete_model_package(ModelPackageName=model_version_arn)\n    sgmk_client.delete_model_package_group(ModelPackageGroupName=group_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_pipeline(pipeline_name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    sgmk_client.delete_pipeline(PipelineName=pipeline_name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_experiment(name):\n    sgmk_client = boto3.client(\"sagemaker\")\n    trials = sgmk_client.list_trials(ExperimentName=name)\n    trials_names = [s[\"TrialName\"] for s in trials[\"TrialSummaries\"]]\n    for trial in trials_names:\n        components = sgmk_client.list_trial_components(TrialName=trial)\n        components_names = [s[\"TrialComponentName\"] for s in components[\"TrialComponentSummaries\"]]\n        for component in components_names:\n            sgmk_client.disassociate_trial_component(TrialComponentName=component, TrialName=trial)\n            sgmk_client.delete_trial_component(TrialComponentName=component)\n        sgmk_client.delete_trial(TrialName=trial)\n    sgmk_client.delete_experiment(ExperimentName=name)\n\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef delete_docker_image(image_name):\n    docker_build = subprocess.Popen(\n        f\"docker rmi {image_name}\",\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    _, stderr = docker_build.communicate()\n    if docker_build.returncode != 0:\n        logging.error(\n            \"Failed to delete local docker image. \"\n            \"Run 'docker images' to see if you need to clean it yourself.\\n\"\n            f\"error message: {stderr}\"\n        )\n\n\nwith DAG(\n    dag_id=DAG_ID,\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    tags=[\"example\"],\n    catchup=False,\n) as dag:\n    test_context = sys_test_context_task()\n    env_id = test_context[ENV_ID_KEY]\n\n    test_setup = set_up(\n        env_id=env_id,\n        role_arn=test_context[ROLE_ARN_KEY],\n    )\n\n    create_bucket = S3CreateBucketOperator(\n        task_id=\"create_bucket\",\n        bucket_name=test_setup[\"bucket_name\"],\n    )\n\n    upload_dataset = S3CreateObjectOperator(\n        task_id=\"upload_dataset\",\n        s3_bucket=test_setup[\"bucket_name\"],\n        s3_key=test_setup[\"raw_data_s3_key\"],\n        data=generate_data(),\n        replace=True,\n    )\n\n    # [START howto_operator_sagemaker_auto_ml]\n    automl = SageMakerAutoMLOperator(\n        task_id=\"auto_ML\",\n        job_name=test_setup[\"auto_ml_job_name\"],\n        s3_input=test_setup[\"input_data_uri\"],\n        target_attribute=\"class\",\n        s3_output=test_setup[\"output_data_uri\"],\n        role_arn=test_context[ROLE_ARN_KEY],\n        time_limit=30,  # will stop the job before it can do anything, but it's not the point here\n    )\n    # [END howto_operator_sagemaker_auto_ml]\n    automl.wait_for_completion = False  # just to be able to test the sensor next\n\n    # [START howto_sensor_sagemaker_auto_ml]\n    await_automl = SageMakerAutoMLSensor(job_name=test_setup[\"auto_ml_job_name\"], task_id=\"await_auto_ML\")\n    # [END howto_sensor_sagemaker_auto_ml]\n    await_automl.poke_interval = 10\n\n    # [START howto_operator_sagemaker_start_pipeline]\n    start_pipeline1 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline1\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n    # [END howto_operator_sagemaker_start_pipeline]\n\n    # [START howto_operator_sagemaker_stop_pipeline]\n    stop_pipeline1 = SageMakerStopPipelineOperator(\n        task_id=\"stop_pipeline1\",\n        pipeline_exec_arn=start_pipeline1.output,\n    )\n    # [END howto_operator_sagemaker_stop_pipeline]\n\n    start_pipeline2 = SageMakerStartPipelineOperator(\n        task_id=\"start_pipeline2\",\n        pipeline_name=test_setup[\"pipeline_name\"],\n    )\n\n    # [START howto_sensor_sagemaker_pipeline]\n    await_pipeline2 = SageMakerPipelineSensor(\n        task_id=\"await_pipeline2\",\n        pipeline_exec_arn=start_pipeline2.output,\n    )\n    # [END howto_sensor_sagemaker_pipeline]\n    await_pipeline2.poke_interval = 10\n\n    # [START howto_operator_sagemaker_experiment]\n    create_experiment = SageMakerCreateExperimentOperator(\n        task_id=\"create_experiment\", name=test_setup[\"experiment_name\"]\n    )\n    # [END howto_operator_sagemaker_experiment]\n\n    # [START howto_operator_sagemaker_processing]\n    preprocess_raw_data = SageMakerProcessingOperator(\n        task_id=\"preprocess_raw_data\",\n        config=test_setup[\"processing_config\"],\n    )\n    # [END howto_operator_sagemaker_processing]\n\n    # [START howto_operator_sagemaker_training]\n    train_model = SageMakerTrainingOperator(\n        task_id=\"train_model\",\n        config=test_setup[\"training_config\"],\n    )\n    # [END howto_operator_sagemaker_training]\n\n    # SageMakerTrainingOperator waits by default, setting as False to test the Sensor below.\n    train_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_training]\n    await_training = SageMakerTrainingSensor(\n        task_id=\"await_training\",\n        job_name=test_setup[\"training_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_training]\n\n    # [START howto_operator_sagemaker_model]\n    create_model = SageMakerModelOperator(\n        task_id=\"create_model\",\n        config=test_setup[\"model_config\"],\n    )\n    # [END howto_operator_sagemaker_model]\n\n    # [START howto_operator_sagemaker_register]\n    register_model = SageMakerRegisterModelVersionOperator(\n        task_id=\"register_model\",\n        image_uri=test_setup[\"inference_code_image\"],\n        model_url=test_setup[\"model_trained_weights\"],\n        package_group_name=test_setup[\"model_package_group_name\"],\n    )\n    # [END howto_operator_sagemaker_register]\n\n    # [START howto_operator_sagemaker_tuning]\n    tune_model = SageMakerTuningOperator(\n        task_id=\"tune_model\",\n        config=test_setup[\"tuning_config\"],\n    )\n    # [END howto_operator_sagemaker_tuning]\n\n    # SageMakerTuningOperator waits by default, setting as False to test the Sensor below.\n    tune_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_tuning]\n    await_tuning = SageMakerTuningSensor(\n        task_id=\"await_tuning\",\n        job_name=test_setup[\"tuning_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_tuning]\n\n    # [START howto_operator_sagemaker_transform]\n    test_model = SageMakerTransformOperator(\n        task_id=\"test_model\",\n        config=test_setup[\"transform_config\"],\n    )\n    # [END howto_operator_sagemaker_transform]\n\n    # SageMakerTransformOperator waits by default, setting as False to test the Sensor below.\n    test_model.wait_for_completion = False\n\n    # [START howto_sensor_sagemaker_transform]\n    await_transform = SageMakerTransformSensor(\n        task_id=\"await_transform\",\n        job_name=test_setup[\"transform_job_name\"],\n    )\n    # [END howto_sensor_sagemaker_transform]\n\n    # [START howto_operator_sagemaker_delete_model]\n    delete_model = SageMakerDeleteModelOperator(\n        task_id=\"delete_model\",\n        config={\"ModelName\": test_setup[\"model_name\"]},\n    )\n    # [END howto_operator_sagemaker_delete_model]\n    delete_model.trigger_rule = TriggerRule.ALL_DONE\n\n    delete_bucket = S3DeleteBucketOperator(\n        task_id=\"delete_bucket\",\n        trigger_rule=TriggerRule.ALL_DONE,\n        bucket_name=test_setup[\"bucket_name\"],\n        force_delete=True,\n    )\n\n    log_cleanup = prune_logs(\n        [\n            # Format: ('log group name', 'log stream prefix')\n            (\"/aws/sagemaker/ProcessingJobs\", env_id),\n            (\"/aws/sagemaker/TrainingJobs\", env_id),\n            (\"/aws/sagemaker/TransformJobs\", env_id),\n        ]\n    )\n\n    chain(\n        # TEST SETUP\n        test_context,\n        test_setup,\n        create_bucket,\n        upload_dataset,\n        # TEST BODY\n        automl,\n        await_automl,\n        start_pipeline1,\n        start_pipeline2,\n        stop_pipeline1,\n        await_pipeline2,\n        create_experiment,\n        preprocess_raw_data,\n        train_model,\n        await_training,\n        create_model,\n        register_model,\n        tune_model,\n        await_tuning,\n        test_model,\n        await_transform,\n        # TEST TEARDOWN\n        delete_ecr_repository(test_setup[\"ecr_repository_name\"]),\n        delete_model_group(test_setup[\"model_package_group_name\"], register_model.output),\n        delete_model,\n        delete_bucket,\n        delete_experiment(test_setup[\"experiment_name\"]),\n        delete_pipeline(test_setup[\"pipeline_name\"]),\n        delete_docker_image(test_setup[\"docker_image\"]),\n        log_cleanup,\n    )\n\n    from tests.system.utils.watcher import watcher\n\n    # This test needs watcher in order to properly mark success/failure\n    # when \"tearDown\" task with trigger rule is part of the DAG\n    list(dag.tasks) >> watcher()\n\nfrom tests.system.utils import get_test_run  # noqa: E402\n\n# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\ntest_run = get_test_run(dag)", "metadata": {"file_name": "example_sagemaker.py", "instruction_source": "claude-3.5", "instruction_type": "alternative", "claude_complexity_score": 2, "claude_complexity_level": "medium", "claude_complexity_reasoning": "Uses multiple AWS operators, involves custom preprocessing, demonstrates task flow API, includes sensors and cloud service interactions", "key_concepts": ["@task Decorator", "AWS Service Operators", "Experiment Tracking", "Model Versioning", "Cloud Data Processing"], "learning_objectives": ["Implement task-level data science workflows", "Understand AWS SageMaker integration patterns", "Learn experiment tracking techniques", "Master modular ML pipeline design"]}}
