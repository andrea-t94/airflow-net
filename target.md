## Repository Overview

* **Root Philosophy:** "Library First." The core logic is a Python package (`src/airflow_net`) that handles generation and validation.
* **Distribution:** Users install the package to run the agent locally via CLI, MCP (Claude), or HTTP (Cursor).
* **Research:** All data mining and training logic lives in `research/` and *imports* the core library to ensure the validator used for training is identical to the one used for inference.

---

## Directory Tree

```text
airflow-net/
â”œâ”€â”€ pyproject.toml              # main dependency definition (for the library)
â”œâ”€â”€ README.md                   # Documentation for the library
â”œâ”€â”€ Makefile                    # Shortcuts: install, test, serve, research-setup
â”‚
â”œâ”€â”€ src/                        # ðŸ“¦ THE PRODUCT (Pip installable package)
â”‚   â””â”€â”€ airflow_net/
â”‚       â”œâ”€â”€ __init__.py         # Exports AirflowAgent, DAGValidator
â”‚       â”œâ”€â”€ agent.py            # The "Loop": Orchestrates Generation -> Validation -> Retry
â”‚       â”œâ”€â”€ engine.py           # The "Muscle": Wraps llama.cpp & Hardware Detection
â”‚       â”œâ”€â”€ validation.py       # The "Brain": Pure logic AST validator (Refactored from lib/dag_parser.py)
â”‚       â”œâ”€â”€ cli.py              # Entry point for `airflow-net` command
â”‚       â”œâ”€â”€ utils.py            # Shared utilities (logging, config loading)
â”‚       â””â”€â”€ interfaces/         # ðŸ”Œ Connectors for external tools
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ mcp.py          # Model Context Protocol server (for Claude Desktop)
â”‚           â””â”€â”€ http.py         # OpenAI-compatible API server (for Cursor/VS Code)
â”‚
â”œâ”€â”€ research/                   # ðŸ­ THE FACTORY (Data creation & Training)
â”‚   â”œâ”€â”€ requirements.txt        # Shared dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                   # Step 1: Dataset Creation
â”‚   â”‚   â”œâ”€â”€ config/             # Configuration for mining/generation
â”‚   â”‚   â”‚   â”œâ”€â”€ mining_config.yaml
â”‚   â”‚   â”‚   â””â”€â”€ generation_config.yaml
â”‚   â”‚   â”œâ”€â”€ lib/                # Data generation logic
â”‚   â”‚   â”‚   â”œâ”€â”€ mining.py
â”‚   â”‚   â”‚   â”œâ”€â”€ instruction.py
â”‚   â”‚   â”‚   â””â”€â”€ config_loader.py    # Configuration and environment utilities
â”‚   â”‚   â””â”€â”€ scripts/            # Workflow steps
â”‚   â”‚       â”œâ”€â”€ 01_mine_dags.py
â”‚   â”‚       â”œâ”€â”€ 02_gen_instruct.py
â”‚   â”‚       â””â”€â”€ 03_upload_hf.py
â”‚   â”‚
â”‚   â””â”€â”€ finetuning/             # Step 2: Model Training
â”‚       â””â”€â”€ notebooks/
â”‚           â”œâ”€â”€ colab_inference.ipynb
â”‚           â”œâ”€â”€ model_evaluation.ipynb
â”‚           â””â”€â”€ finetune.ipynb
â”‚
â””â”€â”€ tests/                      # Unit tests
    â””â”€â”€ test_validation.py      # Critical: Ensure DAGValidator catches known bad syntax

```

---

## Component Details

### 1. `src/airflow_net` (The Library)

*Dependencies: `llama-cpp-python`, `pydantic`, `click`, `mcp`.*

* **`validation.py`**:
* **Source:** Refactored from `lib/dag_parser.py`.
* **Changes:** Removed file I/O. Now accepts a string (`code_content`) and returns a list of `ValidationError` objects. This is the "Shared Kernel."


* **`engine.py`**:
* **Source:** Adapted from `scripts/dag_generation_llamacpp.py`.
* **Responsibilities:**
* `ModelEngine` class: Handles loading GGUF models.
* **Hardware Detection:** Automatically sets `n_gpu_layers` based on `torch.cuda.is_available()` or MPS (Mac) checks, replacing the old `.sh` scripts.




* **`agent.py`**:
* **New Component.**
* **Logic:** Implements the **Self-Correction Loop**.
1. Call `engine.generate(prompt)`.
2. Call `validator.validate(code)`.
3. If errors exist, append them to prompt and recurse (up to `max_retries`).




* **`cli.py`**:
* **Commands:**
* `airflow-net install`: Downloads the recommended GGUF model from your HF repo.
* `airflow-net serve`: Launches the HTTP server (for Cursor).
* `airflow-net mcp`: Launches the MCP server (for Claude).





### 2. `research/` (The Factory)

*Dependencies: `src`, `torch`, `unsloth`, `github`.*

* **`lib/mining.py`**:
* **Source:** Refactored from `lib/dag_miner.py`.
* **Crucial Integration:** It imports `DAGValidator` from `src`.
```python
from airflow_net.validation import DAGValidator
# ...
validator = DAGValidator()
errors = validator.validate(content) # Filter training data using the EXACT same logic as inference.

```




* **`scripts/`**:
* **`01_mine_dags.py`**: Runs the miner, saves raw JSONL.
* **`02_gen_instruct.py`**: Runs the instruction generator (using OpenAI/Claude API) to create the training pairs.
* **`03_upload_hf.py`**: Replaces `prepare_and_upload_dataset.py`.



### 3. Root Configuration

* **`pyproject.toml`**:
* Defines the project as an installable package.
* Defines the `[project.scripts]` entry point so users can just type `airflow-net`.


* **`Makefile`**:
* `install`: `pip install -e .` (Installs the library).
* `research-install`: `pip install -r research/requirements.txt` (Installs ML tools).
* `test`: `pytest tests/`.



---

## Workflow Summary

1. **For You ( The Researcher):**
* Run `make research-install`.
* Run `python research/scripts/01_mine_dags.py` (Uses `src` validator to clean data).
* Open `research/notebooks/finetune.ipynb` to train.
* Run `python research/scripts/03_upload_hf.py` to publish the GGUF.


2. **For the User (The Consumer):**
* `pip install airflow-net`
* `airflow-net install` (Downloads your GGUF).
* **Cursor User:** `airflow-net serve` -> Connect Cursor to `localhost:8000`.
* **Claude User:** Add `airflow-net mcp` to Claude Desktop config.